<!DOCTYPE html>
<html>
<head>
<title>2023-07-12-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.04770">Predicting Outcomes in Long COVID Patients with Spatiotemporal Attention. (arXiv:2307.04770v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hao_D/0/1/0/all/0/1">Degan Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Negahdar_M/0/1/0/all/0/1">Mohammadreza Negahdar</a></p>
<p>Long COVID is a general term of post-acute sequelae of COVID-19. Patients
with long COVID can endure long-lasting symptoms including fatigue, headache,
dyspnea and anosmia, etc. Identifying the cohorts with severe long-term
complications in COVID-19 could benefit the treatment planning and resource
arrangement. However, due to the heterogeneous phenotype presented in long
COVID patients, it is difficult to predict their outcomes from their
longitudinal data. In this study, we proposed a spatiotemporal attention
mechanism to weigh feature importance jointly from the temporal dimension and
feature space. Considering that medical examinations can have interchangeable
orders in adjacent time points, we restricted the learning of short-term
dependency with a Local-LSTM and the learning of long-term dependency with the
joint spatiotemporal attention. We also compared the proposed method with
several state-of-the-art methods and a method in clinical practice. The methods
are evaluated on a hard-to-acquire clinical dataset of patients with long
COVID. Experimental results show the Local-LSTM with joint spatiotemporal
attention outperformed related methods in outcome prediction. The proposed
method provides a clinical tool for the severity assessment of long COVID.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04772">Digital Twins for Patient Care via Knowledge Graphs and Closed-Form Continuous-Time Liquid Neural Networks. (arXiv:2307.04772v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nye_L/0/1/0/all/0/1">Logan Nye</a></p>
<p>Digital twin technology has is anticipated to transform healthcare, enabling
personalized medicines and support, earlier diagnoses, simulated treatment
outcomes, and optimized surgical plans. Digital twins are readily gaining
traction in industries like manufacturing, supply chain logistics, and civil
infrastructure. Not in patient care, however. The challenge of modeling complex
diseases with multimodal patient data and the computational complexities of
analyzing it have stifled digital twin adoption in the biomedical vertical.
Yet, these major obstacles can potentially be handled by approaching these
models in a different way. This paper proposes a novel framework for addressing
the barriers to clinical twin modeling created by computational costs and
modeling complexities. We propose structuring patient health data as a
knowledge graph and using closed-form continuous-time liquid neural networks,
for real-time analytics. By synthesizing multimodal patient data and leveraging
the flexibility and efficiency of closed form continuous time networks and
knowledge graph ontologies, our approach enables real time insights,
personalized medicine, early diagnosis and intervention, and optimal surgical
planning. This novel approach provides a comprehensive and adaptable view of
patient health along with real-time analytics, paving the way for digital twin
simulations and other anticipated benefits in healthcare.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04777">MentalHealthAI: Utilizing Personal Health Device Data to Optimize Psychiatry Treatment. (arXiv:2307.04777v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1">Manan Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Seneviratne_O/0/1/0/all/0/1">Oshani Seneviratne</a></p>
<p>Mental health disorders remain a significant challenge in modern healthcare,
with diagnosis and treatment often relying on subjective patient descriptions
and past medical history. To address this issue, we propose a personalized
mental health tracking and mood prediction system that utilizes patient
physiological data collected through personal health devices. Our system
leverages a decentralized learning mechanism that combines transfer and
federated machine learning concepts using smart contracts, allowing data to
remain on users' devices and enabling effective tracking of mental health
conditions for psychiatric treatment and management in a privacy-aware and
accountable manner. We evaluate our model using a popular mental health dataset
that demonstrates promising results. By utilizing connected health systems and
machine learning models, our approach offers a novel solution to the challenge
of providing psychiatrists with further insight into their patients' mental
health outside of traditional office visits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04778">Formulating A Strategic Plan Based On Statistical Analyses And Applications For Financial Companies Through A Real-World Use Case. (arXiv:2307.04778v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sarraf_S/0/1/0/all/0/1">Saman Sarraf</a></p>
<p>Business statistics play a crucial role in implementing a data-driven
strategic plan at the enterprise level to employ various analytics where the
outcomes of such a plan enable an enterprise to enhance the decision-making
process or to mitigate risks to the organization. In this work, a strategic
plan informed by the statistical analysis is introduced for a financial company
called LendingClub, where the plan is comprised of exploring the possibility of
onboarding a big data platform along with advanced feature selection
capacities. The main objectives of such a plan are to increase the company's
revenue while reducing the risks of granting loans to borrowers who cannot
return their loans. In this study, different hypotheses formulated to address
the company's concerns are studied, where the results reveal that the amount of
loans profoundly impacts the number of borrowers charging off their loans.
Also, the proposed strategic plan includes onboarding advanced analytics such
as machine learning technologies that allow the company to build better
generalized data-driven predictive models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04780">Comparison of Point Cloud and Image-based Models for Calorimeter Fast Simulation. (arXiv:2307.04780v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Acosta_F/0/1/0/all/0/1">Fernando Torales Acosta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikuni_V/0/1/0/all/0/1">Vinicius Mikuni</a>, <a href="http://arxiv.org/find/cs/1/au:+Nachman_B/0/1/0/all/0/1">Benjamin Nachman</a>, <a href="http://arxiv.org/find/cs/1/au:+Arratia_M/0/1/0/all/0/1">Miguel Arratia</a>, <a href="http://arxiv.org/find/cs/1/au:+Barish_K/0/1/0/all/0/1">Kenneth Barish</a>, <a href="http://arxiv.org/find/cs/1/au:+Karki_B/0/1/0/all/0/1">Bishnu Karki</a>, <a href="http://arxiv.org/find/cs/1/au:+Milton_R/0/1/0/all/0/1">Ryan Milton</a>, <a href="http://arxiv.org/find/cs/1/au:+Karande_P/0/1/0/all/0/1">Piyush Karande</a>, <a href="http://arxiv.org/find/cs/1/au:+Angerami_A/0/1/0/all/0/1">Aaron Angerami</a></p>
<p>Score based generative models are a new class of generative models that have
been shown to accurately generate high dimensional calorimeter datasets. Recent
advances in generative models have used images with 3D voxels to represent and
model complex calorimeter showers. Point clouds, however, are likely a more
natural representation of calorimeter showers, particularly in calorimeters
with high granularity. Point clouds preserve all of the information of the
original simulation, more naturally deal with sparse datasets, and can be
implemented with more compact models and data files. In this work, two
state-of-the-art score based models are trained on the same set of calorimeter
simulation and directly compared.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04787">Collaborative Score Distillation for Consistent Visual Synthesis. (arXiv:2307.04787v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Subin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungmin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">June Suk Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1">Jongheon Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1">Kihyuk Sohn</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1">Jinwoo Shin</a></p>
<p>Generative priors of large-scale text-to-image diffusion models enable a wide
range of new generation and editing applications on diverse visual modalities.
However, when adapting these priors to complex visual modalities, often
represented as multiple images (e.g., video), achieving consistency across a
set of images is challenging. In this paper, we address this challenge with a
novel method, Collaborative Score Distillation (CSD). CSD is based on the Stein
Variational Gradient Descent (SVGD). Specifically, we propose to consider
multiple samples as "particles" in the SVGD update and combine their score
functions to distill generative priors over a set of images synchronously.
Thus, CSD facilitates seamless integration of information across 2D images,
leading to a consistent visual synthesis across multiple samples. We show the
effectiveness of CSD in a variety of tasks, encompassing the visual editing of
panorama images, videos, and 3D scenes. Our results underline the competency of
CSD as a versatile method for enhancing inter-sample consistency, thereby
broadening the applicability of text-to-image diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04817">A physics-constrained machine learning method for mapping gapless land surface temperature. (arXiv:2307.04817v1 [physics.ao-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Ma_J/0/1/0/all/0/1">Jun Ma</a>, <a href="http://arxiv.org/find/physics/1/au:+Shen_H/0/1/0/all/0/1">Huanfeng Shen</a>, <a href="http://arxiv.org/find/physics/1/au:+Jiang_M/0/1/0/all/0/1">Menghui Jiang</a>, <a href="http://arxiv.org/find/physics/1/au:+Lin_L/0/1/0/all/0/1">Liupeng Lin</a>, <a href="http://arxiv.org/find/physics/1/au:+Meng_C/0/1/0/all/0/1">Chunlei Meng</a>, <a href="http://arxiv.org/find/physics/1/au:+Zeng_C/0/1/0/all/0/1">Chao Zeng</a>, <a href="http://arxiv.org/find/physics/1/au:+Li_H/0/1/0/all/0/1">Huifang Li</a>, <a href="http://arxiv.org/find/physics/1/au:+Wu_P/0/1/0/all/0/1">Penghai Wu</a></p>
<p>More accurate, spatio-temporally, and physically consistent LST estimation
has been a main interest in Earth system research. Developing physics-driven
mechanism models and data-driven machine learning (ML) models are two major
paradigms for gapless LST estimation, which have their respective advantages
and disadvantages. In this paper, a physics-constrained ML model, which
combines the strengths in the mechanism model and ML model, is proposed to
generate gapless LST with physical meanings and high accuracy. The hybrid model
employs ML as the primary architecture, under which the input variable physical
constraints are incorporated to enhance the interpretability and extrapolation
ability of the model. Specifically, the light gradient-boosting machine (LGBM)
model, which uses only remote sensing data as input, serves as the pure ML
model. Physical constraints (PCs) are coupled by further incorporating key
Community Land Model (CLM) forcing data (cause) and CLM simulation data
(effect) as inputs into the LGBM model. This integration forms the PC-LGBM
model, which incorporates surface energy balance (SEB) constraints underlying
the data in CLM-LST modeling within a biophysical framework. Compared with a
pure physical method and pure ML methods, the PC-LGBM model improves the
prediction accuracy and physical interpretability of LST. It also demonstrates
a good extrapolation ability for the responses to extreme weather cases,
suggesting that the PC-LGBM model enables not only empirical learning from data
but also rationally derived from theory. The proposed method represents an
innovative way to map accurate and physically interpretable gapless LST, and
could provide insights to accelerate knowledge discovery in land surface
processes and data mining in geographical parameter estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04822">On Detecting Some Defective Items in Group Testing. (arXiv:2307.04822v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bshouty_N/0/1/0/all/0/1">Nader H. Bshouty</a>, <a href="http://arxiv.org/find/cs/1/au:+Haddad_Zaknoon_C/0/1/0/all/0/1">Catherine A. Haddad-Zaknoon</a></p>
<p>Group testing is an approach aimed at identifying up to $d$ defective items
among a total of $n$ elements. This is accomplished by examining subsets to
determine if at least one defective item is present. In our study, we focus on
the problem of identifying a subset of $\ell\leq d$ defective items. We develop
upper and lower bounds on the number of tests required to detect $\ell$
defective items in both the adaptive and non-adaptive settings while
considering scenarios where no prior knowledge of $d$ is available, and
situations where an estimate of $d$ or at least some non-trivial upper bound on
$d$ is available.
</p>
<p>When no prior knowledge on $d$ is available, we prove a lower bound of $
\Omega(\frac{\ell \log^2n}{\log \ell +\log\log n})$ tests in the randomized
non-adaptive settings and an upper bound of $O(\ell \log^2 n)$ for the same
settings. Furthermore, we demonstrate that any non-adaptive deterministic
algorithm must ask $\Theta(n)$ tests, signifying a fundamental limitation in
this scenario. For adaptive algorithms, we establish tight bounds in different
scenarios. In the deterministic case, we prove a tight bound of
$\Theta(\ell\log{(n/\ell)})$. Moreover, in the randomized settings, we derive a
tight bound of $\Theta(\ell\log{(n/d)})$.
</p>
<p>When $d$, or at least some non-trivial estimate of $d$, is known, we prove a
tight bound of $\Theta(d\log (n/d))$ for the deterministic non-adaptive
settings, and $\Theta(\ell\log(n/d))$ for the randomized non-adaptive settings.
In the adaptive case, we present an upper bound of $O(\ell \log (n/\ell))$ for
the deterministic settings, and a lower bound of $\Omega(\ell\log(n/d)+\log
n)$. Additionally, we establish a tight bound of $\Theta(\ell \log(n/d))$ for
the randomized adaptive settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04838">CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Subramanyam_R/0/1/0/all/0/1">Rakshith Subramanyam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayram_T/0/1/0/all/0/1">T. S. Jayram</a>, <a href="http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1">Rushil Anirudh</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1">Jayaraman J. Thiagarajan</a></p>
<p>In this paper, we explore the potential of Vision-Language Models (VLMs),
specifically CLIP, in predicting visual object relationships, which involves
interpreting visual features from images into language-based relations. Current
state-of-the-art methods use complex graphical models that utilize language
cues and visual features to address this challenge. We hypothesize that the
strong language priors in CLIP embeddings can simplify these graphical models
paving for a simpler approach. We adopt the UVTransE relation prediction
framework, which learns the relation as a translational embedding with subject,
object, and union box embeddings from a scene. We systematically explore the
design of CLIP-based subject, object, and union-box representations within the
UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate
Estimation). CREPE utilizes text-based representations for all three bounding
boxes and introduces a novel contrastive training strategy to automatically
infer the text prompt for union-box. Our approach achieves state-of-the-art
performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual
Genome benchmark, achieving a 15.3\% gain in performance over recent
state-of-the-art at mR@20. This work demonstrates CLIP's effectiveness in
object relation prediction and encourages further research on VLMs in this
challenging domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04841">Dynamics of Temporal Difference Reinforcement Learning. (arXiv:2307.04841v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1">Blake Bordelon</a>, <a href="http://arxiv.org/find/stat/1/au:+Masset_P/0/1/0/all/0/1">Paul Masset</a>, <a href="http://arxiv.org/find/stat/1/au:+Kuo_H/0/1/0/all/0/1">Henry Kuo</a>, <a href="http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1">Cengiz Pehlevan</a></p>
<p>Reinforcement learning has been successful across several applications in
which agents have to learn to act in environments with sparse feedback.
However, despite this empirical success there is still a lack of theoretical
understanding of how the parameters of reinforcement learning models and the
features used to represent states interact to control the dynamics of learning.
In this work, we use concepts from statistical physics, to study the typical
case learning curves for temporal difference learning of a value function with
linear function approximators. Our theory is derived under a Gaussian
equivalence hypothesis where averages over the random trajectories are replaced
with temporally correlated Gaussian feature averages and we validate our
assumptions on small scale Markov Decision Processes. We find that the
stochastic semi-gradient noise due to subsampling the space of possible
episodes leads to significant plateaus in the value error, unlike in
traditional gradient descent dynamics. We study how learning dynamics and
plateaus depend on feature structure, learning rate, discount factor, and
reward function. We then analyze how strategies like learning rate annealing
and reward shaping can favorably alter learning dynamics and plateaus. To
conclude, our work introduces new tools to open a new direction towards
developing a theory of learning dynamics in reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04849">SigOpt Mulch: An Intelligent System for AutoML of Gradient Boosted Trees. (arXiv:2307.04849v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sorokin_A/0/1/0/all/0/1">Aleksei Sorokin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xinran Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1">Eric Hans Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_B/0/1/0/all/0/1">Bolong Cheng</a></p>
<p>Gradient boosted trees (GBTs) are ubiquitous models used by researchers,
machine learning (ML) practitioners, and data scientists because of their
robust performance, interpretable behavior, and ease-of-use. One critical
challenge in training GBTs is the tuning of their hyperparameters. In practice,
selecting these hyperparameters is often done manually. Recently, the ML
community has advocated for tuning hyperparameters through black-box
optimization and developed state-of-the-art systems to do so. However, applying
such systems to tune GBTs suffers from two drawbacks. First, these systems are
not \textit{model-aware}, rather they are designed to apply to a
\textit{generic} model; this leaves significant optimization performance on the
table. Second, using these systems requires \textit{domain knowledge} such as
the choice of hyperparameter search space, which is an antithesis to the
automatic experimentation that black-box optimization aims to provide. In this
paper, we present SigOpt Mulch, a model-aware hyperparameter tuning system
specifically designed for automated tuning of GBTs that provides two
improvements over existing systems. First, Mulch leverages powerful techniques
in metalearning and multifidelity optimization to perform model-aware
hyperparameter optimization. Second, it automates the process of learning
performant hyperparameters by making intelligent decisions about the
optimization search space, thus reducing the need for user domain knowledge.
These innovations allow Mulch to identify good GBT hyperparameters far more
efficiently -- and in a more seamless and user-friendly way -- than existing
black-box hyperparameter tuning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04850">SHAP@k:Efficient and Probably Approximately Correct (PAC) Identification of Top-k Features. (arXiv:2307.04850v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kariyappa_S/0/1/0/all/0/1">Sanjay Kariyappa</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsepenekas_L/0/1/0/all/0/1">Leonidas Tsepenekas</a>, <a href="http://arxiv.org/find/cs/1/au:+Lecue_F/0/1/0/all/0/1">Freddy L&#xe9;cu&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1">Daniele Magazzeni</a></p>
<p>The SHAP framework provides a principled method to explain the predictions of
a model by computing feature importance. Motivated by applications in finance,
we introduce the Top-k Identification Problem (TkIP), where the objective is to
identify the k features with the highest SHAP values. While any method to
compute SHAP values with uncertainty estimates (such as KernelSHAP and
SamplingSHAP) can be trivially adapted to solve TkIP, doing so is highly sample
inefficient. The goal of our work is to improve the sample efficiency of
existing methods in the context of solving TkIP. Our key insight is that TkIP
can be framed as an Explore-m problem--a well-studied problem related to
multi-armed bandits (MAB). This connection enables us to improve sample
efficiency by leveraging two techniques from the MAB literature: (1) a better
stopping-condition (to stop sampling) that identifies when PAC (Probably
Approximately Correct) guarantees have been met and (2) a greedy sampling
scheme that judiciously allocates samples between different features. By
adopting these methods we develop KernelSHAP@k and SamplingSHAP@k to
efficiently solve TkIP, offering an average improvement of $5\times$ in
sample-efficiency and runtime across most common credit related datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04859">Articulated 3D Head Avatar Generation using Text-to-Image Diffusion Models. (arXiv:2307.04859v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bergman_A/0/1/0/all/0/1">Alexander W. Bergman</a>, <a href="http://arxiv.org/find/cs/1/au:+Yifan_W/0/1/0/all/0/1">Wang Yifan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1">Gordon Wetzstein</a></p>
<p>The ability to generate diverse 3D articulated head avatars is vital to a
plethora of applications, including augmented reality, cinematography, and
education. Recent work on text-guided 3D object generation has shown great
promise in addressing these needs. These methods directly leverage pre-trained
2D text-to-image diffusion models to generate 3D-multi-view-consistent radiance
fields of generic objects. However, due to the lack of geometry and texture
priors, these methods have limited control over the generated 3D objects,
making it difficult to operate inside a specific domain, e.g., human heads. In
this work, we develop a new approach to text-guided 3D head avatar generation
to address this limitation. Our framework directly operates on the geometry and
texture of an articulable 3D morphable model (3DMM) of a head, and introduces
novel optimization procedures to update the geometry and texture while keeping
the 2D and 3D facial features aligned. The result is a 3D head avatar that is
consistent with the text description and can be readily articulated using the
deformation model of the 3DMM. We show that our diffusion-based articulated
head avatars outperform state-of-the-art approaches for this task. The latter
are typically based on CLIP, which is known to provide limited diversity of
generation and accuracy for 3D object generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04866">Automated Detection of Gait Events and Travel Distance Using Waist-worn Accelerometers Across a Typical Range of Walking and Running Speeds. (arXiv:2307.04866v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ramli_A/0/1/0/all/0/1">Albara Ah Ramli</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Berndt_K/0/1/0/all/0/1">Kelly Berndt</a>, <a href="http://arxiv.org/find/eess/1/au:+Chuah_C/0/1/0/all/0/1">Chen-Nee Chuah</a>, <a href="http://arxiv.org/find/eess/1/au:+Goude_E/0/1/0/all/0/1">Erica Goude</a>, <a href="http://arxiv.org/find/eess/1/au:+Kaethler_L/0/1/0/all/0/1">Lynea B. Kaethler</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_A/0/1/0/all/0/1">Amanda Lopez</a>, <a href="http://arxiv.org/find/eess/1/au:+Nicorici_A/0/1/0/all/0/1">Alina Nicorici</a>, <a href="http://arxiv.org/find/eess/1/au:+Owens_C/0/1/0/all/0/1">Corey Owens</a>, <a href="http://arxiv.org/find/eess/1/au:+Rodriguez_D/0/1/0/all/0/1">David Rodriguez</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jane Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Aranki_D/0/1/0/all/0/1">Daniel Aranki</a>, <a href="http://arxiv.org/find/eess/1/au:+McDonald_C/0/1/0/all/0/1">Craig M. McDonald</a>, <a href="http://arxiv.org/find/eess/1/au:+Henricson_E/0/1/0/all/0/1">Erik K. Henricson</a></p>
<p>Background: Estimation of temporospatial clinical features of gait (CFs),
such as step count and length, step duration, step frequency, gait speed and
distance traveled is an important component of community-based mobility
evaluation using wearable accelerometers. However, challenges arising from
device complexity and availability, cost and analytical methodology have
limited widespread application of such tools. Research Question: Can
accelerometer data from commercially-available smartphones be used to extract
gait CFs across a broad range of attainable gait velocities in children with
Duchenne muscular dystrophy (DMD) and typically developing controls (TDs) using
machine learning (ML)-based methods Methods: Fifteen children with DMD and 15
TDs underwent supervised clinical testing across a range of gait speeds using
10 or 25m run/walk (10MRW, 25MRW), 100m run/walk (100MRW), 6-minute walk (6MWT)
and free-walk (FW) evaluations while wearing a mobile phone-based accelerometer
at the waist near the body's center of mass. Gait CFs were extracted from the
accelerometer data using a multi-step machine learning-based process and
results were compared to ground-truth observation data. Results: Model
predictions vs. observed values for step counts, distance traveled, and step
length showed a strong correlation (Pearson's r = -0.9929 to 0.9986, p&lt;0.0001).
The estimates demonstrated a mean (SD) percentage error of 1.49% (7.04%) for
step counts, 1.18% (9.91%) for distance traveled, and 0.37% (7.52%) for step
length compared to ground truth observations for the combined 6MWT, 100MRW, and
FW tasks. Significance: The study findings indicate that a single accelerometer
placed near the body's center of mass can accurately measure CFs across
different gait speeds in both TD and DMD peers, suggesting that there is
potential for accurately measuring CFs in the community with consumer-level
smartphones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04868">Leveraging an Alignment Set in Tackling Instance-Dependent Label Noise. (arXiv:2307.04868v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tjandra_D/0/1/0/all/0/1">Donna Tjandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Wiens_J/0/1/0/all/0/1">Jenna Wiens</a></p>
<p>Noisy training labels can hurt model performance. Most approaches that aim to
address label noise assume label noise is independent from the input features.
In practice, however, label noise is often feature or
\textit{instance-dependent}, and therefore biased (i.e., some instances are
more likely to be mislabeled than others). E.g., in clinical care, female
patients are more likely to be under-diagnosed for cardiovascular disease
compared to male patients. Approaches that ignore this dependence can produce
models with poor discriminative performance, and in many healthcare settings,
can exacerbate issues around health disparities. In light of these limitations,
we propose a two-stage approach to learn in the presence instance-dependent
label noise. Our approach utilizes \textit{\anchor points}, a small subset of
data for which we know the observed and ground truth labels. On several tasks,
our approach leads to consistent improvements over the state-of-the-art in
discriminative performance (AUROC) while mitigating bias (area under the
equalized odds curve, AUEOC). For example, when predicting acute respiratory
failure onset on the MIMIC-III dataset, our approach achieves a harmonic mean
(AUROC and AUEOC) of 0.84 (SD [standard deviation] 0.01) while that of the next
best baseline is 0.81 (SD 0.01). Overall, our approach improves accuracy while
mitigating potential bias compared to existing approaches in the presence of
instance-dependent label noise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04869">Fed-CPrompt: Contrastive Prompt for Rehearsal-Free Federated Continual Learning. (arXiv:2307.04869v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagwe_G/0/1/0/all/0/1">Gaurav Bagwe</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoyong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1">Miao Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lan Zhang</a></p>
<p>Federated continual learning (FCL) learns incremental tasks over time from
confidential datasets distributed across clients. This paper focuses on
rehearsal-free FCL, which has severe forgetting issues when learning new tasks
due to the lack of access to historical task data. To address this issue, we
propose Fed-CPrompt based on prompt learning techniques to obtain task-specific
prompts in a communication-efficient way. Fed-CPrompt introduces two key
components, asynchronous prompt learning, and contrastive continual loss, to
handle asynchronous task arrival and heterogeneous data distributions in FCL,
respectively. Extensive experiments demonstrate the effectiveness of
Fed-CPrompt in achieving SOTA rehearsal-free FCL performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04870">Onion Universe Algorithm: Applications in Weakly Supervised Learning. (arXiv:2307.04870v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Na_W/0/1/0/all/0/1">Woojoo Na</a></p>
<p>We introduce Onion Universe Algorithm (OUA), a novel classification method in
ensemble learning. In particular, we show its applicability as a label model
for weakly supervised learning. OUA offers simplicity in implementation,
computational efficiency, and does not rely on any assumptions regarding the
data or weak signals. The model is well suited for scenarios where fully
labeled data is not available. Our method is built upon geometrical
interpretation of the space spanned by weak signals. Empirical results support
our analysis of the hidden geometric structure underlying general set of weak
signals and also illustrates that OUA works well in practice. We show empirical
evidence that OUA performs favorably on common benchmark datasets compared to
existing label models for weakly supervised learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04887">Measuring and Mitigating Interference in Reinforcement Learning. (arXiv:2307.04887v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_V/0/1/0/all/0/1">Vincent Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Han Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Ruo Yu Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Javed_K/0/1/0/all/0/1">Khurram Javed</a>, <a href="http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1">Adam White</a>, <a href="http://arxiv.org/find/cs/1/au:+White_M/0/1/0/all/0/1">Martha White</a></p>
<p>Catastrophic interference is common in many network-based learning systems,
and many proposals exist for mitigating it. Before overcoming interference we
must understand it better. In this work, we provide a definition and novel
measure of interference for value-based reinforcement learning methods such as
Fitted Q-Iteration and DQN. We systematically evaluate our measure of
interference, showing that it correlates with instability in control
performance, across a variety of network architectures. Our new interference
measure allows us to ask novel scientific questions about commonly used deep
learning architectures and study learning algorithms which mitigate
interference. Lastly, we outline a class of algorithms which we call
online-aware that are designed to mitigate interference, and show they do
reduce interference according to our measure and that they improve stability
and performance in several classic control environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04891">Accelerated Discovery of Machine-Learned Symmetries: Deriving the Exceptional Lie Groups G2, F4 and E6. (arXiv:2307.04891v1 [hep-th])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-th/1/au:+Forestano_R/0/1/0/all/0/1">Roy T. Forestano</a>, <a href="http://arxiv.org/find/hep-th/1/au:+Matchev_K/0/1/0/all/0/1">Konstantin T. Matchev</a>, <a href="http://arxiv.org/find/hep-th/1/au:+Matcheva_K/0/1/0/all/0/1">Katia Matcheva</a>, <a href="http://arxiv.org/find/hep-th/1/au:+Roman_A/0/1/0/all/0/1">Alexander Roman</a>, <a href="http://arxiv.org/find/hep-th/1/au:+Unlu_E/0/1/0/all/0/1">Eyup B. Unlu</a>, <a href="http://arxiv.org/find/hep-th/1/au:+Verner_S/0/1/0/all/0/1">Sarunas Verner</a></p>
<p>Recent work has applied supervised deep learning to derive continuous
symmetry transformations that preserve the data labels and to obtain the
corresponding algebras of symmetry generators. This letter introduces two
improved algorithms that significantly speed up the discovery of these symmetry
transformations. The new methods are demonstrated by deriving the complete set
of generators for the unitary groups U(n) and the exceptional Lie groups $G_2$,
$F_4$, and $E_6$. A third post-processing algorithm renders the found
generators in sparse form. We benchmark the performance improvement of the new
algorithms relative to the standard approach. Given the significant complexity
of the exceptional Lie groups, our results demonstrate that this
machine-learning method for discovering symmetries is completely general and
can be applied to a wide variety of labeled datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04893">Choosing Well Your Opponents: How to Guide the Synthesis of Programmatic Strategies. (arXiv:2307.04893v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moraes_R/0/1/0/all/0/1">Rubens O. Moraes</a>, <a href="http://arxiv.org/find/cs/1/au:+Aleixo_D/0/1/0/all/0/1">David S. Aleixo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferreira_L/0/1/0/all/0/1">Lucas N. Ferreira</a>, <a href="http://arxiv.org/find/cs/1/au:+Lelis_L/0/1/0/all/0/1">Levi H. S. Lelis</a></p>
<p>This paper introduces Local Learner (2L), an algorithm for providing a set of
reference strategies to guide the search for programmatic strategies in
two-player zero-sum games. Previous learning algorithms, such as Iterated Best
Response (IBR), Fictitious Play (FP), and Double-Oracle (DO), can be
computationally expensive or miss important information for guiding search
algorithms. 2L actively selects a set of reference strategies to improve the
search signal. We empirically demonstrate the advantages of our approach while
guiding a local search algorithm for synthesizing strategies in three games,
including MicroRTS, a challenging real-time strategy game. Results show that 2L
learns reference strategies that provide a stronger search signal than IBR, FP,
and DO. We also simulate a tournament of MicroRTS, where a synthesizer using 2L
outperformed the winners of the two latest MicroRTS competitions, which were
programmatic strategies written by human programmers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04895">Learning to Solve Constraint Satisfaction Problems with Recurrent Transformer. (arXiv:2307.04895v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1">Adam Ishay</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joohyung Lee</a></p>
<p>Constraint satisfaction problems (CSPs) are about finding values of variables
that satisfy the given constraints. We show that Transformer extended with
recurrence is a viable approach to learning to solve CSPs in an end-to-end
manner, having clear advantages over state-of-the-art methods such as Graph
Neural Networks, SATNet, and some neuro-symbolic models. With the ability of
Transformer to handle visual input, the proposed Recurrent Transformer can
straightforwardly be applied to visual constraint reasoning problems while
successfully addressing the symbol grounding problem. We also show how to
leverage deductive knowledge of discrete constraints in the Transformer's
inductive learning to achieve sample-efficient learning and semi-supervised
learning for CSPs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04904">Fast dynamic time warping and clustering in C++. (arXiv:2307.04904v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Kumtepeli_V/0/1/0/all/0/1">Volkan Kumtepeli</a>, <a href="http://arxiv.org/find/eess/1/au:+Perriment_R/0/1/0/all/0/1">Rebecca Perriment</a>, <a href="http://arxiv.org/find/eess/1/au:+Howey_D/0/1/0/all/0/1">David A. Howey</a></p>
<p>We present an approach for computationally efficient dynamic time warping
(DTW) and clustering of time-series data. The method frames the dynamic warping
of time series datasets as an optimisation problem solved using dynamic
programming, and then clusters time series data by solving a second
optimisation problem using mixed-integer programming (MIP). There is also an
option to use k-medoids clustering for increased speed, when a certificate for
global optimality is not essential. The improved efficiency of our approach is
due to task-level parallelisation of the clustering alongside DTW. Our approach
was tested using the UCR Time Series Archive, and was found to be, on average,
33% faster than the next fastest option when using the same clustering method.
This increases to 64% faster when considering only larger datasets (with more
than 1000 time series). The MIP clustering is most effective on small numbers
of longer time series, because the DTW computation is faster than other
approaches, but the clustering problem becomes increasingly computationally
expensive as the number of time series to be clustered increases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04905">FedYolo: Augmenting Federated Learning with Pretrained Transformers. (arXiv:2307.04905v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuechen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingchen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xiangyu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiasi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_Chowdhury_A/0/1/0/all/0/1">Amit K. Roy-Chowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1">Ananda Theertha Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1">Samet Oymak</a></p>
<p>The growth and diversity of machine learning applications motivate a
rethinking of learning with mobile and edge devices. How can we address diverse
client goals and learn with scarce heterogeneous data? While federated learning
aims to address these issues, it has challenges hindering a unified solution.
Large transformer models have been shown to work across a variety of tasks
achieving remarkable few-shot adaptation. This raises the question: Can clients
use a single general-purpose model, rather than custom models for each task,
while obeying device and network constraints? In this work, we investigate
pretrained transformers (PTF) to achieve these on-device learning goals and
thoroughly explore the roles of model size and modularity, where the latter
refers to adaptation through modules such as prompts or adapters. Focusing on
federated learning, we demonstrate that: (1) Larger scale shrinks the accuracy
gaps between alternative approaches and improves heterogeneity robustness.
Scale allows clients to run more local SGD epochs which can significantly
reduce the number of communication rounds. At the extreme, clients can achieve
respectable accuracy locally highlighting the potential of fully-local
learning. (2) Modularity, by design, enables $&gt;$100$\times$ less communication
in bits. Surprisingly, it also boosts the generalization capability of local
adaptation methods and the robustness of smaller PTFs. Finally, it enables
clients to solve multiple unrelated tasks simultaneously using a single PTF,
whereas full updates are prone to catastrophic forgetting. These insights on
scale and modularity motivate a new federated learning approach we call "You
Only Load Once" (FedYolo): The clients load a full PTF model once and all
future updates are accomplished through communication-efficient modules with
limited catastrophic-forgetting, where each task is assigned to its own module.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04907">SimpleMTOD: A Simple Language Model for Multimodal Task-Oriented Dialogue with Symbolic Scene Representation. (arXiv:2307.04907v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hemanthage_B/0/1/0/all/0/1">Bhathiya Hemanthage</a>, <a href="http://arxiv.org/find/cs/1/au:+Dondrup_C/0/1/0/all/0/1">Christian Dondrup</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartie_P/0/1/0/all/0/1">Phil Bartie</a>, <a href="http://arxiv.org/find/cs/1/au:+Lemon_O/0/1/0/all/0/1">Oliver Lemon</a></p>
<p>SimpleMTOD is a simple language model which recasts several sub-tasks in
multimodal task-oriented dialogues as sequence prediction tasks. SimpleMTOD is
built on a large-scale transformer-based auto-regressive architecture, which
has already proven to be successful in uni-modal task-oriented dialogues, and
effectively leverages transfer learning from pre-trained GPT-2. In-order to
capture the semantics of visual scenes, we introduce both local and
de-localized tokens for objects within a scene. De-localized tokens represent
the type of an object rather than the specific object itself and so possess a
consistent meaning across the dataset. SimpleMTOD achieves a state-of-the-art
BLEU score (0.327) in the Response Generation sub-task of the SIMMC 2.0
test-std dataset while performing on par in other multimodal sub-tasks:
Disambiguation, Coreference Resolution, and Dialog State Tracking. This is
despite taking a minimalist approach for extracting visual (and non-visual)
information. In addition the model does not rely on task-specific architectural
changes such as classification heads.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04927">Probabilistic Counterexample Guidance for Safer Reinforcement Learning. (arXiv:2307.04927v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1">Xiaotong Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Filieri_A/0/1/0/all/0/1">Antonio Filieri</a></p>
<p>Safe exploration aims at addressing the limitations of Reinforcement Learning
(RL) in safety-critical scenarios, where failures during trial-and-error
learning may incur high costs. Several methods exist to incorporate external
knowledge or to use proximal sensor data to limit the exploration of unsafe
states. However, reducing exploration risks in unknown environments, where an
agent must discover safety threats during exploration, remains challenging. In
this paper, we target the problem of safe exploration by guiding the training
with counterexamples of the safety requirement. Our method abstracts both
continuous and discrete state-space systems into compact abstract models
representing the safety-relevant knowledge acquired by the agent during
exploration. We then exploit probabilistic counterexample generation to
construct minimal simulation submodels eliciting safety requirement violations,
where the agent can efficiently train offline to refine its policy towards
minimising the risk of safety violations during the subsequent online
exploration. We demonstrate our method's effectiveness in reducing safety
violations during online exploration in preliminary experiments by an average
of 40.3% compared with QL and DQN standard algorithms and 29.1% compared with
previous related work, while achieving comparable cumulative rewards with
respect to unrestricted exploration and alternative approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04937">Improving Fairness of Graph Neural Networks: A Graph Counterfactual Perspective. (arXiv:2307.04937v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhimeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jialiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1">Teng Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Suhang Wang</a></p>
<p>Graph neural networks have shown great ability in representation (GNNs)
learning on graphs, facilitating various tasks. Despite their great performance
in modeling graphs, recent works show that GNNs tend to inherit and amplify the
bias from training data, causing concerns of the adoption of GNNs in high-stake
scenarios. Hence, many efforts have been taken for fairness-aware GNNs.
However, most existing fair GNNs learn fair node representations by adopting
statistical fairness notions, which may fail to alleviate bias in the presence
of statistical anomalies. Motivated by causal theory, there are several
attempts utilizing graph counterfactual fairness to mitigate root causes of
unfairness. However, these methods suffer from non-realistic counterfactuals
obtained by perturbation or generation. In this paper, we take a causal view on
fair graph learning problem. Guided by the casual analysis, we propose a novel
framework CAF, which can select counterfactuals from training data to avoid
non-realistic counterfactuals and adopt selected counterfactuals to learn fair
node representations for node classification task. Extensive experiments on
synthetic and real-world datasets show the effectiveness of CAF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04942">Benchmarking Algorithms for Federated Domain Generalization. (arXiv:2307.04942v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_R/0/1/0/all/0/1">Ruqi Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Bagchi_S/0/1/0/all/0/1">Saurabh Bagchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1">David I. Inouye</a></p>
<p>While prior domain generalization (DG) benchmarks consider train-test dataset
heterogeneity, we evaluate Federated DG which introduces federated learning
(FL) specific challenges. Additionally, we explore domain-based heterogeneity
in clients' local datasets - a realistic Federated DG scenario. Prior Federated
DG evaluations are limited in terms of the number or heterogeneity of clients
and dataset diversity. To address this gap, we propose an Federated DG
benchmark methodology that enables control of the number and heterogeneity of
clients and provides metrics for dataset difficulty. We then apply our
methodology to evaluate 13 Federated DG methods, which include centralized DG
methods adapted to the FL context, FL methods that handle client heterogeneity,
and methods designed specifically for Federated DG. Our results suggest that
despite some progress, there remain significant performance gaps in Federated
DG particularly when evaluating with a large number of clients, high client
heterogeneity, or more realistic datasets. Please check our extendable
benchmark code here: https://github.com/inouye-lab/FedDG_Benchmark.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04946">DDGM: Solving inverse problems by Diffusive Denoising of Gradient-based Minimization. (arXiv:2307.04946v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luther_K/0/1/0/all/0/1">Kyle Luther</a>, <a href="http://arxiv.org/find/cs/1/au:+Seung_H/0/1/0/all/0/1">H. Sebastian Seung</a></p>
<p>Inverse problems generally require a regularizer or prior for a good
solution. A recent trend is to train a convolutional net to denoise images, and
use this net as a prior when solving the inverse problem. Several proposals
depend on a singular value decomposition of the forward operator, and several
others backpropagate through the denoising net at runtime. Here we propose a
simpler approach that combines the traditional gradient-based minimization of
reconstruction error with denoising. Noise is also added at each step, so the
iterative dynamics resembles a Langevin or diffusion process. Both the level of
added noise and the size of the denoising step decay exponentially with time.
We apply our method to the problem of tomographic reconstruction from electron
micrographs acquired at multiple tilt angles. With empirical studies using
simulated tilt views, we find parameter settings for our method that produce
good results. We show that high accuracy can be achieved with as few as 50
denoising steps. We also compare with DDRM and DPS, more complex diffusion
methods of the kinds mentioned above. These methods are less accurate (as
measured by MSE and SSIM) for our tomography problem, even after the generation
hyperparameters are optimized. Finally we extend our method to reconstruction
of arbitrary-sized images and show results on 128 $\times$ 1568 pixel images
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04952">Compact Twice Fusion Network for Edge Detection. (arXiv:2307.04952v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yachuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zongmin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+P%2E_X/0/1/0/all/0/1">Xavier Soria P.</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chaozhi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1">Qian Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yun Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hua Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangdong Wang</a></p>
<p>The significance of multi-scale features has been gradually recognized by the
edge detection community. However, the fusion of multi-scale features increases
the complexity of the model, which is not friendly to practical application. In
this work, we propose a Compact Twice Fusion Network (CTFN) to fully integrate
multi-scale features while maintaining the compactness of the model. CTFN
includes two lightweight multi-scale feature fusion modules: a Semantic
Enhancement Module (SEM) that can utilize the semantic information contained in
coarse-scale features to guide the learning of fine-scale features, and a
Pseudo Pixel-level Weighting (PPW) module that aggregate the complementary
merits of multi-scale features by assigning weights to all features.
Notwithstanding all this, the interference of texture noise makes the correct
classification of some pixels still a challenge. For these hard samples, we
propose a novel loss function, coined Dynamic Focal Loss, which reshapes the
standard cross-entropy loss and dynamically adjusts the weights to correct the
distribution of hard samples. We evaluate our method on three datasets, i.e.,
BSDS500, NYUDv2, and BIPEDv2. Compared with state-of-the-art methods, CTFN
achieves competitive accuracy with less parameters and computational cost.
Apart from the backbone, CTFN requires only 0.1M additional parameters, which
reduces its computation cost to just 60% of other state-of-the-art methods. The
codes are available at https://github.com/Li-yachuan/CTFN-pytorch-master.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04954">Hybrid hidden Markov LSTM for short-term traffic flow prediction. (arXiv:2307.04954v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1">Agnimitra Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Adway Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1">S. Ilgin Guler</a></p>
<p>Deep learning (DL) methods have outperformed parametric models such as
historical average, ARIMA and variants in predicting traffic variables into
short and near-short future, that are critical for traffic management.
Specifically, recurrent neural network (RNN) and its variants (e.g. long
short-term memory) are designed to retain long-term temporal correlations and
therefore are suitable for modeling sequences. However, multi-regime models
assume the traffic system to evolve through multiple states (say, free-flow,
congestion in traffic) with distinct characteristics, and hence, separate
models are trained to characterize the traffic dynamics within each regime. For
instance, Markov-switching models with a hidden Markov model (HMM) for regime
identification is capable of capturing complex dynamic patterns and
non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an
observation sequence from a set of latent or, hidden state variables. In LSTM,
the latent variable is computed in a deterministic manner from the current
observation and the previous latent variable, while, in HMM, the set of latent
variables is a Markov chain. Inspired by research in natural language
processing, a hybrid hidden Markov-LSTM model that is capable of learning
complementary features in traffic data is proposed for traffic flow prediction.
Results indicate significant performance gains in using hybrid architecture
compared to conventional methods such as Markov switching ARIMA and LSTM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04957">Reinforcement Learning with Non-Cumulative Objective. (arXiv:2307.04957v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_W/0/1/0/all/0/1">Wei Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wei Yu</a></p>
<p>In reinforcement learning, the objective is almost always defined as a
\emph{cumulative} function over the rewards along the process. However, there
are many optimal control and reinforcement learning problems in various
application fields, especially in communications and networking, where the
objectives are not naturally expressed as summations of the rewards. In this
paper, we recognize the prevalence of non-cumulative objectives in various
problems, and propose a modification to existing algorithms for optimizing such
objectives. Specifically, we dive into the fundamental building block for many
optimal control and reinforcement learning algorithms: the Bellman optimality
equation. To optimize a non-cumulative objective, we replace the original
summation operation in the Bellman update rule with a generalized operation
corresponding to the objective. Furthermore, we provide sufficient conditions
on the form of the generalized operation as well as assumptions on the Markov
decision process under which the globally optimal convergence of the
generalized Bellman updates can be guaranteed. We demonstrate the idea
experimentally with the bottleneck objective, i.e., the objectives determined
by the minimum reward along the process, on classical optimal control and
reinforcement learning tasks, as well as on two network routing problems on
maximizing the flow rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04962">Intrinsically motivated graph exploration using network theories of human curiosity. (arXiv:2307.04962v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1">Shubhankar P. Patankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouellet_M/0/1/0/all/0/1">Mathieu Ouellet</a>, <a href="http://arxiv.org/find/cs/1/au:+Cervino_J/0/1/0/all/0/1">Juan Cervino</a>, <a href="http://arxiv.org/find/cs/1/au:+Ribeiro_A/0/1/0/all/0/1">Alejandro Ribeiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphy_K/0/1/0/all/0/1">Kieran A. Murphy</a>, <a href="http://arxiv.org/find/cs/1/au:+Bassett_D/0/1/0/all/0/1">Dani S. Bassett</a></p>
<p>Intrinsically motivated exploration has proven useful for reinforcement
learning, even without additional extrinsic rewards. When the environment is
naturally represented as a graph, how to guide exploration best remains an open
question. In this work, we propose a novel approach for exploring
graph-structured data motivated by two theories of human curiosity: the
information gap theory and the compression progress theory. The theories view
curiosity as an intrinsic motivation to optimize for topological features of
subgraphs induced by the visited nodes in the environment. We use these
proposed features as rewards for graph neural-network-based reinforcement
learning. On multiple classes of synthetically generated graphs, we find that
trained agents generalize to larger environments and to longer exploratory
walks than are seen during training. Our method computes more efficiently than
the greedy evaluation of the relevant topological properties. The proposed
intrinsic motivations bear particular relevance for recommender systems. We
demonstrate that curiosity-based recommendations are more predictive of human
behavior than PageRank centrality for several real-world graph datasets,
including MovieLens, Amazon Books, and Wikispeedia.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04963">DyCL: Dynamic Neural Network Compilation Via Program Rewriting and Graph Optimization. (arXiv:2307.04963v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Simin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_S/0/1/0/all/0/1">Shiyi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Cong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a></p>
<p>DL compiler's primary function is to translate DNN programs written in
high-level DL frameworks such as PyTorch and TensorFlow into portable
executables. These executables can then be flexibly executed by the deployed
host programs. However, existing DL compilers rely on a tracing mechanism,
which involves feeding a runtime input to a neural network program and tracing
the program execution paths to generate the computational graph necessary for
compilation. Unfortunately, this mechanism falls short when dealing with modern
dynamic neural networks (DyNNs) that possess varying computational graphs
depending on the inputs. Consequently, conventional DL compilers struggle to
accurately compile DyNNs into executable code. To address this limitation, we
propose \tool, a general approach that enables any existing DL compiler to
successfully compile DyNNs. \tool tackles the dynamic nature of DyNNs by
introducing a compilation mechanism that redistributes the control and data
flow of the original DNN programs during the compilation process. Specifically,
\tool develops program analysis and program transformation techniques to
convert a dynamic neural network into multiple sub-neural networks. Each
sub-neural network is devoid of conditional statements and is compiled
independently. Furthermore, \tool synthesizes a host module that models the
control flow of the DyNNs and facilitates the invocation of the sub-neural
networks. Our evaluation demonstrates the effectiveness of \tool, achieving a
100\% success rate in compiling all dynamic neural networks. Moreover, the
compiled executables generated by \tool exhibit significantly improved
performance, running between $1.12\times$ and $20.21\times$ faster than the
original DyNNs executed on general-purpose DL frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04964">Secrets of RLHF in Large Language Models Part I: PPO. (arXiv:2307.04964v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1">Rui Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1">Shihan Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Songyang Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1">Wei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Binghai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">Senjie Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1">Limao Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1">Zhiheng Xi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1">Nuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lai_W/0/1/0/all/0/1">Wenbin Lai</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1">Minghao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_R/0/1/0/all/0/1">Rongxiang Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1">Wensen Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1">Cheng Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1">Yuan Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Haoran Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1">Hang Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1">Tao Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>Large language models (LLMs) have formulated a blueprint for the advancement
of artificial general intelligence. Its primary objective is to function as a
human-centric (helpful, honest, and harmless) assistant. Alignment with humans
assumes paramount significance, and reinforcement learning with human feedback
(RLHF) emerges as the pivotal technological paradigm underpinning this pursuit.
Current technical routes usually include \textbf{reward models} to measure
human preferences, \textbf{Proximal Policy Optimization} (PPO) to optimize
policy model outputs, and \textbf{process supervision} to improve step-by-step
reasoning capabilities. However, due to the challenges of reward design,
environment interaction, and agent training, coupled with huge trial and error
cost of large language models, there is a significant barrier for AI
researchers to motivate the development of technical alignment and safe landing
of LLMs. The stable training of RLHF has still been a puzzle. In the first
report, we dissect the framework of RLHF, re-evaluate the inner workings of
PPO, and explore how the parts comprising PPO algorithms impact policy agent
training. We identify policy constraints being the key factor for the effective
implementation of the PPO algorithm. Therefore, we explore the PPO-max, an
advanced version of PPO algorithm, to efficiently improve the training
stability of the policy model. Based on our main results, we perform a
comprehensive analysis of RLHF abilities compared with SFT models and ChatGPT.
The absence of open-source implementations has posed significant challenges to
the investigation of LLMs alignment. Therefore, we are eager to release
technical reports, reward models and PPO codes
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04988">Benchmarking Bayesian Causal Discovery Methods for Downstream Treatment Effect Estimation. (arXiv:2307.04988v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Emezue_C/0/1/0/all/0/1">Chris Chinenye Emezue</a>, <a href="http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1">Alexandre Drouin</a>, <a href="http://arxiv.org/find/cs/1/au:+Deleu_T/0/1/0/all/0/1">Tristan Deleu</a>, <a href="http://arxiv.org/find/cs/1/au:+Bauer_S/0/1/0/all/0/1">Stefan Bauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1">Yoshua Bengio</a></p>
<p>The practical utility of causality in decision-making is widely recognized,
with causal discovery and inference being inherently intertwined. Nevertheless,
a notable gap exists in the evaluation of causal discovery methods, where
insufficient emphasis is placed on downstream inference. To address this gap,
we evaluate six established baseline causal discovery methods and a newly
proposed method based on GFlowNets, on the downstream task of treatment effect
estimation. Through the implementation of a robust evaluation procedure, we
offer valuable insights into the efficacy of these causal discovery methods for
treatment effect estimation, considering both synthetic and real-world
scenarios, as well as low-data scenarios. Furthermore, the results of our study
demonstrate that GFlowNets possess the capability to effectively capture a wide
range of useful and diverse ATE modes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04990">Monotone deep Boltzmann machines. (arXiv:2307.04990v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1">Zhili Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Winston_E/0/1/0/all/0/1">Ezra Winston</a>, <a href="http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1">J. Zico Kolter</a></p>
<p>Deep Boltzmann machines (DBMs), one of the first ``deep'' learning methods
ever studied, are multi-layered probabilistic models governed by a pairwise
energy function that describes the likelihood of all variables/nodes in the
network. In practice, DBMs are often constrained, i.e., via the
\emph{restricted} Boltzmann machine (RBM) architecture (which does not permit
intra-layer connections), in order to allow for more efficient inference. In
this work, we revisit the generic DBM approach, and ask the question: are there
other possible restrictions to their design that would enable efficient
(approximate) inference? In particular, we develop a new class of restricted
model, the monotone DBM, which allows for arbitrary self-connection in each
layer, but restricts the \emph{weights} in a manner that guarantees the
existence and global uniqueness of a mean-field fixed point. To do this, we
leverage tools from the recently-proposed monotone Deep Equilibrium model and
show that a particular choice of activation results in a fixed-point iteration
that gives a variational mean-field solution. While this approach is still
largely conceptual, it is the first architecture that allows for efficient
approximate inference in fully-general weight structures for DBMs. We apply
this approach to simple deep convolutional Boltzmann architectures and
demonstrate that it allows for tasks such as the joint completion and
classification of images, within a single deep probabilistic setting, while
avoiding the pitfalls of mean-field inference in traditional RBMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04993">Uncertainty Quantification of the Virial Black Hole Mass with Conformal Prediction. (arXiv:2307.04993v1 [astro-ph.CO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Yong_S/0/1/0/all/0/1">Suk Yee Yong</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Ong_C/0/1/0/all/0/1">Cheng Soon Ong</a></p>
<p>Precise measurements of the black hole mass are essential to gain insight on
the black hole and host galaxy co-evolution. A direct measure of the black hole
mass is often restricted to nearest galaxies and instead, an indirect method
using the single-epoch virial black hole mass estimation is used for objects at
high redshifts. However, this method is subjected to biases and uncertainties
as it is reliant on the scaling relation from a small sample of local active
galactic nuclei. In this study, we propose the application of conformalised
quantile regression (CQR) to quantify the uncertainties of the black hole
predictions in a machine learning setting. We compare CQR with various
prediction interval techniques and demonstrated that CQR can provide a more
useful prediction interval indicator. In contrast to baseline approaches for
prediction interval estimation, we show that the CQR method provides prediction
intervals that adjust to the black hole mass and its related properties. That
is it yields a tighter constraint on the prediction interval (hence more
certain) for a larger black hole mass, and accordingly, bright and broad
spectral line width source. Using a combination of neural network model and CQR
framework, the recovered virial black hole mass predictions and uncertainties
are comparable to those measured from the Sloan Digital Sky Survey. The code is
publicly available at https://github.com/yongsukyee/uncertain_blackholemass.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04995">PowerFusion: A Tensor Compiler with Explicit Data Movement Description and Instruction-level Graph IR. (arXiv:2307.04995v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zixuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haojie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1">Jingze Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Liyan Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1">Huanqi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kezhao Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1">Shizhi Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Penghan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_J/0/1/0/all/0/1">Jidong Zhai</a></p>
<p>Deep neural networks (DNNs) are of critical use in different domains. To
accelerate DNN computation, tensor compilers are proposed to generate efficient
code on different domain-specific accelerators. Existing tensor compilers
mainly focus on optimizing computation efficiency. However, memory access is
becoming a key performance bottleneck because the computational performance of
accelerators is increasing much faster than memory performance. The lack of
direct description of memory access and data dependence in current tensor
compilers' intermediate representation (IR) brings significant challenges to
generate memory-efficient code.
</p>
<p>In this paper, we propose IntelliGen, a tensor compiler that can generate
high-performance code for memory-intensive operators by considering both
computation and data movement optimizations. IntelliGen represent a DNN program
using GIR, which includes primitives indicating its computation, data movement,
and parallel strategies. This information will be further composed as an
instruction-level dataflow graph to perform holistic optimizations by searching
different memory access patterns and computation operations, and generating
memory-efficient code on different hardware. We evaluate IntelliGen on NVIDIA
GPU, AMD GPU, and Cambricon MLU, showing speedup up to 1.97x, 2.93x, and
16.91x(1.28x, 1.23x, and 2.31x on average), respectively, compared to current
most performant frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04996">Empowering recommender systems using automatically generated Knowledge Graphs and Reinforcement Learning. (arXiv:2307.04996v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1">Ghanshyam Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1">Shovon Sengupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Simanta_S/0/1/0/all/0/1">Simon Simanta</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Perge_J/0/1/0/all/0/1">Janos A. Perge</a>, <a href="http://arxiv.org/find/cs/1/au:+Pillai_D/0/1/0/all/0/1">Devishree Pillai</a>, <a href="http://arxiv.org/find/cs/1/au:+McCrae_J/0/1/0/all/0/1">John P. McCrae</a>, <a href="http://arxiv.org/find/cs/1/au:+Buitelaar_P/0/1/0/all/0/1">Paul Buitelaar</a></p>
<p>Personalized recommendations have a growing importance in direct marketing,
which motivates research to enhance customer experiences by knowledge graph
(KG) applications. For example, in financial services, companies may benefit
from providing relevant financial articles to their customers to cultivate
relationships, foster client engagement and promote informed financial
decisions. While several approaches center on KG-based recommender systems for
improved content, in this study we focus on interpretable KG-based recommender
systems for decision making.To this end, we present two knowledge graph-based
approaches for personalized article recommendations for a set of customers of a
large multinational financial services company. The first approach employs
Reinforcement Learning and the second approach uses the XGBoost algorithm for
recommending articles to the customers. Both approaches make use of a KG
generated from both structured (tabular data) and unstructured data (a large
body of text data).Using the Reinforcement Learning-based recommender system we
could leverage the graph traversal path leading to the recommendation as a way
to generate interpretations (Path Directed Reasoning (PDR)). In the
XGBoost-based approach, one can also provide explainable results using post-hoc
methods such as SHAP (SHapley Additive exPlanations) and ELI5 (Explain Like I
am Five).Importantly, our approach offers explainable results, promoting better
decision-making. This study underscores the potential of combining advanced
machine learning techniques with KG-driven insights to bolster experience in
customer relationship management.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04998">Selective Sampling and Imitation Learning via Online Regression. (arXiv:2307.04998v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sekhari_A/0/1/0/all/0/1">Ayush Sekhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Sridharan_K/0/1/0/all/0/1">Karthik Sridharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1">Wen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1">Runzhe Wu</a></p>
<p>We consider the problem of Imitation Learning (IL) by actively querying noisy
expert for feedback. While imitation learning has been empirically successful,
much of prior work assumes access to noiseless expert feedback which is not
practical in many applications. In fact, when one only has access to noisy
expert feedback, algorithms that rely on purely offline data (non-interactive
IL) can be shown to need a prohibitively large number of samples to be
successful. In contrast, in this work, we provide an interactive algorithm for
IL that uses selective sampling to actively query the noisy expert for
feedback. Our contributions are twofold: First, we provide a new selective
sampling algorithm that works with general function classes and multiple
actions, and obtains the best-known bounds for the regret and the number of
queries. Next, we extend this analysis to the problem of IL with noisy expert
feedback and provide a new IL algorithm that makes limited queries.
</p>
<p>Our algorithm for selective sampling leverages function approximation, and
relies on an online regression oracle w.r.t.~the given model class to predict
actions, and to decide whether to query the expert for its label. On the
theoretical side, the regret bound of our algorithm is upper bounded by the
regret of the online regression oracle, while the query complexity additionally
depends on the eluder dimension of the model class. We complement this with a
lower bound that demonstrates that our results are tight. We extend our
selective sampling algorithm for IL with general function approximation and
provide bounds on both the regret and the number of queries made to the noisy
expert. A key novelty here is that our regret and query complexity bounds only
depend on the number of times the optimal policy (and not the noisy expert, or
the learner) go to states that have a small margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05004">Control as Probabilistic Inference as an Emergent Communication Mechanism in Multi-Agent Reinforcement Learning. (arXiv:2307.05004v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nakamura_T/0/1/0/all/0/1">Tomoaki Nakamura</a>, <a href="http://arxiv.org/find/cs/1/au:+Taniguchi_A/0/1/0/all/0/1">Akira Taniguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Taniguchi_T/0/1/0/all/0/1">Tadahiro Taniguchi</a></p>
<p>This paper proposes a generative probabilistic model integrating emergent
communication and multi-agent reinforcement learning. The agents plan their
actions by probabilistic inference, called control as inference, and
communicate using messages that are latent variables and estimated based on the
planned actions. Through these messages, each agent can send information about
its actions and know information about the actions of another agent. Therefore,
the agents change their actions according to the estimated messages to achieve
cooperative tasks. This inference of messages can be considered as
communication, and this procedure can be formulated by the Metropolis-Hasting
naming game. Through experiments in the grid world environment, we show that
the proposed PGM can infer meaningful messages to achieve the cooperative task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05006">Improving RNN-Transducers with Acoustic LookAhead. (arXiv:2307.05006v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Unni_V/0/1/0/all/0/1">Vinit S. Unni</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Ashish Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jyothi_P/0/1/0/all/0/1">Preethi Jyothi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarawagi_S/0/1/0/all/0/1">Sunita Sarawagi</a></p>
<p>RNN-Transducers (RNN-Ts) have gained widespread acceptance as an end-to-end
model for speech to text conversion because of their high accuracy and
streaming capabilities. A typical RNN-T independently encodes the input audio
and the text context, and combines the two encodings by a thin joint network.
While this architecture provides SOTA streaming accuracy, it also makes the
model vulnerable to strong LM biasing which manifests as multi-step
hallucination of text without acoustic evidence. In this paper we propose
LookAhead that makes text representations more acoustically grounded by looking
ahead into the future within the audio input. This technique yields a
significant 5%-20% relative reduction in word error rate on both in-domain and
out-of-domain evaluation sets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05014">Test-Time Training on Video Streams. (arXiv:2307.05014v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Renhao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1">Yossi Gandelsman</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xinlei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1">Alexei A. Efros</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaolong Wang</a></p>
<p>Prior work has established test-time training (TTT) as a general framework to
further improve a trained model at test time. Before making a prediction on
each test instance, the model is trained on the same instance using a
self-supervised task, such as image reconstruction with masked autoencoders. We
extend TTT to the streaming setting, where multiple test instances - video
frames in our case - arrive in temporal order. Our extension is online TTT: The
current model is initialized from the previous model, then trained on the
current frame and a small window of frames immediately before. Online TTT
significantly outperforms the fixed-model baseline for four tasks, on three
real-world datasets. The relative improvement is 45% and 66% for instance and
panoptic segmentation. Surprisingly, online TTT also outperforms its offline
variant that accesses more information, training on all frames from the entire
test video regardless of temporal order. This differs from previous findings
using synthetic videos. We conceptualize locality as the advantage of online
over offline TTT. We analyze the role of locality with ablations and a theory
based on bias-variance trade-off.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05017">Feature Activation Map: Visual Explanation of Deep Learning Models for Image Classification. (arXiv:2307.05017v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yi Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yongsheng Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weichuan Zhang</a></p>
<p>Decisions made by convolutional neural networks(CNN) can be understood and
explained by visualizing discriminative regions on images. To this end, Class
Activation Map (CAM) based methods were proposed as powerful interpretation
tools, making the prediction of deep learning models more explainable,
transparent, and trustworthy. However, all the CAM-based methods (e.g., CAM,
Grad-CAM, and Relevance-CAM) can only be used for interpreting CNN models with
fully-connected (FC) layers as a classifier. It is worth noting that many deep
learning models classify images without FC layers, e.g., few-shot learning
image classification, contrastive learning image classification, and image
retrieval tasks. In this work, a post-hoc interpretation tool named feature
activation map (FAM) is proposed, which can interpret deep learning models
without FC layers as a classifier. In the proposed FAM algorithm, the
channel-wise contribution weights are derived from the similarity scores
between two image embeddings. The activation maps are linearly combined with
the corresponding normalized contribution weights, forming the explanation map
for visualization. The quantitative and qualitative experiments conducted on
ten deep learning models for few-shot image classification, contrastive
learning image classification and image retrieval tasks demonstrate the
effectiveness of the proposed FAM algorithm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05025">Unleashing the Potential of Regularization Strategies in Learning with Noisy Labels. (arXiv:2307.05025v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1">Hui Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Sheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huaxi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1">Jun Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dadong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tongliang Liu</a></p>
<p>In recent years, research on learning with noisy labels has focused on
devising novel algorithms that can achieve robustness to noisy training labels
while generalizing to clean data. These algorithms often incorporate
sophisticated techniques, such as noise modeling, label correction, and
co-training. In this study, we demonstrate that a simple baseline using
cross-entropy loss, combined with widely used regularization strategies like
learning rate decay, model weights average, and data augmentations, can
outperform state-of-the-art methods. Our findings suggest that employing a
combination of regularization strategies can be more effective than intricate
algorithms in tackling the challenges of learning with noisy labels. While some
of these regularization strategies have been utilized in previous noisy label
learning research, their full potential has not been thoroughly explored. Our
results encourage a reevaluation of benchmarks for learning with noisy labels
and prompt reconsideration of the role of specialized learning algorithms
designed for training with noisy labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05029">FairLay-ML: Intuitive Remedies for Unfairness in Data-Driven Social-Critical Algorithms. (arXiv:2307.05029v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_N/0/1/0/all/0/1">Normen Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_G/0/1/0/all/0/1">Gang Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tizpaz_Niari_S/0/1/0/all/0/1">Saeid Tizpaz-Niari</a></p>
<p>This thesis explores open-sourced machine learning (ML) model explanation
tools to understand whether these tools can allow a layman to visualize,
understand, and suggest intuitive remedies to unfairness in ML-based
decision-support systems. Machine learning models trained on datasets biased
against minority groups are increasingly used to guide life-altering social
decisions, prompting the urgent need to study their logic for unfairness. Due
to this problem's impact on vast populations of the general public, it is
critical for the layperson -- not just subject matter experts in social justice
or machine learning experts -- to understand the nature of unfairness within
these algorithms and the potential trade-offs. Existing research on fairness in
machine learning focuses mostly on the mathematical definitions and tools to
understand and remedy unfair models, with some directly citing user-interactive
tools as necessary for future work. This thesis presents FairLay-ML, a
proof-of-concept GUI integrating some of the most promising tools to provide
intuitive explanations for unfair logic in ML models by integrating existing
research tools (e.g. Local Interpretable Model-Agnostic Explanations) with
existing ML-focused GUI (e.g. Python Streamlit). We test FairLay-ML using
models of various accuracy and fairness generated by an unfairness detector
tool, Parfait-ML, and validate our results using Themis. Our study finds that
the technology stack used for FairLay-ML makes it easy to install and provides
real-time black-box explanations of pre-trained models to users. Furthermore,
the explanations provided translate to actionable remedies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05035">Number Systems for Deep Neural Network Architectures: A Survey. (arXiv:2307.05035v1 [cs.NE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Alsuhli_G/0/1/0/all/0/1">Ghada Alsuhli</a>, <a href="http://arxiv.org/find/cs/1/au:+Sakellariou_V/0/1/0/all/0/1">Vasileios Sakellariou</a>, <a href="http://arxiv.org/find/cs/1/au:+Saleh_H/0/1/0/all/0/1">Hani Saleh</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Qutayri_M/0/1/0/all/0/1">Mahmoud Al-Qutayri</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammad_B/0/1/0/all/0/1">Baker Mohammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Stouraitis_T/0/1/0/all/0/1">Thanos Stouraitis</a></p>
<p>Deep neural networks (DNNs) have become an enabling component for a myriad of
artificial intelligence applications. DNNs have shown sometimes superior
performance, even compared to humans, in cases such as self-driving, health
applications, etc. Because of their computational complexity, deploying DNNs in
resource-constrained devices still faces many challenges related to computing
complexity, energy efficiency, latency, and cost. To this end, several research
directions are being pursued by both academia and industry to accelerate and
efficiently implement DNNs. One important direction is determining the
appropriate data representation for the massive amount of data involved in DNN
processing. Using conventional number systems has been found to be sub-optimal
for DNNs. Alternatively, a great body of research focuses on exploring suitable
number systems. This article aims to provide a comprehensive survey and
discussion about alternative number systems for more efficient representations
of DNN data. Various number systems (conventional/unconventional) exploited for
DNNs are discussed. The impact of these number systems on the performance and
hardware design of DNNs is considered. In addition, this paper highlights the
challenges associated with each number system and various solutions that are
proposed for addressing them. The reader will be able to understand the
importance of an efficient number system for DNN, learn about the widely used
number systems for DNN, understand the trade-offs between various number
systems, and consider various design aspects that affect the impact of number
systems on DNN performance. In addition, the recent trends and related research
opportunities will be highlighted
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05048">Portfolio Optimization: A Comparative Study. (arXiv:2307.05048v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Sen_J/0/1/0/all/0/1">Jaydip Sen</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Dasgupta_S/0/1/0/all/0/1">Subhasis Dasgupta</a></p>
<p>Portfolio optimization has been an area that has attracted considerable
attention from the financial research community. Designing a profitable
portfolio is a challenging task involving precise forecasting of future stock
returns and risks. This chapter presents a comparative study of three portfolio
design approaches, the mean-variance portfolio (MVP), hierarchical risk parity
(HRP)-based portfolio, and autoencoder-based portfolio. These three approaches
to portfolio design are applied to the historical prices of stocks chosen from
ten thematic sectors listed on the National Stock Exchange (NSE) of India. The
portfolios are designed using the stock price data from January 1, 2018, to
December 31, 2021, and their performances are tested on the out-of-sample data
from January 1, 2022, to December 31, 2022. Extensive results are analyzed on
the performance of the portfolios. It is observed that the performance of the
MVP portfolio is the best on the out-of-sample data for the risk-adjusted
returns. However, the autoencoder portfolios outperformed their counterparts on
annual returns.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05068">A Theory of Bounded Inductive Rationality. (arXiv:2307.05068v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oesterheld_C/0/1/0/all/0/1">Caspar Oesterheld</a> (Carnegie Mellon University), <a href="http://arxiv.org/find/cs/1/au:+Demski_A/0/1/0/all/0/1">Abram Demski</a> (Machine Intelligence Research Institute), <a href="http://arxiv.org/find/cs/1/au:+Conitzer_V/0/1/0/all/0/1">Vincent Conitzer</a> (Carnegie Mellon University)</p>
<p>The dominant theories of rational choice assume logical omniscience. That is,
they assume that when facing a decision problem, an agent can perform all
relevant computations and determine the truth value of all relevant
logical/mathematical claims. This assumption is unrealistic when, for example,
we offer bets on remote digits of pi or when an agent faces a computationally
intractable planning problem. Furthermore, the assumption of logical
omniscience creates contradictions in cases where the environment can contain
descriptions of the agent itself. Importantly, strategic interactions as
studied in game theory are decision problems in which a rational agent is
predicted by its environment (the other players). In this paper, we develop a
theory of rational decision making that does not assume logical omniscience. We
consider agents who repeatedly face decision problems (including ones like
betting on digits of pi or games against other agents). The main contribution
of this paper is to provide a sensible theory of rationality for such agents.
Roughly, we require that a boundedly rational inductive agent tests each
efficiently computable hypothesis infinitely often and follows those hypotheses
that keep their promises of high rewards. We then prove that agents that are
rational in this sense have other desirable properties. For example, they learn
to value random and pseudo-random lotteries at their expected reward. Finally,
we consider strategic interactions between different agents and prove a folk
theorem for what strategies bounded rational inductive agents can converge to.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05080">Estimating label quality and errors in semantic segmentation data via any model. (arXiv:2307.05080v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lad_V/0/1/0/all/0/1">Vedang Lad</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_J/0/1/0/all/0/1">Jonas Mueller</a></p>
<p>The labor-intensive annotation process of semantic segmentation datasets is
often prone to errors, since humans struggle to label every pixel correctly. We
study algorithms to automatically detect such annotation errors, in particular
methods to score label quality, such that the images with the lowest scores are
least likely to be correctly labeled. This helps prioritize what data to review
in order to ensure a high-quality training/evaluation dataset, which is
critical in sensitive applications such as medical imaging and autonomous
vehicles. Widely applicable, our label quality scores rely on probabilistic
predictions from a trained segmentation model -- any model architecture and
training procedure can be utilized. Here we study 7 different label quality
scoring methods used in conjunction with a DeepLabV3+ or a FPN segmentation
model to detect annotation errors in a version of the SYNTHIA dataset.
Precision-recall evaluations reveal a score -- the soft-minimum of the
model-estimated likelihoods of each pixel's annotated class -- that is
particularly effective to identify images that are mislabeled, across multiple
types of annotation error.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05104">A Deep Dive into Perturbations as Evaluation Technique for Time Series XAI. (arXiv:2307.05104v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schlegel_U/0/1/0/all/0/1">Udo Schlegel</a>, <a href="http://arxiv.org/find/cs/1/au:+Keim_D/0/1/0/all/0/1">Daniel A. Keim</a></p>
<p>Explainable Artificial Intelligence (XAI) has gained significant attention
recently as the demand for transparency and interpretability of machine
learning models has increased. In particular, XAI for time series data has
become increasingly important in finance, healthcare, and climate science.
However, evaluating the quality of explanations, such as attributions provided
by XAI techniques, remains challenging. This paper provides an in-depth
analysis of using perturbations to evaluate attributions extracted from time
series models. A perturbation analysis involves systematically modifying the
input data and evaluating the impact on the attributions generated by the XAI
method. We apply this approach to several state-of-the-art XAI techniques and
evaluate their performance on three time series classification datasets. Our
results demonstrate that the perturbation analysis approach can effectively
evaluate the quality of attributions and provide insights into the strengths
and limitations of XAI techniques. Such an approach can guide the selection of
XAI methods for time series data, e.g., focusing on return time rather than
precision, and facilitate the development of more reliable and interpretable
machine learning models for time series analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05109">Conformalization of Sparse Generalized Linear Models. (arXiv:2307.05109v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guha_E/0/1/0/all/0/1">Etash Kumar Guha</a>, <a href="http://arxiv.org/find/cs/1/au:+Ndiaye_E/0/1/0/all/0/1">Eugene Ndiaye</a>, <a href="http://arxiv.org/find/cs/1/au:+Huo_X/0/1/0/all/0/1">Xiaoming Huo</a></p>
<p>Given a sequence of observable variables $\{(x_1, y_1), \ldots, (x_n,
y_n)\}$, the conformal prediction method estimates a confidence set for
$y_{n+1}$ given $x_{n+1}$ that is valid for any finite sample size by merely
assuming that the joint distribution of the data is permutation invariant.
Although attractive, computing such a set is computationally infeasible in most
regression problems. Indeed, in these cases, the unknown variable $y_{n+1}$ can
take an infinite number of possible candidate values, and generating conformal
sets requires retraining a predictive model for each candidate. In this paper,
we focus on a sparse linear model with only a subset of variables for
prediction and use numerical continuation techniques to approximate the
solution path efficiently. The critical property we exploit is that the set of
selected variables is invariant under a small perturbation of the input data.
Therefore, it is sufficient to enumerate and refit the model only at the change
points of the set of active features and smoothly interpolate the rest of the
solution via a Predictor-Corrector mechanism. We show how our path-following
algorithm accurately approximates conformal prediction sets and illustrate its
performance using synthetic and real data examples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05117">$\ell_p$-Regression in the Arbitrary Partition Model of Communication. (arXiv:2307.05117v1 [cs.DS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Honghao Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Woodruff_D/0/1/0/all/0/1">David P. Woodruff</a></p>
<p>We consider the randomized communication complexity of the distributed
$\ell_p$-regression problem in the coordinator model, for $p\in (0,2]$. In this
problem, there is a coordinator and $s$ servers. The $i$-th server receives
$A^i\in\{-M, -M+1, \ldots, M\}^{n\times d}$ and $b^i\in\{-M, -M+1, \ldots,
M\}^n$ and the coordinator would like to find a $(1+\epsilon)$-approximate
solution to $\min_{x\in\mathbb{R}^n} \|(\sum_i A^i)x - (\sum_i b^i)\|_p$. Here
$M \leq \mathrm{poly}(nd)$ for convenience. This model, where the data is
additively shared across servers, is commonly referred to as the arbitrary
partition model.
</p>
<p>We obtain significantly improved bounds for this problem. For $p = 2$, i.e.,
least squares regression, we give the first optimal bound of
$\tilde{\Theta}(sd^2 + sd/\epsilon)$ bits.
</p>
<p>For $p \in (1,2)$,we obtain an $\tilde{O}(sd^2/\epsilon +
sd/\mathrm{poly}(\epsilon))$ upper bound. Notably, for $d$ sufficiently large,
our leading order term only depends linearly on $1/\epsilon$ rather than
quadratically. We also show communication lower bounds of $\Omega(sd^2 +
sd/\epsilon^2)$ for $p\in (0,1]$ and $\Omega(sd^2 + sd/\epsilon)$ for $p\in
(1,2]$. Our bounds considerably improve previous bounds due to (Woodruff et al.
COLT, 2013) and (Vempala et al., SODA, 2020).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05121">Transaction Fraud Detection via Spatial-Temporal-Aware Graph Transformer. (arXiv:2307.05121v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yue Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1">Guanjun Liu</a></p>
<p>How to obtain informative representations of transactions and then perform
the identification of fraudulent transactions is a crucial part of ensuring
financial security. Recent studies apply Graph Neural Networks (GNNs) to the
transaction fraud detection problem. Nevertheless, they encounter challenges in
effectively learning spatial-temporal information due to structural
limitations. Moreover, few prior GNN-based detectors have recognized the
significance of incorporating global information, which encompasses similar
behavioral patterns and offers valuable insights for discriminative
representation learning. Therefore, we propose a novel heterogeneous graph
neural network called Spatial-Temporal-Aware Graph Transformer (STA-GT) for
transaction fraud detection problems. Specifically, we design a temporal
encoding strategy to capture temporal dependencies and incorporate it into the
graph neural network framework, enhancing spatial-temporal information modeling
and improving expressive ability. Furthermore, we introduce a transformer
module to learn local and global information. Pairwise node-node interactions
overcome the limitation of the GNN structure and build up the interactions with
the target node and long-distance ones. Experimental results on two financial
datasets compared to general GNN models and GNN-based fraud detectors
demonstrate that our proposed method STA-GT is effective on the transaction
fraud detection task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05126">Enhancing Continuous Time Series Modelling with a Latent ODE-LSTM Approach. (arXiv:2307.05126v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Coelho_C/0/1/0/all/0/1">C. Coelho</a>, <a href="http://arxiv.org/find/cs/1/au:+Costa_M/0/1/0/all/0/1">M. Fernanda P. Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferras_L/0/1/0/all/0/1">L.L. Ferr&#xe1;s</a></p>
<p>Due to their dynamic properties such as irregular sampling rate and
high-frequency sampling, Continuous Time Series (CTS) are found in many
applications. Since CTS with irregular sampling rate are difficult to model
with standard Recurrent Neural Networks (RNNs), RNNs have been generalised to
have continuous-time hidden dynamics defined by a Neural Ordinary Differential
Equation (Neural ODE), leading to the ODE-RNN model. Another approach that
provides a better modelling is that of the Latent ODE model, which constructs a
continuous-time model where a latent state is defined at all times. The Latent
ODE model uses a standard RNN as the encoder and a Neural ODE as the decoder.
However, since the RNN encoder leads to difficulties with missing data and
ill-defined latent variables, a Latent ODE-RNN model has recently been proposed
that uses a ODE-RNN model as the encoder instead. Both the Latent ODE and
Latent ODE-RNN models are difficult to train due to the vanishing and exploding
gradients problem. To overcome this problem, the main contribution of this
paper is to propose and illustrate a new model based on a new Latent ODE using
an ODE-LSTM (Long Short-Term Memory) network as an encoder -- the Latent
ODE-LSTM model. To limit the growth of the gradients the Norm Gradient Clipping
strategy was embedded on the Latent ODE-LSTM model. The performance evaluation
of the new Latent ODE-LSTM (with and without Norm Gradient Clipping) for
modelling CTS with regular and irregular sampling rates is then demonstrated.
Numerical experiments show that the new Latent ODE-LSTM performs better than
Latent ODE-RNNs and can avoid the vanishing and exploding gradients during
training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05132">On the Use of Self-Supervised Speech Representations in Spontaneous Speech Synthesis. (arXiv:2307.05132v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1">Siyang Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1">Gustav Eje Henter</a>, <a href="http://arxiv.org/find/eess/1/au:+Gustafson_J/0/1/0/all/0/1">Joakim Gustafson</a>, <a href="http://arxiv.org/find/eess/1/au:+Szekely_E/0/1/0/all/0/1">&#xc9;va Sz&#xe9;kely</a></p>
<p>Self-supervised learning (SSL) speech representations learned from large
amounts of diverse, mixed-quality speech data without transcriptions are
gaining ground in many speech technology applications. Prior work has shown
that SSL is an effective intermediate representation in two-stage
text-to-speech (TTS) for both read and spontaneous speech. However, it is still
not clear which SSL and which layer from each SSL model is most suited for
spontaneous TTS. We address this shortcoming by extending the scope of
comparison for SSL in spontaneous TTS to 6 different SSLs and 3 layers within
each SSL. Furthermore, SSL has also shown potential in predicting the mean
opinion scores (MOS) of synthesized speech, but this has only been done in
read-speech MOS prediction. We extend an SSL-based MOS prediction framework
previously developed for scoring read speech synthesis and evaluate its
performance on synthesized spontaneous speech. All experiments are conducted
twice on two different spontaneous corpora in order to find generalizable
trends. Overall, we present comprehensive experimental results on the use of
SSL in spontaneous TTS and MOS prediction to further quantify and understand
how SSL can be used in spontaneous TTS. Audios samples:
https://www.speech.kth.se/tts-demos/sp_ssl_tts
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05134">TIAM -- A Metric for Evaluating Alignment in Text-to-Image Generation. (arXiv:2307.05134v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grimal_P/0/1/0/all/0/1">Paul Grimal</a>, <a href="http://arxiv.org/find/cs/1/au:+Borgne_H/0/1/0/all/0/1">Herv&#xe9; Le Borgne</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferret_O/0/1/0/all/0/1">Olivier Ferret</a>, <a href="http://arxiv.org/find/cs/1/au:+Tourille_J/0/1/0/all/0/1">Julien Tourille</a></p>
<p>The progress in the generation of synthetic images has made it crucial to
assess their quality. While several metrics have been proposed to assess the
rendering of images, it is crucial for Text-to-Image (T2I) models, which
generate images based on a prompt, to consider additional aspects such as to
which extent the generated image matches the important content of the prompt.
Moreover, although the generated images usually result from a random starting
point, the influence of this one is generally not considered. In this article,
we propose a new metric based on prompt templates to study the alignment
between the content specified in the prompt and the corresponding generated
images. It allows us to better characterize the alignment in terms of the type
of the specified objects, their number, and their color. We conducted a study
on several recent T2I models about various aspects. An additional interesting
result we obtained with our approach is that image quality can vary drastically
depending on the latent noise used as a seed for the images. We also quantify
the influence of the number of concepts in the prompt, their order as well as
their (color) attributes. Finally, our method allows us to identify some latent
seeds that produce better images than others, opening novel directions of
research on this understudied topic.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05141">Deep Probabilistic Movement Primitives with a Bayesian Aggregator. (arXiv:2307.05141v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Przystupa_M/0/1/0/all/0/1">Michael Przystupa</a>, <a href="http://arxiv.org/find/cs/1/au:+Haghverd_F/0/1/0/all/0/1">Faezeh Haghverd</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagersand_M/0/1/0/all/0/1">Martin Jagersand</a>, <a href="http://arxiv.org/find/cs/1/au:+Tosatto_S/0/1/0/all/0/1">Samuele Tosatto</a></p>
<p>Movement primitives are trainable parametric models that reproduce robotic
movements starting from a limited set of demonstrations. Previous works
proposed simple linear models that exhibited high sample efficiency and
generalization power by allowing temporal modulation of movements (reproducing
movements faster or slower), blending (merging two movements into one),
via-point conditioning (constraining a movement to meet some particular
via-points) and context conditioning (generation of movements based on an
observed variable, e.g., position of an object). Previous works have proposed
neural network-based motor primitive models, having demonstrated their capacity
to perform tasks with some forms of input conditioning or time-modulation
representations. However, there has not been a single unified deep motor
primitive's model proposed that is capable of all previous operations, limiting
neural motor primitive's potential applications. This paper proposes a deep
movement primitive architecture that encodes all the operations above and uses
a Bayesian context aggregator that allows a more sound context conditioning and
blending. Our results demonstrate our approach can scale to reproduce complex
motions on a larger variety of input choices compared to baselines while
maintaining operations of linear movement primitives provide.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05152">Fast Neural Network Inference on FPGAs for Triggering on Long-Lived Particles at Colliders. (arXiv:2307.05152v1 [hep-ex])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ex/1/au:+Coccaro_A/0/1/0/all/0/1">Andrea Coccaro</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Bello_F/0/1/0/all/0/1">Francesco Armando Di Bello</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Giagu_S/0/1/0/all/0/1">Stefano Giagu</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Rambelli_L/0/1/0/all/0/1">Lucrezia Rambelli</a>, <a href="http://arxiv.org/find/hep-ex/1/au:+Stocchetti_N/0/1/0/all/0/1">Nicola Stocchetti</a></p>
<p>Experimental particle physics demands a sophisticated trigger and acquisition
system capable to efficiently retain the collisions of interest for further
investigation. Heterogeneous computing with the employment of FPGA cards may
emerge as a trending technology for the triggering strategy of the upcoming
high-luminosity program of the Large Hadron Collider at CERN. In this context,
we present two machine-learning algorithms for selecting events where neutral
long-lived particles decay within the detector volume studying their accuracy
and inference time when accelerated on commercially available Xilinx FPGA
accelerator cards. The inference time is also confronted with a CPU- and
GPU-based hardware setup. The proposed new algorithms are proven efficient for
the considered benchmark physics scenario and their accuracy is found to not
degrade when accelerated on the FPGA cards. The results indicate that all
tested architectures fit within the latency requirements of a second-level
trigger farm and that exploiting accelerator technologies for real-time
processing of particle-physics collisions is a promising research field that
deserves additional investigations, in particular with machine-learning models
with a large number of trainable parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05161">On the Effectiveness of Speech Self-supervised Learning for Music. (arXiv:2307.05161v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yinghao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_R/0/1/0/all/0/1">Ruibin Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yizhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xingran Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hanzhi Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chenghua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1">Emmanouil Benetos</a>, <a href="http://arxiv.org/find/cs/1/au:+Ragni_A/0/1/0/all/0/1">Anton Ragni</a>, <a href="http://arxiv.org/find/cs/1/au:+Gyenge_N/0/1/0/all/0/1">Norbert Gyenge</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1">Gus Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Dannenberg_R/0/1/0/all/0/1">Roger Dannenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yike Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jie Fu</a></p>
<p>Self-supervised learning (SSL) has shown promising results in various speech
and natural language processing applications. However, its efficacy in music
information retrieval (MIR) still remains largely unexplored. While previous
SSL models pre-trained on music recordings may have been mostly closed-sourced,
recent speech models such as wav2vec2.0 have shown promise in music modelling.
Nevertheless, research exploring the effectiveness of applying speech SSL
models to music recordings has been limited. We explore the music adaption of
SSL with two distinctive speech-related models, data2vec1.0 and Hubert, and
refer to them as music2vec and musicHuBERT, respectively. We train $12$ SSL
models with 95M parameters under various pre-training configurations and
systematically evaluate the MIR task performances with 13 different MIR tasks.
Our findings suggest that training with music data can generally improve
performance on MIR tasks, even when models are trained using paradigms designed
for speech. However, we identify the limitations of such existing
speech-oriented designs, especially in modelling polyphonic information. Based
on the experimental results, empirical suggestions are also given for designing
future musical SSL strategies and paradigms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05162">SuryaKiran at MEDIQA-Sum 2023: Leveraging LoRA for Clinical Dialogue Summarization. (arXiv:2307.05162v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suri_K/0/1/0/all/0/1">Kunal Suri</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_P/0/1/0/all/0/1">Prakhar Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Saumajit Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Atul Singh</a></p>
<p>Finetuning Large Language Models helps improve the results for
domain-specific use cases. End-to-end finetuning of large language models is
time and resource intensive and has high storage requirements to store the
finetuned version of the large language model. Parameter Efficient Fine Tuning
(PEFT) methods address the time and resource challenges by keeping the large
language model as a fixed base and add additional layers, which the PEFT
methods finetune. This paper demonstrates the evaluation results for one such
PEFT method Low Rank Adaptation (LoRA), for Clinical Dialogue Summarization.
The evaluation results show that LoRA works at par with end-to-end finetuning
for a large language model. The paper presents the evaluations done for solving
both the Subtask A and B from ImageCLEFmedical
{https://www.imageclef.org/2023/medical}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05163">A Mapping Study of Machine Learning Methods for Remaining Useful Life Estimation of Lead-Acid Batteries. (arXiv:2307.05163v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chevtchenko_S/0/1/0/all/0/1">S&#xe9;rgio F Chevtchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Rocha_E/0/1/0/all/0/1">Elisson da Silva Rocha</a>, <a href="http://arxiv.org/find/cs/1/au:+Cruz_B/0/1/0/all/0/1">Bruna Cruz</a>, <a href="http://arxiv.org/find/cs/1/au:+Andrade_E/0/1/0/all/0/1">Ermeson Carneiro de Andrade</a>, <a href="http://arxiv.org/find/cs/1/au:+Araujo_D/0/1/0/all/0/1">Danilo Ricardo Barbosa de Ara&#xfa;jo</a></p>
<p>Energy storage solutions play an increasingly important role in modern
infrastructure and lead-acid batteries are among the most commonly used in the
rechargeable category. Due to normal degradation over time, correctly
determining the battery's State of Health (SoH) and Remaining Useful Life (RUL)
contributes to enhancing predictive maintenance, reliability, and longevity of
battery systems. Besides improving the cost savings, correct estimation of the
SoH can lead to reduced pollution though reuse of retired batteries. This paper
presents a mapping study of the state-of-the-art in machine learning methods
for estimating the SoH and RUL of lead-acid batteries. These two indicators are
critical in the battery management systems of electric vehicles, renewable
energy systems, and other applications that rely heavily on this battery
technology. In this study, we analyzed the types of machine learning algorithms
employed for estimating SoH and RUL, and evaluated their performance in terms
of accuracy and inference time. Additionally, this mapping identifies and
analyzes the most commonly used combinations of sensors in specific
applications, such as vehicular batteries. The mapping concludes by
highlighting potential gaps and opportunities for future research, which lays
the foundation for further advancements in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05187">Decorrelation using Optimal Transport. (arXiv:2307.05187v1 [hep-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/hep-ph/1/au:+Algren_M/0/1/0/all/0/1">Malte Algren</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Raine_J/0/1/0/all/0/1">John Andrew Raine</a>, <a href="http://arxiv.org/find/hep-ph/1/au:+Golling_T/0/1/0/all/0/1">Tobias Golling</a></p>
<p>Being able to decorrelate a feature space from protected attributes is an
area of active research and study in ethics, fairness, and also natural
sciences. We introduce a novel decorrelation method using Convex Neural Optimal
Transport Solvers (Cnots), that is able to decorrelate continuous feature space
against protected attributes with optimal transport. We demonstrate how well it
performs in the context of jet classification in high energy physics, where
classifier scores are desired to be decorrelated from the mass of a jet. The
decorrelation achieved in binary classification approaches the levels achieved
by the state-of-the-art using conditional normalising flows. When moving to
multiclass outputs the optimal transport approach performs significantly better
than the state-of-the-art, suggesting substantial gains at decorrelating
multidimensional feature spaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05189">Using Linear Regression for Iteratively Training Neural Networks. (arXiv:2307.05189v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khadilkar_H/0/1/0/all/0/1">Harshad Khadilkar</a></p>
<p>We present a simple linear regression based approach for learning the weights
and biases of a neural network, as an alternative to standard gradient based
backpropagation. The present work is exploratory in nature, and we restrict the
description and experiments to (i) simple feedforward neural networks, (ii)
scalar (single output) regression problems, and (iii) invertible activation
functions. However, the approach is intended to be extensible to larger, more
complex architectures. The key idea is the observation that the input to every
neuron in a neural network is a linear combination of the activations of
neurons in the previous layer, as well as the parameters (weights and biases)
of the layer. If we are able to compute the ideal total input values to every
neuron by working backwards from the output, we can formulate the learning
problem as a linear least squares problem which iterates between updating the
parameters and the activation values. We present an explicit algorithm that
implements this idea, and we show that (at least for simple problems) the
approach is more stable and faster than gradient-based backpropagation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05193">Membership Inference Attacks on DNNs using Adversarial Perturbations. (arXiv:2307.05193v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ali_H/0/1/0/all/0/1">Hassan Ali</a>, <a href="http://arxiv.org/find/cs/1/au:+Qayyum_A/0/1/0/all/0/1">Adnan Qayyum</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Fuqaha_A/0/1/0/all/0/1">Ala Al-Fuqaha</a>, <a href="http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1">Junaid Qadir</a></p>
<p>Several membership inference (MI) attacks have been proposed to audit a
target DNN. Given a set of subjects, MI attacks tell which subjects the target
DNN has seen during training. This work focuses on the post-training MI attacks
emphasizing high confidence membership detection -- True Positive Rates (TPR)
at low False Positive Rates (FPR). Current works in this category -- likelihood
ratio attack (LiRA) and enhanced MI attack (EMIA) -- only perform well on
complex datasets (e.g., CIFAR-10 and Imagenet) where the target DNN overfits
its train set, but perform poorly on simpler datasets (0% TPR by both attacks
on Fashion-MNIST, 2% and 0% TPR respectively by LiRA and EMIA on MNIST at 1%
FPR). To address this, firstly, we unify current MI attacks by presenting a
framework divided into three stages -- preparation, indication and decision.
Secondly, we utilize the framework to propose two novel attacks: (1)
Adversarial Membership Inference Attack (AMIA) efficiently utilizes the
membership and the non-membership information of the subjects while
adversarially minimizing a novel loss function, achieving 6% TPR on both
Fashion-MNIST and MNIST datasets; and (2) Enhanced AMIA (E-AMIA) combines EMIA
and AMIA to achieve 8% and 4% TPRs on Fashion-MNIST and MNIST datasets
respectively, at 1% FPR. Thirdly, we introduce two novel augmented indicators
that positively leverage the loss information in the Gaussian neighborhood of a
subject. This improves TPR of all four attacks on average by 2.5% and 0.25%
respectively on Fashion-MNIST and MNIST datasets at 1% FPR. Finally, we propose
simple, yet novel, evaluation metric, the running TPR average (RTA) at a given
FPR, that better distinguishes different MI attacks in the low FPR region. We
also show that AMIA and E-AMIA are more transferable to the unknown DNNs (other
than the target DNN) and are more robust to DP-SGD training as compared to LiRA
and EMIA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05194">Differentially Private Statistical Inference through $\beta$-Divergence One Posterior Sampling. (arXiv:2307.05194v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Jewson_J/0/1/0/all/0/1">Jack Jewson</a>, <a href="http://arxiv.org/find/stat/1/au:+Ghalebikesabi_S/0/1/0/all/0/1">Sahra Ghalebikesabi</a>, <a href="http://arxiv.org/find/stat/1/au:+Holmes_C/0/1/0/all/0/1">Chris Holmes</a></p>
<p>Differential privacy guarantees allow the results of a statistical analysis
involving sensitive data to be released without compromising the privacy of any
individual taking part. Achieving such guarantees generally requires the
injection of noise, either directly into parameter estimates or into the
estimation process. Instead of artificially introducing perturbations, sampling
from Bayesian posterior distributions has been shown to be a special case of
the exponential mechanism, producing consistent, and efficient private
estimates without altering the data generative process. The application of
current approaches has, however, been limited by their strong bounding
assumptions which do not hold for basic models, such as simple linear
regressors. To ameliorate this, we propose $\beta$D-Bayes, a posterior sampling
scheme from a generalised posterior targeting the minimisation of the
$\beta$-divergence between the model and the data generating process. This
provides private estimation that is generally applicable without requiring
changes to the underlying model and consistently learns the data generating
parameter. We show that $\beta$D-Bayes produces more precise inference
estimation for the same privacy guarantees, and further facilitates
differentially private estimation via posterior sampling for complex
classifiers and continuous regression models such as neural networks for the
first time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05199">Reject option models comprising out-of-distribution detection. (arXiv:2307.05199v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franc_V/0/1/0/all/0/1">Vojtech Franc</a>, <a href="http://arxiv.org/find/cs/1/au:+Prusa_D/0/1/0/all/0/1">Daniel Prusa</a>, <a href="http://arxiv.org/find/cs/1/au:+Paplham_J/0/1/0/all/0/1">Jakub Paplham</a></p>
<p>The optimal prediction strategy for out-of-distribution (OOD) setups is a
fundamental question in machine learning. In this paper, we address this
question and present several contributions. We propose three reject option
models for OOD setups: the Cost-based model, the Bounded TPR-FPR model, and the
Bounded Precision-Recall model. These models extend the standard reject option
models used in non-OOD setups and define the notion of an optimal OOD selective
classifier. We establish that all the proposed models, despite their different
formulations, share a common class of optimal strategies. Motivated by the
optimal strategy, we introduce double-score OOD methods that leverage
uncertainty scores from two chosen OOD detectors: one focused on OOD/ID
discrimination and the other on misclassification detection. The experimental
results consistently demonstrate the superior performance of this simple
strategy compared to state-of-the-art methods. Additionally, we propose novel
evaluation metrics derived from the definition of the optimal strategy under
the proposed OOD rejection models. These new metrics provide a comprehensive
and reliable assessment of OOD methods without the deficiencies observed in
existing evaluation approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05209">Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azran_G/0/1/0/all/0/1">Guy Azran</a>, <a href="http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1">Mohamad H. Danesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1">Stefano V. Albrecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Keren_S/0/1/0/all/0/1">Sarah Keren</a></p>
<p>Recent studies show that deep reinforcement learning (DRL) agents tend to
overfit to the task on which they were trained and fail to adapt to minor
environment changes. To expedite learning when transferring to unseen tasks, we
propose a novel approach to representing the current task using reward machines
(RM), state machine abstractions that induce subtasks based on the current
task's rewards and dynamics. Our method provides agents with symbolic
representations of optimal transitions from their current abstract state and
rewards them for achieving these transitions. These representations are shared
across tasks, allowing agents to exploit knowledge of previously encountered
symbols and transitions, thus enhancing transfer. Our empirical evaluation
shows that our representations improve sample efficiency and few-shot transfer
in a variety of domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05213">Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning. (arXiv:2307.05213v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Silvestri_M/0/1/0/all/0/1">Mattia Silvestri</a>, <a href="http://arxiv.org/find/cs/1/au:+Berden_S/0/1/0/all/0/1">Senne Berden</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandi_J/0/1/0/all/0/1">Jayanta Mandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmutogullari_A/0/1/0/all/0/1">Ali &#x130;rfan Mahmuto&#x11f;ullar&#x131;</a>, <a href="http://arxiv.org/find/cs/1/au:+Mulamba_M/0/1/0/all/0/1">Maxime Mulamba</a>, <a href="http://arxiv.org/find/cs/1/au:+Filippo_A/0/1/0/all/0/1">Allegra De Filippo</a>, <a href="http://arxiv.org/find/cs/1/au:+Guns_T/0/1/0/all/0/1">Tias Guns</a>, <a href="http://arxiv.org/find/cs/1/au:+Lombardi_M/0/1/0/all/0/1">Michele Lombardi</a></p>
<p>Many real-world optimization problems contain unknown parameters that must be
predicted prior to solving. To train the predictive machine learning (ML)
models involved, the commonly adopted approach focuses on maximizing predictive
accuracy. However, this approach does not always lead to the minimization of
the downstream task loss. Decision-focused learning (DFL) is a recently
proposed paradigm whose goal is to train the ML model by directly minimizing
the task loss. However, state-of-the-art DFL methods are limited by the
assumptions they make about the structure of the optimization problem (e.g.,
that the problem is linear) and by the fact that can only predict parameters
that appear in the objective function. In this work, we address these
limitations by instead predicting \textit{distributions} over parameters and
adopting score function gradient estimation (SFGE) to compute decision-focused
updates to the predictive model, thereby widening the applicability of DFL. Our
experiments show that by using SFGE we can: (1) deal with predictions that
occur both in the objective function and in the constraints; and (2)
effectively tackle two-stage stochastic optimization problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05217">Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1">Michail Chatzianastasis</a>, <a href="http://arxiv.org/find/cs/1/au:+Nikolentzos_G/0/1/0/all/0/1">Giannis Nikolentzos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1">Michalis Vazirgiannis</a></p>
<p>Graph neural networks have become the standard approach for dealing with
learning problems on graphs. Among the different variants of graph neural
networks, graph attention networks (GATs) have been applied with great success
to different tasks. In the GAT model, each node assigns an importance score to
its neighbors using an attention mechanism. However, similar to other graph
neural networks, GATs aggregate messages from nodes that belong to different
classes, and therefore produce node representations that are not well separated
with respect to the different classes, which might hurt their performance. In
this work, to alleviate this problem, we propose a new technique that can be
incorporated into any graph attention model to encourage higher attention
scores between nodes that share the same class label. We evaluate the proposed
method on several node classification datasets demonstrating increased
performance over standard baseline models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05228">Attribute Controlled Dialogue Prompting. (arXiv:2307.05228v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Runcheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Rashid_A/0/1/0/all/0/1">Ahmad Rashid</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobyzev_I/0/1/0/all/0/1">Ivan Kobyzev</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezagholizadeh_M/0/1/0/all/0/1">Mehdi Rezagholizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Poupart_P/0/1/0/all/0/1">Pascal Poupart</a></p>
<p>Prompt-tuning has become an increasingly popular parameter-efficient method
for adapting large pretrained language models to downstream tasks. However,
both discrete prompting and continuous prompting assume fixed prompts for all
data samples within a task, neglecting the fact that inputs vary greatly in
some tasks such as open-domain dialogue generation. In this paper, we present a
novel, instance-specific prompt-tuning algorithm for dialogue generation.
Specifically, we generate prompts based on instance-level control code, rather
than the conversation history, to explore their impact on controlled dialogue
generation. Experiments on popular open-domain dialogue datasets, evaluated on
both automated metrics and human evaluation, demonstrate that our method is
superior to prompting baselines and comparable to fine-tuning with only 5%-6%
of total parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05232">A Survey From Distributed Machine Learning to Distributed Deep Learning. (arXiv:2307.05232v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mohammad Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yazdanparast_Z/0/1/0/all/0/1">Zahra Yazdanparast</a></p>
<p>Artificial intelligence has achieved significant success in handling complex
tasks in recent years. This success is due to advances in machine learning
algorithms and hardware acceleration. In order to obtain more accurate results
and solve more complex problems, algorithms must be trained with more data.
This huge amount of data could be time-consuming to process and require a great
deal of computation. This solution could be achieved by distributing the data
and algorithm across several machines, which is known as distributed machine
learning. There has been considerable effort put into distributed machine
learning algorithms, and different methods have been proposed so far. In this
article, we present a comprehensive summary of the current state-of-the-art in
the field through the review of these algorithms. We divide this algorithms in
classification and clustering (traditional machine learning), deep learning and
deep reinforcement learning groups. Distributed deep learning has gained more
attention in recent years and most of studies worked on this algorithms. As a
result, most of the articles we discussed here belong to this category. Based
on our investigation of algorithms, we highlight limitations that should be
addressed in future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05249">DRMC: A Generalist Model with Dynamic Routing for Multi-Center PET Image Synthesis. (arXiv:2307.05249v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1">Zhiwen Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_Y/0/1/0/all/0/1">Yang Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Hui Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Wei_B/0/1/0/all/0/1">Bingzheng Wei</a>, <a href="http://arxiv.org/find/eess/1/au:+Fan_Y/0/1/0/all/0/1">Yubo Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_Y/0/1/0/all/0/1">Yan Xu</a></p>
<p>Multi-center positron emission tomography (PET) image synthesis aims at
recovering low-dose PET images from multiple different centers. The
generalizability of existing methods can still be suboptimal for a multi-center
study due to domain shifts, which result from non-identical data distribution
among centers with different imaging systems/protocols. While some approaches
address domain shifts by training specialized models for each center, they are
parameter inefficient and do not well exploit the shared knowledge across
centers. To address this, we develop a generalist model that shares
architecture and parameters across centers to utilize the shared knowledge.
However, the generalist model can suffer from the center interference issue,
\textit{i.e.} the gradient directions of different centers can be inconsistent
or even opposite owing to the non-identical data distribution. To mitigate such
interference, we introduce a novel dynamic routing strategy with cross-layer
connections that routes data from different centers to different experts.
Experiments show that our generalist model with dynamic routing (DRMC) exhibits
excellent generalizability across centers. Code and data are available at:
https://github.com/Yaziwel/Multi-Center-PET-Image-Synthesis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05252">MAP- and MLE-Based Teaching. (arXiv:2307.05252v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Simon_H/0/1/0/all/0/1">Hans Ulrich Simon</a>, <a href="http://arxiv.org/find/cs/1/au:+Telle_J/0/1/0/all/0/1">Jan Arne Telle</a></p>
<p>Imagine a learner L who tries to infer a hidden concept from a collection of
observations. Building on the work [4] of Ferri et al., we assume the learner
to be parameterized by priors P(c) and by c-conditional likelihoods P(z|c)
where c ranges over all concepts in a given class C and z ranges over all
observations in an observation set Z. L is called a MAP-learner (resp. an
MLE-learner) if it thinks of a collection S of observations as a random sample
and returns the concept with the maximum a-posteriori probability (resp. the
concept which maximizes the c-conditional likelihood of S). Depending on
whether L assumes that S is obtained from ordered or unordered sampling resp.
from sampling with or without replacement, we can distinguish four different
sampling modes. Given a target concept c in C, a teacher for a MAP-learner L
aims at finding a smallest collection of observations that causes L to return
c. This approach leads in a natural manner to various notions of a MAP- or
MLE-teaching dimension of a concept class C. Our main results are: We show that
this teaching model has some desirable monotonicity properties. We clarify how
the four sampling modes are related to each other. As for the (important!)
special case, where concepts are subsets of a domain and observations are
0,1-labeled examples, we obtain some additional results. First of all, we
characterize the MAP- and MLE-teaching dimension associated with an optimally
parameterized MAP-learner graph-theoretically. From this central result, some
other ones are easy to derive. It is shown, for instance, that the MLE-teaching
dimension is either equal to the MAP-teaching dimension or exceeds the latter
by 1. It is shown furthermore that these dimensions can be bounded from above
by the so-called antichain number, the VC-dimension and related combinatorial
parameters. Moreover they can be computed in polynomial time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05260">U-CREAT: Unsupervised Case Retrieval using Events extrAcTion. (arXiv:2307.05260v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Abhinav Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Akshat Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanikella_S/0/1/0/all/0/1">Sai Kiran Tanikella</a>, <a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1">Ashutosh Modi</a></p>
<p>The task of Prior Case Retrieval (PCR) in the legal domain is about
automatically citing relevant (based on facts and precedence) prior legal cases
in a given query case. To further promote research in PCR, in this paper, we
propose a new large benchmark (in English) for the PCR task: IL-PCR (Indian
Legal Prior Case Retrieval) corpus. Given the complex nature of case relevance
and the long size of legal documents, BM25 remains a strong baseline for
ranking the cited prior documents. In this work, we explore the role of events
in legal case retrieval and propose an unsupervised retrieval method-based
pipeline U-CREAT (Unsupervised Case Retrieval using Events Extraction). We find
that the proposed unsupervised retrieval method significantly increases
performance compared to BM25 and makes retrieval faster by a considerable
margin, making it applicable to real-time case retrieval systems. Our proposed
system is generic, we show that it generalizes across two different legal
systems (Indian and Canadian), and it shows state-of-the-art performance on the
benchmarks for both the legal systems (IL-PCR and COLIEE corpora).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05275">CareFall: Automatic Fall Detection through Wearable Devices and AI Methods. (arXiv:2307.05275v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ruiz_Garcia_J/0/1/0/all/0/1">Juan Carlos Ruiz-Garcia</a>, <a href="http://arxiv.org/find/cs/1/au:+Tolosana_R/0/1/0/all/0/1">Ruben Tolosana</a>, <a href="http://arxiv.org/find/cs/1/au:+Vera_Rodriguez_R/0/1/0/all/0/1">Ruben Vera-Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Moro_C/0/1/0/all/0/1">Carlos Moro</a></p>
<p>The aging population has led to a growing number of falls in our society,
affecting global public health worldwide. This paper presents CareFall, an
automatic Fall Detection System (FDS) based on wearable devices and Artificial
Intelligence (AI) methods. CareFall considers the accelerometer and gyroscope
time signals extracted from a smartwatch. Two different approaches are used for
feature extraction and classification: i) threshold-based, and ii) machine
learning-based. Experimental results on two public databases show that the
machine learning-based approach, which combines accelerometer and gyroscope
information, outperforms the threshold-based approach in terms of accuracy,
sensitivity, and specificity. This research contributes to the design of smart
and user-friendly solutions to mitigate the negative consequences of falls
among older people.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05284">On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets. (arXiv:2307.05284v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiashuo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianyu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1">Peng Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Namkoong_H/0/1/0/all/0/1">Hongseok Namkoong</a></p>
<p>Different distribution shifts require different algorithmic and operational
interventions. Methodological research must be grounded by the specific shifts
they address. Although nascent benchmarks provide a promising empirical
foundation, they implicitly focus on covariate shifts, and the validity of
empirical findings depends on the type of shift, e.g., previous observations on
algorithmic performance can fail to be valid when the $Y|X$ distribution
changes. We conduct a thorough investigation of natural shifts in 5 tabular
datasets over 86,000 model configurations, and find that $Y|X$-shifts are most
prevalent. To encourage researchers to develop a refined language for
distribution shifts, we build WhyShift, an empirical testbed of curated
real-world shifts where we characterize the type of shift we benchmark
performance over. Since $Y|X$-shifts are prevalent in tabular settings, we
identify covariate regions that suffer the biggest $Y|X$-shifts and discuss
implications for algorithmic and data-based interventions. Our testbed
highlights the importance of future research that builds an understanding of
how distributions differ.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05299">Discovering Symbolic Laws Directly from Trajectories with Hamiltonian Graph Neural Networks. (arXiv:2307.05299v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bishnoi_S/0/1/0/all/0/1">Suresh Bishnoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhattoo_R/0/1/0/all/0/1">Ravinder Bhattoo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayadeva/0/1/0/all/0/1">Jayadeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1">Sayan Ranu</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnan_N/0/1/0/all/0/1">N M Anoop Krishnan</a></p>
<p>The time evolution of physical systems is described by differential
equations, which depend on abstract quantities like energy and force.
Traditionally, these quantities are derived as functionals based on observables
such as positions and velocities. Discovering these governing symbolic laws is
the key to comprehending the interactions in nature. Here, we present a
Hamiltonian graph neural network (HGNN), a physics-enforced GNN that learns the
dynamics of systems directly from their trajectory. We demonstrate the
performance of HGNN on n-springs, n-pendulums, gravitational systems, and
binary Lennard Jones systems; HGNN learns the dynamics in excellent agreement
with the ground truth from small amounts of data. We also evaluate the ability
of HGNN to generalize to larger system sizes, and to hybrid spring-pendulum
system that is a combination of two original systems (spring and pendulum) on
which the models are trained independently. Finally, employing symbolic
regression on the learned HGNN, we infer the underlying equations relating the
energy functionals, even for complex systems such as the binary Lennard-Jones
liquid. Our framework facilitates the interpretable discovery of interaction
laws directly from physical system trajectories. Furthermore, this approach can
be extended to other systems with topology-dependent dynamics, such as cells,
polydisperse gels, or deformable bodies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05318">Predicting small molecules solubilities on endpoint devices using deep ensemble neural networks. (arXiv:2307.05318v1 [physics.chem-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Ramos_M/0/1/0/all/0/1">Mayk Caldas Ramos</a>, <a href="http://arxiv.org/find/physics/1/au:+White_A/0/1/0/all/0/1">Andrew D. White</a></p>
<p>Aqueous solubility is a valuable yet challenging property to predict.
Computing solubility using first-principles methods requires accounting for the
competing effects of entropy and enthalpy, resulting in long computations for
relatively poor accuracy. Data-driven approaches, such as deep learning, offer
improved accuracy and computational efficiency but typically lack uncertainty
quantification. Additionally, ease of use remains a concern for any
computational technique, resulting in the sustained popularity of group-based
contribution methods. In this work, we addressed these problems with a deep
learning model with predictive uncertainty that runs on a static website
(without a server). This approach moves computing needs onto the website
visitor without requiring installation, removing the need to pay for and
maintain servers. Our model achieves satisfactory results in solubility
prediction. Furthermore, we demonstrate how to create molecular property
prediction models that balance uncertainty and ease of use. The code is
available at \url{https://github.com/ur-whitelab/mol.dev}, and the model is
usable at \url{https://mol.dev}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05330">The Value of Chess Squares. (arXiv:2307.05330v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Aditya Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Maharaj_S/0/1/0/all/0/1">Shiva Maharaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Polson_N/0/1/0/all/0/1">Nicholas Polson</a>, <a href="http://arxiv.org/find/cs/1/au:+Sokolov_V/0/1/0/all/0/1">Vadim Sokolov</a></p>
<p>Valuing chess squares and determining the placement of pieces on the board
are the main objectives of our study. With the emergence of chess AI, it has
become possible to accurately assess the worth of positions in a game of chess.
The conventional approach assigns fixed values to pieces $(\symking=\infty,
\symqueen=9, \symrook=5, \symbishop=3, \symknight=3, \sympawn=1)$. We enhance
this analysis by introducing marginal valuations for both pieces and squares.
We demonstrate our method by examining the positioning of Knights and Bishops,
and also provide valuable insights into the valuation of pawns. Notably,
Nimzowitsch was among the pioneers in advocating for the significance of Pawn
structure and valuation. Finally, we conclude by suggesting potential avenues
for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05333">Unbiased Pain Assessment through Wearables and EHR Data: Multi-attribute Fairness Loss-based CNN Approach. (arXiv:2307.05333v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sultana_S/0/1/0/all/0/1">Sharmin Sultana</a>, <a href="http://arxiv.org/find/eess/1/au:+Rahman_M/0/1/0/all/0/1">Md Mahmudur Rahman</a>, <a href="http://arxiv.org/find/eess/1/au:+Mahi_A/0/1/0/all/0/1">Atqiya Munawara Mahi</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shao-Hsien Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Alam_M/0/1/0/all/0/1">Mohammad Arif Ul Alam</a></p>
<p>The combination of diverse health data (IoT, EHR, and clinical surveys) and
scalable-adaptable Artificial Intelligence (AI), has enabled the discovery of
physical, behavioral, and psycho-social indicators of pain status. Despite the
hype and promise to fundamentally alter the healthcare system with
technological advancements, much AI adoption in clinical pain evaluation has
been hampered by the heterogeneity of the problem itself and other challenges,
such as personalization and fairness. Studies have revealed that many AI (i.e.,
machine learning or deep learning) models display biases and discriminate
against specific population segments (such as those based on gender or
ethnicity), which breeds skepticism among medical professionals about AI
adaptability. In this paper, we propose a Multi-attribute Fairness Loss (MAFL)
based CNN model that aims to account for any sensitive attributes included in
the data and fairly predict patients' pain status while attempting to minimize
the discrepancies between privileged and unprivileged groups. In order to
determine whether the trade-off between accuracy and fairness can be satisfied,
we compare the proposed model with well-known existing mitigation procedures,
and studies reveal that the implemented model performs favorably in contrast to
state-of-the-art methods. Utilizing NIH All-Of-US data, where a cohort of 868
distinct individuals with wearables and EHR data gathered over 1500 days has
been taken into consideration to analyze our suggested fair pain assessment
system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05339">A Self-Supervised Algorithm for Denoising Photoplethysmography Signals for Heart Rate Estimation from Wearables. (arXiv:2307.05339v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jain_P/0/1/0/all/0/1">Pranay Jain</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_C/0/1/0/all/0/1">Cheng Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a></p>
<p>Smart watches and other wearable devices are equipped with
photoplethysmography (PPG) sensors for monitoring heart rate and other aspects
of cardiovascular health. However, PPG signals collected from such devices are
susceptible to corruption from noise and motion artifacts, which cause errors
in heart rate estimation. Typical denoising approaches filter or reconstruct
the signal in ways that eliminate much of the morphological information, even
from the clean parts of the signal that would be useful to preserve. In this
work, we develop an algorithm for denoising PPG signals that reconstructs the
corrupted parts of the signal, while preserving the clean parts of the PPG
signal. Our novel framework relies on self-supervised training, where we
leverage a large database of clean PPG signals to train a denoising
autoencoder. As we show, our reconstructed signals provide better estimates of
heart rate from PPG signals than the leading heart rate estimation methods.
Further experiments show significant improvement in Heart Rate Variability
(HRV) estimation from PPG signals using our algorithm. We conclude that our
algorithm denoises PPG signals in a way that can improve downstream analysis of
many different health metrics from wearable devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05341">Tracking Most Significant Shifts in Nonparametric Contextual Bandits. (arXiv:2307.05341v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Suk_J/0/1/0/all/0/1">Joe Suk</a>, <a href="http://arxiv.org/find/stat/1/au:+Kpotufe_S/0/1/0/all/0/1">Samory Kpotufe</a></p>
<p>We study nonparametric contextual bandits where Lipschitz mean reward
functions may change over time. We first establish the minimax dynamic regret
rate in this less understood setting in terms of number of changes $L$ and
total-variation $V$, both capturing all changes in distribution over context
space, and argue that state-of-the-art procedures are suboptimal in this
setting.
</p>
<p>Next, we tend to the question of an adaptivity for this setting, i.e.
achieving the minimax rate without knowledge of $L$ or $V$. Quite importantly,
we posit that the bandit problem, viewed locally at a given context $X_t$,
should not be affected by reward changes in other parts of context space $\cal
X$. We therefore propose a notion of change, which we term experienced
significant shifts, that better accounts for locality, and thus counts
considerably less changes than $L$ and $V$. Furthermore, similar to recent work
on non-stationary MAB (Suk &amp; Kpotufe, 2022), experienced significant shifts
only count the most significant changes in mean rewards, e.g., severe best-arm
changes relevant to observed contexts.
</p>
<p>Our main result is to show that this more tolerant notion of change can in
fact be adapted to.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05350">Route, Interpret, Repeat: Blurring the line between post hoc explainability and interpretable models. (arXiv:2307.05350v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shantanu Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Ke Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Arabshahi_F/0/1/0/all/0/1">Forough Arabshahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1">Kayhan Batmanghelich</a></p>
<p>The current approach to ML model design is either to choose a flexible
Blackbox model and explain it post hoc or to start with an interpretable model.
Blackbox models are flexible but difficult to explain, whereas interpretable
models are designed to be explainable. However, developing interpretable models
necessitates extensive ML knowledge, and the resulting models tend to be less
flexible, offering potentially subpar performance compared to their Blackbox
equivalents. This paper aims to blur the distinction between a post hoc
explanation of a BlackBox and constructing interpretable models. We propose
beginning with a flexible BlackBox model and gradually \emph{carving out} a
mixture of interpretable models and a \emph{residual network}. Our design
identifies a subset of samples and \emph{routes} them through the interpretable
models. The remaining samples are routed through a flexible residual network.
We adopt First Order Logic (FOL) as the interpretable model's backbone, which
provides basic reasoning on concepts retrieved from the BlackBox model. On the
residual network, we repeat the method until the proportion of data explained
by the residual network falls below a desired threshold. Our approach offers
several advantages. First, the mixture of interpretable and flexible residual
networks results in almost no compromise in performance. Second, the route,
interpret, and repeat approach yields a highly flexible interpretable model.
Our extensive experiment demonstrates the performance of the model on various
datasets. We show that by editing the FOL model, we can fix the shortcut
learned by the original BlackBox model. Finally, our method provides a
framework for a hybrid symbolic-connectionist network that is simple to train
and adaptable to many applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05356">VisText: A Benchmark for Semantically Rich Chart Captioning. (arXiv:2307.05356v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_B/0/1/0/all/0/1">Benny J. Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Boggust_A/0/1/0/all/0/1">Angie Boggust</a>, <a href="http://arxiv.org/find/cs/1/au:+Satyanarayan_A/0/1/0/all/0/1">Arvind Satyanarayan</a></p>
<p>Captions that describe or explain charts help improve recall and
comprehension of the depicted data and provide a more accessible medium for
people with visual disabilities. However, current approaches for automatically
generating such captions struggle to articulate the perceptual or cognitive
features that are the hallmark of charts (e.g., complex trends and patterns).
In response, we introduce VisText: a dataset of 12,441 pairs of charts and
captions that describe the charts' construction, report key statistics, and
identify perceptual and cognitive phenomena. In VisText, a chart is available
as three representations: a rasterized image, a backing data table, and a scene
graph -- a hierarchical representation of a chart's visual elements akin to a
web page's Document Object Model (DOM). To evaluate the impact of VisText, we
fine-tune state-of-the-art language models on our chart captioning task and
apply prefix-tuning to produce captions that vary the semantic content they
convey. Our models generate coherent, semantically rich captions and perform on
par with state-of-the-art chart captioning models across machine translation
and text generation metrics. Through qualitative analysis, we identify six
broad categories of errors that our models make that can inform future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05358">Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1">Sikai Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuaicheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1">Weiming Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kunlin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Jun Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1">Shuai Yi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Song Guo</a></p>
<p>Federated learning has become a popular method to learn from decentralized
heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train
models from a small fraction of labeled data due to label scarcity on
decentralized clients. Existing FSSL methods assume independent and identically
distributed (IID) labeled data across clients and consistent class distribution
between labeled and unlabeled data within a client. This work studies a more
practical and challenging scenario of FSSL, where data distribution is
different not only across clients but also within a client between labeled and
unlabeled data. To address this challenge, we propose a novel FSSL framework
with dual regulators, FedDure.} FedDure lifts the previous assumption with a
coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg
regularizes the updating of the local model by tracking the learning effect on
labeled data distribution; F-reg learns an adaptive weighting scheme tailored
for unlabeled instances in each client. We further formulate the client model
training as bi-level optimization that adaptively optimizes the model in the
client with two regulators. Theoretically, we show the convergence guarantee of
the dual regulators. Empirically, we demonstrate that FedDure is superior to
the existing methods across a wide range of settings, notably by more than 11%
on CIFAR-10 and CINIC-10 datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05361">A Physics-Informed Low-Shot Learning For sEMG-Based Estimation of Muscle Force and Joint Kinematics. (arXiv:2307.05361v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1">Yue Shi</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1">Shuhao Ma</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_Y/0/1/0/all/0/1">Yihui Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiqiang Zhang</a></p>
<p>Muscle force and joint kinematics estimation from surface electromyography
(sEMG) are essential for real-time biomechanical analysis of the dynamic
interplay among neural muscle stimulation, muscle dynamics, and kinetics.
Recent advances in deep neural networks (DNNs) have shown the potential to
improve biomechanical analysis in a fully automated and reproducible manner.
However, the small sample nature and physical interpretability of biomechanical
analysis limit the applications of DNNs. This paper presents a novel
physics-informed low-shot learning method for sEMG-based estimation of muscle
force and joint kinematics. This method seamlessly integrates Lagrange's
equation of motion and inverse dynamic muscle model into the generative
adversarial network (GAN) framework for structured feature decoding and
extrapolated estimation from the small sample data. Specifically, Lagrange's
equation of motion is introduced into the generative model to restrain the
structured decoding of the high-level features following the laws of physics.
And a physics-informed policy gradient is designed to improve the adversarial
learning efficiency by rewarding the consistent physical representation of the
extrapolated estimations and the physical references. Experimental validations
are conducted on two scenarios (i.e. the walking trials and wrist motion
trials). Results indicate that the estimations of the muscle forces and joint
kinematics are unbiased compared to the physics-based inverse dynamics, which
outperforms the selected benchmark methods, including physics-informed
convolution neural network (PI-CNN), vallina generative adversarial network
(GAN), and multi-layer extreme learning machine (ML-ELM).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05362">SleepEGAN: A GAN-enhanced Ensemble Deep Learning Model for Imbalanced Classification of Sleep Stages. (arXiv:2307.05362v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cheng_X/0/1/0/all/0/1">Xuewei Cheng</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1">Ke Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1">Yi Zou</a>, <a href="http://arxiv.org/find/eess/1/au:+Ma_S/0/1/0/all/0/1">Shujie Ma</a></p>
<p>Deep neural networks have played an important role in automatic sleep stage
classification because of their strong representation and in-model feature
transformation abilities. However, class imbalance and individual heterogeneity
which typically exist in raw EEG signals of sleep data can significantly affect
the classification performance of any machine learning algorithms. To solve
these two problems, this paper develops a generative adversarial network
(GAN)-powered ensemble deep learning model, named SleepEGAN, for the imbalanced
classification of sleep stages. To alleviate class imbalance, we propose a new
GAN (called EGAN) architecture adapted to the features of EEG signals for data
augmentation. The generated samples for the minority classes are used in the
training process. In addition, we design a cost-free ensemble learning strategy
to reduce the model estimation variance caused by the heterogeneity between the
validation and test sets, so as to enhance the accuracy and robustness of
prediction performance. We show that the proposed method can improve
classification accuracy compared to several existing state-of-the-art methods
using three public sleep datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05364">Neural network analysis of neutron and X-ray reflectivity data: Incorporating prior knowledge for tackling the phase problem. (arXiv:2307.05364v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Munteanu_V/0/1/0/all/0/1">Valentin Munteanu</a>, <a href="http://arxiv.org/find/eess/1/au:+Starostin_V/0/1/0/all/0/1">Vladimir Starostin</a>, <a href="http://arxiv.org/find/eess/1/au:+Greco_A/0/1/0/all/0/1">Alessandro Greco</a>, <a href="http://arxiv.org/find/eess/1/au:+Pithan_L/0/1/0/all/0/1">Linus Pithan</a>, <a href="http://arxiv.org/find/eess/1/au:+Gerlach_A/0/1/0/all/0/1">Alexander Gerlach</a>, <a href="http://arxiv.org/find/eess/1/au:+Hinderhofer_A/0/1/0/all/0/1">Alexander Hinderhofer</a>, <a href="http://arxiv.org/find/eess/1/au:+Kowarik_S/0/1/0/all/0/1">Stefan Kowarik</a>, <a href="http://arxiv.org/find/eess/1/au:+Schreiber_F/0/1/0/all/0/1">Frank Schreiber</a></p>
<p>Due to the lack of phase information, determining the physical parameters of
multilayer thin films from measured neutron and X-ray reflectivity curves is,
on a fundamental level, an underdetermined inverse problem. This so-called
phase problem poses limitations on standard neural networks, constraining the
range and number of considered parameters in previous machine learning
solutions. To overcome this, we present an approach that utilizes prior
knowledge to regularize the training process over larger parameter spaces. We
demonstrate the effectiveness of our method in various scenarios, including
multilayer structures with box model parameterization and a physics-inspired
special parameterization of the scattering length density profile for a
multilayer structure. By leveraging the input of prior knowledge, we can
improve the training dynamics and address the underdetermined ("ill-posed")
nature of the problem. In contrast to previous methods, our approach scales
favorably when increasing the complexity of the inverse problem, working
properly even for a 5-layer multilayer model and an N-layer periodic multilayer
model with up to 17 open parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05370">Capafoldable: self-tracking foldable smart textiles with capacitive sensing. (arXiv:2307.05370v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ray_L/0/1/0/all/0/1">Lala Shakti Swarup Ray</a>, <a href="http://arxiv.org/find/cs/1/au:+Geissler_D/0/1/0/all/0/1">Daniel Gei&#xdf;ler</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bo Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukowicz_P/0/1/0/all/0/1">Paul Lukowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Greinke_B/0/1/0/all/0/1">Berit Greinke</a></p>
<p>Folding is an unique structural technique to enable planer materials with
motion or 3D mechanical properties. Textile-based capacitive sensing has shown
to be sensitive to the geometry deformation and relative motion of conductive
textiles. In this work, we propose a novel self-tracking foldable smart textile
by combining folded fabric structures and capacitive sensing to detect the
structural motions using state-of-the-art sensing circuits and deep learning
technologies. We created two folding patterns, Accordion and Chevron, each with
two layouts of capacitive sensors in the form of thermobonded conductive
textile patches. In an experiment of manually moving patches of the folding
patterns, we developed deep neural network to learn and reconstruct the
vision-tracked shape of the patches. Through our approach, the geometry
primitives defining the patch shape can be reconstructed from the capacitive
signals with R-squared value of up to 95\% and tracking error of 1cm for 22.5cm
long patches. With mechanical, electrical and sensing properties, Capafoldable
could enable a new range of smart textile applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05373">Classification of sleep stages from EEG, EOG and EMG signals by SSNet. (arXiv:2307.05373v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Almutairi_H/0/1/0/all/0/1">Haifa Almutairi</a>, <a href="http://arxiv.org/find/eess/1/au:+Hassan_G/0/1/0/all/0/1">Ghulam Mubashar Hassan</a>, <a href="http://arxiv.org/find/eess/1/au:+Datta_A/0/1/0/all/0/1">Amitava Datta</a></p>
<p>Classification of sleep stages plays an essential role in diagnosing
sleep-related diseases including Sleep Disorder Breathing (SDB) disease. In
this study, we propose an end-to-end deep learning architecture, named SSNet,
which comprises of two deep learning networks based on Convolutional Neuron
Networks (CNN) and Long Short Term Memory (LSTM). Both deep learning networks
extract features from the combination of Electrooculogram (EOG),
Electroencephalogram (EEG), and Electromyogram (EMG) signals, as each signal
has distinct features that help in the classification of sleep stages. The
features produced by the two-deep learning networks are concatenated to pass to
the fully connected layer for the classification. The performance of our
proposed model is evaluated by using two public datasets Sleep-EDF Expanded
dataset and ISRUC-Sleep dataset. The accuracy and Kappa coefficient are 96.36%
and 93.40% respectively, for classifying three classes of sleep stages using
Sleep-EDF Expanded dataset. Whereas, the accuracy and Kappa coefficient are
96.57% and 83.05% respectively for five classes of sleep stages using Sleep-EDF
Expanded dataset. Our model achieves the best performance in classifying sleep
stages when compared with the state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05374">Multi-Task Learning to Enhance Generazability of Neural Network Equalizers in Coherent Optical Systems. (arXiv:2307.05374v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Srivallapanondh_S/0/1/0/all/0/1">Sasipim Srivallapanondh</a>, <a href="http://arxiv.org/find/eess/1/au:+Freire_P/0/1/0/all/0/1">Pedro J. Freire</a>, <a href="http://arxiv.org/find/eess/1/au:+Alam_A/0/1/0/all/0/1">Ashraful Alam</a>, <a href="http://arxiv.org/find/eess/1/au:+Costa_N/0/1/0/all/0/1">Nelson Costa</a>, <a href="http://arxiv.org/find/eess/1/au:+Spinnler_B/0/1/0/all/0/1">Bernhard Spinnler</a>, <a href="http://arxiv.org/find/eess/1/au:+Napoli_A/0/1/0/all/0/1">Antonio Napoli</a>, <a href="http://arxiv.org/find/eess/1/au:+Sedov_E/0/1/0/all/0/1">Egor Sedov</a>, <a href="http://arxiv.org/find/eess/1/au:+Turitsyn_S/0/1/0/all/0/1">Sergei K. Turitsyn</a>, <a href="http://arxiv.org/find/eess/1/au:+Prilepsky_J/0/1/0/all/0/1">Jaroslaw E. Prilepsky</a></p>
<p>For the first time, multi-task learning is proposed to improve the
flexibility of NN-based equalizers in coherent systems. A "single" NN-based
equalizer improves Q-factor by up to 4 dB compared to CDC, without re-training,
even with variations in launch power, symbol rate, or transmission distance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05378">M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery. (arXiv:2307.05378v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Du_Y/0/1/0/all/0/1">Yuanqi Du</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wang_Y/0/1/0/all/0/1">Yingheng Wang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Huang_Y/0/1/0/all/0/1">Yining Huang</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Li_J/0/1/0/all/0/1">Jianan Canal Li</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Zhu_Y/0/1/0/all/0/1">Yanqiao Zhu</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Xie_T/0/1/0/all/0/1">Tian Xie</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Duan_C/0/1/0/all/0/1">Chenru Duan</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gregoire_J/0/1/0/all/0/1">John M. Gregoire</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Gomes_C/0/1/0/all/0/1">Carla P. Gomes</a></p>
<p>We introduce M$^2$Hub, a toolkit for advancing machine learning in materials
discovery. Machine learning has achieved remarkable progress in modeling
molecular structures, especially biomolecules for drug discovery. However, the
development of machine learning approaches for modeling materials structures
lag behind, which is partly due to the lack of an integrated platform that
enables access to diverse tasks for materials discovery. To bridge this gap,
M$^2$Hub will enable easy access to materials discovery tasks, datasets,
machine learning methods, evaluations, and benchmark results that cover the
entire workflow. Specifically, the first release of M$^2$Hub focuses on three
key stages in materials discovery: virtual screening, inverse design, and
molecular simulation, including 9 datasets that covers 6 types of materials
with 56 tasks across 8 types of material properties. We further provide 2
synthetic datasets for the purpose of generative tasks on materials. In
addition to random data splits, we also provide 3 additional data partitions to
reflect the real-world materials discovery scenarios. State-of-the-art machine
learning methods (including those are suitable for materials structures but
never compared in the literature) are benchmarked on representative tasks. Our
codes and library are publicly available at https://github.com/yuanqidu/M2Hub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05380">Optimized Crystallographic Graph Generation for Material Science. (arXiv:2307.05380v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Klipfel_A/0/1/0/all/0/1">Astrid Klipfel</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Fregier_Y/0/1/0/all/0/1">Ya&#xeb;l Fr&#xe9;gier</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Sayede_A/0/1/0/all/0/1">Adlane Sayede</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Bouraoui_Z/0/1/0/all/0/1">Zied Bouraoui</a></p>
<p>Graph neural networks are widely used in machine learning applied to
chemistry, and in particular for material science discovery. For crystalline
materials, however, generating graph-based representation from geometrical
information for neural networks is not a trivial task. The periodicity of
crystalline needs efficient implementations to be processed in real-time under
a massively parallel environment. With the aim of training graph-based
generative models of new material discovery, we propose an efficient tool to
generate cutoff graphs and k-nearest-neighbours graphs of periodic structures
within GPU optimization. We provide pyMatGraph a Pytorch-compatible framework
to generate graphs in real-time during the training of neural network
architecture. Our tool can update a graph of a structure, making generative
models able to update the geometry and process the updated graph during the
forward propagation on the GPU side. Our code is publicly available at
https://github.com/aklipf/mat-graph.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05382">Protecting the Future: Neonatal Seizure Detection with Spatial-Temporal Modeling. (arXiv:2307.05382v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1">Ziyue Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1">Yuchen Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">You Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Ren_K/0/1/0/all/0/1">Kan Ren</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1">Yansen Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1">Xufang Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Duan_J/0/1/0/all/0/1">Juanyong Duan</a>, <a href="http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1">Congrui Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Qiu_L/0/1/0/all/0/1">Lili Qiu</a></p>
<p>A timely detection of seizures for newborn infants with electroencephalogram
(EEG) has been a common yet life-saving practice in the Neonatal Intensive Care
Unit (NICU). However, it requires great human efforts for real-time monitoring,
which calls for automated solutions to neonatal seizure detection. Moreover,
the current automated methods focusing on adult epilepsy monitoring often fail
due to (i) dynamic seizure onset location in human brains; (ii) different
montages on neonates and (iii) huge distribution shift among different
subjects. In this paper, we propose a deep learning framework, namely STATENet,
to address the exclusive challenges with exquisite designs at the temporal,
spatial and model levels. The experiments over the real-world large-scale
neonatal EEG dataset illustrate that our framework achieves significantly
better seizure detection performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05383">Human Emotion Recognition Based On Galvanic Skin Response signal Feature Selection and SVM. (arXiv:2307.05383v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Fan_D/0/1/0/all/0/1">Di Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_M/0/1/0/all/0/1">Mingyang Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohan Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Gong_X/0/1/0/all/0/1">Xiaopeng Gong</a></p>
<p>A novel human emotion recognition method based on automatically selected
Galvanic Skin Response (GSR) signal features and SVM is proposed in this paper.
GSR signals were acquired by e-Health Sensor Platform V2.0. Then, the data is
de-noised by wavelet function and normalized to get rid of the individual
difference. 30 features are extracted from the normalized data, however,
directly using of these features will lead to a low recognition rate. In order
to gain the optimized features, a covariance based feature selection is
employed in our method. Finally, a SVM with input of the optimized features is
utilized to achieve the human emotion recognition. The experimental results
indicate that the proposed method leads to good human emotion recognition, and
the recognition accuracy is more than 66.67%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05384">Stochastic Nested Compositional Bi-level Optimization for Robust Feature Learning. (arXiv:2307.05384v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Chen_X/0/1/0/all/0/1">Xuxing Chen</a>, <a href="http://arxiv.org/find/math/1/au:+Balasubramanian_K/0/1/0/all/0/1">Krishnakumar Balasubramanian</a>, <a href="http://arxiv.org/find/math/1/au:+Ghadimi_S/0/1/0/all/0/1">Saeed Ghadimi</a></p>
<p>We develop and analyze stochastic approximation algorithms for solving nested
compositional bi-level optimization problems. These problems involve a nested
composition of $T$ potentially non-convex smooth functions in the upper-level,
and a smooth and strongly convex function in the lower-level. Our proposed
algorithm does not rely on matrix inversions or mini-batches and can achieve an
$\epsilon$-stationary solution with an oracle complexity of approximately
$\tilde{O}_T(1/\epsilon^{2})$, assuming the availability of stochastic
first-order oracles for the individual functions in the composition and the
lower-level, which are unbiased and have bounded moments. Here, $\tilde{O}_T$
hides polylog factors and constants that depend on $T$. The key challenge we
address in establishing this result relates to handling three distinct sources
of bias in the stochastic gradients. The first source arises from the
compositional nature of the upper-level, the second stems from the bi-level
structure, and the third emerges due to the utilization of Neumann series
approximations to avoid matrix inversion. To demonstrate the effectiveness of
our approach, we apply it to the problem of robust feature learning for deep
neural networks under covariate shift, showcasing the benefits and advantages
of our methodology in that context.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05385">Learned Kernels for Interpretable and Efficient PPG Signal Quality Assessment and Artifact Segmentation. (arXiv:2307.05385v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chen_S/0/1/0/all/0/1">Sully F. Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Guo_Z/0/1/0/all/0/1">Zhicheng Guo</a>, <a href="http://arxiv.org/find/eess/1/au:+Ding_C/0/1/0/all/0/1">Cheng Ding</a>, <a href="http://arxiv.org/find/eess/1/au:+Hu_X/0/1/0/all/0/1">Xiao Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a></p>
<p>Photoplethysmography (PPG) provides a low-cost, non-invasive method to
continuously monitor various cardiovascular parameters. PPG signals are
generated by wearable devices and frequently contain large artifacts caused by
external factors, such as motion of the human subject. In order to ensure
robust and accurate extraction of physiological parameters, corrupted areas of
the signal need to be identified and handled appropriately. Previous
methodology relied either on handcrafted feature detectors or signal metrics
which yield sub-optimal performance, or relied on machine learning techniques
such as deep neural networks (DNN) which lack interpretability and are
computationally and memory intensive. In this work, we present a novel method
to learn a small set of interpretable convolutional kernels that has
performance similar to -- and often better than -- the state-of-the-art DNN
approach with several orders of magnitude fewer parameters. This work allows
for efficient, robust, and interpretable signal quality assessment and artifact
segmentation on low-power devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05390">CrysMMNet: Multimodal Representation for Crystal Property Prediction. (arXiv:2307.05390v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Das_K/0/1/0/all/0/1">Kishalay Das</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Goyal_P/0/1/0/all/0/1">Pawan Goyal</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Lee_S/0/1/0/all/0/1">Seung-Cheol Lee</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Bhattacharjee_S/0/1/0/all/0/1">Satadeep Bhattacharjee</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Ganguly_N/0/1/0/all/0/1">Niloy Ganguly</a></p>
<p>Machine Learning models have emerged as a powerful tool for fast and accurate
prediction of different crystalline properties. Exiting state-of-the-art models
rely on a single modality of crystal data i.e. crystal graph structure, where
they construct multi-graph by establishing edges between nearby atoms in 3D
space and apply GNN to learn materials representation. Thereby, they encode
local chemical semantics around the atoms successfully but fail to capture
important global periodic structural information like space group number,
crystal symmetry, rotational information, etc, which influence different
crystal properties. In this work, we leverage textual descriptions of materials
to model global structural information into graph structure and learn a more
robust and enriched representation of crystalline materials. To this effect, we
first curate a textual dataset for crystalline material databases containing
descriptions of each material. Further, we propose CrysMMNet, a simple
multi-modal framework, which fuses both structural and textual representation
together to generate a joint multimodal representation of crystalline
materials. We conduct extensive experiments on two benchmark datasets across
ten different properties to show that CrysMMNet outperforms existing
state-of-the-art baseline methods with a good margin. We also observe that
fusing the textual representation with crystal graph structure provides
consistent improvement for all the SOTA GNN models compared to their own
vanilla versions. We have shared the textual dataset, that we have curated for
both the benchmark material databases, with the community for future use.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05392">Simplicial Message Passing for Chemical Property Prediction. (arXiv:2307.05392v1 [cond-mat.mtrl-sci])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Lan_H/0/1/0/all/0/1">Hai Lan</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Wei_X/0/1/0/all/0/1">Xian Wei</a></p>
<p>Recently, message-passing Neural networks (MPNN) provide a promising tool for
dealing with molecular graphs and have achieved remarkable success in
facilitating the discovery and materials design with desired properties.
However, the classical MPNN methods also suffer from a limitation in capturing
the strong topological information hidden in molecular structures, such as
nonisomorphic graphs. To address this problem, this work proposes a Simplicial
Message Passing (SMP) framework to better capture the topological information
from molecules, which can break through the limitation within the vanilla
message-passing paradigm. In SMP, a generalized message-passing framework is
established for aggregating the information from arbitrary-order simplicial
complex, and a hierarchical structure is elaborated to allow information
exchange between different order simplices. We apply the SMP framework within
deep learning architectures for quantum-chemical properties prediction and
achieve state-of-the-art results. The results show that compared to traditional
MPNN, involving higher-order simplex can better capture the complex structure
of molecules and substantially enhance the performance of tasks. The SMP-based
model can provide a generalized framework for GNNs and aid in the discovery and
design of materials with tailored properties for various applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05399">Domain-Agnostic Neural Architecture for Class Incremental Continual Learning in Document Processing Platform. (arXiv:2307.05399v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1">Mateusz W&#xf3;jcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Kosciukiewicz_W/0/1/0/all/0/1">Witold Ko&#x15b;ciukiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1">Mateusz Baran</a>, <a href="http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1">Tomasz Kajdanowicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Gonczarek_A/0/1/0/all/0/1">Adam Gonczarek</a></p>
<p>Production deployments in complex systems require ML architectures to be
highly efficient and usable against multiple tasks. Particularly demanding are
classification problems in which data arrives in a streaming fashion and each
class is presented separately. Recent methods with stochastic gradient learning
have been shown to struggle in such setups or have limitations like memory
buffers, and being restricted to specific domains that disable its usage in
real-world scenarios. For this reason, we present a fully differentiable
architecture based on the Mixture of Experts model, that enables the training
of high-performance classifiers when examples from each class are presented
separately. We conducted exhaustive experiments that proved its applicability
in various domains and ability to learn online in production environments. The
proposed technique achieves SOTA results without a memory buffer and clearly
outperforms the reference methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05405">Boosting Feedback Efficiency of Interactive Reinforcement Learning by Adaptive Learning from Scores. (arXiv:2307.05405v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Shukai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1">Chenming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Ying Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Liangjun Zhang</a></p>
<p>Interactive reinforcement learning has shown promise in learning complex
robotic tasks. However, the process can be human-intensive due to the
requirement of large amount of interactive feedback. This paper presents a new
method that uses scores provided by humans, instead of pairwise preferences, to
improve the feedback efficiency of interactive reinforcement learning. Our key
insight is that scores can yield significantly more data than pairwise
preferences. Specifically, we require a teacher to interactively score the full
trajectories of an agent to train a behavioral policy in a sparse reward
environment. To avoid unstable scores given by human negatively impact the
training process, we propose an adaptive learning scheme. This enables the
learning paradigm to be insensitive to imperfect or unreliable scores. We
extensively evaluate our method on robotic locomotion and manipulation tasks.
The results show that the proposed method can efficiently learn near-optimal
policies by adaptive learning from scores, while requiring less feedback
compared to pairwise preference learning methods. The source codes are publicly
available at https://github.com/SSKKai/Interactive-Scoring-IRL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05422">Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (arXiv:2307.05422v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1">Hao Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Krishnamurthy_P/0/1/0/all/0/1">Prashanth Krishnamurthy</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1">Siddharth Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Khorrami_F/0/1/0/all/0/1">Farshad Khorrami</a></p>
<p>This paper proposes a data-efficient detection method for deep neural
networks against backdoor attacks under a black-box scenario. The proposed
approach is motivated by the intuition that features corresponding to triggers
have a higher influence in determining the backdoored network output than any
other benign features. To quantitatively measure the effects of triggers and
benign features on determining the backdoored network output, we introduce five
metrics. To calculate the five-metric values for a given input, we first
generate several synthetic samples by injecting the input's partial contents
into clean validation samples. Then, the five metrics are computed by using the
output labels of the corresponding synthetic samples. One contribution of this
work is the use of a tiny clean validation dataset. Having the computed five
metrics, five novelty detectors are trained from the validation dataset. A meta
novelty detector fuses the output of the five trained novelty detectors to
generate a meta confidence score. During online testing, our method determines
if online samples are poisoned or not via assessing their meta confidence
scores output by the meta novelty detector. We show the efficacy of our
methodology through a broad range of backdoor attacks, including ablation
studies and comparison to existing approaches. Our methodology is promising
since the proposed five metrics quantify the inherent differences between clean
and poisoned samples. Additionally, our detection method can be incrementally
improved by appending more metrics that may be proposed to address future
advanced attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05426">Using BOLD-fMRI to Compute the Respiration Volume per Time (RTV) and Respiration Variation (RV) with Convolutional Neural Networks (CNN) in the Human Connectome Development Cohort. (arXiv:2307.05426v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Addeh_A/0/1/0/all/0/1">Abdoljalil Addeh</a>, <a href="http://arxiv.org/find/eess/1/au:+Vega_F/0/1/0/all/0/1">Fernando Vega</a>, <a href="http://arxiv.org/find/eess/1/au:+Williams_R/0/1/0/all/0/1">Rebecca J Williams</a>, <a href="http://arxiv.org/find/eess/1/au:+Golestani_A/0/1/0/all/0/1">Ali Golestani</a>, <a href="http://arxiv.org/find/eess/1/au:+Pike_G/0/1/0/all/0/1">G. Bruce Pike</a>, <a href="http://arxiv.org/find/eess/1/au:+MacDonald_M/0/1/0/all/0/1">M. Ethan MacDonald</a></p>
<p>In many fMRI studies, respiratory signals are unavailable or do not have
acceptable quality. Consequently, the direct removal of low-frequency
respiratory variations from BOLD signals is not possible. This study proposes a
one-dimensional CNN model for reconstruction of two respiratory measures, RV
and RVT. Results show that a CNN can capture informative features from resting
BOLD signals and reconstruct realistic RV and RVT timeseries. It is expected
that application of the proposed method will lower the cost of fMRI studies,
reduce complexity, and decrease the burden on participants as they will not be
required to wear a respiratory bellows.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05431">Geometric Neural Diffusion Processes. (arXiv:2307.05431v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Mathieu_E/0/1/0/all/0/1">Emile Mathieu</a>, <a href="http://arxiv.org/find/stat/1/au:+Dutordoir_V/0/1/0/all/0/1">Vincent Dutordoir</a>, <a href="http://arxiv.org/find/stat/1/au:+Hutchinson_M/0/1/0/all/0/1">Michael J. Hutchinson</a>, <a href="http://arxiv.org/find/stat/1/au:+Bortoli_V/0/1/0/all/0/1">Valentin De Bortoli</a>, <a href="http://arxiv.org/find/stat/1/au:+Teh_Y/0/1/0/all/0/1">Yee Whye Teh</a>, <a href="http://arxiv.org/find/stat/1/au:+Turner_R/0/1/0/all/0/1">Richard E. Turner</a></p>
<p>Denoising diffusion models have proven to be a flexible and effective
paradigm for generative modelling. Their recent extension to infinite
dimensional Euclidean spaces has allowed for the modelling of stochastic
processes. However, many problems in the natural sciences incorporate
symmetries and involve data living in non-Euclidean spaces. In this work, we
extend the framework of diffusion models to incorporate a series of geometric
priors in infinite-dimension modelling. We do so by a) constructing a noising
process which admits, as limiting distribution, a geometric Gaussian process
that transforms under the symmetry group of interest, and b) approximating the
score with a neural network that is equivariant w.r.t. this group. We show that
with these conditions, the generative functional model admits the same
symmetry. We demonstrate scalability and capacity of the model, using a novel
Langevin-based conditional sampler, to fit complex scalar and vector fields,
with Euclidean and spherical codomain, on synthetic and real-world weather
data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05432">Self-Supervised Learning with Lie Symmetries for Partial Differential Equations. (arXiv:2307.05432v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mialon_G/0/1/0/all/0/1">Gr&#xe9;goire Mialon</a>, <a href="http://arxiv.org/find/cs/1/au:+Garrido_Q/0/1/0/all/0/1">Quentin Garrido</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawrence_H/0/1/0/all/0/1">Hannah Lawrence</a>, <a href="http://arxiv.org/find/cs/1/au:+Rehman_D/0/1/0/all/0/1">Danyal Rehman</a>, <a href="http://arxiv.org/find/cs/1/au:+LeCun_Y/0/1/0/all/0/1">Yann LeCun</a>, <a href="http://arxiv.org/find/cs/1/au:+Kiani_B/0/1/0/all/0/1">Bobak T. Kiani</a></p>
<p>Machine learning for differential equations paves the way for computationally
efficient alternatives to numerical solvers, with potentially broad impacts in
science and engineering. Though current algorithms typically require simulated
training data tailored to a given setting, one may instead wish to learn useful
information from heterogeneous sources, or from real dynamical systems
observations that are messy or incomplete. In this work, we learn
general-purpose representations of PDEs from heterogeneous data by implementing
joint embedding methods for self-supervised learning (SSL), a framework for
unsupervised representation learning that has had notable success in computer
vision. Our representation outperforms baseline approaches to invariant tasks,
such as regressing the coefficients of a PDE, while also improving the
time-stepping performance of neural solvers. We hope that our proposed
methodology will prove useful in the eventual development of general-purpose
foundation models for PDEs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05435">One-Versus-Others Attention: Scalable Multimodal Integration. (arXiv:2307.05435v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Golovanevsky_M/0/1/0/all/0/1">Michal Golovanevsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Schiller_E/0/1/0/all/0/1">Eva Schiller</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_A/0/1/0/all/0/1">Akira Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Ritambhara Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1">Carsten Eickhoff</a></p>
<p>Multimodal learning models have become increasingly important as they surpass
single-modality approaches on diverse tasks ranging from question-answering to
autonomous driving. Despite the importance of multimodal learning, existing
efforts focus on NLP applications, where the number of modalities is typically
less than four (audio, video, text, images). However, data inputs in other
domains, such as the medical field, may include X-rays, PET scans, MRIs,
genetic screening, clinical notes, and more, creating a need for both efficient
and accurate information fusion. Many state-of-the-art models rely on pairwise
cross-modal attention, which does not scale well for applications with more
than three modalities. For $n$ modalities, computing attention will result in
$n \choose 2$ operations, potentially requiring considerable amounts of
computational resources. To address this, we propose a new domain-neutral
attention mechanism, One-Versus-Others (OvO) attention, that scales linearly
with the number of modalities and requires only $n$ attention operations, thus
offering a significant reduction in computational complexity compared to
existing cross-modal attention algorithms. Using three diverse real-world
datasets as well as an additional simulation experiment, we show that our
method improves performance compared to popular fusion techniques while
decreasing computation costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05437">Improving the Security of Smartwatch Payment with Deep Learning. (arXiv:2307.05437v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Webber_G/0/1/0/all/0/1">George Webber</a></p>
<p>Making contactless payments using a smartwatch is increasingly popular, but
this payment medium lacks traditional biometric security measures such as
facial or fingerprint recognition. In 2022, Sturgess et al. proposed WatchAuth,
a system for authenticating smartwatch payments using the physical gesture of
reaching towards a payment terminal. While effective, the system requires the
user to undergo a burdensome enrolment period to achieve acceptable error
levels. In this dissertation, we explore whether applications of deep learning
can reduce the number of gestures a user must provide to enrol into an
authentication system for smartwatch payment. We firstly construct a
deep-learned authentication system that outperforms the current
state-of-the-art, including in a scenario where the target user has provided a
limited number of gestures. We then develop a regularised autoencoder model for
generating synthetic user-specific gestures. We show that using these gestures
in training improves classification ability for an authentication system.
Through this technique we can reduce the number of gestures required to enrol a
user into a WatchAuth-like system without negatively impacting its error rates.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05439">Metropolis Sampling for Constrained Diffusion Models. (arXiv:2307.05439v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fishman_N/0/1/0/all/0/1">Nic Fishman</a>, <a href="http://arxiv.org/find/cs/1/au:+Klarner_L/0/1/0/all/0/1">Leo Klarner</a>, <a href="http://arxiv.org/find/cs/1/au:+Mathieu_E/0/1/0/all/0/1">Emile Mathieu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutchinson_M/0/1/0/all/0/1">Michael Hutchinson</a>, <a href="http://arxiv.org/find/cs/1/au:+Bortoli_V/0/1/0/all/0/1">Valentin de Bortoli</a></p>
<p>Denoising diffusion models have recently emerged as the predominant paradigm
for generative modelling. Their extension to Riemannian manifolds has
facilitated their application to an array of problems in the natural sciences.
Yet, in many practical settings, such manifolds are defined by a set of
constraints and are not covered by the existing (Riemannian) diffusion model
methodology. Recent work has attempted to address this issue by employing novel
noising processes based on logarithmic barrier methods or reflected Brownian
motions. However, the associated samplers are computationally burdensome as the
complexity of the constraints increases. In this paper, we introduce an
alternative simple noising scheme based on Metropolis sampling that affords
substantial gains in computational efficiency and empirical performance
compared to the earlier samplers. Of independent interest, we prove that this
new process corresponds to a valid discretisation of the reflected Brownian
motion. We demonstrate the scalability and flexibility of our approach on a
range of problem settings with convex and non-convex constraints, including
applications from geospatial modelling, robotics and protein design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05440">ISLTranslate: Dataset for Translating Indian Sign Language. (arXiv:2307.05440v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Abhinav Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_S/0/1/0/all/0/1">Susmit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Modi_A/0/1/0/all/0/1">Ashutosh Modi</a></p>
<p>Sign languages are the primary means of communication for many
hard-of-hearing people worldwide. Recently, to bridge the communication gap
between the hard-of-hearing community and the rest of the population, several
sign language translation datasets have been proposed to enable the development
of statistical sign language translation systems. However, there is a dearth of
sign language resources for the Indian sign language. This resource paper
introduces ISLTranslate, a translation dataset for continuous Indian Sign
Language (ISL) consisting of 31k ISL-English sentence/phrase pairs. To the best
of our knowledge, it is the largest translation dataset for continuous Indian
Sign Language. We provide a detailed analysis of the dataset. To validate the
performance of existing end-to-end Sign language to spoken language translation
systems, we benchmark the created dataset with a transformer-based model for
ISL translation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05454">Empowering Cross-lingual Behavioral Testing of NLP Models with Typological Features. (arXiv:2307.05454v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hlavnova_E/0/1/0/all/0/1">Ester Hlavnova</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruder_S/0/1/0/all/0/1">Sebastian Ruder</a></p>
<p>A challenge towards developing NLP systems for the world's languages is
understanding how they generalize to typological differences relevant for
real-world applications. To this end, we propose M2C, a morphologically-aware
framework for behavioral testing of NLP models. We use M2C to generate tests
that probe models' behavior in light of specific linguistic features in 12
typologically diverse languages. We evaluate state-of-the-art language models
on the generated tests. While models excel at most tests in English, we
highlight generalization failures to specific typological characteristics such
as temporal expressions in Swahili and compounding possessives in Finish. Our
findings motivate the development of models that address these blind spots.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05469">AdaptiveRec: Adaptively Construct Pairs for Contrastive Learning in Sequential Recommendation. (arXiv:2307.05469v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1">Jaeheyoung Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1">Jung Hyun Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jewoong Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1">Myungjoo Kang</a></p>
<p>This paper presents a solution to the challenges faced by contrastive
learning in sequential recommendation systems. In particular, it addresses the
issue of false negative, which limits the effectiveness of recommendation
algorithms. By introducing an advanced approach to contrastive learning, the
proposed method improves the quality of item embeddings and mitigates the
problem of falsely categorizing similar instances as dissimilar. Experimental
results demonstrate performance enhancements compared to existing systems. The
flexibility and applicability of the proposed approach across various
recommendation scenarios further highlight its value in enhancing sequential
recommendation systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05476">Fisher-Weighted Merge of Contrastive Learning Models in Sequential Recommendation. (arXiv:2307.05476v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1">Jung Hyun Ryu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeon_J/0/1/0/all/0/1">Jaeheyoung Jeon</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1">Jewoong Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+1_M/0/1/0/all/0/1">Myungjoo Kang 1</a></p>
<p>Along with the exponential growth of online platforms and services,
recommendation systems have become essential for identifying relevant items
based on user preferences. The domain of sequential recommendation aims to
capture evolving user preferences over time. To address dynamic preference,
various contrastive learning methods have been proposed to target data
sparsity, a challenge in recommendation systems due to the limited user-item
interactions. In this paper, we are the first to apply the Fisher-Merging
method to Sequential Recommendation, addressing and resolving practical
challenges associated with it. This approach ensures robust fine-tuning by
merging the parameters of multiple models, resulting in improved overall
performance. Through extensive experiments, we demonstrate the effectiveness of
our proposed methods, highlighting their potential to advance the
state-of-the-art in sequential learning and recommendation systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1903.09132">Perturbed-History Exploration in Stochastic Linear Bandits. (arXiv:1903.09132v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a>, <a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1">Csaba Szepesvari</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1">Mohammad Ghavamzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1">Craig Boutilier</a></p>
<p>We propose a new online algorithm for cumulative regret minimization in a
stochastic linear bandit. The algorithm pulls the arm with the highest
estimated reward in a linear model trained on its perturbed history. Therefore,
we call it perturbed-history exploration in a linear bandit (LinPHE). The
perturbed history is a mixture of observed rewards and randomly generated
i.i.d. pseudo-rewards. We derive a $\tilde{O}(d \sqrt{n})$ gap-free bound on
the $n$-round regret of LinPHE, where $d$ is the number of features. The key
steps in our analysis are new concentration and anti-concentration bounds on
the weighted sum of Bernoulli random variables. To show the generality of our
design, we generalize LinPHE to a logistic model. We evaluate our algorithms
empirically and show that they are practical.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1906.08947">Randomized Exploration in Generalized Linear Bandits. (arXiv:1906.08947v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1">Branislav Kveton</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1">Manzil Zaheer</a>, <a href="http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1">Csaba Szepesvari</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lihong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1">Mohammad Ghavamzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Boutilier_C/0/1/0/all/0/1">Craig Boutilier</a></p>
<p>We study two randomized algorithms for generalized linear bandits. The first,
GLM-TSL, samples a generalized linear model (GLM) from the Laplace
approximation to the posterior distribution. The second, GLM-FPL, fits a GLM to
a randomly perturbed history of past rewards. We analyze both algorithms and
derive $\tilde{O}(d \sqrt{n \log K})$ upper bounds on their $n$-round regret,
where $d$ is the number of features and $K$ is the number of arms. The former
improves on prior work while the latter is the first for Gaussian noise
perturbations in non-linear models. We empirically evaluate both GLM-TSL and
GLM-FPL in logistic bandits, and apply GLM-FPL to neural network bandits. Our
work showcases the role of randomization, beyond posterior sampling, in
exploration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.12177">Automated Detection of Double Nuclei Galaxies using GOTHIC and the Discovery of a Large Sample of Dual AGN. (arXiv:2011.12177v4 [astro-ph.GA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Bhattacharya_A/0/1/0/all/0/1">Anwesh Bhattacharya</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+P%2E_N/0/1/0/all/0/1">Nehal C. P.</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Das_M/0/1/0/all/0/1">Mousumi Das</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Paswan_A/0/1/0/all/0/1">Abhishek Paswan</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Saha_S/0/1/0/all/0/1">Snehanshu Saha</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Combes_F/0/1/0/all/0/1">Francoise Combes</a></p>
<p>We present a novel algorithm to detect double nuclei galaxies (DNG) called
GOTHIC (Graph BOosted iterated HIll Climbing) - that detects whether a given
image of a galaxy has two or more closely separated nuclei. Our aim is to
detect samples of dual or multiple active galactic nuclei (AGN) in galaxies.
Although galaxy mergers are common, the detection of dual AGN is rare. Their
detection is very important as they help us understand the formation of
supermassive black hole (SMBH) binaries, SMBH growth and AGN feedback effects
in multiple nuclei systems. There is thus a need for an algorithm to do a
systematic survey of existing imaging data for the discovery of DNGs and dual
AGN. We have tested GOTHIC on a known sample of DNGs and subsequently applied
it to a sample of a million SDSS DR16 galaxies lying in the redshift range of 0
to 0.75 approximately, and have available spectroscopic data. We have detected
159 dual AGN in this sample, of which 2 are triple AGN systems. Our results
show that dual AGN are not common, and triple AGN even rarer. The color (u-r)
magnitude plots of the DNGs indicate that star formation is quenched as the
nuclei come closer and as the AGN fraction increases. The quenching is
especially prominent for dual/triple AGN galaxies that lie in the extreme end
of the red sequence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2105.06295">Gait Characterization in Duchenne Muscular Dystrophy (DMD) Using a Single-Sensor Accelerometer: Classical Machine Learning and Deep Learning Approaches. (arXiv:2105.06295v3 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ramli_A/0/1/0/all/0/1">Albara Ah Ramli</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Berndt_K/0/1/0/all/0/1">Kelly Berndt</a>, <a href="http://arxiv.org/find/eess/1/au:+Goude_E/0/1/0/all/0/1">Erica Goude</a>, <a href="http://arxiv.org/find/eess/1/au:+Hou_J/0/1/0/all/0/1">Jiahui Hou</a>, <a href="http://arxiv.org/find/eess/1/au:+Kaethler_L/0/1/0/all/0/1">Lynea B. Kaethler</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_R/0/1/0/all/0/1">Rex Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Lopez_A/0/1/0/all/0/1">Amanda Lopez</a>, <a href="http://arxiv.org/find/eess/1/au:+Nicorici_A/0/1/0/all/0/1">Alina Nicorici</a>, <a href="http://arxiv.org/find/eess/1/au:+Owens_C/0/1/0/all/0/1">Corey Owens</a>, <a href="http://arxiv.org/find/eess/1/au:+Rodriguez_D/0/1/0/all/0/1">David Rodriguez</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1">Jane Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1">Huanle Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Aranki_D/0/1/0/all/0/1">Daniel Aranki</a>, <a href="http://arxiv.org/find/eess/1/au:+McDonald_C/0/1/0/all/0/1">Craig M. McDonald</a>, <a href="http://arxiv.org/find/eess/1/au:+Henricson_E/0/1/0/all/0/1">Erik K. Henricson</a></p>
<p>Differences in gait patterns of children with Duchenne muscular dystrophy
(DMD) and typically-developing (TD) peers are visible to the eye, but
quantifications of those differences outside of the gait laboratory have been
elusive. In this work, we measured vertical, mediolateral, and anteroposterior
acceleration using a waist-worn iPhone accelerometer during ambulation across a
typical range of velocities. Fifteen TD and fifteen DMD children from 3-16
years of age underwent eight walking/running activities, including five 25
meters walk/run speed-calibration tests at a slow walk to running speeds (SC-L1
to SC-L5), a 6-minute walk test (6MWT), a 100 meters fast-walk/jog/run
(100MRW), and a free walk (FW). For clinical anchoring purposes, participants
completed a Northstar Ambulatory Assessment (NSAA). We extracted temporospatial
gait clinical features (CFs) and applied multiple machine learning (ML)
approaches to differentiate between DMD and TD children using extracted
temporospatial gait CFs and raw data. Extracted temporospatial gait CFs showed
reduced step length and a greater mediolateral component of total power (TP)
consistent with shorter strides and Trendelenberg-like gait commonly observed
in DMD. ML approaches using temporospatial gait CFs and raw data varied in
effectiveness at differentiating between DMD and TD controls at different
speeds, with an accuracy of up to 100%. We demonstrate that by using ML with
accelerometer data from a consumer-grade smartphone, we can capture
DMD-associated gait characteristics in toddlers to teens.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.08756">Automating Augmentation Through Random Unidimensional Search. (arXiv:2106.08756v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiaomeng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1">Michael Potter</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1">Gaurav Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yun-Chan Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1">V. Ratna Saripalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1">Theodore Trafalis</a></p>
<p>It is no secret amongst deep learning researchers that finding the optimal
data augmentation strategy during training can mean the difference between
state-of-the-art performance and a run-of-the-mill result. To that end, the
community has seen many efforts to automate the process of finding the perfect
augmentation procedure for any task at hand. Unfortunately, even recent
cutting-edge methods bring massive computational overhead, requiring as many as
100 full model trainings to settle on an ideal configuration. We show how to
achieve equivalent performance using just 6 trainings with Random
Unidimensional Augmentation. Source code is available at
https://github.com/fastestimator/RUA/tree/v1.0
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.08767">To Raise or Not To Raise: The Autonomous Learning Rate Question. (arXiv:2106.08767v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1">Xiaomeng Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1">Tao Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Potter_M/0/1/0/all/0/1">Michael Potter</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsai_Y/0/1/0/all/0/1">Yun-Chan Tsai</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_G/0/1/0/all/0/1">Gaurav Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Saripalli_V/0/1/0/all/0/1">V. Ratna Saripalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1">Theodore Trafalis</a></p>
<p>There is a parameter ubiquitous throughout the deep learning world: learning
rate. There is likewise a ubiquitous question: what should that learning rate
be? The true answer to this question is often tedious and time consuming to
obtain, and a great deal of arcane knowledge has accumulated in recent years
over how to pick and modify learning rates to achieve optimal training
performance. Moreover, the long hours spent carefully crafting the perfect
learning rate can come to nothing the moment your network architecture,
optimizer, dataset, or initial conditions change ever so slightly. But it need
not be this way. We propose a new answer to the great learning rate question:
the Autonomous Learning Rate Controller. Find it at
https://github.com/fastestimator/ARC/tree/v2.0
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.11644">Hybrid quantum-classical machine learning for generative chemistry and drug design. (arXiv:2108.11644v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Gircha_A/0/1/0/all/0/1">A.I. Gircha</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Boev_A/0/1/0/all/0/1">A.S. Boev</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Avchaciov_K/0/1/0/all/0/1">K. Avchaciov</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Fedichev_P/0/1/0/all/0/1">P.O. Fedichev</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Fedorov_A/0/1/0/all/0/1">A.K. Fedorov</a></p>
<p>Deep generative chemistry models emerge as powerful tools to expedite drug
discovery. However, the immense size and complexity of the structural space of
all possible drug-like molecules pose significant obstacles, which could be
overcome with hybrid architectures combining quantum computers with deep
classical networks. As the first step toward this goal, we built a compact
discrete variational autoencoder (DVAE) with a Restricted Boltzmann Machine
(RBM) of reduced size in its latent layer. The size of the proposed model was
small enough to fit on a state-of-the-art D-Wave quantum annealer and allowed
training on a subset of the ChEMBL dataset of biologically active compounds.
Finally, we generated 2331 novel chemical structures with medicinal chemistry
and synthetic accessibility properties in the ranges typical for molecules from
ChEMBL. The presented results demonstrate the feasibility of using already
existing or soon-to-be-available quantum computing devices as testbeds for
future drug discovery applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.08933">Responsive parallelized architecture for deploying deep learning models in production environments. (arXiv:2112.08933v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1">Nikhil Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Prasad_K/0/1/0/all/0/1">Krishna Prasad</a></p>
<p>Recruiters can easily shortlist candidates for jobs via viewing their
curriculum vitae (CV) document. Unstructured document CV beholds candidate's
portfolio and named entities listing details. The main aim of this study is to
design and propose a web oriented, highly responsive, computational pipeline
that systematically predicts CV entities using hierarchically-refined label
attention networks. Deep learning models specialized for named entity
recognition were trained on large dataset to predict relevant fields. The
article suggests an optimal strategy to use a number of deep learning models in
parallel and predict in real time. We demonstrate selection of light weight
micro web framework using Analytical Hierarchy Processing algorithm and focus
on an approach useful to deploy large deep learning model-based pipelines in
production ready environments using microservices. Deployed models and
architecture proposed helped in parsing normal CV in less than 700 milliseconds
for sequential flow of requests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.13487">The Statistical Complexity of Interactive Decision Making. (arXiv:2112.13487v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1">Dylan J. Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1">Sham M. Kakade</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1">Jian Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakhlin_A/0/1/0/all/0/1">Alexander Rakhlin</a></p>
<p>A fundamental challenge in interactive learning and decision making, ranging
from bandit problems to reinforcement learning, is to provide sample-efficient,
adaptive learning algorithms that achieve near-optimal regret. This question is
analogous to the classical problem of optimal (supervised) statistical
learning, where there are well-known complexity measures (e.g., VC dimension
and Rademacher complexity) that govern the statistical complexity of learning.
However, characterizing the statistical complexity of interactive learning is
substantially more challenging due to the adaptive nature of the problem. The
main result of this work provides a complexity measure, the Decision-Estimation
Coefficient, that is proven to be both necessary and sufficient for
sample-efficient interactive learning. In particular, we provide:
</p>
<p>1. a lower bound on the optimal regret for any interactive decision making
problem, establishing the Decision-Estimation Coefficient as a fundamental
limit.
</p>
<p>2. a unified algorithm design principle, Estimation-to-Decisions (E2D), which
transforms any algorithm for supervised estimation into an online algorithm for
decision making. E2D attains a regret bound that matches our lower bound up to
dependence on a notion of estimation performance, thereby achieving optimal
sample-efficient learning as characterized by the Decision-Estimation
Coefficient.
</p>
<p>Taken together, these results constitute a theory of learnability for
interactive decision making. When applied to reinforcement learning settings,
the Decision-Estimation Coefficient recovers essentially all existing hardness
results and lower bounds. More broadly, the approach can be viewed as a
decision-theoretic analogue of the classical Le Cam theory of statistical
estimation; it also unifies a number of existing approaches -- both Bayesian
and frequentist.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.13934">RELDEC: Reinforcement Learning-Based Decoding of Moderate Length LDPC Codes. (arXiv:2112.13934v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Habib_S/0/1/0/all/0/1">Salman Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Beemer_A/0/1/0/all/0/1">Allison Beemer</a>, <a href="http://arxiv.org/find/cs/1/au:+Kliewer_J/0/1/0/all/0/1">Joerg Kliewer</a></p>
<p>In this work we propose RELDEC, a novel approach for sequential decoding of
moderate length low-density parity-check (LDPC) codes. The main idea behind
RELDEC is that an optimized decoding policy is subsequently obtained via
reinforcement learning based on a Markov decision process (MDP). In contrast to
our previous work, where an agent learns to schedule only a single check node
(CN) within a group (cluster) of CNs per iteration, in this work we train the
agent to schedule all CNs in a cluster, and all clusters in every iteration.
That is, in each learning step of RELDEC an agent learns to schedule CN
clusters sequentially depending on a reward associated with the outcome of
scheduling a particular cluster. We also modify the state space representation
of the MDP, enabling RELDEC to be suitable for larger block length LDPC codes
than those studied in our previous work. Furthermore, to address decoding under
varying channel conditions, we propose agile meta-RELDEC (AM-RELDEC) that
employs meta-reinforcement learning. The proposed RELDEC scheme significantly
outperforms standard flooding and random sequential decoding for a variety of
LDPC codes, including codes designed for 5G new radio.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.14586">Isotuning With Applications To Scale-Free Online Learning. (arXiv:2112.14586v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Orseau_L/0/1/0/all/0/1">Laurent Orseau</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1">Marcus Hutter</a></p>
<p>We extend and combine several tools of the literature to design fast,
adaptive, anytime and scale-free online learning algorithms. Scale-free regret
bounds must scale linearly with the maximum loss, both toward large losses and
toward very small losses. Adaptive regret bounds demonstrate that an algorithm
can take advantage of easy data and potentially have constant regret. We seek
to develop fast algorithms that depend on as few parameters as possible, in
particular they should be anytime and thus not depend on the time horizon. Our
first and main tool, isotuning, is a generalization of the idea of balancing
the trade-off of the regret. We develop a set of tools to design and analyze
such learning rates easily and show that they adapts automatically to the rate
of the regret (whether constant, $O(\log T)$, $O(\sqrt{T})$, etc.) within a
factor 2 of the optimal learning rate in hindsight for the same observed
quantities. The second tool is an online correction, which allows us to obtain
centered bounds for many algorithms, to prevent the regret bounds from being
vacuous when the domain is overly large or only partially constrained. The last
tool, null updates, prevents the algorithm from performing overly large
updates, which could result in unbounded regret, or even invalid updates. We
develop a general theory using these tools and apply it to several standard
algorithms. In particular, we (almost entirely) restore the adaptivity to small
losses of FTRL for unbounded domains, design and prove scale-free adaptive
guarantees for a variant of Mirror Descent (at least when the Bregman
divergence is convex in its second argument), extend Adapt-ML-Prod to
scale-free guarantees, and provide several other minor contributions about
Prod, AdaHedge, BOA and Soft-Bayes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.13739">High Dimensional Quantum Machine Learning With Small Quantum Computers. (arXiv:2203.13739v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Marshall_S/0/1/0/all/0/1">Simon C. Marshall</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gyurik_C/0/1/0/all/0/1">Casper Gyurik</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1">Vedran Dunjko</a></p>
<p>Quantum computers hold great promise to enhance machine learning, but their
current qubit counts restrict the realisation of this promise. In an attempt to
placate this limitation techniques can be applied for evaluating a quantum
circuit using a machine with fewer qubits than the circuit naively requires.
These techniques work by evaluating many smaller circuits on the smaller
machine, that are then combined in a polynomial to replicate the output of the
larger machine. This scheme requires more circuit evaluations than are
practical for general circuits. However, we investigate the possibility that
for certain applications many of these subcircuits are superfluous, and that a
much smaller sum is sufficient to estimate the full circuit. We construct a
machine learning model that may be capable of approximating the outputs of the
larger circuit with much fewer circuit evaluations. We successfully apply our
model to the task of digit recognition, using simulated quantum computers much
smaller than the data dimension. The model is also applied to the task of
approximating a random 10 qubit PQC with simulated access to a 5 qubit
computer, even with only relatively modest number of circuits our model
provides an accurate approximation of the 10 qubit PQCs output, superior to a
neural network attempt. The developed method might be useful for implementing
quantum models on larger data throughout the NISQ era.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.06960">Forming Trees with Treeformers. (arXiv:2207.06960v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1">Nilay Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1">Jeffrey Flanigan</a></p>
<p>Human language is known to exhibit a nested, hierarchical structure, allowing
us to form complex sentences out of smaller pieces. However, many
state-of-the-art neural networks models such as Transformers have no explicit
hierarchical structure in its architecture -- that is, they don't have an
inductive bias toward hierarchical structure. Additionally, Transformers are
known to perform poorly on compositional generalization tasks which require
such structures. In this paper, we introduce Treeformer, a general-purpose
encoder module inspired by the CKY algorithm which learns a composition
operator and pooling function to construct hierarchical encodings for phrases
and sentences. Our extensive experiments demonstrate the benefits of
incorporating hierarchical structure into the Transformer and show significant
improvements in compositional generalization as well as in downstream tasks
such as machine translation, abstractive summarization, and various natural
language understanding tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.05379">Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1">Asha Rani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1">Pankaj Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_Y/0/1/0/all/0/1">Yashaswi Verma</a></p>
<p>Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological
disorder. Its main symptoms include difficulty in (verbal and/or non-verbal)
communication, and rigid/repetitive behavior. These symptoms are often
indistinguishable from a normal (control) individual, due to which this
disorder remains undiagnosed in early childhood leading to delayed treatment.
Since the learning curve is steep during the initial age, an early diagnosis of
autism could allow to take adequate interventions at the right time, which
might positively affect the growth of an autistic child. Further, the
traditional methods of autism diagnosis require multiple visits to a
specialized psychiatrist, however this process can be time-consuming. In this
paper, we present a learning based approach to automate autism diagnosis using
simple and small action video clips of subjects. This task is particularly
challenging because the amount of annotated data available is small, and the
variations among samples from the two categories (ASD and control) are
generally indistinguishable. This is also evident from poor performance of a
binary classifier learned using the cross-entropy loss on top of a baseline
encoder. To address this, we adopt contrastive feature learning in both self
supervised and supervised learning frameworks, and show that these can lead to
a significant increase in the prediction accuracy of a binary classifier on
this task. We further validate this by conducting thorough experimental
analyses under different set-ups on two publicly available datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.08004">Robust Inference of Manifold Density and Geometry by Doubly Stochastic Scaling. (arXiv:2209.08004v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Landa_B/0/1/0/all/0/1">Boris Landa</a>, <a href="http://arxiv.org/find/math/1/au:+Cheng_X/0/1/0/all/0/1">Xiuyuan Cheng</a></p>
<p>The Gaussian kernel and its traditional normalizations (e.g., row-stochastic)
are popular approaches for assessing similarities between data points. Yet,
they can be inaccurate under high-dimensional noise, especially if the noise
magnitude varies considerably across the data, e.g., under heteroskedasticity
or outliers. In this work, we investigate a more robust alternative -- the
doubly stochastic normalization of the Gaussian kernel. We consider a setting
where points are sampled from an unknown density on a low-dimensional manifold
embedded in high-dimensional space and corrupted by possibly strong,
non-identically distributed, sub-Gaussian noise. We establish that the doubly
stochastic affinity matrix and its scaling factors concentrate around certain
population forms, and provide corresponding finite-sample probabilistic error
bounds. We then utilize these results to develop several tools for robust
inference under general high-dimensional noise. First, we derive a robust
density estimator that reliably infers the underlying sampling density and can
substantially outperform the standard kernel density estimator under
heteroskedasticity and outliers. Second, we obtain estimators for the pointwise
noise magnitudes, the pointwise signal magnitudes, and the pairwise Euclidean
distances between clean data points. Lastly, we derive robust graph Laplacian
normalizations that accurately approximate various manifold Laplacians,
including the Laplace Beltrami operator, improving over traditional
normalizations in noisy settings. We exemplify our results in simulations and
on real single-cell RNA-sequencing data. For the latter, we show that in
contrast to traditional methods, our approach is robust to variability in
technical noise levels across cell types.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.10200">Performance Optimization for Variable Bitwidth Federated Learning in Wireless Networks. (arXiv:2209.10200v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sihua Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingzhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1">Christopher G. Brinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1">Changchuan Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1">Walid Saad</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a></p>
<p>This paper considers improving wireless communication and computation
efficiency in federated learning (FL) via model quantization. In the proposed
bitwidth FL scheme, edge devices train and transmit quantized versions of their
local FL model parameters to a coordinating server, which aggregates them into
a quantized global model and synchronizes the devices. The goal is to jointly
determine the bitwidths employed for local FL model quantization and the set of
devices participating in FL training at each iteration. We pose this as an
optimization problem that aims to minimize the training loss of quantized FL
under a per-iteration device sampling budget and delay requirement. However,
the formulated problem is difficult to solve without (i) a concrete
understanding of how quantization impacts global ML performance and (ii) the
ability of the server to construct estimates of this process efficiently. To
address the first challenge, we analytically characterize how limited wireless
resources and induced quantization errors affect the performance of the
proposed FL method. Our results quantify how the improvement of FL training
loss between two consecutive iterations depends on the device selection and
quantization scheme as well as on several parameters inherent to the model
being learned. Then, we show that the FL training process can be described as a
Markov decision process and propose a model-based reinforcement learning (RL)
method to optimize action selection over iterations. Compared to model-free RL,
this model-based RL approach leverages the derived mathematical
characterization of the FL training process to discover an effective device
selection and quantization scheme without imposing additional device
communication overhead. Simulation results show that the proposed FL algorithm
can reduce the convergence time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.04318">Prediction intervals for neural network models using weighted asymmetric loss functions. (arXiv:2210.04318v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Grillo_M/0/1/0/all/0/1">Milo Grillo</a>, <a href="http://arxiv.org/find/stat/1/au:+Han_Y/0/1/0/all/0/1">Yunpeng Han</a>, <a href="http://arxiv.org/find/stat/1/au:+Werpachowska_A/0/1/0/all/0/1">Agnieszka Werpachowska</a></p>
<p>We propose a simple and efficient approach to generate a prediction intervals
(PI) for approximated and forecasted trends. Our method leverages a weighted
asymmetric loss function to estimate the lower and upper bounds of the PI, with
the weights determined by its coverage probability. We provide a concise
mathematical proof of the method, show how it can be extended to derive PIs for
parametrised functions and argue why the method works for predicting PIs of
dependent variables. The presented tests of the method on a real-world
forecasting task using a neural network-based model show that it can produce
reliable PIs in complex machine learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.06959">A Survey on Explainable Anomaly Detection. (arXiv:2210.06959v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yuxuan Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Leeuwen_M/0/1/0/all/0/1">Matthijs van Leeuwen</a></p>
<p>In the past two decades, most research on anomaly detection has focused on
improving the accuracy of the detection, while largely ignoring the
explainability of the corresponding methods and thus leaving the explanation of
outcomes to practitioners. As anomaly detection algorithms are increasingly
used in safety-critical domains, providing explanations for the high-stakes
decisions made in those domains has become an ethical and regulatory
requirement. Therefore, this work provides a comprehensive and structured
survey on state-of-the-art explainable anomaly detection techniques. We propose
a taxonomy based on the main aspects that characterize each explainable anomaly
detection technique, aiming to help practitioners and researchers find the
explainable anomaly detection method that best suits their needs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08413">Decentralized Federated Learning: Fundamentals, State of the Art, Frameworks, Trends, and Challenges. (arXiv:2211.08413v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Beltran_E/0/1/0/all/0/1">Enrique Tom&#xe1;s Mart&#xed;nez Beltr&#xe1;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1">Mario Quiles P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1">Pedro Miguel S&#xe1;nchez S&#xe1;nchez</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernal_S/0/1/0/all/0/1">Sergio L&#xf3;pez Bernal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1">G&#xe9;r&#xf4;me Bovet</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_M/0/1/0/all/0/1">Manuel Gil P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Perez_G/0/1/0/all/0/1">Gregorio Mart&#xed;nez P&#xe9;rez</a>, <a href="http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1">Alberto Huertas Celdr&#xe1;n</a></p>
<p>In the last decade, Federated Learning (FL) has gained relevance in training
collaborative models without sharing sensitive data. Since its birth,
Centralized FL (CFL) has been the most common approach in the literature, where
a central entity creates a global model. However, a centralized approach leads
to increased latency due to bottlenecks, heightened vulnerability to system
failures, and trustworthiness concerns affecting the entity responsible for the
global model creation. Decentralized Federated Learning (DFL) emerged to
address these concerns by promoting decentralized model aggregation and
minimizing reliance on centralized architectures. However, despite the work
done in DFL, the literature has not (i) studied the main aspects
differentiating DFL and CFL; (ii) analyzed DFL frameworks to create and
evaluate new solutions; and (iii) reviewed application scenarios using DFL.
Thus, this article identifies and analyzes the main fundamentals of DFL in
terms of federation architectures, topologies, communication mechanisms,
security approaches, and key performance indicators. Additionally, the paper at
hand explores existing mechanisms to optimize critical DFL fundamentals. Then,
the most relevant features of the current DFL frameworks are reviewed and
compared. After that, it analyzes the most used DFL application scenarios,
identifying solutions based on the fundamentals and frameworks previously
defined. Finally, the evolution of existing DFL solutions is studied to provide
a list of trends, lessons learned, and open challenges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.11030">Adversarial Cheap Talk. (arXiv:2211.11030v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1">Chris Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1">Timon Willi</a>, <a href="http://arxiv.org/find/cs/1/au:+Letcher_A/0/1/0/all/0/1">Alistair Letcher</a>, <a href="http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1">Jakob Foerster</a></p>
<p>Adversarial attacks in reinforcement learning (RL) often assume
highly-privileged access to the victim's parameters, environment, or data.
Instead, this paper proposes a novel adversarial setting called a Cheap Talk
MDP in which an Adversary can merely append deterministic messages to the
Victim's observation, resulting in a minimal range of influence. The Adversary
cannot occlude ground truth, influence underlying environment dynamics or
reward signals, introduce non-stationarity, add stochasticity, see the Victim's
actions, or access their parameters. Additionally, we present a simple
meta-learning algorithm called Adversarial Cheap Talk (ACT) to train
Adversaries in this setting. We demonstrate that an Adversary trained with ACT
still significantly influences the Victim's training and testing performance,
despite the highly constrained setting. Affecting train-time performance
reveals a new attack vector and provides insight into the success and failure
modes of existing RL algorithms. More specifically, we show that an ACT
Adversary is capable of harming performance by interfering with the learner's
function approximation, or instead helping the Victim's performance by
outputting useful features. Finally, we show that an ACT Adversary can
manipulate messages during train-time to directly and arbitrarily control the
Victim at test-time. Project video and code are available at
https://sites.google.com/view/adversarial-cheap-talk
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01977">Distributed Pruning Towards Tiny Neural Networks in Federated Learning. (arXiv:2212.01977v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Hong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chaoyue Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_R/0/1/0/all/0/1">Ruogu Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1">Xiaoyong Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1">Dapeng Wu</a></p>
<p>Neural network pruning is an essential technique for reducing the size and
complexity of deep neural networks, enabling large-scale models on devices with
limited resources. However, existing pruning approaches heavily rely on
training data for guiding the pruning strategies, making them ineffective for
federated learning over distributed and confidential datasets. Additionally,
the memory- and computation-intensive pruning process becomes infeasible for
recourse-constrained devices in federated learning. To address these
challenges, we propose FedTiny, a distributed pruning framework for federated
learning that generates specialized tiny models for memory- and
computing-constrained devices. We introduce two key modules in FedTiny to
adaptively search coarse- and finer-pruned specialized models to fit deployment
scenarios with sparse and cheap local computation. First, an adaptive batch
normalization selection module is designed to mitigate biases in pruning caused
by the heterogeneity of local data. Second, a lightweight progressive pruning
module aims to finer prune the models under strict memory and computational
budgets, allowing the pruning policy for each layer to be gradually determined
rather than evaluating the overall model structure. The experimental results
demonstrate the effectiveness of FedTiny, which outperforms state-of-the-art
approaches, particularly when compressing deep models to extremely sparse tiny
models. FedTiny achieves an accuracy improvement of 2.61% while significantly
reducing the computational cost by 95.91% and the memory footprint by 94.01%
compared to state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.07040">Optimal Algorithms for Latent Bandits with Cluster Structure. (arXiv:2301.07040v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pal_S/0/1/0/all/0/1">Soumyabrata Pal</a>, <a href="http://arxiv.org/find/cs/1/au:+Suggala_A/0/1/0/all/0/1">Arun Sai Suggala</a>, <a href="http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1">Karthikeyan Shanmugam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_P/0/1/0/all/0/1">Prateek Jain</a></p>
<p>We consider the problem of latent bandits with cluster structure where there
are multiple users, each with an associated multi-armed bandit problem. These
users are grouped into \emph{latent} clusters such that the mean reward vectors
of users within the same cluster are identical. At each round, a user, selected
uniformly at random, pulls an arm and observes a corresponding noisy reward.
The goal of the users is to maximize their cumulative rewards. This problem is
central to practical recommendation systems and has received wide attention of
late \cite{gentile2014online, maillard2014latent}. Now, if each user acts
independently, then they would have to explore each arm independently and a
regret of $\Omega(\sqrt{\mathsf{MNT}})$ is unavoidable, where $\mathsf{M},
\mathsf{N}$ are the number of arms and users, respectively. Instead, we propose
LATTICE (Latent bAndiTs via maTrIx ComplEtion) which allows exploitation of the
latent cluster structure to provide the minimax optimal regret of
$\widetilde{O}(\sqrt{(\mathsf{M}+\mathsf{N})\mathsf{T}})$, when the number of
clusters is $\widetilde{O}(1)$. This is the first algorithm to guarantee such
strong regret bound. LATTICE is based on a careful exploitation of arm
information within a cluster while simultaneously clustering users.
Furthermore, it is computationally efficient and requires only
$O(\log{\mathsf{T}})$ calls to an offline matrix completion oracle across all
$\mathsf{T}$ rounds.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08618">Solving PDEs with Unmeasurable Source Terms Using Coupled Physics-Informed Neural Network with Recurrent Prediction for Soft Sensors. (arXiv:2301.08618v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1">Aina Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_P/0/1/0/all/0/1">Pan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xi-Ming Sun</a></p>
<p>Partial differential equations (PDEs) are a model candidate for soft sensors
in industrial processes with spatiotemporal dependence. Although
physics-informed neural networks (PINNs) are a promising machine learning
method for solving PDEs, they are infeasible for the nonhomogeneous PDEs with
unmeasurable source terms. To this end, a coupled PINN (CPINN) with a recurrent
prediction (RP) learning strategy (CPINN- RP) is proposed. First, CPINN
composed of NetU and NetG is proposed. NetU is for approximating PDEs solutions
and NetG is for regularizing the training of NetU. The two networks are
integrated into a data-physics-hybrid loss function. Then, we theoretically
prove that the proposed CPINN has a satisfying approximation capability for
solutions to nonhomogeneous PDEs with unmeasurable source terms. Besides the
theoretical aspects, we propose a hierarchical training strategy to optimize
and couple NetU and NetG. Secondly, NetU-RP is proposed for compensating
information loss in data sampling to improve the prediction performance, in
which RP is the recurrently delayed outputs of well-trained CPINN and hard
sensors. Finally, the artificial and practical datasets are used to verify the
feasibility and effectiveness of CPINN-RP for soft sensors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10343">ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1">Johannes Brandstetter</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1">Ashish Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1">Jayesh K. Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a></p>
<p>Most state-of-the-art approaches for weather and climate modeling are based
on physics-informed numerical models of the atmosphere. These approaches aim to
model the non-linear dynamics and complex interactions between multiple
variables, which are challenging to approximate. Additionally, many such
numerical models are computationally intensive, especially when modeling the
atmospheric phenomenon at a fine-grained spatial and temporal resolution.
Recent data-driven approaches based on machine learning instead aim to directly
solve a downstream forecasting or projection task by learning a data-driven
functional mapping using deep neural networks. However, these networks are
trained using curated and homogeneous climate datasets for specific
spatiotemporal tasks, and thus lack the generality of numerical models. We
develop and demonstrate ClimaX, a flexible and generalizable deep learning
model for weather and climate science that can be trained using heterogeneous
datasets spanning different variables, spatio-temporal coverage, and physical
groundings. ClimaX extends the Transformer architecture with novel encoding and
aggregation blocks that allow effective use of available compute while
maintaining general utility. ClimaX is pre-trained with a self-supervised
learning objective on climate datasets derived from CMIP6. The pre-trained
ClimaX can then be fine-tuned to address a breadth of climate and weather
tasks, including those that involve atmospheric variables and spatio-temporal
scales unseen during pretraining. Compared to existing data-driven baselines,
we show that this generality in ClimaX results in superior performance on
benchmarks for weather forecasting and climate projections, even when
pretrained at lower resolutions and compute budgets. The source code is
available at https://github.com/microsoft/ClimaX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12313">Adapting Neural Link Predictors for Data-Efficient Complex Query Answering. (arXiv:2301.12313v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arakelyan_E/0/1/0/all/0/1">Erik Arakelyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Daza_D/0/1/0/all/0/1">Daniel Daza</a>, <a href="http://arxiv.org/find/cs/1/au:+Cochez_M/0/1/0/all/0/1">Michael Cochez</a>, <a href="http://arxiv.org/find/cs/1/au:+Augenstein_I/0/1/0/all/0/1">Isabelle Augenstein</a></p>
<p>Answering complex queries on incomplete knowledge graphs is a challenging
task where a model needs to answer complex logical queries in the presence of
missing knowledge. Prior work in the literature has proposed to address this
problem by designing architectures trained end-to-end for the complex query
answering task with a reasoning process that is hard to interpret while
requiring data and resource-intensive training. Other lines of research have
proposed re-using simple neural link predictors to answer complex queries,
reducing the amount of training data by orders of magnitude while providing
interpretable answers. The neural link predictor used in such approaches is not
explicitly optimised for the complex query answering task, implying that its
scores are not calibrated to interact together. We propose to address these
problems via CQD$^{\mathcal{A}}$, a parameter-efficient score \emph{adaptation}
model optimised to re-calibrate neural link prediction scores for the complex
query answering task. While the neural link predictor is frozen, the adaptation
component -- which only increases the number of model parameters by $0.03\%$ --
is trained on the downstream complex query answering task. Furthermore, the
calibration component enables us to support reasoning over queries that include
atomic negations, which was previously impossible with link predictors. In our
experiments, CQD$^{\mathcal{A}}$ produces significantly more accurate results
than current state-of-the-art methods, improving from $34.4$ to $35.1$ Mean
Reciprocal Rank values averaged across all datasets and query types while using
$\leq 30\%$ of the available training query types. We further show that
CQD$^{\mathcal{A}}$ is data-efficient, achieving competitive results with only
$1\%$ of the training complex queries, and robust in out-of-domain evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12636">Exploring Image Augmentations for Siamese Representation Learning with Chest X-Rays. (arXiv:2301.12636v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Sluijs_R/0/1/0/all/0/1">Rogier van der Sluijs</a>, <a href="http://arxiv.org/find/eess/1/au:+Bhaskhar_N/0/1/0/all/0/1">Nandita Bhaskhar</a>, <a href="http://arxiv.org/find/eess/1/au:+Rubin_D/0/1/0/all/0/1">Daniel Rubin</a>, <a href="http://arxiv.org/find/eess/1/au:+Langlotz_C/0/1/0/all/0/1">Curtis Langlotz</a>, <a href="http://arxiv.org/find/eess/1/au:+Chaudhari_A/0/1/0/all/0/1">Akshay Chaudhari</a></p>
<p>Image augmentations are quintessential for effective visual representation
learning across self-supervised learning techniques. While augmentation
strategies for natural imaging have been studied extensively, medical images
are vastly different from their natural counterparts. Thus, it is unknown
whether common augmentation strategies employed in Siamese representation
learning generalize to medical images and to what extent. To address this
challenge, in this study, we systematically assess the effect of various
augmentations on the quality and robustness of the learned representations. We
train and evaluate Siamese Networks for abnormality detection on chest X-Rays
across three large datasets (MIMIC-CXR, CheXpert and VinDR-CXR). We investigate
the efficacy of the learned representations through experiments involving
linear probing, fine-tuning, zero-shot transfer, and data efficiency. Finally,
we identify a set of augmentations that yield robust representations that
generalize well to both out-of-distribution data and diseases, while
outperforming supervised baselines using just zero-shot transfer and linear
probes by up to 20%. Our code is available at
https://github.com/StanfordMIMI/siaug.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00390">Hierarchical Classification of Research Fields in the &quot;Web of Science&quot; Using Deep Learning. (arXiv:2302.00390v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rao_S/0/1/0/all/0/1">Susie Xi Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Egger_P/0/1/0/all/0/1">Peter H. Egger</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a></p>
<p>This paper presents a hierarchical classification system that automatically
categorizes a scholarly publication using its abstract into a three-tier
hierarchical label set (discipline, field, subfield) in a multi-class setting.
This system enables a holistic categorization of research activities in the
mentioned hierarchy in terms of knowledge production through articles and
impact through citations, permitting those activities to fall into multiple
categories. The classification system distinguishes 44 disciplines, 718 fields
and 1,485 subfields among 160 million abstract snippets in Microsoft Academic
Graph (version 2018-05-17). We used batch training in a modularized and
distributed fashion to address and allow for interdisciplinary and interfield
classifications in single-label and multi-label settings. In total, we have
conducted 3,140 experiments in all considered models (Convolutional Neural
Networks, Recurrent Neural Networks, Transformers). The classification accuracy
is &gt; 90% in 77.13% and 78.19% of the single-label and multi-label
classifications, respectively. We examine the advantages of our classification
by its ability to better align research texts and output with disciplines, to
adequately classify them in an automated way, and to capture the degree of
interdisciplinarity. The proposed system (a set of pre-trained models) can
serve as a backbone to an interactive system for indexing scientific
publications in the future.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.06170">Restoring the saturation response of a PMT using pulse-shape and artificial-neural-networks. (arXiv:2302.06170v3 [physics.ins-det] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Lee_H/0/1/0/all/0/1">Hyun-Gi Lee</a>, <a href="http://arxiv.org/find/physics/1/au:+Park_J/0/1/0/all/0/1">Jungsic Park</a></p>
<p>The linear response of a photomultiplier tube (PMT) is a required property
for photon counting and reconstruction of the neutrino energy. The linearity
valid region and the saturation response of PMT were investigated using a
linear-alkyl-benzene (LAB)-based liquid scintillator. A correlation was
observed between the two different saturation responses, with pulse-shape
distortion and pulse-area decrease. The observed pulse-shape provides useful
information for the estimation of the linearity region relative to the
pulse-area. This correlation-based diagnosis allows an ${in}$-${situ}$
estimation of the linearity range, which was previously challenging. The
measured correlation between the two saturation responses was employed to train
an artificial-neural-network (ANN) to predict the decrease in pulse-area from
the observed pulse-shape. The ANN-predicted pulse-area decrease enables the
prediction of the ideal number of photoelectrons irrelevant to the saturation
behavior. This pulse-shape-based machine learning technique offers a novel
method for restoring the saturation response of PMTs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13153">Directed Diffusion: Direct Control of Object Placement through Attention Guidance. (arXiv:2302.13153v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wan-Duo Kurt Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1">J.P. Lewis</a>, <a href="http://arxiv.org/find/cs/1/au:+Lahiri_A/0/1/0/all/0/1">Avisek Lahiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Leung_T/0/1/0/all/0/1">Thomas Leung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1">W. Bastiaan Kleijn</a></p>
<p>Text-guided diffusion models such as DALLE-2, Imagen, and Stable Diffusion
are able to generate an effectively endless variety of images given only a
short text prompt describing the desired image content. In many cases the
images are of very high quality. However, these models often struggle to
compose scenes containing several key objects such as characters in specified
positional relationships. The missing capability to "direct" the placement of
characters and objects both within and across images is crucial in
storytelling, as recognized in the literature on film and animation theory. In
this work, we take a particularly straightforward approach to providing the
needed direction. Drawing on the observation that the cross-attention maps for
prompt words reflect the spatial layout of objects denoted by those words, we
introduce an optimization objective that produces ``activation'' at desired
positions in these cross-attention maps. The resulting approach is a step
toward generalizing the applicability of text-guided diffusion models beyond
single images to collections of related images, as in storybooks. To the best
of our knowledge, our Directed Diffusion method is the first diffusion
technique that provides positional control over multiple objects, while making
use of an existing pre-trained model and maintaining a coherent blend between
the positioned objects and the background. Moreover, it requires only a few
lines to implement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00890">Comparison of High-Dimensional Bayesian Optimization Algorithms on BBOB. (arXiv:2303.00890v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santoni_M/0/1/0/all/0/1">Maria Laura Santoni</a>, <a href="http://arxiv.org/find/cs/1/au:+Raponi_E/0/1/0/all/0/1">Elena Raponi</a>, <a href="http://arxiv.org/find/cs/1/au:+Leone_R/0/1/0/all/0/1">Renato De Leone</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerr_C/0/1/0/all/0/1">Carola Doerr</a></p>
<p>Bayesian Optimization (BO) is a class of black-box, surrogate-based
heuristics that can efficiently optimize problems that are expensive to
evaluate, and hence admit only small evaluation budgets. BO is particularly
popular for solving numerical optimization problems in industry, where the
evaluation of objective functions often relies on time-consuming simulations or
physical experiments. However, many industrial problems depend on a large
number of parameters. This poses a challenge for BO algorithms, whose
performance is often reported to suffer when the dimension grows beyond 15
variables. Although many new algorithms have been proposed to address this
problem, it is not well understood which one is the best for which optimization
scenario.
</p>
<p>In this work, we compare five state-of-the-art high-dimensional BO
algorithms, with vanilla BO and CMA-ES on the 24 BBOB functions of the COCO
environment at increasing dimensionality, ranging from 10 to 60 variables. Our
results confirm the superiority of BO over CMA-ES for limited evaluation
budgets and suggest that the most promising approach to improve BO is the use
of trust regions. However, we also observe significant performance differences
for different function landscapes and budget exploitation phases, indicating
improvement potential, e.g., through hybridization of algorithmic components.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.02011">Diagnosing Model Performance Under Distribution Shift. (arXiv:2303.02011v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1">Tiffany Tianhui Cai</a>, <a href="http://arxiv.org/find/stat/1/au:+Namkoong_H/0/1/0/all/0/1">Hongseok Namkoong</a>, <a href="http://arxiv.org/find/stat/1/au:+Yadlowsky_S/0/1/0/all/0/1">Steve Yadlowsky</a></p>
<p>Prediction models can perform poorly when deployed to target distributions
different from the training distribution. To understand these operational
failure modes, we develop a method, called DIstribution Shift DEcomposition
(DISDE), to attribute a drop in performance to different types of distribution
shifts. Our approach decomposes the performance drop into terms for 1) an
increase in harder but frequently seen examples from training, 2) changes in
the relationship between features and outcomes, and 3) poor performance on
examples infrequent or unseen during training. These terms are defined by
fixing a distribution on $X$ while varying the conditional distribution of $Y
\mid X$ between training and target, or by fixing the conditional distribution
of $Y \mid X$ while varying the distribution on $X$. In order to do this, we
define a hypothetical distribution on $X$ consisting of values common in both
training and target, over which it is easy to compare $Y \mid X$ and thus
predictive performance. We estimate performance on this hypothetical
distribution via reweighting methods. Empirically, we show how our method can
1) inform potential modeling improvements across distribution shifts for
employment prediction on tabular census data, and 2) help to explain why
certain domain adaptation methods fail to improve model performance for
satellite image classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02168">I2I: Initializing Adapters with Improvised Knowledge. (arXiv:2304.02168v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_T/0/1/0/all/0/1">Tejas Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_F/0/1/0/all/0/1">Furong Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostami_M/0/1/0/all/0/1">Mohammad Rostami</a>, <a href="http://arxiv.org/find/cs/1/au:+Thomason_J/0/1/0/all/0/1">Jesse Thomason</a></p>
<p>Adapters present a promising solution to the catastrophic forgetting problem
in continual learning. However, training independent Adapter modules for every
new task misses an opportunity for cross-task knowledge transfer. We propose
Improvise to Initialize (I2I), a continual learning algorithm that initializes
Adapters for incoming tasks by distilling knowledge from previously-learned
tasks' Adapters. We evaluate I2I on CLiMB, a multimodal continual learning
benchmark, by conducting experiments on sequences of visual question answering
tasks. Adapters trained with I2I consistently achieve better task accuracy than
independently-trained Adapters, demonstrating that our algorithm facilitates
knowledge transfer between task Adapters. I2I also results in better cross-task
knowledge transfer than the state-of-the-art AdapterFusion without incurring
the associated parametric cost.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02497">Hyper-parameter Tuning for Adversarially Robust Models. (arXiv:2304.02497v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mendes_P/0/1/0/all/0/1">Pedro Mendes</a>, <a href="http://arxiv.org/find/cs/1/au:+Romano_P/0/1/0/all/0/1">Paolo Romano</a>, <a href="http://arxiv.org/find/cs/1/au:+Garlan_D/0/1/0/all/0/1">David Garlan</a></p>
<p>This work focuses on the problem of hyper-parameter tuning (HPT) for robust
(i.e., adversarially trained) models, shedding light on the new challenges and
opportunities arising during the HPT process for robust models. To this end, we
conduct an extensive experimental study based on 3 popular deep models, in
which we explore exhaustively 9 (discretized) HPs, 2 fidelity dimensions, and 2
attack bounds, for a total of 19208 configurations (corresponding to 50
thousand GPU hours). Through this study, we show that the complexity of the HPT
problem is further exacerbated in adversarial settings due to the need to
independently tune the HPs used during standard and adversarial training:
succeeding in doing so (i.e., adopting different HP settings in both phases)
can lead to a reduction of up to 80% and 43% of the error for clean and
adversarial inputs, respectively. On the other hand, we also identify new
opportunities to reduce the cost of HPT for robust models. Specifically, we
propose to leverage cheap adversarial training methods to obtain inexpensive,
yet highly correlated, estimations of the quality achievable using
state-of-the-art methods. We show that, by exploiting this novel idea in
conjunction with a recent multi-fidelity optimizer (taKG), the efficiency of
the HPT process can be enhanced by up to 2.1x.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03017">Improving Code Example Recommendations on Informal Documentation Using BERT and Query-Aware LSH: A Comparative Study. (arXiv:2305.03017v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rahmani_S/0/1/0/all/0/1">Sajjad Rahmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Naghshzan_A/0/1/0/all/0/1">AmirHossein Naghshzan</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerrouj_L/0/1/0/all/0/1">Latifa Guerrouj</a></p>
<p>Our research investigates the recommendation of code examples to aid software
developers, a practice that saves developers significant time by providing
ready-to-use code snippets. The focus of our study is Stack Overflow, a
commonly used resource for coding discussions and solutions, particularly in
the context of the Java programming language.
</p>
<p>We applied BERT, a powerful Large Language Model (LLM) that enables us to
transform code examples into numerical vectors by extracting their semantic
information. Once these numerical representations are prepared, we identify
Approximate Nearest Neighbors (ANN) using Locality-Sensitive Hashing (LSH). Our
research employed two variants of LSH: Random Hyperplane-based LSH and
Query-Aware LSH. We rigorously compared these two approaches across four
parameters: HitRate, Mean Reciprocal Rank (MRR), Average Execution Time, and
Relevance.
</p>
<p>Our study revealed that the Query-Aware (QA) approach showed superior
performance over the Random Hyperplane-based (RH) method. Specifically, it
exhibited a notable improvement of 20% to 35% in HitRate for query pairs
compared to the RH approach. Furthermore, the QA approach proved significantly
more time-efficient, with its speed in creating hashing tables and assigning
data samples to buckets being at least four times faster. It can return code
examples within milliseconds, whereas the RH approach typically requires
several seconds to recommend code examples. Due to the superior performance of
the QA approach, we tested it against PostFinder and FaCoY, the
state-of-the-art baselines. Our QA method showed comparable efficiency proving
its potential for effective code recommendation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03829">Improving Image-Based Precision Medicine with Uncertainty-Aware Causal Models. (arXiv:2305.03829v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Durso_Finley_J/0/1/0/all/0/1">Joshua Durso-Finley</a>, <a href="http://arxiv.org/find/cs/1/au:+Falet_J/0/1/0/all/0/1">Jean-Pierre Falet</a>, <a href="http://arxiv.org/find/cs/1/au:+Mehta_R/0/1/0/all/0/1">Raghav Mehta</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnold_D/0/1/0/all/0/1">Douglas L. Arnold</a>, <a href="http://arxiv.org/find/cs/1/au:+Pawlowski_N/0/1/0/all/0/1">Nick Pawlowski</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbel_T/0/1/0/all/0/1">Tal Arbel</a></p>
<p>Image-based precision medicine aims to personalize treatment decisions based
on an individual's unique imaging features so as to improve their clinical
outcome. Machine learning frameworks that integrate uncertainty estimation as
part of their treatment recommendations would be safer and more reliable.
However, little work has been done in adapting uncertainty estimation
techniques and validation metrics for precision medicine. In this paper, we use
Bayesian deep learning for estimating the posterior distribution over factual
and counterfactual outcomes on several treatments. This allows for estimating
the uncertainty for each treatment option and for the individual treatment
effects (ITE) between any two treatments. We train and evaluate this model to
predict future new and enlarging T2 lesion counts on a large, multi-center
dataset of MR brain images of patients with multiple sclerosis, exposed to
several treatments during randomized controlled trials. We evaluate the
correlation of the uncertainty estimate with the factual error, and, given the
lack of ground truth counterfactual outcomes, demonstrate how uncertainty for
the ITE prediction relates to bounds on the ITE error. Lastly, we demonstrate
how knowledge of uncertainty could modify clinical decision-making to improve
individual patient and clinical trial outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04934">Generative Pretrained Autoregressive Transformer Graph Neural Network applied to the Analysis and Discovery of Novel Proteins. (arXiv:2305.04934v2 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Buehler_M/0/1/0/all/0/1">Markus J. Buehler</a></p>
<p>We report a flexible language-model based deep learning strategy, applied
here to solve complex forward and inverse problems in protein modeling, based
on an attention neural network that integrates transformer and graph
convolutional architectures in a causal multi-headed graph mechanism, to
realize a generative pretrained model. The model is applied to predict
secondary structure content (per-residue level and overall content), protein
solubility, and sequencing tasks. Further trained on inverse tasks, the model
is rendered capable of designing proteins with these properties as target
features. The model is formulated as a general framework, completely
prompt-based, and can be adapted for a variety of downstream tasks. We find
that adding additional tasks yields emergent synergies that the model exploits
in improving overall performance, beyond what would be possible by training a
model on each dataset alone. Case studies are presented to validate the method,
yielding protein designs specifically focused on structural proteins, but also
exploring the applicability in the design of soluble, antimicrobial
biomaterials. While our model is trained to ultimately perform 8 distinct
tasks, with available datasets it can be extended to solve additional problems.
In a broader sense, this work illustrates a form of multiscale modeling that
relates a set of ultimate building blocks (here, byte-level utf8 characters
that define the nature of the physical system at hand) to complex output. This
materiomic scheme captures complex emergent relationships between universal
building block and resulting properties via a synergizing learning capacity to
express a set of potentialities embedded in the knowledge used in training, via
the interplay of universality and diversity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.07996">Successive Affine Learning for Deep Neural Networks. (arXiv:2305.07996v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yuesheng Xu</a></p>
<p>This paper introduces a successive affine learning (SAL) model for
constructing deep neural networks (DNNs). Traditionally, a DNN is built by
solving a non-convex optimization problem. It is often challenging to solve
such a problem numerically due to its non-convexity and having a large number
of layers. To address this challenge, inspired by the human education system,
the multi-grade deep learning (MGDL) model was recently initiated by the author
of this paper. The MGDL model learns a DNN in several grades, in each of which
one constructs a shallow DNN consisting of a relatively small number of layers.
The MGDL model still requires solving several non-convex optimization problems.
The proposed SAL model mutates from the MGDL model. Noting that each layer of a
DNN consists of an affine map followed by an activation function, we propose to
learn the affine map by solving a quadratic/convex optimization problem which
involves the activation function only {\it after} the weight matrix and the
bias vector for the current layer have been trained. In the context of function
approximation, for a given function the SAL model generates an expansion of the
function with adaptive basis functions in the form of DNNs. We establish the
Pythagorean identity and the Parseval identity for the system generated by the
SAL model. Moreover, we provide a convergence theorem of the SAL process in the
sense that either it terminates after a finite number of grades or the norms of
its optimal error functions strictly decrease to a limit as the grade number
increases to infinity. Furthermore, we present numerical examples of proof of
concept which demonstrate that the proposed SAL model significantly outperforms
the traditional deep learning model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13825">Continual Learning on Dynamic Graphs via Parameter Isolation. (arXiv:2305.13825v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Peiyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuchen Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chaozhuo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Senzhang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xing Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1">Guojie Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sunghun Kim</a></p>
<p>Many real-world graph learning tasks require handling dynamic graphs where
new nodes and edges emerge. Dynamic graph learning methods commonly suffer from
the catastrophic forgetting problem, where knowledge learned for previous
graphs is overwritten by updates for new graphs. To alleviate the problem,
continual graph learning methods are proposed. However, existing continual
graph learning methods aim to learn new patterns and maintain old ones with the
same set of parameters of fixed size, and thus face a fundamental tradeoff
between both goals. In this paper, we propose Parameter Isolation GNN (PI-GNN)
for continual learning on dynamic graphs that circumvents the tradeoff via
parameter isolation and expansion. Our motivation lies in that different
parameters contribute to learning different graph patterns. Based on the idea,
we expand model parameters to continually learn emerging graph patterns.
Meanwhile, to effectively preserve knowledge for unaffected patterns, we find
parameters that correspond to them via optimization and freeze them to prevent
them from being rewritten. Experiments on eight real-world datasets corroborate
the effectiveness of PI-GNN compared to state-of-the-art baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15542">TOAST: Transfer Learning via Attention Steering. (arXiv:2305.15542v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1">Baifeng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_S/0/1/0/all/0/1">Siyu Gai</a>, <a href="http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1">Trevor Darrell</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a></p>
<p>Transfer learning involves adapting a pre-trained model to novel downstream
tasks. However, we observe that current transfer learning methods often fail to
focus on task-relevant features. In this work, we explore refocusing model
attention for transfer learning. We introduce Top-Down Attention Steering
(TOAST), a novel transfer learning algorithm that keeps the pre-trained
backbone frozen, selects task-relevant features in the output, and feeds those
features back to the model to steer the attention to the task-specific
features. By refocusing the attention only, TOAST achieves state-of-the-art
results on a number of transfer learning benchmarks, while having a small
number of tunable parameters. Compared to fully fine-tuning, LoRA, and prompt
tuning, TOAST substantially improves performance across a range of fine-grained
visual classification datasets (e.g., 81.1% -&gt; 86.2% on FGVC). TOAST also
outperforms the fully fine-tuned Alpaca and Vicuna models on
instruction-following language generation. Code is available at
https://github.com/bfshi/TOAST.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17303">Distilling BlackBox to Interpretable models for Efficient Transfer Learning. (arXiv:2305.17303v7 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1">Shantanu Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1">Ke Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1">Kayhan Batmanghelich</a></p>
<p>Building generalizable AI models is one of the primary challenges in the
healthcare domain. While radiologists rely on generalizable descriptive rules
of abnormality, Neural Network (NN) models suffer even with a slight shift in
input distribution (e.g., scanner type). Fine-tuning a model to transfer
knowledge from one domain to another requires a significant amount of labeled
data in the target domain. In this paper, we develop an interpretable model
that can be efficiently fine-tuned to an unseen target domain with minimal
computational cost. We assume the interpretable component of NN to be
approximately domain-invariant. However, interpretable models typically
underperform compared to their Blackbox (BB) variants. We start with a BB in
the source domain and distill it into a \emph{mixture} of shallow interpretable
models using human-understandable concepts. As each interpretable model covers
a subset of data, a mixture of interpretable models achieves comparable
performance as BB. Further, we use the pseudo-labeling technique from
semi-supervised learning (SSL) to learn the concept classifier in the target
domain, followed by fine-tuning the interpretable models in the target domain.
We evaluate our model using a real-life large-scale chest-X-ray (CXR)
classification dataset. The code is available at:
\url{https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.01147">Smooth Monotonic Networks. (arXiv:2306.01147v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Igel_C/0/1/0/all/0/1">Christian Igel</a></p>
<p>Monotonicity constraints are powerful regularizers in statistical modelling.
They can support fairness in computer supported decision making and increase
plausibility in data-driven scientific models. The seminal min-max (MM) neural
network architecture ensures monotonicity, but often gets stuck in undesired
local optima during training because of vanishing gradients. We propose a
simple modification of the MM network using strictly-increasing smooth
non-linearities that alleviates this problem. The resulting smooth min-max
(SMM) network module inherits the asymptotic approximation properties from the
MM architecture. It can be used within larger deep learning systems trained
end-to-end. The SMM module is considerably simpler and less computationally
demanding than state-of-the-art neural networks for monotonic modelling. Still,
in our experiments, it compared favorably to alternative neural and non-neural
approaches in terms of generalization performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02733">Realising Synthetic Active Inference Agents, Part II: Variational Message Updates. (arXiv:2306.02733v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Laar_T/0/1/0/all/0/1">Thijs van de Laar</a>, <a href="http://arxiv.org/find/stat/1/au:+Koudahl_M/0/1/0/all/0/1">Magnus Koudahl</a>, <a href="http://arxiv.org/find/stat/1/au:+Vries_B/0/1/0/all/0/1">Bert de Vries</a></p>
<p>The Free Energy Principle (FEP) describes (biological) agents as minimising a
variational Free Energy (FE) with respect to a generative model of their
environment. Active Inference (AIF) is a corollary of the FEP that describes
how agents explore and exploit their environment by minimising an expected FE
objective. In two related papers, we describe a scalable, epistemic approach to
synthetic AIF agents, by message passing on free-form Forney-style Factor
Graphs (FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG)
notation that visually represents (generalised) FE objectives for AIF. The
current paper (part II) derives message passing algorithms that minimise
(generalised) FE objectives on a CFFG by variational calculus. A comparison
between simulated Bethe and generalised FE agents illustrates how synthetic AIF
induces epistemic behaviour on a T-maze navigation task. With a full message
passing account of synthetic AIF agents, it becomes possible to derive and
reuse message updates across models and move closer to industrial applications
of synthetic AIF.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06210">Single-Model Attribution of Generative Models Through Final-Layer Inversion. (arXiv:2306.06210v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Laszkiewicz_M/0/1/0/all/0/1">Mike Laszkiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Ricker_J/0/1/0/all/0/1">Jonas Ricker</a>, <a href="http://arxiv.org/find/cs/1/au:+Lederer_J/0/1/0/all/0/1">Johannes Lederer</a>, <a href="http://arxiv.org/find/cs/1/au:+Fischer_A/0/1/0/all/0/1">Asja Fischer</a></p>
<p>Recent groundbreaking developments on generative modeling have sparked
interest in practical single-model attribution. Such methods predict whether a
sample was generated by a specific generator or not, for instance, to prove
intellectual property theft. However, previous works are either limited to the
closed-world setting or require undesirable changes of the generative model. We
address these shortcomings by proposing FLIPAD, a new approach for single-model
attribution in the open-world setting based on final-layer inversion and
anomaly detection. We show that the utilized final-layer inversion can be
reduced to a convex lasso optimization problem, making our approach
theoretically sound and computationally efficient. The theoretical findings are
accompanied by an experimental study demonstrating the effectiveness of our
approach, outperforming the existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11680">The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear Convolutional Neural Networks. (arXiv:2306.11680v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yuan Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_D/0/1/0/all/0/1">Difan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuanzhi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Quanquan Gu</a></p>
<p>We study the implicit bias of batch normalization trained by gradient
descent. We show that when learning a linear model with batch normalization for
binary classification, gradient descent converges to a uniform margin
classifier on the training data with an $\exp(-\Omega(\log^2 t))$ convergence
rate. This distinguishes linear models with batch normalization from those
without batch normalization in terms of both the type of implicit bias and the
convergence rate. We further extend our result to a class of two-layer,
single-filter linear convolutional neural networks, and show that batch
normalization has an implicit bias towards a patch-wise uniform margin. Based
on two examples, we demonstrate that patch-wise uniform margin classifiers can
outperform the maximum margin classifiers in certain learning problems. Our
results contribute to a better theoretical understanding of batch
normalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12045">Temporal Conditioning Spiking Latent Variable Models of the Neural Response to Natural Visual Scenes. (arXiv:2306.12045v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Ma_G/0/1/0/all/0/1">Gehua Ma</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Jiang_R/0/1/0/all/0/1">Runhao Jiang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yan_R/0/1/0/all/0/1">Rui Yan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tang_H/0/1/0/all/0/1">Huajin Tang</a></p>
<p>Developing computational models of neural response is crucial for
understanding sensory processing and neural computations. Current
state-of-the-art neural network methods use temporal filters to handle temporal
dependencies, resulting in an unrealistic and inflexible processing flow.
Meanwhile, these methods target trial-averaged firing rates and fail to capture
important features in spike trains. This work presents the temporal
conditioning spiking latent variable models (TeCoS-LVM) to simulate the neural
response to natural visual stimuli. We use spiking neurons to produce spike
outputs that directly match the recorded trains. This approach helps to avoid
losing information embedded in the original spike trains. We exclude the
temporal dimension from the model parameter space and introduce a temporal
conditioning operation to allow the model to adaptively explore and exploit
temporal dependencies in stimuli sequences in a natural paradigm. We show that
TeCoS-LVM models can produce more realistic spike activities and accurately fit
spike statistics than powerful alternatives. Additionally, learned TeCoS-LVM
models can generalize well to longer time scales. Overall, while remaining
computationally tractable, our model effectively captures key features of
neural coding systems. It thus provides a useful tool for building accurate
predictive computational accounts for various sensory perception circuits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13370">Physics-constrained Random Forests for Turbulence Model Uncertainty Estimation. (arXiv:2306.13370v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Matha_M/0/1/0/all/0/1">Marcel Matha</a>, <a href="http://arxiv.org/find/cs/1/au:+Morsbach_C/0/1/0/all/0/1">Christian Morsbach</a></p>
<p>To achieve virtual certification for industrial design, quantifying the
uncertainties in simulation-driven processes is crucial. We discuss a
physics-constrained approach to account for epistemic uncertainty of turbulence
models. In order to eliminate user input, we incorporate a data-driven machine
learning strategy. In addition to it, our study focuses on developing an a
priori estimation of prediction confidence when accurate data is scarce.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16015">BayesFlow: Amortized Bayesian Workflows With Neural Networks. (arXiv:2306.16015v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1">Stefan T Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmitt_M/0/1/0/all/0/1">Marvin Schmitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Schumacher_L/0/1/0/all/0/1">Lukas Schumacher</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsemuller_L/0/1/0/all/0/1">Lasse Elsem&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratz_V/0/1/0/all/0/1">Valentin Pratz</a>, <a href="http://arxiv.org/find/cs/1/au:+Schalte_Y/0/1/0/all/0/1">Yannik Sch&#xe4;lte</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1">Ullrich K&#xf6;the</a>, <a href="http://arxiv.org/find/cs/1/au:+Burkner_P/0/1/0/all/0/1">Paul-Christian B&#xfc;rkner</a></p>
<p>Modern Bayesian inference involves a mixture of computational techniques for
estimating, validating, and drawing conclusions from probabilistic models as
part of principled workflows for data analysis. Typical problems in Bayesian
workflows are the approximation of intractable posterior distributions for
diverse model types and the comparison of competing models of the same process
in terms of their complexity and predictive performance. This manuscript
introduces the Python library BayesFlow for simulation-based training of
established neural network architectures for amortized data compression and
inference. Amortized Bayesian inference, as implemented in BayesFlow, enables
users to train custom neural networks on model simulations and re-use these
networks for any subsequent application of the models. Since the trained
networks can perform inference almost instantaneously, the upfront neural
network training is quickly amortized.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16170">Mitigating the Accuracy-Robustness Trade-off via Multi-Teacher Adversarial Distillation. (arXiv:2306.16170v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Shiji Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xizhe Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xingxing Wei</a></p>
<p>Adversarial training is a practical approach for improving the robustness of
deep neural networks against adversarial attacks. Although bringing reliable
robustness, the performance toward clean examples is negatively affected after
adversarial training, which means a trade-off exists between accuracy and
robustness. Recently, some studies have tried to use knowledge distillation
methods in adversarial training, achieving competitive performance in improving
the robustness but the accuracy for clean samples is still limited. In this
paper, to mitigate the accuracy-robustness trade-off, we introduce the
Multi-Teacher Adversarial Robustness Distillation (MTARD) to guide the model's
adversarial training process by applying a strong clean teacher and a strong
robust teacher to handle the clean examples and adversarial examples,
respectively. During the optimization process, to ensure that different
teachers show similar knowledge scales, we design the Entropy-Based Balance
algorithm to adjust the teacher's temperature and keep the teachers'
information entropy consistent. Besides, to ensure that the student has a
relatively consistent learning speed from multiple teachers, we propose the
Normalization Loss Balance algorithm to adjust the learning weights of
different types of knowledge. A series of experiments conducted on public
datasets demonstrate that MTARD outperforms the state-of-the-art adversarial
training and distillation methods against various adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16177">Defining data science: a new field of inquiry. (arXiv:2306.16177v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brodie_M/0/1/0/all/0/1">Michael L Brodie</a></p>
<p>Data science is not a science. It is a research paradigm. Its power, scope,
and scale will surpass science, our most powerful research paradigm, to enable
knowledge discovery and change our world. We have yet to understand and define
it, vital to realizing its potential and managing its risks. Modern data
science is in its infancy. Emerging slowly since 1962 and rapidly since 2000,
it is a fundamentally new field of inquiry, one of the most active, powerful,
and rapidly evolving 21st century innovations. Due to its value, power, and
applicability, it is emerging in 40+ disciplines, hundreds of research areas,
and thousands of applications. Millions of data science publications contain
myriad definitions of data science and data science problem solving. Due to its
infancy, many definitions are independent, application-specific, mutually
incomplete, redundant, or inconsistent, hence so is data science. This research
addresses this data science multiple definitions challenge by proposing the
development of coherent, unified definition based on a data science reference
framework using a data science journal for the data science community to
achieve such a definition. This paper provides candidate definitions for
essential data science artifacts that are required to discuss such a
definition. They are based on the classical research paradigm concept
consisting of a philosophy of data science, the data science problem solving
paradigm, and the six component data science reference framework (axiology,
ontology, epistemology, methodology, methods, technology) that is a frequently
called for unifying framework with which to define, unify, and evolve data
science. It presents challenges for defining data science, solution approaches,
i.e., means for defining data science, and their requirements and benefits as
the basis of a comprehensive solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00033">Application of data engineering approaches to address challenges in microbiome data for optimal medical decision-making. (arXiv:2307.00033v2 [q-bio.QM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Thombre_I/0/1/0/all/0/1">Isha Thombre</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Perepu_P/0/1/0/all/0/1">Pavan Kumar Perepu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Sudhakar_S/0/1/0/all/0/1">Shyam Kumar Sudhakar</a></p>
<p>The human gut microbiota is known to contribute to numerous physiological
functions of the body and also implicated in a myriad of pathological
conditions. Prolific research work in the past few decades have yielded
valuable information regarding the relative taxonomic distribution of gut
microbiota. Unfortunately, the microbiome data suffers from class imbalance and
high dimensionality issues that must be addressed. In this study, we have
implemented data engineering algorithms to address the above-mentioned issues
inherent to microbiome data. Four standard machine learning classifiers
(logistic regression (LR), support vector machines (SVM), random forests (RF),
and extreme gradient boosting (XGB) decision trees) were implemented on a
previously published dataset. The issue of class imbalance and high
dimensionality of the data was addressed through synthetic minority
oversampling technique (SMOTE) and principal component analysis (PCA). Our
results indicate that ensemble classifiers (RF and XGB decision trees) exhibit
superior classification accuracy in predicting the host phenotype. The
application of PCA significantly reduced testing time while maintaining high
classification accuracy. The highest classification accuracy was obtained at
the levels of species for most classifiers. The prototype employed in the study
addresses the issues inherent to microbiome datasets and could be highly
beneficial for providing personalized medicine.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02623">FLuID: Mitigating Stragglers in Federated Learning using Invariant Dropout. (arXiv:2307.02623v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_I/0/1/0/all/0/1">Irene Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nair_P/0/1/0/all/0/1">Prashant J. Nair</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_D/0/1/0/all/0/1">Divya Mahajan</a></p>
<p>Federated Learning (FL) allows machine learning models to train locally on
individual mobile devices, synchronizing model updates via a shared server.
This approach safeguards user privacy; however, it also generates a
heterogeneous training environment due to the varying performance capabilities
across devices. As a result, straggler devices with lower performance often
dictate the overall training time in FL. In this work, we aim to alleviate this
performance bottleneck due to stragglers by dynamically balancing the training
load across the system. We introduce Invariant Dropout, a method that extracts
a sub-model based on the weight update threshold, thereby minimizing potential
impacts on accuracy. Building on this dropout technique, we develop an adaptive
training framework, Federated Learning using Invariant Dropout (FLuID). FLuID
offers a lightweight sub-model extraction to regulate computational intensity,
thereby reducing the load on straggler devices without affecting model quality.
Our method leverages neuron updates from non-straggler devices to construct a
tailored sub-model for each straggler based on client performance profiling.
Furthermore, FLuID can dynamically adapt to changes in stragglers as runtime
conditions shift. We evaluate FLuID using five real-world mobile clients. The
evaluations show that Invariant Dropout maintains baseline model efficiency
while alleviating the performance bottleneck of stragglers through a dynamic,
runtime approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03306">When Fair Classification Meets Noisy Protected Attributes. (arXiv:2307.03306v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1">Avijit Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvitca_P/0/1/0/all/0/1">Pablo Kvitca</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_C/0/1/0/all/0/1">Christo Wilson</a></p>
<p>The operationalization of algorithmic fairness comes with several practical
challenges, not the least of which is the availability or reliability of
protected attributes in datasets. In real-world contexts, practical and legal
impediments may prevent the collection and use of demographic data, making it
difficult to ensure algorithmic fairness. While initial fairness algorithms did
not consider these limitations, recent proposals aim to achieve algorithmic
fairness in classification by incorporating noisiness in protected attributes
or not using protected attributes at all.
</p>
<p>To the best of our knowledge, this is the first head-to-head study of fair
classification algorithms to compare attribute-reliant, noise-tolerant and
attribute-blind algorithms along the dual axes of predictivity and fairness. We
evaluated these algorithms via case studies on four real-world datasets and
synthetic perturbations. Our study reveals that attribute-blind and
noise-tolerant fair classifiers can potentially achieve similar level of
performance as attribute-reliant algorithms, even when protected attributes are
noisy. However, implementing them in practice requires careful nuance. Our
study provides insights into the practical implications of using fair
classification algorithms in scenarios where protected attributes are noisy or
partially available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03500">DEFT: Exploiting Gradient Norm Difference between Model Layers for Scalable Gradient Sparsification. (arXiv:2307.03500v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yoon_D/0/1/0/all/0/1">Daegun Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1">Sangyoon Oh</a></p>
<p>Gradient sparsification is a widely adopted solution for reducing the
excessive communication traffic in distributed deep learning. However, most
existing gradient sparsifiers have relatively poor scalability because of
considerable computational cost of gradient selection and/or increased
communication traffic owing to gradient build-up. To address these challenges,
we propose a novel gradient sparsification scheme, DEFT, that partitions the
gradient selection task into sub tasks and distributes them to workers. DEFT
differs from existing sparsifiers, wherein every worker selects gradients among
all gradients. Consequently, the computational cost can be reduced as the
number of workers increases. Moreover, gradient build-up can be eliminated
because DEFT allows workers to select gradients in partitions that are
non-intersecting (between workers). Therefore, even if the number of workers
increases, the communication traffic can be maintained as per user requirement.
</p>
<p>To avoid the loss of significance of gradient selection, DEFT selects more
gradients in the layers that have a larger gradient norm than the other layers.
Because every layer has a different computational load, DEFT allocates layers
to workers using a bin-packing algorithm to maintain a balanced load of
gradient selection between workers. In our empirical evaluation, DEFT shows a
significant improvement in training performance in terms of speed in gradient
selection over existing sparsifiers while achieving high convergence
performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04390">CT-based Subchondral Bone Microstructural Analysis in Knee Osteoarthritis via MR-Guided Distillation Learning. (arXiv:2307.04390v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1">Yuqi Hu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/eess/1/au:+Qing_G/0/1/0/all/0/1">Gaowei Qing</a>, <a href="http://arxiv.org/find/eess/1/au:+Xie_K/0/1/0/all/0/1">Kai Xie</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_C/0/1/0/all/0/1">Chenglei Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1">Lichi Zhang</a></p>
<p>Background: MR-based subchondral bone effectively predicts knee
osteoarthritis. However, its clinical application is limited by the cost and
time of MR. Purpose: We aim to develop a novel distillation-learning-based
method named SRRD for subchondral bone microstructural analysis using
easily-acquired CT images, which leverages paired MR images to enhance the
CT-based analysis model during training. Materials and Methods: Knee joint
images of both CT and MR modalities were collected from October 2020 to May
2021. Firstly, we developed a GAN-based generative model to transform MR images
into CT images, which was used to establish the anatomical correspondence
between the two modalities. Next, we obtained numerous patches of subchondral
bone regions of MR images, together with their trabecular parameters (BV / TV,
Tb. Th, Tb. Sp, Tb. N) from the corresponding CT image patches via regression.
The distillation-learning technique was used to train the regression model and
transfer MR structural information to the CT-based model. The regressed
trabecular parameters were further used for knee osteoarthritis classification.
Results: A total of 80 participants were evaluated. CT-based regression results
of trabecular parameters achieved intra-class correlation coefficients (ICCs)
of 0.804, 0.773, 0.711, and 0.622 for BV / TV, Tb. Th, Tb. Sp, and Tb. N,
respectively. The use of distillation learning significantly improved the
performance of the CT-based knee osteoarthritis classification method using the
CNN approach, yielding an AUC score of 0.767 (95% CI, 0.681-0.853) instead of
0.658 (95% CI, 0.574-0.742) (p&lt;.001). Conclusions: The proposed SRRD method
showed high reliability and validity in MR-CT registration, regression, and
knee osteoarthritis classification, indicating the feasibility of subchondral
bone microstructural analysis based on CT images.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04526">Self Expanding Neural Networks. (arXiv:2307.04526v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1">Rupert Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1">Martin Mundt</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>The results of training a neural network are heavily dependent on the
architecture chosen; and even a modification of only the size of the network,
however small, typically involves restarting the training process. In contrast
to this, we begin training with a small architecture, only increase its
capacity as necessary for the problem, and avoid interfering with previous
optimization while doing so. We thereby introduce a natural gradient based
approach which intuitively expands both the width and depth of a neural network
when this is likely to substantially reduce the hypothetical converged training
loss. We prove an upper bound on the "rate" at which neurons are added, and a
computationally cheap lower bound on the expansion score. We illustrate the
benefits of such Self-Expanding Neural Networks in both classification and
regression problems, including those where the appropriate architecture size is
substantially uncertain a priori.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04679">Generalization Error of First-Order Methods for Statistical Learning with Generic Oracles. (arXiv:2307.04679v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scaman_K/0/1/0/all/0/1">Kevin Scaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Even_M/0/1/0/all/0/1">Mathieu Even</a>, <a href="http://arxiv.org/find/cs/1/au:+Massoulie_L/0/1/0/all/0/1">Laurent Massouli&#xe9;</a></p>
<p>In this paper, we provide a novel framework for the analysis of
generalization error of first-order optimization algorithms for statistical
learning when the gradient can only be accessed through partial observations
given by an oracle. Our analysis relies on the regularity of the gradient
w.r.t. the data samples, and allows to derive near matching upper and lower
bounds for the generalization error of multiple learning problems, including
supervised learning, transfer learning, robust learning, distributed learning
and communication efficient learning using gradient quantization. These results
hold for smooth and strongly-convex optimization problems, as well as smooth
non-convex optimization problems verifying a Polyak-Lojasiewicz assumption. In
particular, our upper and lower bounds depend on a novel quantity that extends
the notion of conditional standard deviation, and is a measure of the extent to
which the gradient can be approximated by having access to the oracle. As a
consequence, our analysis provides a precise meaning to the intuition that
optimization of the statistical learning objective is as hard as the estimation
of its gradient. Finally, we show that, in the case of standard supervised
learning, mini-batch gradient descent with increasing batch sizes and a warm
start can reach a generalization error that is optimal up to a multiplicative
factor, thus motivating the use of this optimization scheme in practical
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13339">TrustGuard: GNN-based Robust and Explainable Trust Evaluation with Dynamicity Support. (arXiv:2306.13339v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1">Jiahe Lan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bertino_E/0/1/0/all/0/1">Elisa Bertino</a>, <a href="http://arxiv.org/find/cs/1/au:+Pedrycz_W/0/1/0/all/0/1">Witold Pedrycz</a></p>
<p>Trust evaluation assesses trust relationships between entities and
facilitates decision-making. Machine Learning (ML) shows great potential for
trust evaluation owing to its learning capabilities. In recent years, Graph
Neural Networks (GNNs), as a new ML paradigm, have demonstrated superiority in
dealing with graph data. This has motivated researchers to explore their use in
trust evaluation, as trust relationships among entities can be modeled as a
graph. However, current trust evaluation methods that employ GNNs fail to fully
satisfy the dynamicity nature of trust, overlook the adverse effects of attacks
on trust evaluation, and cannot provide convincing explanations on evaluation
results. To address these problems, in this paper, we propose TrustGuard, a
GNN-based accurate trust evaluation model that supports trust dynamicity, is
robust against typical attacks, and provides explanations through
visualization. Specifically, TrustGuard is designed with a layered architecture
that contains a snapshot input layer, a spatial aggregation layer, a temporal
aggregation layer, and a prediction layer. Among them, the spatial aggregation
layer can be plugged into a defense mechanism for a robust aggregation of local
trust relationships, and the temporal aggregation layer applies an attention
mechanism for effective learning of temporal patterns. Extensive experiments on
two real-world datasets show that TrustGuard outperforms state-of-the-art
GNN-based trust evaluation models with respect to trust prediction across
single-timeslot and multi-timeslot, even in the presence of attacks. In
particular, TrustGuard can explain its evaluation results by visualizing both
spatial and temporal views.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03419">QI2 -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.03419v2 [cs.CY] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geerkens_S/0/1/0/all/0/1">Simon Geerkens</a>, <a href="http://arxiv.org/find/cs/1/au:+Sieberichs_C/0/1/0/all/0/1">Christian Sieberichs</a>, <a href="http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1">Alexander Braun</a>, <a href="http://arxiv.org/find/cs/1/au:+Waschulzik_T/0/1/0/all/0/1">Thomas Waschulzik</a></p>
<p>The importance of high data quality is increasing with the growing impact and
distribution of ML systems and big data. Also the planned AI Act from the
European commission defines challenging legal requirements for data quality
especially for the market introduction of safety relevant ML systems. In this
paper we introduce a novel approach that supports the data quality assurance
process of multiple data quality aspects. This approach enables the
verification of quantitative data quality requirements. The concept and
benefits are introduced and explained on small example data sets. How the
method is applied is demonstrated on the well known MNIST data set based an
handwritten digits.
</p>
</p>
</div>

    </div>
    </body>
    