<!DOCTYPE html>
<html>
<head>
<title>2023-12-20-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.10040">Robust Errant Beam Prognostics with Conditional Modeling for Particle Accelerators. (arXiv:2312.10040v1 [physics.acc-ph])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Rajput_k/0/1/0/all/0/1">kishansingh Rajput</a>, <a href="http://arxiv.org/find/physics/1/au:+Schram_M/0/1/0/all/0/1">Malachi Schram</a>, <a href="http://arxiv.org/find/physics/1/au:+Blokland_W/0/1/0/all/0/1">Willem Blokland</a>, <a href="http://arxiv.org/find/physics/1/au:+Alanazi_Y/0/1/0/all/0/1">Yasir Alanazi</a>, <a href="http://arxiv.org/find/physics/1/au:+Ramuhalli_P/0/1/0/all/0/1">Pradeep Ramuhalli</a>, <a href="http://arxiv.org/find/physics/1/au:+Zhukov_A/0/1/0/all/0/1">Alexander Zhukov</a>, <a href="http://arxiv.org/find/physics/1/au:+Peters_C/0/1/0/all/0/1">Charles Peters</a>, <a href="http://arxiv.org/find/physics/1/au:+Vilalta_R/0/1/0/all/0/1">Ricardo Vilalta</a></p>
<p>Particle accelerators are complex and comprise thousands of components, with
many pieces of equipment running at their peak power. Consequently, particle
accelerators can fault and abort operations for numerous reasons. These faults
impact the availability of particle accelerators during scheduled run-time and
hamper the efficiency and the overall science output. To avoid these faults, we
apply anomaly detection techniques to predict any unusual behavior and perform
preemptive actions to improve the total availability of particle accelerators.
Semi-supervised Machine Learning (ML) based anomaly detection approaches such
as autoencoders and variational autoencoders are often used for such tasks.
However, supervised ML techniques such as Siamese Neural Network (SNN) models
can outperform unsupervised or semi-supervised approaches for anomaly detection
by leveraging the label information. One of the challenges specific to anomaly
detection for particle accelerators is the data's variability due to system
configuration changes. To address this challenge, we employ Conditional Siamese
Neural Network (CSNN) models and Conditional Variational Auto Encoder (CVAE)
models to predict errant beam pulses at the Spallation Neutron Source (SNS)
under different system configuration conditions and compare their performance.
We demonstrate that CSNN outperforms CVAE in our application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10042">A Generic Stochastic Hybrid Car-following Model Based on Approximate Bayesian Computation. (arXiv:2312.10042v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1">Jiwan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1">Soyoung Ahn</a></p>
<p>Car following (CF) models are fundamental to describing traffic dynamics.
However, the CF behavior of human drivers is highly stochastic and nonlinear.
As a result, identifying the best CF model has been challenging and
controversial despite decades of research. Introduction of automated vehicles
has further complicated this matter as their CF controllers remain proprietary,
though their behavior appears different than human drivers. This paper develops
a stochastic learning approach to integrate multiple CF models, rather than
relying on a single model. The framework is based on approximate Bayesian
computation that probabilistically concatenates a pool of CF models based on
their relative likelihood of describing observed behavior. The approach, while
data-driven, retains physical tractability and interpretability. Evaluation
results using two datasets show that the proposed approach can better reproduce
vehicle trajectories for both human driven and automated vehicles than any
single CF model considered.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10045">Interpretable Knowledge Tracing via Response Influence-based Counterfactual Reasoning. (arXiv:2312.10045v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jiajun Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1">Minghe Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1">Bo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1">Aimin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianyong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wei Zhang</a></p>
<p>Knowledge tracing (KT) plays a crucial role in computer-aided education and
intelligent tutoring systems, aiming to assess students' knowledge proficiency
by predicting their future performance on new questions based on their past
response records. While existing deep learning knowledge tracing (DLKT) methods
have significantly improved prediction accuracy and achieved state-of-the-art
results, they often suffer from a lack of interpretability. To address this
limitation, current approaches have explored incorporating psychological
influences to achieve more explainable predictions, but they tend to overlook
the potential influences of historical responses. In fact, understanding how
models make predictions based on response influences can enhance the
transparency and trustworthiness of the knowledge tracing process, presenting
an opportunity for a new paradigm of interpretable KT. However, measuring
unobservable response influences is challenging. In this paper, we resort to
counterfactual reasoning that intervenes in each response to answer
\textit{what if a student had answered a question incorrectly that he/she
actually answered correctly, and vice versa}. Based on this, we propose RCKT, a
novel response influence-based counterfactual knowledge tracing framework. RCKT
generates response influences by comparing prediction outcomes from factual
sequences and constructed counterfactual sequences after interventions.
Additionally, we introduce maximization and inference techniques to leverage
accumulated influences from different past responses, further improving the
model's performance and credibility. Extensive experimental results demonstrate
that our RCKT method outperforms state-of-the-art knowledge tracing methods on
four datasets against six baselines, and provides credible interpretations of
response influences.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10046">Deep Metric Learning for Computer Vision: A Brief Overview. (arXiv:2312.10046v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohan_D/0/1/0/all/0/1">Deen Dayal Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jawade_B/0/1/0/all/0/1">Bhavin Jawade</a>, <a href="http://arxiv.org/find/cs/1/au:+Setlur_S/0/1/0/all/0/1">Srirangaraj Setlur</a>, <a href="http://arxiv.org/find/cs/1/au:+Govindaraj_V/0/1/0/all/0/1">Venu Govindaraj</a></p>
<p>Objective functions that optimize deep neural networks play a vital role in
creating an enhanced feature representation of the input data. Although
cross-entropy-based loss formulations have been extensively used in a variety
of supervised deep-learning applications, these methods tend to be less
adequate when there is large intra-class variance and low inter-class variance
in input data distribution. Deep Metric Learning seeks to develop methods that
aim to measure the similarity between data samples by learning a representation
function that maps these data samples into a representative embedding space. It
leverages carefully designed sampling strategies and loss functions that aid in
optimizing the generation of a discriminative embedding space even for
distributions having low inter-class and high intra-class variances. In this
chapter, we will provide an overview of recent progress in this area and
discuss state-of-the-art Deep Metric Learning approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10052">ESTformer: Transformer Utilizing Spatiotemporal Dependencies for EEG Super-resolution. (arXiv:2312.10052v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_D/0/1/0/all/0/1">Dongdong Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zeng_Z/0/1/0/all/0/1">Zhongliang Zeng</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1">Zhe Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1">Hai Yang</a></p>
<p>Towards practical applications of Electroencephalography (EEG) data,
lightweight acquisition devices, equipped with a few electrodes, result in a
predicament where analysis methods can only leverage EEG data with extremely
low spatial resolution. Recent methods mainly focus on using mathematical
interpolation methods and Convolutional Neural Networks for EEG
super-resolution (SR), but they suffer from high computation costs, extra bias,
and few insights in spatiotemporal dependency modeling. To this end, we propose
the ESTformer, an EEG SR framework utilizing spatiotemporal dependencies based
on the Transformer. The ESTformer applies positional encoding methods and the
Multi-head Self-attention mechanism to the space and time dimensions, which can
learn spatial structural information and temporal functional variation. The
ESTformer, with the fixed masking strategy, adopts a mask token to up-sample
the low-resolution (LR) EEG data in case of disturbance from mathematical
interpolation methods. On this basis, we design various Transformer blocks to
construct the Spatial Interpolation Module (SIM) and the Temporal
Reconstruction Module (TRM). Finally, the ESTformer cascades the SIM and the
TRM to capture and model spatiotemporal dependencies for EEG SR with fidelity.
Extensive experimental results on two EEG datasets show the effectiveness of
the ESTformer against previous state-of-the-art methods and verify the
superiority of the SR data to the LR data in EEG-based downstream tasks of
person identification and emotion recognition. The proposed ESTformer
demonstrates the versatility of the Transformer for EEG SR tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10056">ProtoEEGNet: An Interpretable Approach for Detecting Interictal Epileptiform Discharges. (arXiv:2312.10056v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tang_D/0/1/0/all/0/1">Dennis Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Willard_F/0/1/0/all/0/1">Frank Willard</a>, <a href="http://arxiv.org/find/eess/1/au:+Tegerdine_R/0/1/0/all/0/1">Ronan Tegerdine</a>, <a href="http://arxiv.org/find/eess/1/au:+Triplett_L/0/1/0/all/0/1">Luke Triplett</a>, <a href="http://arxiv.org/find/eess/1/au:+Donnelly_J/0/1/0/all/0/1">Jon Donnelly</a>, <a href="http://arxiv.org/find/eess/1/au:+Moffett_L/0/1/0/all/0/1">Luke Moffett</a>, <a href="http://arxiv.org/find/eess/1/au:+Semenova_L/0/1/0/all/0/1">Lesia Semenova</a>, <a href="http://arxiv.org/find/eess/1/au:+Barnett_A/0/1/0/all/0/1">Alina Jade Barnett</a>, <a href="http://arxiv.org/find/eess/1/au:+Jing_J/0/1/0/all/0/1">Jin Jing</a>, <a href="http://arxiv.org/find/eess/1/au:+Rudin_C/0/1/0/all/0/1">Cynthia Rudin</a>, <a href="http://arxiv.org/find/eess/1/au:+Westover_B/0/1/0/all/0/1">Brandon Westover</a></p>
<p>In electroencephalogram (EEG) recordings, the presence of interictal
epileptiform discharges (IEDs) serves as a critical biomarker for seizures or
seizure-like events.Detecting IEDs can be difficult; even highly trained
experts disagree on the same sample. As a result, specialists have turned to
machine-learning models for assistance. However, many existing models are black
boxes and do not provide any human-interpretable reasoning for their decisions.
In high-stakes medical applications, it is critical to have interpretable
models so that experts can validate the reasoning of the model before making
important diagnoses. We introduce ProtoEEGNet, a model that achieves
state-of-the-art accuracy for IED detection while additionally providing an
interpretable justification for its classifications. Specifically, it can
reason that one EEG looks similar to another ''prototypical'' EEG that is known
to contain an IED. ProtoEEGNet can therefore help medical professionals
effectively detect IEDs while maintaining a transparent decision-making
process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10068">Estimation of Physical Parameters of Waveforms With Neural Networks. (arXiv:2312.10068v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Jamal_S/0/1/0/all/0/1">Saad Ahmed Jamal</a>, <a href="http://arxiv.org/find/eess/1/au:+Corpetti_T/0/1/0/all/0/1">Thomas Corpetti</a>, <a href="http://arxiv.org/find/eess/1/au:+Tiede_D/0/1/0/all/0/1">Dirk Tiede</a>, <a href="http://arxiv.org/find/eess/1/au:+Letard_M/0/1/0/all/0/1">Mathilde Letard</a>, <a href="http://arxiv.org/find/eess/1/au:+Lague_D/0/1/0/all/0/1">Dimitri Lague</a></p>
<p>Light Detection and Ranging (LiDAR) are fast emerging sensors in the field of
Earth Observation. It is a remote sensing technology that utilizes laser beams
to measure distances and create detailed three-dimensional representations of
objects and environments. The potential of Full Waveform LiDAR is much greater
than just height estimation and 3D reconstruction only. Overall shape of signal
provides important information about properties of water body. However, the
shape of FWL is unexplored as most LiDAR software work on point cloud by
utilizing the maximum value within the waveform. Existing techniques in the
field of LiDAR data analysis include depth estimation through inverse modeling
and regression of logarithmic intensity and depth for approximating the
attenuation coefficient. However, these methods suffer from limitations in
accuracy. Depth estimation through inverse modeling provides only approximate
values and does not account for variations in surface properties, while the
regression approach for the attenuation coefficient is only able to generalize
a value through several data points which lacks precision and may lead to
significant errors in estimation. Additionally, there is currently no
established modeling method available for predicting bottom reflectance. This
research proposed a novel solution based on neural networks for parameter
estimation in LIDAR data analysis. By leveraging the power of neural networks,
the proposed solution successfully learned the inversion model, was able to do
prediction of parameters such as depth, attenuation coefficient, and bottom
reflectance. Performance of model was validated by testing it on real LiDAR
data. In future, more data availability would enable more accuracy and
reliability of such models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10069">Understanding Representations Pretrained with Auxiliary Losses for Embodied Agent Planning. (arXiv:2312.10069v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yuxuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Weihs_L/0/1/0/all/0/1">Luca Weihs</a></p>
<p>Pretrained representations from large-scale vision models have boosted the
performance of downstream embodied policy learning. We look to understand
whether additional self-supervised pretraining on exploration trajectories can
build on these general-purpose visual representations to better support
embodied planning in realistic environments. We evaluated four common auxiliary
losses in embodied AI, two hindsight-based losses, and a standard imitation
learning loss, by pretraining the agent's visual compression module and state
belief representations with each objective and using CLIP as a representative
visual backbone. The learned representations are then frozen for downstream
multi-step evaluation on two goal-directed tasks. Surprisingly, we find that
imitation learning on these exploration trajectories out-performs all other
auxiliary losses even despite the exploration trajectories being dissimilar
from the downstream tasks. This suggests that imitation of exploration may be
''all you need'' for building powerful planning representations. Additionally,
we find that popular auxiliary losses can benefit from simple modifications to
improve their support for downstream planning ability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10072">Assessing the Usability of GutGPT: A Simulation Study of an AI Clinical Decision Support System for Gastrointestinal Bleeding Risk. (arXiv:2312.10072v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1">Colleen Chan</a>, <a href="http://arxiv.org/find/cs/1/au:+You_K/0/1/0/all/0/1">Kisung You</a>, <a href="http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1">Sunny Chung</a>, <a href="http://arxiv.org/find/cs/1/au:+Giuffre_M/0/1/0/all/0/1">Mauro Giuffr&#xe8;</a>, <a href="http://arxiv.org/find/cs/1/au:+Saarinen_T/0/1/0/all/0/1">Theo Saarinen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajashekar_N/0/1/0/all/0/1">Niroop Rajashekar</a>, <a href="http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1">Yuan Pu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1">Yeo Eun Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Laine_L/0/1/0/all/0/1">Loren Laine</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1">Ambrose Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kizilcec_R/0/1/0/all/0/1">Ren&#xe9; Kizilcec</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekhon_J/0/1/0/all/0/1">Jasjeet Sekhon</a>, <a href="http://arxiv.org/find/cs/1/au:+Shung_D/0/1/0/all/0/1">Dennis Shung</a></p>
<p>Applications of large language models (LLMs) like ChatGPT have potential to
enhance clinical decision support through conversational interfaces. However,
challenges of human-algorithmic interaction and clinician trust are poorly
understood. GutGPT, a LLM for gastrointestinal (GI) bleeding risk prediction
and management guidance, was deployed in clinical simulation scenarios
alongside the electronic health record (EHR) with emergency medicine
physicians, internal medicine physicians, and medical students to evaluate its
effect on physician acceptance and trust in AI clinical decision support
systems (AI-CDSS). GutGPT provides risk predictions from a validated machine
learning model and evidence-based answers by querying extracted clinical
guidelines. Participants were randomized to GutGPT and an interactive
dashboard, or the interactive dashboard and a search engine. Surveys and
educational assessments taken before and after measured technology acceptance
and content mastery. Preliminary results showed mixed effects on acceptance
after using GutGPT compared to the dashboard or search engine but appeared to
improve content mastery based on simulation performance. Overall, this study
demonstrates LLMs like GutGPT could enhance effective AI-CDSS if implemented
optimally and paired with interactive interfaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10078">Early ChatGPT User Portrait through the Lens of Data. (arXiv:2312.10078v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1">Yuyang Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1">Ni Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xin Huang</a></p>
<p>Since its launch, ChatGPT has achieved remarkable success as a versatile
conversational AI platform, drawing millions of users worldwide and garnering
widespread recognition across academic, industrial, and general communities.
This paper aims to point a portrait of early GPT users and understand how they
evolved. Specific questions include their topics of interest and their
potential careers; and how this changes over time. We conduct a detailed
analysis of real-world ChatGPT datasets with multi-turn conversations between
users and ChatGPT. Through a multi-pronged approach, we quantify conversation
dynamics by examining the number of turns, then gauge sentiment to understand
user sentiment variations, and finally employ Latent Dirichlet Allocation (LDA)
to discern overarching topics within the conversation. By understanding shifts
in user demographics and interests, we aim to shed light on the changing nature
of human-AI interaction and anticipate future trends in user engagement with
language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10080">No prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation. (arXiv:2312.10080v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1">Nimesh Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Sirohi_A/0/1/0/all/0/1">Anuj Kumar Sirohi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jayadeva/0/1/0/all/0/1">Jayadeva</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sandeep Kumar</a></p>
<p>Ensuring fairness in Recommendation Systems (RSs) across demographic groups
is critical due to the increased integration of RSs in applications such as
personalized healthcare, finance, and e-commerce. Graph-based RSs play a
crucial role in capturing intricate higher-order interactions among entities.
However, integrating these graph models into the Federated Learning (FL)
paradigm with fairness constraints poses formidable challenges as this requires
access to the entire interaction graph and sensitive user information (such as
gender, age, etc.) at the central server. This paper addresses the pervasive
issue of inherent bias within RSs for different demographic groups without
compromising the privacy of sensitive user attributes in FL environment with
the graph-based model. To address the group bias, we propose F2PGNN (Fair
Federated Personalized Graph Neural Network), a novel framework that leverages
the power of Personalized Graph Neural Network (GNN) coupled with fairness
considerations. Additionally, we use differential privacy techniques to fortify
privacy protection. Experimental evaluation on three publicly available
datasets showcases the efficacy of F2PGNN in mitigating group unfairness by 47%
- 99% compared to the state-of-the-art while preserving privacy and maintaining
the utility. The results validate the significance of our framework in
achieving equitable and personalized recommendations using GNN within the FL
landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10082">Finding Paths for Explainable MOOC Recommendation: A Learner Perspective. (arXiv:2312.10082v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frej_J/0/1/0/all/0/1">Jibril Frej</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1">Neel Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Knezevic_M/0/1/0/all/0/1">Marta Kne&#x17e;evi&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazaretsky_T/0/1/0/all/0/1">Tanya Nazaretsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaser_T/0/1/0/all/0/1">Tanja K&#xe4;ser</a></p>
<p>The increasing availability of Massive Open Online Courses (MOOCs) has
created a necessity for personalized course recommendation systems. These
systems often combine neural networks with Knowledge Graphs (KGs) to achieve
richer representations of learners and courses. While these enriched
representations allow more accurate and personalized recommendations,
explainability remains a significant challenge which is especially problematic
for certain domains with significant impact such as education and online
learning. Recently, a novel class of recommender systems that uses
reinforcement learning and graph reasoning over KGs has been proposed to
generate explainable recommendations in the form of paths over a KG. Despite
their accuracy and interpretability on e-commerce datasets, these approaches
have scarcely been applied to the educational domain and their use in practice
has not been studied. In this work, we propose an explainable recommendation
system for MOOCs that uses graph reasoning. To validate the practical
implications of our approach, we conducted a user study examining user
perceptions of our new explainable recommendations. We demonstrate the
generalizability of our approach by conducting experiments on two educational
datasets: COCO and Xuetang.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10083">The Limits of Fair Medical Imaging AI In The Wild. (arXiv:2312.10083v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuzhe Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haoran Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1">Judy W Gichoya</a>, <a href="http://arxiv.org/find/cs/1/au:+Katabi_D/0/1/0/all/0/1">Dina Katabi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1">Marzyeh Ghassemi</a></p>
<p>As artificial intelligence (AI) rapidly approaches human-level performance in
medical imaging, it is crucial that it does not exacerbate or propagate
healthcare disparities. Prior research has established AI's capacity to infer
demographic data from chest X-rays, leading to a key concern: do models using
demographic shortcuts have unfair predictions across subpopulations? In this
study, we conduct a thorough investigation into the extent to which medical AI
utilizes demographic encodings, focusing on potential fairness discrepancies
within both in-distribution training sets and external test sets. Our analysis
covers three key medical imaging disciplines: radiology, dermatology, and
ophthalmology, and incorporates data from six global chest X-ray datasets. We
confirm that medical imaging AI leverages demographic shortcuts in disease
classification. While correcting shortcuts algorithmically effectively
addresses fairness gaps to create "locally optimal" models within the original
data distribution, this optimality is not true in new test settings.
Surprisingly, we find that models with less encoding of demographic attributes
are often most "globally optimal", exhibiting better fairness during model
evaluation in new test environments. Our work establishes best practices for
medical imaging models which maintain their performance and fairness in
deployments beyond their initial training contexts, underscoring critical
considerations for AI clinical deployments across populations and sites.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10087">Revisiting the Entropy Semiring for Neural Speech Recognition. (arXiv:2312.10087v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chang_O/0/1/0/all/0/1">Oscar Chang</a>, <a href="http://arxiv.org/find/eess/1/au:+Hwang_D/0/1/0/all/0/1">Dongseong Hwang</a>, <a href="http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1">Olivier Siohan</a></p>
<p>In streaming settings, speech recognition models have to map sub-sequences of
speech to text before the full audio stream becomes available. However, since
alignment information between speech and text is rarely available during
training, models need to learn it in a completely self-supervised way. In
practice, the exponential number of possible alignments makes this extremely
challenging, with models often learning peaky or sub-optimal alignments. Prima
facie, the exponential nature of the alignment space makes it difficult to even
quantify the uncertainty of a model's alignment distribution. Fortunately, it
has been known for decades that the entropy of a probabilistic finite state
transducer can be computed in time linear to the size of the transducer via a
dynamic programming reduction based on semirings. In this work, we revisit the
entropy semiring for neural speech recognition models, and show how alignment
entropy can be used to supervise models through regularization or distillation.
We also contribute an open-source implementation of CTC and RNN-T in the
semiring framework that includes numerically stable and highly parallel
variants of the entropy semiring. Empirically, we observe that the addition of
alignment distillation improves the accuracy and latency of an already
well-optimized teacher-student distillation model, achieving state-of-the-art
performance on the Librispeech dataset in the streaming scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10088">On Robustness to Missing Video for Audiovisual Speech Recognition. (arXiv:2312.10088v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chang_O/0/1/0/all/0/1">Oscar Chang</a>, <a href="http://arxiv.org/find/eess/1/au:+Braga_O/0/1/0/all/0/1">Otavio Braga</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1">Hank Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Serdyuk_D/0/1/0/all/0/1">Dmitriy Serdyuk</a>, <a href="http://arxiv.org/find/eess/1/au:+Siohan_O/0/1/0/all/0/1">Olivier Siohan</a></p>
<p>It has been shown that learning audiovisual features can lead to improved
speech recognition performance over audio-only features, especially for noisy
speech. However, in many common applications, the visual features are partially
or entirely missing, e.g.~the speaker might move off screen. Multi-modal models
need to be robust: missing video frames should not degrade the performance of
an audiovisual model to be worse than that of a single-modality audio-only
model. While there have been many attempts at building robust models, there is
little consensus on how robustness should be evaluated. To address this, we
introduce a framework that allows claims about robustness to be evaluated in a
precise and testable way. We also conduct a systematic empirical study of the
robustness of common audiovisual speech recognition architectures on a range of
acoustic noise conditions and test suites. Finally, we show that an
architecture-agnostic solution based on cascades can consistently achieve
robustness to missing video, even in settings where existing techniques for
robustness like dropout fall short.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10089">Advancements in Content-Based Image Retrieval: A Comprehensive Survey of Relevance Feedback Techniques. (arXiv:2312.10089v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qazanfari_H/0/1/0/all/0/1">Hamed Qazanfari</a>, <a href="http://arxiv.org/find/cs/1/au:+AlyanNezhadi_M/0/1/0/all/0/1">Mohammad M. AlyanNezhadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Khoshdaregi_Z/0/1/0/all/0/1">Zohreh Nozari Khoshdaregi</a></p>
<p>Content-based image retrieval (CBIR) systems have emerged as crucial tools in
the field of computer vision, allowing for image search based on visual content
rather than relying solely on metadata. This survey paper presents a
comprehensive overview of CBIR, emphasizing its role in object detection and
its potential to identify and retrieve visually similar images based on content
features. Challenges faced by CBIR systems, including the semantic gap and
scalability, are discussed, along with potential solutions. It elaborates on
the semantic gap, which arises from the disparity between low-level features
and high-level semantic concepts, and explores approaches to bridge this gap.
One notable solution is the integration of relevance feedback (RF), empowering
users to provide feedback on retrieved images and refine search results
iteratively. The survey encompasses long-term and short-term learning
approaches that leverage RF for enhanced CBIR accuracy and relevance. These
methods focus on weight optimization and the utilization of active learning
algorithms to select samples for training classifiers. Furthermore, the paper
investigates machine learning techniques and the utilization of deep learning
and convolutional neural networks to enhance CBIR performance. This survey
paper plays a significant role in advancing the understanding of CBIR and RF
techniques. It guides researchers and practitioners in comprehending existing
methodologies, challenges, and potential solutions while fostering knowledge
dissemination and identifying research gaps. By addressing future research
directions, it sets the stage for advancements in CBIR that will enhance
retrieval accuracy, usability, and effectiveness in various application
domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10091">Look Before You Leap: A Universal Emergent Decomposition of Retrieval Tasks in Language Models. (arXiv:2312.10091v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Variengien_A/0/1/0/all/0/1">Alexandre Variengien</a>, <a href="http://arxiv.org/find/cs/1/au:+Winsor_E/0/1/0/all/0/1">Eric Winsor</a></p>
<p>When solving challenging problems, language models (LMs) are able to identify
relevant information from long and complicated contexts. To study how LMs solve
retrieval tasks in diverse situations, we introduce ORION, a collection of
structured retrieval tasks spanning six domains, from text understanding to
coding. Each task in ORION can be represented abstractly by a request (e.g. a
question) that retrieves an attribute (e.g. the character name) from a context
(e.g. a story). We apply causal analysis on 18 open-source language models with
sizes ranging from 125 million to 70 billion parameters. We find that LMs
internally decompose retrieval tasks in a modular way: middle layers at the
last token position process the request, while late layers retrieve the correct
entity from the context. After causally enforcing this decomposition, models
are still able to solve the original task, preserving 70% of the original
correct token probability in 98 of the 106 studied model-task pairs. We connect
our macroscopic decomposition with a microscopic description by performing a
fine-grained case study of a question-answering task on Pythia-2.8b. Building
on our high-level understanding, we demonstrate a proof of concept application
for scalable internal oversight of LMs to mitigate prompt-injection while
requiring human supervision on only a single input. Our solution improves
accuracy drastically (from 15.5% to 97.5% on Pythia-12b). This work presents
evidence of a universal emergent modular processing of tasks across varied
domains and models and is a pioneering effort in applying interpretability for
scalable internal oversight of LMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10100">Data-Adaptive Dimensional Analysis for Accurate Interpolation and Extrapolation in Computer Experiments. (arXiv:2312.10100v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_Arelis_G/0/1/0/all/0/1">G. Alexi Rodriguez-Arelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Welch_W/0/1/0/all/0/1">William J. Welch</a></p>
<p>Dimensional analysis (DA) pays attention to fundamental physical dimensions
such as length and mass when modelling scientific and engineering systems. It
goes back at least a century to Buckingham's Pi theorem, which characterizes a
scientifically meaningful model in terms of a limited number of dimensionless
variables. The methodology has only been exploited relatively recently by
statisticians for design and analysis of experiments, however, and computer
experiments in particular. The basic idea is to build models in terms of new
dimensionless quantities derived from the original input and output variables.
A scientifically valid formulation has the potential for improved prediction
accuracy in principle, but the implementation of DA is far from
straightforward. There can be a combinatorial number of possible models
satisfying the conditions of the theory. Empirical approaches for finding
effective derived variables will be described, and improvements in prediction
accuracy will be demonstrated. As DA's dimensionless quantities for a
statistical model typically compare the original variables rather than use
their absolute magnitudes, DA is less dependent on the choice of experimental
ranges in the training data. Hence, we are also able to illustrate sustained
accuracy gains even when extrapolating substantially outside the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10102">Robust Estimation of Causal Heteroscedastic Noise Models. (arXiv:2312.10102v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tran_Q/0/1/0/all/0/1">Quang-Duy Tran</a>, <a href="http://arxiv.org/find/stat/1/au:+Duong_B/0/1/0/all/0/1">Bao Duong</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_P/0/1/0/all/0/1">Phuoc Nguyen</a>, <a href="http://arxiv.org/find/stat/1/au:+Nguyen_T/0/1/0/all/0/1">Thin Nguyen</a></p>
<p>Distinguishing the cause and effect from bivariate observational data is the
foundational problem that finds applications in many scientific disciplines.
One solution to this problem is assuming that cause and effect are generated
from a structural causal model, enabling identification of the causal direction
after estimating the model in each direction. The heteroscedastic noise model
is a type of structural causal model where the cause can contribute to both the
mean and variance of the noise. Current methods for estimating heteroscedastic
noise models choose the Gaussian likelihood as the optimization objective which
can be suboptimal and unstable when the data has a non-Gaussian distribution.
To address this limitation, we propose a novel approach to estimating this
model with Student's $t$-distribution, which is known for its robustness in
accounting for sampling variability with smaller sample sizes and extreme
values without significantly altering the overall distribution shape. This
adaptability is beneficial for capturing the parameters of the noise
distribution in heteroscedastic noise models. Our empirical evaluations
demonstrate that our estimators are more robust and achieve better overall
performance across synthetic and real benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10104">ICD-LM: Configuring Vision-Language In-Context Demonstrations by Language Modeling. (arXiv:2312.10104v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yingzhe Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haoxuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1">Shuo Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yucheng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanwang Zhang</a></p>
<p>This paper studies how to configure powerful In-Context Demonstration (ICD)
sequences for a Large Vision-Language Model (LVLM) to solve Vision-Language
tasks through In-Context Learning (ICL). After observing that configuring an
ICD sequence is a mirror process of composing a sentence, i.e., just as a
sentence can be composed word by word via a Language Model, an ICD sequence can
also be configured one by one. Consequently, we introduce an ICD Language Model
(ICD-LM) specifically designed to generate effective ICD sequences. This
involves creating a dataset of hand-crafted ICD sequences for various query
samples and using it to train the ICD-LM. Our approach, diverging from
traditional methods in NLP that select and order ICDs separately, enables to
simultaneously learn how to select and order ICDs, enhancing the effect of the
sequences. Moreover, during data construction, we use the LVLM intended for ICL
implementation to validate the strength of each ICD sequence, resulting in a
model-specific dataset and the ICD-LM trained by this dataset is also
model-specific. We validate our methodology through experiments in Visual
Question Answering and Image Captioning, confirming the viability of using a
Language Model for ICD configuration. Our comprehensive ablation studies
further explore the impact of various dataset construction and ICD-LM
development settings on the outcomes. The code is given in
https://github.com/ForJadeForest/ICD-LM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10107">Towards Context-Aware Domain Generalization: Representing Environments with Permutation-Invariant Networks. (arXiv:2312.10107v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1">Jens M&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuhmichel_L/0/1/0/all/0/1">Lars K&#xfc;hmichel</a>, <a href="http://arxiv.org/find/cs/1/au:+Rohbeck_M/0/1/0/all/0/1">Martin Rohbeck</a>, <a href="http://arxiv.org/find/cs/1/au:+Radev_S/0/1/0/all/0/1">Stefan T. Radev</a>, <a href="http://arxiv.org/find/cs/1/au:+Kothe_U/0/1/0/all/0/1">Ullrich K&#xf6;the</a></p>
<p>In this work, we show that information about the context of an input $X$ can
improve the predictions of deep learning models when applied in new domains or
production environments. We formalize the notion of context as a
permutation-invariant representation of a set of data points that originate
from the same environment/domain as the input itself. These representations are
jointly learned with a standard supervised learning objective, providing
incremental information about the unknown outcome. Furthermore, we offer a
theoretical analysis of the conditions under which our approach can, in
principle, yield benefits, and formulate two necessary criteria that can be
easily verified in practice. Additionally, we contribute insights into the kind
of distribution shifts for which our approach promises robustness. Our
empirical evaluation demonstrates the effectiveness of our approach for both
low-dimensional and high-dimensional data sets. Finally, we demonstrate that we
can reliably detect scenarios where a model is tasked with unwarranted
extrapolation in out-of-distribution (OOD) domains, identifying potential
failure cases. Consequently, we showcase a method to select between the most
predictive and the most robust model, circumventing the well-known trade-off
between predictive performance and robustness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10108">Privacy-Aware Document Visual Question Answering. (arXiv:2312.10108v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tito_R/0/1/0/all/0/1">Rub&#xe8;n Tito</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1">Khanh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tobaben_M/0/1/0/all/0/1">Marlon Tobaben</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerkouche_R/0/1/0/all/0/1">Raouf Kerkouche</a>, <a href="http://arxiv.org/find/cs/1/au:+Souibgui_M/0/1/0/all/0/1">Mohamed Ali Souibgui</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kangsoo Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1">Lei Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Valveny_E/0/1/0/all/0/1">Ernest Valveny</a>, <a href="http://arxiv.org/find/cs/1/au:+Honkela_A/0/1/0/all/0/1">Antti Honkela</a>, <a href="http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1">Mario Fritz</a>, <a href="http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1">Dimosthenis Karatzas</a></p>
<p>Document Visual Question Answering (DocVQA) is a fast growing branch of
document understanding. Despite the fact that documents contain sensitive or
copyrighted information, none of the current DocVQA methods offers strong
privacy guarantees.
</p>
<p>In this work, we explore privacy in the domain of DocVQA for the first time.
We highlight privacy issues in state of the art multi-modal LLM models used for
DocVQA, and explore possible solutions.
</p>
<p>Specifically, we focus on the invoice processing use case as a realistic,
widely used scenario for document understanding, and propose a large scale
DocVQA dataset comprising invoice documents and associated questions and
answers. We employ a federated learning scheme, that reflects the real-life
distribution of documents in different businesses, and we explore the use case
where the ID of the invoice issuer is the sensitive information to be
protected.
</p>
<p>We demonstrate that non-private models tend to memorise, behaviour that can
lead to exposing private information. We then evaluate baseline training
schemes employing federated learning and differential privacy in this
multi-modal scenario, where the sensitive information might be exposed through
any of the two input modalities: vision (document image) or language (OCR
tokens).
</p>
<p>Finally, we design an attack exploiting the memorisation effect of the model,
and demonstrate its effectiveness in probing different DocVQA models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10110">Enhancing Cognitive Diagnosis using Un-interacted Exercises: A Collaboration-aware Mixed Sampling Approach. (arXiv:2312.10110v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Haiping Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Changqian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hengshu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1">Shangshang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaoming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xingyi Zhang</a></p>
<p>Cognitive diagnosis is a crucial task in computational education, aimed at
evaluating students' proficiency levels across various knowledge concepts
through exercises. Current models, however, primarily rely on students'
answered exercises, neglecting the complex and rich information contained in
un-interacted exercises. While recent research has attempted to leverage the
data within un-interacted exercises linked to interacted knowledge concepts,
aiming to address the long-tail issue, these studies fail to fully explore the
informative, un-interacted exercises related to broader knowledge concepts.
This oversight results in diminished performance when these models are applied
to comprehensive datasets. In response to this gap, we present the
Collaborative-aware Mixed Exercise Sampling (CMES) framework, which can
effectively exploit the information present in un-interacted exercises linked
to un-interacted knowledge concepts. Specifically, we introduce a novel
universal sampling module where the training samples comprise not merely raw
data slices, but enhanced samples generated by combining weight-enhanced
attention mixture techniques. Given the necessity of real response labels in
cognitive diagnosis, we also propose a ranking-based pseudo feedback module to
regulate students' responses on generated exercises. The versatility of the
CMES framework bolsters existing models and improves their adaptability.
Finally, we demonstrate the effectiveness and interpretability of our framework
through comprehensive experiments on real-world datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10112">NM-FlowGAN: Modeling sRGB Noise with a Hybrid Approach based on Normalizing Flows and Generative Adversarial Networks. (arXiv:2312.10112v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Young Joo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Ha-Jin Yu</a></p>
<p>Modeling and synthesizing real sRGB noise is crucial for various low-level
vision tasks. The distribution of real sRGB noise is highly complex and
affected by a multitude of factors, making its accurate modeling extremely
challenging. Therefore, recent studies have proposed methods that employ
data-driven generative models, such as generative adversarial networks (GAN)
and Normalizing Flows. These studies achieve more accurate modeling of sRGB
noise compared to traditional noise modeling methods. However, there are
performance limitations due to the inherent characteristics of each generative
model. To address this issue, we propose NM-FlowGAN, a hybrid approach that
exploits the strengths of both GAN and Normalizing Flows. We simultaneously
employ a pixel-wise noise modeling network based on Normalizing Flows, and
spatial correlation modeling networks based on GAN. In our experiments, our
NM-FlowGAN outperforms other baselines on the sRGB noise synthesis task.
Moreover, the denoising neural network, trained with synthesized image pairs
from our model, also shows superior performance compared to other baselines.
Our code is available at: https://github.com/YoungJooHan/NM-FlowGAN
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10116">Bayesian Estimate of Mean Proper Scores for Diversity-Enhanced Active Learning. (arXiv:2312.10116v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1">Wei Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1">Lan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1">Wray Buntine</a></p>
<p>The effectiveness of active learning largely depends on the sampling
efficiency of the acquisition function. Expected Loss Reduction (ELR) focuses
on a Bayesian estimate of the reduction in classification error, and more
general costs fit in the same framework. We propose Bayesian Estimate of Mean
Proper Scores (BEMPS) to estimate the increase in strictly proper scores such
as log probability or negative mean square error within this framework. We also
prove convergence results for this general class of costs. To facilitate better
experimentation with the new acquisition functions, we develop a complementary
batch AL algorithm that encourages diversity in the vector of expected changes
in scores for unlabeled data. To allow high-performance classifiers, we combine
deep ensembles, and dynamic validation set construction on pretrained models,
and further speed up the ensemble process with the idea of Monte Carlo Dropout.
Extensive experiments on both texts and images show that the use of mean square
error and log probability with BEMPS yields robust acquisition functions and
well-calibrated classifiers, and consistently outperforms the others tested.
The advantages of BEMPS over the others are further supported by a set of
qualitative analyses, where we visualise their sampling behaviour using data
maps and t-SNE plots.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10127">How Does It Function? Characterizing Long-term Trends in Production Serverless Workloads. (arXiv:2312.10127v1 [cs.PF])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Joosen_A/0/1/0/all/0/1">Artjom Joosen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassan_A/0/1/0/all/0/1">Ahmed Hassan</a>, <a href="http://arxiv.org/find/cs/1/au:+Asenov_M/0/1/0/all/0/1">Martin Asenov</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1">Rajkarn Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Darlow_L/0/1/0/all/0/1">Luke Darlow</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Barker_A/0/1/0/all/0/1">Adam Barker</a></p>
<p>This paper releases and analyzes two new Huawei cloud serverless traces. The
traces span a period of over 7 months with over 1.4 trillion function
invocations combined. The first trace is derived from Huawei's internal
workloads and contains detailed per-second statistics for 200 functions running
across multiple Huawei cloud data centers. The second trace is a representative
workload from Huawei's public FaaS platform. This trace contains per-minute
arrival rates for over 5000 functions running in a single Huawei data center.
We present the internals of a production FaaS platform by characterizing
resource consumption, cold-start times, programming languages used,
periodicity, per-second versus per-minute burstiness, correlations, and
popularity. Our findings show that there is considerable diversity in how
serverless functions behave: requests vary by up to 9 orders of magnitude
across functions, with some functions executed over 1 billion times per day;
scheduling time, execution time and cold-start distributions vary across 2 to 4
orders of magnitude and have very long tails; and function invocation counts
demonstrate strong periodicity for many individual functions and on an
aggregate level. Our analysis also highlights the need for further research in
estimating resource reservations and time-series prediction to account for the
huge diversity in how serverless functions behave.
</p>
<p>Datasets and code available at https://github.com/sir-lab/data-release
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10128">An Information-Flow Perspective on Algorithmic Fairness. (arXiv:2312.10128v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teuber_S/0/1/0/all/0/1">Samuel Teuber</a>, <a href="http://arxiv.org/find/cs/1/au:+Beckert_B/0/1/0/all/0/1">Bernhard Beckert</a></p>
<p>This work presents insights gained by investigating the relationship between
algorithmic fairness and the concept of secure information flow. The problem of
enforcing secure information flow is well-studied in the context of information
security: If secret information may "flow" through an algorithm or program in
such a way that it can influence the program's output, then that is considered
insecure information flow as attackers could potentially observe (parts of) the
secret.
</p>
<p>There is a strong correspondence between secure information flow and
algorithmic fairness: if protected attributes such as race, gender, or age are
treated as secret program inputs, then secure information flow means that these
``secret'' attributes cannot influence the result of a program. While most
research in algorithmic fairness evaluation concentrates on studying the impact
of algorithms (often treating the algorithm as a black-box), the concepts
derived from information flow can be used both for the analysis of disparate
treatment as well as disparate impact w.r.t. a structural causal model.
</p>
<p>In this paper, we examine the relationship between quantitative as well as
qualitative information-flow properties and fairness. Moreover, based on this
duality, we derive a new quantitative notion of fairness called fairness
spread, which can be easily analyzed using quantitative information flow and
which strongly relates to counterfactual fairness. We demonstrate that
off-the-shelf tools for information-flow properties can be used in order to
formally analyze a program's algorithmic fairness properties, including the new
notion of fairness spread as well as established notions such as demographic
parity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10130">Improving new physics searches with diffusion models for event observables and jet constituents. (arXiv:2312.10130v1 [physics.data-an])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Sengupta_D/0/1/0/all/0/1">Debajyoti Sengupta</a>, <a href="http://arxiv.org/find/physics/1/au:+Leigh_M/0/1/0/all/0/1">Matthew Leigh</a>, <a href="http://arxiv.org/find/physics/1/au:+Raine_J/0/1/0/all/0/1">John Andrew Raine</a>, <a href="http://arxiv.org/find/physics/1/au:+Klein_S/0/1/0/all/0/1">Samuel Klein</a>, <a href="http://arxiv.org/find/physics/1/au:+Golling_T/0/1/0/all/0/1">Tobias Golling</a></p>
<p>We introduce a new technique called Drapes to enhance the sensitivity in
searches for new physics at the LHC. By training diffusion models on side-band
data, we show how background templates for the signal region can be generated
either directly from noise, or by partially applying the diffusion process to
existing data. In the partial diffusion case, data can be drawn from side-band
regions, with the inverse diffusion performed for new target conditional
values, or from the signal region, preserving the distribution over the
conditional property that defines the signal region. We apply this technique to
the hunt for resonances using the LHCO di-jet dataset, and achieve
state-of-the-art performance for background template generation using high
level input features. We also show how Drapes can be applied to low level
inputs with jet constituents, reducing the model dependence on the choice of
input observables. Using jet constituents we can further improve sensitivity to
the signal process, but observe a loss in performance where the signal
significance before applying any selection is below 4$\sigma$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10132">Closing the Gap: Achieving Better Accuracy-Robustness Tradeoffs Against Query-Based Attacks. (arXiv:2312.10132v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zimmer_P/0/1/0/all/0/1">Pascal Zimmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Andreina_S/0/1/0/all/0/1">S&#xe9;bastien Andreina</a>, <a href="http://arxiv.org/find/cs/1/au:+Marson_G/0/1/0/all/0/1">Giorgia Azzurra Marson</a>, <a href="http://arxiv.org/find/cs/1/au:+Karame_G/0/1/0/all/0/1">Ghassan Karame</a></p>
<p>Although promising, existing defenses against query-based attacks share a
common limitation: they offer increased robustness against attacks at the price
of a considerable accuracy drop on clean samples. In this work, we show how to
efficiently establish, at test-time, a solid tradeoff between robustness and
accuracy when mitigating query-based attacks. Given that these attacks
necessarily explore low-confidence regions, our insight is that activating
dedicated defenses, such as RND (Qin et al., NeuRIPS 2021) and Random Image
Transformations (Xie et al., ICLR 2018), only for low-confidence inputs is
sufficient to prevent them. Our approach is independent of training and
supported by theory. We verify the effectiveness of our approach for various
existing defenses by conducting extensive experiments on CIFAR-10, CIFAR-100,
and ImageNet. Our results confirm that our proposal can indeed enhance these
defenses by providing better tradeoffs between robustness and accuracy when
compared to state-of-the-art approaches while being completely training-free.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10144">Data-Efficient Multimodal Fusion on a Single GPU. (arXiv:2312.10144v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vouitsis_N/0/1/0/all/0/1">No&#xeb;l Vouitsis</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhaoyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorti_S/0/1/0/all/0/1">Satya Krishna Gorti</a>, <a href="http://arxiv.org/find/cs/1/au:+Villecroze_V/0/1/0/all/0/1">Valentin Villecroze</a>, <a href="http://arxiv.org/find/cs/1/au:+Cresswell_J/0/1/0/all/0/1">Jesse C. Cresswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Guangwei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Loaiza_Ganem_G/0/1/0/all/0/1">Gabriel Loaiza-Ganem</a>, <a href="http://arxiv.org/find/cs/1/au:+Volkovs_M/0/1/0/all/0/1">Maksims Volkovs</a></p>
<p>The goal of multimodal alignment is to learn a single latent space that is
shared between multimodal inputs. The most powerful models in this space have
been trained using massive datasets of paired inputs and large-scale
computational resources, making them prohibitively expensive to train in many
practical scenarios. We surmise that existing unimodal encoders pre-trained on
large amounts of unimodal data should provide an effective bootstrap to create
multimodal models from unimodal ones at much lower costs. We therefore propose
FuseMix, a multimodal augmentation scheme that operates on the latent spaces of
arbitrary pre-trained unimodal encoders. Using FuseMix for multimodal
alignment, we achieve competitive performance -- and in certain cases
outperform state-of-the art methods -- in both image-text and audio-text
retrieval, with orders of magnitude less compute and data: for example, we
outperform CLIP on the Flickr30K text-to-image retrieval task with $\sim \!
600\times$ fewer GPU days and $\sim \! 80\times$ fewer image-text pairs.
Additionally, we show how our method can be applied to convert pre-trained
text-to-image generative models into audio-to-image ones. Code is available at:
https://github.com/layer6ai-labs/fusemix.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10153">Bayesian Metaplasticity from Synaptic Uncertainty. (arXiv:2312.10153v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bonnet_D/0/1/0/all/0/1">Djohan Bonnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Hirtzlin_T/0/1/0/all/0/1">Tifenn Hirtzlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Januel_T/0/1/0/all/0/1">Tarcisius Januel</a>, <a href="http://arxiv.org/find/cs/1/au:+Dalgaty_T/0/1/0/all/0/1">Thomas Dalgaty</a>, <a href="http://arxiv.org/find/cs/1/au:+Querlioz_D/0/1/0/all/0/1">Damien Querlioz</a>, <a href="http://arxiv.org/find/cs/1/au:+Vianello_E/0/1/0/all/0/1">Elisa Vianello</a></p>
<p>Catastrophic forgetting remains a challenge for neural networks, especially
in lifelong learning scenarios. In this study, we introduce MEtaplasticity from
Synaptic Uncertainty (MESU), inspired by metaplasticity and Bayesian inference
principles. MESU harnesses synaptic uncertainty to retain information over
time, with its update rule closely approximating the diagonal Newton's method
for synaptic updates. Through continual learning experiments on permuted MNIST
tasks, we demonstrate MESU's remarkable capability to maintain learning
performance across 100 tasks without the need of explicit task boundaries.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10163">Towards the Unification of Generative and Discriminative Visual Foundation Model: A Survey. (arXiv:2312.10163v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuanxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuping Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qinjingwen Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1">Weizhi Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yonghuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Junjun He</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1">Yiqing Shen</a></p>
<p>The advent of foundation models, which are pre-trained on vast datasets, has
ushered in a new era of computer vision, characterized by their robustness and
remarkable zero-shot generalization capabilities. Mirroring the transformative
impact of foundation models like large language models (LLMs) in natural
language processing, visual foundation models (VFMs) have become a catalyst for
groundbreaking developments in computer vision. This review paper delineates
the pivotal trajectories of VFMs, emphasizing their scalability and proficiency
in generative tasks such as text-to-image synthesis, as well as their adeptness
in discriminative tasks including image segmentation. While generative and
discriminative models have historically charted distinct paths, we undertake a
comprehensive examination of the recent strides made by VFMs in both domains,
elucidating their origins, seminal breakthroughs, and pivotal methodologies.
Additionally, we collate and discuss the extensive resources that facilitate
the development of VFMs and address the challenges that pave the way for future
research endeavors. A crucial direction for forthcoming innovation is the
amalgamation of generative and discriminative paradigms. The nascent
application of generative models within discriminative contexts signifies the
early stages of this confluence. This survey aspires to be a contemporary
compendium for scholars and practitioners alike, charting the course of VFMs
and illuminating their multifaceted landscape.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10179">3FM: Multi-modal Meta-learning for Federated Tasks. (arXiv:2312.10179v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1">Minh Tran</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Roochi Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zejun Gong</a></p>
<p>We present a novel approach in the domain of federated learning (FL),
particularly focusing on addressing the challenges posed by modality
heterogeneity, variability in modality availability across clients, and the
prevalent issue of missing data. We introduce a meta-learning framework
specifically designed for multimodal federated tasks. Our approach is motivated
by the need to enable federated models to robustly adapt when exposed to new
modalities, a common scenario in FL where clients often differ in the number of
available modalities. The effectiveness of our proposed framework is
demonstrated through extensive experimentation on an augmented MNIST dataset,
enriched with audio and sign language data. We demonstrate that the proposed
algorithm achieves better performance than the baseline on a subset of missing
modality scenarios with careful tuning of the meta-learning rates. This is a
shortened report, and our work will be extended and updated soon.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10181">Coupling Fairness and Pruning in a Single Run: a Bi-level Optimization Perspective. (arXiv:2312.10181v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yucong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Feng Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaolong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yongkai Wu</a></p>
<p>Deep neural networks have demonstrated remarkable performance in various
tasks. With a growing need for sparse deep learning, model compression
techniques, especially pruning, have gained significant attention. However,
conventional pruning techniques can inadvertently exacerbate algorithmic bias,
resulting in unequal predictions. To address this, we define a fair pruning
task where a sparse model is derived subject to fairness requirements. In
particular, we propose a framework to jointly optimize the pruning mask and
weight update processes with fairness constraints. This framework is engineered
to compress models that maintain performance while ensuring fairness in a
single execution. To this end, we formulate the fair pruning problem as a novel
constrained bi-level optimization task and derive efficient and effective
solving strategies. We design experiments spanning various datasets and
settings to validate our proposed method. Our empirical analysis contrasts our
framework with several mainstream pruning strategies, emphasizing our method's
superiority in maintaining model fairness, performance, and efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10185">Student as an Inherent Denoiser of Noisy Teacher. (arXiv:2312.10185v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiachen Zhao</a></p>
<p>Knowledge distillation (KD) has been widely employed to transfer knowledge
from a large language model (LLM) to a specialized model in low-data regimes
through pseudo label learning. However, pseudo labels generated by teacher
models are usually noisy and may influence KD performance. This study delves
into KD with noisy teachers and uncovers that the student model can already
generate more accurate predictions than the teacher labels used to train it
during KD, indicating its inherent ability to denoise noisy teacher labels.
Motivated by this finding, we propose Peer-Advised KD to improve vanilla KD
from noisy teachers. Experiments show that Peer-Advised KD can outperform LLM
by approximately 5% with 50 human-labeled data, and even competitive to
standard supervised finetuning with 750 human-labeled data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10187">TSRNet: Simple Framework for Real-time ECG Anomaly Detection with Multimodal Time and Spectrogram Restoration Network. (arXiv:2312.10187v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bui_N/0/1/0/all/0/1">Nhat-Tan Bui</a>, <a href="http://arxiv.org/find/eess/1/au:+Hoang_D/0/1/0/all/0/1">Dinh-Hieu Hoang</a>, <a href="http://arxiv.org/find/eess/1/au:+Phan_T/0/1/0/all/0/1">Thinh Phan</a>, <a href="http://arxiv.org/find/eess/1/au:+Tran_M/0/1/0/all/0/1">Minh-Triet Tran</a>, <a href="http://arxiv.org/find/eess/1/au:+Patel_B/0/1/0/all/0/1">Brijesh Patel</a>, <a href="http://arxiv.org/find/eess/1/au:+Adjeroh_D/0/1/0/all/0/1">Donald Adjeroh</a>, <a href="http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a></p>
<p>The electrocardiogram (ECG) is a valuable signal used to assess various
aspects of heart health, such as heart rate and rhythm. It plays a crucial role
in identifying cardiac conditions and detecting anomalies in ECG data. However,
distinguishing between normal and abnormal ECG signals can be a challenging
task. In this paper, we propose an approach that leverages anomaly detection to
identify unhealthy conditions using solely normal ECG data for training.
Furthermore, to enhance the information available and build a robust system, we
suggest considering both the time series and time-frequency domain aspects of
the ECG signal. As a result, we introduce a specialized network called the
Multimodal Time and Spectrogram Restoration Network (TSRNet) designed
specifically for detecting anomalies in ECG signals. TSRNet falls into the
category of restoration-based anomaly detection and draws inspiration from both
the time series and spectrogram domains. By extracting representations from
both domains, TSRNet effectively captures the comprehensive characteristics of
the ECG signal. This approach enables the network to learn robust
representations with superior discrimination abilities, allowing it to
distinguish between normal and abnormal ECG patterns more effectively.
Furthermore, we introduce a novel inference method, termed Peak-based Error,
that specifically focuses on ECG peaks, a critical component in detecting
abnormalities. The experimental result on the large-scale dataset PTB-XL has
demonstrated the effectiveness of our approach in ECG anomaly detection, while
also prioritizing efficiency by minimizing the number of trainable parameters.
Our code is available at https://github.com/UARK-AICV/TSRNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10188">WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data. (arXiv:2312.10188v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weber_M/0/1/0/all/0/1">Maurice Weber</a>, <a href="http://arxiv.org/find/cs/1/au:+Siebenschuh_C/0/1/0/all/0/1">Carlo Siebenschuh</a>, <a href="http://arxiv.org/find/cs/1/au:+Butler_R/0/1/0/all/0/1">Rory Butler</a>, <a href="http://arxiv.org/find/cs/1/au:+Alexandrov_A/0/1/0/all/0/1">Anton Alexandrov</a>, <a href="http://arxiv.org/find/cs/1/au:+Thanner_V/0/1/0/all/0/1">Valdemar Thanner</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsolakis_G/0/1/0/all/0/1">Georgios Tsolakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Jabbar_H/0/1/0/all/0/1">Haris Jabbar</a>, <a href="http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1">Ian Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevens_R/0/1/0/all/0/1">Rick Stevens</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Ce Zhang</a></p>
<p>We introduce WordScape, a novel pipeline for the creation of
cross-disciplinary, multilingual corpora comprising millions of pages with
annotations for document layout detection. Relating visual and textual items on
document pages has gained further significance with the advent of multimodal
models. Various approaches proved effective for visual question answering or
layout segmentation. However, the interplay of text, tables, and visuals
remains challenging for a variety of document understanding tasks. In
particular, many models fail to generalize well to diverse domains and new
languages due to insufficient availability of training data. WordScape
addresses these limitations. Our automatic annotation pipeline parses the Open
XML structure of Word documents obtained from the web, jointly providing
layout-annotated document images and their textual representations. In turn,
WordScape offers unique properties as it (1) leverages the ubiquity of the Word
file format on the internet, (2) is readily accessible through the Common Crawl
web corpus, (3) is adaptive to domain-specific documents, and (4) offers
culturally and linguistically diverse document pages with natural semantic
structure and high-quality text. Together with the pipeline, we will
additionally release 9.5M urls to word documents which can be processed using
WordScape to create a dataset of over 40M pages. Finally, we investigate the
quality of text and layout annotations extracted by WordScape, assess the
impact on document understanding benchmarks, and demonstrate that manual
labeling costs can be substantially reduced.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10193">Adaptive Computation Modules: Granular Conditional Computation For Efficient Inference. (arXiv:2312.10193v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wojcik_B/0/1/0/all/0/1">Bartosz W&#xf3;jcik</a>, <a href="http://arxiv.org/find/cs/1/au:+Devoto_A/0/1/0/all/0/1">Alessio Devoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Pustelnik_K/0/1/0/all/0/1">Karol Pustelnik</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Scardapane_S/0/1/0/all/0/1">Simone Scardapane</a></p>
<p>The computational cost of transformer models makes them inefficient in
low-latency or low-power applications. While techniques such as quantization or
linear attention can reduce the computational load, they may incur a reduction
in accuracy. In addition, globally reducing the cost for all inputs may be
sub-optimal. We observe that for each layer, the full width of the layer may be
needed only for a small subset of tokens inside a batch and that the
"effective" width needed to process a token can vary from layer to layer.
Motivated by this observation, we introduce the Adaptive Computation Module
(ACM), a generic module that dynamically adapts its computational load to match
the estimated difficulty of the input on a per-token basis. An ACM consists of
a sequence of learners that progressively refine the output of their preceding
counterparts. An additional gating mechanism determines the optimal number of
learners to execute for each token. We also describe a distillation technique
to replace any pre-trained model with an "ACMized" variant. The distillation
phase is designed to be highly parallelizable across layers while being simple
to plug-and-play into existing networks. Our evaluation of transformer models
in computer vision and speech recognition demonstrates that substituting layers
with ACMs significantly reduces inference costs without degrading the
downstream accuracy for a wide interval of user-defined budgets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10194">Pareto Envelope Augmented with Reinforcement Learning: Multi-objective reinforcement learning-based approach for Large-Scale Constrained Pressurized Water Reactor optimization. (arXiv:2312.10194v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Seurin_P/0/1/0/all/0/1">Paul Seurin</a>, <a href="http://arxiv.org/find/cs/1/au:+Seurin_K/0/1/0/all/0/1">Koroush Seurin</a></p>
<p>A novel method, the Pareto Envelope Augmented with Reinforcement Learning
(PEARL), has been developed to address the challenges posed by multi-objective
problems, particularly in the field of engineering where the evaluation of
candidate solutions can be time-consuming. PEARL distinguishes itself from
traditional policy-based multi-objective Reinforcement Learning methods by
learning a single policy, eliminating the need for multiple neural networks to
independently solve simpler sub-problems. Several versions inspired from deep
learning and evolutionary techniques have been crafted, catering to both
unconstrained and constrained problem domains. Curriculum Learning is harnessed
to effectively manage constraints in these versions. PEARL's performance is
first evaluated on classical multi-objective benchmarks. Additionally, it is
tested on two practical PWR core Loading Pattern optimization problems to
showcase its real-world applicability. The first problem involves optimizing
the Cycle length and the rod-integrated peaking factor as the primary
objectives, while the second problem incorporates the mean average enrichment
as an additional objective. Furthermore, PEARL addresses three types of
constraints related to boron concentration, peak pin burnup, and peak pin
power. The results are systematically compared against a conventional approach,
the Non-dominated Sorting Genetic Algorithm. Notably, PEARL, specifically the
PEARL-NdS variant, efficiently uncovers a Pareto front without necessitating
additional efforts from the algorithm designer, as opposed to a single
optimization with scaled objectives. It also outperforms the classical approach
across multiple performance metrics, including the Hyper-volume.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10199">Automatic nonlinear MPC approximation with closed-loop guarantees. (arXiv:2312.10199v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Tokmak_A/0/1/0/all/0/1">Abdullah Tokmak</a>, <a href="http://arxiv.org/find/eess/1/au:+Fiedler_C/0/1/0/all/0/1">Christian Fiedler</a>, <a href="http://arxiv.org/find/eess/1/au:+Zeilinger_M/0/1/0/all/0/1">Melanie N. Zeilinger</a>, <a href="http://arxiv.org/find/eess/1/au:+Trimpe_S/0/1/0/all/0/1">Sebastian Trimpe</a>, <a href="http://arxiv.org/find/eess/1/au:+Kohler_J/0/1/0/all/0/1">Johannes K&#xf6;hler</a></p>
<p>In this paper, we address the problem of automatically approximating
nonlinear model predictive control (MPC) schemes with closed-loop guarantees.
First, we discuss how this problem can be reduced to a function approximation
problem, which we then tackle by proposing ALKIA-X, the Adaptive and Localized
Kernel Interpolation Algorithm with eXtrapolated reproducing kernel Hilbert
space norm. ALKIA-X is a non-iterative algorithm that ensures numerically
well-conditioned computations, a fast-to-evaluate approximating function, and
the guaranteed satisfaction of any desired bound on the approximation error.
Hence, ALKIA-X automatically computes an explicit function that approximates
the MPC, yielding a controller suitable for safety-critical systems and high
sampling rates. In a numerical experiment, we apply ALKIA-X to a nonlinear MPC
scheme, demonstrating reduced offline computation and online evaluation time
compared to a state-of-the-art method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10209">Beyond Empirical Windowing: An Attention-Based Approach for Trust Prediction in Autonomous Vehicles. (arXiv:2312.10209v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Niu_M/0/1/0/all/0/1">Minxue Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhaobo Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Akash_K/0/1/0/all/0/1">Kumar Akash</a>, <a href="http://arxiv.org/find/cs/1/au:+Misu_T/0/1/0/all/0/1">Teruhisa Misu</a></p>
<p>Humans' internal states play a key role in human-machine interaction, leading
to the rise of human state estimation as a prominent field. Compared to swift
state changes such as surprise and irritation, modeling gradual states like
trust and satisfaction are further challenged by label sparsity: long
time-series signals are usually associated with a single label, making it
difficult to identify the critical span of state shifts. Windowing has been one
widely-used technique to enable localized analysis of long time-series data.
However, the performance of downstream models can be sensitive to the window
size, and determining the optimal window size demands domain expertise and
extensive search. To address this challenge, we propose a Selective Windowing
Attention Network (SWAN), which employs window prompts and masked attention
transformation to enable the selection of attended intervals with flexible
lengths. We evaluate SWAN on the task of trust prediction on a new multimodal
driving simulation dataset. Experiments show that SWAN significantly
outperforms an existing empirical window selection baseline and neural network
baselines including CNN-LSTM and Transformer. Furthermore, it shows robustness
across a wide span of windowing ranges, compared to the traditional windowing
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10212">A Remark on Concept Drift for Dependent Data. (arXiv:2312.10212v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hinder_F/0/1/0/all/0/1">Fabian Hinder</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaquet_V/0/1/0/all/0/1">Valerie Vaquet</a>, <a href="http://arxiv.org/find/cs/1/au:+Hammer_B/0/1/0/all/0/1">Barbara Hammer</a></p>
<p>Concept drift, i.e., the change of the data generating distribution, can
render machine learning models inaccurate. Several works address the phenomenon
of concept drift in the streaming context usually assuming that consecutive
data points are independent of each other. To generalize to dependent data,
many authors link the notion of concept drift to time series. In this work, we
show that the temporal dependencies are strongly influencing the sampling
process. Thus, the used definitions need major modifications. In particular, we
show that the notion of stationarity is not suited for this setup and discuss
alternatives. We demonstrate that these alternative formal notions describe the
observable learning behavior in numerical experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10230">Constrained Meta-Reinforcement Learning for Adaptable Safety Guarantee with Differentiable Convex Programming. (arXiv:2312.10230v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1">Minjae Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chuangchuang Sun</a></p>
<p>Despite remarkable achievements in artificial intelligence, the deployability
of learning-enabled systems in high-stakes real-world environments still faces
persistent challenges. For example, in safety-critical domains like autonomous
driving, robotic manipulation, and healthcare, it is crucial not only to
achieve high performance but also to comply with given constraints.
Furthermore, adaptability becomes paramount in non-stationary domains, where
environmental parameters are subject to change. While safety and adaptability
are recognized as key qualities for the new generation of AI, current
approaches have not demonstrated effective adaptable performance in constrained
settings. Hence, this paper breaks new ground by studying the unique challenges
of ensuring safety in non-stationary environments by solving constrained
problems through the lens of the meta-learning approach (learning-to-learn).
While unconstrained meta-learning al-ready encounters complexities in
end-to-end differentiation of the loss due to the bi-level nature, its
constrained counterpart introduces an additional layer of difficulty, since the
constraints imposed on task-level updates complicate the differentiation
process. To address the issue, we first employ successive convex-constrained
policy updates across multiple tasks with differentiable convexprogramming,
which allows meta-learning in constrained scenarios by enabling end-to-end
differentiation. This approach empowers the agent to rapidly adapt to new tasks
under non-stationarity while ensuring compliance with safety constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10235">Building symmetries into data-driven manifold dynamics models for complex flows. (arXiv:2312.10235v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jesus_C/0/1/0/all/0/1">Carlos E. P&#xe9;rez De Jes&#xfa;s</a>, <a href="http://arxiv.org/find/cs/1/au:+Linot_A/0/1/0/all/0/1">Alec J. Linot</a>, <a href="http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1">Michael D. Graham</a></p>
<p>Symmetries in a dynamical system provide an opportunity to dramatically
improve the performance of data-driven models. For fluid flows, such models are
needed for tasks related to design, understanding, prediction, and control. In
this work we exploit the symmetries of the Navier-Stokes equations (NSE) and
use simulation data to find the manifold where the long-time dynamics live,
which has many fewer degrees of freedom than the full state representation, and
the evolution equation for the dynamics on that manifold. We call this method
''symmetry charting''. The first step is to map to a ''fundamental chart'',
which is a region in the state space of the flow to which all other regions can
be mapped by a symmetry operation. To map to the fundamental chart we identify
a set of indicators from the Fourier transform that uniquely identify the
symmetries of the system. We then find a low-dimensional coordinate
representation of the data in the fundamental chart with the use of an
autoencoder. We use a variation called an implicit rank minimizing autoencoder
with weight decay, which in addition to compressing the dimension of the data,
also gives estimates of how many dimensions are needed to represent the data:
i.e. the dimension of the invariant manifold of the long-time dynamics.
Finally, we learn dynamics on this manifold with the use of neural ordinary
differential equations. We apply symmetry charting to two-dimensional
Kolmogorov flow in a chaotic bursting regime. This system has a continuous
translation symmetry, and discrete rotation and shift-reflect symmetries. With
this framework we observe that less data is needed to learn accurate
data-driven models, more robust estimates of the manifold dimension are
obtained, equivariance of the NSE is satisfied, better short-time tracking with
respect to the true data is observed, and long-time statistics are correctly
captured.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2009.00203">Efficient, Direct, and Restricted Black-Box Graph Evasion Attacks to Any-Layer Graph Neural Networks via Influence Function. (arXiv:2009.00203v3 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Binghui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1">Tianxiang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Minhua Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Ang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_M/0/1/0/all/0/1">Meng Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yiran Chen</a></p>
<p>Graph neural network (GNN), the mainstream method to learn on graph data, is
vulnerable to graph evasion attacks, where an attacker slightly perturbing the
graph structure can fool trained GNN models. Existing work has at least one of
the following drawbacks: 1) limited to directly attack two-layer GNNs; 2)
inefficient; and 3) impractical, as they need to know full or part of GNN model
parameters.
</p>
<p>We address the above drawbacks and propose an influence-based
\emph{efficient, direct, and restricted black-box} evasion attack to
\emph{any-layer} GNNs. Specifically, we first introduce two influence
functions, i.e., feature-label influence and label influence, that are defined
on GNNs and label propagation (LP), respectively. Then we observe that GNNs and
LP are strongly connected in terms of our defined influences. Based on this, we
can then reformulate the evasion attack to GNNs as calculating label influence
on LP, which is \emph{inherently} applicable to any-layer GNNs, while no need
to know information about the internal GNN model. Finally, we propose an
efficient algorithm to calculate label influence. Experimental results on
various graph datasets show that, compared to state-of-the-art white-box
attacks, our attack can achieve comparable attack performance, but has a 5-50x
speedup when attacking two-layer GNNs. Moreover, our attack is effective to
attack multi-layer GNNs\footnote{Source code and full version is in the link:
\url{https://github.com/ventr1c/InfAttack}}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.05152">Rethinking Transfer Learning for Medical Image Classification. (arXiv:2106.05152v7 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Peng_L/0/1/0/all/0/1">Le Peng</a>, <a href="http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1">Hengyue Liang</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_G/0/1/0/all/0/1">Gaoxiang Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_T/0/1/0/all/0/1">Taihui Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Sun_J/0/1/0/all/0/1">Ju Sun</a></p>
<p>Transfer learning (TL) from pretrained deep models is a standard practice in
modern medical image classification (MIC). However, what levels of features to
be reused are problem-dependent, and uniformly finetuning all layers of
pretrained models may be suboptimal. This insight has partly motivated the
recent differential TL strategies, such as TransFusion (TF) and layer-wise
finetuning (LWFT), which treat the layers in the pretrained models
differentially. In this paper, we add one more strategy into this family,
called TruncatedTL, which reuses and finetunes appropriate bottom layers and
directly discards the remaining layers. This yields not only superior MIC
performance but also compact models for efficient inference, compared to other
differential TL methods. Our code is available at:
https://github.com/sun-umn/TTL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2109.05075">On the Compression of Neural Networks Using $\ell_0$-Norm Regularization and Weight Pruning. (arXiv:2109.05075v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1">Felipe Dennis de Resende Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Batista_E/0/1/0/all/0/1">Eduardo Luiz Ortiz Batista</a>, <a href="http://arxiv.org/find/cs/1/au:+Seara_R/0/1/0/all/0/1">Rui Seara</a></p>
<p>Despite the growing availability of high-capacity computational platforms,
implementation complexity still has been a great concern for the real-world
deployment of neural networks. This concern is not exclusively due to the huge
costs of state-of-the-art network architectures, but also due to the recent
push towards edge intelligence and the use of neural networks in embedded
applications. In this context, network compression techniques have been gaining
interest due to their ability for reducing deployment costs while keeping
inference accuracy at satisfactory levels. The present paper is dedicated to
the development of a novel compression scheme for neural networks. To this end,
a new form of $\ell_0$-norm-based regularization is firstly developed, which is
capable of inducing strong sparseness in the network during training. Then,
targeting the smaller weights of the trained network with pruning techniques,
smaller yet highly effective networks can be obtained. The proposed compression
scheme also involves the use of $\ell_2$-norm regularization to avoid
overfitting as well as fine tuning to improve the performance of the pruned
network. Experimental results are presented aiming to show the effectiveness of
the proposed scheme as well as to make comparisons with competing approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.05216">High-order Tensor Pooling with Attention for Action Recognition. (arXiv:2110.05216v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Ke Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1">Piotr Koniusz</a></p>
<p>We aim at capturing high-order statistics of feature vectors formed by a
neural network, and propose end-to-end second- and higher-order pooling to form
a tensor descriptor. Tensor descriptors require a robust similarity measure due
to low numbers of aggregated vectors and the burstiness phenomenon, when a
given feature appears more/less frequently than statistically expected. The
Heat Diffusion Process (HDP) on a graph Laplacian is closely related to the
Eigenvalue Power Normalization (EPN) of the covariance/autocorrelation matrix,
whose inverse forms a loopy graph Laplacian. We show that the HDP and the EPN
play the same role, i.e., to boost or dampen the magnitude of the eigenspectrum
thus preventing the burstiness. We equip higher-order tensors with EPN which
acts as a spectral detector of higher-order occurrences to prevent burstiness.
We also prove that for a tensor of order r built from d dimensional feature
descriptors, such a detector gives the likelihood if at least one higher-order
occurrence is 'projected' into one of binom(d,r) subspaces represented by the
tensor; thus forming a tensor power normalization metric endowed with
binom(d,r) such 'detectors'. For experimental contributions, we apply several
second- and higher-order pooling variants to action recognition, provide
previously not presented comparisons of such pooling variants, and show
state-of-the-art results on HMDB-51, YUP++ and MPII Cooking Activities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.11279">Testing Relative Fairness in Human Decisions With Machine Learning. (arXiv:2112.11279v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhe Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xi_X/0/1/0/all/0/1">Xiaoyin Xi</a></p>
<p>Fairness in decision-making has been a long-standing issue in our society.
Compared to algorithmic fairness, fairness in human decisions is even more
important since there are processes where humans make the final decisions and
that machine learning models inherit bias from the human decisions they were
trained on. However, the standard for fairness in human decisions are highly
subjective and contextual. This leads to the difficulty for testing "absolute"
fairness in human decisions. To bypass this issue, this work aims to test
relative fairness in human decisions. That is, instead of defining what are
"absolute" fair decisions, we check the relative fairness of one decision set
against another. An example outcome can be: Decision Set A favors female over
male more than Decision Set B. Such relative fairness has the following
benefits: (1) it avoids the ambiguous and contradictory definition of
"absolute" fair decisions; (2) it reveals the relative preference and bias
between different human decisions; (3) if a reference set of decisions is
provided, relative fairness of other decision sets against this reference set
can reflect whether those decision sets are fair by the standard of that
reference set. We define the relative fairness with statistical tests (null
hypothesis and effect size tests) of the decision differences across each
sensitive group. Furthermore, we show that a machine learning model trained on
the human decisions can inherit the bias/preference and therefore can be
utilized to estimate the relative fairness between two decision sets made on
different data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.00087">Persistent Homological State-Space Estimation of Functional Human Brain Networks at Rest. (arXiv:2201.00087v4 [math.AT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Chung_M/0/1/0/all/0/1">Moo K. Chung</a>, <a href="http://arxiv.org/find/math/1/au:+Huang_S/0/1/0/all/0/1">Shih-Gu Huang</a>, <a href="http://arxiv.org/find/math/1/au:+Carroll_I/0/1/0/all/0/1">Ian C. Carroll</a>, <a href="http://arxiv.org/find/math/1/au:+Calhoun_V/0/1/0/all/0/1">Vince D. Calhoun</a>, <a href="http://arxiv.org/find/math/1/au:+Goldsmith_H/0/1/0/all/0/1">H. Hill Goldsmith</a></p>
<p>We present a new data driven topological data analysis (TDA) approach for
estimating state spaces in dynamically changing human functional brain networks
of human. Our approach penalizes the topological distance between networks and
clusters dynamically changing brain networks into topologically distinct
states. Our method takes into account the temporal dimension of the data
through the Wasserstein distance between networks. Our method is shown to
outperform the widely used k-means clustering often used in estimating the
state space in brain networks. The method is applied to more accurately
determine the state spaces of dynamically changing functional brain networks.
Subsequently, we address the question of whether the overall topology of brain
networks is a heritable feature using the twin study design. MATLAB code for
the method is available at https://github.com/laplcebeltrami/PH-STAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.03281">IoTGAN: GAN Powered Camouflage Against Machine Learning Based IoT Device Identification. (arXiv:2201.03281v2 [cs.CR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1">Tao Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhuo Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1">Yalin Sagduyu</a></p>
<p>With the proliferation of IoT devices, researchers have developed a variety
of IoT device identification methods with the assistance of machine learning.
Nevertheless, the security of these identification methods mostly depends on
collected training data. In this research, we propose a novel attack strategy
named IoTGAN to manipulate an IoT device's traffic such that it can evade
machine learning based IoT device identification. In the development of IoTGAN,
we have two major technical challenges: (i) How to obtain the discriminative
model in a black-box setting, and (ii) How to add perturbations to IoT traffic
through the manipulative model, so as to evade the identification while not
influencing the functionality of IoT devices. To address these challenges, a
neural network based substitute model is used to fit the target model in
black-box settings, it works as a discriminative model in IoTGAN. A
manipulative model is trained to add adversarial perturbations into the IoT
device's traffic to evade the substitute model. Experimental results show that
IoTGAN can successfully achieve the attack goals. We also develop efficient
countermeasures to protect machine learning based IoT device identification
from been undermined by IoTGAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.12433">FedGCN: Convergence-Communication Tradeoffs in Federated Training of Graph Convolutional Networks. (arXiv:2201.12433v7 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1">Yuhang Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1">Weizhao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravi_S/0/1/0/all/0/1">Srivatsan Ravi</a>, <a href="http://arxiv.org/find/cs/1/au:+Joe_Wong_C/0/1/0/all/0/1">Carlee Joe-Wong</a></p>
<p>Methods for training models on graphs distributed across multiple clients
have recently grown in popularity, due to the size of these graphs as well as
regulations on keeping data where it is generated. However, the cross-client
edges naturally exist among clients. Thus, distributed methods for training a
model on a single graph incur either significant communication overhead between
clients or a loss of available information to the training. We introduce the
Federated Graph Convolutional Network (FedGCN) algorithm, which uses federated
learning to train GCN models for semi-supervised node classification with fast
convergence and little communication. Compared to prior methods that require
extra communication among clients at each training round, FedGCN clients only
communicate with the central server in one pre-training step, greatly reducing
communication costs and allowing the use of homomorphic encryption to further
enhance privacy. We theoretically analyze the tradeoff between FedGCN's
convergence rate and communication cost under different data distributions.
Experimental results show that our FedGCN algorithm achieves better model
accuracy with 51.7% faster convergence on average and at least 100X less
communication compared to prior work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.12975">Rotting Infinitely Many-armed Bandits. (arXiv:2201.12975v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jung-hun Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Vojnovic_M/0/1/0/all/0/1">Milan Vojnovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1">Se-Young Yun</a></p>
<p>We consider the infinitely many-armed bandit problem with rotting rewards,
where the mean reward of an arm decreases at each pull of the arm according to
an arbitrary trend with maximum rotting rate $\varrho=o(1)$. We show that this
learning problem has an $\Omega(\max\{\varrho^{1/3}T,\sqrt{T}\})$ worst-case
regret lower bound where $T$ is the horizon time. We show that a matching upper
bound $\tilde{O}(\max\{\varrho^{1/3}T,\sqrt{T}\})$, up to a poly-logarithmic
factor, can be achieved by an algorithm that uses a UCB index for each arm and
a threshold value to decide whether to continue pulling an arm or remove the
arm from further consideration, when the algorithm knows the value of the
maximum rotting rate $\varrho$. We also show that an
$\tilde{O}(\max\{\varrho^{1/3}T,T^{3/4}\})$ regret upper bound can be achieved
by an algorithm that does not know the value of $\varrho$, by using an adaptive
UCB index along with an adaptive threshold value.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.06491">Adversarial Graph Contrastive Learning with Information Regularization. (arXiv:2202.06491v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1">Shengyu Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jing_B/0/1/0/all/0/1">Baoyu Jing</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yada Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1">Hanghang Tong</a></p>
<p>Contrastive learning is an effective unsupervised method in graph
representation learning. Recently, the data augmentation based contrastive
learning method has been extended from images to graphs. However, most prior
works are directly adapted from the models designed for images. Unlike the data
augmentation on images, the data augmentation on graphs is far less intuitive
and much harder to provide high-quality contrastive samples, which are the key
to the performance of contrastive learning models. This leaves much space for
improvement over the existing graph contrastive learning frameworks. In this
work, by introducing an adversarial graph view and an information regularizer,
we propose a simple but effective method, Adversarial Graph Contrastive
Learning (ARIEL), to extract informative contrastive samples within a
reasonable constraint. It consistently outperforms the current graph
contrastive learning methods in the node classification task over various
real-world datasets and further improves the robustness of graph contrastive
learning. The code is at https://github.com/Shengyu-Feng/ARIEL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.11342">Training Adaptive Reconstruction Networks for Blind Inverse Problems. (arXiv:2202.11342v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gossard_A/0/1/0/all/0/1">Alban Gossard</a> (IMT), <a href="http://arxiv.org/find/cs/1/au:+Weiss_P/0/1/0/all/0/1">Pierre Weiss</a> (IRIT, CBI)</p>
<p>Neural networks allow solving many ill-posed inverse problems with
unprecedented performance. Physics informed approaches already progressively
replace carefully hand-crafted reconstruction algorithms in real applications.
However, these networks suffer from a major defect: when trained on a given
forward operator, they do not generalize well to a different one. The aim of
this paper is twofold. First, we show through various applications that
training the network with a family of forward operators allows solving the
adaptivity problem without compromising the reconstruction quality
significantly.Second, we illustrate that this training procedure allows
tackling challenging blind inverse problems.Our experiments include partial
Fourier sampling problems arising in magnetic resonance imaging (MRI) with
sensitivity estimation and off-resonance effects, computerized tomography (CT)
with a tilted geometry and image deblurring with Fresnel diffraction kernels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.01682">Deep Feature Screening: Feature Selection for Ultra High-Dimensional Data via Deep Neural Networks. (arXiv:2204.01682v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Li_K/0/1/0/all/0/1">Kexuan Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1">Fangfang Wang</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_L/0/1/0/all/0/1">Lingli Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Liu_R/0/1/0/all/0/1">Ruiqi Liu</a></p>
<p>The applications of traditional statistical feature selection methods to
high-dimension, low sample-size data often struggle and encounter challenging
problems, such as overfitting, curse of dimensionality, computational
infeasibility, and strong model assumption. In this paper, we propose a novel
two-step nonparametric approach called Deep Feature Screening (DeepFS) that can
overcome these problems and identify significant features with high precision
for ultra high-dimensional, low-sample-size data. This approach first extracts
a low-dimensional representation of input data and then applies feature
screening based on multivariate rank distance correlation recently developed by
Deb and Sen (2021). This approach combines the strengths of both deep neural
networks and feature screening, and thereby has the following appealing
features in addition to its ability of handling ultra high-dimensional data
with small number of samples: (1) it is model free and distribution free; (2)
it can be used for both supervised and unsupervised feature selection; and (3)
it is capable of recovering the original input data. The superiority of DeepFS
is demonstrated via extensive simulation studies and real data analyses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.01884">Policy Learning with Competing Agents. (arXiv:2204.01884v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Sahoo_R/0/1/0/all/0/1">Roshni Sahoo</a>, <a href="http://arxiv.org/find/stat/1/au:+Wager_S/0/1/0/all/0/1">Stefan Wager</a></p>
<p>Decision makers often aim to learn a treatment assignment policy under a
capacity constraint on the number of agents that they can treat. When agents
can respond strategically to such policies, competition arises, complicating
estimation of the optimal policy. In this paper, we study capacity-constrained
treatment assignment in the presence of such interference. We consider a
dynamic model where the decision maker allocates treatments at each time step
and heterogeneous agents myopically best respond to the previous treatment
assignment policy. When the number of agents is large but finite, we show that
the threshold for receiving treatment under a given policy converges to the
policy's mean-field equilibrium threshold. Based on this result, we develop a
consistent estimator for the policy gradient. In simulations and a
semi-synthetic experiment with data from the National Education Longitudinal
Study of 1988, we demonstrate that this estimator can be used for learning
capacity-constrained policies in the presence of strategic behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11357">POLTER: Policy Trajectory Ensemble Regularization for Unsupervised Reinforcement Learning. (arXiv:2205.11357v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schubert_F/0/1/0/all/0/1">Frederik Schubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Benjamins_C/0/1/0/all/0/1">Carolin Benjamins</a>, <a href="http://arxiv.org/find/cs/1/au:+Dohler_S/0/1/0/all/0/1">Sebastian D&#xf6;hler</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenhahn_B/0/1/0/all/0/1">Bodo Rosenhahn</a>, <a href="http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1">Marius Lindauer</a></p>
<p>The goal of Unsupervised Reinforcement Learning (URL) is to find a
reward-agnostic prior policy on a task domain, such that the sample-efficiency
on supervised downstream tasks is improved. Although agents initialized with
such a prior policy can achieve a significantly higher reward with fewer
samples when finetuned on the downstream task, it is still an open question how
an optimal pretrained prior policy can be achieved in practice. In this work,
we present POLTER (Policy Trajectory Ensemble Regularization) - a general
method to regularize the pretraining that can be applied to any URL algorithm
and is especially useful on data- and knowledge-based URL algorithms. It
utilizes an ensemble of policies that are discovered during pretraining and
moves the policy of the URL algorithm closer to its optimal prior. Our method
is based on a theoretical framework, and we analyze its practical effects on a
white-box benchmark, allowing us to study POLTER with full control. In our main
experiments, we evaluate POLTER on the Unsupervised Reinforcement Learning
Benchmark (URLB), which consists of 12 tasks in 3 domains. We demonstrate the
generality of our approach by improving the performance of a diverse set of
data- and knowledge-based URL algorithms by 19% on average and up to 40% in the
best case. Under a fair comparison with tuned baselines and tuned POLTER, we
establish a new state-of-the-art for model-free methods on the URLB.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.12787">Impartial Games: A Challenge for Reinforcement Learning. (arXiv:2205.12787v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1">Bei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Riis_S/0/1/0/all/0/1">S&#xf8;ren Riis</a></p>
<p>While AlphaZero-style reinforcement learning (RL) algorithms excel in various
board games, in this paper we show that they face challenges on impartial games
where players share pieces. We present a concrete example of a game - namely
the children's game of Nim - and other impartial games that seem to be a
stumbling block for AlphaZero-style and similar self-play reinforcement
learning algorithms.
</p>
<p>Our work is built on the challenges posed by the intricacies of data
distribution on the ability of neural networks to learn parity functions,
exacerbated by the noisy labels issue. Our findings are consistent with recent
studies showing that AlphaZero-style algorithms are vulnerable to adversarial
attacks and adversarial perturbations, showing the difficulty of learning to
master the games in all legal states.
</p>
<p>We show that Nim can be learned on small boards, but the learning progress of
AlphaZero-style algorithms dramatically slows down when the board size
increases. Intuitively, the difference between impartial games like Nim and
partisan games like Chess and Go can be explained by the fact that if a small
part of the board is covered for impartial games it is typically not possible
to predict whether the position is won or lost as there is often zero
correlation between the visible part of a partly blanked-out position and its
correct evaluation. This situation starkly contrasts partisan games where a
partly blanked-out board position typically provides abundant or at least
non-trifle information about the value of the fully uncovered position.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.12841">Marginal Post Processing of Bayesian Inference Products with Normalizing Flows and Kernel Density Estimators. (arXiv:2205.12841v5 [astro-ph.IM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/astro-ph/1/au:+Bevins_H/0/1/0/all/0/1">Harry T. J. Bevins</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Handley_W/0/1/0/all/0/1">William J. Handley</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Lemos_P/0/1/0/all/0/1">Pablo Lemos</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Sims_P/0/1/0/all/0/1">Peter H. Sims</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Acedo_E/0/1/0/all/0/1">Eloy de Lera Acedo</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Fialkov_A/0/1/0/all/0/1">Anastasia Fialkov</a>, <a href="http://arxiv.org/find/astro-ph/1/au:+Alsing_J/0/1/0/all/0/1">Justin Alsing</a></p>
<p>Bayesian analysis has become an indispensable tool across many different
cosmological fields including the study of gravitational waves, the Cosmic
Microwave Background and the 21-cm signal from the Cosmic Dawn among other
phenomena. The method provides a way to fit complex models to data describing
key cosmological and astrophysical signals and a whole host of contaminating
signals and instrumental effects modelled with `nuisance parameters'. In this
paper, we summarise a method that uses Masked Autoregressive Flows and Kernel
Density Estimators to learn marginal posterior densities corresponding to core
science parameters. We find that the marginal or 'nuisance-free' posteriors and
the associated likelihoods have an abundance of applications including; the
calculation of previously intractable marginal Kullback-Leibler divergences and
marginal Bayesian Model Dimensionalities, likelihood emulation and prior
emulation. We demonstrate each application using toy examples, examples from
the field of 21-cm cosmology and samples from the Dark Energy Survey. We
discuss how marginal summary statistics like the Kullback-Leibler divergences
and Bayesian Model Dimensionalities can be used to examine the constraining
power of different experiments and how we can perform efficient joint analysis
by taking advantage of marginal prior and likelihood emulators. We package our
multipurpose code up in the pip-installable code margarine for use in the wider
scientific community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15466">Data Banzhaf: A Robust Data Valuation Framework for Machine Learning. (arXiv:2205.15466v7 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiachen T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1">Ruoxi Jia</a></p>
<p>Data valuation has wide use cases in machine learning, including improving
data quality and creating economic incentives for data sharing. This paper
studies the robustness of data valuation to noisy model performance scores.
Particularly, we find that the inherent randomness of the widely used
stochastic gradient descent can cause existing data value notions (e.g., the
Shapley value and the Leave-one-out error) to produce inconsistent data value
rankings across different runs. To address this challenge, we introduce the
concept of safety margin, which measures the robustness of a data value notion.
We show that the Banzhaf value, a famous value notion that originated from
cooperative game theory literature, achieves the largest safety margin among
all semivalues (a class of value notions that satisfy crucial properties
entailed by ML applications and include the famous Shapley value and
Leave-one-out error). We propose an algorithm to efficiently estimate the
Banzhaf value based on the Maximum Sample Reuse (MSR) principle. Our evaluation
demonstrates that the Banzhaf value outperforms the existing semivalue-based
data value notions on several ML tasks such as learning with weighted samples
and noisy label detection. Overall, our study suggests that when the underlying
ML algorithm is stochastic, the Banzhaf value is a promising alternative to the
other semivalue-based data value schemes given its computational advantage and
ability to robustly differentiate data quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02765">Communication-constrained hypothesis testing: Optimality, robustness, and reverse data processing inequalities. (arXiv:2206.02765v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Pensia_A/0/1/0/all/0/1">Ankit Pensia</a>, <a href="http://arxiv.org/find/math/1/au:+Jog_V/0/1/0/all/0/1">Varun Jog</a>, <a href="http://arxiv.org/find/math/1/au:+Loh_P/0/1/0/all/0/1">Po-Ling Loh</a></p>
<p>We study hypothesis testing under communication constraints, where each
sample is quantized before being revealed to a statistician. Without
communication constraints, it is well known that the sample complexity of
simple binary hypothesis testing is characterized by the Hellinger distance
between the distributions. We show that the sample complexity of simple binary
hypothesis testing under communication constraints is at most a logarithmic
factor larger than in the unconstrained setting and this bound is tight. We
develop a polynomial-time algorithm that achieves the aforementioned sample
complexity. Our framework extends to robust hypothesis testing, where the
distributions are corrupted in the total variation distance. Our proofs rely on
a new reverse data processing inequality and a reverse Markov inequality, which
may be of independent interest. For simple $M$-ary hypothesis testing, the
sample complexity in the absence of communication constraints has a logarithmic
dependence on $M$. We show that communication constraints can cause an
exponential blow-up leading to $\Omega(M)$ sample complexity even for adaptive
algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.06004">A novel multi-layer modular approach for real-time fuzzy-identification of gravitational-wave signals. (arXiv:2206.06004v4 [gr-qc] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/gr-qc/1/au:+Barone_F/0/1/0/all/0/1">Francesco Pio Barone</a>, <a href="http://arxiv.org/find/gr-qc/1/au:+DellAquila_D/0/1/0/all/0/1">Daniele Dell&#x27;Aquila</a>, <a href="http://arxiv.org/find/gr-qc/1/au:+Russo_M/0/1/0/all/0/1">Marco Russo</a></p>
<p>Advanced LIGO and Advanced Virgo ground-based interferometers are instruments
capable to detect gravitational wave signals exploiting advanced laser
interferometry techniques. The underlying data analysis task consists in
identifying specific patterns in noisy timeseries, but it is made extremely
complex by the incredibly small amplitude of the target signals. In this
scenario, the development of effective gravitational wave detection algorithms
is crucial. We propose a novel layered framework for real-time detection of
gravitational waves inspired by speech processing techniques and, in the
present implementation, based on a state-of-the-art machine learning approach
involving a hybridization of genetic programming and neural networks. The key
aspects of the newly proposed framework are: the well structured, layered
approach, and the low computational complexity. The paper describes the basic
concepts of the framework and the derivation of the first three layers. Even if
the layers are based on models derived using a machine learning approach, the
proposed layered structure has a universal nature. Compared to more complex
approaches, such as convolutional neural networks, which comprise a parameter
set of several tens of MB and were tested exclusively for fixed length data
samples, our framework has lower accuracy (e.g., it identifies 45% of low
signal-to-noise-ration gravitational wave signals, against 65% of the
state-of-the-art, at a false alarm probability of $10^{-2}$), but has a much
lower computational complexity and a higher degree of modularity. Furthermore,
the exploitation of short-term features makes the results of the new framework
virtually independent against time-position of gravitational wave signals,
simplifying its future exploitation in real-time multi-layer pipelines for
gravitational-wave detection with new generation interferometers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.03101">A conditional gradient homotopy method with applications to Semidefinite Programming. (arXiv:2207.03101v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Dvurechensky_P/0/1/0/all/0/1">Pavel Dvurechensky</a>, <a href="http://arxiv.org/find/math/1/au:+Shtern_S/0/1/0/all/0/1">Shimrit Shtern</a>, <a href="http://arxiv.org/find/math/1/au:+Staudigl_M/0/1/0/all/0/1">Mathias Staudigl</a></p>
<p>We propose a new homotopy-based conditional gradient method for solving
convex optimization problems with a large number of simple conic constraints.
Instances of this template naturally appear in semidefinite programming
problems arising as convex relaxations of combinatorial optimization problems.
Our method is a double-loop algorithm in which the conic constraint is treated
via a self-concordant barrier, and the inner loop employs a conditional
gradient algorithm to approximate the analytic central path, while the outer
loop updates the accuracy imposed on the temporal solution and the homotopy
parameter. Our theoretical iteration complexity is competitive when confronted
to state-of-the-art SDP solvers, with the decisive advantage of cheap
projection-free subroutines. Preliminary numerical experiments are provided for
illustrating the practical performance of the method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.06950">Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models. (arXiv:2207.06950v5 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Hu_L/0/1/0/all/0/1">Linwei Hu</a>, <a href="http://arxiv.org/find/stat/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/stat/1/au:+Nair_V/0/1/0/all/0/1">Vijayan N. Nair</a></p>
<p>Low-order functional ANOVA (fANOVA) models have been rediscovered in the
machine learning (ML) community under the guise of inherently interpretable
machine learning. Explainable Boosting Machines or EBM (Lou et al. 2013) and
GAMI-Net (Yang et al. 2021) are two recently proposed ML algorithms for fitting
functional main effects and second-order interactions. We propose a new
algorithm, called GAMI-Tree, that is similar to EBM, but has a number of
features that lead to better performance. It uses model-based trees as base
learners and incorporates a new interaction filtering method that is better at
capturing the underlying interactions. In addition, our iterative training
method converges to a model with better predictive performance, and the
embedded purification ensures that interactions are hierarchically orthogonal
to main effects. The algorithm does not need extensive tuning, and our
implementation is fast and efficient. We use simulated and real datasets to
compare the performance and interpretability of GAMI-Tree with EBM and
GAMI-Net.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.08012">Meta-Referential Games to Learn Compositional Learning Behaviours. (arXiv:2207.08012v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Denamganai_K/0/1/0/all/0/1">Kevin Denamgana&#xef;</a>, <a href="http://arxiv.org/find/cs/1/au:+Missaoui_S/0/1/0/all/0/1">Sondess Missaoui</a>, <a href="http://arxiv.org/find/cs/1/au:+Walker_J/0/1/0/all/0/1">James Alfred Walker</a></p>
<p>Human beings use compositionality to generalise from past experiences to
novel experiences. We assume a separation of our experiences into fundamental
atomic components that can be recombined in novel ways to support our ability
to engage with novel experiences. We frame this as the ability to learn to
generalise compositionally, and we will refer to behaviours making use of this
ability as compositional learning behaviours (CLBs). A central problem to
learning CLBs is the resolution of a binding problem (BP). While it is another
feat of intelligence that human beings perform with ease, it is not the case
for state-of-the-art artificial agents. Thus, in order to build artificial
agents able to collaborate with human beings, we propose to develop a novel
benchmark to investigate agents' abilities to exhibit CLBs by solving a
domain-agnostic version of the BP. We take inspiration from the language
emergence and grounding framework of referential games and propose a
meta-learning extension of referential games, entitled Meta-Referential Games,
and use this framework to build our benchmark, the Symbolic Behaviour Benchmark
(S2B). We provide baseline results and error analysis showing that our
benchmark is a compelling challenge that we hope will spur the research
community towards developing more capable artificial agents.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.09147">Disentangled Representation with Causal Constraints for Counterfactual Fairness. (arXiv:2208.09147v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jixue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">Debo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiuyong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a></p>
<p>Much research has been devoted to the problem of learning fair
representations; however, they do not explicitly the relationship between
latent representations. In many real-world applications, there may be causal
relationships between latent representations. Furthermore, most fair
representation learning methods focus on group-level fairness and are based on
correlations, ignoring the causal relationships underlying the data. In this
work, we theoretically demonstrate that using the structured representations
enable downstream predictive models to achieve counterfactual fairness, and
then we propose the Counterfactual Fairness Variational AutoEncoder (CF-VAE) to
obtain structured representations with respect to domain knowledge. The
experimental results show that the proposed method achieves better fairness and
accuracy performance than the benchmark fairness methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2208.12063">Partial Matrix Completion. (arXiv:2208.12063v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1">Elad Hazan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1">Adam Tauman Kalai</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanade_V/0/1/0/all/0/1">Varun Kanade</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohri_C/0/1/0/all/0/1">Clara Mohri</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Y. Jennifer Sun</a></p>
<p>The matrix completion problem aims to reconstruct a low-rank matrix based on
a revealed set of possibly noisy entries. Prior works consider completing the
entire matrix with generalization error guarantees. However, the completion
accuracy can be drastically different over different entries. This work
establishes a new framework of partial matrix completion, where the goal is to
identify a large subset of the entries that can be completed with high
confidence. We propose an efficient algorithm with the following provable
guarantees. Given access to samples from an unknown and arbitrary distribution,
it guarantees: (a) high accuracy over completed entries, and (b) high coverage
of the underlying distribution. We also consider an online learning variant of
this problem, where we propose a low-regret algorithm based on iterative
gradient updates. Preliminary empirical evaluations are included.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.09130">SAMP: A Model Inference Toolkit of Post-Training Quantization for Text Processing via Self-Adaptive Mixed-Precision. (arXiv:2209.09130v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1">Rong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zijing Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Weijie Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Haoyan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1">Weiquan Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1">Kan Zhou</a></p>
<p>The latest industrial inference engines, such as FasterTransformer and
TurboTransformers, have verified that half-precision floating point (FP16) and
8-bit integer (INT8) quantization can greatly improve model inference speed.
However, the existing INT8 quantization methods are too complicated, and
improper usage will lead to model performance damage greatly. In this paper, we
develop a toolkit for users to easily quantize their models for inference, in
which Self-Adaptive Mixed-Precision (SAMP) is proposed to automatically control
quantization rate by a mixed-precision architecture to balance model accuracy
and efficiency. Experimental results show that our SAMP toolkit has a higher
speedup than PyTorch and FasterTransformer while ensuring the required
accuracy. In addition, SAMP is based on a modular design, decoupling the
tokenizer, embedding, encoder and target layers, which allows users to handle
various downstream tasks and can be seamlessly integrated into PyTorch.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.05662">Understanding or Manipulation: Rethinking Online Performance Gains of Modern Recommender Systems. (arXiv:2210.05662v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhengbang Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qin_R/0/1/0/all/0/1">Rongjun Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Junjie Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xinyi Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Weinan Zhang</a></p>
<p>Recommender systems are expected to be assistants that help human users find
relevant information automatically without explicit queries. As recommender
systems evolve, increasingly sophisticated learning techniques are applied and
have achieved better performance in terms of user engagement metrics such as
clicks and browsing time. The increase in the measured performance, however,
can have two possible attributions: a better understanding of user preferences,
and a more proactive ability to utilize human bounded rationality to seduce
user over-consumption. A natural following question is whether current
recommendation algorithms are manipulating user preferences. If so, can we
measure the manipulation level? In this paper, we present a general framework
for benchmarking the degree of manipulations of recommendation algorithms, in
both slate recommendation and sequential recommendation scenarios. The
framework consists of four stages, initial preference calculation, training
data collection, algorithm training and interaction, and metrics calculation
that involves two proposed metrics. We benchmark some representative
recommendation algorithms in both synthetic and real-world datasets under the
proposed framework. We have observed that a high online click-through rate does
not necessarily mean a better understanding of user initial preference, but
ends in prompting users to choose more documents they initially did not favor.
Moreover, we find that the training data have notable impacts on the
manipulation degrees, and algorithms with more powerful modeling abilities are
more sensitive to such impacts. The experiments also verified the usefulness of
the proposed metrics for measuring the degree of manipulations. We advocate
that future recommendation algorithm studies should be treated as an
optimization problem with constrained user preference manipulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.07857">Commutativity and Disentanglement from the Manifold Perspective. (arXiv:2210.07857v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Qiu_F/0/1/0/all/0/1">Frank Qiu</a></p>
<p>In this paper, we interpret disentanglement as the discovery of local charts
of the data manifold and trace how this definition naturally leads to an
equivalent condition for disentanglement: commutativity between factors of
variation. We study the impact of this manifold framework to two classes of
problems: learning matrix exponential operators and compressing data-generating
models. In each problem, the manifold perspective yields interesting results
about the feasibility and fruitful approaches their solutions. We also link our
manifold framework to two other common disentanglement paradigms: group
theoretic and probabilistic approaches to disentanglement. In each case, we
show how these frameworks can be merged with our manifold perspective.
Importantly, we recover commutativity as a central property in both alternative
frameworks, further highlighting its importance in disentanglement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.13879">Proximal Mean Field Learning in Shallow Neural Networks. (arXiv:2210.13879v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teter_A/0/1/0/all/0/1">Alexis Teter</a>, <a href="http://arxiv.org/find/cs/1/au:+Nodozi_I/0/1/0/all/0/1">Iman Nodozi</a>, <a href="http://arxiv.org/find/cs/1/au:+Halder_A/0/1/0/all/0/1">Abhishek Halder</a></p>
<p>We propose a custom learning algorithm for shallow over-parameterized neural
networks, i.e., networks with single hidden layer having infinite width. The
infinite width of the hidden layer serves as an abstraction for the
over-parameterization. Building on the recent mean field interpretations of
learning dynamics in shallow neural networks, we realize mean field learning as
a computational algorithm, rather than as an analytical tool. Specifically, we
design a Sinkhorn regularized proximal algorithm to approximate the
distributional flow for the learning dynamics over weighted point clouds. In
this setting, a contractive fixed point recursion computes the time-varying
weights, numerically realizing the interacting Wasserstein gradient flow of the
parameter distribution supported over the neuronal ensemble. An appealing
aspect of the proposed algorithm is that the measure-valued recursions allow
meshless computation. We demonstrate the proposed computational framework of
interacting weighted particle evolution on binary and multi-class
classification. Our algorithm performs gradient descent of the free energy
associated with the risk functional.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14320">FO-PINNs: A First-Order formulation for Physics Informed Neural Networks. (arXiv:2210.14320v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gladstone_R/0/1/0/all/0/1">Rini J. Gladstone</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabian_M/0/1/0/all/0/1">Mohammad A. Nabian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sukumar_N/0/1/0/all/0/1">N. Sukumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1">Ankit Srivastava</a>, <a href="http://arxiv.org/find/cs/1/au:+Meidani_H/0/1/0/all/0/1">Hadi Meidani</a></p>
<p>Physics-Informed Neural Networks (PINNs) are a class of deep learning neural
networks that learn the response of a physical system without any simulation
data, and only by incorporating the governing partial differential equations
(PDEs) in their loss function. While PINNs are successfully used for solving
forward and inverse problems, their accuracy decreases significantly for
parameterized systems. PINNs also have a soft implementation of boundary
conditions resulting in boundary conditions not being exactly imposed
everywhere on the boundary. With these challenges at hand, we present
first-order physics-informed neural networks (FO-PINNs). These are PINNs that
are trained using a first-order formulation of the PDE loss function. We show
that, compared to standard PINNs, FO-PINNs offer significantly higher accuracy
in solving parameterized systems, and reduce time-per-iteration by removing the
extra backpropagations needed to compute the second or higher-order
derivatives. Additionally, FO-PINNs can enable exact imposition of boundary
conditions using approximate distance functions, which pose challenges when
applied on high-order PDEs. Through three examples, we demonstrate the
advantages of FO-PINNs over standard PINNs in terms of accuracy and training
speedup.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.15657">Detecting fake accounts through Generative Adversarial Network in online social media. (arXiv:2210.15657v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1">Jinus Bordbar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1">Mohammadreza Mohammadrezaie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1">Saman Ardalan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shiri_M/0/1/0/all/0/1">Mohammad Ebrahim Shiri</a></p>
<p>Online social media is integral to human life, facilitating messaging,
information sharing, and confidential communication while preserving privacy.
Platforms like Twitter, Instagram, and Facebook exemplify this phenomenon.
However, users face challenges due to network anomalies, often stemming from
malicious activities such as identity theft for financial gain or harm. This
paper proposes a novel method using user similarity measures and the Generative
Adversarial Network (GAN) algorithm to identify fake user accounts in the
Twitter dataset. Despite the problem's complexity, the method achieves an AUC
rate of 80\% in classifying and detecting fake accounts. Notably, the study
builds on previous research, highlighting advancements and insights into the
evolving landscape of anomaly detection in online social networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.00369">A General Search-based Framework for Generating Textual Counterfactual Explanations. (arXiv:2211.00369v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gilo_D/0/1/0/all/0/1">Daniel Gilo</a>, <a href="http://arxiv.org/find/cs/1/au:+Markovitch_S/0/1/0/all/0/1">Shaul Markovitch</a></p>
<p>One of the prominent methods for explaining the decision of a
machine-learning classifier is by a counterfactual example. Most current
algorithms for generating such examples in the textual domain are based on
generative language models. Generative models, however, are trained to minimize
a specific loss function in order to fulfill certain requirements for the
generated texts. Any change in the requirements may necessitate costly
retraining, thus potentially limiting their applicability. In this paper, we
present a general search-based framework for generating counterfactual
explanations in the textual domain. Our framework is model-agnostic,
domain-agnostic, anytime, and does not require retraining in order to adapt to
changes in the user requirements. We model the task as a search problem in a
space where the initial state is the classified text, and the goal state is a
text in a given target class. Our framework includes domain-independent
modification operators, but can also exploit domain-specific knowledge through
specialized operators. The search algorithm attempts to find a text from the
target class with minimal user-specified distance from the original classified
object.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.01071">Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1">Jinus Bordbar</a>, <a href="http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1">Saman Ardalan</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1">Mohammadreza Mohammadrezaie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghasemi_Z/0/1/0/all/0/1">Zahra Ghasemi</a></p>
<p>As social media continues to grow rapidly, the prevalence of harassment on
these platforms has also increased. This has piqued the interest of researchers
in the field of fake detection. Social media data, often forms complex graphs
with numerous nodes, posing several challenges. These challenges and
limitations include dealing with a significant amount of irrelevant features in
matrices and addressing issues such as high data dispersion and an imbalanced
class distribution within the dataset. To overcome these challenges and
limitations, researchers have employed auto-encoders and a combination of
semi-supervised learning with a GAN algorithm, referred to as SGAN. Our
proposed method utilizes auto-encoders for feature extraction and incorporates
SGAN. By leveraging an unlabeled dataset, the unsupervised layer of SGAN
compensates for the limited availability of labeled data, making efficient use
of the limited number of labeled instances. Multiple evaluation metrics were
employed, including the Confusion Matrix and the ROC curve. The dataset was
divided into training and testing sets, with 100 labeled samples for training
and 1,000 samples for testing. The novelty of our research lies in applying
SGAN to address the issue of imbalanced datasets in fake account detection. By
optimizing the use of a smaller number of labeled instances and reducing the
need for extensive computational power, our method offers a more efficient
solution. Additionally, our study contributes to the field by achieving an 81%
accuracy in detecting fake accounts using only 100 labeled samples. This
demonstrates the potential of SGAN as a powerful tool for handling minority
classes and addressing big data challenges in fake account detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.08262">Uniform Sequence Better: Time Interval Aware Data Augmentation for Sequential Recommendation. (arXiv:2212.08262v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1">Yizhou Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_E/0/1/0/all/0/1">Enneng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_G/0/1/0/all/0/1">Guibing Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Linying Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xingwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xiaoxiao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qinghui Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a></p>
<p>Sequential recommendation is an important task to predict the next-item to
access based on a sequence of interacted items. Most existing works learn user
preference as the transition pattern from the previous item to the next one,
ignoring the time interval between these two items. However, we observe that
the time interval in a sequence may vary significantly different, and thus
result in the ineffectiveness of user modeling due to the issue of
\emph{preference drift}. In fact, we conducted an empirical study to validate
this observation, and found that a sequence with uniformly distributed time
interval (denoted as uniform sequence) is more beneficial for performance
improvement than that with greatly varying time interval. Therefore, we propose
to augment sequence data from the perspective of time interval, which is not
studied in the literature. Specifically, we design five operators (Ti-Crop,
Ti-Reorder, Ti-Mask, Ti-Substitute, Ti-Insert) to transform the original
non-uniform sequence to uniform sequence with the consideration of variance of
time intervals. Then, we devise a control strategy to execute data augmentation
on item sequences in different lengths. Finally, we implement these
improvements on a state-of-the-art model CoSeRec and validate our approach on
four real datasets. The experimental results show that our approach reaches
significantly better performance than the other 11 competing methods. Our
implementation is available: https://github.com/KingGugu/TiCoSeRec.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.02761">Active Learning Guided by Efficient Surrogate Learners. (arXiv:2301.02761v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_Y/0/1/0/all/0/1">Yunpyo An</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Suyeong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1">Kwang In Kim</a></p>
<p>Re-training a deep learning model each time a single data point receives a
new label is impractical due to the inherent complexity of the training
process. Consequently, existing active learning (AL) algorithms tend to adopt a
batch-based approach where, during each AL iteration, a set of data points is
collectively chosen for annotation. However, this strategy frequently leads to
redundant sampling, ultimately eroding the efficacy of the labeling procedure.
In this paper, we introduce a new AL algorithm that harnesses the power of a
Gaussian process surrogate in conjunction with the neural network principal
learner. Our proposed model adeptly updates the surrogate learner for every new
data instance, enabling it to emulate and capitalize on the continuous learning
dynamics of the neural network without necessitating a complete retraining of
the principal model for each individual label. Experiments on four benchmark
datasets demonstrate that this approach yields significant enhancements, either
rivaling or aligning with the performance of state-of-the-art techniques.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.03566">Simple Binary Hypothesis Testing under Local Differential Privacy and Communication Constraints. (arXiv:2301.03566v2 [math.ST] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Pensia_A/0/1/0/all/0/1">Ankit Pensia</a>, <a href="http://arxiv.org/find/math/1/au:+Asadi_A/0/1/0/all/0/1">Amir R. Asadi</a>, <a href="http://arxiv.org/find/math/1/au:+Jog_V/0/1/0/all/0/1">Varun Jog</a>, <a href="http://arxiv.org/find/math/1/au:+Loh_P/0/1/0/all/0/1">Po-Ling Loh</a></p>
<p>We study simple binary hypothesis testing under both local differential
privacy (LDP) and communication constraints. We qualify our results as either
minimax optimal or instance optimal: the former hold for the set of
distribution pairs with prescribed Hellinger divergence and total variation
distance, whereas the latter hold for specific distribution pairs. For the
sample complexity of simple hypothesis testing under pure LDP constraints, we
establish instance-optimal bounds for distributions with binary support;
minimax-optimal bounds for general distributions; and (approximately)
instance-optimal, computationally efficient algorithms for general
distributions. When both privacy and communication constraints are present, we
develop instance-optimal, computationally efficient algorithms that achieve the
minimum possible sample complexity (up to universal constants). Our results on
instance-optimal algorithms hinge on identifying the extreme points of the
joint range set $\mathcal A$ of two distributions $p$ and $q$, defined as
$\mathcal A := \{(\mathbf T p, \mathbf T q) | \mathbf T \in \mathcal C\}$,
where $\mathcal C$ is the set of channels characterizing the constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.03747">Semiparametric Regression for Spatial Data via Deep Learning. (arXiv:2301.03747v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Li_K/0/1/0/all/0/1">Kexuan Li</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhu_J/0/1/0/all/0/1">Jun Zhu</a>, <a href="http://arxiv.org/find/stat/1/au:+Ives_A/0/1/0/all/0/1">Anthony R. Ives</a>, <a href="http://arxiv.org/find/stat/1/au:+Radeloff_V/0/1/0/all/0/1">Volker C. Radeloff</a>, <a href="http://arxiv.org/find/stat/1/au:+Wang_F/0/1/0/all/0/1">Fangfang Wang</a></p>
<p>In this work, we propose a deep learning-based method to perform
semiparametric regression analysis for spatially dependent data. To be
specific, we use a sparsely connected deep neural network with rectified linear
unit (ReLU) activation function to estimate the unknown regression function
that describes the relationship between response and covariates in the presence
of spatial dependence. Under some mild conditions, the estimator is proven to
be consistent, and the rate of convergence is determined by three factors: (1)
the architecture of neural network class, (2) the smoothness and (intrinsic)
dimension of true mean function, and (3) the magnitude of spatial dependence.
Our method can handle well large data set owing to the stochastic gradient
descent optimization algorithm. Simulation studies on synthetic data are
conducted to assess the finite sample performance, the results of which
indicate that the proposed method is capable of picking up the intricate
relationship between response and covariates. Finally, a real data analysis is
provided to demonstrate the validity and effectiveness of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10343">ClimaX: A foundation model for weather and climate. (arXiv:2301.10343v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1">Tung Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Brandstetter_J/0/1/0/all/0/1">Johannes Brandstetter</a>, <a href="http://arxiv.org/find/cs/1/au:+Kapoor_A/0/1/0/all/0/1">Ashish Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1">Jayesh K. Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1">Aditya Grover</a></p>
<p>Most state-of-the-art approaches for weather and climate modeling are based
on physics-informed numerical models of the atmosphere. These approaches aim to
model the non-linear dynamics and complex interactions between multiple
variables, which are challenging to approximate. Additionally, many such
numerical models are computationally intensive, especially when modeling the
atmospheric phenomenon at a fine-grained spatial and temporal resolution.
Recent data-driven approaches based on machine learning instead aim to directly
solve a downstream forecasting or projection task by learning a data-driven
functional mapping using deep neural networks. However, these networks are
trained using curated and homogeneous climate datasets for specific
spatiotemporal tasks, and thus lack the generality of numerical models. We
develop and demonstrate ClimaX, a flexible and generalizable deep learning
model for weather and climate science that can be trained using heterogeneous
datasets spanning different variables, spatio-temporal coverage, and physical
groundings. ClimaX extends the Transformer architecture with novel encoding and
aggregation blocks that allow effective use of available compute while
maintaining general utility. ClimaX is pre-trained with a self-supervised
learning objective on climate datasets derived from CMIP6. The pre-trained
ClimaX can then be fine-tuned to address a breadth of climate and weather
tasks, including those that involve atmospheric variables and spatio-temporal
scales unseen during pretraining. Compared to existing data-driven baselines,
we show that this generality in ClimaX results in superior performance on
benchmarks for weather forecasting and climate projections, even when
pretrained at lower resolutions and compute budgets. The source code is
available at https://github.com/microsoft/ClimaX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10405">Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v7 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hypernetwork to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12212">Efficient Enumeration of Markov Equivalent DAGs. (arXiv:2301.12212v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wienobst_M/0/1/0/all/0/1">Marcel Wien&#xf6;bst</a>, <a href="http://arxiv.org/find/cs/1/au:+Luttermann_M/0/1/0/all/0/1">Malte Luttermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Bannach_M/0/1/0/all/0/1">Max Bannach</a>, <a href="http://arxiv.org/find/cs/1/au:+Liskiewicz_M/0/1/0/all/0/1">Maciej Li&#x15b;kiewicz</a></p>
<p>Enumerating the directed acyclic graphs (DAGs) of a Markov equivalence class
(MEC) is an important primitive in causal analysis. The central resource from
the perspective of computational complexity is the delay, that is, the time an
algorithm that lists all members of the class requires between two consecutive
outputs. Commonly used algorithms for this task utilize the rules proposed by
Meek (1995) or the transformational characterization by Chickering (1995), both
resulting in superlinear delay. In this paper, we present the first linear-time
delay algorithm. On the theoretical side, we show that our algorithm can be
generalized to enumerate DAGs represented by models that incorporate background
knowledge, such as MPDAGs; on the practical side, we provide an efficient
implementation and evaluate it in a series of experiments. Complementary to the
linear-time delay algorithm, we also provide intriguing insights into Markov
equivalence itself: All members of an MEC can be enumerated such that two
successive DAGs have structural Hamming distance at most three.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02420">Variational Inference on the Final-Layer Output of Neural Networks. (arXiv:2302.02420v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yadi Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Khardon_R/0/1/0/all/0/1">Roni Khardon</a></p>
<p>Traditional neural networks are simple to train but they typically produce
overconfident predictions. In contrast, Bayesian neural networks provide good
uncertainty quantification but optimizing them is time consuming due to the
large parameter space. This paper proposes to combine the advantages of both
approaches by performing Variational Inference in the Final layer Output space
(VIFO), because the output space is much smaller than the parameter space. We
use neural networks to learn the mean and the variance of the probabilistic
output. Like standard, non-Beyesian models, VIFO enjoys simple training and one
can use Rademacher complexity to provide risk bounds for the model. On the
other hand, using the Bayesian formulation we incorporate collapsed variational
inference with VIFO which significantly improves the performance in practice.
Experiments show that VIFO and ensembles of VIFO provide a good tradeoff in
terms of run time and uncertainty quantification, especially for out of
distribution data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09694">Disentangled Representation for Causal Mediation Analysis. (arXiv:2302.09694v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Ziqi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_D/0/1/0/all/0/1">Debo Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiuyong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jixue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Lin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Ke Wang</a></p>
<p>Estimating direct and indirect causal effects from observational data is
crucial to understanding the causal mechanisms and predicting the behaviour
under different interventions. Causal mediation analysis is a method that is
often used to reveal direct and indirect effects. Deep learning shows promise
in mediation analysis, but the current methods only assume latent confounders
that affect treatment, mediator and outcome simultaneously, and fail to
identify different types of latent confounders (e.g., confounders that only
affect the mediator or outcome). Furthermore, current methods are based on the
sequential ignorability assumption, which is not feasible for dealing with
multiple types of latent confounders. This work aims to circumvent the
sequential ignorability assumption and applies the piecemeal deconfounding
assumption as an alternative. We propose the Disentangled Mediation Analysis
Variational AutoEncoder (DMAVAE), which disentangles the representations of
latent confounders into three types to accurately estimate the natural direct
effect, natural indirect effect and total effect. Experimental results show
that the proposed method outperforms existing methods and has strong
generalisation ability. We further apply the method to a real-world dataset to
show its potential application.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.09738">Simplifying Momentum-based Positive-definite Submanifold Optimization with Applications to Deep Learning. (arXiv:2302.09738v9 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1">Wu Lin</a>, <a href="http://arxiv.org/find/stat/1/au:+Duruisseaux_V/0/1/0/all/0/1">Valentin Duruisseaux</a>, <a href="http://arxiv.org/find/stat/1/au:+Leok_M/0/1/0/all/0/1">Melvin Leok</a>, <a href="http://arxiv.org/find/stat/1/au:+Nielsen_F/0/1/0/all/0/1">Frank Nielsen</a>, <a href="http://arxiv.org/find/stat/1/au:+Khan_M/0/1/0/all/0/1">Mohammad Emtiyaz Khan</a>, <a href="http://arxiv.org/find/stat/1/au:+Schmidt_M/0/1/0/all/0/1">Mark Schmidt</a></p>
<p>Riemannian submanifold optimization with momentum is computationally
challenging because, to ensure that the iterates remain on the submanifold, we
often need to solve difficult differential equations. Here, we simplify such
difficulties for a class of sparse or structured symmetric positive-definite
matrices with the affine-invariant metric. We do so by proposing a generalized
version of the Riemannian normal coordinates that dynamically orthonormalizes
the metric and locally converts the problem into an unconstrained problem in
the Euclidean space. We use our approach to simplify existing approaches for
structured covariances and develop matrix-inverse-free $2^\text{nd}$-order
optimizers for deep learning with low precision by using only matrix
multiplications. Code: https://github.com/yorkerlin/StructuredNGD-DL
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13483">CrystalBox: Future-Based Explanations for Input-Driven Deep RL Systems. (arXiv:2302.13483v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Sagar Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Jyothi_S/0/1/0/all/0/1">Sangeetha Abdu Jyothi</a>, <a href="http://arxiv.org/find/cs/1/au:+Narodytska_N/0/1/0/all/0/1">Nina Narodytska</a></p>
<p>We present CrystalBox, a novel, model-agnostic, posthoc explainability
framework for Deep Reinforcement Learning (DRL) controllers in the large family
of input-driven environments which includes computer systems. We combine the
natural decomposability of reward functions in input-driven environments with
the explanatory power of decomposed returns. We propose an efficient algorithm
to generate future-based explanations across both discrete and continuous
control environments. Using applications such as adaptive bitrate streaming and
congestion control, we demonstrate CrystalBox's capability to generate
high-fidelity explanations. We further illustrate its higher utility across
three practical use cases: contrastive explanations, network observability, and
guided reward design, as opposed to prior explainability techniques that
identify salient features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00302">Mitigating Backdoors in Federated Learning with FLD. (arXiv:2303.00302v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yihang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1">Pengyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zhiqian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1">Yong Liao</a></p>
<p>Federated learning allows clients to collaboratively train a global model
without uploading raw data for privacy preservation. This feature, i.e., the
inability to review participants' datasets, has recently been found responsible
for federated learning's vulnerability in the face of backdoor attacks.
Existing defense methods fall short from two perspectives: 1) they consider
only very specific and limited attacker models and unable to cope with advanced
backdoor attacks, such as distributed backdoor attacks, which break down the
global trigger into multiple distributed triggers. 2) they conduct detection
based on model granularity thus the performance gets impacted by the model
dimension. To address these challenges, we propose Federated Layer Detection
(FLD), a novel model filtering approach for effectively defending against
backdoor attacks. FLD examines the models based on layer granularity to capture
the complete model details and effectively detect potential backdoor models
regardless of model dimension. We provide theoretical analysis and proof for
the convergence of FLD. Extensive experiments demonstrate that FLD effectively
mitigates state-of-the-art backdoor attacks with negligible impact on the
accuracy of the primary task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.00800">Continuous-Time Functional Diffusion Processes. (arXiv:2303.00800v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1">Giulio Franzese</a>, <a href="http://arxiv.org/find/cs/1/au:+Corallo_G/0/1/0/all/0/1">Giulio Corallo</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_S/0/1/0/all/0/1">Simone Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1">Markus Heinonen</a>, <a href="http://arxiv.org/find/cs/1/au:+Filippone_M/0/1/0/all/0/1">Maurizio Filippone</a>, <a href="http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1">Pietro Michiardi</a></p>
<p>We introduce Functional Diffusion Processes (FDPs), which generalize
score-based diffusion models to infinite-dimensional function spaces. FDPs
require a new mathematical framework to describe the forward and backward
dynamics, and several extensions to derive practical training objectives. These
include infinite-dimensional versions of Girsanov theorem, in order to be able
to compute an ELBO, and of the sampling theorem, in order to guarantee that
functional evaluations in a countable set of points are equivalent to
infinite-dimensional functions. We use FDPs to build a new breed of generative
models in function spaces, which do not require specialized network
architectures, and that can work with any kind of continuous data. Our results
on real data show that FDPs achieve high-quality image generation, using a
simple MLP architecture with orders of magnitude fewer parameters than existing
diffusion models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01213">DSD$^2$: Can We Dodge Sparse Double Descent and Compress the Neural Network Worry-Free?. (arXiv:2303.01213v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Quetu_V/0/1/0/all/0/1">Victor Qu&#xe9;tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1">Enzo Tartaglione</a></p>
<p>Neoteric works have shown that modern deep learning models can exhibit a
sparse double descent phenomenon. Indeed, as the sparsity of the model
increases, the test performance first worsens since the model is overfitting
the training data; then, the overfitting reduces, leading to an improvement in
performance, and finally, the model begins to forget critical information,
resulting in underfitting. Such a behavior prevents using traditional early
stop criteria. In this work, we have three key contributions. First, we propose
a learning framework that avoids such a phenomenon and improves generalization.
Second, we introduce an entropy measure providing more insights into the
insurgence of this phenomenon and enabling the use of traditional stop
criteria. Third, we provide a comprehensive quantitative analysis of contingent
factors such as re-initialization methods, model width and depth, and dataset
noise. The contributions are supported by empirical evidence in typical setups.
Our code is available at https://github.com/VGCQ/DSD2.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03379">SUREL+: Moving from Walks to Sets for Scalable Subgraph-based Graph Representation Learning. (arXiv:2303.03379v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Haoteng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianguo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a></p>
<p>Subgraph-based graph representation learning (SGRL) has recently emerged as a
powerful tool in many prediction tasks on graphs due to its advantages in model
expressiveness and generalization ability. Most previous SGRL models face
computational issues associated with the high cost of subgraph extraction for
each training or test query. Recently, SUREL was proposed to accelerate SGRL,
which samples random walks offline and joins these walks online as a proxy of
subgraphs for representation learning. Thanks to the reusability of sampled
walks across different queries, SUREL achieves state-of-the-art performance in
terms of scalability and prediction accuracy. However, SUREL still suffers from
high computational overhead caused by node redundancy in sampled walks. In this
work, we propose a novel framework SUREL+ that upgrades SUREL by using node
sets instead of walks to represent subgraphs. This set-based representation
avoids repeated nodes by definition, but node sets can be irregular in size. To
address this issue, we design a customized sparse data structure to efficiently
store and index node sets, and provide a specialized operator to join them in
parallel batches. SUREL+ is modularized to support multiple types of set
samplers, structural features, and neural encoders to complement the structure
information loss after the reduction from walks to sets. Extensive experiments
have been performed to validate SUREL+ in the prediction tasks of links,
relation types, and higher-order patterns. SUREL+ achieves 3-11$\times$
speedups of SUREL while maintaining comparable or even better prediction
performance; compared to other SGRL baselines, SUREL+ achieves $\sim$20$\times$
speedups and significantly improves the prediction accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03932">FFT-based Dynamic Token Mixer for Vision. (arXiv:2303.03932v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tatsunami_Y/0/1/0/all/0/1">Yuki Tatsunami</a>, <a href="http://arxiv.org/find/cs/1/au:+Taki_M/0/1/0/all/0/1">Masato Taki</a></p>
<p>Multi-head-self-attention (MHSA)-equipped models have achieved notable
performance in computer vision. Their computational complexity is proportional
to quadratic numbers of pixels in input feature maps, resulting in slow
processing, especially when dealing with high-resolution images. New types of
token-mixer are proposed as an alternative to MHSA to circumvent this problem:
an FFT-based token-mixer involves global operations similar to MHSA but with
lower computational complexity. However, despite its attractive properties, the
FFT-based token-mixer has not been carefully examined in terms of its
compatibility with the rapidly evolving MetaFormer architecture. Here, we
propose a novel token-mixer called Dynamic Filter and novel image recognition
models, DFFormer and CDFFormer, to close the gaps above. The results of image
classification and downstream tasks, analysis, and visualization show that our
models are helpful. Notably, their throughput and memory efficiency when
dealing with high-resolution image recognition is remarkable. Our results
indicate that Dynamic Filter is one of the token-mixer options that should be
seriously considered. The code is available at
https://github.com/okojoalg/dfformer
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.17743">FairGen: Towards Fair Graph Generation. (arXiv:2303.17743v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1">Lecheng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Dawei Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1">Hanghang Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiejun Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yada Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jingrui He</a></p>
<p>There have been tremendous efforts over the past decades dedicated to the
generation of realistic graphs in a variety of domains, ranging from social
networks to computer networks, from gene regulatory networks to online
transaction networks. Despite the remarkable success, the vast majority of
these works are unsupervised in nature and are typically trained to minimize
the expected graph reconstruction loss, which would result in the
representation disparity issue in the generated graphs, i.e., the protected
groups (often minorities) contribute less to the objective and thus suffer from
systematically higher errors. In this paper, we aim to tailor graph generation
to downstream mining tasks by leveraging label information and user-preferred
parity constraints. In particular, we start from the investigation of
representation disparity in the context of graph generative models. To mitigate
the disparity, we propose a fairness-aware graph generative model named
FairGen. Our model jointly trains a label-informed graph generation module and
a fair representation learning module by progressively learning the behaviors
of the protected and unprotected groups, from the `easy' concepts to the `hard'
ones. In addition, we propose a generic context sampling strategy for graph
generative models, which is proven to be capable of fairly capturing the
contextual information of each group with a high probability. Experimental
results on seven real-world data sets, including web-based graphs, demonstrate
that FairGen (1) obtains performance on par with state-of-the-art graph
generative models across nine network properties, (2) mitigates the
representation disparity issues in the generated graphs, and (3) substantially
boosts the model performance by up to 17% in downstream tasks via data
augmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01168">DeepAccident: A Motion and Accident Prediction Benchmark for V2X Autonomous Driving. (arXiv:2304.01168v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tianqi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Sukmin Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1">Wenxuan Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1">Enze Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1">Chongjian Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Junsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1">Ping Luo</a></p>
<p>Safety is the primary priority of autonomous driving. Nevertheless, no
published dataset currently supports the direct and explainable safety
evaluation for autonomous driving. In this work, we propose DeepAccident, a
large-scale dataset generated via a realistic simulator containing diverse
accident scenarios that frequently occur in real-world driving. The proposed
DeepAccident dataset includes 57K annotated frames and 285K annotated samples,
approximately 7 times more than the large-scale nuScenes dataset with 40k
annotated samples. In addition, we propose a new task, end-to-end motion and
accident prediction, which can be used to directly evaluate the accident
prediction ability for different autonomous driving algorithms. Furthermore,
for each scenario, we set four vehicles along with one infrastructure to record
data, thus providing diverse viewpoints for accident scenarios and enabling V2X
(vehicle-to-everything) research on perception and prediction tasks. Finally,
we present a baseline V2X model named V2XFormer that demonstrates superior
performance for motion and accident prediction and 3D object detection compared
to the single-vehicle model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01781">Mixing predictions for online metric algorithms. (arXiv:2304.01781v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Antoniadis_A/0/1/0/all/0/1">Antonios Antoniadis</a>, <a href="http://arxiv.org/find/cs/1/au:+Coester_C/0/1/0/all/0/1">Christian Coester</a>, <a href="http://arxiv.org/find/cs/1/au:+Elias_M/0/1/0/all/0/1">Marek Eli&#xe1;&#x161;</a>, <a href="http://arxiv.org/find/cs/1/au:+Polak_A/0/1/0/all/0/1">Adam Polak</a>, <a href="http://arxiv.org/find/cs/1/au:+Simon_B/0/1/0/all/0/1">Bertrand Simon</a></p>
<p>A major technique in learning-augmented online algorithms is combining
multiple algorithms or predictors. Since the performance of each predictor may
vary over time, it is desirable to use not the single best predictor as a
benchmark, but rather a dynamic combination which follows different predictors
at different times. We design algorithms that combine predictions and are
competitive against such dynamic combinations for a wide class of online
problems, namely, metrical task systems. Against the best (in hindsight)
unconstrained combination of $\ell$ predictors, we obtain a competitive ratio
of $O(\ell^2)$, and show that this is best possible. However, for a benchmark
with slightly constrained number of switches between different predictors, we
can get a $(1+\epsilon)$-competitive algorithm. Moreover, our algorithms can be
adapted to access predictors in a bandit-like fashion, querying only one
predictor at a time. An unexpected implication of one of our lower bounds is a
new structural insight about covering formulations for the $k$-server problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.02041">Low-complexity subspace-descent over symmetric positive definite manifold. (arXiv:2305.02041v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Darmwal_Y/0/1/0/all/0/1">Yogesh Darmwal</a>, <a href="http://arxiv.org/find/stat/1/au:+Rajawat_K/0/1/0/all/0/1">Ketan Rajawat</a></p>
<p>This work puts forth low-complexity Riemannian subspace descent algorithms
for the minimization of functions over the symmetric positive definite (SPD)
manifold. Different from the existing Riemannian gradient descent variants, the
proposed approach utilizes carefully chosen subspaces that allow the update to
be written as a product of the Cholesky factor of the iterate and a sparse
matrix. The resulting updates avoid the costly matrix operations like matrix
exponentiation and dense matrix multiplication, which are generally required in
almost all other Riemannian optimization algorithms on SPD manifold. We further
identify a broad class of functions, arising in diverse applications, such as
kernel matrix learning, covariance estimation of Gaussian distributions,
maximum likelihood parameter estimation of elliptically contoured
distributions, and parameter estimation in Gaussian mixture model problems,
over which the Riemannian gradients can be calculated efficiently. The proposed
uni-directional and multi-directional Riemannian subspace descent variants
incur per-iteration complexities of $O(n)$ and $O(n^2)$ respectively, as
compared to the $O(n^3)$ or higher complexity incurred by all existing
Riemannian gradient descent variants. The superior runtime and low
per-iteration complexity of the proposed algorithms is also demonstrated via
numerical tests on large-scale covariance estimation and matrix square root
problems. MATLAB code implementation is publicly available on GitHub :
https://github.com/yogeshd-iitk/subspace_descent_over_SPD_manifold
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.10391">Optimality of Message-Passing Architectures for Sparse Graphs. (arXiv:2305.10391v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baranwal_A/0/1/0/all/0/1">Aseem Baranwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Fountoulakis_K/0/1/0/all/0/1">Kimon Fountoulakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagannath_A/0/1/0/all/0/1">Aukosh Jagannath</a></p>
<p>We study the node classification problem on feature-decorated graphs in the
sparse setting, i.e., when the expected degree of a node is $O(1)$ in the
number of nodes, in the fixed-dimensional asymptotic regime, i.e., the
dimension of the feature data is fixed while the number of nodes is large. Such
graphs are typically known to be locally tree-like. We introduce a notion of
Bayes optimality for node classification tasks, called asymptotic local Bayes
optimality, and compute the optimal classifier according to this criterion for
a fairly general statistical data model with arbitrary distributions of the
node features and edge connectivity. The optimal classifier is implementable
using a message-passing graph neural network architecture. We then compute the
generalization error of this classifier and compare its performance against
existing learning methods theoretically on a well-studied statistical model
with naturally identifiable signal-to-noise ratios (SNRs) in the data. We find
that the optimal message-passing architecture interpolates between a standard
MLP in the regime of low graph signal and a typical convolution in the regime
of high graph signal. Furthermore, we prove a corresponding non-asymptotic
result.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14196">ZeroSCROLLS: A Zero-Shot Benchmark for Long Text Understanding. (arXiv:2305.14196v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shaham_U/0/1/0/all/0/1">Uri Shaham</a>, <a href="http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1">Maor Ivgi</a>, <a href="http://arxiv.org/find/cs/1/au:+Efrat_A/0/1/0/all/0/1">Avia Efrat</a>, <a href="http://arxiv.org/find/cs/1/au:+Berant_J/0/1/0/all/0/1">Jonathan Berant</a>, <a href="http://arxiv.org/find/cs/1/au:+Levy_O/0/1/0/all/0/1">Omer Levy</a></p>
<p>We introduce ZeroSCROLLS, a zero-shot benchmark for natural language
understanding over long texts, which contains only test and small validation
sets, without training data. We adapt six tasks from the SCROLLS benchmark, and
add four new datasets, including two novel information fusing tasks, such as
aggregating the percentage of positive reviews. Using ZeroSCROLLS, we conduct a
comprehensive evaluation of both open-source and closed large language models,
finding that Claude outperforms ChatGPT, and that GPT-4 achieves the highest
average score. However, there is still room for improvement on multiple open
challenges in ZeroSCROLLS, such as aggregation tasks, where models struggle to
pass the naive baseline. As the state of the art is a moving target, we invite
researchers to evaluate their ideas on the live ZeroSCROLLS leaderboard.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.14561">Negative Feedback Training: A Novel Concept to Improve Robustness of NVCIM DNN Accelerators. (arXiv:2305.14561v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1">Yifan Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1">Zheyu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wujie Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xiaobo Sharon Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yiyu Shi</a></p>
<p>Compute-in-memory (CIM) accelerators built upon non-volatile memory (NVM)
devices excel in energy efficiency and latency when performing Deep Neural
Network (DNN) inference, thanks to their in-situ data processing capability.
However, the stochastic nature and intrinsic variations of NVM devices often
result in performance degradation in DNN inference. Introducing these non-ideal
device behaviors during DNN training enhances robustness, but drawbacks include
limited accuracy improvement, reduced prediction confidence, and convergence
issues. This arises from a mismatch between the deterministic training and
non-deterministic device variations, as such training, though considering
variations, relies solely on the model's final output. In this work, we draw
inspiration from the control theory and propose a novel training concept:
Negative Feedback Training (NFT) leveraging the multi-scale noisy information
captured from network. We develop two specific NFT instances, Oriented
Variational Forward (OVF) and Intermediate Representation Snapshot (IRS).
Extensive experiments show that our methods outperform existing
state-of-the-art methods with up to a 46.71% improvement in inference accuracy
while reducing epistemic uncertainty, boosting output confidence, and improving
convergence probability. Their effectiveness highlights the generality and
practicality of our NFT concept in enhancing DNN robustness against device
variations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.15747">Union Subgraph Neural Networks. (arXiv:2305.15747v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiaxing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Aihu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_Q/0/1/0/all/0/1">Qingtian Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_V/0/1/0/all/0/1">Vijay Prakash Dwivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_Y/0/1/0/all/0/1">Yiping Ke</a></p>
<p>Graph Neural Networks (GNNs) are widely used for graph representation
learning in many application domains. The expressiveness of vanilla GNNs is
upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL) test as they operate on
rooted subtrees through iterative message passing. In this paper, we empower
GNNs by injecting neighbor-connectivity information extracted from a new type
of substructure. We first investigate different kinds of connectivities
existing in a local neighborhood and identify a substructure called union
subgraph, which is able to capture the complete picture of the 1-hop
neighborhood of an edge. We then design a shortest-path-based substructure
descriptor that possesses three nice properties and can effectively encode the
high-order connectivities in union subgraphs. By infusing the encoded neighbor
connectivities, we propose a novel model, namely Union Subgraph Neural Network
(UnionSNN), which is proven to be strictly more powerful than 1-WL in
distinguishing non-isomorphic graphs. Additionally, the local encoding from
union subgraphs can also be injected into arbitrary message-passing neural
networks (MPNNs) and Transformer-based models as a plugin. Extensive
experiments on 18 benchmarks of both graph-level and node-level tasks
demonstrate that UnionSNN outperforms state-of-the-art baseline models, with
competitive computational efficiency. The injection of our local encoding to
existing models is able to boost the performance by up to 11.09\%. Our code is
available at https://github.com/AngusMonroe/UnionSNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.17021">GLOBE-CE: A Translation-Based Approach for Global Counterfactual Explanations. (arXiv:2305.17021v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ley_D/0/1/0/all/0/1">Dan Ley</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Saumitra Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1">Daniele Magazzeni</a></p>
<p>Counterfactual explanations have been widely studied in explainability, with
a range of application dependent methods prominent in fairness, recourse and
model understanding. The major shortcoming associated with these methods,
however, is their inability to provide explanations beyond the local or
instance-level. While many works touch upon the notion of a global explanation,
typically suggesting to aggregate masses of local explanations in the hope of
ascertaining global properties, few provide frameworks that are both reliable
and computationally tractable. Meanwhile, practitioners are requesting more
efficient and interactive explainability tools. We take this opportunity to
propose Global &amp; Efficient Counterfactual Explanations (GLOBE-CE), a flexible
framework that tackles the reliability and scalability issues associated with
current state-of-the-art, particularly on higher dimensional datasets and in
the presence of continuous features. Furthermore, we provide a unique
mathematical analysis of categorical feature translations, utilising it in our
method. Experimental evaluation with publicly available datasets and user
studies demonstrate that GLOBE-CE performs significantly better than the
current state-of-the-art across multiple metrics (e.g., speed, reliability).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18270">How Two-Layer Neural Networks Learn, One (Giant) Step at a Time. (arXiv:2305.18270v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dandi_Y/0/1/0/all/0/1">Yatin Dandi</a>, <a href="http://arxiv.org/find/stat/1/au:+Krzakala_F/0/1/0/all/0/1">Florent Krzakala</a>, <a href="http://arxiv.org/find/stat/1/au:+Loureiro_B/0/1/0/all/0/1">Bruno Loureiro</a>, <a href="http://arxiv.org/find/stat/1/au:+Pesce_L/0/1/0/all/0/1">Luca Pesce</a>, <a href="http://arxiv.org/find/stat/1/au:+Stephan_L/0/1/0/all/0/1">Ludovic Stephan</a></p>
<p>We investigate theoretically how the features of a two-layer neural network
adapt to the structure of the target function through a few large batch
gradient descent steps, leading to improvement in the approximation capacity
with respect to the initialization. We compare the influence of batch size and
that of multiple (but finitely many) steps. For a single gradient step, a batch
of size $n = \mathcal{O}(d)$ is both necessary and sufficient to align with the
target function, although only a single direction can be learned. In contrast,
$n = \mathcal{O}(d^2)$ is essential for neurons to specialize to multiple
relevant directions of the target with a single gradient step. Even in this
case, we show there might exist ``hard'' directions requiring $n =
\mathcal{O}(d^\ell)$ samples to be learned, where $\ell$ is known as the leap
index of the target. The picture drastically improves over multiple gradient
steps: we show that a batch-size of $n = \mathcal{O}(d)$ is indeed enough to
learn multiple target directions satisfying a staircase property, where more
and more directions can be learned over time. Finally, we discuss how these
directions allows to drastically improve the approximation capacity and
generalization error over the initialization, illustrating a separation of
scale between the random features/lazy regime, and the feature learning regime.
Our technical analysis leverages a combination of techniques related to
concentration, projection-based conditioning, and Gaussian equivalence which we
believe are of independent interest. By pinning down the conditions necessary
for specialization and learning, our results highlight the interaction between
batch size and number of iterations, and lead to a hierarchical depiction where
learning performance exhibits a stairway to accuracy over time and batch size,
shedding new light on how neural networks adapt to features of the data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19496">Is Learning in Games Good for the Learners?. (arXiv:2305.19496v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brown_W/0/1/0/all/0/1">William Brown</a>, <a href="http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1">Jon Schneider</a>, <a href="http://arxiv.org/find/cs/1/au:+Vodrahalli_K/0/1/0/all/0/1">Kiran Vodrahalli</a></p>
<p>We consider a number of questions related to tradeoffs between reward and
regret in repeated gameplay between two agents. To facilitate this, we
introduce a notion of $\textit{generalized equilibrium}$ which allows for
asymmetric regret constraints, and yields polytopes of feasible values for each
agent and pair of regret constraints, where we show that any such equilibrium
is reachable by a pair of algorithms which maintain their regret guarantees
against arbitrary opponents. As a central example, we highlight the case one
agent is no-swap and the other's regret is unconstrained. We show that this
captures an extension of $\textit{Stackelberg}$ equilibria with a matching
optimal value, and that there exists a wide class of games where a player can
significantly increase their utility by deviating from a no-swap-regret
algorithm against a no-swap learner (in fact, almost any game without pure Nash
equilibria is of this form). Additionally, we make use of generalized
equilibria to consider tradeoffs in terms of the opponent's algorithm choice.
We give a tight characterization for the maximal reward obtainable against
$\textit{some}$ no-regret learner, yet we also show a class of games in which
this is bounded away from the value obtainable against the class of common
"mean-based" no-regret algorithms. Finally, we consider the question of
learning reward-optimal strategies via repeated play with a no-regret agent
when the game is initially unknown. Again we show tradeoffs depending on the
opponent's learning algorithm: the Stackelberg strategy is learnable in
exponential time with any no-regret agent (and in polynomial time with any
no-$\textit{adaptive}$-regret agent) for any game where it is learnable via
queries, and there are games where it is learnable in polynomial time against
any no-swap-regret agent but requires exponential time against a mean-based
no-regret agent.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19951">Not All Neuro-Symbolic Concepts Are Created Equal: Analysis and Mitigation of Reasoning Shortcuts. (arXiv:2305.19951v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marconato_E/0/1/0/all/0/1">Emanuele Marconato</a>, <a href="http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1">Stefano Teso</a>, <a href="http://arxiv.org/find/cs/1/au:+Vergari_A/0/1/0/all/0/1">Antonio Vergari</a>, <a href="http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1">Andrea Passerini</a></p>
<p>Neuro-Symbolic (NeSy) predictive models hold the promise of improved
compliance with given constraints, systematic generalization, and
interpretability, as they allow to infer labels that are consistent with some
prior knowledge by reasoning over high-level concepts extracted from
sub-symbolic inputs. It was recently shown that NeSy predictors are affected by
reasoning shortcuts: they can attain high accuracy but by leveraging concepts
with unintended semantics, thus coming short of their promised advantages. Yet,
a systematic characterization of reasoning shortcuts and of potential
mitigation strategies is missing. This work fills this gap by characterizing
them as unintended optima of the learning objective and identifying four key
conditions behind their occurrence. Based on this, we derive several natural
mitigation strategies, and analyze their efficacy both theoretically and
empirically. Our analysis shows reasoning shortcuts are difficult to deal with,
casting doubts on the trustworthiness and interpretability of existing NeSy
solutions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.20052">Integrated Decision Gradients: Compute Your Attributions Where the Model Makes Its Decision. (arXiv:2305.20052v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Walker_C/0/1/0/all/0/1">Chase Walker</a>, <a href="http://arxiv.org/find/cs/1/au:+Jha_S/0/1/0/all/0/1">Sumit Jha</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kenny Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ewetz_R/0/1/0/all/0/1">Rickard Ewetz</a></p>
<p>Attribution algorithms are frequently employed to explain the decisions of
neural network models. Integrated Gradients (IG) is an influential attribution
method due to its strong axiomatic foundation. The algorithm is based on
integrating the gradients along a path from a reference image to the input
image. Unfortunately, it can be observed that gradients computed from regions
where the output logit changes minimally along the path provide poor
explanations for the model decision, which is called the saturation effect
problem. In this paper, we propose an attribution algorithm called integrated
decision gradients (IDG). The algorithm focuses on integrating gradients from
the region of the path where the model makes its decision, i.e., the portion of
the path where the output logit rapidly transitions from zero to its final
value. This is practically realized by scaling each gradient by the derivative
of the output logit with respect to the path. The algorithm thereby provides a
principled solution to the saturation problem. Additionally, we minimize the
errors within the Riemann sum approximation of the path integral by utilizing
non-uniform subdivisions determined by adaptive sampling. In the evaluation on
ImageNet, it is demonstrated that IDG outperforms IG, Left-IG, Guided IG, and
adversarial gradient integration both qualitatively and quantitatively using
standard insertion and deletion metrics across three common models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00899">Pitfalls in Link Prediction with Graph Neural Networks: Understanding the Impact of Target-link Inclusion &amp; Better Practices. (arXiv:2306.00899v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1">Jing Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuhang Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Ioannidis_V/0/1/0/all/0/1">Vassilis N. Ioannidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_S/0/1/0/all/0/1">Shengyi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ai_W/0/1/0/all/0/1">Wei Ai</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1">Xiang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1">Danai Koutra</a></p>
<p>While Graph Neural Networks (GNNs) are remarkably successful in a variety of
high-impact applications, we demonstrate that, in link prediction, the common
practices of including the edges being predicted in the graph at training
and/or test have outsized impact on the performance of low-degree nodes. We
theoretically and empirically investigate how these practices impact node-level
performance across different degrees. Specifically, we explore three issues
that arise: (I1) overfitting; (I2) distribution shift; and (I3) implicit test
leakage. The former two issues lead to poor generalizability to the test data,
while the latter leads to overestimation of the model's performance and
directly impacts the deployment of GNNs. To address these issues in a
systematic way, we introduce an effective and efficient GNN training framework,
SpotTarget, which leverages our insight on low-degree nodes: (1) at training
time, it excludes a (training) edge to be predicted if it is incident to at
least one low-degree node; and (2) at test time, it excludes all test edges to
be predicted (thus, mimicking real scenarios of using GNNs, where the test data
is not included in the graph). SpotTarget helps researchers and practitioners
adhere to best practices for learning from graph data, which are frequently
overlooked even by the most widely-used frameworks. Our experiments on various
real-world datasets show that SpotTarget makes GNNs up to 15x more accurate in
sparse graphs, and significantly improves their performance for low-degree
nodes in dense graphs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02235">Learning Linear Causal Representations from Interventions under General Nonlinear Mixing. (arXiv:2306.02235v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Buchholz_S/0/1/0/all/0/1">Simon Buchholz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendran_G/0/1/0/all/0/1">Goutham Rajendran</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenfeld_E/0/1/0/all/0/1">Elan Rosenfeld</a>, <a href="http://arxiv.org/find/cs/1/au:+Aragam_B/0/1/0/all/0/1">Bryon Aragam</a>, <a href="http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1">Bernhard Sch&#xf6;lkopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Ravikumar_P/0/1/0/all/0/1">Pradeep Ravikumar</a></p>
<p>We study the problem of learning causal representations from unknown, latent
interventions in a general setting, where the latent distribution is Gaussian
but the mixing function is completely general. We prove strong identifiability
results given unknown single-node interventions, i.e., without having access to
the intervention targets. This generalizes prior works which have focused on
weaker classes, such as linear maps or paired counterfactual data. This is also
the first instance of causal identifiability from non-paired interventions for
deep neural network embeddings. Our proof relies on carefully uncovering the
high-dimensional geometric structure present in the data distribution after a
non-linear density transformation, which we capture by analyzing quadratic
forms of precision matrices of the latent distributions. Finally, we propose a
contrastive algorithm to identify the latent variables in practice and evaluate
its performance on various tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03832">Sequential Principal-Agent Problems with Communication: Efficient Computation and Learning. (arXiv:2306.03832v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_J/0/1/0/all/0/1">Jiarui Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Majumdar_R/0/1/0/all/0/1">Rupak Majumdar</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandal_D/0/1/0/all/0/1">Debmalya Mandal</a>, <a href="http://arxiv.org/find/cs/1/au:+Radanovic_G/0/1/0/all/0/1">Goran Radanovic</a></p>
<p>We study a sequential decision making problem between a principal and an
agent with incomplete information on both sides. In this model, the principal
and the agent interact in a stochastic environment, and each is privy to
observations about the state not available to the other. The principal has the
power of commitment, both to elicit information from the agent and to provide
signals about her own information. The principal and the agent communicate
their signals to each other, and select their actions independently based on
this communication. Each player receives a payoff based on the state and their
joint actions, and the environment moves to a new state. The interaction
continues over a finite time horizon, and both players act to optimize their
own total payoffs over the horizon. Our model encompasses as special cases
stochastic games of incomplete information and POMDPs, as well as sequential
Bayesian persuasion and mechanism design problems. We study both computation of
optimal policies and learning in our setting. While the general problems are
computationally intractable, we study algorithmic solutions under a conditional
independence assumption on the underlying state-observation distributions. We
present a polynomial-time algorithm to compute the principal's optimal policy
up to an additive approximation. Additionally, we show an efficient learning
algorithm in the case where the transition probabilities are not known
beforehand. The algorithm guarantees sublinear regret for both players.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04445">Multi-modal Latent Diffusion. (arXiv:2306.04445v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bounoua_M/0/1/0/all/0/1">Mustapha Bounoua</a>, <a href="http://arxiv.org/find/cs/1/au:+Franzese_G/0/1/0/all/0/1">Giulio Franzese</a>, <a href="http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1">Pietro Michiardi</a></p>
<p>Multi-modal data-sets are ubiquitous in modern applications, and multi-modal
Variational Autoencoders are a popular family of models that aim to learn a
joint representation of the different modalities. However, existing approaches
suffer from a coherence-quality tradeoff, where models with good generation
quality lack generative coherence across modalities, and vice versa. We discuss
the limitations underlying the unsatisfactory performance of existing methods,
to motivate the need for a different approach. We propose a novel method that
uses a set of independently trained, uni-modal, deterministic autoencoders.
Individual latent variables are concatenated into a common latent space, which
is fed to a masked diffusion model to enable generative modeling. We also
introduce a new multi-time training method to learn the conditional score
network for multi-modal diffusion. Our methodology substantially outperforms
competitors in both generation quality and coherence, as shown through an
extensive experimental campaign.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.05144">Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean. (arXiv:2306.05144v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kondylatos_S/0/1/0/all/0/1">Spyros Kondylatos</a>, <a href="http://arxiv.org/find/cs/1/au:+Prapas_I/0/1/0/all/0/1">Ioannis Prapas</a>, <a href="http://arxiv.org/find/cs/1/au:+Camps_Valls_G/0/1/0/all/0/1">Gustau Camps-Valls</a>, <a href="http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1">Ioannis Papoutsis</a></p>
<p>We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.07254">On the Expected Size of Conformal Prediction Sets. (arXiv:2306.07254v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Dhillon_G/0/1/0/all/0/1">Guneet S. Dhillon</a>, <a href="http://arxiv.org/find/stat/1/au:+Deligiannidis_G/0/1/0/all/0/1">George Deligiannidis</a>, <a href="http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1">Tom Rainforth</a></p>
<p>While conformal predictors reap the benefits of rigorous statistical
guarantees on their error frequency, the size of their corresponding prediction
sets is critical to their practical utility. Unfortunately, there is currently
a lack of finite-sample analysis and guarantees for their prediction set sizes.
To address this shortfall, we theoretically quantify the expected size of the
prediction sets under the split conformal prediction framework. As this precise
formulation cannot usually be calculated directly, we further derive point
estimates and high-probability interval bounds that can be empirically
computed, providing a practical method for characterizing the expected set
size. We corroborate the efficacy of our results with experiments on real-world
datasets for both regression and classification problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08393">Provably Personalized and Robust Federated Learning. (arXiv:2306.08393v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Werner_M/0/1/0/all/0/1">Mariel Werner</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Lie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_M/0/1/0/all/0/1">Michael Jordan</a>, <a href="http://arxiv.org/find/cs/1/au:+Jaggi_M/0/1/0/all/0/1">Martin Jaggi</a>, <a href="http://arxiv.org/find/cs/1/au:+Karimireddy_S/0/1/0/all/0/1">Sai Praneeth Karimireddy</a></p>
<p>Identifying clients with similar objectives and learning a model-per-cluster
is an intuitive and interpretable approach to personalization in federated
learning. However, doing so with provable and optimal guarantees has remained
an open challenge. We formalize this problem as a stochastic optimization
problem, achieving optimal convergence rates for a large class of loss
functions. We propose simple iterative algorithms which identify clusters of
similar clients and train a personalized model-per-cluster, using local client
gradients and flexible constraints on the clusters. The convergence rates of
our algorithms asymptotically match those obtained if we knew the true
underlying clustering of the clients and are provably robust in the Byzantine
setting where some fraction of the clients are malicious.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.08451">A Survey on Blood Pressure Measurement Technologies: Addressing Potential Sources of Bias. (arXiv:2306.08451v3 [physics.med-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Mousavi_S/0/1/0/all/0/1">Seyedeh Somayyeh Mousavi</a>, <a href="http://arxiv.org/find/physics/1/au:+Reyna_M/0/1/0/all/0/1">Matthew A. Reyna</a>, <a href="http://arxiv.org/find/physics/1/au:+Clifford_G/0/1/0/all/0/1">Gari D. Clifford</a>, <a href="http://arxiv.org/find/physics/1/au:+Sameni_R/0/1/0/all/0/1">Reza Sameni</a></p>
<p>Regular blood pressure (BP) monitoring in clinical and ambulatory settings
plays a crucial role in the prevention, diagnosis, treatment, and management of
cardiovascular diseases. Recently, the widespread adoption of ambulatory BP
measurement devices has been driven predominantly by the increased prevalence
of hypertension and its associated risks and clinical conditions. Recent
guidelines advocate for regular BP monitoring as part of regular clinical
visits or even at home. This increased utilization of BP measurement
technologies has brought up significant concerns, regarding the accuracy of
reported BP values across settings. In this survey, focusing mainly on
cuff-based BP monitoring technologies, we highlight how BP measurements can
demonstrate substantial biases and variances due to factors such as measurement
and device errors, demographics, and body habitus. With these inherent biases,
the development of a new generation of cuff-based BP devices which use
artificial-intelligence (AI) has significant potential. We present future
avenues where AI-assisted technologies can leverage the extensive clinical
literature on BP-related studies together with the large collections of BP
records available in electronic health records. These resources can be combined
with machine learning approaches, including deep learning and Bayesian
inference, to remove BP measurement biases and to provide individualized
BP-related cardiovascular risk indexes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.12212">MimiC: Combating Client Dropouts in Federated Learning by Mimicking Central Updates. (arXiv:2306.12212v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yuchang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuyi Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jun Zhang</a></p>
<p>Federated learning (FL) is a promising framework for privacy-preserving
collaborative learning, where model training tasks are distributed to clients
and only the model updates need to be collected at a server. However, when
being deployed at mobile edge networks, clients may have unpredictable
availability and drop out of the training process, which hinders the
convergence of FL. This paper tackles such a critical challenge. Specifically,
we first investigate the convergence of the classical FedAvg algorithm with
arbitrary client dropouts. We find that with the common choice of a decaying
learning rate, FedAvg oscillates around a stationary point of the global loss
function, which is caused by the divergence between the aggregated and desired
central update. Motivated by this new observation, we then design a novel
training algorithm named MimiC, where the server modifies each received model
update based on the previous ones. The proposed modification of the received
model updates mimics the imaginary central update irrespective of dropout
clients. The theoretical analysis of MimiC shows that divergence between the
aggregated and central update diminishes with proper learning rates, leading to
its convergence. Simulation results further demonstrate that MimiC maintains
stable convergence performance and learns better models than the baseline
methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13803">Elephants and Algorithms: A Review of the Current and Future Role of AI in Elephant Monitoring. (arXiv:2306.13803v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brickson_L/0/1/0/all/0/1">Leandra Brickson</a>, <a href="http://arxiv.org/find/cs/1/au:+Vollrath_F/0/1/0/all/0/1">Fritz Vollrath</a>, <a href="http://arxiv.org/find/cs/1/au:+Titus_A/0/1/0/all/0/1">Alexander J. Titus</a></p>
<p>Artificial intelligence (AI) and machine learning (ML) present revolutionary
opportunities to enhance our understanding of animal behavior and conservation
strategies. Using elephants, a crucial species in Africa's protected areas, as
our focal point, we delve into the role of AI and ML in their conservation.
Given the increasing amounts of data gathered from a variety of sensors like
cameras, microphones, geophones, drones, and satellites, the challenge lies in
managing and interpreting this vast data. New AI and ML techniques offer
solutions to streamline this process, helping us extract vital information that
might otherwise be overlooked. This paper focuses on the different AI-driven
monitoring methods and their potential for improving elephant conservation.
Collaborative efforts between AI experts and ecological researchers are
essential in leveraging these innovative technologies for enhanced wildlife
conservation, setting a precedent for numerous other species.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01146">AVSegFormer: Audio-Visual Segmentation with Transformer. (arXiv:2307.01146v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1">Shengyi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1">Guo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenhai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tong Lu</a></p>
<p>The combination of audio and vision has long been a topic of interest in the
multi-modal community. Recently, a new audio-visual segmentation (AVS) task has
been introduced, aiming to locate and segment the sounding objects in a given
video. This task demands audio-driven pixel-level scene understanding for the
first time, posing significant challenges. In this paper, we propose
AVSegFormer, a novel framework for AVS tasks that leverages the transformer
architecture. Specifically, we introduce audio queries and learnable queries
into the transformer decoder, enabling the network to selectively attend to
interested visual features. Besides, we present an audio-visual mixer, which
can dynamically adjust visual features by amplifying relevant and suppressing
irrelevant spatial channels. Additionally, we devise an intermediate mask loss
to enhance the supervision of the decoder, encouraging the network to produce
more accurate intermediate predictions. Extensive experiments demonstrate that
AVSegFormer achieves state-of-the-art results on the AVS benchmark. The code is
available at https://github.com/vvvb-github/AVSegFormer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.01504">All in One: Multi-task Prompting for Graph Neural Networks. (arXiv:2307.01504v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xiangguo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hong Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1">Jihong Guan</a></p>
<p>Recently, ''pre-training and fine-tuning'' has been adopted as a standard
workflow for many graph tasks since it can take general graph knowledge to
relieve the lack of graph annotations from each application. However, graph
tasks with node level, edge level, and graph level are far diversified, making
the pre-training pretext often incompatible with these multiple tasks. This gap
may even cause a ''negative transfer'' to the specific application, leading to
poor results. Inspired by the prompt learning in natural language processing
(NLP), which has presented significant effectiveness in leveraging prior
knowledge for various NLP tasks, we study the prompting topic for graphs with
the motivation of filling the gap between pre-trained models and various graph
tasks. In this paper, we propose a novel multi-task prompting method for graph
models. Specifically, we first unify the format of graph prompts and language
prompts with the prompt token, token structure, and inserting pattern. In this
way, the prompting idea from NLP can be seamlessly introduced to the graph
area. Then, to further narrow the gap between various graph tasks and
state-of-the-art pre-training strategies, we further study the task space of
various graph applications and reformulate downstream problems to the
graph-level task. Afterward, we introduce meta-learning to efficiently learn a
better initialization for the multi-task prompt of graphs so that our prompting
framework can be more reliable and general for different tasks. We conduct
extensive experiments, results from which demonstrate the superiority of our
method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02507">STS-CCL: Spatial-Temporal Synchronous Contextual Contrastive Learning for Urban Traffic Forecasting. (arXiv:2307.02507v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lincan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kaixiang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1">Fengji Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_J/0/1/0/all/0/1">Jichao Bi</a></p>
<p>Efficiently capturing the complex spatiotemporal representations from
large-scale unlabeled traffic data remains to be a challenging task. In
considering of the dilemma, this work employs the advanced contrastive learning
and proposes a novel Spatial-Temporal Synchronous Contextual Contrastive
Learning (STS-CCL) model. First, we elaborate the basic and strong augmentation
methods for spatiotemporal graph data, which not only perturb the data in terms
of graph structure and temporal characteristics, but also employ a
learning-based dynamic graph view generator for adaptive augmentation. Second,
we introduce a Spatial-Temporal Synchronous Contrastive Module (STS-CM) to
simultaneously capture the decent spatial-temporal dependencies and realize
graph-level contrasting. To further discriminate node individuals in negative
filtering, a Semantic Contextual Contrastive method is designed based on
semantic features and spatial heterogeneity, achieving node-level contrastive
learning along with negative filtering. Finally, we present a hard mutual-view
contrastive training scheme and extend the classic contrastive loss to an
integrated objective function, yielding better performance. Extensive
experiments and evaluations demonstrate that building a predictor upon STS-CCL
contrastive learning model gains superior performance than existing traffic
forecasting benchmarks. The proposed STS-CCL is highly suitable for large
datasets with only a few labeled data and other spatiotemporal tasks with data
scarcity issue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05845">PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1">Lukas Haas</a>, <a href="http://arxiv.org/find/cs/1/au:+Skreta_M/0/1/0/all/0/1">Michal Skreta</a>, <a href="http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1">Silas Alberti</a>, <a href="http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1">Chelsea Finn</a></p>
<p>Planet-scale image geolocalization remains a challenging problem due to the
diversity of images originating from anywhere in the world. Although approaches
based on vision transformers have made significant progress in geolocalization
accuracy, success in prior literature is constrained to narrow distributions of
images of landmarks, and performance has not generalized to unseen places. We
present a new geolocalization system that combines semantic geocell creation,
multi-task contrastive pretraining, and a novel loss function. Additionally,
our work is the first to perform retrieval over location clusters for guess
refinements. We train two models for evaluations on street-level data and
general-purpose image geolocalization; the first model, PIGEON, is trained on
data from the game of Geoguessr and is capable of placing over 40% of its
guesses within 25 kilometers of the target location globally. We also develop a
bot and deploy PIGEON in a blind experiment against humans, ranking in the top
0.01% of players. We further challenge one of the world's foremost professional
Geoguessr players to a series of six matches with millions of viewers, winning
all six games. Our second model, PIGEOTTO, differs in that it is trained on a
dataset of images from Flickr and Wikipedia, achieving state-of-the-art results
on a wide range of image geolocalization benchmarks, outperforming the previous
SOTA by up to 7.7 percentage points on the city accuracy level and up to 38.8
percentage points on the country level. Our findings suggest that PIGEOTTO is
the first image geolocalization model that effectively generalizes to unseen
places and that our approach can pave the way for highly accurate, planet-scale
image geolocalization systems. Our code is available on GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06148">NetGPT: A Native-AI Network Architecture Beyond Provisioning Personalized Generative Services. (arXiv:2307.06148v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1">Rongpeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1">Zhifeng Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1">Chenghui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jianjun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1">Ekram Hossain</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Honggang Zhang</a></p>
<p>Large language models (LLMs) have triggered tremendous success to empower our
daily life by generative information. The personalization of LLMs could further
contribute to their applications due to better alignment with human intents.
Towards personalized generative services, a collaborative cloud-edge
methodology is promising, as it facilitates the effective orchestration of
heterogeneous distributed communication and computing resources. In this
article, we put forward NetGPT to capably synergize appropriate LLMs at the
edge and the cloud based on their computing capacity. In addition, edge LLMs
could efficiently leverage location-based information for personalized prompt
completion, thus benefiting the interaction with the cloud LLM. In particular,
we present the feasibility of NetGPT by leveraging low-rank adaptation-based
fine-tuning of open-source LLMs (i.e., GPT-2-base model and LLaMA model), and
conduct comprehensive numerical comparisons with alternative cloud-edge
collaboration or cloud-only techniques, so as to demonstrate the superiority of
NetGPT. Subsequently, we highlight the essential changes required for an
artificial intelligence (AI)-native network architecture towards NetGPT, with
emphasis on deeper integration of communications and computing resources and
careful calibration of logical AI workflow. Furthermore, we demonstrate several
benefits of NetGPT, which come as by-products, as the edge LLMs' capability to
predict trends and infer intents promises a unified solution for intelligent
network management &amp; orchestration. We argue that NetGPT is a promising
AI-native network architecture for provisioning beyond personalized generative
services.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08506">Does Visual Pretraining Help End-to-End Reasoning?. (arXiv:2307.08506v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Chen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Calvin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xingyi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Arnab_A/0/1/0/all/0/1">Anurag Arnab</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_C/0/1/0/all/0/1">Cordelia Schmid</a></p>
<p>We aim to investigate whether end-to-end learning of visual reasoning can be
achieved with general-purpose neural networks, with the help of visual
pretraining. A positive result would refute the common belief that explicit
visual abstraction (e.g. object detection) is essential for compositional
generalization on visual reasoning, and confirm the feasibility of a neural
network "generalist" to solve visual recognition and reasoning tasks. We
propose a simple and general self-supervised framework which "compresses" each
video frame into a small set of tokens with a transformer network, and
reconstructs the remaining frames based on the compressed temporal context. To
minimize the reconstruction loss, the network must learn a compact
representation for each image, as well as capture temporal dynamics and object
permanence from temporal context. We perform evaluation on two visual reasoning
benchmarks, CATER and ACRE. We observe that pretraining is essential to achieve
compositional generalization for end-to-end visual reasoning. Our proposed
framework outperforms traditional supervised pretraining, including image
classification and explicit object detection, by large margins.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.13709">Neural Bradley-Terry Rating: Quantifying Properties from Comparisons. (arXiv:2307.13709v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fujii_S/0/1/0/all/0/1">Satoru Fujii</a></p>
<p>Many properties in the real world doesn't have metrics and can't be
numerically observed, making them difficult to learn. To deal with this
challenging problem, prior works have primarily focused on estimating those
properties by using graded human scores as the target label in the training.
Meanwhile, rating algorithms based on the Bradley-Terry model are extensively
studied to evaluate the competitiveness of players based on their match
history. In this paper, we introduce the Neural Bradley-Terry Rating (NBTR), a
novel machine learning framework designed to quantify and evaluate properties
of unknown items. Our method seamlessly integrates the Bradley-Terry model into
the neural network structure. Moreover, we generalize this architecture further
to asymmetric environments with unfairness, a condition more commonly
encountered in real-world settings. Through experimental analysis, we
demonstrate that NBTR successfully learns to quantify and estimate desired
properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.06764">Few-shot Class-incremental Learning: A Survey. (arXiv:2308.06764v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jinghua Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1">Li Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Silven_O/0/1/0/all/0/1">Olli Silv&#xe9;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Pietikainen_M/0/1/0/all/0/1">Matti Pietik&#xe4;inen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1">Dewen Hu</a></p>
<p>Few-shot Class-Incremental Learning (FSCIL) presents a unique challenge in
Machine Learning (ML), as it necessitates the Incremental Learning (IL) of new
classes from sparsely labeled training samples without forgetting previous
knowledge. While this field has seen recent progress, it remains an active
exploration area. This paper aims to provide a comprehensive and systematic
review of FSCIL. In our in-depth examination, we delve into various facets of
FSCIL, encompassing the problem definition, the discussion of the primary
challenges of unreliable empirical risk minimization and the
stability-plasticity dilemma, general schemes, and relevant problems of IL and
Few-shot Learning (FSL). Besides, we offer an overview of benchmark datasets
and evaluation metrics. Furthermore, we introduce the Few-shot
Class-incremental Classification (FSCIC) methods from data-based,
structure-based, and optimization-based approaches and the Few-shot
Class-incremental Object Detection (FSCIOD) methods from anchor-free and
anchor-based approaches. Beyond these, we present several promising research
directions within FSCIL that merit further investigation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07170">Human Voice Pitch Estimation: A Convolutional Network with Auto-Labeled and Synthetic Data. (arXiv:2308.07170v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cochoy_J/0/1/0/all/0/1">Jeremy Cochoy</a></p>
<p>In the domain of music and sound processing, pitch extraction plays a pivotal
role. Our research presents a specialized convolutional neural network designed
for pitch extraction, particularly from the human singing voice in acapella
performances. Notably, our approach combines synthetic data with auto-labeled
acapella sung audio, creating a robust training environment. Evaluation across
datasets comprising synthetic sounds, opera recordings, and time-stretched
vowels demonstrates its efficacy. This work paves the way for enhanced pitch
extraction in both music and voice settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07728">Domain-Aware Fine-Tuning: Enhancing Neural Network Adaptability. (arXiv:2308.07728v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1">Seokhyeon Ha</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1">Sunbeom Jung</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jungwoo Lee</a></p>
<p>Fine-tuning pre-trained neural network models has become a widely adopted
approach across various domains. However, it can lead to the distortion of
pre-trained feature extractors that already possess strong generalization
capabilities. Mitigating feature distortion during adaptation to new target
domains is crucial. Recent studies have shown promising results in handling
feature distortion by aligning the head layer on in-distribution datasets
before performing fine-tuning. Nonetheless, a significant limitation arises
from the treatment of batch normalization layers during fine-tuning, leading to
suboptimal performance. In this paper, we propose Domain-Aware Fine-Tuning
(DAFT), a novel approach that incorporates batch normalization conversion and
the integration of linear probing and fine-tuning. Our batch normalization
conversion method effectively mitigates feature distortion by reducing
modifications to the neural network during fine-tuning. Additionally, we
introduce the integration of linear probing and fine-tuning to optimize the
head layer with gradual adaptation of the feature extractor. By leveraging
batch normalization layers and integrating linear probing and fine-tuning, our
DAFT significantly mitigates feature distortion and achieves improved model
performance on both in-distribution and out-of-distribution datasets. Extensive
experiments demonstrate that our method outperforms other baseline methods,
demonstrating its effectiveness in not only improving performance but also
mitigating feature distortion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08536">Can Transformers Learn Optimal Filtering for Unknown Systems?. (arXiv:2308.08536v2 [eess.SY] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Balim_H/0/1/0/all/0/1">Haldun Balim</a>, <a href="http://arxiv.org/find/eess/1/au:+Du_Z/0/1/0/all/0/1">Zhe Du</a>, <a href="http://arxiv.org/find/eess/1/au:+Oymak_S/0/1/0/all/0/1">Samet Oymak</a>, <a href="http://arxiv.org/find/eess/1/au:+Ozay_N/0/1/0/all/0/1">Necmiye Ozay</a></p>
<p>Transformer models have shown great success in natural language processing;
however, their potential remains mostly unexplored for dynamical systems. In
this work, we investigate the optimal output estimation problem using
transformers, which generate output predictions using all the past ones.
Particularly, we train the transformer using various distinct systems and then
evaluate the performance on unseen systems with unknown dynamics. Empirically,
the trained transformer adapts exceedingly well to different unseen systems and
even matches the optimal performance given by the Kalman filter for linear
systems. In more complex settings with non-i.i.d. noise, time-varying dynamics,
and nonlinear dynamics like a quadrotor system with unknown parameters,
transformers also demonstrate promising results. To support our experimental
findings, we provide statistical guarantees that quantify the amount of
training data required for the transformer to achieve a desired excess risk.
Finally, we point out some limitations by identifying two classes of problems
that lead to degraded performance, highlighting the need for caution when using
transformers for control and estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.08989">Neural oscillators for generalization of physics-informed machine learning. (arXiv:2308.08989v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kapoor_T/0/1/0/all/0/1">Taniya Kapoor</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandra_A/0/1/0/all/0/1">Abhishek Chandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartakovsky_D/0/1/0/all/0/1">Daniel M. Tartakovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hongrui Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nunez_A/0/1/0/all/0/1">Alfredo Nunez</a>, <a href="http://arxiv.org/find/cs/1/au:+Dollevoet_R/0/1/0/all/0/1">Rolf Dollevoet</a></p>
<p>A primary challenge of physics-informed machine learning (PIML) is its
generalization beyond the training domain, especially when dealing with complex
physical problems represented by partial differential equations (PDEs). This
paper aims to enhance the generalization capabilities of PIML, facilitating
practical, real-world applications where accurate predictions in unexplored
regions are crucial. We leverage the inherent causality and temporal sequential
characteristics of PDE solutions to fuse PIML models with recurrent neural
architectures based on systems of ordinary differential equations, referred to
as neural oscillators. Through effectively capturing long-time dependencies and
mitigating the exploding and vanishing gradient problem, neural oscillators
foster improved generalization in PIML tasks. Extensive experimentation
involving time-dependent nonlinear PDEs and biharmonic beam equations
demonstrates the efficacy of the proposed approach. Incorporating neural
oscillators outperforms existing state-of-the-art methods on benchmark problems
across various metrics. Consequently, the proposed method improves the
generalization capabilities of PIML, providing accurate solutions for
extrapolation and prediction beyond the training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09430">Towards Understanding the Generalizability of Delayed Stochastic Gradient Descent. (arXiv:2308.09430v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1">Xiaoge Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1">Li Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tao Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongsheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Stochastic gradient descent (SGD) performed in an asynchronous manner plays a
crucial role in training large-scale machine learning models. However, the
generalization performance of asynchronous delayed SGD, which is an essential
metric for assessing machine learning algorithms, has rarely been explored.
Existing generalization error bounds are rather pessimistic and cannot reveal
the correlation between asynchronous delays and generalization. In this paper,
we investigate sharper generalization error bound for SGD with asynchronous
delay $\tau$. Leveraging the generating function analysis tool, we first
establish the average stability of the delayed gradient algorithm. Based on
this algorithmic stability, we provide upper bounds on the generalization error
of $\tilde{\mathcal{O}}(\frac{T-\tau}{n\tau})$ and
$\tilde{\mathcal{O}}(\frac{1}{n})$ for quadratic convex and strongly convex
problems, respectively, where $T$ refers to the iteration number and $n$ is the
amount of training data. Our theoretical results indicate that asynchronous
delays reduce the generalization error of the delayed SGD algorithm. Analogous
analysis can be generalized to the random delay setting, and the experimental
results validate our theoretical findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09437">From Hope to Safety: Unlearning Biases of Deep Models via Gradient Penalization in Latent Space. (arXiv:2308.09437v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1">Maximilian Dreyer</a>, <a href="http://arxiv.org/find/cs/1/au:+Pahde_F/0/1/0/all/0/1">Frederik Pahde</a>, <a href="http://arxiv.org/find/cs/1/au:+Anders_C/0/1/0/all/0/1">Christopher J. Anders</a>, <a href="http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1">Wojciech Samek</a>, <a href="http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1">Sebastian Lapuschkin</a></p>
<p>Deep Neural Networks are prone to learning spurious correlations embedded in
the training data, leading to potentially biased predictions. This poses risks
when deploying these models for high-stake decision-making, such as in medical
applications. Current methods for post-hoc model correction either require
input-level annotations which are only possible for spatially localized biases,
or augment the latent feature space, thereby hoping to enforce the right
reasons. We present a novel method for model correction on the concept level
that explicitly reduces model sensitivity towards biases via gradient
penalization. When modeling biases via Concept Activation Vectors, we highlight
the importance of choosing robust directions, as traditional regression-based
approaches such as Support Vector Machines tend to result in diverging
directions. We effectively mitigate biases in controlled and real-world
settings on the ISIC, Bone Age, ImageNet and CelebA datasets using VGG, ResNet
and EfficientNet architectures. Code is available on
https://github.com/frederikpahde/rrclarc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09720">On the Unexpected Abilities of Large Language Models. (arXiv:2308.09720v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nolfi_S/0/1/0/all/0/1">Stefano Nolfi</a></p>
<p>Large Language Models (LLMs) are capable of displaying a wide range of
abilities that are not directly connected with the task for which they are
trained: predicting the next words of human-written texts. In this article, I
review recent research investigating the cognitive abilities developed by LLMs
and their relation to human cognition. I discuss the nature of the indirect
process that leads to the acquisition of these cognitive abilities, their
relation to other indirect processes, and the implications for the acquisition
of integrated abilities. Moreover, I propose the factors that enable the
development of abilities that are related only very indirectly to the proximal
objective of the training task. Finally, I discuss whether the full set of
capabilities that LLMs could possibly develop is predictable.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.09936">BLIVA: A Simple Multimodal LLM for Better Handling of Text-Rich Visual Questions. (arXiv:2308.09936v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenbo Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weiyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zeyuan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a></p>
<p>Vision Language Models (VLMs), which extend Large Language Models (LLM) by
incorporating visual understanding capability, have demonstrated significant
advancements in addressing open-ended visual question-answering (VQA) tasks.
However, these models cannot accurately interpret images infused with text, a
common occurrence in real-world scenarios. Standard procedures for extracting
information from images often involve learning a fixed set of query embeddings.
These embeddings are designed to encapsulate image contexts and are later used
as soft prompt inputs in LLMs. Yet, this process is limited to the token count,
potentially curtailing the recognition of scenes with text-rich context. To
improve upon them, the present study introduces BLIVA: an augmented version of
InstructBLIP with Visual Assistant. BLIVA incorporates the query embeddings
from InstructBLIP and also directly projects encoded patch embeddings into the
LLM, a technique inspired by LLaVA. This approach assists the model to capture
intricate details potentially missed during the query decoding process.
Empirical evidence demonstrates that our model, BLIVA, significantly enhances
performance in processing text-rich VQA benchmarks (up to 17.76% in OCR-VQA
benchmark) and in undertaking general (not particularly text-rich) VQA
benchmarks (up to 7.9% in Visual Spatial Reasoning benchmark), and achieved
17.72% overall improvement in a comprehensive multimodal LLM benchmark (MME),
comparing to our baseline InstructBLIP. BLIVA demonstrates significant
capability in decoding real-world images, irrespective of text presence. To
demonstrate the broad industry applications enabled by BLIVA, we evaluate the
model using a new dataset comprising YouTube thumbnails paired with
question-answer sets across 11 diverse categories. Our code and models are
freely accessible at https://github.com/mlpc-ucsd/BLIVA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10144">ExpeL: LLM Agents Are Experiential Learners. (arXiv:2308.10144v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1">Andrew Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1">Daniel Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1">Quentin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Matthieu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong-Jin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1">Gao Huang</a></p>
<p>The recent surge in research interest in applying large language models
(LLMs) to decision-making tasks has flourished by leveraging the extensive
world knowledge embedded in LLMs. While there is a growing demand to tailor
LLMs for custom decision-making tasks, finetuning them for specific tasks is
resource-intensive and may diminish the model's generalization capabilities.
Moreover, state-of-the-art language models like GPT-4 and Claude are primarily
accessible through API calls, with their parametric weights remaining
proprietary and unavailable to the public. This scenario emphasizes the growing
need for new methodologies that allow learning from agent experiences without
requiring parametric updates. To address these problems, we introduce the
Experiential Learning (ExpeL) agent. Our agent autonomously gathers experiences
and extracts knowledge using natural language from a collection of training
tasks. At inference, the agent recalls its extracted insights and past
experiences to make informed decisions. Our empirical results highlight the
robust learning efficacy of the ExpeL agent, indicating a consistent
enhancement in its performance as it accumulates experiences. We further
explore the emerging capabilities and transfer learning potential of the ExpeL
agent through qualitative observations and additional experiments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10997">MarkovGen: Structured Prediction for Efficient Text-to-Image Generation. (arXiv:2308.10997v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jayasumana_S/0/1/0/all/0/1">Sadeep Jayasumana</a>, <a href="http://arxiv.org/find/cs/1/au:+Glasner_D/0/1/0/all/0/1">Daniel Glasner</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramalingam_S/0/1/0/all/0/1">Srikumar Ramalingam</a>, <a href="http://arxiv.org/find/cs/1/au:+Veit_A/0/1/0/all/0/1">Andreas Veit</a>, <a href="http://arxiv.org/find/cs/1/au:+Chakrabarti_A/0/1/0/all/0/1">Ayan Chakrabarti</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1">Sanjiv Kumar</a></p>
<p>Modern text-to-image generation models produce high-quality images that are
both photorealistic and faithful to the text prompts. However, this quality
comes at significant computational cost: nearly all of these models are
iterative and require running sampling multiple times with large models. This
iterative process is needed to ensure that different regions of the image are
not only aligned with the text prompt, but also compatible with each other. In
this work, we propose a light-weight approach to achieving this compatibility
between different regions of an image, using a Markov Random Field (MRF) model.
We demonstrate the effectiveness of this method on top of the latent
token-based Muse text-to-image model. The MRF richly encodes the compatibility
among image tokens at different spatial locations to improve quality and
significantly reduce the required number of Muse sampling steps. Inference with
the MRF is significantly cheaper, and its parameters can be quickly learned
through back-propagation by modeling MRF inference as a differentiable
neural-network layer. Our full model, MarkovGen, uses this proposed MRF model
to both speed up Muse by 1.5X and produce higher quality images by decreasing
undesirable image artifacts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11730">Knowledge Graph Prompting for Multi-Document Question Answering. (arXiv:2308.11730v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1">Nedim Lipka</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1">Ryan A. Rossi</a>, <a href="http://arxiv.org/find/cs/1/au:+Siu_A/0/1/0/all/0/1">Alexa Siu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruiyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1">Tyler Derr</a></p>
<p>The `pre-train, prompt, predict' paradigm of large language models (LLMs) has
achieved remarkable success in open-domain question answering (OD-QA). However,
few works explore this paradigm in the scenario of multi-document question
answering (MD-QA), a task demanding a thorough understanding of the logical
associations among the contents and structures of different documents. To fill
this crucial gap, we propose a Knowledge Graph Prompting (KGP) method to
formulate the right context in prompting LLMs for MD-QA, which consists of a
graph construction module and a graph traversal module. For graph construction,
we create a knowledge graph (KG) over multiple documents with nodes symbolizing
passages or document structures (e.g., pages/tables), and edges denoting the
semantic/lexical similarity between passages or intra-document structural
relations. For graph traversal, we design an LLM-based graph traversal agent
that navigates across nodes and gathers supporting passages assisting LLMs in
MD-QA. The constructed graph serves as the global ruler that regulates the
transitional space among passages and reduces retrieval latency. Concurrently,
the graph traversal agent acts as a local navigator that gathers pertinent
context to progressively approach the question and guarantee retrieval quality.
Extensive experiments underscore the efficacy of KGP for MD-QA, signifying the
potential of leveraging graphs in enhancing the prompt design for LLMs. Our
code: https://github.com/YuWVandy/KG-LLM-MDQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11940">Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhifang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jianguo Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Rui Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1">Long Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouchi_K/0/1/0/all/0/1">Kazushige Ouchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangdong Wang</a></p>
<p>Text-based audio generation models have limitations as they cannot encompass
all the information in audio, leading to restricted controllability when
relying solely on text. To address this issue, we propose a novel model that
enhances the controllability of existing pre-trained text-to-audio models by
incorporating additional conditions including content (timestamp) and style
(pitch contour and energy contour) as supplements to the text. This approach
achieves fine-grained control over the temporal order, pitch, and energy of
generated audio. To preserve the diversity of generation, we employ a trainable
control condition encoder that is enhanced by a large language model and a
trainable Fusion-Net to encode and fuse the additional conditions while keeping
the weights of the pre-trained text-to-audio model frozen. Due to the lack of
suitable datasets and evaluation metrics, we consolidate existing datasets into
a new dataset comprising the audio and corresponding conditions and use a
series of evaluation metrics to evaluate the controllability performance.
Experimental results demonstrate that our model successfully achieves
fine-grained control to accomplish controllable audio generation. Audio samples
and our dataset are publicly available at
https://conditionaudiogen.github.io/conditionaudiogen/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11978">Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1">Xiandong Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiangyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1">Yiren Zhao</a></p>
<p>Graph generation poses a significant challenge as it involves predicting a
complete graph with multiple nodes and edges based on simply a given label.
This task also carries fundamental importance to numerous real-world
applications, including de-novo drug and molecular design. In recent years,
several successful methods have emerged in the field of graph generation.
However, these approaches suffer from two significant shortcomings: (1) the
underlying Graph Neural Network (GNN) architectures used in these methods are
often underexplored; and (2) these methods are often evaluated on only a
limited number of metrics. To fill this gap, we investigate the expressiveness
of GNNs under the context of the molecular graph generation task, by replacing
the underlying GNNs of graph generative models with more expressive GNNs.
Specifically, we analyse the performance of six GNNs on six different molecular
generative objectives on the ZINC-250k dataset in two different generative
frameworks: autoregressive generation models, such as GCPN and GraphAF, and
one-shot generation models, such as GraphEBM. Through our extensive
experiments, we demonstrate that advanced GNNs can indeed improve the
performance of GCPN, GraphAF, and GraphEBM on molecular generation tasks, but
GNN expressiveness is not a necessary condition for a good GNN-based generative
model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve
state-of-the-art results across 17 other non-GNN-based graph generative
approaches, such as variational autoencoders and Bayesian optimisation models,
on the proposed molecular generative objectives (DRD2, Median1, Median2), which
are important metrics for de-novo molecular design.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13616">Channel Estimation in RIS-Enabled mmWave Wireless Systems: A Variational Inference Approach. (arXiv:2308.13616v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Fredj_F/0/1/0/all/0/1">Firas Fredj</a>, <a href="http://arxiv.org/find/eess/1/au:+Feriani_A/0/1/0/all/0/1">Amal Feriani</a>, <a href="http://arxiv.org/find/eess/1/au:+Mezghani_A/0/1/0/all/0/1">Amine Mezghani</a>, <a href="http://arxiv.org/find/eess/1/au:+Hossain_E/0/1/0/all/0/1">Ekram Hossain</a></p>
<p>Channel estimation in reconfigurable intelligent surfaces (RIS)-aided systems
is crucial for optimal configuration of the RIS and various downstream tasks
such as user localization. In RIS-aided systems, channel estimation involves
estimating two channels for the user-RIS (UE-RIS) and RIS-base station (RIS-BS)
links. In the literature, two approaches are proposed: (i) cascaded channel
estimation where the two channels are collapsed into a single one and estimated
using training signals at the BS, and (ii) separate channel estimation that
estimates each channel separately either in a passive or semi-passive RIS
setting. In this work, we study the separate channel estimation problem in a
fully passive RIS-aided millimeter-wave (mmWave) single-user single-input
multiple-output (SIMO) communication system. First, we adopt a
variational-inference (VI) approach to jointly estimate the UE-RIS and RIS-BS
instantaneous channel state information (I-CSI). In particular, auxiliary
posterior distributions of the I-CSI are learned through the maximization of
the evidence lower bound. However, estimating the I-CSI for both links in every
coherence block results in a high signaling overhead to control the RIS in
scenarios with highly mobile users. Thus, we extend our first approach to
estimate the slow-varying statistical CSI of the UE-RIS link overcoming the
highly variant I-CSI. Precisely, our second method estimates the I-CSI of
RIS-BS channel and the UE-RIS channel covariance matrix (CCM) directly from the
uplink training signals in a fully passive RIS-aided system. The simulation
results demonstrate that using maximum a posteriori channel estimation using
the auxiliary posteriors can provide a capacity that approaches the capacity
with perfect CSI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13838">Price-Discrimination Game for Distributed Resource Management in Federated Learning. (arXiv:2308.13838v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Halvin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guopeng Zhang</a></p>
<p>In vanilla federated learning (FL) such as FedAvg, the parameter server (PS)
and multiple distributed clients can form a typical buyer's market, where the
number of PS/buyers of FL services is far less than the number of
clients/sellers. In order to improve the performance of FL and reduce the cost
of motivating clients to participate in FL, this paper proposes to
differentiate the pricing for services provided by different clients rather
than simply providing the same service pricing for different clients. The price
is differentiated based on the performance improvements brought to FL and their
heterogeneity in computing and communication capabilities. To this end, a
price-discrimination game (PDG) is formulated to comprehensively address the
distributed resource management problems in FL, including multi-objective
trade-off, client selection, and incentive mechanism. As the PDG is a
mixed-integer nonlinear programming (MINLP) problem, a distributed
semi-heuristic algorithm with low computational complexity and low
communication overhead is designed to solve it. The simulation result verifies
the effectiveness of the proposed approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15452">When Do Program-of-Thoughts Work for Reasoning?. (arXiv:2308.15452v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1">Zhen Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yinuo Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1">Shumin Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1">Guozhou Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>In the realm of embodied artificial intelligence, the reasoning capabilities
of Large Language Models (LLMs) play a pivotal role. Although there are
effective methods like program-of-thought prompting for LLMs which uses
programming language to tackle complex reasoning tasks, the specific impact of
code data on the improvement of reasoning capabilities remains under-explored.
To address this gap, we propose complexity-impacted reasoning score (CIRS),
which combines structural and logical attributes, to measure the correlation
between code and reasoning abilities. Specifically, we use the abstract syntax
tree to encode the structural information and calculate logical complexity by
considering the difficulty and the cyclomatic complexity. Through an empirical
analysis, we find not all code data of complexity can be learned or understood
by LLMs. Optimal level of complexity is critical to the improvement of
reasoning abilities by program-aided prompting. Then we design an
auto-synthesizing and stratifying algorithm, and apply it to instruction
generation for mathematical reasoning and code data filtering for code
generation tasks. Extensive results demonstrates the effectiveness of our
proposed approach. Code will be integrated into the EasyInstruct framework at
https://github.com/zjunlp/EasyInstruct.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04788">Stochastic Gradient Descent outperforms Gradient Descent in recovering a high-dimensional signal in a glassy energy landscape. (arXiv:2309.04788v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kamali_P/0/1/0/all/0/1">Persia Jana Kamali</a>, <a href="http://arxiv.org/find/cs/1/au:+Urbani_P/0/1/0/all/0/1">Pierfrancesco Urbani</a></p>
<p>Stochastic Gradient Descent (SGD) is an out-of-equilibrium algorithm used
extensively to train artificial neural networks. However very little is known
on to what extent SGD is crucial for to the success of this technology and, in
particular, how much it is effective in optimizing high-dimensional non-convex
cost functions as compared to other optimization algorithms such as Gradient
Descent (GD). In this work we leverage dynamical mean field theory to benchmark
its performances in the high-dimensional limit. To do that, we consider the
problem of recovering a hidden high-dimensional non-linearly encrypted signal,
a prototype high-dimensional non-convex hard optimization problem. We compare
the performances of SGD to GD and we show that SGD largely outperforms GD for
sufficiently small batch sizes. In particular, a power law fit of the
relaxation time of these algorithms shows that the recovery threshold for SGD
with small batch size is smaller than the corresponding one of GD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05173">DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhengxiang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1">Aldo Lipani</a></p>
<p>Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer's quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving over 20% memory and time costs compared to vanilla PT
and its variants, without changing trainable parameter sizes. Through extensive
experiments on 23 natural language processing (NLP) and vision-language (VL)
tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,
including the full fine-tuning baseline in some scenarios. Additionally, we
empirically show that DEPT grows more efficient as the model size increases.
Our further study reveals that DePT integrates seamlessly with
parameter-efficient transfer learning in the few-shot learning setting and
highlights its adaptability to various model architectures and sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.10639">Geometric structure of Deep Learning networks and construction of global ${\mathcal L}^2$ minimizers. (arXiv:2309.10639v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1">Thomas Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ewald_P/0/1/0/all/0/1">Patricia Mu&#xf1;oz Ewald</a></p>
<p>In this paper, we provide a geometric interpretation of the structure of Deep
Learning (DL) networks, characterized by $L$ hidden layers, a ReLU ramp
activation function, an $\mathcal{L}^2$ Schatten class (or Hilbert-Schmidt)
cost function, and input and output spaces $\mathbb{R}^Q$ with equal dimension
$Q\geq1$. The hidden layers are also defined on $\mathbb{R}^{Q}$; the training
input size $N$ can be arbitrarily large - thus, we are considering the
underparametrized regime. We apply our recent results on shallow neural
networks to construct an explicit family of minimizers for the global minimum
of the cost function in the case $L\geq Q$, which we show to be degenerate. In
the context presented here, the hidden layers of the DL network "curate" the
training inputs by recursive application of a truncation map that minimizes the
noise to signal ratio of the training inputs. Moreover, we determine a set of
$2^Q-1$ distinct degenerate local minima of the cost function. Our
constructions make no use of gradient descent algorithms at all.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11343">Using Property Elicitation to Understand the Impacts of Fairness Regularizers. (arXiv:2309.11343v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Finocchiaro_J/0/1/0/all/0/1">Jessie Finocchiaro</a></p>
<p>Predictive algorithms are often trained by optimizing some loss function, to
which regularization functions are added to impose a penalty for violating
constraints. As expected, the addition of such regularization functions can
change the minimizer of the objective. It is not well-understood which
regularizers change the minimizer of the loss, and, when the minimizer does
change, how it changes. We use property elicitation to take first steps towards
understanding the joint relationship between the loss and regularization
functions and the optimal decision for a given problem instance. In particular,
we give a necessary and sufficient condition on loss and regularizer pairs for
when a property changes with the addition of the regularizer, and examine some
regularizers satisfying this condition standard in the fair machine learning
literature. We empirically demonstrate how algorithmic decision-making changes
as a function of both data distribution changes and hardness of the
constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.12458">A Theory of Multimodal Learning. (arXiv:2309.12458v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhou Lu</a></p>
<p>Human perception of the empirical world involves recognizing the diverse
appearances, or 'modalities', of underlying objects. Despite the longstanding
consideration of this perspective in philosophy and cognitive science, the
study of multimodality remains relatively under-explored within the field of
machine learning. Nevertheless, current studies of multimodal machine learning
are limited to empirical practices, lacking theoretical foundations beyond
heuristic arguments. An intriguing finding from the practice of multimodal
learning is that a model trained on multiple modalities can outperform a
finely-tuned unimodal model, even on unimodal tasks. This paper provides a
theoretical framework that explains this phenomenon, by studying generalization
properties of multimodal learning algorithms. We demonstrate that multimodal
learning allows for a superior generalization bound compared to unimodal
learning, up to a factor of $O(\sqrt{n})$, where $n$ represents the sample
size. Such advantage occurs when both connection and heterogeneity exist
between the modalities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13782">On the Computational Benefit of Multimodal Learning. (arXiv:2309.13782v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhou Lu</a></p>
<p>Human perception inherently operates in a multimodal manner. Similarly, as
machines interpret the empirical world, their learning processes ought to be
multimodal. The recent, remarkable successes in empirical multimodal learning
underscore the significance of understanding this paradigm. Yet, a solid
theoretical foundation for multimodal learning has eluded the field for some
time. While a recent study by Lu (2023) has shown the superior sample
complexity of multimodal learning compared to its unimodal counterpart, another
basic question remains: does multimodal learning also offer computational
advantages over unimodal learning? This work initiates a study on the
computational benefit of multimodal learning. We demonstrate that, under
certain conditions, multimodal learning can outpace unimodal learning
exponentially in terms of computation. Specifically, we present a learning task
that is NP-hard for unimodal learning but is solvable in polynomial time by a
multimodal algorithm. Our construction is based on a novel modification to the
intersection of two half-spaces problem.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15293">Maximum diffusion reinforcement learning. (arXiv:2309.15293v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Berrueta_T/0/1/0/all/0/1">Thomas A. Berrueta</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinosky_A/0/1/0/all/0/1">Allison Pinosky</a>, <a href="http://arxiv.org/find/cs/1/au:+Murphey_T/0/1/0/all/0/1">Todd D. Murphey</a></p>
<p>The assumption that data are independent and identically distributed
underpins all machine learning. When data are collected sequentially from agent
experiences this assumption does not generally hold, as in reinforcement
learning. Here, we derive a method that overcomes these limitations by
exploiting the statistical mechanics of ergodic processes, which we term
maximum diffusion reinforcement learning. By decorrelating agent experiences,
our approach provably enables single-shot learning in continuous deployments
over the course of individual task attempts. Moreover, we prove our approach
generalizes well-known maximum entropy techniques, and robustly exceeds
state-of-the-art performance across popular benchmarks. Our results at the
nexus of physics, learning, and control pave the way towards more transparent
and reliable decision-making in reinforcement learning agents, such as
locomoting robots and self-driving cars.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00681">PharmacoNet: Accelerating Large-Scale Virtual Screening by Deep Pharmacophore Modeling. (arXiv:2310.00681v3 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Seo_S/0/1/0/all/0/1">Seonghwan Seo</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kim_W/0/1/0/all/0/1">Woo Youn Kim</a></p>
<p>As the size of accessible compound libraries expands to over 10 billion, the
need for more efficient structure-based virtual screening methods is emerging.
Different pre-screening methods have been developed for rapid screening, but
there is still a lack of structure-based methods applicable to various proteins
that perform protein-ligand binding conformation prediction and scoring in an
extremely short time. Here, we describe for the first time a deep-learning
framework for structure-based pharmacophore modeling to address this challenge.
We frame pharmacophore modeling as an instance segmentation problem to
determine each protein hotspot and the location of corresponding
pharmacophores, and protein-ligand binding pose prediction as a graph-matching
problem. PharmacoNet is significantly faster than state-of-the-art
structure-based approaches, yet reasonably accurate with a simple scoring
function. Furthermore, we show the promising result that PharmacoNet
effectively retains hit candidates even under the high pre-screening filtration
rates. Overall, our study uncovers the hitherto untapped potential of a
pharmacophore modeling approach in deep learning-based drug discovery.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03059">Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1">Yiwen Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ray Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zoey Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xianzheng Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Dong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhigang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1">Bin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuelong Li</a></p>
<p>The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code is released at
https://github.com/Ivan-Tang-3D/PEFT-3D.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04314">Latent Graph Inference with Limited Supervision. (arXiv:2310.04314v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jianglin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Huan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1">Yue Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yun Fu</a></p>
<p>Latent graph inference (LGI) aims to jointly learn the underlying graph
structure and node representations from data features. However, existing LGI
methods commonly suffer from the issue of supervision starvation, where massive
edge weights are learned without semantic supervision and do not contribute to
the training loss. Consequently, these supervision-starved weights, which may
determine the predictions of testing samples, cannot be semantically optimal,
resulting in poor generalization. In this paper, we observe that this issue is
actually caused by the graph sparsification operation, which severely destroys
the important connections established between pivotal nodes and labeled ones.
To address this, we propose to restore the corrupted affinities and replenish
the missed supervision for better LGI. The key challenge then lies in
identifying the critical nodes and recovering the corrupted affinities. We
begin by defining the pivotal nodes as $k$-hop starved nodes, which can be
identified based on a given adjacency matrix. Considering the high
computational burden, we further present a more efficient alternative inspired
by CUR matrix decomposition. Subsequently, we eliminate the starved nodes by
reconstructing the destroyed connections. Extensive experiments on
representative benchmarks demonstrate that reducing the starved nodes
consistently improves the performance of state-of-the-art LGI methods,
especially under extremely limited supervision (6.12% improvement on Pubmed
with a labeling rate of only 0.3%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04469">Taming Binarized Neural Networks and Mixed-Integer Programs. (arXiv:2310.04469v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Aspman_J/0/1/0/all/0/1">Johannes Aspman</a>, <a href="http://arxiv.org/find/cs/1/au:+Korpas_G/0/1/0/all/0/1">Georgios Korpas</a>, <a href="http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1">Jakub Marecek</a></p>
<p>There has been a great deal of recent interest in binarized neural networks,
especially because of their explainability. At the same time, automatic
differentiation algorithms such as backpropagation fail for binarized neural
networks, which limits their applicability. By reformulating the problem of
training binarized neural networks as a subadditive dual of a mixed-integer
program, we show that binarized neural networks admit a tame representation.
This, in turn, makes it possible to use the framework of Bolte et al. for
implicit differentiation, which offers the possibility for practical
implementation of backpropagation in the context of binarized neural networks.
This approach could also be used for a broader class of mixed-integer programs,
beyond the training of binarized neural networks, as encountered in symbolic
approaches to AI and beyond.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04796">Accelerate Multi-Agent Reinforcement Learning in Zero-Sum Games with Subgame Curriculum Learning. (arXiv:2310.04796v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiayu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zelai Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaming Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huazhong Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1">Fei Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yi Wu</a></p>
<p>Learning Nash equilibrium (NE) in complex zero-sum games with multi-agent
reinforcement learning (MARL) can be extremely computationally expensive.
Curriculum learning is an effective way to accelerate learning, but an
under-explored dimension for generating a curriculum is the difficulty-to-learn
of the subgames -- games induced by starting from a specific state. In this
work, we present a novel subgame curriculum learning framework for zero-sum
games. It adopts an adaptive initial state distribution by resetting agents to
some previously visited states where they can quickly learn to improve
performance. Building upon this framework, we derive a subgame selection metric
that approximates the squared distance to NE values and further adopt a
particle-based state sampler for subgame generation. Integrating these
techniques leads to our new algorithm, Subgame Automatic Curriculum Learning
(SACL), which is a realization of the subgame curriculum learning framework.
SACL can be combined with any MARL algorithm such as MAPPO. Experiments in the
particle-world environment and Google Research Football environment show SACL
produces much stronger policies than baselines. In the challenging
hide-and-seek quadrant environment, SACL produces all four emergent stages and
uses only half the samples of MAPPO with self-play. The project website is at
https://sites.google.com/view/sacl-rl.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05161">Efficiently Representing Finite-state Automata With Recurrent Neural Networks. (arXiv:2310.05161v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Svete_A/0/1/0/all/0/1">Anej Svete</a>, <a href="http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1">Ryan Cotterell</a></p>
<p>Understanding neural network architectures with formal models of computation
promises to spark a better understanding of the network's capabilities and
limitations. A long line of work has described recurrent neural networks (RNN)
in terms of their connection to the well-understood finite-state automata
(FSAs), whose sequential nature provides a useful analogy to how RNNs function.
Minsky's [1954] construction first showed how RNNs can simulate FSAs and
provided a way of understanding RNNs as FSAs. This paper presents a
comprehensive review of this construction along with two additional classical
results showcasing the relationship between RNNs and FSAs: The constructions
due to Dewdney [1977] and Indyk [1995]. We are not only interested in
\emph{whether} an RNN can simulate an FSA, but also in the space requirements
to do so: Whereas Minsky [1954] shows that an RNN can simulate an FSA with $N$
states using $\mathcal{O}\left(N\right)$ neurons, Dewdney [1977] improved this
to $\mathcal{O}\left(N^\frac{3}{4}\right)$ and Indyk [1995] further to
$\mathcal{O}\left(\sqrt{N}\right)$, which he also showed to be optimal. We
discuss the constructions, emphasizing their commonalities, and put them into
the context of more modern research, focusing on the representational capacity
of neural language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07047">A predict-and-optimize approach to profit-driven churn prevention. (arXiv:2310.07047v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gomez_Vargas_N/0/1/0/all/0/1">Nuria G&#xf3;mez-Vargas</a>, <a href="http://arxiv.org/find/cs/1/au:+Maldonado_S/0/1/0/all/0/1">Sebasti&#xe1;n Maldonado</a>, <a href="http://arxiv.org/find/cs/1/au:+Vairetti_C/0/1/0/all/0/1">Carla Vairetti</a></p>
<p>In this paper, we introduce a novel predict-and-optimize method for
profit-driven churn prevention. We frame the task of targeting customers for a
retention campaign as a regret minimization problem. The main objective is to
leverage individual customer lifetime values (CLVs) to ensure that only the
most valuable customers are targeted. In contrast, many profit-driven
strategies focus on churn probabilities while considering average CLVs. This
often results in significant information loss due to data aggregation. Our
proposed model aligns with the guidelines of Predict-and-Optimize (PnO)
frameworks and can be efficiently solved using stochastic gradient descent
methods. Results from 12 churn prediction datasets underscore the effectiveness
of our approach, which achieves the best average performance compared to other
well-established strategies in terms of average profit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10059">Flow Dynamics Correction for Action Recognition. (arXiv:2310.10059v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1">Piotr Koniusz</a></p>
<p>Various research studies indicate that action recognition performance highly
depends on the types of motions being extracted and how accurate the human
actions are represented. In this paper, we investigate different optical flow,
and features extracted from these optical flow that capturing both short-term
and long-term motion dynamics. We perform power normalization on the magnitude
component of optical flow for flow dynamics correction to boost subtle or
dampen sudden motions. We show that existing action recognition models which
rely on optical flow are able to get performance boosted with our corrected
optical flow. To further improve performance, we integrate our corrected flow
dynamics into popular models through a simple hallucination step by selecting
only the best performing optical flow features, and we show that by
'translating' the CNN feature maps into these optical flow features with
different scales of motions leads to the new state-of-the-art performance on
several benchmarks including HMDB-51, YUP++, fine-grained action recognition on
MPII Cooking Activities, and large-scale Charades.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10505">ReMax: A Simple, Effective, and Efficient Reinforcement Learning Method for Aligning Large Language Models. (arXiv:2310.10505v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Ziniu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tian Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yushun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zhihang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_R/0/1/0/all/0/1">Ruoyu Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1">Zhi-Quan Luo</a></p>
<p>Alignment is crucial for training large language models. The predominant
strategy is Reinforcement Learning from Human Feedback (RLHF), with Proximal
Policy Optimization (PPO) as the de-facto algorithm. Yet, PPO is known to
struggle with computational inefficiency, a challenge that this paper aims to
address. We identify three important properties of RLHF tasks: fast simulation,
deterministic transitions, and trajectory-level rewards, which are not
leveraged in PPO. Based on these properties, we develop ReMax, a new algorithm
tailored for RLHF. The design of ReMax builds on the celebrated algorithm
REINFORCE but is enhanced with a new variance-reduction technique. ReMax offers
threefold advantages over PPO: first, it is simple to implement with just 6
lines of code. It further eliminates more than 4 hyper-parameters in PPO, which
are laborious to tune. Second, ReMax reduces memory usage by about 50%. To
illustrate, PPO runs out of memory when fine-tuning a Llama2-7B model on
A100-80GB GPUs, whereas ReMax can support the training. Even though
memory-efficient techniques (e.g., ZeRO and offload) are employed for PPO to
afford training, ReMax can utilize a larger batch size to increase throughput.
Third, in terms of wall-clock time, PPO is about twice as slow as ReMax per
iteration. Importantly, these improvements do not sacrifice task performance.
We hypothesize that these advantages can be maintained in larger-scale models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10605">ForceGen: End-to-end de novo protein generation based on nonlinear mechanical unfolding responses using a protein language diffusion model. (arXiv:2310.10605v3 [cond-mat.mtrl-sci] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cond-mat/1/au:+Ni_B/0/1/0/all/0/1">Bo Ni</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Kaplan_D/0/1/0/all/0/1">David L. Kaplan</a>, <a href="http://arxiv.org/find/cond-mat/1/au:+Buehler_M/0/1/0/all/0/1">Markus J. Buehler</a></p>
<p>Through evolution, nature has presented a set of remarkable protein
materials, including elastins, silks, keratins and collagens with superior
mechanical performances that play crucial roles in mechanobiology. However,
going beyond natural designs to discover proteins that meet specified
mechanical properties remains challenging. Here we report a generative model
that predicts protein designs to meet complex nonlinear mechanical
property-design objectives. Our model leverages deep knowledge on protein
sequences from a pre-trained protein language model and maps mechanical
unfolding responses to create novel proteins. Via full-atom molecular
simulations for direct validation, we demonstrate that the designed proteins
are novel, and fulfill the targeted mechanical properties, including unfolding
energy and mechanical strength, as well as the detailed unfolding
force-separation curves. Our model offers rapid pathways to explore the
enormous mechanobiological protein sequence space unconstrained by biological
synthesis, using mechanical features as target to enable the discovery of
protein materials with superior mechanical properties.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.13916">Southern Ocean Dynamics Under Climate Change: New Knowledge Through Physics-Guided Machine Learning. (arXiv:2310.13916v2 [physics.ao-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Yik_W/0/1/0/all/0/1">William Yik</a>, <a href="http://arxiv.org/find/physics/1/au:+Sonnewald_M/0/1/0/all/0/1">Maike Sonnewald</a>, <a href="http://arxiv.org/find/physics/1/au:+Clare_M/0/1/0/all/0/1">Mariana C. A. Clare</a>, <a href="http://arxiv.org/find/physics/1/au:+Lguensat_R/0/1/0/all/0/1">Redouane Lguensat</a></p>
<p>Complex ocean systems such as the Antarctic Circumpolar Current play key
roles in the climate, and current models predict shifts in their strength and
area under climate change. However, the physical processes underlying these
changes are not well understood, in part due to the difficulty of
characterizing and tracking changes in ocean physics in complex models. Using
the Antarctic Circumpolar Current as a case study, we extend the method
Tracking global Heating with Ocean Regimes (THOR) to a mesoscale eddy
permitting climate model and identify regions of the ocean characterized by
similar physics, called dynamical regimes, using readily accessible fields from
climate models. To this end, we cluster grid cells into dynamical regimes and
train an ensemble of neural networks, allowing uncertainty quantification, to
predict these regimes and track them under climate change. Finally, we leverage
this new knowledge to elucidate the dynamical drivers of the identified regime
shifts as noted by the neural network using the 'explainability' methods SHAP
and Layer-wise Relevance Propagation. A region undergoing a profound shift is
where the Antarctic Circumpolar Current intersects the Pacific-Antarctic Ridge,
an area important for carbon draw-down and fisheries. In this region, THOR
specifically reveals a shift in dynamical regime under climate change driven by
changes in wind stress and interactions with bathymetry. Using this knowledge
to guide further exploration, we find that as the Antarctic Circumpolar Current
shifts north under intensifying wind stress, the dominant dynamical role of
bathymetry weakens and the flow intensifies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14845">ULTRA-DP: Unifying Graph Pre-training with Multi-task Graph Dual Prompt. (arXiv:2310.14845v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mouxiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zemin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jundong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Q/0/1/0/all/0/1">Qiheng Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jianling Sun</a></p>
<p>Recent research has demonstrated the efficacy of pre-training graph neural
networks (GNNs) to capture the transferable graph semantics and enhance the
performance of various downstream tasks. However, the semantic knowledge
learned from pretext tasks might be unrelated to the downstream task, leading
to a semantic gap that limits the application of graph pre-training. To reduce
this gap, traditional approaches propose hybrid pre-training to combine various
pretext tasks together in a multi-task learning fashion and learn multi-grained
knowledge, which, however, cannot distinguish tasks and results in some
transferable task-specific knowledge distortion by each other. Moreover, most
GNNs cannot distinguish nodes located in different parts of the graph, making
them fail to learn position-specific knowledge and lead to suboptimal
performance. In this work, inspired by the prompt-based tuning in natural
language processing, we propose a unified framework for graph hybrid
pre-training which injects the task identification and position identification
into GNNs through a prompt mechanism, namely multi-task graph dual prompt
(ULTRA-DP). Based on this framework, we propose a prompt-based transferability
test to find the most relevant pretext task in order to reduce the semantic
gap. To implement the hybrid pre-training tasks, beyond the classical edge
prediction task (node-node level), we further propose a novel pre-training
paradigm based on a group of $k$-nearest neighbors (node-group level). The
combination of them across different scales is able to comprehensively express
more structural semantics and derive richer multi-grained knowledge. Extensive
experiments show that our proposed ULTRA-DP can significantly enhance the
performance of hybrid pre-training methods and show the generalizability to
other pre-training tasks and backbone architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15318">HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yihong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_N/0/1/0/all/0/1">Ning Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiayu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Mortazavi_M/0/1/0/all/0/1">Masood Mortazavi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1">Nitesh V. Chawla</a></p>
<p>Graphs have emerged as a natural choice to represent and analyze the
intricate patterns and rich information of the Web, enabling applications such
as online page classification and social recommendation. The prevailing
"pre-train, fine-tune" paradigm has been widely adopted in graph machine
learning tasks, particularly in scenarios with limited labeled nodes. However,
this approach often exhibits a misalignment between the training objectives of
pretext tasks and those of downstream tasks. This gap can result in the
"negative transfer" problem, wherein the knowledge gained from pre-training
adversely affects performance in the downstream tasks. The surge in
prompt-based learning within Natural Language Processing (NLP) suggests the
potential of adapting a "pre-train, prompt" paradigm to graphs as an
alternative. However, existing graph prompting techniques are tailored to
homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To
bridge this gap, we propose HetGPT, a general post-training prompting framework
to improve the predictive performance of pre-trained heterogeneous graph neural
networks (HGNNs). The key is the design of a novel prompting function that
integrates a virtual class prompt and a heterogeneous feature prompt, with the
aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT
introduces a multi-view neighborhood aggregation mechanism, capturing the
complex neighborhood structure in heterogeneous graphs. Extensive experiments
on three benchmark datasets demonstrate HetGPT's capability to enhance the
performance of state-of-the-art HGNNs on semi-supervised node classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16407">Information-Theoretic Generalization Analysis for Topology-aware Heterogeneous Federated Edge Learning over Noisy Channels. (arXiv:2310.16407v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zheshun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zenglin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hongfang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jie Liu</a></p>
<p>With the rapid growth of edge intelligence, the deployment of federated
learning (FL) over wireless networks has garnered increasing attention, which
is called Federated Edge Learning (FEEL). In FEEL, both mobile devices
transmitting model parameters over noisy channels and collecting data in
diverse environments pose challenges to the generalization of trained models.
Moreover, devices can engage in decentralized FL via Device-to-Device
communication while the communication topology of connected devices also
impacts the generalization of models. Most recent theoretical studies overlook
the incorporation of all these effects into FEEL when developing generalization
analyses. In contrast, our work presents an information-theoretic
generalization analysis for topology-aware FEEL in the presence of data
heterogeneity and noisy channels. Additionally, we propose a novel
regularization method called Federated Global Mutual Information Reduction
(FedGMIR) to enhance the performance of models based on our analysis. Numerical
results validate our theoretical findings and provide evidence for the
effectiveness of the proposed method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16485">A Comprehensive Python Library for Deep Learning-Based Event Detection in Multivariate Time Series Data and Information Retrieval in NLP. (arXiv:2310.16485v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Azib_M/0/1/0/all/0/1">Menouar Azib</a>, <a href="http://arxiv.org/find/cs/1/au:+Renard_B/0/1/0/all/0/1">Benjamin Renard</a>, <a href="http://arxiv.org/find/cs/1/au:+Garnier_P/0/1/0/all/0/1">Philippe Garnier</a>, <a href="http://arxiv.org/find/cs/1/au:+Genot_V/0/1/0/all/0/1">Vincent G&#xe9;not</a>, <a href="http://arxiv.org/find/cs/1/au:+Andre_N/0/1/0/all/0/1">Nicolas Andr&#xe9;</a></p>
<p>Event detection in time series data is crucial in various domains, including
finance, healthcare, cybersecurity, and science. Accurately identifying events
in time series data is vital for making informed decisions, detecting
anomalies, and predicting future trends. Despite extensive research exploring
diverse methods for event detection in time series, with deep learning
approaches being among the most advanced, there is still room for improvement
and innovation in this field. In this paper, we present a new deep learning
supervised method for detecting events in multivariate time series data. Our
method combines four distinct novelties compared to existing deep-learning
supervised methods. Firstly, it is based on regression instead of binary
classification. Secondly, it does not require labeled datasets where each point
is labeled; instead, it only requires reference events defined as time points
or intervals of time. Thirdly, it is designed to be robust by using a stacked
ensemble learning meta-model that combines deep learning models, ranging from
classic feed-forward neural networks (FFNs) to state-of-the-art architectures
like transformers. This ensemble approach can mitigate individual model
weaknesses and biases, resulting in more robust predictions. Finally, to
facilitate practical implementation, we have developed a Python package to
accompany our proposed method. The package, called eventdetector-ts, can be
installed through the Python Package Index (PyPI). In this paper, we present
our method and provide a comprehensive guide on the usage of the package. We
showcase its versatility and effectiveness through different real-world use
cases from natural language processing (NLP) to financial security domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16741">Stochastic Latent Transformer: Efficient Modelling of Stochastically Forced Zonal Jets. (arXiv:2310.16741v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shokar_I/0/1/0/all/0/1">Ira J. S. Shokar</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerswell_R/0/1/0/all/0/1">Rich R. Kerswell</a>, <a href="http://arxiv.org/find/cs/1/au:+Haynes_P/0/1/0/all/0/1">Peter H. Haynes</a></p>
<p>We present a novel probabilistic deep learning approach, the 'Stochastic
Latent Transformer' (SLT), designed for the efficient reduced-order modelling
of stochastic partial differential equations. Stochastically driven flow models
are pertinent to a diverse range of natural phenomena, including jets on giant
planets, ocean circulation, and the variability of midlatitude weather.
However, much of the recent progress in deep learning has predominantly focused
on deterministic systems. The SLT comprises a stochastically-forced transformer
paired with a translation-equivariant autoencoder, trained towards the
Continuous Ranked Probability Score. We showcase its effectiveness by applying
it to a well-researched zonal jet system, where the interaction between
stochastically forced eddies and the zonal mean flow results in a rich
low-frequency variability. The SLT accurately reproduces system dynamics across
various integration periods, validated through quantitative diagnostics that
include spectral properties and the rate of transitions between distinct
states. The SLT achieves a five-order-of-magnitude speedup in emulating the
zonally-averaged flow compared to direct numerical simulations. This
acceleration facilitates the cost-effective generation of large ensembles,
enabling the exploration of statistical questions concerning the probabilities
of spontaneous transition events.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17658">Is Channel Independent strategy optimal for Time Series Forecasting?. (arXiv:2310.17658v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peiwen_Y/0/1/0/all/0/1">Yuan Peiwen</a>, <a href="http://arxiv.org/find/cs/1/au:+Changsheng_Z/0/1/0/all/0/1">Zhu Changsheng</a></p>
<p>There has been an emergence of various models for long-term time series
forecasting. Recent studies have demonstrated that a single linear layer, using
Channel Dependent (CD) or Channel Independent (CI) modeling, can even
outperform a large number of sophisticated models. However, current research
primarily considers CD and CI as two complementary yet mutually exclusive
approaches, unable to harness these two extremes simultaneously. And it is also
a challenging issue that both CD and CI are static strategies that cannot be
determined to be optimal for a specific dataset without extensive experiments.
In this paper, we reconsider whether the current CI strategy is the best
solution for time series forecasting. First, we propose a simple yet effective
strategy called CSC, which stands for $\mathbf{C}$hannel
$\mathbf{S}$elf-$\mathbf{C}$lustering strategy, for linear models. Our Channel
Self-Clustering (CSC) enhances CI strategy's performance improvements while
reducing parameter size, for exmpale by over 10 times on electricity dataset,
and significantly cutting training time. Second, we further propose Channel
Rearrangement (CR), a method for deep models inspired by the self-clustering.
CR attains competitive performance against baselines. Finally, we also discuss
whether it is best to forecast the future values using the historical values of
the same channel as inputs. We hope our findings and methods could inspire new
solutions beyond CD/CI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18541">ReConTab: Regularized Contrastive Representation Learning for Tabular Data. (arXiv:2310.18541v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Suiyao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jing Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hovakimyan_N/0/1/0/all/0/1">Naira Hovakimyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1">Handong Yao</a></p>
<p>Representation learning stands as one of the critical machine learning
techniques across various domains. Through the acquisition of high-quality
features, pre-trained embeddings significantly reduce input space redundancy,
benefiting downstream pattern recognition tasks such as classification,
regression, or detection. Nonetheless, in the domain of tabular data, feature
engineering and selection still heavily rely on manual intervention, leading to
time-consuming processes and necessitating domain expertise. In response to
this challenge, we introduce ReConTab, a deep automatic representation learning
framework with regularized contrastive learning. Agnostic to any type of
modeling task, ReConTab constructs an asymmetric autoencoder based on the same
raw features from model inputs, producing low-dimensional representative
embeddings. Specifically, regularization techniques are applied for raw feature
selection. Meanwhile, ReConTab leverages contrastive learning to distill the
most pertinent information for downstream tasks. Experiments conducted on
extensive real-world datasets substantiate the framework's capacity to yield
substantial and robust performance improvements. Furthermore, we empirically
demonstrate that pre-trained embeddings can seamlessly integrate as easily
adaptable features, enhancing the performance of various traditional methods
such as XGBoost and Random Forest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.18786">A Competitive Algorithm for Agnostic Active Learning. (arXiv:2310.18786v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Price_E/0/1/0/all/0/1">Eric Price</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yihan Zhou</a></p>
<p>For some hypothesis classes and input distributions, active agnostic learning
needs exponentially fewer samples than passive learning; for other classes and
distributions, it offers little to no improvement. The most popular algorithms
for agnostic active learning express their performance in terms of a parameter
called the disagreement coefficient, but it is known that these algorithms are
inefficient on some inputs.
</p>
<p>We take a different approach to agnostic active learning, getting an
algorithm that is competitive with the optimal algorithm for any binary
hypothesis class $H$ and distribution $D_X$ over $X$. In particular, if any
algorithm can use $m^*$ queries to get $O(\eta)$ error, then our algorithm uses
$O(m^* \log |H|)$ queries to get $O(\eta)$ error. Our algorithm lies in the
vein of the splitting-based approach of Dasgupta [2004], which gets a similar
result for the realizable ($\eta = 0$) setting.
</p>
<p>We also show that it is NP-hard to do better than our algorithm's $O(\log
|H|)$ overhead in general.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19629">RayDF: Neural Ray-surface Distance Fields with Multi-view Consistency. (arXiv:2310.19629v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhuoman Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Bo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luximon_Y/0/1/0/all/0/1">Yan Luximon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1">Ajay Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinxi Li</a></p>
<p>In this paper, we study the problem of continuous 3D shape representations.
The majority of existing successful methods are coordinate-based implicit
neural representations. However, they are inefficient to render novel views or
recover explicit surface points. A few works start to formulate 3D shapes as
ray-based neural functions, but the learned structures are inferior due to the
lack of multi-view geometry consistency. To tackle these challenges, we propose
a new framework called RayDF. It consists of three major components: 1) the
simple ray-surface distance field, 2) the novel dual-ray visibility classifier,
and 3) a multi-view consistency optimization module to drive the learned
ray-surface distances to be multi-view geometry consistent. We extensively
evaluate our method on three public datasets, demonstrating remarkable
performance in 3D surface point reconstruction on both synthetic and
challenging real-world 3D scenes, clearly surpassing existing coordinate-based
and ray-based baselines. Most notably, our method achieves a 1000x faster speed
than coordinate-based methods to render an 800x800 depth image, showing the
superiority of our method for 3D shape representation. Our code and data are
available at https://github.com/vLAR-group/RayDF
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07723">Generalization Analogies: A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1">Joshua Clymer</a>, <a href="http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1">Garrett Baker</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1">Rohan Subramani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sam Wang</a></p>
<p>As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENeralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12435">Fair Enough? A map of the current limitations of the requirements to have &quot;fair&quot; algorithms. (arXiv:2311.12435v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Castelnovo_A/0/1/0/all/0/1">Alessandro Castelnovo</a>, <a href="http://arxiv.org/find/cs/1/au:+Inverardi_N/0/1/0/all/0/1">Nicole Inverardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nanino_G/0/1/0/all/0/1">Gabriele Nanino</a>, <a href="http://arxiv.org/find/cs/1/au:+Penco_I/0/1/0/all/0/1">Ilaria Giuseppina Penco</a>, <a href="http://arxiv.org/find/cs/1/au:+Regoli_D/0/1/0/all/0/1">Daniele Regoli</a></p>
<p>In the recent years, the raise in the usage and efficiency of Artificial
Intelligence and, more in general, of Automated Decision-Making systems has
brought with it an increasing and welcome awareness of the risks associated
with such systems. One of such risks is that of perpetuating or even amplifying
bias and unjust disparities present in the data from which many of these
systems learn to adjust and optimise their decisions. This awareness has on one
side encouraged several scientific communities to come up with more and more
appropriate ways and methods to assess, quantify, and possibly mitigate such
biases and disparities. On the other hand, it has prompted more and more layers
of society, including policy makers, to call for "fair" algorithms. We believe
that while a lot of excellent and multidisciplinary research is currently being
conducted, what is still fundamentally missing is the awareness that having
"fair" algorithms is per se a nearly meaningless requirement, that needs to be
complemented with a lot of additional societal choices to become actionable.
Namely, there is a hiatus between what the society is demanding from Automated
Decision-Making systems, and what this demand actually means in real-world
scenarios. In this work, we outline the key features of such a hiatus, and
pinpoint a list of fundamental ambiguities and attention points that we as a
society must address in order to give a concrete meaning to the increasing
demand of fairness in Automated Decision-Making systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.12813">Targeted Activation Penalties Help CNNs Ignore Spurious Signals. (arXiv:2311.12813v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dekai Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_M/0/1/0/all/0/1">Matthew Williams</a>, <a href="http://arxiv.org/find/cs/1/au:+Toni_F/0/1/0/all/0/1">Francesca Toni</a></p>
<p>Neural networks (NNs) can learn to rely on spurious signals in the training
data, leading to poor generalisation. Recent methods tackle this problem by
training NNs with additional ground-truth annotations of such signals. These
methods may, however, let spurious signals re-emerge in deep convolutional NNs
(CNNs). We propose Targeted Activation Penalty (TAP), a new method tackling the
same problem by penalising activations to control the re-emergence of spurious
signals in deep CNNs, while also lowering training times and memory usage. In
addition, ground-truth annotations can be expensive to obtain. We show that TAP
still works well with annotations generated by pre-trained models as effective
substitutes of ground-truth annotations. We demonstrate the power of TAP
against two state-of-the-art baselines on the MNIST benchmark and on two
clinical image datasets, using four different CNN architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13648">Evaluating Pretrained models for Deployable Lifelong Learning. (arXiv:2311.13648v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lekkala_K/0/1/0/all/0/1">Kiran Lekkala</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhargava_E/0/1/0/all/0/1">Eshan Bhargava</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yunhao Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1">Laurent Itti</a></p>
<p>We create a novel benchmark for evaluating a Deployable Lifelong Learning
system for Visual Reinforcement Learning (RL) that is pretrained on a curated
dataset, and propose a novel Scalable Lifelong Learning system capable of
retaining knowledge from the previously learnt RL tasks. Our benchmark measures
the efficacy of a deployable Lifelong Learning system that is evaluated on
scalability, performance and resource utilization. Our proposed system, once
pretrained on the dataset, can be deployed to perform continual learning on
unseen tasks. Our proposed method consists of a Few Shot Class Incremental
Learning (FSCIL) based task-mapper and an encoder/backbone trained entirely
using the pretrain dataset. The policy parameters corresponding to the
recognized task are then loaded to perform the task. We show that this system
can be scaled to incorporate a large number of tasks due to the small memory
footprint and fewer computational resources. We perform experiments on our DeLL
(Deployment for Lifelong Learning) benchmark on the Atari games to determine
the efficacy of the system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17326">Mostly Beneficial Clustering: Aggregating Data for Operational Decision Making. (arXiv:2311.17326v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chengzhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhenkang Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1">Ying Rong</a></p>
<p>With increasingly volatile market conditions and rapid product innovations,
operational decision-making for large-scale systems entails solving thousands
of problems with limited data. Data aggregation is proposed to combine the data
across problems to improve the decisions obtained by solving those problems
individually. We propose a novel cluster-based Shrunken-SAA approach that can
exploit the cluster structure among problems when implementing the data
aggregation approaches. We prove that, as the number of problems grows,
leveraging the given cluster structure among problems yields additional
benefits over the data aggregation approaches that neglect such structure. When
the cluster structure is unknown, we show that unveiling the cluster structure,
even at the cost of a few data points, can be beneficial, especially when the
distance between clusters of problems is substantial. Our proposed approach can
be extended to general cost functions under mild conditions. When the number of
problems gets large, the optimality gap of our proposed approach decreases
exponentially in the distance between the clusters. We explore the performance
of the proposed approach through the application of managing newsvendor systems
via numerical experiments. We investigate the impacts of distance metrics
between problem instances on the performance of the cluster-based Shrunken-SAA
approach with synthetic data. We further validate our proposed approach with
real data and highlight the advantages of cluster-based data aggregation,
especially in the small-data large-scale regime, compared to the existing
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17401">Gene-MOE: A sparsely gated prognosis and classification framework exploiting pan-cancer genomic information. (arXiv:2311.17401v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1">Xiangyu Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qing Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1">Huanhuan Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1">Lian Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1">Hongzhen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1">Long Hao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xun Wang</a></p>
<p>Benefiting from the advancements in deep learning, various genomic analytical
techniques, such as survival analysis, classification of tumors and their
subtypes, and exploration of specific pathways, have significantly enhanced our
understanding of the biological mechanisms driving cancer. However, the
overfitting issue, arising from the limited number of patient samples, poses a
challenge in improving the accuracy of genome analysis by deepening the neural
network. Furthermore, it remains uncertain whether novel approaches such as the
sparsely gated mixture of expert (MOE) and self-attention mechanisms can
improve the accuracy of genomic analysis. In this paper, we introduce a novel
sparsely gated RNA-seq analysis framework called Gene-MOE. This framework
exploits the potential of the MOE layers and the proposed mixture of attention
expert (MOAE) layers to enhance the analysis accuracy. Additionally, it
addresses overfitting challenges by integrating pan-cancer information from 33
distinct cancer types through pre-training.We pre-trained Gene-MOE on TCGA
pan-cancer RNA-seq dataset with 33 cancer types. Subsequently, we conducted
experiments involving cancer classification and survival analysis based on the
pre-trained Gene-MOE. According to the survival analysis results on 14 cancer
types, Gene-MOE outperformed state-of-the-art models on 12 cancer types.
Through detailed feature analysis, we found that the Gene-MOE model could learn
rich feature representations of high-dimensional genes. According to the
classification results, the total accuracy of the classification model for 33
cancer classifications reached 95.8%, representing the best performance
compared to state-of-the-art models. These results indicate that Gene-MOE holds
strong potential for use in cancer classification and survival analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.01479">OpenVoice: Versatile Instant Voice Cloning. (arXiv:2312.01479v3 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zengyi Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wenliang Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1">Xumin Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xin Sun</a></p>
<p>We introduce OpenVoice, a versatile voice cloning approach that requires only
a short audio clip from the reference speaker to replicate their voice and
generate speech in multiple languages. OpenVoice represents a significant
advancement in addressing the following open challenges in the field: 1)
Flexible Voice Style Control. OpenVoice enables granular control over voice
styles, including emotion, accent, rhythm, pauses, and intonation, in addition
to replicating the tone color of the reference speaker. The voice styles are
not directly copied from and constrained by the style of the reference speaker.
Previous approaches lacked the ability to flexibly manipulate voice styles
after cloning. 2) Zero-Shot Cross-Lingual Voice Cloning. OpenVoice achieves
zero-shot cross-lingual voice cloning for languages not included in the
massive-speaker training set. Unlike previous approaches, which typically
require extensive massive-speaker multi-lingual (MSML) dataset for all
languages, OpenVoice can clone voices into a new language without any
massive-speaker training data for that language. OpenVoice is also
computationally efficient, costing tens of times less than commercially
available APIs that offer even inferior performance. To foster further research
in the field, we have made the source code and trained model publicly
accessible. We also provide qualitative results in our demo website. Prior to
its public release, our internal version of OpenVoice was used tens of millions
of times by users worldwide between May and October 2023, serving as the
backend of MyShell.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02102">Mitigating Data Injection Attacks on Federated Learning. (arXiv:2312.02102v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shalom_O/0/1/0/all/0/1">Or Shalom</a>, <a href="http://arxiv.org/find/cs/1/au:+Leshem_A/0/1/0/all/0/1">Amir Leshem</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajwa_W/0/1/0/all/0/1">Waheed U. Bajwa</a></p>
<p>Federated learning is a technique that allows multiple entities to
collaboratively train models using their data without compromising data
privacy. However, despite its advantages, federated learning can be susceptible
to false data injection attacks. In these scenarios, a malicious entity with
control over specific agents in the network can manipulate the learning
process, leading to a suboptimal model. Consequently, addressing these data
injection attacks presents a significant research challenge in federated
learning systems. In this paper, we propose a novel technique to detect and
mitigate data injection attacks on federated learning systems. Our mitigation
method is a local scheme, performed during a single instance of training by the
coordinating node, allowing the mitigation during the convergence of the
algorithm. Whenever an agent is suspected to be an attacker, its data will be
ignored for a certain period, this decision will often be re-evaluated. We
prove that with probability 1, after a finite time, all attackers will be
ignored while the probability of ignoring a trustful agent becomes 0, provided
that there is a majority of truthful agents. Simulations show that when the
coordinating node detects and isolates all the attackers, the model recovers
and converges to the truthful model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02246">Conditional Variational Diffusion Models. (arXiv:2312.02246v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maggiora_G/0/1/0/all/0/1">Gabriel della Maggiora</a>, <a href="http://arxiv.org/find/cs/1/au:+Croquevielle_L/0/1/0/all/0/1">Luis Alberto Croquevielle</a>, <a href="http://arxiv.org/find/cs/1/au:+Desphande_N/0/1/0/all/0/1">Nikita Desphande</a>, <a href="http://arxiv.org/find/cs/1/au:+Horsley_H/0/1/0/all/0/1">Harry Horsley</a>, <a href="http://arxiv.org/find/cs/1/au:+Heinis_T/0/1/0/all/0/1">Thomas Heinis</a>, <a href="http://arxiv.org/find/cs/1/au:+Yakimovich_A/0/1/0/all/0/1">Artur Yakimovich</a></p>
<p>Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02674">Amortized Bayesian Decision Making for simulation-based models. (arXiv:2312.02674v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gorecki_M/0/1/0/all/0/1">Mila Gorecki</a>, <a href="http://arxiv.org/find/cs/1/au:+Macke_J/0/1/0/all/0/1">Jakob H. Macke</a>, <a href="http://arxiv.org/find/cs/1/au:+Deistler_M/0/1/0/all/0/1">Michael Deistler</a></p>
<p>Simulation-based inference (SBI) provides a powerful framework for inferring
posterior distributions of stochastic simulators in a wide range of domains. In
many settings, however, the posterior distribution is not the end goal itself
-- rather, the derived parameter values and their uncertainties are used as a
basis for deciding what actions to take. Unfortunately, because posterior
distributions provided by SBI are (potentially crude) approximations of the
true posterior, the resulting decisions can be suboptimal. Here, we address the
question of how to perform Bayesian decision making on stochastic simulators,
and how one can circumvent the need to compute an explicit approximation to the
posterior. Our method trains a neural network on simulated data and can predict
the expected cost given any data and action, and can, thus, be directly used to
infer the action with lowest cost. We apply our method to several benchmark
problems and demonstrate that it induces similar cost as the true posterior
distribution. We then apply the method to infer optimal actions in a real-world
simulator in the medical neurosciences, the Bayesian Virtual Epileptic Patient,
and demonstrate that it allows to infer actions associated with low cost after
few simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02855">Exploring Error Bits for Memory Failure Prediction: An In-Depth Correlative Study. (arXiv:2312.02855v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qiao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wengui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1">Jorge Cardoso</a>, <a href="http://arxiv.org/find/cs/1/au:+Kao_O/0/1/0/all/0/1">Odej Kao</a></p>
<p>In large-scale datacenters, memory failure is a common cause of server
crashes, with Uncorrectable Errors (UEs) being a major indicator of Dual Inline
Memory Module (DIMM) defects. Existing approaches primarily focus on predicting
UEs using Correctable Errors (CEs), without fully considering the information
provided by error bits. However, error bit patterns have a strong correlation
with the occurrence of UEs. In this paper, we present a comprehensive study on
the correlation between CEs and UEs, specifically emphasizing the importance of
spatio-temporal error bit information. Our analysis reveals a strong
correlation between spatio-temporal error bits and UE occurrence. Through
evaluations using real-world datasets, we demonstrate that our approach
significantly improves prediction performance by 15% in F1-score compared to
the state-of-the-art algorithms. Overall, our approach effectively reduces the
number of virtual machine interruptions caused by UEs by approximately 59%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.03360">Teaching Specific Scientific Knowledge into Large Language Models through Additional Training. (arXiv:2312.03360v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hatakeyama_Sato_K/0/1/0/all/0/1">Kan Hatakeyama-Sato</a>, <a href="http://arxiv.org/find/cs/1/au:+Igarashi_Y/0/1/0/all/0/1">Yasuhiko Igarashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Katakami_S/0/1/0/all/0/1">Shun Katakami</a>, <a href="http://arxiv.org/find/cs/1/au:+Nabae_Y/0/1/0/all/0/1">Yuta Nabae</a>, <a href="http://arxiv.org/find/cs/1/au:+Hayakawa_T/0/1/0/all/0/1">Teruaki Hayakawa</a></p>
<p>Through additional training, we explore embedding specialized scientific
knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that
effective knowledge integration requires reading texts from multiple
perspectives, especially in instructional formats. We utilize text augmentation
to tackle the scarcity of specialized texts, including style conversions and
translations. Hyperparameter optimization proves crucial, with different size
models (7b, 13b, and 70b) reasonably undergoing additional training. Validating
our methods, we construct a dataset of 65,000 scientific papers. Although we
have succeeded in partially embedding knowledge, the study highlights the
complexities and limitations of incorporating specialized information into
LLMs, suggesting areas for further improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05429">Mitigating Nonlinear Algorithmic Bias in Binary Classification. (arXiv:2312.05429v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hui_W/0/1/0/all/0/1">Wendy Hui</a>, <a href="http://arxiv.org/find/cs/1/au:+Lau_W/0/1/0/all/0/1">Wai Kwong Lau</a></p>
<p>This paper proposes the use of causal modeling to detect and mitigate
algorithmic bias that is nonlinear in the protected attribute. We provide a
general overview of our approach. We use the German Credit data set, which is
available for download from the UC Irvine Machine Learning Repository, to
develop (1) a prediction model, which is treated as a black box, and (2) a
causal model for bias mitigation. In this paper, we focus on age bias and the
problem of binary classification. We show that the probability of getting
correctly classified as "low risk" is lowest among young people. The
probability increases with age nonlinearly. To incorporate the nonlinearity
into the causal model, we introduce a higher order polynomial term. Based on
the fitted causal model, the de-biased probability estimates are computed,
showing improved fairness with little impact on overall classification
accuracy. Causal modeling is intuitive and, hence, its use can enhance
explicability and promotes trust among different stakeholders of AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05705">Structured Inverse-Free Natural Gradient: Memory-Efficient &amp; Numerically-Stable KFAC for Large Neural Nets. (arXiv:2312.05705v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1">Wu Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Dangel_F/0/1/0/all/0/1">Felix Dangel</a>, <a href="http://arxiv.org/find/cs/1/au:+Eschenhagen_R/0/1/0/all/0/1">Runa Eschenhagen</a>, <a href="http://arxiv.org/find/cs/1/au:+Neklyudov_K/0/1/0/all/0/1">Kirill Neklyudov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kristiadi_A/0/1/0/all/0/1">Agustinus Kristiadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1">Richard E. Turner</a>, <a href="http://arxiv.org/find/cs/1/au:+Makhzani_A/0/1/0/all/0/1">Alireza Makhzani</a></p>
<p>Second-order methods for deep learning -- such as KFAC -- can be useful for
neural net training. However, they are often memory-inefficient and numerically
unstable for low-precision training since their preconditioning Kronecker
factors are dense, and require high-precision matrix inversion or
decomposition. Consequently, such methods are not widely used for training
large neural networks such as transformer-based models. We address these two
issues by (i) formulating an inverse-free update of KFAC and (ii) imposing
structures in each of the Kronecker factors, resulting in a method we term
structured inverse-free natural gradient descent (SINGD). On large modern
neural networks, we show that, in contrast to KFAC, SINGD is memory efficient
and numerically robust, and often outperforms AdamW even in half precision.
Hence, our work closes a gap between first-order and second-order methods in
modern low precision training for large neural nets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05955">Learning Differentiable Particle Filter on the Fly. (arXiv:2312.05955v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiaxi Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiongjie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yunpeng Li</a></p>
<p>Differentiable particle filters are an emerging class of sequential Bayesian
inference techniques that use neural networks to construct components in state
space models. Existing approaches are mostly based on offline supervised
training strategies. This leads to the delay of the model deployment and the
obtained filters are susceptible to distribution shift of test-time data. In
this paper, we propose an online learning framework for differentiable particle
filters so that model parameters can be updated as data arrive. The technical
constraint is that there is no known ground truth state information in the
online inference setting. We address this by adopting an unsupervised loss to
construct the online model updating procedure, which involves a sequence of
filtering operations for online maximum likelihood-based parameter estimation.
We empirically evaluate the effectiveness of the proposed method, and compare
it with supervised learning methods in simulation settings including a
multivariate linear Gaussian state-space model and a simulated object tracking
experiment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05974">Learning the Causal Structure of Networked Dynamical Systems under Latent Nodes and Structured Noise. (arXiv:2312.05974v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1">Augusto Santos</a>, <a href="http://arxiv.org/find/cs/1/au:+Rente_D/0/1/0/all/0/1">Diogo Rente</a>, <a href="http://arxiv.org/find/cs/1/au:+Seabra_R/0/1/0/all/0/1">Rui Seabra</a>, <a href="http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1">Jos&#xe9; M. F. Moura</a></p>
<p>This paper considers learning the hidden causal network of a linear networked
dynamical system (NDS) from the time series data at some of its nodes --
partial observability. The dynamics of the NDS are driven by colored noise that
generates spurious associations across pairs of nodes, rendering the problem
much harder. To address the challenge of noise correlation and partial
observability, we assign to each pair of nodes a feature vector computed from
the time series data of observed nodes. The feature embedding is engineered to
yield structural consistency: there exists an affine hyperplane that
consistently partitions the set of features, separating the feature vectors
corresponding to connected pairs of nodes from those corresponding to
disconnected pairs. The causal inference problem is thus addressed via
clustering the designed features. We demonstrate with simple baseline
supervised methods the competitive performance of the proposed causal inference
mechanism under broad connectivity regimes and noise correlation levels,
including a real world network. Further, we devise novel technical guarantees
of structural consistency for linear NDS under the considered regime.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06290">Exploiting Label Skews in Federated Learning with Model Concatenation. (arXiv:2312.06290v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diao_Y/0/1/0/all/0/1">Yiqun Diao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qinbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1">Bingsheng He</a></p>
<p>Federated Learning (FL) has emerged as a promising solution to perform deep
learning on different data owners without exchanging raw data. However, non-IID
data has been a key challenge in FL, which could significantly degrade the
accuracy of the final model. Among different non-IID types, label skews have
been challenging and common in image classification and other tasks. Instead of
averaging the local models in most previous studies, we propose FedConcat, a
simple and effective approach that concatenates these local models as the base
of the global model to effectively aggregate the local knowledge. To reduce the
size of the global model, we adopt the clustering technique to group the
clients by their label distributions and collaboratively train a model inside
each cluster. We theoretically analyze the advantage of concatenation over
averaging by analyzing the information bottleneck of deep neural networks.
Experimental results demonstrate that FedConcat achieves significantly higher
accuracy than previous state-of-the-art FL methods in various heterogeneous
label skew distribution settings and meanwhile has lower communication costs.
Our code is publicly available at https://github.com/sjtudyq/FedConcat.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06833">The unreasonable effectiveness of AI CADe polyp detectors to generalize to new countries. (arXiv:2312.06833v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shor_J/0/1/0/all/0/1">Joel Shor</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamano_H/0/1/0/all/0/1">Hiro-o Yamano</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsurumaru_D/0/1/0/all/0/1">Daisuke Tsurumaru</a>, <a href="http://arxiv.org/find/cs/1/au:+Intrator_Y/0/1/0/all/0/1">Yotami Intrator</a>, <a href="http://arxiv.org/find/cs/1/au:+Kayama_H/0/1/0/all/0/1">Hiroki Kayama</a>, <a href="http://arxiv.org/find/cs/1/au:+Ledsam_J/0/1/0/all/0/1">Joe Ledsam</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamabe_A/0/1/0/all/0/1">Atsushi Hamabe</a>, <a href="http://arxiv.org/find/cs/1/au:+Ando_K/0/1/0/all/0/1">Koji Ando</a>, <a href="http://arxiv.org/find/cs/1/au:+Ota_M/0/1/0/all/0/1">Mitsuhiko Ota</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogino_H/0/1/0/all/0/1">Haruei Ogino</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakase_H/0/1/0/all/0/1">Hiroshi Nakase</a>, <a href="http://arxiv.org/find/cs/1/au:+Kobayashi_K/0/1/0/all/0/1">Kaho Kobayashi</a>, <a href="http://arxiv.org/find/cs/1/au:+Oki_E/0/1/0/all/0/1">Eiji Oki</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldenberg_R/0/1/0/all/0/1">Roman Goldenberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Rivlin_E/0/1/0/all/0/1">Ehud Rivlin</a>, <a href="http://arxiv.org/find/cs/1/au:+Takemasa_I/0/1/0/all/0/1">Ichiro Takemasa</a></p>
<p>$\textbf{Background and aims}$: Artificial Intelligence (AI) Computer-Aided
Detection (CADe) is commonly used for polyp detection, but data seen in
clinical settings can differ from model training. Few studies evaluate how well
CADe detectors perform on colonoscopies from countries not seen during
training, and none are able to evaluate performance without collecting
expensive and time-intensive labels.
</p>
<p>$\textbf{Methods}$: We trained a CADe polyp detector on Israeli colonoscopy
videos (5004 videos, 1106 hours) and evaluated on Japanese videos (354 videos,
128 hours) by measuring the True Positive Rate (TPR) versus false alarms per
minute (FAPM). We introduce a colonoscopy dissimilarity measure called "MAsked
mediCal Embedding Distance" (MACE) to quantify differences between
colonoscopies, without labels. We evaluated CADe on all Japan videos and on
those with the highest MACE.
</p>
<p>$\textbf{Results}$: MACE correctly quantifies that narrow-band imaging (NBI)
and chromoendoscopy (CE) frames are less similar to Israel data than Japan
whitelight (bootstrapped z-test, |z| &gt; 690, p &lt; $10^{-8}$ for both). Despite
differences in the data, CADe performance on Japan colonoscopies was
non-inferior to Israel ones without additional training (TPR at 0.5 FAPM: 0.957
and 0.972 for Israel and Japan; TPR at 1.0 FAPM: 0.972 and 0.989 for Israel and
Japan; superiority test t &gt; 45.2, p &lt; $10^{-8}$). Despite not being trained on
NBI or CE, TPR on those subsets were non-inferior to Japan overall
(non-inferiority test t &gt; 47.3, p &lt; $10^{-8}$, $\delta$ = 1.5% for both).
</p>
<p>$\textbf{Conclusion}$: Differences that prevent CADe detectors from
performing well in non-medical settings do not degrade the performance of our
AI CADe polyp detector when applied to data from a new country. MACE can help
medical AI models internationalize by identifying the most "dissimilar" data on
which to evaluate models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06872">ELSA: Partial Weight Freezing for Overhead-Free Sparse Network Deployment. (arXiv:2312.06872v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Halvachi_P/0/1/0/all/0/1">Paniz Halvachi</a>, <a href="http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1">Alexandra Peste</a>, <a href="http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1">Dan Alistarh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1">Christoph H. Lampert</a></p>
<p>We present ELSA, a practical solution for creating deep networks that can
easily be deployed at different levels of sparsity. The core idea is to embed
one or more sparse networks within a single dense network as a proper subset of
the weights. At prediction time, any sparse model can be extracted effortlessly
simply be zeroing out weights according to a predefined mask. ELSA is simple,
powerful and highly flexible. It can use essentially any existing technique for
network sparsification and network training. In particular, it does not
restrict the loss function, architecture or the optimization technique. Our
experiments show that ELSA's advantages of flexible deployment comes with no or
just a negligible reduction in prediction quality compared to the standard way
of using multiple sparse networks that are trained and stored independently.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07392">ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning. (arXiv:2312.07392v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1">Xiangyu Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Sihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaxu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1">Meng Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xingyu Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xiaowei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1">Wenjie Ruan</a></p>
<p>While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,
its algorithmic robustness against adversarial perturbations remains
unexplored. The attacks and robust representation training methods that are
designed for traditional RL become less effective when applied to GCRL. To
address this challenge, we first propose the Semi-Contrastive Representation
attack, a novel approach inspired by the adversarial contrastive attack. Unlike
existing attacks in RL, it only necessitates information from the policy
function and can be seamlessly implemented during deployment. Then, to mitigate
the vulnerability of existing GCRL algorithms, we introduce Adversarial
Representation Tactics, which combines Semi-Contrastive Adversarial
Augmentation with Sensitivity-Aware Regularizer to improve the adversarial
robustness of the underlying RL agent against various types of perturbations.
Extensive experiments validate the superior performance of our attack and
defence methods across multiple state-of-the-art GCRL algorithms. Our tool
ReRoGCRL is available at https://github.com/TrustAI/ReRoGCRL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07492">SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1">Manish Nagireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1">Lamogha Chiazor</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Moninder Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1">Ioana Baldini</a></p>
<p>Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. Taking inspiration
from social science research, we start with a documented list of 93 US-centric
stigmas and curate a question-answering (QA) dataset which involves simple
social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,
with a variety of prompt styles, carefully constructed to systematically test
for both social bias and model robustness. We present results for
SocialStigmaQA with two open source generative language models and we find that
the proportion of socially biased output ranges from 45% to 59% across a
variety of decoding strategies and prompting styles. We demonstrate that the
deliberate design of the templates in our benchmark (e.g., adding biasing text
to the prompt or using different verbs that change the answer that indicates
bias) impacts the model tendencies to generate socially biased output.
Additionally, through manual evaluation, we discover problematic patterns in
the generated chain-of-thought output that range from subtle bias to lack of
reasoning.
</p>
<p>Warning: This paper contains examples of text which are toxic, biased, and
potentially harmful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07547">Active Inference and Intentional Behaviour. (arXiv:2312.07547v2 [q-bio.NC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Friston_K/0/1/0/all/0/1">Karl J. Friston</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Salvatori_T/0/1/0/all/0/1">Tommaso Salvatori</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Isomura_T/0/1/0/all/0/1">Takuya Isomura</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Tschantz_A/0/1/0/all/0/1">Alexander Tschantz</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kiefer_A/0/1/0/all/0/1">Alex Kiefer</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Verbelen_T/0/1/0/all/0/1">Tim Verbelen</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Koudahl_M/0/1/0/all/0/1">Magnus Koudahl</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Paul_A/0/1/0/all/0/1">Aswin Paul</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Parr_T/0/1/0/all/0/1">Thomas Parr</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Razi_A/0/1/0/all/0/1">Adeel Razi</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kagan_B/0/1/0/all/0/1">Brett Kagan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Buckley_C/0/1/0/all/0/1">Christopher L. Buckley</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ramstead_M/0/1/0/all/0/1">Maxwell J. D. Ramstead</a></p>
<p>Recent advances in theoretical biology suggest that basal cognition and
sentient behaviour are emergent properties of in vitro cell cultures and
neuronal networks, respectively. Such neuronal networks spontaneously learn
structured behaviours in the absence of reward or reinforcement. In this paper,
we characterise this kind of self-organisation through the lens of the free
energy principle, i.e., as self-evidencing. We do this by first discussing the
definitions of reactive and sentient behaviour in the setting of active
inference, which describes the behaviour of agents that model the consequences
of their actions. We then introduce a formal account of intentional behaviour,
that describes agents as driven by a preferred endpoint or goal in latent
state-spaces. We then investigate these forms of (reactive, sentient, and
intentional) behaviour using simulations. First, we simulate the aforementioned
in vitro experiments, in which neuronal cultures spontaneously learn to play
Pong, by implementing nested, free energy minimising processes. The simulations
are then used to deconstruct the ensuing predictive behaviour, leading to the
distinction between merely reactive, sentient, and intentional behaviour, with
the latter formalised in terms of inductive planning. This distinction is
further studied using simple machine learning benchmarks (navigation in a grid
world and the Tower of Hanoi problem), that show how quickly and efficiently
adaptive behaviour emerges under an inductive form of active inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07759">IDKM: Memory Efficient Neural Network Quantization via Implicit, Differentiable k-Means. (arXiv:2312.07759v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jaffe_S/0/1/0/all/0/1">Sean Jaffe</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Ambuj K. Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bullo_F/0/1/0/all/0/1">Francesco Bullo</a></p>
<p>Compressing large neural networks with minimal performance loss is crucial to
enabling their deployment on edge devices. (Cho et al., 2022) proposed a weight
quantization method that uses an attention-based clustering algorithm called
differentiable $k$-means (DKM). Despite achieving state-of-the-art results,
DKM's performance is constrained by its heavy memory dependency. We propose an
implicit, differentiable $k$-means algorithm (IDKM), which eliminates the major
memory restriction of DKM. Let $t$ be the number of $k$-means iterations, $m$
be the number of weight-vectors, and $b$ be the number of bits per cluster
address. IDKM reduces the overall memory complexity of a single $k$-means layer
from $\mathcal{O}(t \cdot m \cdot 2^b)$ to $\mathcal{O}( m \cdot 2^b)$. We also
introduce a variant, IDKM with Jacobian-Free-Backpropagation (IDKM-JFB), for
which the time complexity of the gradient calculation is independent of $t$ as
well. We provide a proof of concept of our methods by showing that, under the
same settings, IDKM achieves comparable performance to DKM with less compute
time and less memory. We also use IDKM and IDKM-JFB to quantize a large neural
network, Resnet18, on hardware where DKM cannot train at all.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07859">Invariant Graph Transformer. (arXiv:2312.07859v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhe Xu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1">Menghai Pan</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuzhong Chen</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huiyuan Chen</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yuchen Yan</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Das_M/0/1/0/all/0/1">Mahashweta Das</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Tong_H/0/1/0/all/0/1">Hanghang Tong</a> (1) ((1) University of Illinois Urbana-Champaign, (2) Visa Research)</p>
<p>Rationale discovery is defined as finding a subset of the input data that
maximally supports the prediction of downstream tasks. In graph machine
learning context, graph rationale is defined to locate the critical subgraph in
the given graph topology, which fundamentally determines the prediction
results. In contrast to the rationale subgraph, the remaining subgraph is named
the environment subgraph. Graph rationalization can enhance the model
performance as the mapping between the graph rationale and prediction label is
viewed as invariant, by assumption. To ensure the discriminative power of the
extracted rationale subgraphs, a key technique named "intervention" is applied.
The core idea of intervention is that given any changing environment subgraphs,
the semantics from the rationale subgraph is invariant, which guarantees the
correct prediction result. However, most, if not all, of the existing
rationalization works on graph data develop their intervention strategies on
the graph level, which is coarse-grained. In this paper, we propose
well-tailored intervention strategies on graph data. Our idea is driven by the
development of Transformer models, whose self-attention module provides rich
interactions between input nodes. Based on the self-attention module, our
proposed invariant graph Transformer (IGT) can achieve fine-grained, more
specifically, node-level and virtual node-level intervention. Our comprehensive
experiments involve 7 real-world datasets, and the proposed IGT shows
significant performance advantages compared to 13 baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08470">Best practices for machine learning in antibody discovery and development. (arXiv:2312.08470v2 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Wossnig_L/0/1/0/all/0/1">Leonard Wossnig</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Furtmann_N/0/1/0/all/0/1">Norbert Furtmann</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Buchanan_A/0/1/0/all/0/1">Andrew Buchanan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kumar_S/0/1/0/all/0/1">Sandeep Kumar</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Greiff_V/0/1/0/all/0/1">Victor Greiff</a></p>
<p>Over the past 40 years, the discovery and development of therapeutic
antibodies to treat disease has become common practice. However, as therapeutic
antibody constructs are becoming more sophisticated (e.g., multi-specifics),
conventional approaches to optimisation are increasingly inefficient. Machine
learning (ML) promises to open up an in silico route to antibody discovery and
help accelerate the development of drug products using a reduced number of
experiments and hence cost. Over the past few years, we have observed rapid
developments in the field of ML-guided antibody discovery and development
(D&amp;D). However, many of the results are difficult to compare or hard to assess
for utility by other experts in the field due to the high diversity in the
datasets and evaluation techniques and metrics that are across industry and
academia. This limitation of the literature curtails the broad adoption of ML
across the industry and slows down overall progress in the field, highlighting
the need to develop standards and guidelines that may help improve the
reproducibility of ML models across different research groups. To address these
challenges, we set out in this perspective to critically review current
practices, explain common pitfalls, and clearly define a set of method
development and evaluation guidelines that can be applied to different types of
ML-based techniques for therapeutic antibody D&amp;D. Specifically, we address in
an end-to-end analysis, challenges associated with all aspects of the ML
process and recommend a set of best practices for each stage.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08533">World Models via Policy-Guided Trajectory Diffusion. (arXiv:2312.08533v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rigter_M/0/1/0/all/0/1">Marc Rigter</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamada_J/0/1/0/all/0/1">Jun Yamada</a>, <a href="http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1">Ingmar Posner</a></p>
<p>World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in "in imagination". Existing world models are autoregressive in that they
interleave predicting the next state with sampling the next action from the
policy. Prediction error inevitably compounds as the trajectory length grows.
In this work, we propose a novel world modelling approach that is not
autoregressive and generates entire on-policy trajectories in a single pass
through a diffusion model. Our approach, Policy-Guided Trajectory Diffusion
(PolyGRAD), leverages a denoising model in addition to the gradient of the
action distribution of the policy to diffuse a trajectory of initially random
states and actions into an on-policy synthetic trajectory. We analyse the
connections between PolyGRAD, score-based generative models, and
classifier-guided diffusion models. Our results demonstrate that PolyGRAD
outperforms state-of-the-art baselines in terms of trajectory prediction error
for moderate-length trajectories, with the exception of autoregressive
diffusion. At short horizons, PolyGRAD obtains comparable errors to
autoregressive diffusion, but with significantly lower computational
requirements. Our experiments also demonstrate that PolyGRAD enables performant
policies to be trained via on-policy RL in imagination for MuJoCo continuous
control domains. Thus, PolyGRAD introduces a new paradigm for scalable and
non-autoregressive on-policy world modelling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08579">Identifying Planetary Names in Astronomy Papers: A Multi-Step Approach. (arXiv:2312.08579v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shapurian_G/0/1/0/all/0/1">Golnaz Shapurian</a>, <a href="http://arxiv.org/find/cs/1/au:+Kurtz_M/0/1/0/all/0/1">Michael J Kurtz</a>, <a href="http://arxiv.org/find/cs/1/au:+Accomazzi_A/0/1/0/all/0/1">Alberto Accomazzi</a></p>
<p>The automatic identification of planetary feature names in astronomy
publications presents numerous challenges. These features include craters,
defined as roughly circular depressions resulting from impact or volcanic
activity; dorsas, which are elongate raised structures or wrinkle ridges; and
lacus, small irregular patches of dark, smooth material on the Moon, referred
to as "lake" (Planetary Names Working Group, n.d.). Many feature names overlap
with places or people's names that they are named after, for example, Syria,
Tempe, Einstein, and Sagan, to name a few (U.S. Geological Survey, n.d.). Some
feature names have been used in many contexts, for instance, Apollo, which can
refer to mission, program, sample, astronaut, seismic, seismometers, core, era,
data, collection, instrument, and station, in addition to the crater on the
Moon. Some feature names can appear in the text as adjectives, like the lunar
craters Black, Green, and White. Some feature names in other contexts serve as
directions, like craters West and South on the Moon. Additionally, some
features share identical names across different celestial bodies, requiring
disambiguation, such as the Adams crater, which exists on both the Moon and
Mars. We present a multi-step pipeline combining rule-based filtering,
statistical relevance analysis, part-of-speech (POS) tagging, named entity
recognition (NER) model, hybrid keyword harvesting, knowledge graph (KG)
matching, and inference with a locally installed large language model (LLM) to
reliably identify planetary names despite these challenges. When evaluated on a
dataset of astronomy papers from the Astrophysics Data System (ADS), this
methodology achieves an F1-score over 0.97 in disambiguating planetary feature
names.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08656">MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training. (arXiv:2312.08656v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1">Hongwu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1">Xi Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Shivdikar_K/0/1/0/all/0/1">Kaustubh Shivdikar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1">MD Amit Hasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jiahui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shaoyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Khan_O/0/1/0/all/0/1">Omer Khan</a>, <a href="http://arxiv.org/find/cs/1/au:+Kaeli_D/0/1/0/all/0/1">David Kaeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Caiwen Ding</a></p>
<p>In the acceleration of deep neural network training, the GPU has become the
mainstream platform. GPUs face substantial challenges on GNNs, such as workload
imbalance and memory access irregularities, leading to underutilized hardware.
Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks
partially address these challenges but memory traffic is still significant.
</p>
<p>We argue that drastic performance improvements can only be achieved by the
vertical optimization of algorithm and system innovations, rather than treating
the speedup optimization as an "after-thought" (i.e., (i) given a GNN
algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing
the GNN algorithm). In this paper, we present MaxK-GNN, an advanced
high-performance GPU training system integrating algorithm and system
innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical
analysis of MaxK nonlinearity as a universal approximator, and present the
Compressed Balanced Sparse Row (CBSR) format, designed to store the data and
index of the feature matrix after nonlinearity; (ii) We design a coalescing
enhanced forward computation with row-wise product-based SpGEMM Kernel using
CBSR for input feature matrix fetching and strategic placement of a sparse
output accumulation buffer in shared memory; (iii) We develop an optimized
backward computation with outer product-based and SSpMM Kernel.
</p>
<p>We conduct extensive evaluations of MaxK-GNN and report the end-to-end system
run-time. Experiments show that MaxK-GNN system could approach the theoretical
speedup limit according to Amdahl's law. We achieve comparable accuracy to SOTA
GNNs, but at a significantly increased speed: 3.22/4.24 times speedup (vs.
theoretical limits, 5.52/7.27 times) on Reddit compared to DGL and GNNAdvisor
implementations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08763">Learning from Polar Representation: An Extreme-Adaptive Model for Long-Term Time Series Forecasting. (arXiv:2312.08763v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanhong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jack Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1">David C. Anastasiu</a></p>
<p>In the hydrology field, time series forecasting is crucial for efficient
water resource management, improving flood and drought control and increasing
the safety and quality of life for the general population. However, predicting
long-term streamflow is a complex task due to the presence of extreme events.
It requires the capture of long-range dependencies and the modeling of rare but
important extreme values. Existing approaches often struggle to tackle these
dual challenges simultaneously. In this paper, we specifically delve into these
issues and propose Distance-weighted Auto-regularized Neural network (DAN), a
novel extreme-adaptive model for long-range forecasting of stremflow enhanced
by polar representation learning. DAN utilizes a distance-weighted multi-loss
mechanism and stackable blocks to dynamically refine indicator sequences from
exogenous data, while also being able to handle uni-variate time-series by
employing Gaussian Mixture probability modeling to improve robustness to severe
events. We also introduce Kruskal-Wallis sampling and gate control vectors to
handle imbalanced extreme data. On four real-life hydrologic streamflow
datasets, we demonstrate that DAN significantly outperforms both
state-of-the-art hydrologic time series prediction methods and general methods
designed for long-term time series prediction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08793">Forbidden Facts: An Investigation of Competing Objectives in Llama-2. (arXiv:2312.08793v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tony T. Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Miles Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hariharan_K/0/1/0/all/0/1">Kaivalya Hariharan</a>, <a href="http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1">Nir Shavit</a></p>
<p>LLMs often face competing pressures (for example helpfulness vs.
harmlessness). To understand how models resolve such conflicts, we study
Llama-2-chat models on the forbidden fact task. Specifically, we instruct
Llama-2 to truthfully complete a factual recall statement while forbidding it
from saying the correct answer. This often makes the model give incorrect
answers. We decompose Llama-2 into 1000+ components, and rank each one with
respect to how useful it is for forbidding the correct answer. We find that in
aggregate, around 35 components are enough to reliably implement the full
suppression behavior. However, these components are fairly heterogeneous and
many operate using faulty heuristics. We discover that one of these heuristics
can be exploited via a manually designed adversarial attack which we call The
California Attack. Our results highlight some roadblocks standing in the way of
being able to successfully interpret advanced ML systems. Project website
available at https://forbiddenfacts.github.io .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08818">A Cyber-Physical Architecture for Microgrids based on Deep learning and LORA Technology. (arXiv:2312.08818v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadi_M/0/1/0/all/0/1">Mojtaba Mohammadi</a>, <a href="http://arxiv.org/find/cs/1/au:+KavousiFard_A/0/1/0/all/0/1">Abdollah KavousiFard</a>, <a href="http://arxiv.org/find/cs/1/au:+Dabbaghjamanesh_M/0/1/0/all/0/1">Mortza Dabbaghjamanesh</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaaban_M/0/1/0/all/0/1">Mostafa Shaaban</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeineldin_H/0/1/0/all/0/1">Hatem. H. Zeineldin</a>, <a href="http://arxiv.org/find/cs/1/au:+El_Saadany_E/0/1/0/all/0/1">Ehab Fahmy El-Saadany</a></p>
<p>This paper proposes a cyber-physical architecture for the secured social
operation of isolated hybrid microgrids (HMGs). On the physical side of the
proposed architecture, an optimal scheduling scheme considering various
renewable energy sources (RESs) and fossil fuel-based distributed generation
units (DGs) is proposed. Regarding the cyber layer of MGs, a wireless
architecture based on low range wide area (LORA) technology is introduced for
advanced metering infrastructure (AMI) in smart electricity grids. In the
proposed architecture, the LORA data frame is described in detail and designed
for the application of smart meters considering DGs and ac-dc converters.
Additionally, since the cyber layer of smart grids is highly vulnerable to
cyber-attacks, t1his paper proposes a deep-learning-based cyber-attack
detection model (CADM) based on bidirectional long short-term memory (BLSTM)
and sequential hypothesis testing (SHT) to detect false data injection attacks
(FDIA) on the smart meters within AMI. The performance of the proposed energy
management architecture is evaluated using the IEEE 33-bus test system. In
order to investigate the effect of FDIA on the isolated HMGs and highlight the
interactions between the cyber layer and physical layer, an FDIA is launched
against the test system. The results showed that a successful attack can highly
damage the system and cause widespread load shedding. Also, the performance of
the proposed CADM is examined using a real-world dataset. Results prove the
effectiveness of the proposed CADM in detecting the attacks using only two
samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09002">Localization with Reconfigurable Intelligent Surface: An Active Sensing Approach. (arXiv:2312.09002v2 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhongze Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wei Yu</a></p>
<p>This paper addresses an uplink localization problem in which a base station
(BS) aims to locate a remote user with the help of reconfigurable intelligent
surfaces (RISs). We propose a strategy in which the user transmits pilots
sequentially and the BS adaptively adjusts the sensing vectors, including the
BS beamforming vector and multiple RIS reflection coefficients based on the
observations already made, to eventually produce an estimated user position.
This is a challenging active sensing problem for which finding an optimal
solution involves searching through a complicated functional space whose
dimension increases with the number of measurements. We show that the long
short-term memory (LSTM) network can be used to exploit the latent temporal
correlation between measurements to automatically construct scalable state
vectors. Subsequently, the state vector is mapped to the sensing vectors for
the next time frame via a deep neural network (DNN). A final DNN is used to map
the state vector to the estimated user position. Numerical result illustrates
the advantage of the active sensing design as compared to non-active sensing
methods. The proposed solution produces interpretable results and is
generalizable in the number of sensing stages. Remarkably, we show that a
network with one BS and multiple RISs can outperform a comparable setting with
multiple BSs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09038">Object Recognition from Scientific Document based on Compartment Refinement Framework. (arXiv:2312.09038v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinghong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1">Wen Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ota_K/0/1/0/all/0/1">Koichi Ota</a>, <a href="http://arxiv.org/find/cs/1/au:+Hasegawa_S/0/1/0/all/0/1">Shinobu Hasegawa</a></p>
<p>With the rapid development of the internet in the past decade, it has become
increasingly important to extract valuable information from vast resources
efficiently, which is crucial for establishing a comprehensive digital
ecosystem, particularly in the context of research surveys and comprehension.
The foundation of these tasks focuses on accurate extraction and deep mining of
data from scientific documents, which are essential for building a robust data
infrastructure. However, parsing raw data or extracting data from complex
scientific documents have been ongoing challenges. Current data extraction
methods for scientific documents typically use rule-based (RB) or machine
learning (ML) approaches. However, using rule-based methods can incur high
coding costs for articles with intricate typesetting. Conversely, relying
solely on machine learning methods necessitates annotation work for complex
content types within the scientific document, which can be costly.
Additionally, few studies have thoroughly defined and explored the hierarchical
layout within scientific documents. The lack of a comprehensive definition of
the internal structure and elements of the documents indirectly impacts the
accuracy of text classification and object recognition tasks. From the
perspective of analyzing the standard layout and typesetting used in the
specified publication, we propose a new document layout analysis framework
called CTBR(Compartment &amp; Text Blocks Refinement). Firstly, we define
scientific documents into hierarchical divisions: base domain, compartment, and
text blocks. Next, we conduct an in-depth exploration and classification of the
meanings of text blocks. Finally, we utilize the results of text block
classification to implement object recognition within scientific documents
based on rule-based compartment segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09323">Perspectives on the State and Future of Deep Learning -- 2023. (arXiv:2312.09323v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1">Micah Goldblum</a>, <a href="http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1">Anima Anandkumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Baraniuk_R/0/1/0/all/0/1">Richard Baraniuk</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1">Kyunghyun Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1">Zachary C Lipton</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1">Melanie Mitchell</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1">Preetum Nakkiran</a>, <a href="http://arxiv.org/find/cs/1/au:+Welling_M/0/1/0/all/0/1">Max Welling</a>, <a href="http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1">Andrew Gordon Wilson</a></p>
<p>The goal of this series is to chronicle opinions and issues in the field of
machine learning as they stand today and as they change over time. The plan is
to host this survey periodically until the AI singularity
paperclip-frenzy-driven doomsday, keeping an updated list of topical questions
and interviewing new community members for each edition. In this issue, we
probed people's opinions on interpretable AI, the value of benchmarking in
modern NLP, the state of progress towards understanding deep learning, and the
future of academia.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09404">Unbiasing Enhanced Sampling on a High-dimensional Free Energy Surface with Deep Generative Model. (arXiv:2312.09404v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yikai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ghosh_T/0/1/0/all/0/1">Tushar K. Ghosh</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Ming Chen</a></p>
<p>Biased enhanced sampling methods utilizing collective variables (CVs) are
powerful tools for sampling conformational ensembles. Due to high intrinsic
dimensions, efficiently generating conformational ensembles for complex systems
requires enhanced sampling on high-dimensional free energy surfaces. While
methods like temperature-accelerated molecular dynamics (TAMD) can adopt many
CVs in a simulation, unbiasing the simulation requires accurate modeling of a
high-dimensional CV probability distribution, which is challenging for
traditional density estimation techniques. Here we propose an unbiasing method
based on the score-based diffusion model, a deep generative learning method
that excels in density estimation across complex data landscapes. We test the
score-based diffusion unbiasing method on TAMD simulations. The results
demonstrate that this unbiasing approach significantly outperforms traditional
unbiasing methods, and can generate accurate unbiased conformational ensembles
for simulations with a number of CVs higher than usual ranges.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09783">Keep the Faith: Faithful Explanations in Convolutional Neural Networks for Case-Based Reasoning. (arXiv:2312.09783v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wolf_T/0/1/0/all/0/1">Tom Nuno Wolf</a>, <a href="http://arxiv.org/find/cs/1/au:+Bongratz_F/0/1/0/all/0/1">Fabian Bongratz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rickmann_A/0/1/0/all/0/1">Anne-Marie Rickmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Polsterl_S/0/1/0/all/0/1">Sebastian P&#xf6;lsterl</a>, <a href="http://arxiv.org/find/cs/1/au:+Wachinger_C/0/1/0/all/0/1">Christian Wachinger</a></p>
<p>Explaining predictions of black-box neural networks is crucial when applied
to decision-critical tasks. Thus, attribution maps are commonly used to
identify important image regions, despite prior work showing that humans prefer
explanations based on similar examples. To this end, ProtoPNet learns a set of
class-representative feature vectors (prototypes) for case-based reasoning.
During inference, similarities of latent features to prototypes are linearly
classified to form predictions and attribution maps are provided to explain the
similarity. In this work, we evaluate whether architectures for case-based
reasoning fulfill established axioms required for faithful explanations using
the example of ProtoPNet. We show that such architectures allow the extraction
of faithful explanations. However, we prove that the attribution maps used to
explain the similarities violate the axioms. We propose a new procedure to
extract explanations for trained ProtoPNets, named ProtoPFaith. Conceptually,
these explanations are Shapley values, calculated on the similarity scores of
each prototype. They allow to faithfully answer which prototypes are present in
an unseen image and quantify each pixel's contribution to that presence,
thereby complying with all axioms. The theoretical violations of ProtoPNet
manifest in our experiments on three datasets (CUB-200-2011, Stanford Dogs,
RSNA) and five architectures (ConvNet, ResNet, ResNet50, WideResNet50,
ResNeXt50). Our experiments show a qualitative difference between the
explanations given by ProtoPNet and ProtoPFaith. Additionally, we quantify the
explanations with the Area Over the Perturbation Curve, on which ProtoPFaith
outperforms ProtoPNet on all experiments by a factor $&gt;10^3$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09857">Deep Unsupervised Domain Adaptation for Time Series Classification: a Benchmark. (arXiv:2312.09857v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fawaz_H/0/1/0/all/0/1">Hassan Ismail Fawaz</a>, <a href="http://arxiv.org/find/cs/1/au:+Grosso_G/0/1/0/all/0/1">Ganesh Del Grosso</a>, <a href="http://arxiv.org/find/cs/1/au:+Kerdoncuff_T/0/1/0/all/0/1">Tanguy Kerdoncuff</a>, <a href="http://arxiv.org/find/cs/1/au:+Boisbunon_A/0/1/0/all/0/1">Aurelie Boisbunon</a>, <a href="http://arxiv.org/find/cs/1/au:+Saffar_I/0/1/0/all/0/1">Illyyne Saffar</a></p>
<p>Unsupervised Domain Adaptation (UDA) aims to harness labeled source data to
train models for unlabeled target data. Despite extensive research in domains
like computer vision and natural language processing, UDA remains underexplored
for time series data, which has widespread real-world applications ranging from
medicine and manufacturing to earth observation and human activity recognition.
Our paper addresses this gap by introducing a comprehensive benchmark for
evaluating UDA techniques for time series classification, with a focus on deep
learning methods. We provide seven new benchmark datasets covering various
domain shifts and temporal dynamics, facilitating fair and standardized UDA
method assessments with state of the art neural network backbones (e.g.
Inception) for time series data. This benchmark offers insights into the
strengths and limitations of the evaluated approaches while preserving the
unsupervised nature of domain adaptation, making it directly applicable to
practical problems. Our paper serves as a vital resource for researchers and
practitioners, advancing domain adaptation solutions for time series data and
fostering innovation in this critical field. The implementation code of this
benchmark is available at https://github.com/EricssonResearch/UDA-4-TSC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09983">Toward Computationally Efficient Inverse Reinforcement Learning via Reward Shaping. (arXiv:2312.09983v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cooke_L/0/1/0/all/0/1">Lauren H. Cooke</a>, <a href="http://arxiv.org/find/cs/1/au:+Klyne_H/0/1/0/all/0/1">Harvey Klyne</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1">Edwin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1">Cassidy Laidlaw</a>, <a href="http://arxiv.org/find/cs/1/au:+Tambe_M/0/1/0/all/0/1">Milind Tambe</a>, <a href="http://arxiv.org/find/cs/1/au:+Doshi_Velez_F/0/1/0/all/0/1">Finale Doshi-Velez</a></p>
<p>Inverse reinforcement learning (IRL) is computationally challenging, with
common approaches requiring the solution of multiple reinforcement learning
(RL) sub-problems. This work motivates the use of potential-based reward
shaping to reduce the computational burden of each RL sub-problem. This work
serves as a proof-of-concept and we hope will inspire future developments
towards computationally efficient IRL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10029">Challenges with unsupervised LLM knowledge discovery. (arXiv:2312.10029v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farquhar_S/0/1/0/all/0/1">Sebastian Farquhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Varma_V/0/1/0/all/0/1">Vikrant Varma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kenton_Z/0/1/0/all/0/1">Zachary Kenton</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasteiger_J/0/1/0/all/0/1">Johannes Gasteiger</a>, <a href="http://arxiv.org/find/cs/1/au:+Mikulik_V/0/1/0/all/0/1">Vladimir Mikulik</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_R/0/1/0/all/0/1">Rohin Shah</a></p>
<p>We show that existing unsupervised methods on large language model (LLM)
activations do not discover knowledge -- instead they seem to discover whatever
feature of the activations is most prominent. The idea behind unsupervised
knowledge elicitation is that knowledge satisfies a consistency structure,
which can be used to discover knowledge. We first prove theoretically that
arbitrary features (not just knowledge) satisfy the consistency structure of a
particular leading unsupervised knowledge-elicitation method,
contrast-consistent search (Burns et al. - <a href="/abs/2212.03827">arXiv:2212.03827</a>). We then present a
series of experiments showing settings in which unsupervised methods result in
classifiers that do not predict knowledge, but instead predict a different
prominent feature. We conclude that existing unsupervised methods for
discovering latent knowledge are insufficient, and we contribute sanity checks
to apply to evaluating future knowledge elicitation methods. Conceptually, we
hypothesise that the identification issues explored here, e.g. distinguishing a
model's knowledge from that of a simulated character's, will persist for future
unsupervised methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.01402">GALAXY: Graph-based Active Learning at the Extreme. (arXiv:2202.01402v2 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_Samuels_J/0/1/0/all/0/1">Julian Katz-Samuels</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert Nowak</a></p>
<p>Active learning is a label-efficient approach to train highly effective
models while interactively selecting only small subsets of unlabelled data for
labelling and training. In "open world" settings, the classes of interest can
make up a small fraction of the overall dataset -- most of the data may be
viewed as an out-of-distribution or irrelevant class. This leads to extreme
class-imbalance, and our theory and methods focus on this core issue. We
propose a new strategy for active learning called GALAXY (Graph-based Active
Learning At the eXtrEme), which blends ideas from graph-based active learning
and deep learning. GALAXY automatically and adaptively selects more
class-balanced examples for labeling than most other methods for active
learning. Our theory shows that GALAXY performs a refined form of uncertainty
sampling that gathers a much more class-balanced dataset than vanilla
uncertainty sampling. Experimentally, we demonstrate GALAXY's superiority over
existing state-of-art deep active learning algorithms in unbalanced vision
classification settings generated from popular datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2203.14673">Using Machine Learning to generate an open-access cropland map from satellite images time series in the Indian Himalayan Region. (arXiv:2203.14673v1 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Danya Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gajardo_J/0/1/0/all/0/1">Joaquin Gajardo</a>, <a href="http://arxiv.org/find/cs/1/au:+Volpi_M/0/1/0/all/0/1">Michele Volpi</a>, <a href="http://arxiv.org/find/cs/1/au:+Defraeye_T/0/1/0/all/0/1">Thijs Defraeye</a></p>
<p>Crop maps are crucial for agricultural monitoring and food management and can
additionally support domain-specific applications, such as setting cold supply
chain infrastructure in developing countries. Machine learning (ML) models,
combined with freely-available satellite imagery, can be used to produce
cost-effective and high spatial-resolution crop maps. However, accessing ground
truth data for supervised learning is especially challenging in developing
countries due to factors such as smallholding and fragmented geography, which
often results in a lack of crop type maps or even reliable cropland maps. Our
area of interest for this study lies in Himachal Pradesh, India, where we aim
at producing an open-access binary cropland map at 10-meter resolution for the
Kullu, Shimla, and Mandi districts. To this end, we developed an ML pipeline
that relies on Sentinel-2 satellite images time series. We investigated two
pixel-based supervised classifiers, support vector machines (SVM) and random
forest (RF), which are used to classify per-pixel time series for binary
cropland mapping. The ground truth data used for training, validation and
testing was manually annotated from a combination of field survey reference
points and visual interpretation of very high resolution (VHR) imagery. We
trained and validated the models via spatial cross-validation to account for
local spatial autocorrelation and selected the RF model due to overall
robustness and lower computational cost. We tested the generalization
capability of the chosen model at the pixel level by computing the accuracy,
recall, precision, and F1-score on hold-out test sets of each district,
achieving an average accuracy for the RF (our best model) of 87%. We used this
model to generate a cropland map for three districts of Himachal Pradesh,
spanning 14,600 km2, which improves the resolution and quality of existing
public maps.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.07317">Algorithm Selection for Deep Active Learning with Imbalanced Datasets. (arXiv:2302.07317v3 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_S/0/1/0/all/0/1">Shuai Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1">Saurabh Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert Nowak</a></p>
<p>Label efficiency has become an increasingly important objective in deep
learning applications. Active learning aims to reduce the number of labeled
examples needed to train deep networks, but the empirical performance of active
learning algorithms can vary dramatically across datasets and applications. It
is difficult to know in advance which active learning strategy will perform
well or best in a given application. To address this, we propose the first
adaptive algorithm selection strategy for deep active learning. For any
unlabeled dataset, our (meta) algorithm TAILOR (Thompson ActIve Learning
algORithm selection) iteratively and adaptively chooses among a set of
candidate active learning algorithms. TAILOR uses novel reward functions aimed
at gathering class-balanced examples. Extensive experiments in multi-class and
multi-label applications demonstrate TAILOR's effectiveness in achieving
accuracy comparable or better than that of the best of the candidate
algorithms. Our implementation of TAILOR is open-sourced at
https://github.com/jifanz/TAILOR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09910">LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning. (arXiv:2306.09910v2 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Canal_G/0/1/0/all/0/1">Gregory Canal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1">Stephen Mussmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1">Arnav M. Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1">Gantavya Bhatt</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yinglun Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1">Simon Shaolei Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1">Kevin Jamieson</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert D Nowak</a></p>
<p>Labeled data are critical to modern machine learning applications, but
obtaining labels can be expensive. To mitigate this cost, machine learning
methods, such as transfer learning, semi-supervised learning and active
learning, aim to be label-efficient: achieving high predictive performance from
relatively few labeled examples. While obtaining the best label-efficiency in
practice often requires combinations of these techniques, existing benchmark
and evaluation frameworks do not capture a concerted combination of all such
techniques. This paper addresses this deficiency by introducing LabelBench, a
new computationally-efficient framework for joint evaluation of multiple
label-efficient learning techniques. As an application of LabelBench, we
introduce a novel benchmark of state-of-the-art active learning methods in
combination with semi-supervised learning for fine-tuning pretrained vision
transformers. Our benchmark demonstrates better label-efficiencies than
previously reported in active learning. LabelBench's modular codebase is
open-sourced for the broader community to contribute label-efficient learning
methods and benchmarks. The repository can be found at:
https://github.com/EfficientTraining/LabelBench.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09196">DIRECT: Deep Active Learning under Imbalance and Label Noise. (arXiv:2312.09196v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nuggehalli_S/0/1/0/all/0/1">Shyam Nuggehalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_L/0/1/0/all/0/1">Lalit Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1">Robert Nowak</a></p>
<p>Class imbalance is a prevalent issue in real world machine learning
applications, often leading to poor performance in rare and minority classes.
With an abundance of wild unlabeled data, active learning is perhaps the most
effective technique in solving the problem at its root -- collecting a more
balanced and informative set of labeled examples during annotation. In this
work, we propose a novel algorithm that first identifies the class separation
threshold and then annotate the most uncertain examples from the minority
classes, close to the separation threshold. Through a novel reduction to
one-dimensional active learning, our algorithm DIRECT is able to leverage the
classic active learning literature to address issues such as batch labeling and
tolerance towards label noise. Compared to existing algorithms, our algorithm
saves more than 15\% of the annotation budget compared to state-of-art active
learning algorithm and more than 90\% of annotation budget compared to random
sampling.
</p>
</p>
</div>

    </div>
    </body>
    