<!DOCTYPE html>
<html>
<head>
<title>2023-07-18-cs-cl</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2307.07513">An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Mingquan Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Song Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1">Ying Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lihui Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1">Yifan Peng</a></p>
<p>Background: The predictive Intensive Care Unit (ICU) scoring system plays an
important role in ICU management because it predicts important outcomes,
especially mortality. Many scoring systems have been developed and used in the
ICU. These scoring systems are primarily based on the structured clinical data
in the electronic health record (EHR), which may suffer the loss of important
clinical information in the narratives and images. Methods: In this work, we
build a deep learning based survival prediction model with multi-modality data
to predict ICU mortality. Four sets of features are investigated: (1)
physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2)
common thorax diseases pre-defined by radiologists, (3) BERT-based text
representations, and (4) chest X-ray image features. We use the Medical
Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the
proposed model. Results: Our model achieves the average C-index of 0.7829 (95%
confidence interval, 0.7620-0.8038), which substantially exceeds that of the
baseline with SAPS-II features (0.7470 (0.7263-0.7676)). Ablation studies
further demonstrate the contributions of pre-defined labels (2.00%), text
features (2.44%), and image features (2.82%).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07516">Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Touma_L/0/1/0/all/0/1">Lana Touma</a>, <a href="http://arxiv.org/find/cs/1/au:+Horani_M/0/1/0/all/0/1">Mohammad Al Horani</a>, <a href="http://arxiv.org/find/cs/1/au:+Tailouni_M/0/1/0/all/0/1">Manar Tailouni</a>, <a href="http://arxiv.org/find/cs/1/au:+Dahabiah_A/0/1/0/all/0/1">Anas Dahabiah</a>, <a href="http://arxiv.org/find/cs/1/au:+Jallad_K/0/1/0/all/0/1">Khloud Al Jallad</a></p>
<p>Automatic Deception Detection has been a hot research topic for a long time,
using machine learning and deep learning to automatically detect deception,
brings new light to this old field. In this paper, we proposed a voting-based
method for automatic deception detection from videos using audio, visual and
lexical features. Experiments were done on two datasets, the Real-life trial
dataset by Michigan University and the Miami University deception detection
dataset. Video samples were split into frames of images, audio, and
manuscripts. Our Voting-based Multimodal proposed solution consists of three
models. The first model is CNN for detecting deception from images, the second
model is Support Vector Machine (SVM) on Mel spectrograms for detecting
deception from audio and the third model is Word2Vec on Support Vector Machine
(SVM) for detecting deception from manuscripts. Our proposed solution
outperforms state of the art. Best results achieved on images, audio and text
were 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%
on video, audio and text respectively on Miami University Deception Detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07518">CephGPT-4: An Interactive Multimodal Cephalometric Measurement and Diagnostic System with Visual Large Language Model. (arXiv:2307.07518v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Lei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jincong Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhaoxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dian Zhang</a></p>
<p>Large-scale multimodal language models (LMMs) have achieved remarkable
success in general domains. However, the exploration of diagnostic language
models based on multimodal cephalometric medical data remains limited. In this
paper, we propose a novel multimodal cephalometric analysis and diagnostic
dialogue model. Firstly, a multimodal orthodontic medical dataset is
constructed, comprising cephalometric images and doctor-patient dialogue data,
with automatic analysis of cephalometric landmarks using U-net and generation
of diagnostic reports. Then, the cephalometric dataset and generated diagnostic
reports are separately fine-tuned on Minigpt-4 and VisualGLM. Results
demonstrate that the CephGPT-4 model exhibits excellent performance and has the
potential to revolutionize orthodontic measurement and diagnostic applications.
These innovations hold revolutionary application potential in the field of
orthodontics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07520">Translating Latin with Artificial Intelligence. (arXiv:2307.07520v1 [math.HO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Bistafa_S/0/1/0/all/0/1">Sylvio R. Bistafa</a></p>
<p>The major hindrance in the study of earlier scientific literature is the
availability of Latin translations into modern languages. This is particular
true for the works of Euler who authored about 850 manuscripts and wrote a
thousand letters and received back almost two thousand more. The translation of
many of these manuscripts, books and letters have been published in various
sources over the last two centuries, but many more have not yet appeared.
Fortunately, nowadays, the artificial intelligence AI translation can be used
to circumvent the challenges of translating such substantial number of texts.
To validate this tool, benchmark tests have been performed to compare the
performance of two popular AI translating algorithms, namely Google Translate
and ChatGPT. Since it was found that ChatGPT performed better on these tests,
this translating support was then used on an excerpt of a 1739 letter from
Johann Bernoulli to Euler, where he notifies that he was sending to Euler the
first part of his manuscript Hydraulica. The findings highlight ChatGPT as a
valuable translation tool, catering not only to general Latin practitioners but
also proving beneficial for specialized Latin translators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07523">PapagAI:Automated Feedback for Reflective Essays. (arXiv:2307.07523v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Solopova_V/0/1/0/all/0/1">Veronika Solopova</a>, <a href="http://arxiv.org/find/cs/1/au:+Gruszczynski_A/0/1/0/all/0/1">Adrian Gruszczynski</a>, <a href="http://arxiv.org/find/cs/1/au:+Rostom_E/0/1/0/all/0/1">Eiad Rostom</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremer_F/0/1/0/all/0/1">Fritz Cremer</a>, <a href="http://arxiv.org/find/cs/1/au:+Witte_S/0/1/0/all/0/1">Sascha Witte</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chengming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Plossl_F/0/1/0/all/0/1">Fernando Ramos L&#xf3;pez Lea Pl&#xf6;&#xdf;l</a>, <a href="http://arxiv.org/find/cs/1/au:+Hofmann_F/0/1/0/all/0/1">Florian Hofmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Romeike_R/0/1/0/all/0/1">Ralf Romeike</a>, <a href="http://arxiv.org/find/cs/1/au:+Glaser_Zikuda_M/0/1/0/all/0/1">Michaela Gl&#xe4;ser-Zikuda</a>, <a href="http://arxiv.org/find/cs/1/au:+Benzmuller_C/0/1/0/all/0/1">Christoph Benzm&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Landgraf_T/0/1/0/all/0/1">Tim Landgraf</a></p>
<p>Written reflective practice is a regular exercise pre-service teachers
perform during their higher education. Usually, their lecturers are expected to
provide individual feedback, which can be a challenging task to perform on a
regular basis. In this paper, we present the first open-source automated
feedback tool based on didactic theory and implemented as a hybrid AI system.
We describe the components and discuss the advantages and disadvantages of our
system compared to the state-of-art generative large language models. The main
objective of our work is to enable better learning outcomes for students and to
complement the teaching activities of lecturers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07544">A Dialogue System for Assessing Activities of Daily Living: Improving Consistency with Grounded Knowledge. (arXiv:2307.07544v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sheng_Z/0/1/0/all/0/1">Zhecheng Sheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Finzel_R/0/1/0/all/0/1">Raymond Finzel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lucke_M/0/1/0/all/0/1">Michael Lucke</a>, <a href="http://arxiv.org/find/cs/1/au:+Dufresne_S/0/1/0/all/0/1">Sheena Dufresne</a>, <a href="http://arxiv.org/find/cs/1/au:+Gini_M/0/1/0/all/0/1">Maria Gini</a>, <a href="http://arxiv.org/find/cs/1/au:+Pakhomov_S/0/1/0/all/0/1">Serguei Pakhomov</a></p>
<p>In healthcare, the ability to care for oneself is reflected in the
"Activities of Daily Living (ADL)," which serve as a measure of functional
ability (functioning). A lack of functioning may lead to poor living conditions
requiring personal care and assistance. To accurately identify those in need of
support, assistance programs continuously evaluate participants' functioning
across various domains. However, the assessment process may encounter
consistency issues when multiple assessors with varying levels of expertise are
involved. Novice assessors, in particular, may lack the necessary preparation
for real-world interactions with participants. To address this issue, we
developed a dialogue system that simulates interactions between assessors and
individuals of varying functioning in a natural and reproducible way. The
dialogue system consists of two major modules, one for natural language
understanding (NLU) and one for natural language generation (NLG),
respectively. In order to generate responses consistent with the underlying
knowledge base, the dialogue system requires both an understanding of the
user's query and of biographical details of an individual being simulated. To
fulfill this requirement, we experimented with query classification and
generated responses based on those biographical details using some recently
released InstructGPT-like models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07558">Exploring the Emotional and Mental Well-Being of Individuals with Long COVID Through Twitter Analysis. (arXiv:2307.07558v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_G/0/1/0/all/0/1">Guocheng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1">Huaiyu Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Quan_W/0/1/0/all/0/1">Wei Quan</a></p>
<p>The COVID-19 pandemic has led to the emergence of Long COVID, a cluster of
symptoms that persist after infection. Long COVID patients may also experience
mental health challenges, making it essential to understand individuals'
emotional and mental well-being. This study aims to gain a deeper understanding
of Long COVID individuals' emotional and mental well-being, identify the topics
that most concern them, and explore potential correlations between their
emotions and social media activity. Specifically, we classify tweets into four
categories based on the content, detect the presence of six basic emotions, and
extract prevalent topics. Our analyses reveal that negative emotions dominated
throughout the study period, with two peaks during critical periods, such as
the outbreak of new COVID variants. The findings of this study have
implications for policy and measures for addressing the mental health
challenges of individuals with Long COVID and provide a foundation for future
work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07586">QontSum: On Contrasting Salient Content for Query-focused Summarization. (arXiv:2307.07586v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sotudeh_S/0/1/0/all/0/1">Sajad Sotudeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Goharian_N/0/1/0/all/0/1">Nazli Goharian</a></p>
<p>Query-focused summarization (QFS) is a challenging task in natural language
processing that generates summaries to address specific queries. The broader
field of Generative Information Retrieval (Gen-IR) aims to revolutionize
information extraction from vast document corpora through generative
approaches, encompassing Generative Document Retrieval (GDR) and Grounded
Answer Retrieval (GAR). This paper highlights the role of QFS in Grounded
Answer Generation (GAR), a key subdomain of Gen-IR that produces human-readable
answers in direct correspondence with queries, grounded in relevant documents.
In this study, we propose QontSum, a novel approach for QFS that leverages
contrastive learning to help the model attend to the most relevant regions of
the input document. We evaluate our approach on a couple of benchmark datasets
for QFS and demonstrate that it either outperforms existing state-of-the-art or
exhibits a comparable performance with considerably reduced computational cost
through enhancements in the fine-tuning stage, rather than relying on
large-scale pre-training experiments, which is the focus of current SOTA.
Moreover, we conducted a human study and identified improvements in the
relevance of generated summaries to the posed queries without compromising
fluency. We further conduct an error analysis study to understand our model's
limitations and propose avenues for future research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07614">Towards Generalizable Detection of Urgency of Discussion Forum Posts. (arXiv:2307.07614v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Svabensky_V/0/1/0/all/0/1">Valdemar &#x160;v&#xe1;bensk&#xfd;</a>, <a href="http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1">Ryan S. Baker</a>, <a href="http://arxiv.org/find/cs/1/au:+Zambrano_A/0/1/0/all/0/1">Andr&#xe9;s Zambrano</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yishan Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Slater_S/0/1/0/all/0/1">Stefan Slater</a></p>
<p>Students who take an online course, such as a MOOC, use the course's
discussion forum to ask questions or reach out to instructors when encountering
an issue. However, reading and responding to students' questions is difficult
to scale because of the time needed to consider each message. As a result,
critical issues may be left unresolved, and students may lose the motivation to
continue in the course. To help address this problem, we build predictive
models that automatically determine the urgency of each forum post, so that
these posts can be brought to instructors' attention. This paper goes beyond
previous work by predicting not just a binary decision cut-off but a post's
level of urgency on a 7-point scale. First, we train and cross-validate several
models on an original data set of 3,503 posts from MOOCs at University of
Pennsylvania. Second, to determine the generalizability of our models, we test
their performance on a separate, previously published data set of 29,604 posts
from MOOCs at Stanford University. While the previous work on post urgency used
only one data set, we evaluated the prediction across different data sets and
courses. The best-performing model was a support vector regressor trained on
the Universal Sentence Encoder embeddings of the posts, achieving an RMSE of
1.1 on the training set and 1.4 on the test set. Understanding the urgency of
forum posts enables instructors to focus their time more effectively and, as a
result, better support student learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07645">Othering and low prestige framing of immigrant cuisines in US restaurant reviews and large language models. (arXiv:2307.07645v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yiwei Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gligoric_K/0/1/0/all/0/1">Kristina Gligori&#x107;</a>, <a href="http://arxiv.org/find/cs/1/au:+Jurafsky_D/0/1/0/all/0/1">Dan Jurafsky</a></p>
<p>Identifying and understanding implicit attitudes toward food can help efforts
to mitigate social prejudice due to food's pervasive role as a marker of
cultural and ethnic identity. Stereotypes about food are a form of
microaggression that contribute to harmful public discourse that may in turn
perpetuate prejudice toward ethnic groups and negatively impact economic
outcomes for restaurants. Through careful linguistic analyses, we evaluate
social theories about attitudes toward immigrant cuisine in a large-scale study
of framing differences in 2.1M English language Yelp reviews of restaurants in
14 US states. Controlling for factors such as restaurant price and neighborhood
racial diversity, we find that immigrant cuisines are more likely to be framed
in objectifying and othering terms of authenticity (e.g., authentic,
traditional), exoticism (e.g., exotic, different), and prototypicality (e.g.,
typical, usual), but that non-Western immigrant cuisines (e.g., Indian,
Mexican) receive more othering than European cuisines (e.g., French, Italian).
We further find that non-Western immigrant cuisines are framed less positively
and as lower status, being evaluated in terms of affordability and hygiene.
Finally, we show that reviews generated by large language models (LLMs)
reproduce many of the same framing tendencies. Our results empirically
corroborate social theories of taste and gastronomic stereotyping, and reveal
linguistic processes by which such attitudes are reified.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07683">Single and Multi-Speaker Cloned Voice Detection: From Perceptual to Learned Features. (arXiv:2307.07683v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barrington_S/0/1/0/all/0/1">Sarah Barrington</a>, <a href="http://arxiv.org/find/cs/1/au:+Barua_R/0/1/0/all/0/1">Romit Barua</a>, <a href="http://arxiv.org/find/cs/1/au:+Koorma_G/0/1/0/all/0/1">Gautham Koorma</a>, <a href="http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1">Hany Farid</a></p>
<p>Synthetic-voice cloning technologies have seen significant advances in recent
years, giving rise to a range of potential harms. From small- and large-scale
financial fraud to disinformation campaigns, the need for reliable methods to
differentiate real and synthesized voices is imperative. We describe three
techniques for differentiating a real from a cloned voice designed to
impersonate a specific person. These three approaches differ in their feature
extraction stage with low-dimensional perceptual features offering high
interpretability but lower accuracy, to generic spectral features, and
end-to-end learned features offering less interpretability but higher accuracy.
We show the efficacy of these approaches when trained on a single speaker's
voice and when trained on multiple voices. The learned features consistently
yield an equal error rate between $0\%$ and $4\%$, and are reasonably robust to
adversarial laundering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07696">Coupling Large Language Models with Logic Programming for Robust and General Reasoning from Text. (arXiv:2307.07696v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1">Adam Ishay</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joohyung Lee</a></p>
<p>While large language models (LLMs), such as GPT-3, appear to be robust and
general, their reasoning ability is not at a level to compete with the best
models trained for specific natural language reasoning problems. In this study,
we observe that a large language model can serve as a highly effective few-shot
semantic parser. It can convert natural language sentences into a logical form
that serves as input for answer set programs, a logic-based declarative
knowledge representation formalism. The combination results in a robust and
general system that can handle multiple question-answering tasks without
requiring retraining for each new task. It only needs a few examples to guide
the LLM's adaptation to a specific task, along with reusable ASP knowledge
modules that can be applied to multiple tasks. We demonstrate that this method
achieves state-of-the-art performance on several NLP benchmarks, including
bAbI, StepGame, CLUTRR, and gSCAN. Additionally, it successfully tackles robot
planning tasks that an LLM alone fails to solve.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07697">Think-on-Graph: Deep and Responsible Reasoning of Large Language Model with Knowledge Graph. (arXiv:2307.07697v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiashuo Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chengjin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">Lumingyuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Saizhuo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1">Chen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1">Yeyun Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1">Heung-Yeung Shum</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1">Jian Guo</a></p>
<p>Large language models (LLMs) have made significant strides in various tasks,
yet they often struggle with complex reasoning and exhibit poor performance in
scenarios where knowledge traceability, timeliness, and accuracy are crucial.
To address these limitations, we present Think-on-Graph (ToG), a novel
framework that leverages knowledge graphs to enhance LLMs' ability for deep and
responsible reasoning. By employing ToG, we can identify entities relevant to a
given question and conduct exploration and reasoning to retrieve related
triples from an external knowledge database. This iterative procedure generates
multiple reasoning pathways consisting of sequentially connected triplets until
sufficient information is gathered to answer the question or the maximum depth
is reached. Through experiments on complex multi-hop reasoning
question-answering tasks, we demonstrate that ToG outperforms existing methods,
effectively addressing the aforementioned limitations of LLMs without incurring
additional training costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07699">Leveraging Large Language Models to Generate Answer Set Programs. (arXiv:2307.07699v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1">Adam Ishay</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Joohyung Lee</a></p>
<p>Large language models (LLMs), such as GPT-3 and GPT-4, have demonstrated
exceptional performance in various natural language processing tasks and have
shown the ability to solve certain reasoning problems. However, their reasoning
capabilities are limited and relatively shallow, despite the application of
various prompting techniques. In contrast, formal logic is adept at handling
complex reasoning, but translating natural language descriptions into formal
logic is a challenging task that non-experts struggle with. This paper proposes
a neuro-symbolic method that combines the strengths of large language models
and answer set programming. Specifically, we employ an LLM to transform natural
language descriptions of logic puzzles into answer set programs. We carefully
design prompts for an LLM to convert natural language descriptions into answer
set programs in a step by step manner. Surprisingly, with just a few in-context
learning examples, LLMs can generate reasonably complex answer set programs.
The majority of errors made are relatively simple and can be easily corrected
by humans, thus enabling LLMs to effectively assist in the creation of answer
set programs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07705">CPET: Effective Parameter-Efficient Tuning for Compressed Large Language Models. (arXiv:2307.07705v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Weilin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1">Yuxiang Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1">Xu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhengyan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>Parameter-efficient tuning (PET) has been widely explored in recent years
because it tunes much fewer parameters (PET modules) than full-parameter
fine-tuning (FT) while still stimulating sufficient knowledge from large
language models (LLMs) for downstream tasks. Moreover, when PET is employed to
serve multiple tasks, different task-specific PET modules can be built on a
frozen LLM, avoiding redundant LLM deployments. Although PET significantly
reduces the cost of tuning and deploying LLMs, its inference still suffers from
the computational bottleneck of LLMs. To address the above issue, we propose an
effective PET framework based on compressed LLMs, named "CPET". In CPET, we
evaluate the impact of mainstream LLM compression techniques on PET performance
and then introduce knowledge inheritance and recovery strategies to restore the
knowledge loss caused by these compression techniques. Our experimental results
demonstrate that, owing to the restoring strategies of CPET, collaborating
task-specific PET modules with a compressed LLM can achieve comparable
performance to collaborating PET modules with the original version of the
compressed LLM and outperform directly applying vanilla PET methods to the
compressed LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07740">Political Sentiment Analysis of Persian Tweets Using CNN-LSTM Model. (arXiv:2307.07740v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1">Mohammad Dehghani</a>, <a href="http://arxiv.org/find/cs/1/au:+Yazdanparast_Z/0/1/0/all/0/1">Zahra Yazdanparast</a></p>
<p>Sentiment analysis is the process of identifying and categorizing people's
emotions or opinions regarding various topics. The analysis of Twitter
sentiment has become an increasingly popular topic in recent years. In this
paper, we present several machine learning and a deep learning model to
analysis sentiment of Persian political tweets. Our analysis was conducted
using Bag of Words and ParsBERT for word representation. We applied Gaussian
Naive Bayes, Gradient Boosting, Logistic Regression, Decision Trees, Random
Forests, as well as a combination of CNN and LSTM to classify the polarities of
tweets. The results of this study indicate that deep learning with ParsBERT
embedding performs better than machine learning. The CNN-LSTM model had the
highest classification accuracy with 89 percent on the first dataset with three
classes and 71 percent on the second dataset with seven classes. Due to the
complexity of Persian, it was a difficult task to achieve this level of
efficiency.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07798">Opinion mining using Double Channel CNN for Recommender System. (arXiv:2307.07798v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sayyadpour_M/0/1/0/all/0/1">Minoo Sayyadpour</a>, <a href="http://arxiv.org/find/cs/1/au:+Nazarizadeh_A/0/1/0/all/0/1">Ali Nazarizadeh</a></p>
<p>Much unstructured data has been produced with the growth of the Internet and
social media. A significant volume of textual data includes users' opinions
about products in online stores and social media. By exploring and categorizing
them, helpful information can be acquired, including customer satisfaction,
user feedback about a particular event, predicting the sale of a specific
product, and other similar cases. In this paper, we present an approach for
sentiment analysis with a deep learning model and use it to recommend products.
A two-channel convolutional neural network model has been used for opinion
mining, which has five layers and extracts essential features from the data. We
increased the number of comments by applying the SMOTE algorithm to the initial
dataset and balanced the data. Then we proceed to cluster the aspects. We also
assign a weight to each cluster using tensor decomposition algorithms that
improve the recommender system's performance. Our proposed method has reached
91.6% accuracy, significantly improved compared to previous aspect-based
approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07843">Transformers are Universal Predictors. (arXiv:2307.07843v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Sourya Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Choraria_M/0/1/0/all/0/1">Moulik Choraria</a>, <a href="http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1">Lav R. Varshney</a></p>
<p>We find limits to the Transformer architecture for language modeling and show
it has a universal prediction property in an information-theoretic sense. We
further analyze performance in non-asymptotic data regimes to understand the
role of various components of the Transformer architecture, especially in the
context of data-efficient training. We validate our theoretical analysis with
experiments on both synthetic and real datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07851">AspectCSE: Sentence Embeddings for Aspect-based Semantic Textual Similarity using Contrastive Learning and Structured Knowledge. (arXiv:2307.07851v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1">Tim Schopf</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerber_E/0/1/0/all/0/1">Emanuel Gerber</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostendorff_M/0/1/0/all/0/1">Malte Ostendorff</a>, <a href="http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1">Florian Matthes</a></p>
<p>Generic sentence embeddings provide a coarse-grained approximation of
semantic textual similarity but ignore specific aspects that make texts
similar. Conversely, aspect-based sentence embeddings provide similarities
between texts based on certain predefined aspects. Thus, similarity predictions
of texts are more targeted to specific requirements and more easily
explainable. In this paper, we present AspectCSE, an approach for aspect-based
contrastive learning of sentence embeddings. Results indicate that AspectCSE
achieves an average improvement of 3.97% on information retrieval tasks across
multiple aspects compared to the previous best results. We also propose using
Wikidata knowledge graph properties to train models of multi-aspect sentence
embeddings in which multiple specific aspects are simultaneously considered
during similarity predictions. We demonstrate that multi-aspect embeddings
outperform single-aspect embeddings on aspect-specific information retrieval
tasks. Finally, we examine the aspect-based sentence embedding space and
demonstrate that embeddings of semantically similar aspect labels are often
close, even without explicit similarity training between different aspect
labels.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07864">CIDER: Context sensitive sentiment analysis for short-form text. (arXiv:2307.07864v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Young_J/0/1/0/all/0/1">James C. Young</a>, <a href="http://arxiv.org/find/cs/1/au:+Arthur_R/0/1/0/all/0/1">Rudy Arthur</a>, <a href="http://arxiv.org/find/cs/1/au:+Williams_H/0/1/0/all/0/1">Hywel T.P. Williams</a></p>
<p>Researchers commonly perform sentiment analysis on large collections of short
texts like tweets, Reddit posts or newspaper headlines that are all focused on
a specific topic, theme or event. Usually, general purpose sentiment analysis
methods are used which perform well on average but miss the variation in
meaning that happens across different contexts, for example, the word "active"
has a very different intention and valence in the phrase "active lifestyle"
versus "active volcano". This work presents a new approach, CIDER (Context
Informed Dictionary and sEntiment Reasoner), which performs context sensitive
sentiment analysis, where the valence of sentiment laden terms is inferred from
the whole corpus before being used to score the individual texts. In this paper
we detail the CIDER algorithm and demonstrate that it outperforms
state-of-the-art generalist sentiment analysis on a large collection of tweets
about the weather. We have made our implementation of CIDER available as a
python package: https://pypi.org/project/ciderpolarity/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07870">Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1">Grgur Kova&#x10d;</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1">Masataka Sawayama</a>, <a href="http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1">R&#xe9;my Portelas</a>, <a href="http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1">C&#xe9;dric Colas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1">Peter Ford Dominey</a>, <a href="http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1">Pierre-Yves Oudeyer</a></p>
<p>Large Language Models (LLMs) are often misleadingly recognized as having a
personality or a set of values. We argue that an LLM can be seen as a
superposition of perspectives with different values and personality traits.
LLMs exhibit context-dependent values and personality traits that change based
on the induced perspective (as opposed to humans, who tend to have more
coherent values and personality traits across contexts). We introduce the
concept of perspective controllability, which refers to a model's affordance to
adopt various perspectives with differing values and personality traits. In our
experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study
how exhibited values and personality traits change based on different
perspectives. Through qualitative experiments, we show that LLMs express
different values when those are (implicitly or explicitly) implied in the
prompt, and that LLMs express different values even when those are not
obviously implied (demonstrating their context-dependent nature). We then
conduct quantitative experiments to study the controllability of different
models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the
effectiveness of various methods for inducing perspectives, and the smoothness
of the models' drivability. We conclude by examining the broader implications
of our work and outline a variety of associated scientific questions. The
project website is available at
https://sites.google.com/view/llm-superpositions .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07880">Is Prompt-Based Finetuning Always Better than Vanilla Finetuning? Insights from Cross-Lingual Language Understanding. (arXiv:2307.07880v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_B/0/1/0/all/0/1">Bolei Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_E/0/1/0/all/0/1">Ercong Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmid_H/0/1/0/all/0/1">Helmut Schmid</a>, <a href="http://arxiv.org/find/cs/1/au:+Schutze_H/0/1/0/all/0/1">Hinrich Sch&#xfc;tze</a></p>
<p>Multilingual pretrained language models (MPLMs) have demonstrated substantial
performance improvements in zero-shot cross-lingual transfer across various
natural language understanding tasks by finetuning MPLMs on task-specific
labelled data of a source language (e.g. English) and evaluating on a wide
range of target languages. Recent studies show that prompt-based finetuning
surpasses regular finetuning in few-shot scenarios. However, the exploration of
prompt-based learning in multilingual tasks remains limited. In this study, we
propose the ProFiT pipeline to investigate the cross-lingual capabilities of
Prompt-based Finetuning. We conduct comprehensive experiments on diverse
cross-lingual language understanding tasks (sentiment classification,
paraphrase identification, and natural language inference) and empirically
analyze the variation trends of prompt-based finetuning performance in
cross-lingual transfer across different few-shot and full-data settings. Our
results reveal the effectiveness and versatility of prompt-based finetuning in
cross-lingual language understanding. Our findings indicate that prompt-based
finetuning outperforms vanilla finetuning in full-data scenarios and exhibits
greater advantages in few-shot scenarios, with different performance patterns
dependent on task types. Additionally, we analyze underlying factors such as
language similarity and pretraining data size that impact the cross-lingual
performance of prompt-based finetuning. Overall, our work provides valuable
insights into the cross-lingual prowess of prompt-based finetuning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07889">Zero-shot NLG evaluation through Pairware Comparisons with LLMs. (arXiv:2307.07889v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liusie_A/0/1/0/all/0/1">Adian Liusie</a>, <a href="http://arxiv.org/find/cs/1/au:+Manakul_P/0/1/0/all/0/1">Potsawee Manakul</a>, <a href="http://arxiv.org/find/cs/1/au:+Gales_M/0/1/0/all/0/1">Mark J. F. Gales</a></p>
<p>Evaluating Natural Language Generation (NLG) outputs is crucial but laborious
and expensive. While various automatic NLG assessment methods have been
proposed, they often are quite task-specific and have to be engineered with a
particular domain and attribute in mind. In this work, we propose a robust
zero-shot approach to NLG evaluation using pairwise comparative judgment with
open-source Large Language Models (LLMs). The motivation for this approach is
that even as humans, it is easier to determine which of two options are better,
than it is to independently objectively score each option. We use this insight
and leverage the emergent abilities of LLMs, where we probe FlanT5 to determine
which of two candidate responses is better, rather than assigning absolute
scores. Our results demonstrate that comparative assessment is a more effective
approach than absolute scoring, enabling smaller open-source LLMs to achieve
comparable performance to larger public access APIs. We evaluate systems on
both summary evaluation and dialogue response generation, and show that
opensource LLMs can lead to good correlations with human scores for a range of
different attributes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07924">Communicative Agents for Software Development. (arXiv:2307.07924v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1">Chen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cong_X/0/1/0/all/0/1">Xin Cong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Cheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weize Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1">Yusheng Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Juyuan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhiyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Maosong Sun</a></p>
<p>Software engineering is a domain characterized by intricate decision-making
processes, often relying on nuanced intuition and consultation. Recent
advancements in deep learning have started to revolutionize software
engineering practices through elaborate designs implemented at various stages
of software development. In this paper, we present an innovative paradigm that
leverages large language models (LLMs) throughout the entire software
development process, streamlining and unifying key processes through natural
language communication, thereby eliminating the need for specialized models at
each phase. At the core of this paradigm lies ChatDev, a virtual chat-powered
software development company that mirrors the established waterfall model,
meticulously dividing the development process into four distinct chronological
stages: designing, coding, testing, and documenting. Each stage engages a team
of agents, such as programmers, code reviewers, and test engineers, fostering
collaborative dialogue and facilitating a seamless workflow. The chat chain
acts as a facilitator, breaking down each stage into atomic subtasks. This
enables dual roles, allowing for proposing and validating solutions through
context-aware communication, leading to efficient resolution of specific
subtasks. The instrumental analysis of ChatDev highlights its remarkable
efficacy in software generation, enabling the completion of the entire software
development process in under seven minutes at a cost of less than one dollar.
It not only identifies and alleviates potential vulnerabilities but also
rectifies potential hallucinations while maintaining commendable efficiency and
cost-effectiveness. The potential of ChatDev unveils fresh possibilities for
integrating LLMs into the realm of software development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07930">GeoGPT: Understanding and Processing Geospatial Tasks through An Autonomous GPT. (arXiv:2307.07930v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Cheng Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1">Shangyou Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhengting He</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1">Wenhao Yu</a></p>
<p>Decision-makers in GIS need to combine a series of spatial algorithms and
operations to solve geospatial tasks. For example, in the task of facility
siting, the Buffer tool is usually first used to locate areas close or away
from some specific entities; then, the Intersect or Erase tool is used to
select candidate areas satisfied multiple requirements. Though professionals
can easily understand and solve these geospatial tasks by sequentially
utilizing relevant tools, it is difficult for non-professionals to handle these
problems. Recently, Generative Pre-trained Transformer (e.g., ChatGPT) presents
strong performance in semantic understanding and reasoning. Especially, AutoGPT
can further extend the capabilities of large language models (LLMs) by
automatically reasoning and calling externally defined tools. Inspired by these
studies, we attempt to lower the threshold of non-professional users to solve
geospatial tasks by integrating the semantic understanding ability inherent in
LLMs with mature tools within the GIS community. Specifically, we develop a new
framework called GeoGPT that can conduct geospatial data collection,
processing, and analysis in an autonomous manner with the instruction of only
natural language. In other words, GeoGPT is used to understand the demands of
non-professional users merely based on input natural language descriptions, and
then think, plan, and execute defined GIS tools to output final effective
results. Several cases including geospatial data crawling, spatial query,
facility siting, and mapping validate the effectiveness of our framework.
Though limited cases are presented in this paper, GeoGPT can be further
extended to various tasks by equipping with more GIS tools, and we think the
paradigm of "foundational plus professional" implied in GeoGPT provides an
effective way to develop next-generation GIS in this era of large foundation
models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07940">Deduplicating and Ranking Solution Programs for Suggesting Reference Solutions. (arXiv:2307.07940v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shirafuji_A/0/1/0/all/0/1">Atsushi Shirafuji</a>, <a href="http://arxiv.org/find/cs/1/au:+Watanobe_Y/0/1/0/all/0/1">Yutaka Watanobe</a></p>
<p>Referring to the solution programs written by the other users is helpful for
learners in programming education. However, current online judge systems just
list all solution programs submitted by users for references, and the programs
are sorted based on the submission date and time, execution time, or user
rating, ignoring to what extent the program can be a reference. In addition,
users struggle to refer to a variety of solution approaches since there are too
many duplicated and near-duplicated programs. To motivate the learners to refer
to various solutions to learn the better solution approaches, in this paper, we
propose an approach to deduplicate and rank common solution programs in each
programming problem. Based on the hypothesis that the more duplicated programs
adopt a more common approach and can be a reference, we remove the
near-duplicated solution programs and rank the unique programs based on the
duplicate count. The experiments on the solution programs submitted to a
real-world online judge system demonstrate that the number of programs is
reduced by 60.20%, whereas the baseline only reduces by 29.59% after the
deduplication, meaning that the users only need to refer to 39.80% of programs
on average. Furthermore, our analysis shows that top-10 ranked programs cover
29.95% of programs on average, indicating that the users can grasp 29.95% of
solution approaches by referring to only 10 programs. The proposed approach
shows the potential of reducing the learners' burden of referring to too many
solutions and motivating them to learn a variety of better approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07946">Unifying Token and Span Level Supervisions for Few-Shot Sequence Labeling. (arXiv:2307.07946v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1">Zifeng Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qingyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zhiwei Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xuemin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yunbo Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1">Qing Gu</a></p>
<p>Few-shot sequence labeling aims to identify novel classes based on only a few
labeled samples. Existing methods solve the data scarcity problem mainly by
designing token-level or span-level labeling models based on metric learning.
However, these methods are only trained at a single granularity (i.e., either
token level or span level) and have some weaknesses of the corresponding
granularity. In this paper, we first unify token and span level supervisions
and propose a Consistent Dual Adaptive Prototypical (CDAP) network for few-shot
sequence labeling. CDAP contains the token-level and span-level networks,
jointly trained at different granularities. To align the outputs of two
networks, we further propose a consistent loss to enable them to learn from
each other. During the inference phase, we propose a consistent greedy
inference algorithm that first adjusts the predicted probability and then
greedily selects non-overlapping spans with maximum probability. Extensive
experiments show that our model achieves new state-of-the-art results on three
benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07948">Model Adaptation for ASR in low-resource Indian Languages. (arXiv:2307.07948v1 [eess.AS])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Singh_A/0/1/0/all/0/1">Abhayjeet Singh</a>, <a href="http://arxiv.org/find/eess/1/au:+Mehta_A/0/1/0/all/0/1">Arjun Singh Mehta</a>, <a href="http://arxiv.org/find/eess/1/au:+S_A/0/1/0/all/0/1">Ashish Khuraishi K S</a>, <a href="http://arxiv.org/find/eess/1/au:+G_D/0/1/0/all/0/1">Deekshitha G</a>, <a href="http://arxiv.org/find/eess/1/au:+Date_G/0/1/0/all/0/1">Gauri Date</a>, <a href="http://arxiv.org/find/eess/1/au:+Nanavati_J/0/1/0/all/0/1">Jai Nanavati</a>, <a href="http://arxiv.org/find/eess/1/au:+Bandekar_J/0/1/0/all/0/1">Jesuraja Bandekar</a>, <a href="http://arxiv.org/find/eess/1/au:+Basumatary_K/0/1/0/all/0/1">Karnalius Basumatary</a>, <a href="http://arxiv.org/find/eess/1/au:+P_K/0/1/0/all/0/1">Karthika P</a>, <a href="http://arxiv.org/find/eess/1/au:+Badiger_S/0/1/0/all/0/1">Sandhya Badiger</a>, <a href="http://arxiv.org/find/eess/1/au:+Udupa_S/0/1/0/all/0/1">Sathvik Udupa</a>, <a href="http://arxiv.org/find/eess/1/au:+Kumar_S/0/1/0/all/0/1">Saurabh Kumar</a>, <a href="http://arxiv.org/find/eess/1/au:+Savitha/0/1/0/all/0/1">Savitha</a>, <a href="http://arxiv.org/find/eess/1/au:+Ghosh_P/0/1/0/all/0/1">Prasanta Kumar Ghosh</a>, <a href="http://arxiv.org/find/eess/1/au:+V_P/0/1/0/all/0/1">Prashanthi V</a>, <a href="http://arxiv.org/find/eess/1/au:+Pai_P/0/1/0/all/0/1">Priyanka Pai</a>, <a href="http://arxiv.org/find/eess/1/au:+Nanavati_R/0/1/0/all/0/1">Raoul Nanavati</a>, <a href="http://arxiv.org/find/eess/1/au:+Saxena_R/0/1/0/all/0/1">Rohan Saxena</a>, <a href="http://arxiv.org/find/eess/1/au:+Mora_S/0/1/0/all/0/1">Sai Praneeth Reddy Mora</a>, <a href="http://arxiv.org/find/eess/1/au:+Raghavan_S/0/1/0/all/0/1">Srinivasa Raghavan</a></p>
<p>Automatic speech recognition (ASR) performance has improved drastically in
recent years, mainly enabled by self-supervised learning (SSL) based acoustic
models such as wav2vec2 and large-scale multi-lingual training like Whisper. A
huge challenge still exists for low-resource languages where the availability
of both audio and text is limited. This is further complicated by the presence
of multiple dialects like in Indian languages. However, many Indian languages
can be grouped into the same families and share the same script and grammatical
structure. This is where a lot of adaptation and fine-tuning techniques can be
applied to overcome the low-resource nature of the data by utilising
well-resourced similar languages.
</p>
<p>In such scenarios, it is important to understand the extent to which each
modality, like acoustics and text, is important in building a reliable ASR. It
could be the case that an abundance of acoustic data in a language reduces the
need for large text-only corpora. Or, due to the availability of various
pretrained acoustic models, the vice-versa could also be true. In this proposed
special session, we encourage the community to explore these ideas with the
data in two low-resource Indian languages of Bengali and Bhojpuri. These
approaches are not limited to Indian languages, the solutions are potentially
applicable to various languages spoken around the world.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07951">MinT: Boosting Generalization in Mathematical Reasoning via Multi-View Fine-Tuning. (arXiv:2307.07951v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1">Zhenwen Liang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dian Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1">Xiaoman Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1">Wenlin Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1">Qingkai Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiangliang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a></p>
<p>Reasoning in mathematical domains remains a significant challenge for
relatively small language models (LMs). Many current methods focus on
specializing LMs in mathematical reasoning and rely heavily on knowledge
distillation from powerful but inefficient large LMs (LLMs). In this work, we
explore a new direction that avoids over-reliance on LLM teachers, introducing
a multi-view fine-tuning method that efficiently exploits existing mathematical
problem datasets with diverse annotation styles. Our approach uniquely
considers the various annotation formats as different "views" and leverages
them in training the model. By postpending distinct instructions to input
questions, models can learn to generate solutions in diverse formats in a
flexible manner. Experimental results show that our strategy enables a LLaMA-7B
model to outperform prior approaches that utilize knowledge distillation, as
well as carefully established baselines. Additionally, the proposed method
grants the models promising generalization ability across various views and
datasets, and the capability to learn from inaccurate or incomplete noisy data.
We hope our multi-view training paradigm could inspire future studies in other
machine reasoning domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07982">A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chitty_Venkata_K/0/1/0/all/0/1">Krishna Teja Chitty-Venkata</a>, <a href="http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1">Sparsh Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Emani_M/0/1/0/all/0/1">Murali Emani</a>, <a href="http://arxiv.org/find/cs/1/au:+Vishwanath_V/0/1/0/all/0/1">Venkatram Vishwanath</a>, <a href="http://arxiv.org/find/cs/1/au:+Somani_A/0/1/0/all/0/1">Arun K. Somani</a></p>
<p>Recent years have seen a phenomenal rise in performance and applications of
transformer neural networks. The family of transformer networks, including
Bidirectional Encoder Representations from Transformer (BERT), Generative
Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their
effectiveness across Natural Language Processing (NLP) and Computer Vision (CV)
domains. Transformer-based networks such as ChatGPT have impacted the lives of
common men. However, the quest for high predictive performance has led to an
exponential increase in transformers' memory and compute footprint. Researchers
have proposed techniques to optimize transformer inference at all levels of
abstraction. This paper presents a comprehensive survey of techniques for
optimizing the inference phase of transformer networks. We survey techniques
such as knowledge distillation, pruning, quantization, neural architecture
search and lightweight network design at the algorithmic level. We further
review hardware-level optimization techniques and the design of novel hardware
accelerators for transformers. We summarize the quantitative results on the
number of parameters/FLOPs and accuracy of several models/techniques to
showcase the tradeoff exercised by them. We also outline future directions in
this rapidly evolving field of research. We believe that this survey will
educate both novice and seasoned researchers and also spark a plethora of
research efforts in this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07994">Facilitating Multi-turn Emotional Support Conversation with Positive Emotion Elicitation: A Reinforcement Learning Approach. (arXiv:2307.07994v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jinfeng Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhuang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1">Minlie Huang</a></p>
<p>Emotional support conversation (ESC) aims to provide emotional support (ES)
to improve one's mental state. Existing works stay at fitting grounded
responses and responding strategies (e.g., question), which ignore the effect
on ES and lack explicit goals to guide emotional positive transition. To this
end, we introduce a new paradigm to formalize multi-turn ESC as a process of
positive emotion elicitation. Addressing this task requires finely adjusting
the elicitation intensity in ES as the conversation progresses while
maintaining conversational goals like coherence. In this paper, we propose
Supporter, a mixture-of-expert-based reinforcement learning model, and well
design ES and dialogue coherence rewards to guide policy's learning for
responding. Experiments verify the superiority of Supporter in achieving
positive emotion elicitation during responding while maintaining conversational
goals including coherence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.08036">A Neural-Symbolic Approach Towards Identifying Grammatically Correct Sentences. (arXiv:2307.08036v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Isaak_N/0/1/0/all/0/1">Nicos Isaak</a></p>
<p>Textual content around us is growing on a daily basis. Numerous articles are
being written as we speak on online newspapers, blogs, or social media.
Similarly, recent advances in the AI field, like language models or traditional
classic AI approaches, are utilizing all the above to improve their learned
representation to tackle NLP challenges with human-like accuracy. It is
commonly accepted that it is crucial to have access to well-written text from
valid sources to tackle challenges like text summarization, question-answering,
machine translation, or even pronoun resolution. For instance, to summarize
well, one needs to select the most important sentences in order to concatenate
them to form the summary. However, what happens if we do not have access to
well-formed English sentences or even non-valid sentences? Despite the
importance of having access to well-written sentences, figuring out ways to
validate them is still an open area of research. To address this problem, we
present a simplified way to validate English sentences through a novel
neural-symbolic approach. Lately, neural-symbolic approaches have triggered an
increasing interest towards tackling various NLP challenges, as they are
demonstrating their effectiveness as a central component in various AI systems.
Through combining Classic with Modern AI, which involves the blending of
grammatical and syntactical rules with language models, we effectively tackle
the Corpus of Linguistic Acceptability (COLA), a task that shows whether or not
a sequence of words is an English grammatical sentence. Among others,
undertaken experiments effectively show that blending symbolic and non-symbolic
systems helps the former provide insights about the latter's accuracy results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2110.05006">Pre-trained Language Models in Biomedical Domain: A Systematic Survey. (arXiv:2110.05006v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Benyou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qianqian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1">Jiahuan Pei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhihong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_P/0/1/0/all/0/1">Prayag Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+fu_J/0/1/0/all/0/1">Jie fu</a></p>
<p>Pre-trained language models (PLMs) have been the de facto paradigm for most
natural language processing (NLP) tasks. This also benefits biomedical domain:
researchers from informatics, medicine, and computer science (CS) communities
propose various PLMs trained on biomedical datasets, e.g., biomedical text,
electronic health records, protein, and DNA sequences for various biomedical
tasks. However, the cross-discipline characteristics of biomedical PLMs hinder
their spreading among communities; some existing works are isolated from each
other without comprehensive comparison and discussions. It expects a survey
that not only systematically reviews recent advances of biomedical PLMs and
their applications but also standardizes terminology and benchmarks. In this
paper, we summarize the recent progress of pre-trained language models in the
biomedical domain and their applications in biomedical downstream tasks.
Particularly, we discuss the motivations and propose a taxonomy of existing
biomedical PLMs. Their applications in biomedical downstream tasks are
exhaustively discussed. At last, we illustrate various limitations and future
trends, which we hope can provide inspiration for the future research of the
research community.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2111.07533">Automated scholarly paper review: Concepts, technologies, and challenges. (arXiv:2111.07533v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jialiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhangping Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yidong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaodong Shi</a></p>
<p>Peer review is a widely accepted mechanism for research evaluation, playing a
pivotal role in academic publishing. However, criticisms have long been leveled
at this mechanism, mostly because of its poor efficiency and low
reproducibility. Recent years have seen the application of artificial
intelligence (AI) in assisting the peer review process. Nonetheless, with the
involvement of humans, such limitations remain inevitable. In this paper, we
propose the concept and pipeline of automated scholarly paper review (ASPR) and
review the relevant literature and technologies of achieving a full-scale
computerized review process. On the basis of the review and discussion, we
conclude that there is already corresponding research and preliminary
implementation at each stage of ASPR. We further look into the challenges in
ASPR with the existing technologies. The major difficulties lie in inadequate
data, imperfect document parsing and representation, defective
human$\unicode{x2013}$computer interaction, and flawed deep logical reasoning.
Moreover, we point out the future directions and discuss the possible moral and
ethical issues of ASPR. In the foreseeable future, ASPR and peer review will
coexist in a reinforcing manner before ASPR is able to fully undertake the
reviewing workload from humans.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.05783">The Hierarchical Organization of Syntax. (arXiv:2112.05783v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ravandi_B/0/1/0/all/0/1">Babak Ravandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Concu_V/0/1/0/all/0/1">Valentina Concu</a></p>
<p>Hierarchies are the hidden backbones of complex systems and their analysis
allows for a deeper understanding of their structure and how they evolve. We
consider languages also to be complex adaptive systems with several intricate
networks that capture their structure and function. Hence, we decided to
analyze the hierarchical organization of historical syntactic networks to
understand how syntax evolves over time. We created these networks from a
corpus of German texts from the 11th to 17th centuries, focusing on the
hierarchical levels of these networks. diachronically and to map them to
specific communicative needs of speakers. We developed a framework to
empirically track the emergence of syntactic structures diachronically,
enabling us to map the communicative needs of speakers with these structures.
We named these syntactic structures "syntactic communicative hierarchies." We
showed that the communicative needs of speakers are the organizational force of
syntax. Thus, we argue that the emergence of syntactic communicative
hierarchies plays a crucial role in shaping syntax over time. This may indicate
that languages evolve not only to increase the efficiency of transferring
information, but also to increase our capacity, as a species, to communicate
our needs with more and more sophisticated abstractions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.11472">Diversity Over Size: On the Effect of Sample and Topic Sizes for Argument Mining Datasets. (arXiv:2205.11472v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schiller_B/0/1/0/all/0/1">Benjamin Schiller</a>, <a href="http://arxiv.org/find/cs/1/au:+Daxenberger_J/0/1/0/all/0/1">Johannes Daxenberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1">Iryna Gurevych</a></p>
<p>The task of Argument Mining, that is extracting argumentative sentences for a
specific topic from large document sources, is an inherently difficult task for
machine learning models and humans alike, as large Argument Mining datasets are
rare and recognition of argumentative sentences requires expert knowledge. The
task becomes even more difficult if it also involves stance detection of
retrieved arguments. Given the cost and complexity of creating suitably large
Argument Mining datasets, we ask whether it is necessary for acceptable
performance to have datasets growing in size. Our findings show that, when
using carefully composed training samples and a model pretrained on related
tasks, we can reach 95% of the maximum performance while reducing the training
sample size by at least 85%. This gain is consistent across three Argument
Mining tasks on three different datasets. We also publish a new dataset for
future benchmarking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.00465">On Grounded Planning for Embodied Tasks with Language Models. (arXiv:2209.00465v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1">Bill Yuchen Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1">Chengsong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qian Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1">Wenda Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sommerer_S/0/1/0/all/0/1">Sam Sommerer</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a></p>
<p>Language models (LMs) have demonstrated their capability in possessing
commonsense knowledge of the physical world, a crucial aspect of performing
tasks in everyday life. However, it remains unclear **whether LMs have the
capacity to generate grounded, executable plans for embodied tasks.** This is a
challenging task as LMs lack the ability to perceive the environment through
vision and feedback from the physical environment. In this paper, we address
this important research question and present the first investigation into the
topic. Our novel problem formulation, named **G-PlanET**, inputs a high-level
goal and a data table about objects in a specific environment, and then outputs
a step-by-step actionable plan for a robotic agent to follow. To facilitate the
study, we establish an **evaluation protocol** and design a dedicated metric to
assess the quality of the plans. Our experiments demonstrate that the use of
tables for encoding the environment and an iterative decoding strategy can
significantly enhance the LMs' ability in grounded planning. Our analysis also
reveals interesting and non-trivial findings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.01232">Elaboration-Generating Commonsense Question Answering at Scale. (arXiv:2209.01232v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenya Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1">Vivek Srikumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1">Hanna Hajishirzi</a>, <a href="http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1">Noah A. Smith</a></p>
<p>In question answering requiring common sense, language models (e.g., GPT-3)
have been used to generate text expressing background knowledge that helps
improve performance. Yet the cost of working with such models is very high; in
this work, we finetune smaller language models to generate useful intermediate
context, referred to here as elaborations. Our framework alternates between
updating two language models -- an elaboration generator and an answer
predictor -- allowing each to influence the other. Using less than 0.5% of the
parameters of GPT-3, our model outperforms alternatives with similar sizes and
closes the gap on GPT-3 on four commonsense question answering benchmarks.
Human evaluations show that the quality of the generated elaborations is high.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.00131">Underspecification in Language Modeling Tasks: A Causality-Informed Study of Gendered Pronoun Resolution. (arXiv:2210.00131v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McMilin_E/0/1/0/all/0/1">Emily McMilin</a></p>
<p>Modern language modeling tasks are often underspecified: for a given token
prediction, many words may satisfy the user's intent of producing natural
language at inference time, however only one word would minimize the task's
loss function at training time. We provide a simple yet plausible causal
mechanism describing the role underspecification plays in the generation of
spurious correlations. Despite its simplicity, our causal model directly
informs the development of two lightweight black-box evaluation methods, that
we apply to gendered pronoun resolution tasks on a wide range of LLMs to 1) aid
in the detection of inference-time task underspecification by exploiting 2)
previously unreported gender vs. time and gender vs. location spurious
correlations on LLMs with a range of A) sizes: from BERT-base to GPT 3.5, B)
pre-training objectives: from masked &amp; autoregressive language modeling to a
mixture of these objectives, and C) training stages: from pre-training only to
reinforcement learning from human feedback (RLHF). Code and open-source demos
available at https: //github.com/2dot71mily/sib_paper.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.06282">DialoGen: Generalized Long-Range Context Representation for Dialogue Systems. (arXiv:2210.06282v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dey_S/0/1/0/all/0/1">Suvodip Dey</a>, <a href="http://arxiv.org/find/cs/1/au:+Desarkar_M/0/1/0/all/0/1">Maunendra Sankar Desarkar</a>, <a href="http://arxiv.org/find/cs/1/au:+Srijith_P/0/1/0/all/0/1">P. K. Srijith</a></p>
<p>Long-range context modeling is crucial to both dialogue understanding and
generation. The most popular method for dialogue context representation is to
concatenate the last-$k$ previous utterances. However, this method may not be
ideal for conversations containing long-range dependencies as it cannot look
beyond last-$k$ utterances. In this work, we propose DialoGen, a novel
encoder-decoder based framework for conversational response generation with a
generalized context representation that can look beyond the last-$k$
utterances. Hence the method is adaptive to conversations with long-range
dependencies. The main idea of our approach is to identify and utilize the most
relevant historical utterances instead of the last-$k$ utterances in
chronological order. We study the effectiveness of our proposed method on both
dialogue generation (open-domain) and understanding (DST) tasks. DialoGen
achieves comparable performance with the state-of-the-art models on DailyDialog
dataset. We also observe performance gain in existing DST models with our
proposed context representation strategy on MultiWOZ dataset. We discuss the
generalizability and interpretability of DialoGen and show that the relevance
score of previous utterances agrees well with human cognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13709">Undesirable biases in NLP: Averting a crisis of measurement. (arXiv:2211.13709v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wal_O/0/1/0/all/0/1">Oskar van der Wal</a>, <a href="http://arxiv.org/find/cs/1/au:+Bachmann_D/0/1/0/all/0/1">Dominik Bachmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Leidinger_A/0/1/0/all/0/1">Alina Leidinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Maanen_L/0/1/0/all/0/1">Leendert van Maanen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuidema_W/0/1/0/all/0/1">Willem Zuidema</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_K/0/1/0/all/0/1">Katrin Schulz</a></p>
<p>As Large Language Models and Natural Language Processing (NLP) technology
rapidly develops and spreads into daily life, it becomes crucial to anticipate
how its use could harm people. One problem that has received a lot of attention
in recent years is that this technology has displayed harmful biases in its
behavior. Although a lot of effort has been invested in assessing and
mitigating these biases, our methods of measuring the biases of NLP models have
serious problems (e.g., it is often unclear what they actually measure). In
this paper, we provide an interdisciplinary approach to discussing the issue of
NLP model bias by adopting the lens of psychometrics -- a field specialized in
the measurement of concepts like bias that are not directly observable. In
particular, we will explore two central notions from psychometrics, the
construct validity and the reliability of measurement tools, and discuss how
they can be applied in the context of measuring model bias. Our goal is to
provide NLP practitioners with methodological tools for designing better bias
measures, and to inspire them more generally to explore tools from
psychometrics when working on bias measurement tools.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.16198">SuS-X: Training-Free Name-Only Transfer of Vision-Language Models. (arXiv:2211.16198v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Udandarao_V/0/1/0/all/0/1">Vishaal Udandarao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1">Ankush Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1">Samuel Albanie</a></p>
<p>Contrastive Language-Image Pre-training (CLIP) has emerged as a simple yet
effective way to train large-scale vision-language models. CLIP demonstrates
impressive zero-shot classification and retrieval on diverse downstream tasks.
However, to leverage its full potential, fine-tuning still appears to be
necessary. Fine-tuning the entire CLIP model can be resource-intensive and
unstable. Moreover, recent methods that aim to circumvent this need for
fine-tuning still require access to images from the target distribution. In
this paper, we pursue a different approach and explore the regime of
training-free "name-only transfer" in which the only knowledge we possess about
the downstream task comprises the names of downstream target categories. We
propose a novel method, SuS-X, consisting of two key building blocks -- SuS and
TIP-X, that requires neither intensive fine-tuning nor costly labelled data.
SuS-X achieves state-of-the-art zero-shot classification results on 19
benchmark datasets. We further show the utility of TIP-X in the training-free
few-shot setting, where we again achieve state-of-the-art results over strong
training-free baselines. Code is available at
https://github.com/vishaal27/SuS-X.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.08913">Unifying Structure Reasoning and Language Model Pre-training for Complex Reasoning. (arXiv:2301.08913v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Siyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhongyu Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiarong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Taishan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1">Zhihao Fan</a></p>
<p>Recent pre-trained language models (PLMs) equipped with foundation reasoning
skills have shown remarkable performance on downstream complex tasks. However,
the significant structure reasoning skill has been rarely studied, which
involves modeling implicit structure information within the text and performing
explicit logical reasoning over them to deduce the conclusion. This paper
proposes a unified learning framework that combines explicit structure
reasoning and language pre-training to endow PLMs with the structure reasoning
skill. It first identifies several elementary structures within contexts to
construct structured queries and performs step-by-step reasoning along the
queries to identify the answer entity. The fusion of textual semantics and
structure reasoning is achieved by using contextual representations learned by
PLMs to initialize the representation space of structures, and performing
stepwise reasoning on this semantic representation space. Experimental results
on four datasets demonstrate that the proposed model achieves significant
improvements in complex reasoning tasks involving diverse structures, and shows
transferability to downstream tasks with limited training data and
effectiveness for complex reasoning of KGs modality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.02064">Lived Experience Matters: Automatic Detection of Stigma on Social Media Toward People Who Use Substances. (arXiv:2302.02064v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giorgi_S/0/1/0/all/0/1">Salvatore Giorgi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bellew_D/0/1/0/all/0/1">Douglas Bellew</a>, <a href="http://arxiv.org/find/cs/1/au:+Habib_D/0/1/0/all/0/1">Daniel Roy Sadek Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Sherman_G/0/1/0/all/0/1">Garrick Sherman</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedoc_J/0/1/0/all/0/1">Joao Sedoc</a>, <a href="http://arxiv.org/find/cs/1/au:+Smitterberg_C/0/1/0/all/0/1">Chase Smitterberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Devoto_A/0/1/0/all/0/1">Amanda Devoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Himelein_Wachowiak_M/0/1/0/all/0/1">McKenzie Himelein-Wachowiak</a>, <a href="http://arxiv.org/find/cs/1/au:+Curtis_B/0/1/0/all/0/1">Brenda Curtis</a></p>
<p>Stigma toward people who use substances (PWUS) is a leading barrier to
seeking treatment.Further, those in treatment are more likely to drop out if
they experience higher levels of stigmatization. While related concepts of hate
speech and toxicity, including those targeted toward vulnerable populations,
have been the focus of automatic content moderation research, stigma and, in
particular, people who use substances have not. This paper explores stigma
toward PWUS using a data set of roughly 5,000 public Reddit posts. We performed
a crowd-sourced annotation task where workers are asked to annotate each post
for the presence of stigma toward PWUS and answer a series of questions related
to their experiences with substance use. Results show that workers who use
substances or know someone with a substance use disorder are more likely to
rate a post as stigmatizing. Building on this, we use a supervised machine
learning framework that centers workers with lived substance use experience to
label each Reddit post as stigmatizing. Modeling person-level demographics in
addition to comment-level language results in a classification accuracy (as
measured by AUC) of 0.69 -- a 17% increase over modeling language alone.
Finally, we explore the linguist cues which distinguish stigmatizing content:
PWUS substances and those who don't agree that language around othering
("people", "they") and terms like "addict" are stigmatizing, while PWUS (as
opposed to those who do not) find discussions around specific substances more
stigmatizing. Our findings offer insights into the nature of perceived stigma
in substance use. Additionally, these results further establish the subjective
nature of such machine learning tasks, highlighting the need for understanding
their social contexts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.12200">A Neural Span-Based Continual Named Entity Recognition Model. (arXiv:2302.12200v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yunan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qingcai Chen</a></p>
<p>Named Entity Recognition (NER) models capable of Continual Learning (CL) are
realistically valuable in areas where entity types continuously increase (e.g.,
personal assistants). Meanwhile the learning paradigm of NER advances to new
patterns such as the span-based methods. However, its potential to CL has not
been fully explored. In this paper, we propose SpanKL, a simple yet effective
Span-based model with Knowledge distillation (KD) to preserve memories and
multi-Label prediction to prevent conflicts in CL-NER. Unlike prior sequence
labeling approaches, the inherently independent modeling in span and entity
level with the designed coherent optimization on SpanKL promotes its learning
at each incremental step and mitigates the forgetting. Experiments on synthetic
CL datasets derived from OntoNotes and Few-NERD show that SpanKL significantly
outperforms previous SoTA in many aspects, and obtains the smallest gap from CL
to the upper bound revealing its high practiced value. The code is available at
https://github.com/Qznan/SpanKL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.04185">Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1">Azade Nova</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1">Hanjun Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1">Dale Schuurmans</a></p>
<p>Large Language Models (LLMs) have achieved great success in solving difficult
tasks across many domains, but such success comes with a high computation cost,
and inference latency. As developers and third parties customize these models,
the need to provide efficient inference has increased. Many efforts have
attempted to reduce inference cost through model compression techniques such as
pruning and distillation. However, these techniques either require labeled
data, or are time-consuming as they require the compressed model to be
retrained to regain accuracy. In this paper, we propose a gradient-free
structured pruning framework that uses only unlabeled data. An evaluation on
the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates
the effectiveness of the proposed approach. By only using the weights of the
pre-trained model and unlabeled data, in a matter of a few minutes on a single
GPU, up to 40% of the original FLOP count can be reduced with less than a 4%
accuracy loss across all tasks considered.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06662">Fuzzy Alignments in Directed Acyclic Graph for Non-Autoregressive Machine Translation. (arXiv:2303.06662v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhengrui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1">Chenze Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1">Shangtong Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Min Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Non-autoregressive translation (NAT) reduces the decoding latency but suffers
from performance degradation due to the multi-modality problem. Recently, the
structure of directed acyclic graph has achieved great success in NAT, which
tackles the multi-modality problem by introducing dependency between vertices.
However, training it with negative log-likelihood loss implicitly requires a
strict alignment between reference tokens and vertices, weakening its ability
to handle multiple translation modalities. In this paper, we hold the view that
all paths in the graph are fuzzily aligned with the reference sentence. We do
not require the exact alignment but train the model to maximize a fuzzy
alignment score between the graph and reference, which takes captured
translations in all modalities into account. Extensive experiments on major WMT
benchmarks show that our method substantially improves translation performance
and increases prediction confidence, setting a new state of the art for NAT on
the raw training data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.09522">P+: Extended Textual Conditioning in Text-to-Image Generation. (arXiv:2303.09522v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1">Andrey Voynov</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1">Qinghao Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1">Daniel Cohen-Or</a>, <a href="http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1">Kfir Aberman</a></p>
<p>We introduce an Extended Textual Conditioning space in text-to-image models,
referred to as $P+$. This space consists of multiple textual conditions,
derived from per-layer prompts, each corresponding to a layer of the denoising
U-net of the diffusion model.
</p>
<p>We show that the extended space provides greater disentangling and control
over image synthesis. We further introduce Extended Textual Inversion (XTI),
where the images are inverted into $P+$, and represented by per-layer tokens.
</p>
<p>We show that XTI is more expressive and precise, and converges faster than
the original Textual Inversion (TI) space. The extended inversion method does
not involve any noticeable trade-off between reconstruction and editability and
induces more regular inversions.
</p>
<p>We conduct a series of extensive experiments to analyze and understand the
properties of the new space, and to showcase the effectiveness of our method
for personalizing text-to-image models. Furthermore, we utilize the unique
properties of this space to achieve previously unattainable results in
object-style mixing using text-to-image models. Project page:
https://prompt-plus.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08467">Learning to Compress Prompts with Gist Tokens. (arXiv:2304.08467v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1">Jesse Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiang Lisa Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1">Noah Goodman</a></p>
<p>Prompting is the primary way to utilize the multitask capabilities of
language models (LMs), but prompts occupy valuable space in the input context
window, and repeatedly encoding the same prompt is computationally inefficient.
Finetuning and distillation methods allow for specialization of LMs without
prompting, but require retraining the model for each task. To avoid this
trade-off entirely, we present gisting, which trains an LM to compress prompts
into smaller sets of "gist" tokens which can be cached and reused for compute
efficiency. Gist models can be trained with no additional cost over standard
instruction finetuning by simply modifying Transformer attention masks to
encourage prompt compression. On decoder (LLaMA-7B) and encoder-decoder
(FLAN-T5-XXL) LMs, gisting enables up to 26x compression of prompts, resulting
in up to 40% FLOPs reductions, 4.2% wall time speedups, and storage savings,
all with minimal loss in output quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.01879">SCOTT: Self-Consistent Chain-of-Thought Distillation. (arXiv:2305.01879v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peifeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yifan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Bing Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1">Xiang Ren</a></p>
<p>Large language models (LMs) beyond a certain scale, demonstrate the emergent
capability of generating free-text rationales for their predictions via
chain-of-thought (CoT) prompting. While CoT can yield dramatically improved
performance, such gains are only observed for sufficiently large LMs. Even more
concerning, there is little guarantee that the generated rationales are
consistent with LM's predictions or faithfully justify the decisions. In this
work, we propose a faithful knowledge distillation method to learn a small,
self-consistent CoT model from a teacher model that is orders of magnitude
larger. To form better supervision, we elicit rationales supporting the gold
answers from a large LM (teacher) by contrastive decoding, which encourages the
teacher to generate tokens that become more plausible only when the answer is
considered. To ensure faithful distillation, we use the teacher-generated
rationales to learn a student LM with a counterfactual reasoning objective,
which prevents the student from ignoring the rationales to make inconsistent
predictions. Experiments show that, while yielding comparable end-task
performance, our method can generate CoT rationales that are more faithful than
baselines do. Further analysis suggests that such a model respects the
rationales more when making decisions; thus, we can improve its performance
more by refining its rationales.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.04195">Cross-Modal Retrieval for Motion and Text via MildTriple Loss. (arXiv:2305.04195v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Sheng Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Haoqiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1">Xin Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengyuan Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a></p>
<p>Cross-modal retrieval has become a prominent research topic in computer
vision and natural language processing with advances made in image-text and
video-text retrieval technologies. However, cross-modal retrieval between human
motion sequences and text has not garnered sufficient attention despite the
extensive application value it holds, such as aiding virtual reality
applications in better understanding users' actions and language. This task
presents several challenges, including joint modeling of the two modalities,
demanding the understanding of person-centered information from text, and
learning behavior features from 3D human motion sequences. Previous work on
motion data modeling mainly relied on autoregressive feature extractors that
may forget previous information, while we propose an innovative model that
includes simple yet powerful transformer-based motion and text encoders, which
can learn representations from the two different modalities and capture
long-term dependencies. Furthermore, the overlap of the same atomic actions of
different human motions can cause semantic conflicts, leading us to explore a
new triplet loss function, MildTriple Loss. it leverages the similarity between
samples in intra-modal space to guide soft-hard negative sample mining in the
joint embedding space to train the triplet loss and reduce the violation caused
by false negative samples. We evaluated our model and method on the latest
HumanML3D and KIT Motion-Language datasets, achieving a 62.9\% recall for
motion retrieval and a 71.5\% recall for text retrieval (based on R@10) on the
HumanML3D dataset. Our code is available at
https://github.com/eanson023/rehamot.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13654">Understanding and Mitigating Spurious Correlations in Text Classification with Neighborhood Analysis. (arXiv:2305.13654v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chew_O/0/1/0/all/0/1">Oscar Chew</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1">Hsuan-Tien Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1">Kai-Wei Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kuan-Hao Huang</a></p>
<p>Recent research has revealed that deep learning models have a tendency to
leverage spurious correlations that exist in the training set but may not hold
true in general circumstances. For instance, a sentiment classifier may
erroneously learn that the token performances is commonly associated with
positive movie reviews. Relying on these spurious correlations degrades the
classifiers performance when it deploys on out-of-distribution data. In this
paper, we examine the implications of spurious correlations through a novel
perspective called neighborhood analysis. The analysis uncovers how spurious
correlations lead unrelated words to erroneously cluster together in the
embedding space. Driven by the analysis, we design a metric to detect spurious
tokens and also propose a family of regularization methods, NFL (doN't Forget
your Language) to mitigate spurious correlations in text classification.
Experiments show that NFL can effectively prevent erroneous clusters and
significantly improve the robustness of classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16960">Training Socially Aligned Language Models in Simulated Human Society. (arXiv:2305.16960v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruibo Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ruixin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Chenyan Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Ge Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1">Andrew M. Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Diyi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Vosoughi_S/0/1/0/all/0/1">Soroush Vosoughi</a></p>
<p>Social alignment in AI systems aims to ensure that these models behave
according to established societal values. However, unlike humans, who derive
consensus on value judgments through social interaction, current language
models (LMs) are trained to rigidly replicate their training corpus in
isolation, leading to subpar generalization in unfamiliar scenarios and
vulnerability to adversarial attacks. This work presents a novel training
paradigm that permits LMs to learn from simulated social interactions. In
comparison to existing methodologies, our approach is considerably more
scalable and efficient, demonstrating superior performance in alignment
benchmarks and human evaluations. This paradigm shift in the training of LMs
brings us a step closer to developing AI systems that can robustly and
accurately reflect societal norms and values.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.18624">W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mingchen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yang Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1">Jeremy Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Huixue Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1">Huaiyuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rui Zhang</a></p>
<p>Contrastive learning has become a popular solution for few-shot Name Entity
Recognization (NER). The conventional configuration strives to reduce the
distance between tokens with the same labels and increase the distance between
tokens with different labels. The effect of this setup may, however, in the
medical domain, there are a lot of entities annotated as OUTSIDE (O), and they
are undesirably pushed apart to other entities that are not labeled as OUTSIDE
(O) by the current contrastive learning method end up with a noisy prototype
for the semantic representation of the label, though there are many OUTSIDE (O)
labeled entities are relevant to the labeled entities. To address this
challenge, we propose a novel method named Weighted Prototypical Contrastive
Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our
approach primarily revolves around constructing the prototype-based contractive
loss and weighting network. These components play a crucial role in assisting
the model in differentiating the negative samples from OUTSIDE (O) tokens and
enhancing the discrimination ability of contrastive learning. Experimental
results show that our proposed W-PROCER framework significantly outperforms the
strong baselines on the three medical benchmark datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00804">Adaptive Contextual Biasing for Transducer Based Streaming Speech Recognition. (arXiv:2306.00804v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Tianyi Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhanheng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaixun Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1">Pengcheng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Biao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changru Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1">Lei Xie</a></p>
<p>By incorporating additional contextual information, deep biasing methods have
emerged as a promising solution for speech recognition of personalized words.
However, for real-world voice assistants, always biasing on such personalized
words with high prediction scores can significantly degrade the performance of
recognizing common words. To address this issue, we propose an adaptive
contextual biasing method based on Context-Aware Transformer Transducer (CATT)
that utilizes the biased encoder and predictor embeddings to perform streaming
prediction of contextual phrase occurrences. Such prediction is then used to
dynamically switch the bias list on and off, enabling the model to adapt to
both personalized and common scenarios. Experiments on Librispeech and internal
voice assistant datasets show that our approach can achieve up to 6.7% and
20.7% relative reduction in WER and CER compared to the baseline respectively,
mitigating up to 96.7% and 84.9% of the relative WER and CER increase for
common cases. Furthermore, our approach has a minimal performance impact in
personalized scenarios while maintaining a streaming inference pipeline with
negligible RTF increase.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02520">A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiarui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1">Filip Ilievski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1">Kaixin Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kollaa_A/0/1/0/all/0/1">Aravinda Kollaa</a>, <a href="http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1">Jonathan Francis</a>, <a href="http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1">Alessandro Oltramari</a></p>
<p>Intelligent Traffic Monitoring (ITMo) technologies hold the potential for
improving road safety/security and for enabling smart city infrastructure.
Understanding traffic situations requires a complex fusion of perceptual
information with domain-specific and causal commonsense knowledge. Whereas
prior work has provided benchmarks and methods for traffic monitoring, it
remains unclear whether models can effectively align these information sources
and reason in novel scenarios. To address this assessment gap, we devise three
novel text-based tasks for situational reasoning in the traffic domain: i)
BDD-QA, which evaluates the ability of Language Models (LMs) to perform
situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason
about complex event causality, and iii) HDT-QA, which evaluates the ability of
models to solve human driving exams. We adopt four knowledge-enhanced methods
that have shown generalization capability across language reasoning tasks in
prior work, based on natural language inference, commonsense knowledge-graph
self-supervision, multi-QA joint training, and dense retrieval of domain
information. We associate each method with a relevant knowledge source,
including knowledge graphs, relevant benchmarks, and driving manuals. In
extensive experiments, we benchmark various knowledge-aware methods against the
three datasets, under zero-shot evaluation; we provide in-depth analyses of
model performance on data partitions and examine model predictions
categorically, to yield useful insights on traffic understanding, given
different background knowledge and reasoning strategies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.02797">Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1">Kevin Ellis</a></p>
<p>We model learning of abstract symbolic concepts by performing Bayesian
inference over utterances in natural language. For efficient inference, we use
a large language model as a proposal distribution. We fit a prior to human data
to better model human learners, and evaluate on both generative and logical
concepts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00259">InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1">Anirudh Ajith</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1">Chris Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Mengzhou Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1">Ameet Deshpande</a>, <a href="http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1">Karthik Narasimhan</a></p>
<p>In-context learning (ICL) performs tasks by prompting a large language model
(LLM) using an instruction and a small set of annotated examples called
demonstrations. Recent work has shown that precise details of the inputs used
in the ICL prompt significantly impact performance, which has incentivized
instruction selection algorithms. The effect of instruction-choice however is
severely underexplored, with existing analyses restricted to shallow subsets of
models and tasks, limiting the generalizability of their insights. We develop
InstructEval, an ICL evaluation suite to conduct a thorough assessment of these
techniques. The suite includes 13 open-sourced LLMs of varying scales from four
model families, and covers nine tasks across three categories. Using the suite,
we evaluate the relative performance of seven popular instruction selection
methods over five metrics relevant to ICL. Our experiments reveal that using
curated manually-written instructions or simple instructions without any
task-specific descriptions often elicits superior ICL performance overall than
that of automatic instruction-induction methods, pointing to a lack of
generalizability among the latter. We release our evaluation suite for
benchmarking instruction selection approaches and enabling more generalizable
methods in this space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02054">Emoji Prediction using Transformer Models. (arXiv:2307.02054v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nusrat_M/0/1/0/all/0/1">Muhammad Osama Nusrat</a>, <a href="http://arxiv.org/find/cs/1/au:+Habib_Z/0/1/0/all/0/1">Zeeshan Habib</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1">Mehreen Alam</a>, <a href="http://arxiv.org/find/cs/1/au:+Jamal_S/0/1/0/all/0/1">Saad Ahmed Jamal</a></p>
<p>In recent years, the use of emojis in social media has increased
dramatically, making them an important element in understanding online
communication. However, predicting the meaning of emojis in a given text is a
challenging task due to their ambiguous nature. In this study, we propose a
transformer-based approach for emoji prediction using BERT, a widely-used
pre-trained language model. We fine-tuned BERT on a large corpus of text
containing both text and emojis to predict the most appropriate emoji for a
given text. Our experimental results demonstrate that our approach outperforms
several state-of-the-art models in predicting emojis with an accuracy of over
75 percent. This work has potential applications in natural language
processing, sentiment analysis, and social media marketing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03917">On decoder-only architecture for speech-to-text and large language model integration. (arXiv:2307.03917v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wu_J/0/1/0/all/0/1">Jian Wu</a>, <a href="http://arxiv.org/find/eess/1/au:+Gaur_Y/0/1/0/all/0/1">Yashesh Gaur</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1">Zhuo Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_L/0/1/0/all/0/1">Long Zhou</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_Y/0/1/0/all/0/1">Yimeng Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1">Tianrui Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1">Jinyu Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shujie Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Ren_B/0/1/0/all/0/1">Bo Ren</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1">Linquan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Wu_Y/0/1/0/all/0/1">Yu Wu</a></p>
<p>Large language models (LLMs) have achieved remarkable success in the field of
natural language processing, enabling better human-computer interaction using
natural language. However, the seamless integration of speech signals into LLMs
has not been explored well. The "decoder-only" architecture has also not been
well studied for speech processing tasks. In this research, we introduce
Speech-LLaMA, a novel approach that effectively incorporates acoustic
information into text-based large language models. Our method leverages
Connectionist Temporal Classification and a simple audio encoder to map the
compressed acoustic features to the continuous semantic space of the LLM. In
addition, we further probe the decoder-only architecture for speech-to-text
tasks by training a smaller scale randomly initialized speech-LLaMA model from
speech-text paired data alone. We conduct experiments on multilingual
speech-to-text translation tasks and demonstrate a significant improvement over
strong baselines, highlighting the potential advantages of decoder-only models
for speech-to-text conversion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.04251">ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1">Salman Mohamadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1">Ghulam Mujtaba</a>, <a href="http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1">Ngan Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1">Gianfranco Doretto</a>, <a href="http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1">Donald A. Adjeroh</a></p>
<p>ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.05360">Unmasking the giant: A comprehensive evaluation of ChatGPT&#x27;s proficiency in coding algorithms and data structures. (arXiv:2307.05360v2 [cs.SE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arefin_S/0/1/0/all/0/1">Sayed Erfan Arefin</a>, <a href="http://arxiv.org/find/cs/1/au:+Heya_T/0/1/0/all/0/1">Tasnia Ashrafi Heya</a>, <a href="http://arxiv.org/find/cs/1/au:+Al_Qudah_H/0/1/0/all/0/1">Hasan Al-Qudah</a>, <a href="http://arxiv.org/find/cs/1/au:+Ineza_Y/0/1/0/all/0/1">Ynes Ineza</a>, <a href="http://arxiv.org/find/cs/1/au:+Serwadda_A/0/1/0/all/0/1">Abdul Serwadda</a></p>
<p>The transformative influence of Large Language Models (LLMs) is profoundly
reshaping the Artificial Intelligence (AI) technology domain. Notably, ChatGPT
distinguishes itself within these models, demonstrating remarkable performance
in multi-turn conversations and exhibiting code proficiency across an array of
languages. In this paper, we carry out a comprehensive evaluation of ChatGPT's
coding capabilities based on what is to date the largest catalog of coding
challenges. Our focus is on the python programming language and problems
centered on data structures and algorithms, two topics at the very foundations
of Computer Science. We evaluate ChatGPT for its ability to generate correct
solutions to the problems fed to it, its code quality, and nature of run-time
errors thrown by its code. Where ChatGPT code successfully executes, but fails
to solve the problem at hand, we look into patterns in the test cases passed in
order to gain some insights into how wrong ChatGPT code is in these kinds of
situations. To infer whether ChatGPT might have directly memorized some of the
data that was used to train it, we methodically design an experiment to
investigate this phenomena. Making comparisons with human performance whenever
feasible, we investigate all the above questions from the context of both its
underlying learning models (GPT-3.5 and GPT-4), on a vast array sub-topics
within the main topics, and on problems having varying degrees of difficulty.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06576">Going Beyond Local: Global Graph-Enhanced Personalized News Recommendations. (arXiv:2307.06576v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1">Boming Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dairui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Suzumura_T/0/1/0/all/0/1">Toyotaro Suzumura</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_R/0/1/0/all/0/1">Ruihai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1">Irene Li</a></p>
<p>Precisely recommending candidate news articles to users has always been a
core challenge for personalized news recommendation systems. Most recent works
primarily focus on using advanced natural language processing techniques to
extract semantic information from rich textual data, employing content-based
methods derived from local historical news. However, this approach lacks a
global perspective, failing to account for users' hidden motivations and
behaviors beyond semantic information. To address this challenge, we propose a
novel model called GLORY (Global-LOcal news Recommendation sYstem), which
combines global representations learned from other users with local
representations to enhance personalized recommendation systems. We accomplish
this by constructing a Global-aware Historical News Encoder, which includes a
global news graph and employs gated graph neural networks to enrich news
representations, thereby fusing historical news representations by a historical
news aggregator. Similarly, we extend this approach to a Global Candidate News
Encoder, utilizing a global entity graph and a candidate news aggregator to
enhance candidate news representation. Evaluation results on two public news
datasets demonstrate that our method outperforms existing approaches.
Furthermore, our model offers more diverse recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06699">Parmesan: mathematical concept extraction for education. (arXiv:2307.06699v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Collard_J/0/1/0/all/0/1">Jacob Collard</a>, <a href="http://arxiv.org/find/cs/1/au:+Paiva_V/0/1/0/all/0/1">Valeria de Paiva</a>, <a href="http://arxiv.org/find/cs/1/au:+Subrahmanian_E/0/1/0/all/0/1">Eswaran Subrahmanian</a></p>
<p>Mathematics is a highly specialized domain with its own unique set of
challenges that has seen limited study in natural language processing. However,
mathematics is used in a wide variety of fields and multidisciplinary research
in many different domains often relies on an understanding of mathematical
concepts. To aid researchers coming from other fields, we develop a prototype
system for searching for and defining mathematical concepts in context,
focusing on the field of category theory. This system, Parmesan, depends on
natural language processing components including concept extraction, relation
extraction, definition extraction, and entity linking. In developing this
system, we show that existing techniques cannot be applied directly to the
category theory domain, and suggest hybrid techniques that do perform well,
though we expect the system to evolve over time. We also provide two cleaned
mathematical corpora that power the prototype system, which are based on
journal articles and wiki pages, respectively. The corpora have been annotated
with dependency trees, lemmas, and part-of-speech tags.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07417">RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Sihan Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1">Furao Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Jian Zhao</a></p>
<p>Data augmentation has been widely used in low-resource NER tasks to tackle
the problem of data sparsity. However, previous data augmentation methods have
the disadvantages of disrupted syntactic structures, token-label mismatch, and
requirement for external knowledge or manual effort. To address these issues,
we propose Robust Prompt-based Data Augmentation (RoPDA) for low-resource NER.
Based on pre-trained language models (PLMs) with continuous prompt, RoPDA
performs entity augmentation and context augmentation through five fundamental
augmentation operations to generate label-flipping and label-preserving
examples. To optimize the utilization of the augmented samples, we present two
techniques: Self-Consistency Filtering and mixup. The former effectively
eliminates low-quality samples, while the latter prevents performance
degradation arising from the direct utilization of label-flipping samples.
Extensive experiments on three benchmarks from different domains demonstrate
that RoPDA significantly improves upon strong baselines, and also outperforms
state-of-the-art semi-supervised learning methods when unlabeled data is
included.
</p>
</p>
</div>

    </div>
    </body>
    