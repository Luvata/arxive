<!DOCTYPE html>
<html>
<head>
<title>2023-12-30-cs-lg</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2312.16176">GreenFlow: A Computation Allocation Framework for Building Environmentally Sound Recommendation System. (arXiv:2312.16176v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xingyu Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhining Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1">Yanchu Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongxuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1">Chenyi Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1">Wenqi Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1">Yize Tan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jinjie Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1">Guannan Zhang</a></p>
<p>Given the enormous number of users and items, industrial cascade
recommendation systems (RS) are continuously expanded in size and complexity to
deliver relevant items, such as news, services, and commodities, to the
appropriate users. In a real-world scenario with hundreds of thousands requests
per second, significant computation is required to infer personalized results
for each request, resulting in a massive energy consumption and carbon emission
that raises concern.
</p>
<p>This paper proposes GreenFlow, a practical computation allocation framework
for RS, that considers both accuracy and carbon emission during inference. For
each stage (e.g., recall, pre-ranking, ranking, etc.) of a cascade RS, when a
user triggers a request, we define two actions that determine the computation:
(1) the trained instances of models with different computational complexity;
and (2) the number of items to be inferred in the stage. We refer to the
combinations of actions in all stages as action chains. A reward score is
estimated for each action chain, followed by dynamic primal-dual optimization
considering both the reward and computation budget. Extensive experiments
verify the effectiveness of the framework, reducing computation consumption by
41% in an industrial mobile application while maintaining commercial revenue.
Moreover, the proposed framework saves approximately 5000kWh of electricity and
reduces 3 tons of carbon emissions per day.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16177">Learning to Infer Unobserved Behaviors: Estimating User&#x27;s Preference for a Site over Other Sites. (arXiv:2312.16177v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1">Atanu R Sinha</a>, <a href="http://arxiv.org/find/cs/1/au:+Anand_T/0/1/0/all/0/1">Tanay Anand</a>, <a href="http://arxiv.org/find/cs/1/au:+Maheshwari_P/0/1/0/all/0/1">Paridhi Maheshwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Lakshmy_A/0/1/0/all/0/1">A V Lakshmy</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1">Vishal Jain</a></p>
<p>A site's recommendation system relies on knowledge of its users' preferences
to offer relevant recommendations to them. These preferences are for attributes
that comprise items and content shown on the site, and are estimated from the
data of users' interactions with the site. Another form of users' preferences
is material too, namely, users' preferences for the site over other sites,
since that shows users' base level propensities to engage with the site.
Estimating users' preferences for the site, however, faces major obstacles
because (a) the focal site usually has no data of its users' interactions with
other sites; these interactions are users' unobserved behaviors for the focal
site; and (b) the Machine Learning literature in recommendation does not offer
a model of this situation. Even if (b) is resolved, the problem in (a) persists
since without access to data of its users' interactions with other sites, there
is no ground truth for evaluation. Moreover, it is most useful when (c) users'
preferences for the site can be estimated at the individual level, since the
site can then personalize recommendations to individual users. We offer a
method to estimate individual user's preference for a focal site, under this
premise. In particular, we compute the focal site's share of a user's online
engagements without any data from other sites. We show an evaluation framework
for the model using only the focal site's data, allowing the site to test the
model. We rely upon a Hierarchical Bayes Method and perform estimation in two
different ways - Markov Chain Monte Carlo and Stochastic Gradient with Langevin
Dynamics. Our results find good support for the approach to computing
personalized share of engagement and for its evaluation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16180">Investigating salient representations and label Variance in Dimensional Speech Emotion Analysis. (arXiv:2312.16180v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mitra_V/0/1/0/all/0/1">Vikramjit Mitra</a>, <a href="http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1">Jingping Nie</a>, <a href="http://arxiv.org/find/cs/1/au:+Azemi_E/0/1/0/all/0/1">Erdrin Azemi</a></p>
<p>Representations derived from models such as BERT (Bidirectional Encoder
Representations from Transformers) and HuBERT (Hidden units BERT), have helped
to achieve state-of-the-art performance in dimensional speech emotion
recognition. Despite their large dimensionality, and even though these
representations are not tailored for emotion recognition tasks, they are
frequently used to train large speech emotion models with high memory and
computational costs. In this work, we show that there exist lower-dimensional
subspaces within the these pre-trained representational spaces that offer a
reduction in downstream model complexity without sacrificing performance on
emotion estimation. In addition, we model label uncertainty in the form of
grader opinion variance, and demonstrate that such information can improve the
models generalization capacity and robustness. Finally, we compare the
robustness of the emotion models against acoustic degradations and observed
that the reduced dimensional representations were able to retain the
performance similar to the full-dimensional representations without significant
regression in dimensional emotion performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16183">LightGCN: Evaluated and Enhanced. (arXiv:2312.16183v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kapralova_M/0/1/0/all/0/1">Milena Kapralova</a>, <a href="http://arxiv.org/find/cs/1/au:+Pantea_L/0/1/0/all/0/1">Luca Pantea</a>, <a href="http://arxiv.org/find/cs/1/au:+Blahovici_A/0/1/0/all/0/1">Andrei Blahovici</a></p>
<p>This paper analyses LightGCN in the context of graph recommendation
algorithms. Despite the initial design of Graph Convolutional Networks for
graph classification, the non-linear operations are not always essential.
LightGCN enables linear propagation of embeddings, enhancing performance. We
reproduce the original findings, assess LightGCN's robustness on diverse
datasets and metrics, and explore Graph Diffusion as an augmentation of signal
propagation in LightGCN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16184">Dynamic Knowledge Injection for AIXI Agents. (arXiv:2312.16184v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Zhao_S/0/1/0/all/0/1">Samuel Yang-Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1">Kee Siong Ng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1">Marcus Hutter</a></p>
<p>Prior approximations of AIXI, a Bayesian optimality notion for general
reinforcement learning, can only approximate AIXI's Bayesian environment model
using an a-priori defined set of models. This is a fundamental source of
epistemic uncertainty for the agent in settings where the existence of
systematic bias in the predefined model class cannot be resolved by simply
collecting more data from the environment. We address this issue in the context
of Human-AI teaming by considering a setup where additional knowledge for the
agent in the form of new candidate models arrives from a human operator in an
online fashion. We introduce a new agent called DynamicHedgeAIXI that maintains
an exact Bayesian mixture over dynamically changing sets of models via a
time-adaptive prior constructed from a variant of the Hedge algorithm. The
DynamicHedgeAIXI agent is the richest direct approximation of AIXI known to
date and comes with good performance guarantees. Experimental results on
epidemic control on contact networks validates the agent's practical utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16188">The curious case of the test set AUROC. (arXiv:2312.16188v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roberts_M/0/1/0/all/0/1">Michael Roberts</a>, <a href="http://arxiv.org/find/cs/1/au:+Hazan_A/0/1/0/all/0/1">Alon Hazan</a>, <a href="http://arxiv.org/find/cs/1/au:+Dittmer_S/0/1/0/all/0/1">S&#xf6;ren Dittmer</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudd_J/0/1/0/all/0/1">James H.F. Rudd</a>, <a href="http://arxiv.org/find/cs/1/au:+Schonlieb_C/0/1/0/all/0/1">Carola-Bibiane Sch&#xf6;nlieb</a></p>
<p>Whilst the size and complexity of ML models have rapidly and significantly
increased over the past decade, the methods for assessing their performance
have not kept pace. In particular, among the many potential performance
metrics, the ML community stubbornly continues to use (a) the area under the
receiver operating characteristic curve (AUROC) for a validation and test
cohort (distinct from training data) or (b) the sensitivity and specificity for
the test data at an optimal threshold determined from the validation ROC.
However, we argue that considering scores derived from the test ROC curve alone
gives only a narrow insight into how a model performs and its ability to
generalise.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16189">OpenRL: A Unified Reinforcement Learning Framework. (arXiv:2312.16189v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1">Shiyu Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wentse Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1">Yiwen Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Bie_F/0/1/0/all/0/1">Fuqing Bie</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_W/0/1/0/all/0/1">Wei-Wei Tu</a></p>
<p>We present OpenRL, an advanced reinforcement learning (RL) framework designed
to accommodate a diverse array of tasks, from single-agent challenges to
complex multi-agent systems. OpenRL's robust support for self-play training
empowers agents to develop advanced strategies in competitive settings.
Notably, OpenRL integrates Natural Language Processing (NLP) with RL, enabling
researchers to address a combination of RL training and language-centric tasks
effectively. Leveraging PyTorch's robust capabilities, OpenRL exemplifies
modularity and a user-centric approach. It offers a universal interface that
simplifies the user experience for beginners while maintaining the flexibility
experts require for innovation and algorithm development. This equilibrium
enhances the framework's practicality, adaptability, and scalability,
establishing a new standard in RL research. To delve into OpenRL's features, we
invite researchers and enthusiasts to explore our GitHub repository at
https://github.com/OpenRL-Lab/openrl and access our comprehensive documentation
at https://openrl-docs.readthedocs.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16190">Hawkes-based cryptocurrency forecasting via Limit Order Book data. (arXiv:2312.16190v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Cestari_R/0/1/0/all/0/1">Raffaele Giuseppe Cestari</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Barchi_F/0/1/0/all/0/1">Filippo Barchi</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Busetto_R/0/1/0/all/0/1">Riccardo Busetto</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Marazzina_D/0/1/0/all/0/1">Daniele Marazzina</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Formentin_S/0/1/0/all/0/1">Simone Formentin</a></p>
<p>Accurately forecasting the direction of financial returns poses a formidable
challenge, given the inherent unpredictability of financial time series. The
task becomes even more arduous when applied to cryptocurrency returns, given
the chaotic and intricately complex nature of crypto markets. In this study, we
present a novel prediction algorithm using limit order book (LOB) data rooted
in the Hawkes model, a category of point processes. Coupled with a continuous
output error (COE) model, our approach offers a precise forecast of return
signs by leveraging predictions of future financial interactions. Capitalizing
on the non-uniformly sampled structure of the original time series, our
strategy surpasses benchmark models in both prediction accuracy and cumulative
profit when implemented in a trading environment. The efficacy of our approach
is validated through Monte Carlo simulations across 50 scenarios. The research
draws on LOB measurements from a centralized cryptocurrency exchange where the
stablecoin Tether is exchanged against the U.S. dollar.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16191">SoK: Taming the Triangle -- On the Interplays between Fairness, Interpretability and Privacy in Machine Learning. (arXiv:2312.16191v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferry_J/0/1/0/all/0/1">Julien Ferry</a> (LAAS-ROC), <a href="http://arxiv.org/find/cs/1/au:+Aivodji_U/0/1/0/all/0/1">Ulrich A&#xef;vodji</a> (ETS), <a href="http://arxiv.org/find/cs/1/au:+Gambs_S/0/1/0/all/0/1">S&#xe9;bastien Gambs</a> (UQAM), <a href="http://arxiv.org/find/cs/1/au:+Huguet_M/0/1/0/all/0/1">Marie-Jos&#xe9; Huguet</a> (LAAS-ROC), <a href="http://arxiv.org/find/cs/1/au:+Siala_M/0/1/0/all/0/1">Mohamed Siala</a> (LAAS-ROC)</p>
<p>Machine learning techniques are increasingly used for high-stakes
decision-making, such as college admissions, loan attribution or recidivism
prediction. Thus, it is crucial to ensure that the models learnt can be audited
or understood by human users, do not create or reproduce discrimination or
bias, and do not leak sensitive information regarding their training data.
Indeed, interpretability, fairness and privacy are key requirements for the
development of responsible machine learning, and all three have been studied
extensively during the last decade. However, they were mainly considered in
isolation, while in practice they interplay with each other, either positively
or negatively. In this Systematization of Knowledge (SoK) paper, we survey the
literature on the interactions between these three desiderata. More precisely,
for each pairwise interaction, we summarize the identified synergies and
tensions. These findings highlight several fundamental theoretical and
empirical conflicts, while also demonstrating that jointly considering these
different requirements is challenging when one aims at preserving a high level
of utility. To solve this issue, we also discuss possible conciliation
mechanisms, showing that a careful design can enable to successfully handle
these different concerns in practice.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16192">A Method for Auto-Differentiation of the Voronoi Tessellation. (arXiv:2312.16192v1 [cs.CG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shumilin_S/0/1/0/all/0/1">Sergei Shumilin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ryabov_A/0/1/0/all/0/1">Alexander Ryabov</a>, <a href="http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1">Evgeny Burnaev</a>, <a href="http://arxiv.org/find/cs/1/au:+Vanovskii_V/0/1/0/all/0/1">Vladimir Vanovskii</a></p>
<p>Voronoi tessellation, also known as Voronoi diagram, is an important
computational geometry technique that has applications in various scientific
disciplines. It involves dividing a given space into regions based on the
proximity to a set of points. Autodifferentiation is a powerful tool for
solving optimization tasks. Autodifferentiation assumes constructing a
computational graph that allows to compute gradients using backpropagation
algorithm. However, often the Voronoi tessellation remains the only
non-differentiable part of a pipeline, prohibiting end-to-end differentiation.
We present the method for autodifferentiation of the 2D Voronoi tessellation.
The method allows one to construct the Voronoi tessellation and pass gradients,
making the construction end-to-end differentiable. We provide the
implementation details and present several important applications. To the best
of our knowledge this is the first autodifferentiable realization of the
Voronoi tessellation providing full set of Voronoi geometrical parameters in a
differentiable way.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16197">INFAMOUS-NeRF: ImproviNg FAce MOdeling Using Semantically-Aligned Hypernetworks with Neural Radiance Fields. (arXiv:2312.16197v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_A/0/1/0/all/0/1">Andrew Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1">Feng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1">Zhiyuan Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Sarkis_M/0/1/0/all/0/1">Michel Sarkis</a>, <a href="http://arxiv.org/find/cs/1/au:+Bi_N/0/1/0/all/0/1">Ning Bi</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yiying Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaoming Liu</a></p>
<p>We propose INFAMOUS-NeRF, an implicit morphable face model that introduces
hypernetworks to NeRF to improve the representation power in the presence of
many training subjects. At the same time, INFAMOUS-NeRF resolves the classic
hypernetwork tradeoff of representation power and editability by learning
semantically-aligned latent spaces despite the subject-specific models, all
without requiring a large pretrained model. INFAMOUS-NeRF further introduces a
novel constraint to improve NeRF rendering along the face boundary. Our
constraint can leverage photometric surface rendering and multi-view
supervision to guide surface color prediction and improve rendering near the
surface. Finally, we introduce a novel, loss-guided adaptive sampling method
for more effective NeRF training by reducing the sampling redundancy. We show
quantitatively and qualitatively that our method achieves higher representation
power than prior face modeling methods in both controlled and in-the-wild
settings. Code and models will be released upon publication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16199">Enhancing User Intent Capture in Session-Based Recommendation with Attribute Patterns. (arXiv:2312.16199v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1">Yifan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jingfeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1">Tianyu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengyang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1">Bing Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>The goal of session-based recommendation in E-commerce is to predict the next
item that an anonymous user will purchase based on the browsing and purchase
history. However, constructing global or local transition graphs to supplement
session data can lead to noisy correlations and user intent vanishing. In this
work, we propose the Frequent Attribute Pattern Augmented Transformer (FAPAT)
that characterizes user intents by building attribute transition graphs and
matching attribute patterns. Specifically, the frequent and compact attribute
patterns are served as memory to augment session representations, followed by a
gate and a transformer block to fuse the whole session information. Through
extensive experiments on two public benchmarks and 100 million industrial data
in three domains, we demonstrate that FAPAT consistently outperforms
state-of-the-art methods by an average of 4.5% across various evaluation
metrics (Hits, NDCG, MRR). Besides evaluating the next-item prediction, we
estimate the models' capabilities to capture user intents via predicting items'
attributes and period-item recommendations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16203">User Consented Federated Recommender System Against Personalized Attribute Inference Attack. (arXiv:2312.16203v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yangqiu Song</a></p>
<p>Recommender systems can be privacy-sensitive. To protect users' private
historical interactions, federated learning has been proposed in distributed
learning for user representations. Using federated recommender (FedRec)
systems, users can train a shared recommendation model on local devices and
prevent raw data transmissions and collections. However, the recommendation
model learned by a common FedRec may still be vulnerable to private information
leakage risks, particularly attribute inference attacks, which means that the
attacker can easily infer users' personal attributes from the learned model.
Additionally, traditional FedRecs seldom consider the diverse privacy
preference of users, leading to difficulties in balancing the recommendation
utility and privacy preservation. Consequently, FedRecs may suffer from
unnecessary recommendation performance loss due to over-protection and private
information leakage simultaneously. In this work, we propose a novel
user-consented federated recommendation system (UC-FedRec) to flexibly satisfy
the different privacy needs of users by paying a minimum recommendation
accuracy price. UC-FedRec allows users to self-define their privacy preferences
to meet various demands and makes recommendations with user consent.
Experiments conducted on different real-world datasets demonstrate that our
framework is more efficient and flexible compared to baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16211">An Explainable AI Approach to Large Language Model Assisted Causal Model Auditing and Development. (arXiv:2312.16211v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yanming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fitzgibbon_B/0/1/0/all/0/1">Brette Fitzgibbon</a>, <a href="http://arxiv.org/find/cs/1/au:+Garofolo_D/0/1/0/all/0/1">Dino Garofolo</a>, <a href="http://arxiv.org/find/cs/1/au:+Kota_A/0/1/0/all/0/1">Akshith Kota</a>, <a href="http://arxiv.org/find/cs/1/au:+Papenhausen_E/0/1/0/all/0/1">Eric Papenhausen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mueller_K/0/1/0/all/0/1">Klaus Mueller</a></p>
<p>Causal networks are widely used in many fields, including epidemiology,
social science, medicine, and engineering, to model the complex relationships
between variables. While it can be convenient to algorithmically infer these
models directly from observational data, the resulting networks are often
plagued with erroneous edges. Auditing and correcting these networks may
require domain expertise frequently unavailable to the analyst. We propose the
use of large language models such as ChatGPT as an auditor for causal networks.
Our method presents ChatGPT with a causal network, one edge at a time, to
produce insights about edge directionality, possible confounders, and mediating
variables. We ask ChatGPT to reflect on various aspects of each causal link and
we then produce visualizations that summarize these viewpoints for the human
analyst to direct the edge, gather more data, or test further hypotheses. We
envision a system where large language models, automated causal inference, and
the human analyst and domain expert work hand in hand as a team to derive
holistic and comprehensive causal models for any given case scenario. This
paper presents first results obtained with an emerging prototype.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16223">Increasing Profitability and Confidence by using Interpretable Model for Investment Decisions. (arXiv:2312.16223v1 [q-fin.ST])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Arshad_S/0/1/0/all/0/1">Sahar Arshad</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Latif_S/0/1/0/all/0/1">Seemab Latif</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Salman_A/0/1/0/all/0/1">Ahmad Salman</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Irfan_S/0/1/0/all/0/1">Saadia Irfan</a></p>
<p>Financial forecasting plays an important role in making informed decisions
for financial stakeholders, specifically in the stock exchange market. In a
traditional setting, investors commonly rely on the equity research department
for valuable reports on market insights and investment recommendations. The
equity research department, however, faces challenges in effectuating
decision-making due to the demanding cognitive effort required for analyzing
the inherently volatile nature of market dynamics. Furthermore, financial
forecasting systems employed by analysts pose potential risks in terms of
interpretability and gaining the trust of all stakeholders. This paper presents
an interpretable decision-making model leveraging the SHAP-based explainability
technique to forecast investment recommendations. The proposed solution not
only provides valuable insights into the factors that influence forecasted
recommendations but also caters to investors of varying types, including those
interested in daily and short-term investment opportunities. To ascertain the
efficacy of the proposed model, a case study is devised that demonstrates a
notable enhancement in investor's portfolio value, employing our trading
strategies. The results highlight the significance of incorporating
interpretability in forecasting models to boost stakeholders' confidence and
foster transparency in the stock exchange domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16228">Deformable Audio Transformer for Audio Event Detection. (arXiv:2312.16228v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1">Wentao Zhu</a></p>
<p>Transformers have achieved promising results on a variety of tasks. However,
the quadratic complexity in self-attention computation has limited the
applications, especially in low-resource settings and mobile or edge devices.
Existing works have proposed to exploit hand-crafted attention patterns to
reduce computation complexity. However, such hand-crafted patterns are
data-agnostic and may not be optimal. Hence, it is likely that relevant keys or
values are being reduced, while less important ones are still preserved. Based
on this key insight, we propose a novel deformable audio Transformer for audio
recognition, named DATAR, where a deformable attention equipping with a pyramid
transformer backbone is constructed and learnable. Such an architecture has
been proven effective in prediction tasks,~\textit{e.g.}, event classification.
Moreover, we identify that the deformable attention map computation may
over-simplify the input feature, which can be further enhanced. Hence, we
introduce a learnable input adaptor to alleviate this issue, and DATAR achieves
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16242">Revisiting Knowledge Distillation under Distribution Shift. (arXiv:2312.16242v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1">Ziyu Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xiaofeng Chen</a></p>
<p>Knowledge distillation transfers knowledge from large models into small
models, and has recently made remarkable achievements. However, few studies has
investigated the mechanism of knowledge distillation against distribution
shift. Distribution shift refers to the data distribution drifts between
training and testing phases. In this paper, we reconsider the paradigm of
knowledge distillation by reformulating the objective function in shift
situations. Under the real scenarios, we propose a unified and systematic
framework to benchmark knowledge distillation against two general
distributional shifts including diversity and correlation shift. The evaluation
benchmark covers more than 30 methods from algorithmic, data-driven, and
optimization perspectives for five benchmark datasets. Overall, we conduct
extensive experiments on the student model. We reveal intriguing observations
of poor teaching performance under distribution shifts; in particular, complex
algorithms and data augmentation offer limited gains in many cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16243">Are All Unseen Data Out-of-Distribution?. (arXiv:2312.16243v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yuxiao Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qizhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chi_H/0/1/0/all/0/1">Haoang Chi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">Weikai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Bo Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jinyan Li</a></p>
<p>Distributions of unseen data have been all treated as out-of-distribution
(OOD), making their generalization a significant challenge. Much evidence
suggests that the size increase of training data can monotonically decrease
generalization errors in test data. However, this is not true from other
observations and analysis. In particular, when the training data have multiple
source domains and the test data contain distribution drifts, then not all
generalization errors on the test data decrease monotonically with the
increasing size of training data. Such a non-decreasing phenomenon is formally
investigated under a linear setting with empirical verification across varying
visual benchmarks. Motivated by these results, we redefine the OOD data as a
type of data outside the convex hull of the training domains and prove a new
generalization bound based on this new definition. It implies that the
effectiveness of a well-trained model can be guaranteed for the unseen data
that is within the convex hull of the training domains. But, for some data
beyond the convex hull, a non-decreasing error trend can happen. Therefore, we
investigate the performance of popular strategies such as data augmentation and
pre-training to overcome this issue. Moreover, we propose a novel reinforcement
learning selection algorithm in the source domains only that can deliver
superior performance over the baseline methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16248">XuanCe: A Comprehensive and Unified Deep Reinforcement Learning Library. (arXiv:2312.16248v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenzhang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1">Wenzhe Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1">Kun Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_G/0/1/0/all/0/1">Guangran Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuanda Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiawei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jingyu Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1">Lele Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1">Chaoxu Mu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1">Changyin Sun</a></p>
<p>In this paper, we present XuanCe, a comprehensive and unified deep
reinforcement learning (DRL) library designed to be compatible with PyTorch,
TensorFlow, and MindSpore. XuanCe offers a wide range of functionalities,
including over 40 classical DRL and multi-agent DRL algorithms, with the
flexibility to easily incorporate new algorithms and environments. It is a
versatile DRL library that supports CPU, GPU, and Ascend, and can be executed
on various operating systems such as Ubuntu, Windows, MacOS, and EulerOS.
Extensive benchmarks conducted on popular environments including MuJoCo, Atari,
and StarCraftII multi-agent challenge demonstrate the library's impressive
performance. XuanCe is open-source and can be accessed at
https://github.com/agi-brain/xuance.git.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16257">More than Correlation: Do Large Language Models Learn Causal Representations of Space?. (arXiv:2312.16257v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yida Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yixian Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sijia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1">Li Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xiaohan Zhao</a></p>
<p>Recent work found high mutual information between the learned representations
of large language models (LLMs) and the geospatial property of its input,
hinting an emergent internal model of space. However, whether this internal
space model has any causal effects on the LLMs' behaviors was not answered by
that work, led to criticism of these findings as mere statistical correlation.
Our study focused on uncovering the causality of the spatial representations in
LLMs. In particular, we discovered the potential spatial representations in
DeBERTa, GPT-Neo using representational similarity analysis and linear and
non-linear probing. Our casual intervention experiments showed that the spatial
representations influenced the model's performance on next word prediction and
a downstream task that relies on geospatial information. Our experiments
suggested that the LLMs learn and use an internal model of space in solving
geospatial related tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16261">AdapterDistillation: Non-Destructive Task Composition with Knowledge Distillation. (arXiv:2312.16261v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yicheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wangshu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Sen Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1">Teng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Jing Zheng</a></p>
<p>Leveraging knowledge from multiple tasks through introducing a small number
of task specific parameters into each transformer layer, also known as
adapters, receives much attention recently. However, adding an extra fusion
layer to implement knowledge composition not only increases the inference time
but also is non-scalable for some applications. To avoid these issues, we
propose a two-stage knowledge distillation algorithm called
AdapterDistillation. In the first stage, we extract task specific knowledge by
using local data to train a student adapter. In the second stage, we distill
the knowledge from the existing teacher adapters into the student adapter to
help its inference. Extensive experiments on frequently asked question
retrieval in task-oriented dialog systems validate the efficiency of
AdapterDistillation. We show that AdapterDistillation outperforms existing
algorithms in terms of accuracy, resource consumption and inference time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16267">Maximizing the Success Probability of Policy Allocations in Online Systems. (arXiv:2312.16267v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Betlei_A/0/1/0/all/0/1">Artem Betlei</a>, <a href="http://arxiv.org/find/cs/1/au:+Vladimirova_M/0/1/0/all/0/1">Mariia Vladimirova</a>, <a href="http://arxiv.org/find/cs/1/au:+Sebbar_M/0/1/0/all/0/1">Mehdi Sebbar</a>, <a href="http://arxiv.org/find/cs/1/au:+Urien_N/0/1/0/all/0/1">Nicolas Urien</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahier_T/0/1/0/all/0/1">Thibaud Rahier</a>, <a href="http://arxiv.org/find/cs/1/au:+Heymann_B/0/1/0/all/0/1">Benjamin Heymann</a></p>
<p>The effectiveness of advertising in e-commerce largely depends on the ability
of merchants to bid on and win impressions for their targeted users. The
bidding procedure is highly complex due to various factors such as market
competition, user behavior, and the diverse objectives of advertisers. In this
paper we consider the problem at the level of user timelines instead of
individual bid requests, manipulating full policies (i.e. pre-defined bidding
strategies) and not bid values. In order to optimally allocate policies to
users, typical multiple treatments allocation methods solve knapsack-like
problems which aim at maximizing an expected value under constraints. In the
industrial contexts such as online advertising, we argue that optimizing for
the probability of success is a more suited objective than expected value
maximization, and we introduce the SuccessProbaMax algorithm that aims at
finding the policy allocation which is the most likely to outperform a fixed
reference policy. Finally, we conduct comprehensive experiments both on
synthetic and real-world data to evaluate its performance. The results
demonstrate that our proposed algorithm outperforms conventional expected-value
maximization algorithms in terms of success rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16291">Observable Propagation: A Data-Efficient Approach to Uncover Feature Vectors in Transformers. (arXiv:2312.16291v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dunefsky_J/0/1/0/all/0/1">Jacob Dunefsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Cohan_A/0/1/0/all/0/1">Arman Cohan</a></p>
<p>A key goal of current mechanistic interpretability research in NLP is to find
linear features (also called "feature vectors") for transformers: directions in
activation space corresponding to concepts that are used by a given model in
its computation. Present state-of-the-art methods for finding linear features
require large amounts of labelled data -- both laborious to acquire and
computationally expensive to utilize. In this work, we introduce a novel
method, called "observable propagation" (in short: ObsProp), for finding linear
features used by transformer language models in computing a given task -- using
almost no data. Our paradigm centers on the concept of observables, linear
functionals corresponding to given tasks. We then introduce a mathematical
theory for the analysis of feature vectors: we provide theoretical motivation
for why LayerNorm nonlinearities do not affect the direction of feature
vectors; we also introduce a similarity metric between feature vectors called
the coupling coefficient which estimates the degree to which one feature's
output correlates with another's. We use ObsProp to perform extensive
qualitative investigations into several tasks, including gendered occupational
bias, political party prediction, and programming language detection. Our
results suggest that ObsProp surpasses traditional approaches for finding
feature vectors in the low-data regime, and that ObsProp can be used to better
understand the mechanisms responsible for bias in large language models. Code
for experiments can be found at github.com/jacobdunefsky/ObservablePropagation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16307">Incentive-Aware Synthetic Control: Accurate Counterfactual Estimation via Incentivized Exploration. (arXiv:2312.16307v1 [econ.EM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Ngo_D/0/1/0/all/0/1">Daniel Ngo</a>, <a href="http://arxiv.org/find/econ/1/au:+Harris_K/0/1/0/all/0/1">Keegan Harris</a>, <a href="http://arxiv.org/find/econ/1/au:+Agarwal_A/0/1/0/all/0/1">Anish Agarwal</a>, <a href="http://arxiv.org/find/econ/1/au:+Syrgkanis_V/0/1/0/all/0/1">Vasilis Syrgkanis</a>, <a href="http://arxiv.org/find/econ/1/au:+Wu_Z/0/1/0/all/0/1">Zhiwei Steven Wu</a></p>
<p>We consider a panel data setting in which one observes measurements of units
over time, under different interventions. Our focus is on the canonical family
of synthetic control methods (SCMs) which, after a pre-intervention time period
when all units are under control, estimate counterfactual outcomes for test
units in the post-intervention time period under control by using data from
donor units who have remained under control for the entire post-intervention
period. In order for the counterfactual estimate produced by synthetic control
for a test unit to be accurate, there must be sufficient overlap between the
outcomes of the donor units and the outcomes of the test unit. As a result, a
canonical assumption in the literature on SCMs is that the outcomes for the
test units lie within either the convex hull or the linear span of the outcomes
for the donor units. However despite their ubiquity, such overlap assumptions
may not always hold, as is the case when, e.g., units select their own
interventions and different subpopulations of units prefer different
interventions a priori.
</p>
<p>We shed light on this typically overlooked assumption, and we address this
issue by incentivizing units with different preferences to take interventions
they would not normally consider. Specifically, we provide a SCM for
incentivizing exploration in panel data settings which provides
incentive-compatible intervention recommendations to units by leveraging tools
from information design and online learning. Using our algorithm, we show how
to obtain valid counterfactual estimates using SCMs without the need for an
explicit overlap assumption on the unit outcomes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16313">Unraveling the Key Components of OOD Generalization via Diversification. (arXiv:2312.16313v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Benoit_H/0/1/0/all/0/1">Harold Benoit</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1">Liangze Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Atanov_A/0/1/0/all/0/1">Andrei Atanov</a>, <a href="http://arxiv.org/find/cs/1/au:+Kar_O/0/1/0/all/0/1">O&#x11f;uzhan Fatih Kar</a>, <a href="http://arxiv.org/find/cs/1/au:+Rigotti_M/0/1/0/all/0/1">Mattia Rigotti</a>, <a href="http://arxiv.org/find/cs/1/au:+Zamir_A/0/1/0/all/0/1">Amir Zamir</a></p>
<p>Real-world datasets may contain multiple features that explain the training
data equally well, i.e., learning any of them would lead to correct predictions
on the training data. However, many of them can be spurious, i.e., lose their
predictive power under a distribution shift and fail to generalize to
out-of-distribution (OOD) data. Recently developed ``diversification'' methods
approach this problem by finding multiple diverse hypotheses that rely on
different features. This paper aims to study this class of methods and identify
the key components contributing to their OOD generalization abilities.
</p>
<p>We show that (1) diversification methods are highly sensitive to the
distribution of the unlabeled data used for diversification and can
underperform significantly when away from a method-specific sweet spot. (2)
Diversification alone is insufficient for OOD generalization. The choice of the
used learning algorithm, e.g., the model's architecture and pretraining, is
crucial, and using the second-best choice leads to an up to 20% absolute drop
in accuracy.(3) The optimal choice of learning algorithm depends on the
unlabeled data, and vice versa.Finally, we show that the above pitfalls cannot
be alleviated by increasing the number of diverse hypotheses, allegedly the
major feature of diversification methods.
</p>
<p>These findings provide a clearer understanding of the critical design factors
influencing the OOD generalization of diversification methods. They can guide
practitioners in how to use the existing methods best and guide researchers in
developing new, better ones.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16335">LeanVec: Search your vectors faster by making them fit. (arXiv:2312.16335v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tepper_M/0/1/0/all/0/1">Mariano Tepper</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhati_I/0/1/0/all/0/1">Ishwar Singh Bhati</a>, <a href="http://arxiv.org/find/cs/1/au:+Aguerrebere_C/0/1/0/all/0/1">Cecilia Aguerrebere</a>, <a href="http://arxiv.org/find/cs/1/au:+Hildebrand_M/0/1/0/all/0/1">Mark Hildebrand</a>, <a href="http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1">Ted Willke</a></p>
<p>Modern deep learning models have the ability to generate high-dimensional
vectors whose similarity reflects semantic resemblance. Thus, similarity
search, i.e., the operation of retrieving those vectors in a large collection
that are similar to a given query, has become a critical component of a wide
range of applications that demand highly accurate and timely answers. In this
setting, the high vector dimensionality puts similarity search systems under
compute and memory pressure, leading to subpar performance. Additionally,
cross-modal retrieval tasks have become increasingly common, e.g., where a user
inputs a text query to find the most relevant images for that query. However,
these queries often have different distributions than the database embeddings,
making it challenging to achieve high accuracy. In this work, we present
LeanVec, a framework that combines linear dimensionality reduction with vector
quantization to accelerate similarity search on high-dimensional vectors while
maintaining accuracy. We present LeanVec variants for in-distribution (ID) and
out-of-distribution (OOD) queries. LeanVec-ID yields accuracies on par with
those from recently introduced deep learning alternatives whose computational
overhead precludes their usage in practice. LeanVec-OOD uses a novel technique
for dimensionality reduction that considers the query and database
distributions to simultaneously boost the accuracy and the performance of the
framework even further (even presenting competitive results when the query and
database distributions match). All in all, our extensive and varied
experimental results show that LeanVec produces state-of-the-art results, with
up to 3.7x improvement in search throughput and up to 4.9x faster index build
time over the state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16336">Learning temporal formulas from examples is hard. (arXiv:2312.16336v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mascle_C/0/1/0/all/0/1">Corto Mascle</a>, <a href="http://arxiv.org/find/cs/1/au:+Fijalkow_N/0/1/0/all/0/1">Nathana&#xeb;l Fijalkow</a>, <a href="http://arxiv.org/find/cs/1/au:+Lagarde_G/0/1/0/all/0/1">Guillaume Lagarde</a></p>
<p>We study the problem of learning linear temporal logic (LTL) formulas from
examples, as a first step towards expressing a property separating positive and
negative instances in a way that is comprehensible for humans. In this paper we
initiate the study of the computational complexity of the problem. Our main
results are hardness results: we show that the LTL learning problem is
NP-complete, both for the full logic and for almost all of its fragments. This
motivates the search for efficient heuristics, and highlights the complexity of
expressing separating properties in concise natural language.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16339">Universal Pyramid Adversarial Training for Improved ViT Performance. (arXiv:2312.16339v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chiang_P/0/1/0/all/0/1">Ping-yeh Chiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yipin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Poursaeed_O/0/1/0/all/0/1">Omid Poursaeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Shukla_S/0/1/0/all/0/1">Satya Narayan Shukla</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1">Ashish Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1">Tom Goldstein</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Ser-Nam Lim</a></p>
<p>Recently, Pyramid Adversarial training (Herrmann et al., 2022) has been shown
to be very effective for improving clean accuracy and distribution-shift
robustness of vision transformers. However, due to the iterative nature of
adversarial training, the technique is up to 7 times more expensive than
standard training. To make the method more efficient, we propose Universal
Pyramid Adversarial training, where we learn a single pyramid adversarial
pattern shared across the whole dataset instead of the sample-wise patterns.
With our proposed technique, we decrease the computational cost of Pyramid
Adversarial training by up to 70% while retaining the majority of its benefit
on clean performance and distribution-shift robustness. In addition, to the
best of our knowledge, we are also the first to find that universal adversarial
training can be leveraged to improve clean model performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16340">Alternate Training of Shared and Task-Specific Parameters for Multi-Task Neural Networks. (arXiv:2312.16340v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bellavia_S/0/1/0/all/0/1">Stefania Bellavia</a>, <a href="http://arxiv.org/find/cs/1/au:+Santa_F/0/1/0/all/0/1">Francesco Della Santa</a>, <a href="http://arxiv.org/find/cs/1/au:+Papini_A/0/1/0/all/0/1">Alessandra Papini</a></p>
<p>This paper introduces novel alternate training procedures for hard-parameter
sharing Multi-Task Neural Networks (MTNNs). Traditional MTNN training faces
challenges in managing conflicting loss gradients, often yielding sub-optimal
performance. The proposed alternate training method updates shared and
task-specific weights alternately, exploiting the multi-head architecture of
the model. This approach reduces computational costs, enhances training
regularization, and improves generalization. Convergence properties similar to
those of the classical stochastic gradient method are established. Empirical
experiments demonstrate delayed overfitting, improved prediction, and reduced
computational demands. In summary, our alternate training procedures offer a
promising advancement for the training of hard-parameter sharing MTNNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16341">Harnessing the Power of Federated Learning in Federated Contextual Bandits. (arXiv:2312.16341v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Shi_C/0/1/0/all/0/1">Chengshuai Shi</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhou_R/0/1/0/all/0/1">Ruida Zhou</a>, <a href="http://arxiv.org/find/stat/1/au:+Yang_K/0/1/0/all/0/1">Kun Yang</a>, <a href="http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1">Cong Shen</a></p>
<p>Federated learning (FL) has demonstrated great potential in revolutionizing
distributed machine learning, and tremendous efforts have been made to extend
it beyond the original focus on supervised learning. Among many directions,
federated contextual bandits (FCB), a pivotal integration of FL and sequential
decision-making, has garnered significant attention in recent years. Despite
substantial progress, existing FCB approaches have largely employed their
tailored FL components, often deviating from the canonical FL framework.
Consequently, even renowned algorithms like FedAvg remain under-utilized in
FCB, let alone other FL advancements. Motivated by this disconnection, this
work takes one step towards building a tighter relationship between the
canonical FL study and the investigations on FCB. In particular, a novel FCB
design, termed FedIGW, is proposed to leverage a regression-based CB algorithm,
i.e., inverse gap weighting. Compared with existing FCB approaches, the
proposed FedIGW design can better harness the entire spectrum of FL
innovations, which is concretely reflected as (1) flexible incorporation of
(both existing and forthcoming) FL protocols; (2) modularized plug-in of FL
analyses in performance guarantees; (3) seamless integration of FL appendages
(such as personalization, robustness, and privacy). We substantiate these
claims through rigorous theoretical analyses and empirical evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16352">Smuche: Scalar-Multiplicative Caching in Homomorphic Encryption. (arXiv:2312.16352v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1">Dongfang Zhao</a></p>
<p>Addressing the challenge of balancing security and efficiency when deploying
machine learning systems in untrusted environments, such as federated learning,
remains a critical concern. A promising strategy to tackle this issue involves
optimizing the performance of fully homomorphic encryption (HE). Recent
research highlights the efficacy of advanced caching techniques, such as Rache,
in significantly enhancing the performance of HE schemes without compromising
security. However, Rache is constrained by an inherent limitation: its
performance overhead is heavily influenced by the characteristics of plaintext
models, specifically exhibiting a caching time complexity of $\mathcal{O}(N)$,
where $N$ represents the number of cached pivots based on specific radixes.
This caching overhead becomes impractical for handling large-scale data. In
this study, we introduce a novel \textit{constant-time} caching technique that
is independent of any parameters. The core concept involves applying scalar
multiplication to a single cached ciphertext, followed by the introduction of a
completely new and constant-time randomness. Leveraging the inherent
characteristics of constant-time construction, we coin the term ``Smuche'' for
this innovative caching technique, which stands for Scalar-multiplicative
Caching of Homomorphic Encryption. We implemented Smuche from scratch and
conducted comparative evaluations against two baseline schemes, Rache and CKKS.
Our experimental results underscore the effectiveness of Smuche in addressing
the identified limitations and optimizing the performance of homomorphic
encryption in practical scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16362">Keeping Teams in the Game: Predicting Dropouts in Online Problem-Based Learning Competition. (arXiv:2312.16362v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Panwar_A/0/1/0/all/0/1">Aditya Panwar</a>, <a href="http://arxiv.org/find/cs/1/au:+S_A/0/1/0/all/0/1">Ashwin T S</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendran_R/0/1/0/all/0/1">Ramkumar Rajendran</a>, <a href="http://arxiv.org/find/cs/1/au:+Arya_K/0/1/0/all/0/1">Kavi Arya</a></p>
<p>Online learning and MOOCs have become increasingly popular in recent years,
and the trend will continue, given the technology boom. There is a dire need to
observe learners' behavior in these online courses, similar to what instructors
do in a face-to-face classroom. Learners' strategies and activities become
crucial to understanding their behavior. One major challenge in online courses
is predicting and preventing dropout behavior. While several studies have tried
to perform such analysis, there is still a shortage of studies that employ
different data streams to understand and predict the drop rates. Moreover,
studies rarely use a fully online team-based collaborative environment as their
context. Thus, the current study employs an online longitudinal problem-based
learning (PBL) collaborative robotics competition as the testbed. Through
methodological triangulation, the study aims to predict dropout behavior via
the contributions of Discourse discussion forum 'activities' of participating
teams, along with a self-reported Online Learning Strategies Questionnaire
(OSLQ). The study also uses Qualitative interviews to enhance the ground truth
and results. The OSLQ data is collected from more than 4000 participants.
Furthermore, the study seeks to establish the reliability of OSLQ to advance
research within online environments. Various Machine Learning algorithms are
applied to analyze the data. The findings demonstrate the reliability of OSLQ
with our substantial sample size and reveal promising results for predicting
the dropout rate in online competition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16365">Active Third-Person Imitation Learning. (arXiv:2312.16365v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Klein_T/0/1/0/all/0/1">Timo Klein</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinberger_S/0/1/0/all/0/1">Susanna Weinberger</a>, <a href="http://arxiv.org/find/cs/1/au:+Singla_A/0/1/0/all/0/1">Adish Singla</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschiatschek_S/0/1/0/all/0/1">Sebastian Tschiatschek</a></p>
<p>We consider the problem of third-person imitation learning with the
additional challenge that the learner must select the perspective from which
they observe the expert. In our setting, each perspective provides only limited
information about the expert's behavior, and the learning agent must carefully
select and combine information from different perspectives to achieve
competitive performance. This setting is inspired by real-world imitation
learning applications, e.g., in robotics, a robot might observe a human
demonstrator via camera and receive information from different perspectives
depending on the camera's position. We formalize the aforementioned active
third-person imitation learning problem, theoretically analyze its
characteristics, and propose a generative adversarial network-based active
learning approach. Empirically, we demstrate that our proposed approach can
effectively learn from expert demonstrations and explore the importance of
different architectural choices for the learner's performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16379">Photovoltaic power forecasting using quantum machine learning. (arXiv:2312.16379v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sagingalieva_A/0/1/0/all/0/1">Asel Sagingalieva</a>, <a href="http://arxiv.org/find/cs/1/au:+Komornyik_S/0/1/0/all/0/1">Stefan Komornyik</a>, <a href="http://arxiv.org/find/cs/1/au:+Senokosov_A/0/1/0/all/0/1">Arsenii Senokosov</a>, <a href="http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1">Ayush Joshi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedykh_A/0/1/0/all/0/1">Alexander Sedykh</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansell_C/0/1/0/all/0/1">Christopher Mansell</a>, <a href="http://arxiv.org/find/cs/1/au:+Tsurkan_O/0/1/0/all/0/1">Olga Tsurkan</a>, <a href="http://arxiv.org/find/cs/1/au:+Pinto_K/0/1/0/all/0/1">Karan Pinto</a>, <a href="http://arxiv.org/find/cs/1/au:+Pflitsch_M/0/1/0/all/0/1">Markus Pflitsch</a>, <a href="http://arxiv.org/find/cs/1/au:+Melnikov_A/0/1/0/all/0/1">Alexey Melnikov</a></p>
<p>Predicting solar panel power output is crucial for advancing the energy
transition but is complicated by the variable and non-linear nature of solar
energy. This is influenced by numerous meteorological factors, geographical
positioning, and photovoltaic cell properties, posing significant challenges to
forecasting accuracy and grid stability. Our study introduces a suite of
solutions centered around hybrid quantum neural networks designed to tackle
these complexities. The first proposed model, the Hybrid Quantum Long
Short-Term Memory, surpasses all tested models by over 40% lower mean absolute
and mean squared errors. The second proposed model, Hybrid Quantum
Sequence-to-Sequence neural network, once trained, predicts photovoltaic power
with 16% lower mean absolute error for arbitrary time intervals without the
need for prior meteorological data, highlighting its versatility. Moreover, our
hybrid models perform better even when trained on limited datasets, underlining
their potential utility in data-scarce scenarios. These findings represent a
stride towards resolving time series prediction challenges in energy power
forecasting through hybrid quantum models, showcasing the transformative
potential of quantum machine learning in catalyzing the renewable energy
transition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16403">Learning Time-aware Graph Structures for Spatially Correlated Time Series Forecasting. (arXiv:2312.16403v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Minbo Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jilin Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jensen_C/0/1/0/all/0/1">Christian S. Jensen</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_F/0/1/0/all/0/1">Fei Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_P/0/1/0/all/0/1">Peng Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhiqiang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianrui Li</a></p>
<p>Spatio-temporal forecasting of future values of spatially correlated time
series is important across many cyber-physical systems (CPS). Recent studies
offer evidence that the use of graph neural networks to capture latent
correlations between time series holds a potential for enhanced forecasting.
However, most existing methods rely on pre-defined or self-learning graphs,
which are either static or unintentionally dynamic, and thus cannot model the
time-varying correlations that exhibit trends and periodicities caused by the
regularity of the underlying processes in CPS. To tackle such limitation, we
propose Time-aware Graph Structure Learning (TagSL), which extracts time-aware
correlations among time series by measuring the interaction of node and time
representations in high-dimensional spaces. Notably, we introduce time
discrepancy learning that utilizes contrastive learning with distance-based
regularization terms to constrain learned spatial correlations to a trend
sequence. Additionally, we propose a periodic discriminant function to enable
the capture of periodic changes from the state of nodes. Next, we present a
Graph Convolution-based Gated Recurrent Unit (GCGRU) that jointly captures
spatial and temporal dependencies while learning time-aware and node-specific
patterns. Finally, we introduce a unified framework named Time-aware Graph
Convolutional Recurrent Network (TGCRN), combining TagSL, and GCGRU in an
encoder-decoder architecture for multi-step spatio-temporal forecasting. We
report on experiments with TGCRN and popular existing approaches on five
real-world datasets, thus providing evidence that TGCRN is capable of advancing
the state-of-the-art. We also cover a detailed ablation study and visualization
analysis, offering detailed insight into the effectiveness of time-aware
structure learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16409">Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning. (arXiv:2312.16409v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1">Pengfei Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1">Qinghua Hu</a></p>
<p>Continual learning (CL) has shown promising results and comparable
performance to learning at once in a fully supervised manner. However, CL
strategies typically require a large number of labeled samples, making their
real-life deployment challenging. In this work, we focus on semi-supervised
continual learning (SSCL), where the model progressively learns from partially
labeled data with unknown categories. We provide a comprehensive analysis of
SSCL and demonstrate that unreliable distributions of unlabeled data lead to
unstable training and refinement of the progressing stages. This problem
severely impacts the performance of SSCL. To address the limitations, we
propose a novel approach called Dynamic Sub-Graph Distillation (DSGD) for
semi-supervised continual learning, which leverages both semantic and
structural information to achieve more stable knowledge distillation on
unlabeled data and exhibit robustness against distribution bias. Firstly, we
formalize a general model of structural distillation and design a dynamic graph
construction for the continual learning progress. Next, we define a structure
distillation vector and design a dynamic sub-graph distillation algorithm,
which enables end-to-end training and adaptability to scale up tasks. The
entire proposed method is adaptable to various CL methods and supervision
settings. Finally, experiments conducted on three datasets CIFAR10, CIFAR100,
and ImageNet-100, with varying supervision ratios, demonstrate the
effectiveness of our proposed approach in mitigating the catastrophic
forgetting problem in semi-supervised continual learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16414">Bellman Optimal Step-size Straightening of Flow-Matching Models. (arXiv:2312.16414v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1">Bao Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1">Binh Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1">Viet Anh Nguyen</a></p>
<p>Flow matching is a powerful framework for generating high-quality samples in
various applications, especially image synthesis. However, the intensive
computational demands of these models, especially during the fine-tuning
process and sampling processes, pose significant challenges for low-resource
scenarios. This paper introduces Bellman Optimal Step-size Straightening (BOSS)
technique for distilling flow-matching generative models: it aims specifically
for a few step efficient image sampling while adhering to a computational
budget constraint. First, this technique involves a dynamic programming
algorithm that optimizes the step sizes of the pretrained network. Then, it
refines the velocity network to match the optimal step sizes, aiming to
straighten the generation paths. Extensive experimental evaluations across
image generation tasks demonstrate the efficacy of BOSS in terms of both
resource utilization and image quality. Our results reveal that BOSS achieves
substantial gains in efficiency while maintaining competitive sample quality,
effectively bridging the gap between low-resource constraints and the demanding
requirements of flow-matching generative models. Our paper also fortifies the
responsible development of artificial intelligence, offering a more sustainable
generative model that reduces computational costs and environmental footprints.
Our code can be found at https://anonymous.4open.science/r/DRL-8E88.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16418">Refining Latent Homophilic Structures over Heterophilic Graphs for Robust Graph Convolution Networks. (arXiv:2312.16418v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_C/0/1/0/all/0/1">Chenyang Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1">Guoshun Nan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiong_T/0/1/0/all/0/1">Tianyu Xiong</a>, <a href="http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1">Wendi Deng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Di Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1">Zhiyang Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lijuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qimei Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1">Xiaofeng Tao</a></p>
<p>Graph convolution networks (GCNs) are extensively utilized in various graph
tasks to mine knowledge from spatial data. Our study marks the pioneering
attempt to quantitatively investigate the GCN robustness over omnipresent
heterophilic graphs for node classification. We uncover that the predominant
vulnerability is caused by the structural out-of-distribution (OOD) issue. This
finding motivates us to present a novel method that aims to harden GCNs by
automatically learning Latent Homophilic Structures over heterophilic graphs.
We term such a methodology as LHS. To elaborate, our initial step involves
learning a latent structure by employing a novel self-expressive technique
based on multi-node interactions. Subsequently, the structure is refined using
a pairwisely constrained dual-view contrastive learning approach. We
iteratively perform the above procedure, enabling a GCN model to aggregate
information in a homophilic way on heterophilic graphs. Armed with such an
adaptable structure, we can properly mitigate the structural OOD threats over
heterophilic graphs. Experiments on various benchmarks show the effectiveness
of the proposed LHS approach for robust GCNs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16424">Soft Contrastive Learning for Time Series. (arXiv:2312.16424v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seunghan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1">Taeyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kibok Lee</a></p>
<p>Contrastive learning has shown to be effective to learn representations from
time series in a self-supervised way. However, contrasting similar time series
instances or values from adjacent timestamps within a time series leads to
ignore their inherent correlations, which results in deteriorating the quality
of learned representations. To address this issue, we propose SoftCLT, a simple
yet effective soft contrastive learning strategy for time series. This is
achieved by introducing instance-wise and temporal contrastive loss with soft
assignments ranging from zero to one. Specifically, we define soft assignments
for 1) instance-wise contrastive loss by the distance between time series on
the data space, and 2) temporal contrastive loss by the difference of
timestamps. SoftCLT is a plug-and-play method for time series contrastive
learning that improves the quality of learned representations without bells and
whistles. In experiments, we demonstrate that SoftCLT consistently improves the
performance in various downstream tasks including classification,
semi-supervised learning, transfer learning, and anomaly detection, showing
state-of-the-art performance. Code is available at this repository:
https://github.com/seunghan96/softclt.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16427">Learning to Embed Time Series Patches Independently. (arXiv:2312.16427v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seunghan Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1">Taeyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kibok Lee</a></p>
<p>Masked time series modeling has recently gained much attention as a
self-supervised representation learning strategy for time series. Inspired by
masked image modeling in computer vision, recent works first patchify and
partially mask out time series, and then train Transformers to capture the
dependencies between patches by predicting masked patches from unmasked
patches. However, we argue that capturing such patch dependencies might not be
an optimal strategy for time series representation learning; rather, learning
to embed patches independently results in better time series representations.
Specifically, we propose to use 1) the simple patch reconstruction task, which
autoencode each patch without looking at other patches, and 2) the simple
patch-wise MLP that embeds each patch independently. In addition, we introduce
complementary contrastive learning to hierarchically capture adjacent time
series information efficiently. Our proposed method improves time series
forecasting and classification performance compared to state-of-the-art
Transformer-based models, while it is more efficient in terms of the number of
parameters and training/inference time. Code is available at this repository:
https://github.com/seunghan96/pits.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16429">GAD-PVI: A General Accelerated Dynamic-Weight Particle-Based Variational Inference Framework. (arXiv:2312.16429v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fangyikang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Huminhao Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Hanbin Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1">Hui Qian</a></p>
<p>Particle-based Variational Inference (ParVI) methods approximate the target
distribution by iteratively evolving finite weighted particle systems. Recent
advances of ParVI methods reveal the benefits of accelerated position update
strategies and dynamic weight adjustment approaches. In this paper, we propose
the first ParVI framework that possesses both accelerated position update and
dynamical weight adjustment simultaneously, named the General Accelerated
Dynamic-Weight Particle-based Variational Inference (GAD-PVI) framework.
Generally, GAD-PVI simulates the semi-Hamiltonian gradient flow on a novel
Information-Fisher-Rao space, which yields an additional decrease on the local
functional dissipation. GAD-PVI is compatible with different dissimilarity
functionals and associated smoothing approaches under three information
metrics. Experiments on both synthetic and real-world data demonstrate the
faster convergence and reduced approximation error of GAD-PVI methods over the
state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16430">Preference as Reward, Maximum Preference Optimization with Importance Sampling. (arXiv:2312.16430v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1">Zaifan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_C/0/1/0/all/0/1">Chao Wei</a></p>
<p>Preference learning is a key technology for aligning language models with
human values. Reinforcement Learning from Human Feedback (RLHF) is a model
based algorithm to optimize preference learning, which first fitting a reward
model for preference score, and then optimizing generating policy with
on-policy PPO algorithm to maximize the reward. The processing of RLHF is
complex, time-consuming and unstable. Direct Preference Optimization (DPO)
algorithm using off-policy algorithm to direct optimize generating policy and
eliminating the need for reward model, which is data efficient and stable. DPO
use Bradley-Terry model and log-loss which leads to over-fitting to the
preference data at the expense of ignoring KL-regularization term when
preference near deterministic. IPO uses a root-finding pairwise MSE loss to
solve the ignoring KL-regularization problem, and learning an optimal policy.
But IPO's pairwise loss still can't s make the KL-regularization to work. In
this paper, we design a simple and intuitive off-policy preferences
optimization algorithm from an importance sampling view, and add an off-policy
KL-regularization term which makes KL-regularization truly effective. To
simplify the learning process and save memory usage, we can generate
regularization data in advance, which eliminate the needs for both reward model
and reference policy in the stage of optimization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16448">Randomized Signature Methods in Optimal Portfolio Selection. (arXiv:2312.16448v1 [q-fin.PM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-fin/1/au:+Akyildirim_E/0/1/0/all/0/1">Erdinc Akyildirim</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Gambara_M/0/1/0/all/0/1">Matteo Gambara</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Teichmann_J/0/1/0/all/0/1">Josef Teichmann</a>, <a href="http://arxiv.org/find/q-fin/1/au:+Zhou_S/0/1/0/all/0/1">Syang Zhou</a></p>
<p>We present convincing empirical results on the application of Randomized
Signature Methods for non-linear, non-parametric drift estimation for a
multi-variate financial market. Even though drift estimation is notoriously ill
defined due to small signal to noise ratio, one can still try to learn optimal
non-linear maps from data to future returns for the purposes of portfolio
optimization. Randomized Signatures, in contrast to classical signatures, allow
for high dimensional market dimension and provide features on the same scale.
We do not contribute to the theory of Randomized Signatures here, but rather
present our empirical findings on portfolio selection in real world settings
including real market data and transaction costs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16450">FCDNet: Frequency-Guided Complementary Dependency Modeling for Multivariate Time-Series Forecasting. (arXiv:2312.16450v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weijun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Heyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Ye Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_S/0/1/0/all/0/1">Shijie Guan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ning Liu</a></p>
<p>Multivariate time-series (MTS) forecasting is a challenging task in many
real-world non-stationary dynamic scenarios. In addition to intra-series
temporal signals, the inter-series dependency also plays a crucial role in
shaping future trends. How to enable the model's awareness of dependency
information has raised substantial research attention. Previous approaches have
either presupposed dependency constraints based on domain knowledge or imposed
them using real-time feature similarity. However, MTS data often exhibit both
enduring long-term static relationships and transient short-term interactions,
which mutually influence their evolving states. It is necessary to recognize
and incorporate the complementary dependencies for more accurate MTS
prediction. The frequency information in time series reflects the evolutionary
rules behind complex temporal dynamics, and different frequency components can
be used to well construct long-term and short-term interactive dependency
structures between variables. To this end, we propose FCDNet, a concise yet
effective framework for multivariate time-series forecasting. Specifically,
FCDNet overcomes the above limitations by applying two light-weight dependency
constructors to help extract long- and short-term dependency information
adaptively from multi-level frequency patterns. With the growth of input
variables, the number of trainable parameters in FCDNet only increases
linearly, which is conducive to the model's scalability and avoids
over-fitting. Additionally, adopting a frequency-based perspective can
effectively mitigate the influence of noise within MTS data, which helps
capture more genuine dependencies. The experimental results on six real-world
datasets from multiple fields show that FCDNet significantly exceeds strong
baselines, with an average improvement of 6.82% on MAE, 4.98% on RMSE, and
4.91% on MAPE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16455">Learn From Orientation Prior for Radiograph Super-Resolution: Orientation Operator Transformer. (arXiv:2312.16455v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1">Yongsong Huang</a>, <a href="http://arxiv.org/find/eess/1/au:+Miyazaki_T/0/1/0/all/0/1">Tomo Miyazaki</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1">Xiaofeng Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_K/0/1/0/all/0/1">Kaiyuan Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Tang_Z/0/1/0/all/0/1">Zhengmi Tang</a>, <a href="http://arxiv.org/find/eess/1/au:+Omachi_S/0/1/0/all/0/1">Shinichiro Omachi</a></p>
<p>Background and objective: High-resolution radiographic images play a pivotal
role in the early diagnosis and treatment of skeletal muscle-related diseases.
It is promising to enhance image quality by introducing single-image
super-resolution (SISR) model into the radiology image field. However, the
conventional image pipeline, which can learn a mixed mapping between SR and
denoising from the color space and inter-pixel patterns, poses a particular
challenge for radiographic images with limited pattern features. To address
this issue, this paper introduces a novel approach: Orientation Operator
Transformer - $O^{2}$former. Methods: We incorporate an orientation operator in
the encoder to enhance sensitivity to denoising mapping and to integrate
orientation prior. Furthermore, we propose a multi-scale feature fusion
strategy to amalgamate features captured by different receptive fields with the
directional prior, thereby providing a more effective latent representation for
the decoder. Based on these innovative components, we propose a
transformer-based SISR model, i.e., $O^{2}$former, specifically designed for
radiographic images. Results: The experimental results demonstrate that our
method achieves the best or second-best performance in the objective metrics
compared with the competitors at $\times 4$ upsampling factor. For qualitative,
more objective details are observed to be recovered. Conclusions: In this
study, we propose a novel framework called $O^{2}$former for radiological image
super-resolution tasks, which improves the reconstruction model's performance
by introducing an orientation operator and multi-scale feature fusion strategy.
Our approach is promising to further promote the radiographic image enhancement
field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16456">Adaptive trajectory-constrained exploration strategy for deep reinforcement learning. (arXiv:2312.16456v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1">Guojian Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Faguo Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_N/0/1/0/all/0/1">Ning Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhiming Zheng</a></p>
<p>Deep reinforcement learning (DRL) faces significant challenges in addressing
the hard-exploration problems in tasks with sparse or deceptive rewards and
large state spaces. These challenges severely limit the practical application
of DRL. Most previous exploration methods relied on complex architectures to
estimate state novelty or introduced sensitive hyperparameters, resulting in
instability. To mitigate these issues, we propose an efficient adaptive
trajectory-constrained exploration strategy for DRL. The proposed method guides
the policy of the agent away from suboptimal solutions by leveraging incomplete
offline demonstrations as references. This approach gradually expands the
exploration scope of the agent and strives for optimality in a constrained
optimization manner. Additionally, we introduce a novel policy-gradient-based
optimization algorithm that utilizes adaptively clipped trajectory-distance
rewards for both single- and multi-agent reinforcement learning. We provide a
theoretical analysis of our method, including a deduction of the worst-case
approximation error bounds, highlighting the validity of our approach for
enhancing exploration. To evaluate the effectiveness of the proposed method, we
conducted experiments on two large 2D grid world mazes and several MuJoCo
tasks. The extensive experimental results demonstrate the significant
advantages of our method in achieving temporally extended exploration and
avoiding myopic and suboptimal behaviors in both single- and multi-agent
settings. Notably, the specific metrics and quantifiable results further
support these findings. The code used in the study is available at
\url{https://github.com/buaawgj/TACE}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16467">Transfer and Alignment Network for Generalized Category Discovery. (arXiv:2312.16467v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1">Wenbin An</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_F/0/1/0/all/0/1">Feng Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1">Wenkai Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yaqiang Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1">Qianying Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1">Ping Chen</a></p>
<p>Generalized Category Discovery is a crucial real-world task. Despite the
improved performance on known categories, current methods perform poorly on
novel categories. We attribute the poor performance to two reasons: biased
knowledge transfer between labeled and unlabeled data and noisy representation
learning on the unlabeled data. To mitigate these two issues, we propose a
Transfer and Alignment Network (TAN), which incorporates two knowledge transfer
mechanisms to calibrate the biased knowledge and two feature alignment
mechanisms to learn discriminative features. Specifically, we model different
categories with prototypes and transfer the prototypes in labeled data to
correct model bias towards known categories. On the one hand, we pull instances
with known categories in unlabeled data closer to these prototypes to form more
compact clusters and avoid boundary overlap between known and novel categories.
On the other hand, we use these prototypes to calibrate noisy prototypes
estimated from unlabeled data based on category similarities, which allows for
more accurate estimation of prototypes for novel categories that can be used as
reliable learning targets later. After knowledge transfer, we further propose
two feature alignment mechanisms to acquire both instance- and category-level
knowledge from unlabeled data by aligning instance features with both augmented
features and the calibrated prototypes, which can boost model performance on
both known and novel categories with less noise. Experiments on three benchmark
datasets show that our model outperforms SOTA methods, especially on novel
categories. Theoretical analysis is provided for an in-depth understanding of
our model in general. Our code and data are available at
https://github.com/Lackel/TAN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16471">A Survey on Super Resolution for video Enhancement Using GAN. (arXiv:2312.16471v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Maity_A/0/1/0/all/0/1">Ankush Maity</a>, <a href="http://arxiv.org/find/eess/1/au:+Pious_R/0/1/0/all/0/1">Roshan Pious</a>, <a href="http://arxiv.org/find/eess/1/au:+Lenka_S/0/1/0/all/0/1">Sourabh Kumar Lenka</a>, <a href="http://arxiv.org/find/eess/1/au:+Choudhary_V/0/1/0/all/0/1">Vishal Choudhary</a>, <a href="http://arxiv.org/find/eess/1/au:+Lokande_P/0/1/0/all/0/1">Prof.Sharyau Lokande</a></p>
<p>This compilation of various research paper highlights provides a
comprehensive overview of recent developments in super-resolution image and
video using deep learning algorithms such as Generative Adversarial Networks.
The studies covered in these summaries provide fresh techniques to addressing
the issues of improving image and video quality, such as recursive learning for
video super-resolution, novel loss functions, frame-rate enhancement, and
attention model integration. These approaches are frequently evaluated using
criteria such as PSNR, SSIM, and perceptual indices. These advancements, which
aim to increase the visual clarity and quality of low-resolution video, have
tremendous potential in a variety of sectors ranging from surveillance
technology to medical imaging. In addition, this collection delves into the
wider field of Generative Adversarial Networks, exploring their principles,
training approaches, and applications across a broad range of domains, while
also emphasizing the challenges and opportunities for future research in this
rapidly advancing and changing field of artificial intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16473">MolSets: Molecular Graph Deep Sets Learning for Mixture Property Modeling. (arXiv:2312.16473v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hengrui Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Rondinelli_J/0/1/0/all/0/1">James M. Rondinelli</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wei Chen</a></p>
<p>Recent advances in machine learning (ML) have expedited materials discovery
and design. One significant challenge faced in ML for materials is the
expansive combinatorial space of potential materials formed by diverse
constituents and their flexible configurations. This complexity is particularly
evident in molecular mixtures, a frequently explored space for materials such
as battery electrolytes. Owing to the complex structures of molecules and the
sequence-independent nature of mixtures, conventional ML methods have
difficulties in modeling such systems. Here we present MolSets, a specialized
ML model for molecular mixtures. Representing individual molecules as graphs
and their mixture as a set, MolSets leverages a graph neural network and the
deep sets architecture to extract information at the molecule level and
aggregate it at the mixture level, thus addressing local complexity while
retaining global flexibility. We demonstrate the efficacy of MolSets in
predicting the conductivity of lithium battery electrolytes and highlight its
benefits in virtual screening of the combinatorial chemical space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16475">Federated Continual Learning via Knowledge Fusion: A Survey. (arXiv:2312.16475v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1">Xin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Junbo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianrui Li</a></p>
<p>Data privacy and silos are nontrivial and greatly challenging in many
real-world applications. Federated learning is a decentralized approach to
training models across multiple local clients without the exchange of raw data
from client devices to global servers. However, existing works focus on a
static data environment and ignore continual learning from streaming data with
incremental tasks. Federated Continual Learning (FCL) is an emerging paradigm
to address model learning in both federated and continual learning
environments. The key objective of FCL is to fuse heterogeneous knowledge from
different clients and retain knowledge of previous tasks while learning on new
ones. In this work, we delineate federated learning and continual learning
first and then discuss their integration, i.e., FCL, and particular FCL via
knowledge fusion. In summary, our motivations are four-fold: we (1) raise a
fundamental problem called ''spatial-temporal catastrophic forgetting'' and
evaluate its impact on the performance using a well-known method called
federated averaging (FedAvg), (2) integrate most of the existing FCL methods
into two generic frameworks, namely synchronous FCL and asynchronous FCL, (3)
categorize a large number of methods according to the mechanism involved in
knowledge fusion, and finally (4) showcase an outlook on the future work of
FCL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16478">Noisy Correspondence Learning with Self-Reinforcing Errors Mitigation. (arXiv:2312.16478v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dang_Z/0/1/0/all/0/1">Zhuohang Dang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1">Minnan Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Chengyou Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1">Guang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_X/0/1/0/all/0/1">Xiaojun Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jingdong Wang</a></p>
<p>Cross-modal retrieval relies on well-matched large-scale datasets that are
laborious in practice. Recently, to alleviate expensive data collection,
co-occurring pairs from the Internet are automatically harvested for training.
However, it inevitably includes mismatched pairs, \ie, noisy correspondences,
undermining supervision reliability and degrading performance. Current methods
leverage deep neural networks' memorization effect to address noisy
correspondences, which overconfidently focus on \emph{similarity-guided
training with hard negatives} and suffer from self-reinforcing errors. In light
of above, we introduce a novel noisy correspondence learning framework, namely
\textbf{S}elf-\textbf{R}einforcing \textbf{E}rrors \textbf{M}itigation (SREM).
Specifically, by viewing sample matching as classification tasks within the
batch, we generate classification logits for the given sample. Instead of a
single similarity score, we refine sample filtration through energy uncertainty
and estimate model's sensitivity of selected clean samples using swapped
classification entropy, in view of the overall prediction distribution.
Additionally, we propose cross-modal biased complementary learning to leverage
negative matches overlooked in hard-negative training, further improving model
optimization stability and curbing self-reinforcing errors. Extensive
experiments on challenging benchmarks affirm the efficacy and efficiency of
SREM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16483">Expressivity and Approximation Properties of Deep Neural Networks with ReLU$^k$ Activation. (arXiv:2312.16483v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Juncai He</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_T/0/1/0/all/0/1">Tong Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jinchao Xu</a></p>
<p>In this paper, we investigate the expressivity and approximation properties
of deep neural networks employing the ReLU$^k$ activation function for $k \geq
2$. Although deep ReLU networks can approximate polynomials effectively, deep
ReLU$^k$ networks have the capability to represent higher-degree polynomials
precisely. Our initial contribution is a comprehensive, constructive proof for
polynomial representation using deep ReLU$^k$ networks. This allows us to
establish an upper bound on both the size and count of network parameters.
Consequently, we are able to demonstrate a suboptimal approximation rate for
functions from Sobolev spaces as well as for analytic functions. Additionally,
through an exploration of the representation power of deep ReLU$^k$ networks
for shallow networks, we reveal that deep ReLU$^k$ networks can approximate
functions from a range of variation spaces, extending beyond those generated
solely by the ReLU$^k$ activation function. This finding demonstrates the
adaptability of deep ReLU$^k$ networks in approximating functions within
various variation spaces.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16489">Best-of-Both-Worlds Linear Contextual Bandits. (arXiv:2312.16489v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kato_M/0/1/0/all/0/1">Masahiro Kato</a>, <a href="http://arxiv.org/find/cs/1/au:+Ito_S/0/1/0/all/0/1">Shinji Ito</a></p>
<p>This study investigates the problem of $K$-armed linear contextual bandits,
an instance of the multi-armed bandit problem, under an adversarial corruption.
At each round, a decision-maker observes an independent and identically
distributed context and then selects an arm based on the context and past
observations. After selecting an arm, the decision-maker incurs a loss
corresponding to the selected arm. The decision-maker aims to minimize the
cumulative loss over the trial. The goal of this study is to develop a strategy
that is effective in both stochastic and adversarial environments, with
theoretical guarantees. We first formulate the problem by introducing a novel
setting of bandits with adversarial corruption, referred to as the contextual
adversarial regime with a self-bounding constraint. We assume linear models for
the relationship between the loss and the context. Then, we propose a strategy
that extends the RealLinExp3 by Neu &amp; Olkhovskaya (2020) and the
Follow-The-Regularized-Leader (FTRL). The regret of our proposed algorithm is
shown to be upper-bounded by $O\left(\min\left\{\frac{(\log(T))^3}{\Delta_{*}}
+ \sqrt{\frac{C(\log(T))^3}{\Delta_{*}}},\ \
\sqrt{T}(\log(T))^2\right\}\right)$, where $T \in\mathbb{N}$ is the number of
rounds, $\Delta_{*} &gt; 0$ is the constant minimum gap between the best and
suboptimal arms for any context, and $C\in[0, T] $ is an adversarial corruption
parameter. This regret upper bound implies
$O\left(\frac{(\log(T))^3}{\Delta_{*}}\right)$ in a stochastic environment and
by $O\left( \sqrt{T}(\log(T))^2\right)$ in an adversarial environment. We refer
to our strategy as the Best-of-Both-Worlds (BoBW) RealFTRL, due to its
theoretical guarantees in both stochastic and adversarial regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16503">Attention-Enhanced Reservoir Computing. (arXiv:2312.16503v1 [cs.ET])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koster_F/0/1/0/all/0/1">Felix K&#xf6;ster</a>, <a href="http://arxiv.org/find/cs/1/au:+Kanno_K/0/1/0/all/0/1">Kazutaka Kanno</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohkubo_J/0/1/0/all/0/1">Jun Ohkubo</a>, <a href="http://arxiv.org/find/cs/1/au:+Uchida_A/0/1/0/all/0/1">Atsushi Uchida</a></p>
<p>Photonic reservoir computing has been recently utilized in time series
forecasting as the need for hardware implementations to accelerate these
predictions has increased. Forecasting chaotic time series remains a
significant challenge, an area where the conventional reservoir computing
framework encounters limitations of prediction accuracy. We introduce an
attention mechanism to the reservoir computing model in the output stage. This
attention layer is designed to prioritize distinct features and temporal
sequences, thereby substantially enhancing the forecasting accuracy. Our
results show that a photonic reservoir computer enhanced with the attention
mechanism exhibits improved forecasting capabilities for smaller reservoirs.
These advancements highlight the transformative possibilities of reservoir
computing for practical applications where accurate forecasting of chaotic time
series is crucial.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16529">Using Enriched Category Theory to Construct the Nearest Neighbour Classification Algorithm. (arXiv:2312.16529v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pugh_M/0/1/0/all/0/1">Matthew Pugh</a>, <a href="http://arxiv.org/find/cs/1/au:+Grundy_J/0/1/0/all/0/1">Jo Grundy</a>, <a href="http://arxiv.org/find/cs/1/au:+Cirstea_C/0/1/0/all/0/1">Corina Cirstea</a>, <a href="http://arxiv.org/find/cs/1/au:+Harris_N/0/1/0/all/0/1">Nick Harris</a></p>
<p>Exploring whether Enriched Category Theory could provide the foundation of an
alternative approach to Machine Learning. This paper is the first to construct
and motivate a Machine Learning algorithm solely with Enriched Category Theory.
In order to supplement evidence that Category Theory can be used to motivate
robust and explainable algorithms, it is shown that a series of reasonable
assumptions about a dataset lead to the construction of the Nearest Neighbours
Algorithm. In particular, as an extension of the original dataset using
profunctors in the category of Lawvere metric spaces. This leads to a
definition of an Enriched Nearest Neighbours Algorithm, which consequently also
produces an enriched form of the Voronoi diagram. This paper is intended to be
accessible without any knowledge of Category Theory
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16531">Fl RDT based ultimate lowering of the negative spherical perceptron capacity. (arXiv:2312.16531v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Stojnic_M/0/1/0/all/0/1">Mihailo Stojnic</a></p>
<p>We consider the classical \emph{spherical} perceptrons and study their
capacities. The famous zero-threshold case was solved in the sixties of the
last century (see, \cite{Wendel62,Winder,Cover65}) through the high-dimensional
combinatorial considerations. The general threshold, $\kappa$, case though
turned out to be much harder and stayed out of reach for the following several
decades. A substantial progress was then made in \cite{SchTir02} and
\cite{StojnicGardGen13} where the \emph{positive} threshold ($\kappa\geq 0$)
scenario was finally fully settled. While the negative counterpart ($\kappa\leq
0$) remained out of reach, \cite{StojnicGardGen13} did show that the random
duality theory (RDT) is still powerful enough to provide excellent upper
bounds. Moreover, in \cite{StojnicGardSphNeg13}, a \emph{partially lifted} RDT
variant was considered and it was shown that the upper bounds of
\cite{StojnicGardGen13} can be lowered. After recent breakthroughs in studying
bilinearly indexed (bli) random processes in
\cite{Stojnicsflgscompyx23,Stojnicnflgscompyx23}, \emph{fully lifted} random
duality theory (fl RDT) was developed in \cite{Stojnicflrdt23}. We here first
show that the \emph{negative spherical perceptrons} can be fitted into the
frame of the fl RDT and then employ the whole fl RDT machinery to characterize
the capacity. To be fully practically operational, the fl RDT requires a
substantial numerical work. We, however, uncover remarkable closed form
analytical relations among key lifting parameters. Such a discovery enables
performing the needed numerical calculations to obtain concrete capacity
values. We also observe that an excellent convergence (with the relative
improvement $\sim 0.1\%$) is achieved already on the third (second non-trivial)
level of the \emph{stationarized} full lifting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16542">FALCON: Feature-Label Constrained Graph Net Collapse for Memory Efficient GNNs. (arXiv:2312.16542v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Adnel_C/0/1/0/all/0/1">Christopher Adnel</a>, <a href="http://arxiv.org/find/cs/1/au:+Rekik_I/0/1/0/all/0/1">Islem Rekik</a></p>
<p>Graph Neural Network (GNN) ushered in a new era of machine learning with
interconnected datasets. While traditional neural networks can only be trained
on independent samples, GNN allows for the inclusion of inter-sample
interactions in the training process. This gain, however, incurs additional
memory cost, rendering most GNNs unscalable for real-world applications
involving vast and complicated networks with tens of millions of nodes (e.g.,
social circles, web graphs, and brain graphs). This means that storing the
graph in the main memory can be difficult, let alone training the GNN model
with significantly less GPU memory. While much of the recent literature has
focused on either mini-batching GNN methods or quantization, graph reduction
methods remain largely scarce. Furthermore, present graph reduction approaches
have several drawbacks. First, most graph reduction focuses only on the
inference stage (e.g., condensation and distillation) and requires full graph
GNN training, which does not reduce training memory footprint. Second, many
methods focus solely on the graph's structural aspect, ignoring the initial
population feature-label distribution, resulting in a skewed post-reduction
label distribution. Here, we propose a Feature-Label COnstrained graph Net
collapse, FALCON, to address these limitations. Our three core contributions
lie in (i) designing FALCON, a topology-aware graph reduction technique that
preserves feature-label distribution; (ii) implementation of FALCON with other
memory reduction methods (i.e., mini-batched GNN and quantization) for further
memory reduction; (iii) extensive benchmarking and ablation studies against
SOTA methods to evaluate FALCON memory reduction. Our extensive results show
that FALCON can significantly collapse various public datasets while achieving
equal prediction quality across GNN models. Code:
https://github.com/basiralab/FALCON
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16549">How Robust are LLMs to In-Context Majority Label Bias?. (arXiv:2312.16549v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_K/0/1/0/all/0/1">Karan Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1">Sumegh Roychowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1">Siva Rajesh Kasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasa_S/0/1/0/all/0/1">Santhosh Kumar Kasa</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhanushali_A/0/1/0/all/0/1">Anish Bhanushali</a>, <a href="http://arxiv.org/find/cs/1/au:+Pattisapu_N/0/1/0/all/0/1">Nikhil Pattisapu</a>, <a href="http://arxiv.org/find/cs/1/au:+Murthy_P/0/1/0/all/0/1">Prasanna Srinivasa Murthy</a></p>
<p>In the In-Context Learning (ICL) setup, various forms of label biases can
manifest. One such manifestation is majority label bias, which arises when the
distribution of labeled examples in the in-context samples is skewed towards
one or more specific classes making Large Language Models (LLMs) more prone to
predict those labels. Such discrepancies can arise from various factors,
including logistical constraints, inherent biases in data collection methods,
limited access to diverse data sources, etc. which are unavoidable in a
real-world industry setup. In this work, we study the robustness of in-context
learning in LLMs to shifts that occur due to majority label bias within the
purview of text classification tasks. Prior works have shown that in-context
learning with LLMs is susceptible to such biases. In our study, we go one level
deeper and show that the robustness boundary varies widely for different models
and tasks, with certain LLMs being highly robust (~90%) to majority label bias.
Additionally, our findings also highlight the impact of model size and the
richness of instructional prompts contributing towards model robustness. We
restrict our study to only publicly available open-source models to ensure
transparency and reproducibility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16552">AE-Flow: AutoEncoder Normalizing Flow. (arXiv:2312.16552v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mosinski_J/0/1/0/all/0/1">Jakub Mosi&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Bilinski_P/0/1/0/all/0/1">Piotr Bili&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Merritt_T/0/1/0/all/0/1">Thomas Merritt</a>, <a href="http://arxiv.org/find/cs/1/au:+Ezzerg_A/0/1/0/all/0/1">Abdelhamid Ezzerg</a>, <a href="http://arxiv.org/find/cs/1/au:+Korzekwa_D/0/1/0/all/0/1">Daniel Korzekwa</a></p>
<p>Recently normalizing flows have been gaining traction in text-to-speech (TTS)
and voice conversion (VC) due to their state-of-the-art (SOTA) performance.
Normalizing flows are unsupervised generative models. In this paper, we
introduce supervision to the training process of normalizing flows, without the
need for parallel data. We call this training paradigm AutoEncoder Normalizing
Flow (AE-Flow). It adds a reconstruction loss forcing the model to use
information from the conditioning to reconstruct an audio sample. Our goal is
to understand the impact of each component and find the right combination of
the negative log-likelihood (NLL) and the reconstruction loss in training
normalizing flows with coupling blocks. For that reason we will compare
flow-based mapping model trained with: (i) NLL loss, (ii) NLL and
reconstruction losses, as well as (iii) reconstruction loss only. Additionally,
we compare our model with SOTA VC baseline. The models are evaluated in terms
of naturalness, speaker similarity, intelligibility in many-to-many and
many-to-any VC settings. The results show that the proposed training paradigm
systematically improves speaker similarity and naturalness when compared to
regular training methods of normalizing flows. Furthermore, we show that our
method improves speaker similarity and intelligibility over the
state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16554">A Theoretical Analysis of Efficiency Constrained Utility-Privacy Bi-Objective Optimization in Federated Learning. (arXiv:2312.16554v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinyuan Zhao</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1">Yuxing Han</a> (2), <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a> (1 and 3) ((1) WeBank, China, (2) Tsinghua University, China, (3) Hong Kong University of Science and Technology, China)</p>
<p>Federated learning (FL) enables multiple clients to collaboratively learn a
shared model without sharing their individual data. Concerns about utility,
privacy, and training efficiency in FL have garnered significant research
attention. Differential privacy has emerged as a prevalent technique in FL,
safeguarding the privacy of individual user data while impacting utility and
training efficiency. Within Differential Privacy Federated Learning (DPFL),
previous studies have primarily focused on the utility-privacy trade-off,
neglecting training efficiency, which is crucial for timely completion.
Moreover, differential privacy achieves privacy by introducing controlled
randomness (noise) on selected clients in each communication round. Previous
work has mainly examined the impact of noise level ($\sigma$) and communication
rounds ($T$) on the privacy-utility dynamic, overlooking other influential
factors like the sample ratio ($q$, the proportion of selected clients). This
paper systematically formulates an efficiency-constrained utility-privacy
bi-objective optimization problem in DPFL, focusing on $\sigma$, $T$, and $q$.
We provide a comprehensive theoretical analysis, yielding analytical solutions
for the Pareto front. Extensive empirical experiments verify the validity and
efficacy of our analysis, offering valuable guidance for low-cost parameter
design in DPFL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16557">Joint empirical risk minimization for instance-dependent positive-unlabeled data. (arXiv:2312.16557v1 [stat.ML])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Rejchel_W/0/1/0/all/0/1">Wojciech Rejchel</a>, <a href="http://arxiv.org/find/stat/1/au:+Teisseyre_P/0/1/0/all/0/1">Pawe&#x142; Teisseyre</a>, <a href="http://arxiv.org/find/stat/1/au:+Mielniczuk_J/0/1/0/all/0/1">Jan Mielniczuk</a></p>
<p>Learning from positive and unlabeled data (PU learning) is actively
researched machine learning task. The goal is to train a binary classification
model based on a training dataset containing part of positives which are
labeled, and unlabeled instances. Unlabeled set includes remaining part of
positives and all negative observations. An important element in PU learning is
modeling of the labeling mechanism, i.e. labels' assignment to positive
observations. Unlike in many prior works, we consider a realistic setting for
which probability of label assignment, i.e. propensity score, is
instance-dependent. In our approach we investigate minimizer of an empirical
counterpart of a joint risk which depends on both posterior probability of
inclusion in a positive class as well as on a propensity score. The non-convex
empirical risk is alternately optimised with respect to parameters of both
functions. In the theoretical analysis we establish risk consistency of the
minimisers using recently derived methods from the theory of empirical
processes. Besides, the important development here is a proposed novel
implementation of an optimisation algorithm, for which sequential approximation
of a set of positive observations among unlabeled ones is crucial. This relies
on modified technique of 'spies' as well as on a thresholding rule based on
conditional probabilities. Experiments conducted on 20 data sets for various
labeling scenarios show that the proposed method works on par or more
effectively than state-of-the-art methods based on propensity function
estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16560">Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching. (arXiv:2312.16560v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Errica_F/0/1/0/all/0/1">Federico Errica</a>, <a href="http://arxiv.org/find/cs/1/au:+Christiansen_H/0/1/0/all/0/1">Henrik Christiansen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zaverkin_V/0/1/0/all/0/1">Viktor Zaverkin</a>, <a href="http://arxiv.org/find/cs/1/au:+Maruyama_T/0/1/0/all/0/1">Takashi Maruyama</a>, <a href="http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1">Mathias Niepert</a>, <a href="http://arxiv.org/find/cs/1/au:+Alesiani_F/0/1/0/all/0/1">Francesco Alesiani</a></p>
<p>Long-range interactions are essential for the correct description of complex
systems in many scientific fields. The price to pay for including them in the
calculations, however, is a dramatic increase in the overall computational
costs. Recently, deep graph networks have been employed as efficient,
data-driven surrogate models for predicting properties of complex systems
represented as graphs. These models rely on a local and iterative message
passing strategy that should, in principle, capture long-range information
without explicitly modeling the corresponding interactions. In practice, most
deep graph networks cannot really model long-range dependencies due to the
intrinsic limitations of (synchronous) message passing, namely oversmoothing,
oversquashing, and underreaching. This work proposes a general framework that
learns to mitigate these limitations: within a variational inference framework,
we endow message passing architectures with the ability to freely adapt their
depth and filter messages along the way. With theoretical and empirical
arguments, we show that this simple strategy better captures long-range
interactions, by surpassing the state of the art on five node and graph
prediction datasets suited for this problem. Our approach consistently improves
the performances of the baselines tested on these tasks. We complement the
exposition with qualitative analyses and ablations to get a deeper
understanding of the framework's inner workings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16563">RDGCL: Reaction-Diffusion Graph Contrastive Learning for Recommendation. (arXiv:2312.16563v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1">Jeongwhan Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Wi_H/0/1/0/all/0/1">Hyowon Wi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1">Chaejeong Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sung-Bae Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Dongha Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1">Noseong Park</a></p>
<p>Contrastive learning (CL) has emerged as a promising technique for improving
recommender systems, addressing the challenge of data sparsity by leveraging
self-supervised signals from raw data. Integration of CL with graph
convolutional network (GCN)-based collaborative filterings (CFs) has been
explored in recommender systems. However, current CL-based recommendation
models heavily rely on low-pass filters and graph augmentations. In this paper,
we propose a novel CL method for recommender systems called the
reaction-diffusion graph contrastive learning model (RDGCL). We design our own
GCN for CF based on both the diffusion, i.e., low-pass filter, and the
reaction, i.e., high-pass filter, equations. Our proposed CL-based training
occurs between reaction and diffusion-based embeddings, so there is no need for
graph augmentations. Experimental evaluation on 6 benchmark datasets
demonstrates that our proposed method outperforms state-of-the-art CL-based
recommendation models. By enhancing recommendation accuracy and diversity, our
method brings an advancement in CL for recommender systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16566">Inverse Reinforcement Learning with Unknown Reward Model based on Structural Risk Minimization. (arXiv:2312.16566v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qu_C/0/1/0/all/0/1">Chendi Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jianping He</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1">Xiaoming Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jiming Chen</a></p>
<p>Inverse reinforcement learning (IRL) usually assumes the model of the reward
function is pre-specified and estimates the parameter only. However, how to
determine a proper reward model is nontrivial. A simplistic model is less
likely to contain the real reward function, while a model with high complexity
leads to substantial computation cost and risks overfitting. This paper
addresses this trade-off in IRL model selection by introducing the structural
risk minimization (SRM) method from statistical learning. SRM selects an
optimal reward function class from a hypothesis set minimizing both estimation
error and model complexity. To formulate an SRM scheme for IRL, we estimate
policy gradient by demonstration serving as empirical risk and establish the
upper bound of Rademacher complexity of hypothesis classes as model penalty.
The learning guarantee is further presented. In particular, we provide explicit
SRM for the common linear weighted sum setting in IRL. Simulations demonstrate
the performance and efficiency of our scheme.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16581">Continuous-time Autoencoders for Regular and Irregular Time Series Imputation. (arXiv:2312.16581v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wi_H/0/1/0/all/0/1">Hyowon Wi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1">Yehjin Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1">Noseong Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1">Sungpil Woo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1">Sunhwan Lim</a></p>
<p>Time series imputation is one of the most fundamental tasks for time series.
Real-world time series datasets are frequently incomplete (or irregular with
missing observations), in which case imputation is strongly required. Many
different time series imputation methods have been proposed. Recent
self-attention-based methods show the state-of-the-art imputation performance.
However, it has been overlooked for a long time to design an imputation method
based on continuous-time recurrent neural networks (RNNs), i.e., neural
controlled differential equations (NCDEs). To this end, we redesign time series
(variational) autoencoders based on NCDEs. Our method, called continuous-time
autoencoder (CTA), encodes an input time series sample into a continuous hidden
path (rather than a hidden vector) and decodes it to reconstruct and impute the
input. In our experiments with 4 datasets and 19 baselines, our method shows
the best imputation performance in almost all cases.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16596">Enhancing Traffic Flow Prediction using Outlier-Weighted AutoEncoders: Handling Real-Time Changes. (arXiv:2312.16596v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Choudhary_H/0/1/0/all/0/1">Himanshu Choudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Hassani_M/0/1/0/all/0/1">Marwan Hassani</a></p>
<p>In today's urban landscape, traffic congestion poses a critical challenge,
especially during outlier scenarios. These outliers can indicate abrupt traffic
peaks, drops, or irregular trends, often arising from factors such as
accidents, events, or roadwork. Moreover, Given the dynamic nature of traffic,
the need for real-time traffic modeling also becomes crucial to ensure accurate
and up-to-date traffic predictions. To address these challenges, we introduce
the Outlier Weighted Autoencoder Modeling (OWAM) framework. OWAM employs
autoencoders for local outlier detection and generates correlation scores to
assess neighboring traffic's influence. These scores serve as a weighted factor
for neighboring sensors, before fusing them into the model. This information
enhances the traffic model's performance and supports effective real-time
updates, a crucial aspect for capturing dynamic traffic patterns. OWAM
demonstrates a favorable trade-off between accuracy and efficiency, rendering
it highly suitable for real-world applications. The research findings
contribute significantly to the development of more efficient and adaptive
traffic prediction models, advancing the field of transportation management for
the future. The code and datasets of our framework is publicly available under
https://github.com/himanshudce/OWAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16600">scRNA-seq Data Clustering by Cluster-aware Iterative Contrastive Learning. (arXiv:2312.16600v1 [q-bio.GN])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Jiang_W/0/1/0/all/0/1">Weikang Jiang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wang_J/0/1/0/all/0/1">Jinxian Wang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Guan_J/0/1/0/all/0/1">Jihong Guan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zhou_S/0/1/0/all/0/1">Shuigeng Zhou</a></p>
<p>Single-cell RNA sequencing (scRNA-seq) enables researchers to analyze gene
expression at single-cell level. One important task in scRNA-seq data analysis
is unsupervised clustering, which helps identify distinct cell types, laying
down the foundation for other downstream analysis tasks. In this paper, we
propose a novel method called Cluster-aware Iterative Contrastive Learning
(CICL in short) for scRNA-seq data clustering, which utilizes an iterative
representation learning and clustering framework to progressively learn the
clustering structure of scRNA-seq data with a cluster-aware contrastive loss.
CICL consists of a Transformer encoder, a clustering head, a projection head
and a contrastive loss module. First, CICL extracts the feature vectors of the
original and augmented data by the Transformer encoder. Then, it computes the
clustering centroids by K-means and employs the student t-distribution to
assign pseudo-labels to all cells in the clustering head. The projection-head
uses a Multi-Layer Perceptron (MLP) to obtain projections of the augmented
data. At last, both pseudo-labels and projections are used in the contrastive
loss to guide the model training. Such a process goes iteratively so that the
clustering result becomes better and better. Extensive experiments on 25 real
world scRNA-seq datasets show that CICL outperforms the SOTA methods.
Concretely, CICL surpasses the existing methods by from 14% to 280%, and from
5% to 133% on average in terms of performance metrics ARI and NMI respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16604">Twice Class Bias Correction for Imbalanced Semi-Supervised Learning. (arXiv:2312.16604v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_B/0/1/0/all/0/1">Bowen Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1">Lu Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhan_D/0/1/0/all/0/1">De-chuan Zhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1">Han-jia Ye</a></p>
<p>Differing from traditional semi-supervised learning, class-imbalanced
semi-supervised learning presents two distinct challenges: (1) The imbalanced
distribution of training samples leads to model bias towards certain classes,
and (2) the distribution of unlabeled samples is unknown and potentially
distinct from that of labeled samples, which further contributes to class bias
in the pseudo-labels during training. To address these dual challenges, we
introduce a novel approach called \textbf{T}wice \textbf{C}lass \textbf{B}ias
\textbf{C}orrection (\textbf{TCBC}). We begin by utilizing an estimate of the
class distribution from the participating training samples to correct the
model, enabling it to learn the posterior probabilities of samples under a
class-balanced prior. This correction serves to alleviate the inherent class
bias of the model. Building upon this foundation, we further estimate the class
bias of the current model parameters during the training process. We apply a
secondary correction to the model's pseudo-labels for unlabeled samples, aiming
to make the assignment of pseudo-labels across different classes of unlabeled
samples as equitable as possible. Through extensive experimentation on
CIFAR10/100-LT, STL10-LT, and the sizable long-tailed dataset SUN397, we
provide conclusive evidence that our proposed TCBC method reliably enhances the
performance of class-imbalanced semi-supervised learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16609">Exploiting hidden structures in non-convex games for convergence to Nash equilibrium. (arXiv:2312.16609v1 [cs.GT])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sakos_I/0/1/0/all/0/1">Iosif Sakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlatakis_Gkaragkounis_E/0/1/0/all/0/1">Emmanouil-Vasileios Vlatakis-Gkaragkounis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mertikopoulos_P/0/1/0/all/0/1">Panayotis Mertikopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Piliouras_G/0/1/0/all/0/1">Georgios Piliouras</a></p>
<p>A wide array of modern machine learning applications - from adversarial
models to multi-agent reinforcement learning - can be formulated as
non-cooperative games whose Nash equilibria represent the system's desired
operational states. Despite having a highly non-convex loss landscape, many
cases of interest possess a latent convex structure that could potentially be
leveraged to yield convergence to equilibrium. Driven by this observation, our
paper proposes a flexible first-order method that successfully exploits such
"hidden structures" and achieves convergence under minimal assumptions for the
transformation connecting the players' control variables to the game's latent,
convex-structured layer. The proposed method - which we call preconditioned
hidden gradient descent (PHGD) - hinges on a judiciously chosen gradient
preconditioning scheme related to natural gradient methods. Importantly, we
make no separability assumptions for the game's hidden structure, and we
provide explicit convergence rate guarantees for both deterministic and
stochastic environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16610">Efficient Deweather Mixture-of-Experts with Uncertainty-aware Feature-wise Linear Modulation. (arXiv:2312.16610v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Rongyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1">Yulin Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1">Huanrui Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1">Zhen Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Gudovskiy_D/0/1/0/all/0/1">Denis Gudovskiy</a>, <a href="http://arxiv.org/find/cs/1/au:+Okuno_T/0/1/0/all/0/1">Tomoyuki Okuno</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakata_Y/0/1/0/all/0/1">Yohei Nakata</a>, <a href="http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1">Kurt Keutzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1">Yuan Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shanghang Zhang</a></p>
<p>The Mixture-of-Experts (MoE) approach has demonstrated outstanding
scalability in multi-task learning including low-level upstream tasks such as
concurrent removal of multiple adverse weather effects. However, the
conventional MoE architecture with parallel Feed Forward Network (FFN) experts
leads to significant parameter and computational overheads that hinder its
efficient deployment. In addition, the naive MoE linear router is suboptimal in
assigning task-specific features to multiple experts which limits its further
scalability. In this work, we propose an efficient MoE architecture with weight
sharing across the experts. Inspired by the idea of linear feature modulation
(FM), our architecture implicitly instantiates multiple experts via learnable
activation modulations on a single shared expert block. The proposed Feature
Modulated Expert (FME) serves as a building block for the novel
Mixture-of-Feature-Modulation-Experts (MoFME) architecture, which can scale up
the number of experts with low overhead. We further propose an
Uncertainty-aware Router (UaR) to assign task-specific features to different FM
modules with well-calibrated weights. This enables MoFME to effectively learn
diverse expert functions for multiple tasks. The conducted experiments on the
multi-deweather task show that our MoFME outperforms the baselines in the image
restoration quality by 0.1-0.2 dB and achieves SOTA-compatible performance
while saving more than 72% of parameters and 39% inference time over the
conventional MoE counterpart. Experiments on the downstream segmentation and
classification tasks further demonstrate the generalizability of MoFME to real
open-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16611">Learning from small data sets: Patch-based regularizers in inverse problems for image reconstruction. (arXiv:2312.16611v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piening_M/0/1/0/all/0/1">Moritz Piening</a>, <a href="http://arxiv.org/find/cs/1/au:+Altekruger_F/0/1/0/all/0/1">Fabian Altekr&#xfc;ger</a>, <a href="http://arxiv.org/find/cs/1/au:+Hertrich_J/0/1/0/all/0/1">Johannes Hertrich</a>, <a href="http://arxiv.org/find/cs/1/au:+Hagemann_P/0/1/0/all/0/1">Paul Hagemann</a>, <a href="http://arxiv.org/find/cs/1/au:+Walther_A/0/1/0/all/0/1">Andrea Walther</a>, <a href="http://arxiv.org/find/cs/1/au:+Steidl_G/0/1/0/all/0/1">Gabriele Steidl</a></p>
<p>The solution of inverse problems is of fundamental interest in medical and
astronomical imaging, geophysics as well as engineering and life sciences.
Recent advances were made by using methods from machine learning, in particular
deep neural networks. Most of these methods require a huge amount of (paired)
data and computer capacity to train the networks, which often may not be
available. Our paper addresses the issue of learning from small data sets by
taking patches of very few images into account. We focus on the combination of
model-based and data-driven methods by approximating just the image prior, also
known as regularizer in the variational model. We review two methodically
different approaches, namely optimizing the maximum log-likelihood of the patch
distribution, and penalizing Wasserstein-like discrepancies of whole empirical
patch distributions. From the point of view of Bayesian inverse problems, we
show how we can achieve uncertainty quantification by approximating the
posterior using Langevin Monte Carlo methods. We demonstrate the power of the
methods in computed tomography, image super-resolution, and inpainting. Indeed,
the approach provides also high-quality results in zero-shot super-resolution,
where only a low-resolution image is available. The paper is accompanied by a
GitHub repository containing implementations of all methods as well as data
examples so that the reader can get their own insight into the performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16612">Exploring intra-task relations to improve meta-learning algorithms. (arXiv:2312.16612v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_P/0/1/0/all/0/1">Prabhat Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Shreya Singh</a></p>
<p>Meta-learning has emerged as an effective methodology to model several
real-world tasks and problems due to its extraordinary effectiveness in the
low-data regime. There are many scenarios ranging from the classification of
rare diseases to language modelling of uncommon languages where the
availability of large datasets is rare. Similarly, for more broader scenarios
like self-driving, an autonomous vehicle needs to be trained to handle every
situation well. This requires training the ML model on a variety of tasks with
good quality data. But often times, we find that the data distribution across
various tasks is skewed, i.e.the data follows a long-tail distribution. This
leads to the model performing well on some tasks and not performing so well on
others leading to model robustness issues. Meta-learning has recently emerged
as a potential learning paradigm which can effectively learn from one task and
generalize that learning to unseen tasks. In this study, we aim to exploit
external knowledge of task relations to improve training stability via
effective mini-batching of tasks. We hypothesize that selecting a diverse set
of tasks in a mini-batch will lead to a better estimate of the full gradient
and hence will lead to a reduction of noise in training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16613">Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions. (arXiv:2312.16613v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bovbjerg_H/0/1/0/all/0/1">Holger Severin Bovbjerg</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Jensen_J/0/1/0/all/0/1">Jesper Jensen</a> (1, 2), <a href="http://arxiv.org/find/cs/1/au:+Ostergaard_J/0/1/0/all/0/1">Jan &#xd8;stergaard</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1">Zheng-Hua Tan</a> (1, 3) ((1) Aalborg University, (2) Oticon, (3) Pioneer Centre for AI, Denmark)</p>
<p>In this paper, we propose the use of self-supervised pretraining on a large
unlabelled data set to improve the performance of a personalized voice activity
detection (VAD) model in adverse conditions. We pretrain a long short-term
memory (LSTM)-encoder using the autoregressive predictive coding (APC)
framework and fine-tune it for personalized VAD. We also propose a denoising
variant of APC, with the goal of improving the robustness of personalized VAD.
The trained models are systematically evaluated on both clean speech and speech
contaminated by various types of noise at different SNR-levels and compared to
a purely supervised model. Our experiments show that self-supervised
pretraining not only improves performance in clean conditions, but also yields
models which are more robust to adverse conditions compared to purely
supervised learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16616">Agnostically Learning Multi-index Models with Queries. (arXiv:2312.16616v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Diakonikolas_I/0/1/0/all/0/1">Ilias Diakonikolas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kane_D/0/1/0/all/0/1">Daniel M. Kane</a>, <a href="http://arxiv.org/find/cs/1/au:+Kontonis_V/0/1/0/all/0/1">Vasilis Kontonis</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1">Christos Tzamos</a>, <a href="http://arxiv.org/find/cs/1/au:+Zarifis_N/0/1/0/all/0/1">Nikos Zarifis</a></p>
<p>We study the power of query access for the task of agnostic learning under
the Gaussian distribution. In the agnostic model, no assumptions are made on
the labels and the goal is to compute a hypothesis that is competitive with the
{\em best-fit} function in a known class, i.e., it achieves error
$\mathrm{opt}+\epsilon$, where $\mathrm{opt}$ is the error of the best function
in the class. We focus on a general family of Multi-Index Models (MIMs), which
are $d$-variate functions that depend only on few relevant directions, i.e.,
have the form $g(\mathbf{W} \mathbf{x})$ for an unknown link function $g$ and a
$k \times d$ matrix $\mathbf{W}$. Multi-index models cover a wide range of
commonly studied function classes, including constant-depth neural networks
with ReLU activations, and intersections of halfspaces.
</p>
<p>Our main result shows that query access gives significant runtime
improvements over random examples for agnostically learning MIMs. Under
standard regularity assumptions for the link function (namely, bounded
variation or surface area), we give an agnostic query learner for MIMs with
complexity $O(k)^{\mathrm{poly}(1/\epsilon)} \; \mathrm{poly}(d) $. In
contrast, algorithms that rely only on random examples inherently require
$d^{\mathrm{poly}(1/\epsilon)}$ samples and runtime, even for the basic problem
of agnostically learning a single ReLU or a halfspace.
</p>
<p>Our algorithmic result establishes a strong computational separation between
the agnostic PAC and the agnostic PAC+Query models under the Gaussian
distribution. Prior to our work, no such separation was known -- even for the
special case of agnostically learning a single halfspace, for which it was an
open problem first posed by Feldman. Our results are enabled by a general
dimension-reduction technique that leverages query access to estimate gradients
of (a smoothed version of) the underlying label function.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16624">DOSA-MO: Dual-stage Optimizer for Systematic overestimation Adjustment in Multi-Objective problems improves biomarker discovery. (arXiv:2312.16624v1 [q-bio.QM])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Cattelani_L/0/1/0/all/0/1">Luca Cattelani</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Fortino_V/0/1/0/all/0/1">Vittorio Fortino</a></p>
<p>The challenge in biomarker discovery and validation using machine learning
from omics data lies in the abundance of molecular features but scarcity of
samples. Most machine learning-based feature selection methods necessitate of
hyperparameter tuning, typically performed by evaluating numerous alternatives
on a validation set. Every evaluation has a performance estimation error and
when the selection takes place between many models the best ones are almost
certainly overestimated. Biomarker identification is a typical multi-objective
problem with trade-offs between the predictive ability and the parsimony in the
number of molecular features. Genetic algorithms are a popular tool for
multi-objective optimization but they evolve numerous solutions and are prone
to overestimation. Methods have been proposed to reduce the overestimation
after a model has already been selected in single-objective problems, but to
the best of our knowledge no algorithm existed that was capable of reducing the
overestimation during the optimization, leading to a better model selection, or
that had been applied in the more general domain of multi-objective problems.
We propose DOSA-MO, a novel multi-objective optimization wrapper algorithm that
learns how the original estimation, its variance, and the feature set size of
the solutions predict the overestimation, and adjusts the expectation of the
performance during the optimization, improving the composition of the solution
set. We verify that DOSA-MO improves the performance of a state-of-the-art
genetic algorithm on left-out or external sample sets, when predicting cancer
subtypes and/or patient overall survival, using three transcriptomics datasets
for kidney and breast cancer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16626">Sorting of Smartphone Components for Recycling Through Convolutional Neural Networks. (arXiv:2312.16626v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Becker_A/0/1/0/all/0/1">&#xc1;lvaro G. Becker</a>, <a href="http://arxiv.org/find/cs/1/au:+Cenci_M/0/1/0/all/0/1">Marcelo P. Cenci</a>, <a href="http://arxiv.org/find/cs/1/au:+Silveira_T/0/1/0/all/0/1">Thiago L. T. da Silveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Veit_H/0/1/0/all/0/1">Hugo M. Veit</a></p>
<p>The recycling of waste electrical and electronic equipment is an essential
tool in allowing for a circular economy, presenting the potential for
significant environmental and economic gain. However, traditional material
separation techniques, based on physical and chemical processes, require
substantial investment and do not apply to all cases. In this work, we
investigate using an image classification neural network as a potential means
to control an automated material separation process in treating smartphone
waste, acting as a more efficient, less costly, and more widely applicable
alternative to existing tools. We produced a dataset with 1,127 images of
pyrolyzed smartphone components, which was then used to train and assess a
VGG-16 image classification model. The model achieved 83.33% accuracy, lending
credence to the viability of using such a neural network in material
separation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16627">MIM4DD: Mutual Information Maximization for Dataset Distillation. (arXiv:2312.16627v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shang_Y/0/1/0/all/0/1">Yuzhang Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1">Zhihang Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yan Yan</a></p>
<p>Dataset distillation (DD) aims to synthesize a small dataset whose test
performance is comparable to a full dataset using the same model.
State-of-the-art (SoTA) methods optimize synthetic datasets primarily by
matching heuristic indicators extracted from two networks: one from real data
and one from synthetic data (see Fig.1, Left), such as gradients and training
trajectories. DD is essentially a compression problem that emphasizes
maximizing the preservation of information contained in the data. We argue that
well-defined metrics which measure the amount of shared information between
variables in information theory are necessary for success measurement but are
never considered by previous works. Thus, we introduce mutual information (MI)
as the metric to quantify the shared information between the synthetic and the
real datasets, and devise MIM4DD numerically maximizing the MI via a newly
designed optimizable objective within a contrastive learning framework to
update the synthetic dataset. Specifically, we designate the samples in
different datasets that share the same labels as positive pairs and vice versa
negative pairs. Then we respectively pull and push those samples in positive
and negative pairs into contrastive space via minimizing NCE loss. As a result,
the targeted MI can be transformed into a lower bound represented by feature
maps of samples, which is numerically feasible. Experiment results show that
MIM4DD can be implemented as an add-on module to existing SoTA DD methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16638">Fault-Tolerant Vertical Federated Learning on Dynamic Networks. (arXiv:2312.16638v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1">Surojit Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zeyu Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1">Christopher G. Brinton</a>, <a href="http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1">David I. Inouye</a></p>
<p>Vertical Federated learning (VFL) is a class of FL where each client shares
the same sample space but only holds a subset of the features. While VFL
tackles key privacy challenges of distributed learning, it often assumes
perfect hardware and communication capabilities. This assumption hinders the
broad deployment of VFL, particularly on edge devices, which are heterogeneous
in their in-situ capabilities and will connect/disconnect from the network over
time. To address this gap, we define Internet Learning (IL) including its data
splitting and network context and which puts good performance under extreme
dynamic condition of clients as the primary goal. We propose VFL as a naive
baseline and develop several extensions to handle the IL paradigm of learning.
Furthermore, we implement new methods, propose metrics, and extensively analyze
results based on simulating a sensor network. The results show that the
developed methods are more robust to changes in the network than VFL baseline.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16699">Computational Tradeoffs of Optimization-Based Bound Tightening in ReLU Networks. (arXiv:2312.16699v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Badilla_F/0/1/0/all/0/1">Fabian Badilla</a>, <a href="http://arxiv.org/find/math/1/au:+Goycoolea_M/0/1/0/all/0/1">Marcos Goycoolea</a>, <a href="http://arxiv.org/find/math/1/au:+Munoz_G/0/1/0/all/0/1">Gonzalo Mu&#xf1;oz</a>, <a href="http://arxiv.org/find/math/1/au:+Serra_T/0/1/0/all/0/1">Thiago Serra</a></p>
<p>The use of Mixed-Integer Linear Programming (MILP) models to represent neural
networks with Rectified Linear Unit (ReLU) activations has become increasingly
widespread in the last decade. This has enabled the use of MILP technology to
test-or stress-their behavior, to adversarially improve their training, and to
embed them in optimization models leveraging their predictive power. Many of
these MILP models rely on activation bounds. That is, bounds on the input
values of each neuron. In this work, we explore the tradeoff between the
tightness of these bounds and the computational effort of solving the resulting
MILP models. We provide guidelines for implementing these models based on the
impact of network structure, regularization, and rounding.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16702">Rethinking Tabular Data Understanding with Large Language Models. (arXiv:2312.16702v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianyang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Muhao Chen</a></p>
<p>Large Language Models (LLMs) have shown to be capable of various tasks, yet
their capability in interpreting and reasoning over tabular data remains an
underexplored area. In this context, this study investigates from three core
perspectives: the robustness of LLMs to structural perturbations in tables, the
comparative analysis of textual and symbolic reasoning on tables, and the
potential of boosting model performance through the aggregation of multiple
reasoning pathways. We discover that structural variance of tables presenting
the same content reveals a notable performance decline, particularly in
symbolic reasoning tasks. This prompts the proposal of a method for table
structure normalization. Moreover, textual reasoning slightly edges out
symbolic reasoning, and a detailed error analysis reveals that each exhibits
different strengths depending on the specific tasks. Notably, the aggregation
of textual and symbolic reasoning pathways, bolstered by a mix self-consistency
mechanism, resulted in achieving SOTA performance, with an accuracy of 73.6% on
WIKITABLEQUESTIONS, representing a substantial advancement over previous
existing table processing paradigms of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16713">Knowledge Enhanced Conditional Imputation for Healthcare Time-series. (arXiv:2312.16713v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1">Linglong Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Ibrahim_Z/0/1/0/all/0/1">Zina Ibrahim</a>, <a href="http://arxiv.org/find/cs/1/au:+Ellis_H/0/1/0/all/0/1">Hugh Logan Ellis</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Ao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yuezhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1">Tao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dobson_R/0/1/0/all/0/1">Richard Dobson</a></p>
<p>This study presents a novel approach to addressing the challenge of missing
data in multivariate time series, with a particular focus on the complexities
of healthcare data. Our Conditional Self-Attention Imputation (CSAI) model,
grounded in a transformer-based framework, introduces a conditional hidden
state initialization tailored to the intricacies of medical time series data.
This methodology diverges from traditional imputation techniques by
specifically targeting the imbalance in missing data distribution, a crucial
aspect often overlooked in healthcare datasets. By integrating advanced
knowledge embedding and a non-uniform masking strategy, CSAI adeptly adjusts to
the distinct patterns of missing data in Electronic Health Records (EHRs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16715">Adversarial Attacks on LoRa Device Identification and Rogue Signal Detection with Deep Learning. (arXiv:2312.16715v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sagduyu_Y/0/1/0/all/0/1">Yalin E. Sagduyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Erpek_T/0/1/0/all/0/1">Tugba Erpek</a></p>
<p>Low-Power Wide-Area Network (LPWAN) technologies, such as LoRa, have gained
significant attention for their ability to enable long-range, low-power
communication for Internet of Things (IoT) applications. However, the security
of LoRa networks remains a major concern, particularly in scenarios where
device identification and classification of legitimate and spoofed signals are
crucial. This paper studies a deep learning framework to address these
challenges, considering LoRa device identification and legitimate vs. rogue
LoRa device classification tasks. A deep neural network (DNN), either a
convolutional neural network (CNN) or feedforward neural network (FNN), is
trained for each task by utilizing real experimental I/Q data for LoRa signals,
while rogue signals are generated by using kernel density estimation (KDE) of
received signals by rogue devices. Fast Gradient Sign Method (FGSM)-based
adversarial attacks are considered for LoRa signal classification tasks using
deep learning models. The impact of these attacks is assessed on the
performance of two tasks, namely device identification and legitimate vs. rogue
device classification, by utilizing separate or common perturbations against
these signal classification tasks. Results presented in this paper quantify the
level of transferability of adversarial attacks on different LoRa signal
classification tasks as a major vulnerability and highlight the need to make
IoT applications robust to adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16717">Landslide Detection and Segmentation Using Remote Sensing Images and Deep Neural Network. (arXiv:2312.16717v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Le_C/0/1/0/all/0/1">Cam Le</a>, <a href="http://arxiv.org/find/cs/1/au:+Pham_L/0/1/0/all/0/1">Lam Pham</a>, <a href="http://arxiv.org/find/cs/1/au:+Lampert_J/0/1/0/all/0/1">Jasmin Lampert</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlogl_M/0/1/0/all/0/1">Matthias Schl&#xf6;gl</a>, <a href="http://arxiv.org/find/cs/1/au:+Schindler_A/0/1/0/all/0/1">Alexander Schindler</a></p>
<p>Knowledge about historic landslide event occurrence is important for
supporting disaster risk reduction strategies. Building upon findings from 2022
Landslide4Sense Competition, we propose a deep neural network based system for
landslide detection and segmentation from multisource remote sensing image
input. We use a U-Net trained with Cross Entropy loss as baseline model. We
then improve the U-Net baseline model by leveraging a wide range of deep
learning techniques. In particular, we conduct feature engineering by
generating new band data from the original bands, which helps to enhance the
quality of remote sensing image input. Regarding the network architecture, we
replace traditional convolutional layers in the U-Net baseline by a
residual-convolutional layer. We also propose an attention layer which
leverages the multi-head attention scheme. Additionally, we generate multiple
output masks with three different resolutions, which creates an ensemble of
three outputs in the inference process to enhance the performance. Finally, we
propose a combined loss function which leverages Focal loss and IoU loss to
train the network. Our experiments on the development set of the
Landslide4Sense challenge achieve an F1 score and an mIoU score of 84.07 and
76.07, respectively. Our best model setup outperforms the challenge baseline
and the proposed U-Net baseline, improving the F1 score/mIoU score by 6.8/7.4
and 10.5/8.8, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16726">FairCompass: Operationalising Fairness in Machine Learning. (arXiv:2312.16726v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jessica Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huaming Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1">Jun Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Choo_K/0/1/0/all/0/1">Kim-Kwang Raymond Choo</a></p>
<p>As artificial intelligence (AI) increasingly becomes an integral part of our
societal and individual activities, there is a growing imperative to develop
responsible AI solutions. Despite a diverse assortment of machine learning
fairness solutions is proposed in the literature, there is reportedly a lack of
practical implementation of these tools in real-world applications. Industry
experts have participated in thorough discussions on the challenges associated
with operationalising fairness in the development of machine learning-empowered
solutions, in which a shift toward human-centred approaches is promptly
advocated to mitigate the limitations of existing techniques. In this work, we
propose a human-in-the-loop approach for fairness auditing, presenting a mixed
visual analytical system (hereafter referred to as 'FairCompass'), which
integrates both subgroup discovery technique and the decision tree-based schema
for end users. Moreover, we innovatively integrate an Exploration, Guidance and
Informed Analysis loop, to facilitate the use of the Knowledge Generation Model
for Visual Analytics in FairCompass. We evaluate the effectiveness of
FairCompass for fairness auditing in a real-world scenario, and the findings
demonstrate the system's potential for real-world deployability. We anticipate
this work will address the current gaps in research for fairness and facilitate
the operationalisation of fairness in machine learning systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16730">Foundations of Reinforcement Learning and Interactive Decision Making. (arXiv:2312.16730v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1">Dylan J. Foster</a>, <a href="http://arxiv.org/find/cs/1/au:+Rakhlin_A/0/1/0/all/0/1">Alexander Rakhlin</a></p>
<p>These lecture notes give a statistical perspective on the foundations of
reinforcement learning and interactive decision making. We present a unifying
framework for addressing the exploration-exploitation dilemma using frequentist
and Bayesian approaches, with connections and parallels between supervised
learning/estimation and decision making as an overarching theme. Special
attention is paid to function approximation and flexible model classes such as
neural networks. Topics covered include multi-armed and contextual bandits,
structured bandits, and reinforcement learning with high-dimensional feedback.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16731">Disentangled Continual Learning: Separating Memory Edits from Model Updates. (arXiv:2312.16731v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dziadzio_S/0/1/0/all/0/1">Sebastian Dziadzio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yildiz_C/0/1/0/all/0/1">&#xc7;a&#x11f;atay Y&#x131;ld&#x131;z</a>, <a href="http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1">Gido M. van de Ven</a>, <a href="http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1">Tomasz Trzci&#x144;ski</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a>, <a href="http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1">Matthias Bethge</a></p>
<p>The ability of machine learning systems to learn continually is hindered by
catastrophic forgetting, the tendency of neural networks to overwrite existing
knowledge when learning a new task. Existing continual learning methods
alleviate this problem through regularisation, parameter isolation, or
rehearsal, and are typically evaluated on benchmarks consisting of a handful of
tasks. We propose a novel conceptual approach to continual classification that
aims to disentangle class-specific information that needs to be memorised from
the class-agnostic knowledge that encapsulates generalization. We store the
former in a buffer that can be easily pruned or updated when new categories
arrive, while the latter is represented with a neural network that generalizes
across tasks. We show that the class-agnostic network does not suffer from
catastrophic forgetting and by leveraging it to perform classification, we
improve accuracy on past tasks over time. In addition, our approach supports
open-set classification and one-shot generalization. To test our conceptual
framework, we introduce Infinite dSprites, a tool for creating continual
classification and disentanglement benchmarks of arbitrary length with full
control over generative factors. We show that over a sufficiently long time
horizon all major types of continual learning methods break down, while our
approach enables continual learning over hundreds of tasks with explicit
control over memorization and forgetting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16733">SuperServe: Fine-Grained Inference Serving for Unpredictable Workloads. (arXiv:2312.16733v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khare_A/0/1/0/all/0/1">Alind Khare</a>, <a href="http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1">Dhruv Garg</a>, <a href="http://arxiv.org/find/cs/1/au:+Kalra_S/0/1/0/all/0/1">Sukrit Kalra</a>, <a href="http://arxiv.org/find/cs/1/au:+Grandhi_S/0/1/0/all/0/1">Snigdha Grandhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Stoica_I/0/1/0/all/0/1">Ion Stoica</a>, <a href="http://arxiv.org/find/cs/1/au:+Tumanov_A/0/1/0/all/0/1">Alexey Tumanov</a></p>
<p>The increasing deployment of ML models on the critical path of production
applications in both datacenter and the edge requires ML inference serving
systems to serve these models under unpredictable and bursty request arrival
rates. Serving models under such conditions requires these systems to strike a
careful balance between the latency and accuracy requirements of the
application and the overall efficiency of utilization of scarce resources.
State-of-the-art systems resolve this tension by either choosing a static point
in the latency-accuracy tradeoff space to serve all requests or load specific
models on the critical path of request serving. In this work, we instead
resolve this tension by simultaneously serving the entire-range of models
spanning the latency-accuracy tradeoff space. Our novel mechanism, SubNetAct,
achieves this by carefully inserting specialized operators in weight-shared
SuperNetworks. These operators enable SubNetAct to dynamically route requests
through the network to meet a latency and accuracy target. SubNetAct requires
upto 2.6x lower memory to serve a vastly-higher number of models than prior
state-of-the-art. In addition, SubNetAct's near-instantaneous actuation of
models unlocks the design space of fine-grained, reactive scheduling policies.
We explore the design of one such extremely effective policy, SlackFit and
instantiate both SubNetAct and SlackFit in a real system, SuperServe.
SuperServe achieves 4.67% higher accuracy for the same SLO attainment and 2.85x
higher SLO attainment for the same accuracy on a trace derived from the
real-world Microsoft Azure Functions workload and yields the best trade-offs on
a wide range of extremely-bursty synthetic traces automatically.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16760">The Fourth International Verification of Neural Networks Competition (VNN-COMP 2023): Summary and Results. (arXiv:2312.16760v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brix_C/0/1/0/all/0/1">Christopher Brix</a>, <a href="http://arxiv.org/find/cs/1/au:+Bak_S/0/1/0/all/0/1">Stanley Bak</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Changliu Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Johnson_T/0/1/0/all/0/1">Taylor T. Johnson</a></p>
<p>This report summarizes the 4th International Verification of Neural Networks
Competition (VNN-COMP 2023), held as a part of the 6th Workshop on Formal
Methods for ML-Enabled Autonomous Systems (FoMLAS), that was collocated with
the 35th International Conference on Computer-Aided Verification (CAV).
VNN-COMP is held annually to facilitate the fair and objective comparison of
state-of-the-art neural network verification tools, encourage the
standardization of tool interfaces, and bring together the neural network
verification community. To this end, standardized formats for networks (ONNX)
and specification (VNN-LIB) were defined, tools were evaluated on equal-cost
hardware (using an automatic evaluation pipeline based on AWS instances), and
tool parameters were chosen by the participants before the final test sets were
made public. In the 2023 iteration, 7 teams participated on a diverse set of 10
scored and 4 unscored benchmarks. This report summarizes the rules, benchmarks,
participating tools, results, and lessons learned from this iteration of this
competition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16762">Neural Operator Approximations of Backstepping Kernels for $2\times 2$ Hyperbolic PDEs. (arXiv:2312.16762v1 [math.OC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Wang_S/0/1/0/all/0/1">Shanshan Wang</a>, <a href="http://arxiv.org/find/math/1/au:+Diagne_M/0/1/0/all/0/1">Mamadou Diagne</a>, <a href="http://arxiv.org/find/math/1/au:+Krstic_M/0/1/0/all/0/1">Miroslav Krsti&#x107;</a></p>
<p>Deep neural network approximation of nonlinear operators, commonly referred
to as DeepONet, has so far proven capable of approximating PDE backstepping
designs in which a single Goursat-form PDE governs a single feedback gain
function. In boundary control of coupled PDEs, coupled Goursat-form PDEs govern
two or more gain kernels - a PDE structure unaddressed thus far with DeepONet.
In this note we open the subject of approximating systems of gain kernel PDEs
for hyperbolic PDE plants by considering a simple counter-convecting $2\times
2$ coupled system in whose control a $2\times 2$ Goursat form kernel PDE system
arises. Such a coupled kernel PDE problem arises in several canonical $2\times
2$ hyperbolic PDE problems: oil drilling, Saint-Venant model of shallow water
waves, and Aw-Rascle model of stop-and-go instability in congested traffic
flow. In this paper, we establish the continuity of the mapping from (a total
of five) plant PDE functional coefficients to the kernel PDE solutions, prove
the existence of an arbitrarily close DeepONet approximation to the kernel
PDEs, and establish that the DeepONet-approximated gains guarantee
stabilization when replacing the exact backstepping gain kernels. The DeepONet
operator speeds the computation of the controller gains by multiple orders of
magnitude and its theoretically proven stabilizing capability is illustrated by
simulations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16772">Unsupversied feature correlation model to predict breast abnormal variation maps in longitudinal mammograms. (arXiv:2312.16772v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Bai_J/0/1/0/all/0/1">Jun Bai</a>, <a href="http://arxiv.org/find/eess/1/au:+Jin_A/0/1/0/all/0/1">Annie Jin</a>, <a href="http://arxiv.org/find/eess/1/au:+Adams_M/0/1/0/all/0/1">Madison Adams</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1">Clifford Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Nabavi_S/0/1/0/all/0/1">Sheida Nabavi</a></p>
<p>Breast cancer continues to be a significant cause of mortality among women
globally. Timely identification and precise diagnosis of breast abnormalities
are critical for enhancing patient prognosis. In this study, we focus on
improving the early detection and accurate diagnosis of breast abnormalities,
which is crucial for improving patient outcomes and reducing the mortality rate
of breast cancer. To address the limitations of traditional screening methods,
a novel unsupervised feature correlation network was developed to predict maps
indicating breast abnormal variations using longitudinal 2D mammograms. The
proposed model utilizes the reconstruction process of current year and prior
year mammograms to extract tissue from different areas and analyze the
differences between them to identify abnormal variations that may indicate the
presence of cancer. The model is equipped with a feature correlation module, an
attention suppression gate, and a breast abnormality detection module that work
together to improve the accuracy of the prediction. The proposed model not only
provides breast abnormal variation maps, but also distinguishes between normal
and cancer mammograms, making it more advanced compared to the state-of the-art
baseline models. The results of the study show that the proposed model
outperforms the baseline models in terms of Accuracy, Sensitivity, Specificity,
Dice score, and cancer detection rate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16784">Learning Scalable Structural Representations for Link Prediction with Bloom Signatures. (arXiv:2312.16784v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianyi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Haoteng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_R/0/1/0/all/0/1">Rongzhe Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1">Anshumali Shrivastava</a></p>
<p>Graph neural networks (GNNs) have shown great potential in learning on
graphs, but they are known to perform sub-optimally on link prediction tasks.
Existing GNNs are primarily designed to learn node-wise representations and
usually fail to capture pairwise relations between target nodes, which proves
to be crucial for link prediction. Recent works resort to learning more
expressive edge-wise representations by enhancing vanilla GNNs with structural
features such as labeling tricks and link prediction heuristics, but they
suffer from high computational overhead and limited scalability. To tackle this
issue, we propose to learn structural link representations by augmenting the
message-passing framework of GNNs with Bloom signatures. Bloom signatures are
hashing-based compact encodings of node neighborhoods, which can be efficiently
merged to recover various types of edge-wise structural features. We further
show that any type of neighborhood overlap-based heuristic can be estimated by
a neural network that takes Bloom signatures as input. GNNs with Bloom
signatures are provably more expressive than vanilla GNNs and also more
scalable than existing edge-wise models. Experimental results on five standard
link prediction benchmarks show that our proposed model achieves comparable or
better performance than existing edge-wise GNN models while being 3-200
$\times$ faster and more memory-efficient for online inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16788">Mitigating Degree Biases in Message Passing Mechanism by Utilizing Community Structures. (arXiv:2312.16788v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hoang_V/0/1/0/all/0/1">Van Thuy Hoang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_O/0/1/0/all/0/1">O-Joun Lee</a></p>
<p>This study utilizes community structures to address node degree biases in
message-passing (MP) via learnable graph augmentations and novel graph
transformers. Recent augmentation-based methods showed that MP neural networks
often perform poorly on low-degree nodes, leading to degree biases due to a
lack of messages reaching low-degree nodes. Despite their success, most methods
use heuristic or uniform random augmentations, which are non-differentiable and
may not always generate valuable edges for learning representations. In this
paper, we propose Community-aware Graph Transformers, namely CGT, to learn
degree-unbiased representations based on learnable augmentations and graph
transformers by extracting within community structures. We first design a
learnable graph augmentation to generate more within-community edges connecting
low-degree nodes through edge perturbation. Second, we propose an improved
self-attention to learn underlying proximity and the roles of nodes within the
community. Third, we propose a self-supervised learning task that could learn
the representations to preserve the global graph structure and regularize the
graph augmentations. Extensive experiments on various benchmark datasets showed
CGT outperforms state-of-the-art baselines and significantly improves the node
degree biases. The source code is available at
https://github.com/NSLab-CUK/Community-aware-Graph-Transformer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.16790">Learning the Dynamic Correlations and Mitigating Noise by Hierarchical Convolution for Long-term Sequence Forecasting. (arXiv:2312.16790v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zhihao Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1">Liantao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yasha Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1">Junfeng Zhao</a></p>
<p>Deep learning algorithms, especially Transformer-based models, have achieved
significant performance by capturing long-range dependencies and historical
information. However, the power of convolution has not been fully investigated.
Moreover, most existing works ignore the dynamic interaction among variables
and evolutionary noise in series. Addressing these issues, we propose a
Hierarchical Memorizing Network (HMNet). In particular, a hierarchical
convolution structure is introduced to extract the information from the series
at various scales. Besides, we propose a dynamic variable interaction module to
learn the varying correlation and an adaptive denoising module to search and
exploit similar patterns to alleviate noises. These modules can cooperate with
the hierarchical structure from the perspective of fine to coarse grain.
Experiments on five benchmarks demonstrate that HMNet significantly outperforms
the state-of-the-art models by 10.6% on MSE and 5.7% on MAE. Our code is
released at https://github.com/yzhHoward/HMNet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1901.09269">Distributed Learning with Compressed Gradient Differences. (arXiv:1901.09269v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1">Konstantin Mishchenko</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorbunov_E/0/1/0/all/0/1">Eduard Gorbunov</a>, <a href="http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1">Martin Tak&#xe1;&#x10d;</a>, <a href="http://arxiv.org/find/cs/1/au:+Richtarik_P/0/1/0/all/0/1">Peter Richt&#xe1;rik</a></p>
<p>Training large machine learning models requires a distributed computing
approach, with communication of the model updates being the bottleneck. For
this reason, several methods based on the compression (e.g., sparsification
and/or quantization) of updates were recently proposed, including QSGD
(Alistarh et al., 2017), TernGrad (Wen et al., 2017), SignSGD (Bernstein et
al., 2018), and DQGD (Khirirat et al., 2018). However, none of these methods
are able to learn the gradients, which renders them incapable of converging to
the true optimum in the batch mode. In this work we propose a new distributed
learning method -- DIANA -- which resolves this issue via compression of
gradient differences. We perform a theoretical analysis in the strongly convex
and nonconvex settings and show that our rates are superior to existing rates.
We also provide theory to support non-smooth regularizers study the difference
between quantization schemes. Our analysis of block-quantization and
differences between $\ell_2$ and $\ell_{\infty}$ quantization closes the gaps
in theory and practice. Finally, by applying our analysis technique to
TernGrad, we establish the first convergence rate for this method.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1905.11027">A Geometric Modeling of Occam&#x27;s Razor in Deep Learning. (arXiv:1905.11027v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Ke Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Nielsen_F/0/1/0/all/0/1">Frank Nielsen</a></p>
<p>Why do deep neural networks (DNNs) benefit from very high dimensional
parameter spaces? Their huge parameter complexities vs. stunning performances
in practice is all the more intriguing and not explainable using the standard
theory of regular models. In this work, we propose a geometrically flavored
information-theoretic approach to study this phenomenon. Namely, we introduce
the locally varying dimensionality of the parameter space of neural network
models by considering the number of significant dimensions of the Fisher
information matrix, and model the parameter space as a manifold using the
framework of singular semi-Riemannian geometry. We derive model complexity
measures which yield short description lengths for deep neural network models
based on their singularity analysis thus explaining the good performance of
DNNs despite their large number of parameters.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/1912.11939">On the Principle of Least Symmetry Breaking in Shallow ReLU Models. (arXiv:1912.11939v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Arjevani_Y/0/1/0/all/0/1">Yossi Arjevani</a>, <a href="http://arxiv.org/find/cs/1/au:+Field_M/0/1/0/all/0/1">Michael Field</a></p>
<p>We consider the optimization problem associated with fitting two-layer ReLU
networks with respect to the squared loss, where labels are assumed to be
generated by a target network. Focusing first on standard Gaussian inputs, we
show that the structure of spurious local minima detected by stochastic
gradient descent (SGD) is, in a well-defined sense, the \emph{least loss of
symmetry} with respect to the target weights. A closer look at the analysis
indicates that this principle of least symmetry breaking may apply to a broader
range of settings. Motivated by this, we conduct a series of experiments which
corroborate this hypothesis for different classes of non-isotropic non-product
distributions, smooth activation functions and networks with a few layers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2003.04117">The Utility of Feature Reuse: Transfer Learning in Data-Starved Regimes. (arXiv:2003.04117v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shadman_R/0/1/0/all/0/1">Rashik Shadman</a>, <a href="http://arxiv.org/find/cs/1/au:+Murshed_M/0/1/0/all/0/1">M.G. Sarwar Murshed</a>, <a href="http://arxiv.org/find/cs/1/au:+Verenich_E/0/1/0/all/0/1">Edward Verenich</a>, <a href="http://arxiv.org/find/cs/1/au:+Velasquez_A/0/1/0/all/0/1">Alvaro Velasquez</a>, <a href="http://arxiv.org/find/cs/1/au:+Hussain_F/0/1/0/all/0/1">Faraz Hussain</a></p>
<p>The use of transfer learning with deep neural networks has increasingly
become widespread for deploying well-tested computer vision systems to newer
domains, especially those with limited datasets. We describe a transfer
learning use case for a domain with a data-starved regime, having fewer than
100 labeled target samples. We evaluate the effectiveness of convolutional
feature extraction and fine-tuning of overparameterized models with respect to
the size of target training data, as well as their generalization performance
on data with covariate shift, or out-of-distribution (OOD) data. Our
experiments demonstrate that both overparameterization and feature reuse
contribute to the successful application of transfer learning in training image
classifiers in data-starved regimes. We provide visual explanations to support
our findings and conclude that transfer learning enhances the performance of
CNN architectures in data-starved regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2007.10642">Gasper: GrAph Signal ProcEssing in R. (arXiv:2007.10642v5 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Loynes_B/0/1/0/all/0/1">Basile de Loynes</a>, <a href="http://arxiv.org/find/eess/1/au:+Navarro_F/0/1/0/all/0/1">Fabien Navarro</a>, <a href="http://arxiv.org/find/eess/1/au:+Olivier_B/0/1/0/all/0/1">Baptiste Olivier</a></p>
<p>We present a short tutorial on to the use of the R gasper package. Gasper is
a package dedicated to signal processing on graphs. It also provides an
interface to the SuiteSparse Matrix Collection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2011.08174">Policy design in experiments with unknown interference. (arXiv:2011.08174v8 [econ.EM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/econ/1/au:+Viviano_D/0/1/0/all/0/1">Davide Viviano</a>, <a href="http://arxiv.org/find/econ/1/au:+Rudder_J/0/1/0/all/0/1">Jess Rudder</a></p>
<p>This paper studies experimental designs for estimation and inference on
policies with spillover effects. Units are organized into a finite number of
large clusters and interact in unknown ways within each cluster. First, we
introduce a single-wave experiment that, by varying the randomization across
cluster pairs, estimates the marginal effect of a change in treatment
probabilities, taking spillover effects into account. Using the marginal
effect, we propose a test for policy optimality. Second, we design a
multiple-wave experiment to estimate welfare-maximizing treatment rules. We
provide strong theoretical guarantees and an implementation in a large-scale
field experiment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2102.06197">Estimating a Directed Tree for Extremes. (arXiv:2102.06197v4 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tran_N/0/1/0/all/0/1">Ngoc Mai Tran</a>, <a href="http://arxiv.org/find/stat/1/au:+Buck_J/0/1/0/all/0/1">Johannes Buck</a>, <a href="http://arxiv.org/find/stat/1/au:+Kluppelberg_C/0/1/0/all/0/1">Claudia Kl&#xfc;ppelberg</a></p>
<p>We propose a new method to estimate a root-directed spanning tree from
extreme data. A prominent example is a river network, to be discovered from
extreme flow measured at a set of stations. Our new algorithm utilizes
qualitative aspects of a max-linear Bayesian network, which has been designed
for modelling causality in extremes. The algorithm estimates bivariate scores
and returns a root-directed spanning tree. It performs extremely well on
benchmark data and new data. We prove that the new estimator is consistent
under a max-linear Bayesian network model with noise. We also assess its
strengths and limitations in a small simulation study.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2103.06234">Symmetry Breaking in Symmetric Tensor Decomposition. (arXiv:2103.06234v2 [math.OC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Arjevani_Y/0/1/0/all/0/1">Yossi Arjevani</a>, <a href="http://arxiv.org/find/math/1/au:+Bruna_J/0/1/0/all/0/1">Joan Bruna</a>, <a href="http://arxiv.org/find/math/1/au:+Field_M/0/1/0/all/0/1">Michael Field</a>, <a href="http://arxiv.org/find/math/1/au:+Kileel_J/0/1/0/all/0/1">Joe Kileel</a>, <a href="http://arxiv.org/find/math/1/au:+Trager_M/0/1/0/all/0/1">Matthew Trager</a>, <a href="http://arxiv.org/find/math/1/au:+Williams_F/0/1/0/all/0/1">Francis Williams</a></p>
<p>In this note, we consider the highly nonconvex optimization problem
associated with computing the rank decomposition of symmetric tensors. We
formulate the invariance properties of the loss function and show that critical
points detected by standard gradient based methods are \emph{symmetry breaking}
with respect to the target tensor. The phenomena, seen for different choices of
target tensors and norms, make possible the use of recently developed analytic
and algebraic tools for studying nonconvex optimization landscapes exhibiting
symmetry breaking phenomena of similar nature.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2107.09133">The Limiting Dynamics of SGD: Modified Loss, Phase Space Oscillations, and Anomalous Diffusion. (arXiv:2107.09133v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kunin_D/0/1/0/all/0/1">Daniel Kunin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sagastuy_Brena_J/0/1/0/all/0/1">Javier Sagastuy-Brena</a>, <a href="http://arxiv.org/find/cs/1/au:+Gillespie_L/0/1/0/all/0/1">Lauren Gillespie</a>, <a href="http://arxiv.org/find/cs/1/au:+Margalit_E/0/1/0/all/0/1">Eshed Margalit</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_H/0/1/0/all/0/1">Hidenori Tanaka</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguli_S/0/1/0/all/0/1">Surya Ganguli</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamins_D/0/1/0/all/0/1">Daniel L. K. Yamins</a></p>
<p>In this work we explore the limiting dynamics of deep neural networks trained
with stochastic gradient descent (SGD). As observed previously, long after
performance has converged, networks continue to move through parameter space by
a process of anomalous diffusion in which distance travelled grows as a power
law in the number of gradient updates with a nontrivial exponent. We reveal an
intricate interaction between the hyperparameters of optimization, the
structure in the gradient noise, and the Hessian matrix at the end of training
that explains this anomalous diffusion. To build this understanding, we first
derive a continuous-time model for SGD with finite learning rates and batch
sizes as an underdamped Langevin equation. We study this equation in the
setting of linear regression, where we can derive exact, analytic expressions
for the phase space dynamics of the parameters and their instantaneous
velocities from initialization to stationarity. Using the Fokker-Planck
equation, we show that the key ingredient driving these dynamics is not the
original training loss, but rather the combination of a modified loss, which
implicitly regularizes the velocity, and probability currents, which cause
oscillations in phase space. We identify qualitative and quantitative
predictions of this theory in the dynamics of a ResNet-18 model trained on
ImageNet. Through the lens of statistical physics, we uncover a mechanistic
origin for the anomalous limiting dynamics of deep neural networks trained with
SGD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2108.10859">Cumulative Regret Analysis of the Piyavskii--Shubert Algorithm and Its Variants for Global Optimization. (arXiv:2108.10859v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gokcesu_K/0/1/0/all/0/1">Kaan Gokcesu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gokcesu_H/0/1/0/all/0/1">Hakan Gokcesu</a></p>
<p>We study the problem of global optimization, where we analyze the performance
of the Piyavskii--Shubert algorithm and its variants. For any given time
duration $T$, instead of the extensively studied simple regret (which is the
difference of the losses between the best estimate up to $T$ and the global
minimum), we study the cumulative regret up to time $T$. For $L$-Lipschitz
continuous functions, we show that the cumulative regret is $O(L\log T)$. For
$H$-Lipschitz smooth functions, we show that the cumulative regret is $O(H)$.
We analytically extend our results for functions with Holder continuous
derivatives, which cover both the Lipschitz continuous and the Lipschitz smooth
functions, individually. We further show that a simpler variant of the
Piyavskii-Shubert algorithm performs just as well as the traditional variants
for the Lipschitz continuous or the Lipschitz smooth functions. We further
extend our results to broader classes of functions, and show that, our
algorithm efficiently determines its queries; and achieves nearly minimax
optimal (up to log factors) cumulative regret, for general convex or even
concave regularity conditions on the extrema of the objective (which
encompasses many preceding regularities). We consider further extensions by
investigating the performance of the Piyavskii-Shubert variants in the
scenarios with unknown regularity, noisy evaluation and multivariate domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.00145">Matrix Decomposition and Applications. (arXiv:2201.00145v3 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Lu_J/0/1/0/all/0/1">Jun Lu</a></p>
<p>In 1954, Alston S. Householder published Principles of Numerical Analysis,
one of the first modern treatments on matrix decomposition that favored a
(block) LU decomposition-the factorization of a matrix into the product of
lower and upper triangular matrices. And now, matrix decomposition has become a
core technology in machine learning, largely due to the development of the back
propagation algorithm in fitting a neural network. The sole aim of this survey
is to give a self-contained introduction to concepts and mathematical tools in
numerical linear algebra and matrix analysis in order to seamlessly introduce
matrix decomposition techniques and their applications in subsequent sections.
However, we clearly realize our inability to cover all the useful and
interesting results concerning matrix decomposition and given the paucity of
scope to present this discussion, e.g., the separated analysis of the Euclidean
space, Hermitian space, Hilbert space, and things in the complex domain. We
refer the reader to literature in the field of linear algebra for a more
detailed introduction to the related fields.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.08832">Occupancy Information Ratio: Infinite-Horizon, Information-Directed, Parameterized Policy Search. (arXiv:2201.08832v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suttle_W/0/1/0/all/0/1">Wesley A. Suttle</a>, <a href="http://arxiv.org/find/cs/1/au:+Koppel_A/0/1/0/all/0/1">Alec Koppel</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Ji Liu</a></p>
<p>In this work, we propose an information-directed objective for
infinite-horizon reinforcement learning (RL), called the occupancy information
ratio (OIR), inspired by the information ratio objectives used in previous
information-directed sampling schemes for multi-armed bandits and Markov
decision processes as well as recent advances in general utility RL. The OIR,
comprised of a ratio between the average cost of a policy and the entropy of
its induced state occupancy measure, enjoys rich underlying structure and
presents an objective to which scalable, model-free policy search methods
naturally apply. Specifically, we show by leveraging connections between
quasiconcave optimization and the linear programming theory for Markov decision
processes that the OIR problem can be transformed and solved via concave
programming methods when the underlying model is known. Since model knowledge
is typically lacking in practice, we lay the foundations for model-free OIR
policy search methods by establishing a corresponding policy gradient theorem.
Building on this result, we subsequently derive REINFORCE- and
actor-critic-style algorithms for solving the OIR problem in policy parameter
space. Crucially, exploiting the powerful hidden quasiconcavity property
implied by the concave programming transformation of the OIR problem, we
establish finite-time convergence of the REINFORCE-style scheme to global
optimality and asymptotic convergence of the actor-critic-style scheme to
(near) global optimality under suitable conditions. Finally, we experimentally
illustrate the utility of OIR-based methods over vanilla methods in
sparse-reward settings, supporting the OIR as an alternative to existing RL
objectives.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2204.04876">Lyapunov-Guided Representation of Recurrent Neural Network Performance. (arXiv:2204.04876v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vogt_R/0/1/0/all/0/1">Ryan Vogt</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Shlizerman_E/0/1/0/all/0/1">Eli Shlizerman</a></p>
<p>Recurrent Neural Networks (RNN) are ubiquitous computing systems for
sequences and multivariate time series data. While several robust architectures
of RNN are known, it is unclear how to relate RNN initialization, architecture,
and other hyperparameters with accuracy for a given task. In this work, we
propose to treat RNN as dynamical systems and to correlate hyperparameters with
accuracy through Lyapunov spectral analysis, a methodology specifically
designed for nonlinear dynamical systems. To address the fact that RNN features
go beyond the existing Lyapunov spectral analysis, we propose to infer relevant
features from the Lyapunov spectrum with an Autoencoder and an embedding of its
latent representation (AeLLE). Our studies of various RNN architectures show
that AeLLE successfully correlates RNN Lyapunov spectrum with accuracy.
Furthermore, the latent representation learned by AeLLE is generalizable to
novel inputs from the same task and is formed early in the process of RNN
training. The latter property allows for the prediction of the accuracy to
which RNN would converge when training is complete. We conclude that
representation of RNN through Lyapunov spectrum along with AeLLE provides a
novel method for organization and interpretation of variants of RNN
architectures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.15677">Augmentation-Aware Self-Supervision for Data-Efficient GAN Training. (arXiv:2205.15677v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Liang Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qi Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1">Yige Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1">Songtao Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chongyang Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1">Siyuan Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_P/0/1/0/all/0/1">Pengfei Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1">Huawei Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xueqi Cheng</a></p>
<p>Training generative adversarial networks (GANs) with limited data is
challenging because the discriminator is prone to overfitting. Previously
proposed differentiable augmentation demonstrates improved data efficiency of
training GANs. However, the augmentation implicitly introduces undesired
invariance to augmentation for the discriminator since it ignores the change of
semantics in the label space caused by data transformation, which may limit the
representation learning ability of the discriminator and ultimately affect the
generative modeling performance of the generator. To mitigate the negative
impact of invariance while inheriting the benefits of data augmentation, we
propose a novel augmentation-aware self-supervised discriminator that predicts
the augmentation parameter of the augmented data. Particularly, the prediction
targets of real data and generated data are required to be distinguished since
they are different during training. We further encourage the generator to
adversarially learn from the self-supervised discriminator by generating
augmentation-predictable real and not fake data. This formulation connects the
learning objective of the generator and the arithmetic $-$ harmonic mean
divergence under certain assumptions. We compare our method with
state-of-the-art (SOTA) methods using the class-conditional BigGAN and
unconditional StyleGAN2 architectures on data-limited CIFAR-10, CIFAR-100,
FFHQ, LSUN-Cat, and five low-shot datasets. Experimental results demonstrate
significant improvements of our method over SOTA methods in training
data-efficient GANs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.05148">Weakly-supervised segmentation using inherently-explainable classification models and their application to brain tumour classification. (arXiv:2206.05148v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1">Soumick Chatterjee</a>, <a href="http://arxiv.org/find/eess/1/au:+Yassin_H/0/1/0/all/0/1">Hadya Yassin</a>, <a href="http://arxiv.org/find/eess/1/au:+Dubost_F/0/1/0/all/0/1">Florian Dubost</a>, <a href="http://arxiv.org/find/eess/1/au:+Nurnberger_A/0/1/0/all/0/1">Andreas N&#xfc;rnberger</a>, <a href="http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1">Oliver Speck</a></p>
<p>Deep learning models have shown their potential for several applications.
However, most of the models are opaque and difficult to trust due to their
complex reasoning - commonly known as the black-box problem. Some fields, such
as medicine, require a high degree of transparency to accept and adopt such
technologies. Consequently, creating explainable/interpretable models or
applying post-hoc methods on classifiers to build trust in deep learning models
are required. Moreover, deep learning methods can be used for segmentation
tasks, which typically require hard-to-obtain, time-consuming
manually-annotated segmentation labels for training. This paper introduces
three inherently-explainable classifiers to tackle both of these problems as
one. The localisation heatmaps provided by the networks -- representing the
models' focus areas and being used in classification decision-making -- can be
directly interpreted, without requiring any post-hoc methods to derive
information for model explanation. The models are trained by using the input
image and only the classification labels as ground-truth in a supervised
fashion - without using any information about the location of the region of
interest (i.e. the segmentation labels), making the segmentation training of
the models weakly-supervised through classification labels. The final
segmentation is obtained by thresholding these heatmaps. The models were
employed for the task of multi-class brain tumour classification using two
different datasets, resulting in the best F1-score of 0.93 for the supervised
classification task while securing a median Dice score of 0.67$\pm$0.08 for the
weakly-supervised segmentation task. Furthermore, the obtained accuracy on a
subset of tumour-only images outperformed the state-of-the-art glioma tumour
grading binary classifiers with the best model achieving 98.7\% accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.02346">Many-body localized hidden generative models. (arXiv:2207.02346v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Zhong_W/0/1/0/all/0/1">Weishun Zhong</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Gao_X/0/1/0/all/0/1">Xun Gao</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Yelin_S/0/1/0/all/0/1">Susanne F. Yelin</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Najafi_K/0/1/0/all/0/1">Khadijeh Najafi</a></p>
<p>Born machines are quantum-inspired generative models that leverage the
probabilistic nature of quantum states. Here, we present a new architecture
called many-body localized (MBL) hidden Born machine that utilizes both MBL
dynamics and hidden units as learning resources. We show that the hidden units
act as an effective thermal bath that enhances the trainability of the system,
while the MBL dynamics stabilize the training trajectories. We numerically
demonstrate that the MBL hidden Born machine is capable of learning a variety
of tasks, including a toy version of MNIST handwritten digits, quantum data
obtained from quantum many-body states, and non-local parity data. Our
architecture and algorithm provide novel strategies of utilizing quantum
many-body systems as learning resources, and reveal a powerful connection
between disorder, interaction, and learning in quantum many-body systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.07038">SHAP-XRT: The Shapley Value Meets Conditional Independence Testing. (arXiv:2207.07038v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Teneggi_J/0/1/0/all/0/1">Jacopo Teneggi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bharti_B/0/1/0/all/0/1">Beepul Bharti</a>, <a href="http://arxiv.org/find/cs/1/au:+Romano_Y/0/1/0/all/0/1">Yaniv Romano</a>, <a href="http://arxiv.org/find/cs/1/au:+Sulam_J/0/1/0/all/0/1">Jeremias Sulam</a></p>
<p>The complex nature of artificial neural networks raises concerns on their
reliability, trustworthiness, and fairness in real-world scenarios. The Shapley
value -- a solution concept from game theory -- is one of the most popular
explanation methods for machine learning models. More traditionally, from a
statistical perspective, feature importance is defined in terms of conditional
independence. So far, these two approaches to interpretability and feature
importance have been considered separate and distinct. In this work, we show
that Shapley-based explanation methods and conditional independence testing are
closely related. We introduce the SHAPley EXplanation Randomization Test
(SHAP-XRT), a testing procedure inspired by the Conditional Randomization Test
(CRT) for a specific notion of local (i.e., on a sample) conditional
independence. With it, we prove that for binary classification problems, the
marginal contributions in the Shapley value provide lower and upper bounds to
the expected $p$-values of their respective tests. Furthermore, we show that
the Shapley value itself provides an upper bound to the expected $p$-value of a
global (i.e., overall) null hypothesis. As a result, we further our
understanding of Shapley-based explanation methods from a novel perspective and
characterize the conditions under which one can make statistically valid claims
about feature importance via the Shapley value.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.15224">Robust Unsupervised Multi-task and Transfer Learning on Gaussian Mixture Models. (arXiv:2209.15224v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Tian_Y/0/1/0/all/0/1">Ye Tian</a>, <a href="http://arxiv.org/find/stat/1/au:+Weng_H/0/1/0/all/0/1">Haolei Weng</a>, <a href="http://arxiv.org/find/stat/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Unsupervised learning has been widely used in many real-world applications.
One of the simplest and most important unsupervised learning models is the
Gaussian mixture model (GMM). In this work, we study the multi-task learning
problem on GMMs, which aims to leverage potentially similar GMM parameter
structures among tasks to obtain improved learning performance compared to
single-task learning. We propose a multi-task GMM learning procedure based on
the EM algorithm that not only can effectively utilize unknown similarity
between related tasks but is also robust against a fraction of outlier tasks
from arbitrary distributions. The proposed procedure is shown to achieve
minimax optimal rate of convergence for both parameter estimation error and the
excess mis-clustering error, in a wide range of regimes. Moreover, we
generalize our approach to tackle the problem of transfer learning for GMMs,
where similar theoretical results are derived. Finally, we demonstrate the
effectiveness of our methods through simulations and real data examples. To the
best of our knowledge, this is the first work studying multi-task and transfer
learning on GMMs with theoretical guarantees.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00462">A Latent Space Correlation-Aware Autoencoder for Anomaly Detection in Skewed Data. (arXiv:2301.00462v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roy_P/0/1/0/all/0/1">Padmaksha Roy</a></p>
<p>Unsupervised learning-based anomaly detection in latent space has gained
importance since discriminating anomalies from normal data becomes difficult in
high-dimensional space. Both density estimation and distance-based methods to
detect anomalies in latent space have been explored in the past. These methods
prove that retaining valuable properties of input data in latent space helps in
the better reconstruction of test data. Moreover, real-world sensor data is
skewed and non-Gaussian in nature, making mean-based estimators unreliable for
skewed data. Again, anomaly detection methods based on reconstruction error
rely on Euclidean distance, which does not consider useful correlation
information in the feature space and also fails to accurately reconstruct the
data when it deviates from the training distribution. In this work, we address
the limitations of reconstruction error-based autoencoders and propose a
kernelized autoencoder that leverages a robust form of Mahalanobis distance
(MD) to measure latent dimension correlation to effectively detect both near
and far anomalies. This hybrid loss is aided by the principle of maximizing the
mutual information gain between the latent dimension and the high-dimensional
prior data space by maximizing the entropy of the latent space while preserving
useful correlation information of the original data in the low-dimensional
latent space. The multi-objective function has two goals -- it measures
correlation information in the latent feature space in the form of robust MD
distance and simultaneously tries to preserve useful correlation information
from the original data space in the latent space by maximizing mutual
information between the prior and latent space.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.00812">One-shot domain adaptation in video-based assessment of surgical skills. (arXiv:2301.00812v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yanik_E/0/1/0/all/0/1">Erim Yanik</a>, <a href="http://arxiv.org/find/cs/1/au:+Schwaitzberg_S/0/1/0/all/0/1">Steven Schwaitzberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Gene Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Intes_X/0/1/0/all/0/1">Xavier Intes</a>, <a href="http://arxiv.org/find/cs/1/au:+De_S/0/1/0/all/0/1">Suvranu De</a></p>
<p>Deep Learning (DL) has achieved automatic and objective assessment of
surgical skills. However, the applicability of DL models is often hampered by
their substantial data requirements and confinement to specific training
domains. This prevents them from transitioning to new tasks with scarce data.
Therefore, domain adaptation emerges as a critical element for the practical
implementation of DL in real-world scenarios. Herein, we introduce A-VBANet, a
novel meta-learning model capable of delivering domain-agnostic surgical skill
classification via one-shot learning. A-VBANet has been rigorously developed
and tested on five diverse laparoscopic and robotic surgical simulators.
Furthermore, we extend its validation to operating room (OR) videos of
laparoscopic cholecystectomy. Our model successfully adapts with accuracies up
to 99.5% in one-shot and 99.9% in few-shot settings for simulated tasks and
89.7% for laparoscopic cholecystectomy. This research marks the first instance
of a domain-agnostic methodology for surgical skill assessment, paving the way
for more precise and accessible training evaluation across diverse high-stakes
environments such as real-life surgery where data is scarce.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.02288">gRoMA: a Tool for Measuring the Global Robustness of Deep Neural Networks. (arXiv:2301.02288v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Levy_N/0/1/0/all/0/1">Natan Levy</a>, <a href="http://arxiv.org/find/cs/1/au:+Yerushalmi_R/0/1/0/all/0/1">Raz Yerushalmi</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_G/0/1/0/all/0/1">Guy Katz</a></p>
<p>Deep neural networks (DNNs) are at the forefront of cutting-edge technology,
and have been achieving remarkable performance in a variety of complex tasks.
Nevertheless, their integration into safety-critical systems, such as in the
aerospace or automotive domains, poses a significant challenge due to the
threat of adversarial inputs: perturbations in inputs that might cause the DNN
to make grievous mistakes. Multiple studies have demonstrated that even modern
DNNs are susceptible to adversarial inputs, and this risk must thus be measured
and mitigated to allow the deployment of DNNs in critical settings. Here, we
present gRoMA (global Robustness Measurement and Assessment), an innovative and
scalable tool that implements a probabilistic approach to measure the global
categorial robustness of a DNN. Specifically, gRoMA measures the probability of
encountering adversarial inputs for a specific output category. Our tool
operates on pre-trained, black-box classification DNNs, and generates input
samples belonging to an output category of interest. It measures the DNN's
susceptibility to adversarial inputs around these inputs, and aggregates the
results to infer the overall global categorial robustness of the DNN up to some
small bounded statistical error.
</p>
<p>We evaluate our tool on the popular Densenet DNN model over the CIFAR10
dataset. Our results reveal significant gaps in the robustness of the different
output categories. This experiment demonstrates the usefulness and scalability
of our approach and its potential for allowing DNNs to be deployed within
critical systems of interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.04027">Differentiable modeling to unify machine learning and physical models and advance Geosciences. (arXiv:2301.04027v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chaopeng Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Appling_A/0/1/0/all/0/1">Alison P. Appling</a>, <a href="http://arxiv.org/find/cs/1/au:+Gentine_P/0/1/0/all/0/1">Pierre Gentine</a>, <a href="http://arxiv.org/find/cs/1/au:+Bandai_T/0/1/0/all/0/1">Toshiyuki Bandai</a>, <a href="http://arxiv.org/find/cs/1/au:+Gupta_H/0/1/0/all/0/1">Hoshin Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Tartakovsky_A/0/1/0/all/0/1">Alexandre Tartakovsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Baity_Jesi_M/0/1/0/all/0/1">Marco Baity-Jesi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fenicia_F/0/1/0/all/0/1">Fabrizio Fenicia</a>, <a href="http://arxiv.org/find/cs/1/au:+Kifer_D/0/1/0/all/0/1">Daniel Kifer</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Li Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiaofeng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1">Wei Ren</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yi Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Harman_C/0/1/0/all/0/1">Ciaran J. Harman</a>, <a href="http://arxiv.org/find/cs/1/au:+Clark_M/0/1/0/all/0/1">Martyn Clark</a>, <a href="http://arxiv.org/find/cs/1/au:+Farthing_M/0/1/0/all/0/1">Matthew Farthing</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1">Dapeng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_P/0/1/0/all/0/1">Praveen Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Aboelyazeed_D/0/1/0/all/0/1">Doaa Aboelyazeed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rahmani_F/0/1/0/all/0/1">Farshid Rahmani</a>, <a href="http://arxiv.org/find/cs/1/au:+Beck_H/0/1/0/all/0/1">Hylke E. Beck</a>, <a href="http://arxiv.org/find/cs/1/au:+Bindas_T/0/1/0/all/0/1">Tadd Bindas</a>, <a href="http://arxiv.org/find/cs/1/au:+Dwivedi_D/0/1/0/all/0/1">Dipankar Dwivedi</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_K/0/1/0/all/0/1">Kuai Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoge_M/0/1/0/all/0/1">Marvin H&#xf6;ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Rackauckas_C/0/1/0/all/0/1">Chris Rackauckas</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_T/0/1/0/all/0/1">Tirthankar Roy</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chonggang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohanty_B/0/1/0/all/0/1">Binayak Mohanty</a>, <a href="http://arxiv.org/find/cs/1/au:+Lawson_K/0/1/0/all/0/1">Kathryn Lawson</a></p>
<p>Process-Based Modeling (PBM) and Machine Learning (ML) are often perceived as
distinct paradigms in the geosciences. Here we present differentiable
geoscientific modeling as a powerful pathway toward dissolving the perceived
barrier between them and ushering in a paradigm shift. For decades, PBM offered
benefits in interpretability and physical consistency but struggled to
efficiently leverage large datasets. ML methods, especially deep networks,
presented strong predictive skills yet lacked the ability to answer specific
scientific questions. While various methods have been proposed for ML-physics
integration, an important underlying theme -- differentiable modeling -- is not
sufficiently recognized. Here we outline the concepts, applicability, and
significance of differentiable geoscientific modeling (DG). "Differentiable"
refers to accurately and efficiently calculating gradients with respect to
model variables, critically enabling the learning of high-dimensional unknown
relationships. DG refers to a range of methods connecting varying amounts of
prior knowledge to neural networks and training them together, capturing a
different scope than physics-guided machine learning and emphasizing first
principles. Preliminary evidence suggests DG offers better interpretability and
causality than ML, improved generalizability and extrapolation capability, and
strong potential for knowledge discovery, while approaching the performance of
purely data-driven ML. DG models require less training data while scaling
favorably in performance and efficiency with increasing amounts of data. With
DG, geoscientists may be better able to frame and investigate questions, test
hypotheses, and discover unrecognized linkages.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.10405">Editing Language Model-based Knowledge Graph Embeddings. (arXiv:2301.10405v8 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a></p>
<p>Recently decades have witnessed the empirical success of framing Knowledge
Graph (KG) embeddings via language models. However, language model-based KG
embeddings are usually deployed as static artifacts, making them difficult to
modify post-deployment without re-training after deployment. To address this
issue, we propose a new task of editing language model-based KG embeddings in
this paper. This task is designed to facilitate rapid, data-efficient updates
to KG embeddings without compromising the performance of other aspects. We
build four new datasets: E-FB15k237, A-FB15k237, E-WN18RR, and A-WN18RR, and
evaluate several knowledge editing baselines demonstrating the limited ability
of previous models to handle the proposed challenging task. We further propose
a simple yet strong baseline dubbed KGEditor, which utilizes additional
parametric layers of the hypernetwork to edit/add facts. Our comprehensive
experimental results reveal that KGEditor excels in updating specific facts
without impacting the overall performance, even when faced with limited
training resources. Code and datasets are available in
https://github.com/zjunlp/PromptKG/tree/main/deltaKG.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.00209">Towards Large Certified Radius in Randomized Smoothing using Quasiconcave Optimization. (arXiv:2302.00209v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kung_B/0/1/0/all/0/1">Bo-Han Kung</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shang-Tse Chen</a></p>
<p>Randomized smoothing is currently the state-of-the-art method that provides
certified robustness for deep neural networks. However, due to its excessively
conservative nature, this method of incomplete verification often cannot
achieve an adequate certified radius on real-world datasets. One way to obtain
a larger certified radius is to use an input-specific algorithm instead of
using a fixed Gaussian filter for all data points. Several methods based on
this idea have been proposed, but they either suffer from high computational
costs or gain marginal improvement in certified radius. In this work, we show
that by exploiting the quasiconvex problem structure, we can find the optimal
certified radii for most data points with slight computational overhead. This
observation leads to an efficient and effective input-specific randomized
smoothing algorithm. We conduct extensive experiments and empirical analysis on
CIFAR-10 and ImageNet. The results show that the proposed method significantly
enhances the certified radii with low computational overhead.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.03791">How to Trust Your Diffusion Model: A Convex Optimization Approach to Conformal Risk Control. (arXiv:2302.03791v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Teneggi_J/0/1/0/all/0/1">Jacopo Teneggi</a>, <a href="http://arxiv.org/find/stat/1/au:+Tivnan_M/0/1/0/all/0/1">Matthew Tivnan</a>, <a href="http://arxiv.org/find/stat/1/au:+Stayman_J/0/1/0/all/0/1">J. Webster Stayman</a>, <a href="http://arxiv.org/find/stat/1/au:+Sulam_J/0/1/0/all/0/1">Jeremias Sulam</a></p>
<p>Score-based generative modeling, informally referred to as diffusion models,
continue to grow in popularity across several important domains and tasks.
While they provide high-quality and diverse samples from empirical
distributions, important questions remain on the reliability and
trustworthiness of these sampling procedures for their responsible use in
critical scenarios. Conformal prediction is a modern tool to construct
finite-sample, distribution-free uncertainty guarantees for any black-box
predictor. In this work, we focus on image-to-image regression tasks and we
present a generalization of the Risk-Controlling Prediction Sets (RCPS)
procedure, that we term $K$-RCPS, which allows to $(i)$ provide entrywise
calibrated intervals for future samples of any diffusion model, and $(ii)$
control a certain notion of risk with respect to a ground truth image with
minimal mean interval length. Differently from existing conformal risk control
procedures, ours relies on a novel convex optimization approach that allows for
multidimensional risk control while provably minimizing the mean interval
length. We illustrate our approach on two real-world image denoising problems:
on natural images of faces as well as on computed tomography (CT) scans of the
abdomen, demonstrating state of the art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11048">Adversarial Model for Offline Reinforcement Learning. (arXiv:2302.11048v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhardwaj_M/0/1/0/all/0/1">Mohak Bhardwaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1">Tengyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1">Byron Boots</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1">Nan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1">Ching-An Cheng</a></p>
<p>We propose a novel model-based offline Reinforcement Learning (RL) framework,
called Adversarial Model for Offline Reinforcement Learning (ARMOR), which can
robustly learn policies to improve upon an arbitrary reference policy
regardless of data coverage. ARMOR is designed to optimize policies for the
worst-case performance relative to the reference policy through adversarially
training a Markov decision process model. In theory, we prove that ARMOR, with
a well-tuned hyperparameter, can compete with the best policy within data
coverage when the reference policy is supported by the data. At the same time,
ARMOR is robust to hyperparameter choices: the policy learned by ARMOR, with
"any" admissible hyperparameter, would never degrade the performance of the
reference policy, even when the reference policy is not covered by the dataset.
To validate these properties in practice, we design a scalable implementation
of ARMOR, which by adversarial training, can optimize policies without using
model ensembles in contrast to typical model-based methods. We show that ARMOR
achieves competent performance with both state-of-the-art offline model-free
and model-based RL algorithms and can robustly improve the reference policy
over various hyperparameter choices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.13390">MDF-Net for abnormality detection by fusing X-rays with clinical data. (arXiv:2302.13390v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Hsieh_C/0/1/0/all/0/1">Chihcheng Hsieh</a>, <a href="http://arxiv.org/find/eess/1/au:+Nobre_I/0/1/0/all/0/1">Isabel Blanco Nobre</a>, <a href="http://arxiv.org/find/eess/1/au:+Sousa_S/0/1/0/all/0/1">Sandra Costa Sousa</a>, <a href="http://arxiv.org/find/eess/1/au:+Ouyang_C/0/1/0/all/0/1">Chun Ouyang</a>, <a href="http://arxiv.org/find/eess/1/au:+Brereton_M/0/1/0/all/0/1">Margot Brereton</a>, <a href="http://arxiv.org/find/eess/1/au:+Nascimento_J/0/1/0/all/0/1">Jacinto C. Nascimento</a>, <a href="http://arxiv.org/find/eess/1/au:+Jorge_J/0/1/0/all/0/1">Joaquim Jorge</a>, <a href="http://arxiv.org/find/eess/1/au:+Moreira_C/0/1/0/all/0/1">Catarina Moreira</a></p>
<p>This study investigates the effects of including patients' clinical
information on the performance of deep learning (DL) classifiers for disease
location in chest X-ray images. Although current classifiers achieve high
performance using chest X-ray images alone, our interviews with radiologists
indicate that clinical data is highly informative and essential for
interpreting images and making proper diagnoses.
</p>
<p>In this work, we propose a novel architecture consisting of two fusion
methods that enable the model to simultaneously process patients' clinical data
(structured data) and chest X-rays (image data). Since these data modalities
are in different dimensional spaces, we propose a spatial arrangement strategy,
spatialization, to facilitate the multimodal learning process in a Mask R-CNN
model. We performed an extensive experimental evaluation using MIMIC-Eye, a
dataset comprising modalities: MIMIC-CXR (chest X-ray images), MIMIC IV-ED
(patients' clinical data), and REFLACX (annotations of disease locations in
chest X-rays).
</p>
<p>Results show that incorporating patients' clinical data in a DL model
together with the proposed fusion methods improves the disease localization in
chest X-rays by 12\% in terms of Average Precision compared to a standard Mask
R-CNN using only chest X-rays. Further ablation studies also emphasize the
importance of multimodal DL architectures and the incorporation of patients'
clinical data in disease localization. The architecture proposed in this work
is publicly available to promote the scientific reproducibility of our study
(https://github.com/ChihchengHsieh/multimodal-abnormalities-detection)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03379">SUREL+: Moving from Walks to Sets for Scalable Subgraph-based Graph Representation Learning. (arXiv:2303.03379v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Haoteng Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Muhan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianguo Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pan Li</a></p>
<p>Subgraph-based graph representation learning (SGRL) has recently emerged as a
powerful tool in many prediction tasks on graphs due to its advantages in model
expressiveness and generalization ability. Most previous SGRL models face
computational challenges associated with the high cost of subgraph extraction
for each training or test query. Recently, SUREL was proposed to accelerate
SGRL, which samples random walks offline and joins these walks online as a
proxy of subgraph for representation learning. Thanks to the reusability of
sampled walks across different queries, SUREL achieves state-of-the-art
performance in terms of scalability and prediction accuracy. However, SUREL
still suffers from high computational overhead caused by node duplication in
sampled walks. In this work, we propose a novel framework SUREL+ that upgrades
SUREL by using node sets instead of walks to represent subgraphs. This
set-based representation eliminates repeated nodes by definition but can also
be irregular in size. To address this issue, we design a customized sparse data
structure to efficiently store and access node sets and provide a specialized
operator to join them in parallel batches. SUREL+ is modularized to support
multiple types of set samplers, structural features, and neural encoders to
complement the structural information loss after the reduction from walks to
sets. Extensive experiments have been performed to validate SUREL+ in the
prediction tasks of links, relation types, and higher-order patterns. SUREL+
achieves 3-11$\times$ speedups of SUREL while maintaining comparable or even
better prediction performance; compared to other SGRL baselines, SUREL+
achieves $\sim$20$\times$ speedups and significantly improves the prediction
accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.03428">Towards provably efficient quantum algorithms for large-scale machine-learning models. (arXiv:2303.03428v5 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1">Junyu Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_M/0/1/0/all/0/1">Minzhao Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Liu_J/0/1/0/all/0/1">Jin-Peng Liu</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Ye_Z/0/1/0/all/0/1">Ziyu Ye</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Wang_Y/0/1/0/all/0/1">Yunfei Wang</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Alexeev_Y/0/1/0/all/0/1">Yuri Alexeev</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Eisert_J/0/1/0/all/0/1">Jens Eisert</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Jiang_L/0/1/0/all/0/1">Liang Jiang</a></p>
<p>Large machine learning models are revolutionary technologies of artificial
intelligence whose bottlenecks include huge computational expenses, power, and
time used both in the pre-training and fine-tuning process. In this work, we
show that fault-tolerant quantum computing could possibly provide provably
efficient resolutions for generic (stochastic) gradient descent algorithms,
scaling as O(T^2 polylog(n)), where n is the size of the models and T is the
number of iterations in the training, as long as the models are both
sufficiently dissipative and sparse, with small learning rates. Based on
earlier efficient quantum algorithms for dissipative differential equations, we
find and prove that similar algorithms work for (stochastic) gradient descent,
the primary algorithm for machine learning. In practice, we benchmark instances
of large machine learning models from 7 million to 103 million parameters. We
find that, in the context of sparse training, a quantum enhancement is possible
at the early stage of learning after model pruning, motivating a sparse
parameter download and re-upload scheme. Our work shows solidly that
fault-tolerant quantum algorithms could potentially contribute to most
state-of-the-art, large-scale machine-learning problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.14582">Identification of Negative Transfers in Multitask Learning Using Surrogate Models. (arXiv:2303.14582v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1">Dongyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1">Huy L. Nguyen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongyang R. Zhang</a></p>
<p>Multitask learning is widely used in practice to train a low-resource target
task by augmenting it with multiple related source tasks. Yet, naively
combining all the source tasks with a target task does not always improve the
prediction performance for the target task due to negative transfers. Thus, a
critical problem in multitask learning is identifying subsets of source tasks
that would benefit the target task. This problem is computationally challenging
since the number of subsets grows exponentially with the number of source
tasks; efficient heuristics for subset selection do not always capture the
relationship between task subsets and multitask learning performances. In this
paper, we introduce an efficient procedure to address this problem via
surrogate modeling. In surrogate modeling, we sample (random) subsets of source
tasks and precompute their multitask learning performances. Then, we
approximate the precomputed performances with a linear regression model that
can also predict the multitask performance of unseen task subsets. We show
theoretically and empirically that fitting this model only requires sampling
linearly many subsets in the number of source tasks. The fitted model provides
a relevance score between each source and target task. We use the relevance
scores to perform subset selection for multitask learning by thresholding.
Through extensive experiments, we show that our approach predicts negative
transfers from multiple source tasks to target tasks much more accurately than
existing task affinity measures. Additionally, we demonstrate that for several
weak supervision datasets, our approach consistently improves upon existing
optimization methods for multitask learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.04027">NeBLa: Neural Beer-Lambert for 3D Reconstruction of Oral Structures from Panoramic Radiographs. (arXiv:2304.04027v5 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1">Sihwa Park</a>, <a href="http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1">Seongjun Kim</a>, <a href="http://arxiv.org/find/eess/1/au:+Kwon_D/0/1/0/all/0/1">Doeyoung Kwon</a>, <a href="http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1">Yohan Jang</a>, <a href="http://arxiv.org/find/eess/1/au:+Song_I/0/1/0/all/0/1">In-Seok Song</a>, <a href="http://arxiv.org/find/eess/1/au:+Baek_S/0/1/0/all/0/1">Seungjun Baek</a></p>
<p>Panoramic radiography (Panoramic X-ray, PX) is a widely used imaging modality
for dental examination. However, PX only provides a flattened 2D image, lacking
in a 3D view of the oral structure. In this paper, we propose NeBLa (Neural
Beer-Lambert) to estimate 3D oral structures from real-world PX. NeBLa tackles
full 3D reconstruction for varying subjects (patients) where each
reconstruction is based only on a single panoramic image. We create an
intermediate representation called simulated PX (SimPX) from 3D Cone-beam
computed tomography (CBCT) data based on the Beer-Lambert law of X-ray
rendering and rotational principles of PX imaging. SimPX aims at not only
truthfully simulating PX, but also facilitates the reverting process back to 3D
data. We propose a novel neural model based on ray tracing which exploits both
global and local input features to convert SimPX to 3D output. At inference, a
real PX image is translated to a SimPX-style image with semantic
regularization, and the translated image is processed by generation module to
produce high-quality outputs. Experiments show that NeBLa outperforms prior
state-of-the-art in reconstruction tasks both quantitatively and qualitatively.
Unlike prior methods, NeBLa does not require any prior information such as the
shape of dental arches, nor the matched PX-CBCT dataset for training, which is
difficult to obtain in clinical practice. Our code is available at
https://github.com/sihwa-park/nebla.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05430">Transfer Learning Across Heterogeneous Features For Efficient Tensor Program Generation. (arXiv:2304.05430v2 [cs.PL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Verma_G/0/1/0/all/0/1">Gaurav Verma</a>, <a href="http://arxiv.org/find/cs/1/au:+Raskar_S/0/1/0/all/0/1">Siddhisanket Raskar</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1">Zhen Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1">Abid M Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Emani_M/0/1/0/all/0/1">Murali Emani</a>, <a href="http://arxiv.org/find/cs/1/au:+Chapman_B/0/1/0/all/0/1">Barbara Chapman</a></p>
<p>Tuning tensor program generation involves searching for various possible
program transformation combinations for a given program on target hardware to
optimize the tensor program execution. It is already a complex process because
of the massive search space and exponential combinations of transformations
make auto-tuning tensor program generation more challenging, especially when we
have a heterogeneous target. In this research, we attempt to address these
problems by learning the joint neural network and hardware features and
transferring them to the new target hardware. We extensively study the existing
state-of-the-art dataset, TenSet, perform comparative analysis on the test
split strategies and propose methodologies to prune the dataset. We adopt an
attention-inspired approach for tuning the tensor programs enabling them to
embed neural network and hardware-specific features. Our approach could prune
the dataset up to 45\% of the baseline without compromising the Pairwise
Comparison Accuracy (PCA). Further, the proposed methodology can achieve on-par
or improved mean inference time with 25%-40% of the baseline tuning time across
different networks and target hardware.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.05977">ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. (arXiv:2304.05977v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jiazheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuchen Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_Y/0/1/0/all/0/1">Yuxuan Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qinkai Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1">Ming Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1">Jie Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuxiao Dong</a></p>
<p>We present a comprehensive solution to learn and improve text-to-image models
from human preference feedback. To begin with, we build ImageReward -- the
first general-purpose text-to-image human preference reward model -- to
effectively encode human preferences. Its training is based on our systematic
annotation pipeline including rating and ranking, which collects 137k expert
comparisons to date. In human evaluation, ImageReward outperforms existing
scoring models and metrics, making it a promising automatic metric for
evaluating text-to-image synthesis. On top of it, we propose Reward Feedback
Learning (ReFL), a direct tuning algorithm to optimize diffusion models against
a scorer. Both automatic and human evaluation support ReFL's advantages over
compared methods. All code and datasets are provided at
\url{https://github.com/THUDM/ImageReward}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.08818">Align your Latents: High-Resolution Video Synthesis with Latent Diffusion Models. (arXiv:2304.08818v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Blattmann_A/0/1/0/all/0/1">Andreas Blattmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1">Robin Rombach</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1">Huan Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Dockhorn_T/0/1/0/all/0/1">Tim Dockhorn</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Seung Wook Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Fidler_S/0/1/0/all/0/1">Sanja Fidler</a>, <a href="http://arxiv.org/find/cs/1/au:+Kreis_K/0/1/0/all/0/1">Karsten Kreis</a></p>
<p>Latent Diffusion Models (LDMs) enable high-quality image synthesis while
avoiding excessive compute demands by training a diffusion model in a
compressed lower-dimensional latent space. Here, we apply the LDM paradigm to
high-resolution video generation, a particularly resource-intensive task. We
first pre-train an LDM on images only; then, we turn the image generator into a
video generator by introducing a temporal dimension to the latent space
diffusion model and fine-tuning on encoded image sequences, i.e., videos.
Similarly, we temporally align diffusion model upsamplers, turning them into
temporally consistent video super resolution models. We focus on two relevant
real-world applications: Simulation of in-the-wild driving data and creative
content creation with text-to-video modeling. In particular, we validate our
Video LDM on real driving videos of resolution 512 x 1024, achieving
state-of-the-art performance. Furthermore, our approach can easily leverage
off-the-shelf pre-trained image LDMs, as we only need to train a temporal
alignment model in that case. Doing so, we turn the publicly available,
state-of-the-art text-to-image LDM Stable Diffusion into an efficient and
expressive text-to-video model with resolution up to 1280 x 2048. We show that
the temporal layers trained in this way generalize to different fine-tuned
text-to-image LDMs. Utilizing this property, we show the first results for
personalized text-to-video generation, opening exciting directions for future
content creation. Project page:
https://research.nvidia.com/labs/toronto-ai/VideoLDM/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.09870">Heterogeneous-Agent Reinforcement Learning. (arXiv:2304.09870v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yifan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1">Jakub Grudzien Kuba</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1">Xidong Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1">Siyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1">Jiaming Ji</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yaodong Yang</a></p>
<p>The necessity for cooperation among intelligent machines has popularised
cooperative multi-agent reinforcement learning (MARL) in AI research. However,
many research endeavours heavily rely on parameter sharing among agents, which
confines them to only homogeneous-agent setting and leads to training
instability and lack of convergence guarantees. To achieve effective
cooperation in the general heterogeneous-agent setting, we propose
Heterogeneous-Agent Reinforcement Learning (HARL) algorithms that resolve the
aforementioned issues. Central to our findings are the multi-agent advantage
decomposition lemma and the sequential update scheme. Based on these, we
develop the provably correct Heterogeneous-Agent Trust Region Learning (HATRL),
and derive HATRPO and HAPPO by tractable approximations. Furthermore, we
discover a novel framework named Heterogeneous-Agent Mirror Learning (HAML),
which strengthens theoretical guarantees for HATRPO and HAPPO and provides a
general template for cooperative MARL algorithmic designs. We prove that all
algorithms derived from HAML inherently enjoy monotonic improvement of joint
return and convergence to Nash Equilibrium. As its natural outcome, HAML
validates more novel algorithms in addition to HATRPO and HAPPO, including
HAA2C, HADDPG, and HATD3, which generally outperform their existing
MA-counterparts. We comprehensively test HARL algorithms on six challenging
benchmarks and demonstrate their superior effectiveness and stability for
coordinating heterogeneous agents compared to strong baselines such as MAPPO
and QMIX.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.14530">Generating images of rare concepts using pre-trained diffusion models. (arXiv:2304.14530v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Samuel_D/0/1/0/all/0/1">Dvir Samuel</a>, <a href="http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1">Rami Ben-Ari</a>, <a href="http://arxiv.org/find/cs/1/au:+Raviv_S/0/1/0/all/0/1">Simon Raviv</a>, <a href="http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1">Nir Darshan</a>, <a href="http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1">Gal Chechik</a></p>
<p>Text-to-image diffusion models can synthesize high-quality images, but they
have various limitations. Here we highlight a common failure mode of these
models, namely, generating uncommon concepts and structured concepts like hand
palms. We show that their limitation is partly due to the long-tail nature of
their training data: web-crawled data sets are strongly unbalanced, causing
models to under-represent concepts from the tail of the distribution. We
characterize the effect of unbalanced training data on text-to-image models and
offer a remedy. We show that rare concepts can be correctly generated by
carefully selecting suitable generation seeds in the noise space, using a small
reference set of images, a technique that we call SeedSelect. SeedSelect does
not require retraining or finetuning the diffusion model. We assess the
faithfulness, quality and diversity of SeedSelect in creating rare objects and
generating complex formations like hand images, and find it consistently
achieves superior performance. We further show the advantage of SeedSelect in
semantic data augmentation. Generating semantically appropriate images can
successfully improve performance in few-shot recognition benchmarks, for
classes from the head and from the tail of the training data of diffusion
models
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03236">A Survey on Out-of-Distribution Detection in NLP. (arXiv:2305.03236v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1">Hao Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1">Yinhe Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yixuan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a></p>
<p>Out-of-distribution (OOD) detection is essential for the reliable and safe
deployment of machine learning systems in the real world. Great progress has
been made over the past years. This paper presents the first review of recent
advances in OOD detection with a particular focus on natural language
processing approaches. First, we provide a formal definition of OOD detection
and discuss several related fields. We then categorize recent algorithms into
three classes according to the data they used: (1) OOD data available, (2) OOD
data unavailable + in-distribution (ID) label available, and (3) OOD data
unavailable + ID label unavailable. Third, we introduce datasets, applications,
and metrics. Finally, we summarize existing work and present potential future
research topics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08559">Designing Discontinuities. (arXiv:2305.08559v3 [cs.IT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ferwana_I/0/1/0/all/0/1">Ibtihal Ferwana</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1">Suyoung Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1">Ting-Yi Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1">Lav R. Varshney</a></p>
<p>Discontinuities can be fairly arbitrary but also cause a significant impact
on outcomes in larger systems. Indeed, their arbitrariness is why they have
been used to infer causal relationships among variables in numerous settings.
Regression discontinuity from econometrics assumes the existence of a
discontinuous variable that splits the population into distinct partitions to
estimate the causal effects of a given phenomenon. Here we consider the design
of partitions for a given discontinuous variable to optimize a certain effect
previously studied using regression discontinuity. To do so, we propose a
quantization-theoretic approach to optimize the effect of interest, first
learning the causal effect size of a given discontinuous variable and then
applying dynamic programming for optimal quantization design of discontinuities
to balance the gain and loss in that effect size. We also develop a
computationally-efficient reinforcement learning algorithm for the dynamic
programming formulation of optimal quantization. We demonstrate our approach by
designing optimal time zone borders for counterfactuals of social capital,
social mobility, and health. This is based on regression discontinuity analyses
we perform on novel data, which may be of independent empirical interest.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.08777">Question-Answering System Extracts Information on Injection Drug Use from Clinical Notes. (arXiv:2305.08777v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mahbub_M/0/1/0/all/0/1">Maria Mahbub</a>, <a href="http://arxiv.org/find/cs/1/au:+Goethert_I/0/1/0/all/0/1">Ian Goethert</a>, <a href="http://arxiv.org/find/cs/1/au:+Danciu_I/0/1/0/all/0/1">Ioana Danciu</a>, <a href="http://arxiv.org/find/cs/1/au:+Knight_K/0/1/0/all/0/1">Kathryn Knight</a>, <a href="http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1">Sudarshan Srinivasan</a>, <a href="http://arxiv.org/find/cs/1/au:+Tamang_S/0/1/0/all/0/1">Suzanne Tamang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rozenberg_Ben_Dror_K/0/1/0/all/0/1">Karine Rozenberg-Ben-Dror</a>, <a href="http://arxiv.org/find/cs/1/au:+Solares_H/0/1/0/all/0/1">Hugo Solares</a>, <a href="http://arxiv.org/find/cs/1/au:+Martins_S/0/1/0/all/0/1">Susana Martins</a>, <a href="http://arxiv.org/find/cs/1/au:+Trafton_J/0/1/0/all/0/1">Jodie Trafton</a>, <a href="http://arxiv.org/find/cs/1/au:+Begoli_E/0/1/0/all/0/1">Edmon Begoli</a>, <a href="http://arxiv.org/find/cs/1/au:+Peterson_G/0/1/0/all/0/1">Gregory Peterson</a></p>
<p>Background: Injection drug use (IDU) is a dangerous health behavior that
increases mortality and morbidity. Identifying IDU early and initiating harm
reduction interventions can benefit individuals at risk. However, extracting
IDU behaviors from patients' electronic health records (EHR) is difficult
because there is no International Classification of Disease (ICD) code and the
only place IDU information can be indicated is unstructured free-text clinical
notes. Although natural language processing can efficiently extract this
information from unstructured data, there are no validated tools. Methods: To
address this gap in clinical information, we design and demonstrate a
question-answering (QA) framework to extract information on IDU from clinical
notes. Our framework involves two main steps: (1) generating a gold-standard QA
dataset and (2) developing and testing the QA model. We utilize 2323 clinical
notes of 1145 patients sourced from the VA Corporate Data Warehouse to
construct the gold-standard dataset for developing and evaluating the QA model.
We also demonstrate the QA model's ability to extract IDU-related information
on temporally out-of-distribution data. Results: Here we show that for a strict
match between gold-standard and predicted answers, the QA model achieves 51.65%
F1 score. For a relaxed match between the gold-standard and predicted answers,
the QA model obtains 78.03% F1 score, along with 85.38% Precision and 79.02%
Recall scores. Moreover, the QA model demonstrates consistent performance when
subjected to temporally out-of-distribution data. Conclusions: Our study
introduces a QA framework designed to extract IDU information from clinical
notes, aiming to enhance the accurate and efficient detection of people who
inject drugs, extract relevant information, and ultimately facilitate informed
patient care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.09480">Cross-Gate MLP with Protein Complex Invariant Embedding is A One-Shot Antibody Designer. (arXiv:2305.09480v4 [q-bio.BM] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Tan_C/0/1/0/all/0/1">Cheng Tan</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Gao_Z/0/1/0/all/0/1">Zhangyang Gao</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Wu_L/0/1/0/all/0/1">Lirong Wu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Xia_J/0/1/0/all/0/1">Jun Xia</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Zheng_J/0/1/0/all/0/1">Jiangbin Zheng</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Yang_X/0/1/0/all/0/1">Xihong Yang</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Liu_Y/0/1/0/all/0/1">Yue Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Hu_B/0/1/0/all/0/1">Bozhen Hu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1">Stan Z. Li</a></p>
<p>Antibodies are crucial proteins produced by the immune system in response to
foreign substances or antigens. The specificity of an antibody is determined by
its complementarity-determining regions (CDRs), which are located in the
variable domains of the antibody chains and form the antigen-binding site.
Previous studies have utilized complex techniques to generate CDRs, but they
suffer from inadequate geometric modeling. Moreover, the common iterative
refinement strategies lead to an inefficient inference. In this paper, we
propose a \textit{simple yet effective} model that can co-design 1D sequences
and 3D structures of CDRs in a one-shot manner. To achieve this, we decouple
the antibody CDR design problem into two stages: (i) geometric modeling of
protein complex structures and (ii) sequence-structure co-learning. We develop
a novel macromolecular structure invariant embedding, typically for protein
complexes, that captures both intra- and inter-component interactions among the
backbone atoms, including C$\alpha$, N, C, and O atoms, to achieve
comprehensive geometric modeling. Then, we introduce a simple cross-gate MLP
for sequence-structure co-learning, allowing sequence and structure
representations to implicitly refine each other. This enables our model to
design desired sequences and structures in a one-shot manner. Extensive
experiments are conducted to evaluate our results at both the sequence and
structure levels, which demonstrate that our model achieves superior
performance compared to the state-of-the-art antibody CDR design methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12066">Multi-Task Models Adversarial Attacks. (arXiv:2305.12066v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lijun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahmood_K/0/1/0/all/0/1">Kaleel Mahmood</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1">Caiwen Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1">Hui Guan</a></p>
<p>Multi-Task Learning (MTL) involves developing a singular model, known as a
multi-task model, to concurrently perform multiple tasks. While the security of
single-task models has been thoroughly studied, multi-task models pose several
critical security questions, such as 1) their vulnerability to single-task
adversarial attacks, 2) the possibility of designing attacks that target
multiple tasks, and 3) the impact of task sharing and adversarial training on
their resilience to such attacks. This paper addresses these queries through
detailed analysis and rigorous experimentation. First, we explore the
adaptation of single-task white-box attacks to multi-task models and identify
their limitations. We then introduce a novel attack framework, the Gradient
Balancing Multi-Task Attack (GB-MTA), which treats attacking a multi-task model
as an optimization problem. This problem, based on averaged relative loss
change across tasks, is approximated as an integer linear programming problem.
Extensive evaluations on MTL benchmarks, NYUv2 and Tiny-Taxonomy, demonstrate
GB-MTA's effectiveness against both standard and adversarially trained
multi-task models. The results also highlight a trade-off between task accuracy
improvement via parameter sharing and increased model vulnerability due to
enhanced attack transferability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.12467">Understanding Multi-phase Optimization Dynamics and Rich Nonlinear Behaviors of ReLU Networks. (arXiv:2305.12467v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1">Mingze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chao Ma</a></p>
<p>The training process of ReLU neural networks often exhibits complicated
nonlinear phenomena. The nonlinearity of models and non-convexity of loss pose
significant challenges for theoretical analysis. Therefore, most previous
theoretical works on the optimization dynamics of neural networks focus either
on local analysis (like the end of training) or approximate linear models (like
Neural Tangent Kernel). In this work, we conduct a complete theoretical
characterization of the training process of a two-layer ReLU network trained by
Gradient Flow on a linearly separable data. In this specific setting, our
analysis captures the whole optimization process starting from random
initialization to final convergence. Despite the relatively simple model and
data that we studied, we reveal four different phases from the whole training
process showing a general simplifying-to-complicating learning trend. Specific
nonlinear behaviors can also be precisely identified and captured
theoretically, such as initial condensation, saddle-to-plateau dynamics,
plateau escape, changes of activation patterns, learning with increasing
complexity, etc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.13033">Towards generalizing deep-audio fake detection networks. (arXiv:2305.13033v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gasenzer_K/0/1/0/all/0/1">Konstantin Gasenzer</a> (1), <a href="http://arxiv.org/find/cs/1/au:+Wolter_M/0/1/0/all/0/1">Moritz Wolter</a> (1) ((1) High Performance Computing and Analytics Lab, Universit&#xe4;t Bonn, Germany)</p>
<p>Today's generative neural networks allow the creation of high-quality
synthetic speech at scale. While we welcome the creative use of this new
technology, we must also recognize the risks. As synthetic speech is abused for
monetary and identity theft, we require a broad set of deepfake identification
tools. Furthermore, previous work reported a limited ability of deep
classifiers to generalize to unseen audio generators. We study the frequency
domain fingerprints of current audio generators. Building on top of the
discovered frequency footprints, we train excellent lightweight detectors that
generalize. We report improved results on the WaveFake dataset and an extended
version. To account for the rapid progress in the field, we extend the WaveFake
dataset by additionally considering samples drawn from the novel Avocodo and
BigVGAN networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19082">Embedding Inequalities for Barron-type Spaces. (arXiv:2305.19082v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Wu_L/0/1/0/all/0/1">Lei Wu</a></p>
<p>An important problem in machine learning theory is to understand the
approximation and generalization properties of two-layer neural networks in
high dimensions. To this end, researchers have introduced the Barron space
$\mathcal{B}_s(\Omega)$ and the spectral Barron space $\mathcal{F}_s(\Omega)$,
where the index $s\in [0,\infty)$ indicates the smoothness of functions within
these spaces and $\Omega\subset\mathbb{R}^d$ denotes the input domain. However,
the precise relationship between the two types of Barron spaces remains
unclear. In this paper, we establish a continuous embedding between them as
implied by the following inequality: for any $\delta\in (0,1), s\in
\mathbb{N}^{+}$ and $f: \Omega \mapsto\mathbb{R}$, it holds that \[ \delta
\|f\|_{\mathcal{F}_{s-\delta}(\Omega)}\lesssim_s
\|f\|_{\mathcal{B}_s(\Omega)}\lesssim_s \|f\|_{\mathcal{F}_{s+1}(\Omega)}. \]
</p>
<p>Importantly, the constants do not depend on the input dimension $d$,
suggesting that the embedding is effective in high dimensions. Moreover, we
also show that the lower and upper bound are both tight.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19442">SimFBO: Towards Simple, Flexible and Communication-efficient Federated Bilevel Learning. (arXiv:2305.19442v5 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1">Peiyao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1">Kaiyi Ji</a></p>
<p>Federated bilevel optimization (FBO) has shown great potential recently in
machine learning and edge computing due to the emerging nested optimization
structure in meta-learning, fine-tuning, hyperparameter tuning, etc. However,
existing FBO algorithms often involve complicated computations and require
multiple sub-loops per iteration, each of which contains a number of
communication rounds. In this paper, we propose a simple and flexible FBO
framework named SimFBO, which is easy to implement without sub-loops, and
includes a generalized server-side aggregation and update for improving
communication efficiency. We further propose System-level heterogeneity robust
FBO (ShroFBO) as a variant of SimFBO with stronger resilience to heterogeneous
local computation. We show that SimFBO and ShroFBO provably achieve a linear
convergence speedup with partial client participation and client sampling
without replacement, as well as improved sample and communication complexities.
Experiments demonstrate the effectiveness of the proposed methods over existing
FBO algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.03502">Russo-Ukrainian War: Prediction and explanation of Twitter suspension. (arXiv:2306.03502v2 [cs.SI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shevtsov_A/0/1/0/all/0/1">Alexander Shevtsov</a>, <a href="http://arxiv.org/find/cs/1/au:+Antonakaki_D/0/1/0/all/0/1">Despoina Antonakaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Lamprou_I/0/1/0/all/0/1">Ioannis Lamprou</a>, <a href="http://arxiv.org/find/cs/1/au:+Kontogiorgakis_I/0/1/0/all/0/1">Ioannis Kontogiorgakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratikakis_P/0/1/0/all/0/1">Polyvios Pratikakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Ioannidis_S/0/1/0/all/0/1">Sotiris Ioannidis</a></p>
<p>On 24 February 2022, Russia invaded Ukraine, starting what is now known as
the Russo-Ukrainian War, initiating an online discourse on social media.
Twitter as one of the most popular SNs, with an open and democratic character,
enables a transparent discussion among its large user base. Unfortunately, this
often leads to Twitter's policy violations, propaganda, abusive actions, civil
integrity violation, and consequently to user accounts' suspension and
deletion. This study focuses on the Twitter suspension mechanism and the
analysis of shared content and features of the user accounts that may lead to
this. Toward this goal, we have obtained a dataset containing 107.7M tweets,
originating from 9.8 million users, using Twitter API. We extract the
categories of shared content of the suspended accounts and explain their
characteristics, through the extraction of text embeddings in junction with
cosine similarity clustering. Our results reveal scam campaigns taking
advantage of trending topics regarding the Russia-Ukrainian conflict for
Bitcoin and Ethereum fraud, spam, and advertisement campaigns. Additionally, we
apply a machine learning methodology including a SHapley Additive
explainability model to understand and explain how user accounts get suspended.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.04047">CAVEN: An Embodied Conversational Agent for Efficient Audio-Visual Navigation in Noisy Environments. (arXiv:2306.04047v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiulong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_S/0/1/0/all/0/1">Sudipta Paul</a>, <a href="http://arxiv.org/find/cs/1/au:+Chatterjee_M/0/1/0/all/0/1">Moitreya Chatterjee</a>, <a href="http://arxiv.org/find/cs/1/au:+Cherian_A/0/1/0/all/0/1">Anoop Cherian</a></p>
<p>Audio-visual navigation of an agent towards locating an audio goal is a
challenging task especially when the audio is sporadic or the environment is
noisy. In this paper, we present CAVEN, a Conversation-based Audio-Visual
Embodied Navigation framework in which the agent may interact with a
human/oracle for solving the task of navigating to an audio goal. Specifically,
CAVEN is modeled as a budget-aware partially observable semi-Markov decision
process that implicitly learns the uncertainty in the audio-based navigation
policy to decide when and how the agent may interact with the oracle. Our CAVEN
agent can engage in fully-bidirectional natural language conversations by
producing relevant questions and interpret free-form, potentially noisy
responses from the oracle based on the audio-visual context. To enable such a
capability, CAVEN is equipped with: (i) a trajectory forecasting network that
is grounded in audio-visual cues to produce a potential trajectory to the
estimated goal, and (ii) a natural language based question generation and
reasoning network to pose an interactive question to the oracle or interpret
the oracle's response to produce navigation instructions. To train the
interactive modules, we present a large scale dataset: AVN-Instruct, based on
the Landmark-RxR dataset. To substantiate the usefulness of conversations, we
present experiments on the benchmark audio-goal task using the SoundSpaces
simulator under various noisy settings. Our results reveal that our
fully-conversational approach leads to nearly an order-of-magnitude improvement
in success rate, especially in localizing new sound sources and against methods
that only use uni-directional interaction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.11203">AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator. (arXiv:2306.11203v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Smyers_E/0/1/0/all/0/1">Elysia Q. Smyers</a>, <a href="http://arxiv.org/find/cs/1/au:+Katz_S/0/1/0/all/0/1">Sydney M. Katz</a>, <a href="http://arxiv.org/find/cs/1/au:+Corso_A/0/1/0/all/0/1">Anthony L. Corso</a>, <a href="http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1">Mykel J. Kochenderfer</a></p>
<p>Designing robust machine learning systems remains an open problem, and there
is a need for benchmark problems that cover both environmental changes and
evaluation on a downstream task. In this work, we introduce AVOIDDS, a
realistic object detection benchmark for the vision-based aircraft
detect-and-avoid problem. We provide a labeled dataset consisting of 72,000
photorealistic images of intruder aircraft with various lighting conditions,
weather conditions, relative geometries, and geographic locations. We also
provide an interface that evaluates trained models on slices of this dataset to
identify changes in performance with respect to changing environmental
conditions. Finally, we implement a fully-integrated, closed-loop simulator of
the vision-based detect-and-avoid problem to evaluate trained models with
respect to the downstream collision avoidance task. This benchmark will enable
further research in the design of robust machine learning systems for use in
safety-critical applications. The AVOIDDS dataset and code are publicly
available at https://purl.stanford.edu/hj293cv5980 and
https://github.com/sisl/VisionBasedAircraftDAA respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.14731">Leveraging Locality and Robustness to Achieve Massively Scalable Gaussian Process Regression. (arXiv:2306.14731v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Allison_R/0/1/0/all/0/1">Robert Allison</a>, <a href="http://arxiv.org/find/stat/1/au:+Stephenson_A/0/1/0/all/0/1">Anthony Stephenson</a>, <a href="http://arxiv.org/find/stat/1/au:+F_S/0/1/0/all/0/1">Samuel F</a>, <a href="http://arxiv.org/find/stat/1/au:+Pyzer_Knapp_E/0/1/0/all/0/1">Edward Pyzer-Knapp</a></p>
<p>The accurate predictions and principled uncertainty measures provided by GP
regression incur O(n^3) cost which is prohibitive for modern-day large-scale
applications. This has motivated extensive work on computationally efficient
approximations. We introduce a new perspective by exploring robustness
properties and limiting behaviour of GP nearest-neighbour (GPnn) prediction. We
demonstrate through theory and simulation that as the data-size n increases,
accuracy of estimated parameters and GP model assumptions become increasingly
irrelevant to GPnn predictive accuracy. Consequently, it is sufficient to spend
small amounts of work on parameter estimation in order to achieve high MSE
accuracy, even in the presence of gross misspecification. In contrast, as n
tends to infinity, uncertainty calibration and NLL are shown to remain
sensitive to just one parameter, the additive noise-variance; but we show that
this source of inaccuracy can be corrected for, thereby achieving both
well-calibrated uncertainty measures and accurate predictions at remarkably low
computational cost. We exhibit a very simple GPnn regression algorithm with
stand-out performance compared to other state-of-the-art GP approximations as
measured on large UCI datasets. It operates at a small fraction of those other
methods' training costs, for example on a basic laptop taking about 30 seconds
to train on a dataset of size n = 1.6 x 10^6.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17052">Safe Model-Based Multi-Agent Mean-Field Reinforcement Learning. (arXiv:2306.17052v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jusup_M/0/1/0/all/0/1">Matej Jusup</a>, <a href="http://arxiv.org/find/cs/1/au:+Pasztor_B/0/1/0/all/0/1">Barna P&#xe1;sztor</a>, <a href="http://arxiv.org/find/cs/1/au:+Janik_T/0/1/0/all/0/1">Tadeusz Janik</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1">Kenan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Corman_F/0/1/0/all/0/1">Francesco Corman</a>, <a href="http://arxiv.org/find/cs/1/au:+Krause_A/0/1/0/all/0/1">Andreas Krause</a>, <a href="http://arxiv.org/find/cs/1/au:+Bogunovic_I/0/1/0/all/0/1">Ilija Bogunovic</a></p>
<p>Many applications, e.g., in shared mobility, require coordinating a large
number of agents. Mean-field reinforcement learning addresses the resulting
scalability challenge by optimizing the policy of a representative agent
interacting with the infinite population of identical agents instead of
considering individual pairwise interactions. In this paper, we address an
important generalization where there exist global constraints on the
distribution of agents (e.g., requiring capacity constraints or minimum
coverage requirements to be met). We propose Safe-M$^3$-UCRL, the first
model-based mean-field reinforcement learning algorithm that attains safe
policies even in the case of unknown transitions. As a key ingredient, it uses
epistemic uncertainty in the transition model within a log-barrier approach to
ensure pessimistic constraints satisfaction with high probability. Beyond the
synthetic swarm motion benchmark, we showcase Safe-M$^3$-UCRL on the vehicle
repositioning problem faced by many shared mobility operators and evaluate its
performance through simulations built on vehicle trajectory data from a service
provider in Shenzhen. Our algorithm effectively meets the demand in critical
areas while ensuring service accessibility in regions with low demand.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17648">Enhancing training of physics-informed neural networks using domain-decomposition based preconditioning strategies. (arXiv:2306.17648v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Kopanicakova_A/0/1/0/all/0/1">Alena Kopani&#x10d;&#xe1;kov&#xe1;</a>, <a href="http://arxiv.org/find/math/1/au:+Kothari_H/0/1/0/all/0/1">Hardik Kothari</a>, <a href="http://arxiv.org/find/math/1/au:+Karniadakis_G/0/1/0/all/0/1">George Em Karniadakis</a>, <a href="http://arxiv.org/find/math/1/au:+Krause_R/0/1/0/all/0/1">Rolf Krause</a></p>
<p>We propose to enhance the training of physics-informed neural networks
(PINNs). To this aim, we introduce nonlinear additive and multiplicative
preconditioning strategies for the widely used L-BFGS optimizer. The nonlinear
preconditioners are constructed by utilizing the Schwarz domain-decomposition
framework, where the parameters of the network are decomposed in a layer-wise
manner. Through a series of numerical experiments, we demonstrate that both,
additive and multiplicative preconditioners significantly improve the
convergence of the standard L-BFGS optimizer, while providing more accurate
solutions of the underlying partial differential equations. Moreover, the
additive preconditioner is inherently parallel, thus giving rise to a novel
approach to model parallelism.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17778">Look, Remember and Reason: Grounded reasoning in videos with language models. (arXiv:2306.17778v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bhattacharyya_A/0/1/0/all/0/1">Apratim Bhattacharyya</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchal_S/0/1/0/all/0/1">Sunny Panchal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1">Mingu Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Pourreza_R/0/1/0/all/0/1">Reza Pourreza</a>, <a href="http://arxiv.org/find/cs/1/au:+Madan_P/0/1/0/all/0/1">Pulkit Madan</a>, <a href="http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1">Roland Memisevic</a></p>
<p>Multi-modal language models (LM) have recently shown promising performance in
high-level reasoning tasks on videos. However, existing methods still fall
short in tasks like causal or compositional spatiotemporal reasoning over
actions, in which model predictions need to be grounded in fine-grained
low-level details, such as object motions and object interactions. In this
work, we propose training an LM end-to-end on low-level surrogate tasks,
including object detection, re-identification, and tracking, to endow the model
with the required low-level visual capabilities. We show that a two-stream
video encoder with spatiotemporal attention is effective at capturing the
required static and motion-based cues in the video. By leveraging the LM's
ability to perform the low-level surrogate tasks, we can cast reasoning in
videos as the three-step process of Look, Remember, Reason wherein visual
information is extracted using low-level visual skills step-by-step and then
integrated to arrive at a final answer. We demonstrate the effectiveness of our
framework on diverse visual reasoning tasks from the ACRE, CATER, and
Something-Else datasets. Our approach is trainable end-to-end and surpasses
state-of-the-art task-specific methods across these tasks by a large margin.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06104">Deep learning for dynamic graphs: models and benchmarks. (arXiv:2307.06104v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gravina_A/0/1/0/all/0/1">Alessio Gravina</a>, <a href="http://arxiv.org/find/cs/1/au:+Bacciu_D/0/1/0/all/0/1">Davide Bacciu</a></p>
<p>Recent progress in research on Deep Graph Networks (DGNs) has led to a
maturation of the domain of learning on graphs. Despite the growth of this
research field, there are still important challenges that are yet unsolved.
Specifically, there is an urge of making DGNs suitable for predictive tasks on
realworld systems of interconnected entities, which evolve over time. With the
aim of fostering research in the domain of dynamic graphs, at first, we survey
recent advantages in learning both temporal and spatial information, providing
a comprehensive overview of the current state-of-the-art in the domain of
representation learning for dynamic graphs. Secondly, we conduct a fair
performance comparison among the most popular proposed approaches on node and
edge-level tasks, leveraging rigorous model selection and assessment for all
the methods, thus establishing a sound baseline for evaluating new
architectures and approaches
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.09259">Adaptive Topological Feature via Persistent Homology: Filtration Learning for Point Clouds. (arXiv:2307.09259v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nishikawa_N/0/1/0/all/0/1">Naoki Nishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ike_Y/0/1/0/all/0/1">Yuichi Ike</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamanishi_K/0/1/0/all/0/1">Kenji Yamanishi</a></p>
<p>Machine learning for point clouds has been attracting much attention, with
many applications in various fields, such as shape recognition and material
science. For enhancing the accuracy of such machine learning methods, it is
often effective to incorporate global topological features, which are typically
extracted by persistent homology. In the calculation of persistent homology for
a point cloud, we choose a filtration for the point cloud, an increasing
sequence of spaces. Since the performance of machine learning methods combined
with persistent homology is highly affected by the choice of a filtration, we
need to tune it depending on data and tasks. In this paper, we propose a
framework that learns a filtration adaptively with the use of neural networks.
In order to make the resulting persistent homology isometry-invariant, we
develop a neural network architecture with such invariance. Additionally, we
show a theoretical result on a finite-dimensional approximation of filtration
functions, which justifies the proposed network architecture. Experimental
results demonstrated the efficacy of our framework in several classification
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.10422">PreDiff: Precipitation Nowcasting with Latent Diffusion Models. (arXiv:2307.10422v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zhihan Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xingjian Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1">Boran Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1">Xiaoyong Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Maddix_D/0/1/0/all/0/1">Danielle Maddix</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1">Yi Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1">Mu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuyang Wang</a></p>
<p>Earth system forecasting has traditionally relied on complex physical models
that are computationally expensive and require significant domain expertise. In
the past decade, the unprecedented increase in spatiotemporal Earth observation
data has enabled data-driven forecasting models using deep learning techniques.
These models have shown promise for diverse Earth system forecasting tasks but
either struggle with handling uncertainty or neglect domain-specific prior
knowledge, resulting in averaging possible futures to blurred forecasts or
generating physically implausible predictions. To address these limitations, we
propose a two-stage pipeline for probabilistic spatiotemporal forecasting: 1)
We develop PreDiff, a conditional latent diffusion model capable of
probabilistic forecasts. 2) We incorporate an explicit knowledge alignment
mechanism to align forecasts with domain-specific physical constraints. This is
achieved by estimating the deviation from imposed constraints at each denoising
step and adjusting the transition distribution accordingly. We conduct
empirical studies on two datasets: N-body MNIST, a synthetic dataset with
chaotic behavior, and SEVIR, a real-world precipitation nowcasting dataset.
Specifically, we impose the law of conservation of energy in N-body MNIST and
anticipated precipitation intensity in SEVIR. Experiments demonstrate the
effectiveness of PreDiff in handling uncertainty, incorporating domain-specific
prior knowledge, and generating forecasts that exhibit high operational
utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.11351">Bounded P-values in Parametric Programming-based Selective Inference. (arXiv:2307.11351v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Shiraishi_T/0/1/0/all/0/1">Tomohiro Shiraishi</a>, <a href="http://arxiv.org/find/stat/1/au:+Miwa_D/0/1/0/all/0/1">Daiki Miwa</a>, <a href="http://arxiv.org/find/stat/1/au:+Duy_V/0/1/0/all/0/1">Vo Nguyen Le Duy</a>, <a href="http://arxiv.org/find/stat/1/au:+Takeuchi_I/0/1/0/all/0/1">Ichiro Takeuchi</a></p>
<p>Selective inference (SI) has been actively studied as a promising framework
for statistical hypothesis testing for data-driven hypotheses. The basic idea
of SI is to make inferences conditional on an event that a hypothesis is
selected. In order to perform SI, this event must be characterized in a
traceable form. When selection event is too difficult to characterize,
additional conditions are introduced for tractability. This additional
conditions often causes the loss of power, and this issue is referred to as
over-conditioning in [Fithian et al., 2014]. Parametric programming-based SI
(PP-based SI) has been proposed as one way to address the over-conditioning
issue. The main problem of PP-based SI is its high computational cost due to
the need to exhaustively explore the data space. In this study, we introduce a
procedure to reduce the computational cost while guaranteeing the desired
precision, by proposing a method to compute the lower and upper bounds of
p-values. We also proposed three types of search strategies that efficiently
improve these bounds. We demonstrate the effectiveness of the proposed method
in hypothesis testing problems for feature selection in linear models and
attention region identification in deep neural networks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10253">StableLLaVA: Enhanced Visual Instruction Tuning with Synthesized Image-Dialogue Data. (arXiv:2308.10253v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanda Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Gang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhibin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1">Bin Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1">Guosheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Chunhua Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Ling Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1">Yunchao Wei</a></p>
<p>The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have
sparked significant interest in the development of multimodal Large Language
Models (LLMs). A primary research objective of such models is to align visual
and textual modalities effectively while comprehending human instructions.
Current methodologies often rely on annotations derived from benchmark datasets
to construct image-dialogue datasets for training purposes, akin to instruction
tuning in LLMs. However, these datasets often exhibit domain bias, potentially
constraining the generative capabilities of the models. In an effort to
mitigate these limitations, we propose a novel data collection methodology that
synchronously synthesizes images and dialogues for visual instruction tuning.
This approach harnesses the power of generative models, marrying the abilities
of ChatGPT and text-to-image generative models to yield a diverse and
controllable dataset with varied image content. Additionally, datasets can be
arbitrarily scaled. This not only provides greater flexibility compared to
existing methodologies but also significantly enhances several model
capabilities. Our research includes comprehensive experiments conducted on
various datasets. The results emphasize substantial enhancements in more than
ten commonly assessed capabilities. Additionally, our model achieves
state-of-the-art results across multiple widely recognized multimodal
benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11940">Audio Generation with Multiple Conditional Diffusion Model. (arXiv:2308.11940v4 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhifang Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1">Jianguo Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_R/0/1/0/all/0/1">Rui Tao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1">Long Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouchi_K/0/1/0/all/0/1">Kazushige Ouchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiangdong Wang</a></p>
<p>Text-based audio generation models have limitations as they cannot encompass
all the information in audio, leading to restricted controllability when
relying solely on text. To address this issue, we propose a novel model that
enhances the controllability of existing pre-trained text-to-audio models by
incorporating additional conditions including content (timestamp) and style
(pitch contour and energy contour) as supplements to the text. This approach
achieves fine-grained control over the temporal order, pitch, and energy of
generated audio. To preserve the diversity of generation, we employ a trainable
control condition encoder that is enhanced by a large language model and a
trainable Fusion-Net to encode and fuse the additional conditions while keeping
the weights of the pre-trained text-to-audio model frozen. Due to the lack of
suitable datasets and evaluation metrics, we consolidate existing datasets into
a new dataset comprising the audio and corresponding conditions and use a
series of evaluation metrics to evaluate the controllability performance.
Experimental results demonstrate that our model successfully achieves
fine-grained control to accomplish controllable audio generation. Audio samples
and our dataset are publicly available at
https://conditionaudiogen.github.io/conditionaudiogen/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13568">Region-Disentangled Diffusion Model for High-Fidelity PPG-to-ECG Translation. (arXiv:2308.13568v2 [eess.SP] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shome_D/0/1/0/all/0/1">Debaditya Shome</a>, <a href="http://arxiv.org/find/eess/1/au:+Sarkar_P/0/1/0/all/0/1">Pritam Sarkar</a>, <a href="http://arxiv.org/find/eess/1/au:+Etemad_A/0/1/0/all/0/1">Ali Etemad</a></p>
<p>The high prevalence of cardiovascular diseases (CVDs) calls for accessible
and cost-effective continuous cardiac monitoring tools. Despite
Electrocardiography (ECG) being the gold standard, continuous monitoring
remains a challenge, leading to the exploration of Photoplethysmography (PPG),
a promising but more basic alternative available in consumer wearables. This
notion has recently spurred interest in translating PPG to ECG signals. In this
work, we introduce Region-Disentangled Diffusion Model (RDDM), a novel
diffusion model designed to capture the complex temporal dynamics of ECG.
Traditional Diffusion models like Denoising Diffusion Probabilistic Models
(DDPM) face challenges in capturing such nuances due to the indiscriminate
noise addition process across the entire signal. Our proposed RDDM overcomes
such limitations by incorporating a novel forward process that selectively adds
noise to specific regions of interest (ROI) such as QRS complex in ECG signals,
and a reverse process that disentangles the denoising of ROI and non-ROI
regions. Quantitative experiments demonstrate that RDDM can generate
high-fidelity ECG from PPG in as few as 10 diffusion steps, making it highly
effective and computationally efficient. Additionally, to rigorously validate
the usefulness of the generated ECG signals, we introduce CardioBench, a
comprehensive evaluation benchmark for a variety of cardiac-related tasks
including heart rate and blood pressure estimation, stress classification, and
the detection of atrial fibrillation and diabetes. Our thorough experiments
show that RDDM achieves state-of-the-art performance on CardioBench. To the
best of our knowledge, RDDM is the first diffusion model for cross-modal
signal-to-signal translation in the bio-signal domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.14165">Distributional Off-Policy Evaluation for Slate Recommendations. (arXiv:2308.14165v2 [cs.IR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chaudhari_S/0/1/0/all/0/1">Shreyas Chaudhari</a>, <a href="http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1">David Arbour</a>, <a href="http://arxiv.org/find/cs/1/au:+Theocharous_G/0/1/0/all/0/1">Georgios Theocharous</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlassis_N/0/1/0/all/0/1">Nikos Vlassis</a></p>
<p>Recommendation strategies are typically evaluated by using previously logged
data, employing off-policy evaluation methods to estimate their expected
performance. However, for strategies that present users with slates of multiple
items, the resulting combinatorial action space renders many of these methods
impractical. Prior work has developed estimators that leverage the structure in
slates to estimate the expected off-policy performance, but the estimation of
the entire performance distribution remains elusive. Estimating the complete
distribution allows for a more comprehensive evaluation of recommendation
strategies, particularly along the axes of risk and fairness that employ
metrics computable from the distribution. In this paper, we propose an
estimator for the complete off-policy performance distribution for slates and
establish conditions under which the estimator is unbiased and consistent. This
builds upon prior work on off-policy evaluation for slates and off-policy
distribution estimation in reinforcement learning. We validate the efficacy of
our method empirically on synthetic data as well as on a slate recommendation
simulator constructed from real-world data (MovieLens-20M). Our results show a
significant reduction in estimation variance and improved sample efficiency
over prior work across a range of slate structures.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.15478">An Adaptive Tangent Feature Perspective of Neural Networks. (arXiv:2308.15478v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+LeJeune_D/0/1/0/all/0/1">Daniel LeJeune</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemohammad_S/0/1/0/all/0/1">Sina Alemohammad</a></p>
<p>In order to better understand feature learning in neural networks, we propose
a framework for understanding linear models in tangent feature space where the
features are allowed to be transformed during training. We consider linear
transformations of features, resulting in a joint optimization over parameters
and transformations with a bilinear interpolation constraint. We show that this
optimization problem has an equivalent linearly constrained optimization with
structured regularization that encourages approximately low rank solutions.
Specializing to neural network structure, we gain insights into how the
features and thus the kernel function change, providing additional nuance to
the phenomenon of kernel alignment when the target function is poorly
represented using tangent features. In addition to verifying our theoretical
observations in real neural networks on a simple regression problem, we
empirically show that an adaptive feature implementation of tangent feature
classification has an order of magnitude lower sample complexity than the fixed
tangent feature model on MNIST and CIFAR-10.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.02842">Random Postprocessing for Combinatorial Bayesian Optimization. (arXiv:2309.02842v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morita_K/0/1/0/all/0/1">Keisuke Morita</a>, <a href="http://arxiv.org/find/cs/1/au:+Nishikawa_Y/0/1/0/all/0/1">Yoshihiko Nishikawa</a>, <a href="http://arxiv.org/find/cs/1/au:+Ohzeki_M/0/1/0/all/0/1">Masayuki Ohzeki</a></p>
<p>Model-based sequential approaches to discrete "black-box" optimization,
including Bayesian optimization techniques, often access the same points
multiple times for a given objective function in interest, resulting in many
steps to find the global optimum. Here, we numerically study the effect of a
postprocessing method on Bayesian optimization that strictly prohibits
duplicated samples in the dataset. We find the postprocessing method
significantly reduces the number of sequential steps to find the global
optimum, especially when the acquisition function is of maximum a posterior
estimation. Our results provide a simple but general strategy to solve the slow
convergence of Bayesian optimization for high-dimensional problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08140">PromptTTS++: Controlling Speaker Identity in Prompt-Based Text-to-Speech Using Natural Language Descriptions. (arXiv:2309.08140v2 [eess.AS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Shimizu_R/0/1/0/all/0/1">Reo Shimizu</a>, <a href="http://arxiv.org/find/eess/1/au:+Yamamoto_R/0/1/0/all/0/1">Ryuichi Yamamoto</a>, <a href="http://arxiv.org/find/eess/1/au:+Kawamura_M/0/1/0/all/0/1">Masaya Kawamura</a>, <a href="http://arxiv.org/find/eess/1/au:+Shirahata_Y/0/1/0/all/0/1">Yuma Shirahata</a>, <a href="http://arxiv.org/find/eess/1/au:+Doi_H/0/1/0/all/0/1">Hironori Doi</a>, <a href="http://arxiv.org/find/eess/1/au:+Komatsu_T/0/1/0/all/0/1">Tatsuya Komatsu</a>, <a href="http://arxiv.org/find/eess/1/au:+Tachibana_K/0/1/0/all/0/1">Kentaro Tachibana</a></p>
<p>We propose PromptTTS++, a prompt-based text-to-speech (TTS) synthesis system
that allows control over speaker identity using natural language descriptions.
To control speaker identity within the prompt-based TTS framework, we introduce
the concept of speaker prompt, which describes voice characteristics (e.g.,
gender-neutral, young, old, and muffled) designed to be approximately
independent of speaking style. Since there is no large-scale dataset containing
speaker prompts, we first construct a dataset based on the LibriTTS-R corpus
with manually annotated speaker prompts. We then employ a diffusion-based
acoustic model with mixture density networks to model diverse speaker factors
in the training data. Unlike previous studies that rely on style prompts
describing only a limited aspect of speaker individuality, such as pitch,
speaking speed, and energy, our method utilizes an additional speaker prompt to
effectively learn the mapping from natural language descriptions to the
acoustic features of diverse speakers. Our subjective evaluation results show
that the proposed method can better control speaker characteristics than the
methods without the speaker prompt. Audio samples are available at
https://reppy4620.github.io/demo.promptttspp/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11646">An Evaluation of Machine Learning Approaches for Early Diagnosis of Autism Spectrum Disorder. (arXiv:2309.11646v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rasul_R/0/1/0/all/0/1">Rownak Ara Rasul</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_P/0/1/0/all/0/1">Promy Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Bala_D/0/1/0/all/0/1">Diponkor Bala</a>, <a href="http://arxiv.org/find/cs/1/au:+Karim_S/0/1/0/all/0/1">S M Rakib Ul Karim</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullah_M/0/1/0/all/0/1">Md. Ibrahim Abdullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Saha_B/0/1/0/all/0/1">Bishwajit Saha</a></p>
<p>Autistic Spectrum Disorder (ASD) is a neurological disease characterized by
difficulties with social interaction, communication, and repetitive activities.
While its primary origin lies in genetics, early detection is crucial, and
leveraging machine learning offers a promising avenue for a faster and more
cost-effective diagnosis. This study employs diverse machine learning methods
to identify crucial ASD traits, aiming to enhance and automate the diagnostic
process. We study eight state-of-the-art classification models to determine
their effectiveness in ASD detection. We evaluate the models using accuracy,
precision, recall, specificity, F1-score, area under the curve (AUC), kappa,
and log loss metrics to find the best classifier for these binary datasets.
Among all the classification models, for the children dataset, the SVM and LR
models achieve the highest accuracy of 100% and for the adult dataset, the LR
model produces the highest accuracy of 97.14%. Our proposed ANN model provides
the highest accuracy of 94.24% for the new combined dataset when
hyperparameters are precisely tuned for each model. As almost all
classification models achieve high accuracy which utilize true labels, we
become interested in delving into five popular clustering algorithms to
understand model behavior in scenarios without true labels. We calculate
Normalized Mutual Information (NMI), Adjusted Rand Index (ARI), and Silhouette
Coefficient (SC) metrics to select the best clustering models. Our evaluation
finds that spectral clustering outperforms all other benchmarking clustering
models in terms of NMI and ARI metrics while demonstrating comparability to the
optimal SC achieved by k-means. The implemented code is available at GitHub.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13108">Data is often loadable in short depth: Quantum circuits from tensor networks for finance, images, fluids, and proteins. (arXiv:2309.13108v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Jumade_R/0/1/0/all/0/1">Raghav Jumade</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Sawaya_N/0/1/0/all/0/1">Nicolas PD Sawaya</a></p>
<p>Though there has been substantial progress in developing quantum algorithms
to study classical datasets, the cost of simply \textit{loading} classical data
is an obstacle to quantum advantage. When the amplitude encoding is used,
loading an arbitrary classical vector requires up to exponential circuit depths
with respect to the number of qubits. Here, we address this ``input problem''
with two contributions. First, we introduce a circuit compilation method based
on tensor network (TN) theory. Our method -- AMLET (Automatic Multi-layer
Loader Exploiting TNs) -- proceeds via careful construction of a specific TN
topology and can be tailored to arbitrary circuit depths. Second, we perform
numerical experiments on real-world classical data from four distinct areas:
finance, images, fluid mechanics, and proteins. To the best of our knowledge,
this is the broadest numerical analysis to date of loading classical data into
a quantum computer. The required circuit depths are often several orders of
magnitude lower than the exponentially-scaling general loading algorithm would
require. Besides introducing a more efficient loading algorithm, this work
demonstrates that many classical datasets are loadable in depths that are much
shorter than previously expected, which has positive implications for speeding
up classical workloads on quantum computers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00806">Bayesian Design Principles for Frequentist Sequential Learning. (arXiv:2310.00806v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yunbei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeevi_A/0/1/0/all/0/1">Assaf Zeevi</a></p>
<p>We develop a general theory to optimize the frequentist regret for sequential
learning problems, where efficient bandit and reinforcement learning algorithms
can be derived from unified Bayesian principles. We propose a novel
optimization approach to generate "algorithmic beliefs" at each round, and use
Bayesian posteriors to make decisions. The optimization objective to create
"algorithmic beliefs," which we term "Algorithmic Information Ratio,"
represents an intrinsic complexity measure that effectively characterizes the
frequentist regret of any algorithm. To the best of our knowledge, this is the
first systematical approach to make Bayesian-type algorithms prior-free and
applicable to adversarial settings, in a generic and optimal manner. Moreover,
the algorithms are simple and often efficient to implement. As a major
application, we present a novel algorithm for multi-armed bandits that achieves
the "best-of-all-worlds" empirical performance in the stochastic, adversarial,
and non-stationary environments. And we illustrate how these principles can be
used in linear bandits, bandit convex optimization, and reinforcement learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01824">Mini-BEHAVIOR: A Procedurally Generated Benchmark for Long-horizon Decision-Making in Embodied AI. (arXiv:2310.01824v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jin_E/0/1/0/all/0/1">Emily Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1">Jiaheng Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1">Zhuoyi Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1">Ruohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1">Jiajun Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1">Li Fei-Fei</a>, <a href="http://arxiv.org/find/cs/1/au:+Martin_Martin_R/0/1/0/all/0/1">Roberto Mart&#xed;n-Mart&#xed;n</a></p>
<p>We present Mini-BEHAVIOR, a novel benchmark for embodied AI that challenges
agents to use reasoning and decision-making skills to solve complex activities
that resemble everyday human challenges. The Mini-BEHAVIOR environment is a
fast, realistic Gridworld environment that offers the benefits of rapid
prototyping and ease of use while preserving a symbolic level of physical
realism and complexity found in complex embodied AI benchmarks. We introduce
key features such as procedural generation, to enable the creation of countless
task variations and support open-ended learning. Mini-BEHAVIOR provides
implementations of various household tasks from the original BEHAVIOR
benchmark, along with starter code for data collection and reinforcement
learning agent training. In essence, Mini-BEHAVIOR offers a fast, open-ended
benchmark for evaluating decision-making and planning solutions in embodied AI.
It serves as a user-friendly entry point for research and facilitates the
evaluation and development of solutions, simplifying their assessment and
development while advancing the field of embodied AI. Code is publicly
available at https://github.com/StanfordVL/mini_behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.02508">Ophiuchus: Scalable Modeling of Protein Structures through Hierarchical Coarse-graining SO(3)-Equivariant Autoencoders. (arXiv:2310.02508v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Costa_A/0/1/0/all/0/1">Allan dos Santos Costa</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitnikov_I/0/1/0/all/0/1">Ilan Mitnikov</a>, <a href="http://arxiv.org/find/cs/1/au:+Geiger_M/0/1/0/all/0/1">Mario Geiger</a>, <a href="http://arxiv.org/find/cs/1/au:+Ponnapati_M/0/1/0/all/0/1">Manvitha Ponnapati</a>, <a href="http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1">Tess Smidt</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacobson_J/0/1/0/all/0/1">Joseph Jacobson</a></p>
<p>Three-dimensional native states of natural proteins display recurring and
hierarchical patterns. Yet, traditional graph-based modeling of protein
structures is often limited to operate within a single fine-grained resolution,
and lacks hourglass neural architectures to learn those high-level building
blocks. We narrow this gap by introducing Ophiuchus, an SO(3)-equivariant
coarse-graining model that efficiently operates on all-atom protein structures.
Our model departs from current approaches that employ graph modeling, instead
focusing on local convolutional coarsening to model sequence-motif interactions
with efficient time complexity in protein length. We measure the reconstruction
capabilities of Ophiuchus across different compression rates, and compare it to
existing models. We examine the learned latent space and demonstrate its
utility through conformational interpolation. Finally, we leverage denoising
diffusion probabilistic models (DDPM) in the latent space to efficiently sample
protein structures. Our experiments demonstrate Ophiuchus to be a scalable
basis for efficient protein modeling and generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03149">Attributing Learned Concepts in Neural Networks to Training Data. (arXiv:2310.03149v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konz_N/0/1/0/all/0/1">Nicholas Konz</a>, <a href="http://arxiv.org/find/cs/1/au:+Godfrey_C/0/1/0/all/0/1">Charles Godfrey</a>, <a href="http://arxiv.org/find/cs/1/au:+Shapiro_M/0/1/0/all/0/1">Madelyn Shapiro</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_J/0/1/0/all/0/1">Jonathan Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kvinge_H/0/1/0/all/0/1">Henry Kvinge</a>, <a href="http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1">Davis Brown</a></p>
<p>By now there is substantial evidence that deep learning models learn certain
human-interpretable features as part of their internal representations of data.
As having the right (or wrong) concepts is critical to trustworthy machine
learning systems, it is natural to ask which inputs from the model's original
training set were most important for learning a concept at a given layer. To
answer this, we combine data attribution methods with methods for probing the
concepts learned by a model. Training network and probe ensembles for two
concept datasets on a range of network layers, we use the recently developed
TRAK method for large-scale data attribution. We find some evidence for
convergence, where removing the 10,000 top attributing images for a concept and
retraining the model does not change the location of the concept in the network
nor the probing sparsity of the concept. This suggests that rather than being
highly dependent on a few specific examples, the features that inform the
development of a concept are spread in a more diffuse manner across its
exemplars, implying robustness in concept formation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05858">DSAC-T: Distributional Soft Actor-Critic with Three Refinements. (arXiv:2310.05858v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1">Jingliang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenxuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Liming Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Jiaxin Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shengbo Eben Li</a></p>
<p>Reinforcement learning (RL) has proven to be highly effective in tackling
complex decision-making and control tasks. However, prevalent model-free RL
methods often face severe performance degradation due to the well-known
overestimation issue. In response to this problem, we recently introduced an
off-policy RL algorithm, called distributional soft actor-critic (DSAC or
DSAC-v1), which can effectively improve the value estimation accuracy by
learning a continuous Gaussian value distribution. Nonetheless, standard DSAC
has its own shortcomings, including occasionally unstable learning processes
and the necessity for task-specific reward scaling, which may hinder its
overall performance and adaptability in some special tasks. This paper further
introduces three important refinements to standard DSAC in order to address
these shortcomings. These refinements consist of expected value substituting,
twin value distribution learning, and variance-based critic gradient adjusting.
The modified RL algorithm is named as DSAC with three refinements (DSAC-T or
DSAC-v2), and its performances are systematically evaluated on a diverse set of
benchmark tasks. Without any task-specific hyperparameter tuning, DSAC-T
surpasses or matches a lot of mainstream model-free RL algorithms, including
SAC, TD3, DDPG, TRPO, and PPO, in all tested environments. Additionally,
DSAC-T, unlike its standard version, ensures a highly stable learning process
and delivers similar performance across varying reward scales.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08475">Can We Edit Multimodal Large Language Models?. (arXiv:2310.08475v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Siyuan Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1">Bozhong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qingbin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xi Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yongheng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Huajun Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1">Ningyu Zhang</a></p>
<p>In this paper, we focus on editing Multimodal Large Language Models (MLLMs).
Compared to editing single-modal LLMs, multimodal model editing is more
challenging, which demands a higher level of scrutiny and careful consideration
in the editing process. To facilitate research in this area, we construct a new
benchmark, dubbed MMEdit, for editing multimodal LLMs and establishing a suite
of innovative metrics for evaluation. We conduct comprehensive experiments
involving various model editing baselines and analyze the impact of editing
different components for multimodal LLMs. Empirically, we notice that previous
baselines can implement editing multimodal LLMs to some extent, but the effect
is still barely satisfactory, indicating the potential difficulty of this task.
We hope that our work can provide the NLP community with insights. Code and
dataset are available in https://github.com/zjunlp/EasyEdit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.10477">Gaining Wisdom from Setbacks: Aligning Large Language Models via Mistake Analysis. (arXiv:2310.10477v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1">Kai Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chunwei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kuo Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1">Jianhua Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1">Lanqing Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1">Fei Mi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1">Hang Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengying Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1">Wenyong Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhenguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1">Dit-Yan Yeung</a>, <a href="http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1">Lifeng Shang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1">Xin Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1">Qun Liu</a></p>
<p>The rapid development of large language models (LLMs) has not only provided
numerous opportunities but also presented significant challenges. This becomes
particularly evident when LLMs inadvertently generate harmful or toxic content,
either unintentionally or because of intentional inducement. Existing alignment
methods usually direct LLMs toward favorable outcomes by utilizing
human-annotated, flawless instruction-response pairs. Conversely, this study
proposes a novel alignment technique based on mistake analysis, which
deliberately exposes LLMs to erroneous content to learn the reasons for
mistakes and how to avoid them. In this case, mistakes are repurposed into
valuable data for alignment, effectively helping to avoid the production of
erroneous responses. Without external models or human annotations, our method
leverages a model's intrinsic ability to discern undesirable mistakes and
improves the safety of its generated responses. Experimental results reveal
that our method outperforms existing alignment approaches in enhancing model
safety while maintaining the overall utility.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11077">United We Stand: Using Epoch-wise Agreement of Ensembles to Combat Overfit. (arXiv:2310.11077v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stern_U/0/1/0/all/0/1">Uri Stern</a>, <a href="http://arxiv.org/find/cs/1/au:+Shwartz_D/0/1/0/all/0/1">Daniel Shwartz</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1">Daphna Weinshall</a></p>
<p>Deep neural networks have become the method of choice for solving many
classification tasks, largely because they can fit very complex functions
defined over raw data. The downside of such powerful learners is the danger of
overfit. In this paper, we introduce a novel ensemble classifier for deep
networks that effectively overcomes overfitting by combining models generated
at specific intermediate epochs during training. Our method allows for the
incorporation of useful knowledge obtained by the models during the overfitting
phase without deterioration of the general performance, which is usually missed
when early stopping is used. To motivate this approach, we begin with the
theoretical analysis of a regression model, whose prediction -- that the
variance among classifiers increases when overfit occurs -- is demonstrated
empirically in deep networks in common use. Guided by these results, we
construct a new ensemble-based prediction method, where the prediction is
determined by the class that attains the most consensual prediction throughout
the training epochs. Using multiple image and text classification datasets, we
show that when regular ensembles suffer from overfit, our method eliminates the
harmful reduction in generalization due to overfit, and often even surpasses
the performance obtained by early stopping. Our method is easy to implement and
can be integrated with any training scheme and architecture, without additional
prior knowledge beyond the training set. It is thus a practical and useful tool
to overcome overfit. Code is available at
https://github.com/uristern123/United-We-Stand-Using-Epoch-wise-Agreement-of-Ensembles-to-Combat-Overfit.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.11094">Relearning Forgotten Knowledge: on Forgetting, Overfit and Training-Free Ensembles of DNNs. (arXiv:2310.11094v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Stern_U/0/1/0/all/0/1">Uri Stern</a>, <a href="http://arxiv.org/find/cs/1/au:+Weinshall_D/0/1/0/all/0/1">Daphna Weinshall</a></p>
<p>The infrequent occurrence of overfit in deep neural networks is perplexing.
On the one hand, theory predicts that as models get larger they should
eventually become too specialized for a specific training set, with ensuing
decrease in generalization. In contrast, empirical results in image
classification indicate that increasing the training time of deep models or
using bigger models almost never hurts generalization. Is it because the way we
measure overfit is too limited? Here, we introduce a novel score for
quantifying overfit, which monitors the forgetting rate of deep models on
validation data. Presumably, this score indicates that even while
generalization improves overall, there are certain regions of the data space
where it deteriorates. When thus measured, we show that overfit can occur with
and without a decrease in validation accuracy, and may be more common than
previously appreciated. This observation may help to clarify the aforementioned
confusing picture. We use our observations to construct a new ensemble method,
based solely on the training history of a single network, which provides
significant improvement in performance without any additional cost in training
time. An extensive empirical evaluation with modern deep models shows our
method's utility on multiple datasets, neural networks architectures and
training schemes, both when training from scratch and when using pre-trained
networks in transfer learning. Notably, our method outperforms comparable
methods while being easier to implement and use, and further improves the
performance of competitive networks on Imagenet by 1%.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15202">Predicting Transcription Factor Binding Sites using Transformer based Capsule Network. (arXiv:2310.15202v2 [q-bio.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Ghosh_N/0/1/0/all/0/1">Nimisha Ghosh</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Santoni_D/0/1/0/all/0/1">Daniele Santoni</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Saha_I/0/1/0/all/0/1">Indrajit Saha</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Felici_G/0/1/0/all/0/1">Giovanni Felici</a></p>
<p>Prediction of binding sites for transcription factors is important to
understand how they regulate gene expression and how this regulation can be
modulated for therapeutic purposes. Although in the past few years there are
significant works addressing this issue, there is still space for improvement.
In this regard, a transformer based capsule network viz. DNABERT-Cap is
proposed in this work to predict transcription factor binding sites mining
ChIP-seq datasets. DNABERT-Cap is a bidirectional encoder pre-trained with
large number of genomic DNA sequences, empowered with a capsule layer
responsible for the final prediction. The proposed model builds a predictor for
transcription factor binding sites using the joint optimisation of features
encompassing both bidirectional encoder and capsule layer, along with
convolutional and bidirectional long-short term memory layers. To evaluate the
efficiency of the proposed approach, we use a benchmark ChIP-seq datasets of
five cell lines viz. A549, GM12878, Hep-G2, H1-hESC and Hela, available in the
ENCODE repository. The results show that the average area under the receiver
operating characteristic curve score exceeds 0.91 for all such five cell lines.
DNABERT-Cap is also compared with existing state-of-the-art deep learning based
predictors viz. DeepARC, DeepTF, CNN-Zeng and DeepBind, and is seen to
outperform them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.15976">Convergence of Sign-based Random Reshuffling Algorithms for Nonconvex Optimization. (arXiv:2310.15976v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1">Zhen Qin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhishuai Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1">Pan Xu</a></p>
<p>signSGD is popular in nonconvex optimization due to its communication
efficiency. Yet, existing analyses of signSGD rely on assuming that data are
sampled with replacement in each iteration, contradicting the practical
implementation where data are randomly reshuffled and sequentially fed into the
algorithm. We bridge this gap by proving the first convergence result of
signSGD with random reshuffling (SignRR) for nonconvex optimization. Given the
dataset size $n$, the number of epochs of data passes $T$, and the variance
bound of a stochastic gradient $\sigma^2$, we show that SignRR has the same
convergence rate $O(\log(nT)/\sqrt{nT} + \|\sigma\|_1)$ as signSGD
\citep{bernstein2018signsgd}. We then present SignRVR and SignRVM, which
leverage variance-reduced gradients and momentum updates respectively, both
converging at $O(\log (nT)/\sqrt{nT} + \log (nT)\sqrt{n}/\sqrt{T})$. In
contrast with the analysis of signSGD, our results do not require an extremely
large batch size in each iteration to be of the same order as the total number
of iterations \citep{bernstein2018signsgd} or the signs of stochastic and true
gradients match element-wise with a minimum probability of 1/2
\citep{safaryan2021stochastic}. We also extend our algorithms to cases where
data are distributed across different machines, yielding dist-SignRVR and
dist-SignRVM, both converging at $O(\log (n_0T)/\sqrt{n_0T} + \log
(n_0T)\sqrt{n_0}/\sqrt{T})$, where $n_0$ is the dataset size of a single
machine. We back up our theoretical findings through experiments on simulated
and real-world problems, verifying that randomly reshuffled sign methods match
or surpass existing baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16221">Hierarchical Randomized Smoothing. (arXiv:2310.16221v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scholten_Y/0/1/0/all/0/1">Yan Scholten</a>, <a href="http://arxiv.org/find/cs/1/au:+Schuchardt_J/0/1/0/all/0/1">Jan Schuchardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Bojchevski_A/0/1/0/all/0/1">Aleksandar Bojchevski</a>, <a href="http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1">Stephan G&#xfc;nnemann</a></p>
<p>Real-world data is complex and often consists of objects that can be
decomposed into multiple entities (e.g. images into pixels, graphs into
interconnected nodes). Randomized smoothing is a powerful framework for making
models provably robust against small changes to their inputs - by guaranteeing
robustness of the majority vote when randomly adding noise before
classification. Yet, certifying robustness on such complex data via randomized
smoothing is challenging when adversaries do not arbitrarily perturb entire
objects (e.g. images) but only a subset of their entities (e.g. pixels). As a
solution, we introduce hierarchical randomized smoothing: We partially smooth
objects by adding random noise only on a randomly selected subset of their
entities. By adding noise in a more targeted manner than existing methods we
obtain stronger robustness guarantees while maintaining high accuracy. We
initialize hierarchical smoothing using different noising distributions,
yielding novel robustness certificates for discrete and continuous domains. We
experimentally demonstrate the importance of hierarchical smoothing in image
and node classification, where it yields superior robustness-accuracy
trade-offs. Overall, hierarchical smoothing is an important contribution
towards models that are both - certifiably robust to perturbations and
accurate.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17646">Do Graph Neural Networks Dream of Landau Damping? Insights from Kinetic Simulations of a Plasma Sheet Model. (arXiv:2310.17646v2 [physics.plasm-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Carvalho_D/0/1/0/all/0/1">Diogo D Carvalho</a>, <a href="http://arxiv.org/find/physics/1/au:+Ferreira_D/0/1/0/all/0/1">Diogo R Ferreira</a>, <a href="http://arxiv.org/find/physics/1/au:+Silva_L/0/1/0/all/0/1">Luis O Silva</a></p>
<p>We explore the possibility of fully replacing a plasma physics kinetic
simulator with a graph neural network-based simulator. We focus on this class
of surrogate models given the similarity between their message-passing update
mechanism and the traditional physics solver update, and the possibility of
enforcing known physical priors into the graph construction and update. We show
that our model learns the kinetic plasma dynamics of the one-dimensional plasma
model, a predecessor of contemporary kinetic plasma simulation codes, and
recovers a wide range of well-known kinetic plasma processes, including plasma
thermalization, electrostatic fluctuations about thermal equilibrium, and the
drag on a fast sheet and Landau damping. We compare the performance against the
original plasma model in terms of run-time, conservation laws, and temporal
evolution of key physical quantities. The limitations of the model are
presented and possible directions for higher-dimensional surrogate models for
kinetic plasmas are discussed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02594">scBeacon: single-cell biomarker extraction via identifying paired cell clusters across biological conditions with contrastive siamese networks. (arXiv:2311.02594v2 [q-bio.GN] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/q-bio/1/au:+Liu_C/0/1/0/all/0/1">Chenyu Liu</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Kweon_Y/0/1/0/all/0/1">Yong Jin Kweon</a>, <a href="http://arxiv.org/find/q-bio/1/au:+Ding_J/0/1/0/all/0/1">Jun Ding</a></p>
<p>Despite the breakthroughs in biomarker discovery facilitated by differential
gene analysis, challenges remain, particularly at the single-cell level.
Traditional methodologies heavily rely on user-supplied cell annotations,
focusing on individually expressed data, often neglecting the critical
interactions between biological conditions, such as healthy versus diseased
states. In response, here we introduce scBeacon, an innovative framework built
upon a deep contrastive siamese network. scBeacon pioneers an unsupervised
approach, adeptly identifying matched cell populations across varied
conditions, enabling a refined differential gene analysis. By utilizing a
VQ-VAE framework, a contrastive siamese network, and a greedy iterative
strategy, scBeacon effectively pinpoints differential genes that hold potential
as key biomarkers. Comprehensive evaluations on a diverse array of datasets
validate scBeacon's superiority over existing single-cell differential gene
analysis tools. Its precision and adaptability underscore its significant role
in enhancing diagnostic accuracy in biomarker discovery. With the emphasis on
the importance of biomarkers in diagnosis, scBeacon is positioned to be a
pivotal asset in the evolution of personalized medicine and targeted
treatments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.03489">Leveraging High-Level Synthesis and Large Language Models to Generate, Simulate, and Deploy a Uniform Random Number Generator Hardware Design. (arXiv:2311.03489v3 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Meech_J/0/1/0/all/0/1">James T. Meech</a></p>
<p>We present a new high-level synthesis methodology for using large language
model tools to generate hardware designs. The methodology uses exclusively
open-source tools excluding the large language model. As a case study, we use
our methodology to generate a permuted congruential random number generator
design with a wishbone interface. We verify the functionality and quality of
the random number generator design using large language model-generated
simulations and the Dieharder randomness test suite. We document all the large
language model chat logs, Python scripts, Verilog scripts, and simulation
results used in the case study. We believe that our method of hardware design
generation coupled with the open source silicon 130 nm design tools will
revolutionize application-specific integrated circuit design. Our methodology
significantly lowers the bar to entry when building domain-specific computing
accelerators for the Internet of Things and proof of concept prototypes for
later fabrication in more modern process nodes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04787">Why Do Probabilistic Clinical Models Fail To Transport Between Sites?. (arXiv:2311.04787v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lasko_T/0/1/0/all/0/1">Thomas A. Lasko</a>, <a href="http://arxiv.org/find/cs/1/au:+Strobl_E/0/1/0/all/0/1">Eric V. Strobl</a>, <a href="http://arxiv.org/find/cs/1/au:+Stead_W/0/1/0/all/0/1">William W. Stead</a></p>
<p>The rising popularity of artificial intelligence in healthcare is
highlighting the problem that a computational model achieving super-human
clinical performance at its training sites may perform substantially worse at
new sites. In this perspective, we present common sources for this failure to
transport, which we divide into sources under the control of the experimenter
and sources inherent to the clinical data-generating process. Of the inherent
sources we look a little deeper into site-specific clinical practices that can
affect the data distribution, and propose a potential solution intended to
isolate the imprint of those practices on the data from the patterns of disease
cause and effect that are the usual target of probabilistic clinical models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05739">Prune-Deprune: Adaptive Compression-Aware Split Learning and Inference for Enhanced Network Efficiency. (arXiv:2311.05739v2 [cs.NI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mudvari_A/0/1/0/all/0/1">Akrit Mudvari</a>, <a href="http://arxiv.org/find/cs/1/au:+Vainio_A/0/1/0/all/0/1">Antero Vainio</a>, <a href="http://arxiv.org/find/cs/1/au:+Ofeidis_I/0/1/0/all/0/1">Iason Ofeidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Tarkoma_S/0/1/0/all/0/1">Sasu Tarkoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tassiulas_L/0/1/0/all/0/1">Leandros Tassiulas</a></p>
<p>The growing number of AI-driven applications in mobile devices has led to
solutions that integrate deep learning models with the available edge-cloud
resources. Due to multiple benefits such as reduction in on-device energy
consumption, improved latency, improved network usage, and certain privacy
improvements, split learning, where deep learning models are split away from
the mobile device and computed in a distributed manner, has become an
extensively explored topic. Incorporating compression-aware methods (where
learning adapts to compression level of the communicated data) has made split
learning even more advantageous. This method could even offer a viable
alternative to traditional methods, such as federated learning techniques. In
this work, we develop an adaptive compression-aware split learning method
('deprune') to improve and train deep learning models so that they are much
more network-efficient, which would make them ideal to deploy in weaker devices
with the help of edge-cloud resources. This method is also extended ('prune')
to very quickly train deep learning models through a transfer learning
approach, which trades off little accuracy for much more network-efficient
inference abilities. We show that the 'deprune' method can reduce network usage
by 4x when compared with a split-learning approach (that does not use our
method) without loss of accuracy, while also improving accuracy over
compression-aware split-learning by 4 percent. Lastly, we show that the 'prune'
method can reduce the training time for certain models by up to 6x without
affecting the accuracy when compared against a compression-aware split-learning
approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06281">Efficient Parallelization of a Ubiquitous Sequential Computation. (arXiv:2311.06281v4 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Heinsen_F/0/1/0/all/0/1">Franz A. Heinsen</a></p>
<p>We find a succinct expression for computing the sequence $x_t = a_t x_{t-1} +
b_t$ in parallel with two prefix sums, given $t = (1, 2, \dots, n)$, $a_t \in
\mathbb{R}^n$, $b_t \in \mathbb{R}^n$, and initial value $x_0 \in \mathbb{R}$.
On $n$ parallel processors, the computation of $n$ elements incurs
$\mathcal{O}(\log n)$ time and $\mathcal{O}(n)$ space. Sequences of this form
are ubiquitous in science and engineering, making efficient parallelization
useful for a vast number of applications. We implement our expression in
software, test it on parallel hardware, and verify that it executes faster than
sequential computation by a factor of $\frac{n}{\log n}$.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.13326">Curriculum Learning and Imitation Learning for Model-free Control on Financial Time-series. (arXiv:2311.13326v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Koh_W/0/1/0/all/0/1">Woosung Koh</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_I/0/1/0/all/0/1">Insu Choi</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yuntae Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kang_G/0/1/0/all/0/1">Gimin Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Woo Chang Kim</a></p>
<p>Curriculum learning and imitation learning have been leveraged extensively in
the robotics domain. However, minimal research has been done on leveraging
these ideas on control tasks over highly stochastic time-series data. Here, we
theoretically and empirically explore these approaches in a representative
control task over complex time-series data. We implement the fundamental ideas
of curriculum learning via data augmentation, while imitation learning is
implemented via policy distillation from an oracle. Our findings reveal that
curriculum learning should be considered a novel direction in improving
control-task performance over complex time-series. Our ample random-seed
out-sample empirics and ablation studies are highly encouraging for curriculum
learning for time-series control. These findings are especially encouraging as
we tune all overlapping hyperparameters on the baseline -- giving an advantage
to the baseline. On the other hand, we find that imitation learning should be
used with caution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14743">A Baseline Analysis of Reward Models&#x27; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v6 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1">Will LeVine</a>, <a href="http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1">Ben Pikus</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1">Anthony Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1">Sean Hendryx</a></p>
<p>Foundation models, specifically Large Language Models (LLM's), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align LLM's. These reward models are additionally used at
inference-time to estimate LLM responses' adherence to those desired behaviors.
However, there is little work measuring how robust these reward models are to
distribution shifts. In this work, we evaluate how reward model performance -
measured via accuracy and calibration (i.e. alignment between accuracy and
confidence) - is affected by distribution shift. We show novel calibration
patterns and accuracy drops due to OOD prompts and responses, and that the
reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting to detect these distribution shifts
in prompts and responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.14773">Set Features for Anomaly Detection. (arXiv:2311.14773v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1">Niv Cohen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tzachor_I/0/1/0/all/0/1">Issar Tzachor</a>, <a href="http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1">Yedid Hoshen</a></p>
<p>This paper proposes set features for detecting anomalies in samples that
consist of unusual combinations of normal elements. Many leading methods
discover anomalies by detecting an unusual part of a sample. For example,
state-of-the-art segmentation-based approaches, first classify each element of
the sample (e.g., image patch) as normal or anomalous and then classify the
entire sample as anomalous if it contains anomalous elements. However, such
approaches do not extend well to scenarios where the anomalies are expressed by
an unusual combination of normal elements. In this paper, we overcome this
limitation by proposing set features that model each sample by the distribution
of its elements. We compute the anomaly score of each sample using a simple
density estimation method, using fixed features. Our approach outperforms the
previous state-of-the-art in image-level logical anomaly detection and
sequence-level time series anomaly detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16487">On the Robustness of Decision-Focused Learning. (arXiv:2311.16487v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Farhat_Y/0/1/0/all/0/1">Yehya Farhat</a></p>
<p>Decision-Focused Learning (DFL) is an emerging learning paradigm that tackles
the task of training a machine learning (ML) model to predict missing
parameters of an incomplete optimization problem, where the missing parameters
are predicted. DFL trains an ML model in an end-to-end system, by integrating
the prediction and optimization tasks, providing better alignment of the
training and testing objectives. DFL has shown a lot of promise and holds the
capacity to revolutionize decision-making in many real-world applications.
However, very little is known about the performance of these models under
adversarial attacks. We adopt ten unique DFL methods and benchmark their
performance under two distinctly focused attacks adapted towards the
Predict-then-Optimize problem setting. Our study proposes the hypothesis that
the robustness of a model is highly correlated with its ability to find
predictions that lead to optimal decisions without deviating from the
ground-truth label. Furthermore, we provide insight into how to target the
models that violate this condition and show how these models respond
differently depending on the achieved optimality at the end of their training
cycles.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.16509">StyleCap: Automatic Speaking-Style Captioning from Speech Based on Speech and Language Self-supervised Learning Models. (arXiv:2311.16509v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yamauchi_K/0/1/0/all/0/1">Kazuki Yamauchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ijima_Y/0/1/0/all/0/1">Yusuke Ijima</a>, <a href="http://arxiv.org/find/cs/1/au:+Saito_Y/0/1/0/all/0/1">Yuki Saito</a></p>
<p>We propose StyleCap, a method to generate natural language descriptions of
speaking styles appearing in speech. Although most of conventional techniques
for para-/non-linguistic information recognition focus on the category
classification or the intensity estimation of pre-defined labels, they cannot
provide the reasoning of the recognition result in an interpretable manner.
StyleCap is a first step towards an end-to-end method for generating
speaking-style prompts from speech, i.e., automatic speaking-style captioning.
StyleCap is trained with paired data of speech and natural language
descriptions. We train neural networks that convert a speech representation
vector into prefix vectors that are fed into a large language model (LLM)-based
text decoder. We explore an appropriate text decoder and speech feature
representation suitable for this new task. The experimental results demonstrate
that our StyleCap leveraging richer LLMs for the text decoder, speech
self-supervised learning (SSL) features, and sentence rephrasing augmentation
improves the accuracy and diversity of generated speaking-style captions.
Samples of speaking-style captions generated by our StyleCap are publicly
available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17431">Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v6 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1">Yan Kang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1">Tao Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1">Hanlin Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaojin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1">Lixin Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1">Qiang Yang</a></p>
<p>Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.17929">New Online Communities: Graph Deep Learning on Anonymous Voting Networks to Identify Sybils in Polycentric Governance. (arXiv:2311.17929v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DuPont_Q/0/1/0/all/0/1">Quinn DuPont</a></p>
<p>This research examines the polycentric governance of digital assets in
blockchain-based Decentralized Autonomous Organizations (DAOs). It offers a
theoretical framework and addresses a critical challenge facing decentralized
governance by developing a method to identify sybils, or spurious identities.
The method uses graph deep learning techniques to identify sybil activity in a
DAO governance dataset (snapshot.org). Specifically, a Graph Convolutional
Neural Network (GCNN) learned voting behaviours and a fast k-means vector
clustering algorithm (FAISS) used the high dimensional embeddings to identify
similar nodes in a graph. The results reveal that deep learning can effectively
identify sybils, reducing the voting graph by 2-5%. This research underscores
the importance of sybil resistance in DAOs and offers a novel perspective on
decentralized governance, informing future policy, regulation, and governance
practices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.02225">Digital Histopathology with Graph Neural Networks: Concepts and Explanations for Clinicians. (arXiv:2312.02225v2 [physics.med-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/physics/1/au:+Villaforesta_A/0/1/0/all/0/1">Alessandro Farace di Villaforesta</a>, <a href="http://arxiv.org/find/physics/1/au:+Magister_L/0/1/0/all/0/1">Lucie Charlotte Magister</a>, <a href="http://arxiv.org/find/physics/1/au:+Barbiero_P/0/1/0/all/0/1">Pietro Barbiero</a>, <a href="http://arxiv.org/find/physics/1/au:+Lio_P/0/1/0/all/0/1">Pietro Li&#xf2;</a></p>
<p>To address the challenge of the ``black-box" nature of deep learning in
medical settings, we combine GCExplainer - an automated concept discovery
solution - along with Logic Explained Networks to provide global explanations
for Graph Neural Networks. We demonstrate this using a generally applicable
graph construction and classification pipeline, involving panoptic segmentation
with HoVer-Net and cancer prediction with Graph Convolution Networks. By
training on H&amp;E slides of breast cancer, we show promising results in offering
explainable and trustworthy AI tools for clinicians.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05654">Spectral methods for Neural Integral Equations. (arXiv:2312.05654v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+Zappala_E/0/1/0/all/0/1">Emanuele Zappala</a></p>
<p>Neural integral equations are deep learning models based on the theory of
integral equations, where the model consists of an integral operator and the
corresponding equation (of the second kind) which is learned through an
optimization procedure. This approach allows to leverage the nonlocal
properties of integral operators in machine learning, but it is computationally
expensive. In this article, we introduce a framework for neural integral
equations based on spectral methods that allows us to learn an operator in the
spectral domain, resulting in a cheaper computational cost, as well as in high
interpolation accuracy. We study the properties of our methods and show various
theoretical guarantees regarding the approximation capabilities of the model,
and convergence to solutions of the numerical methods. We provide numerical
experiments to demonstrate the practical effectiveness of the resulting model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.05715">Micro-Macro Consistency in Multiscale Modeling: Score-Based Model Assisted Sampling of Fast/Slow Dynamical Systems. (arXiv:2312.05715v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Crabtree_E/0/1/0/all/0/1">Ellis R. Crabtree</a>, <a href="http://arxiv.org/find/cs/1/au:+Bello_Rivas_J/0/1/0/all/0/1">Juan M. Bello-Rivas</a>, <a href="http://arxiv.org/find/cs/1/au:+Kevrekidis_I/0/1/0/all/0/1">Ioannis G. Kevrekidis</a></p>
<p>A valuable step in the modeling of multiscale dynamical systems in fields
such as computational chemistry, biology, materials science and more, is the
representative sampling of the phase space over long timescales of interest;
this task is not, however, without challenges. For example, the long term
behavior of a system with many degrees of freedom often cannot be efficiently
computationally explored by direct dynamical simulation; such systems can often
become trapped in local free energy minima. In the study of physics-based
multi-time-scale dynamical systems, techniques have been developed for
enhancing sampling in order to accelerate exploration beyond free energy
barriers. On the other hand, in the field of Machine Learning, a generic goal
of generative models is to sample from a target density, after training on
empirical samples from this density. Score based generative models (SGMs) have
demonstrated state-of-the-art capabilities in generating plausible data from
target training distributions. Conditional implementations of such generative
models have been shown to exhibit significant parallels with long-established
-- and physics based -- solutions to enhanced sampling. These physics-based
methods can then be enhanced through coupling with the ML generative models,
complementing the strengths and mitigating the weaknesses of each technique. In
this work, we show that that SGMs can be used in such a coupling framework to
improve sampling in multiscale dynamical systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06528">Transformers Implement Functional Gradient Descent to Learn Non-Linear Functions In Context. (arXiv:2312.06528v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1">Xiang Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Sra_S/0/1/0/all/0/1">Suvrit Sra</a></p>
<p>Many neural network architectures have been shown to be Turing Complete, and
can thus implement arbitrary algorithms. However, Transformers are unique in
that they can implement gradient-based learning algorithms \emph{under simple
parameter configurations}. A line of recent work shows that linear Transformers
naturally learn to implement gradient descent (GD) when trained on a linear
regression in-context learning task. But the linearity assumption (either in
the Transformer architecture or in the learning task) is far from realistic
settings where non-linear activations crucially enable Transformers to learn
complicated non-linear functions. In this paper, we provide theoretical and
empirical evidence that non-linear Transformers can, and \emph{in fact do},
learn to implement learning algorithms to learn non-linear functions in
context. Our results apply to a broad class of combinations of non-linear
architectures, and non-linear in-context learning tasks. Interestingly, we show
that the optimal choice of non-linear activation depends in a natural way on
the non-linearity of the learning task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.06681">Steering Llama 2 via Contrastive Activation Addition. (arXiv:2312.06681v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rimsky_N/0/1/0/all/0/1">Nina Rimsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Gabrieli_N/0/1/0/all/0/1">Nick Gabrieli</a>, <a href="http://arxiv.org/find/cs/1/au:+Schulz_J/0/1/0/all/0/1">Julian Schulz</a>, <a href="http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1">Meg Tong</a>, <a href="http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1">Evan Hubinger</a>, <a href="http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1">Alexander Matt Turner</a></p>
<p>We introduce Contrastive Activation Addition (CAA), an innovative method for
steering language models by modifying activations during their forward passes.
CAA computes ``steering vectors'' by averaging the difference in residual
stream activations between pairs of positive and negative examples of a
particular behavior such as factual versus hallucinatory responses. During
inference, these steering vectors are added at all token positions after the
user's prompt with either a positive or negative coefficient, allowing precise
control over the degree of the targeted behavior. We evaluate CAA's
effectiveness on Llama 2 Chat using both multiple-choice behavioral question
datasets and open-ended generation tasks. We demonstrate that CAA significantly
alters model behavior, outperforms traditional methods like finetuning and
few-shot prompting, and minimally reduces capabilities. Moreover, by employing
various activation space interpretation methods, we gain deeper insights into
CAA's mechanisms. CAA both accurately steers model outputs and also sheds light
on how high-level concepts are represented in Large Language Models (LLMs).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.07492">SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v4 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1">Manish Nagireddy</a>, <a href="http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1">Lamogha Chiazor</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1">Moninder Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1">Ioana Baldini</a></p>
<p>Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. Taking inspiration
from social science research, we start with a documented list of 93 US-centric
stigmas and curate a question-answering (QA) dataset which involves simple
social situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts,
with a variety of prompt styles, carefully constructed to systematically test
for both social bias and model robustness. We present results for
SocialStigmaQA with two open source generative language models and we find that
the proportion of socially biased output ranges from 45% to 59% across a
variety of decoding strategies and prompting styles. We demonstrate that the
deliberate design of the templates in our benchmark (e.g., adding biasing text
to the prompt or using different verbs that change the answer that indicates
bias) impacts the model tendencies to generate socially biased output.
Additionally, through manual evaluation, we discover problematic patterns in
the generated chain-of-thought output that range from subtle bias to lack of
reasoning.
</p>
<p>Warning: This paper contains examples of text which are toxic, biased, and
potentially harmful.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08532">Cooperative Learning for Cost-Adaptive Inference. (arXiv:2312.08532v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1">Xingli Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bradford_R/0/1/0/all/0/1">Richard Bradford</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Jung-Eun Kim</a></p>
<p>We propose a cooperative training framework for deep neural network
architectures that enables the runtime network depths to change to satisfy
dynamic computing resource requirements. In our framework, the number of layers
participating in computation can be chosen dynamically to meet performance-cost
trade-offs at inference runtime. Our method trains two Teammate nets and a
Leader net, and two sets of Teammate sub-networks with various depths through
knowledge distillation. The Teammate nets derive sub-networks and transfer
knowledge to them, and to each other, while the Leader net guides Teammate nets
to ensure accuracy. The approach trains the framework atomically at once
instead of individually training various sizes of models; in a sense, the
various-sized networks are all trained at once, in a "package deal." The
proposed framework is not tied to any specific architecture but can incorporate
any existing models/architectures, therefore it can maintain stable results and
is insensitive to the size of a dataset's feature map. Compared with other
related approaches, it provides comparable accuracy to its full network while
various sizes of models are available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08820">How to Raise a Robot -- A Case for Neuro-Symbolic AI in Constrained Task Planning for Humanoid Assistive Robots. (arXiv:2312.08820v3 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hemken_N/0/1/0/all/0/1">Niklas Hemken</a>, <a href="http://arxiv.org/find/cs/1/au:+Jacob_F/0/1/0/all/0/1">Florian Jacob</a>, <a href="http://arxiv.org/find/cs/1/au:+Peller_Konrad_F/0/1/0/all/0/1">Fabian Peller-Konrad</a>, <a href="http://arxiv.org/find/cs/1/au:+Kartmann_R/0/1/0/all/0/1">Rainer Kartmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Asfour_T/0/1/0/all/0/1">Tamim Asfour</a>, <a href="http://arxiv.org/find/cs/1/au:+Hartenstein_H/0/1/0/all/0/1">Hannes Hartenstein</a></p>
<p>Humanoid robots will be able to assist humans in their daily life, in
particular due to their versatile action capabilities. However, while these
robots need a certain degree of autonomy to learn and explore, they also should
respect various constraints, for access control and beyond. We explore the
novel field of incorporating privacy, security, and access control constraints
with robot task planning approaches. We report preliminary results on the
classical symbolic approach, deep-learned neural networks, and modern ideas
using large language models as knowledge base. From analyzing their trade-offs,
we conclude that a hybrid approach is necessary, and thereby present a new use
case for the emerging field of neuro-symbolic artificial intelligence.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.08935">Math-Shepherd: Verify and Reinforce LLMs Step-by-step without Human Annotations. (arXiv:2312.08935v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1">Peiyi Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Lei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1">Zhihong Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">R.X. Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1">Damai Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yifei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Deli Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Y.Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1">Zhifang Sui</a></p>
<p>In this paper, we present an innovative process-oriented math process reward
model called \textbf{Math-Shepherd}, which assigns a reward score to each step
of math problem solutions. The training of Math-Shepherd is achieved using
automatically constructed process-wise supervision data, breaking the
bottleneck of heavy reliance on manual annotation in existing work. We explore
the effectiveness of Math-Shepherd in two scenarios: 1) \textit{Verification}:
Math-Shepherd is utilized for reranking multiple outputs generated by Large
Language Models (LLMs); 2) \textit{Reinforcement Learning}: Math-Shepherd is
employed to reinforce LLMs with step-by-step Proximal Policy Optimization
(PPO). With Math-Shepherd, a series of open-source LLMs demonstrates
exceptional performance. For instance, the step-by-step PPO with Math-Shepherd
significantly improves the accuracy of Mistral-7B (77.9\%$\to$84.1\% on GSM8K
and 28.6\%$\to$33.0\% on MATH). The accuracy can be further enhanced to 89.1\%
and 43.5\% on GSM8K and MATH with the verification of Math-Shepherd,
respectively. We believe that automatic process supervision holds significant
potential for the future evolution of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.09939">Quantum Generative Adversarial Networks: Bridging Classical and Quantum Realms. (arXiv:2312.09939v2 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Nokhwal_S/0/1/0/all/0/1">Sahil Nokhwal</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Nokhwal_S/0/1/0/all/0/1">Suman Nokhwal</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Pahune_S/0/1/0/all/0/1">Saurabh Pahune</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Chaudhary_A/0/1/0/all/0/1">Ankit Chaudhary</a></p>
<p>In this pioneering research paper, we present a groundbreaking exploration
into the synergistic fusion of classical and quantum computing paradigms within
the realm of Generative Adversarial Networks (GANs). Our objective is to
seamlessly integrate quantum computational elements into the conventional GAN
architecture, thereby unlocking novel pathways for enhanced training processes.
</p>
<p>Drawing inspiration from the inherent capabilities of quantum bits (qubits),
we delve into the incorporation of quantum data representation methodologies
within the GAN framework. By capitalizing on the unique quantum features, we
aim to accelerate the training process of GANs, offering a fresh perspective on
the optimization of generative models.
</p>
<p>Our investigation deals with theoretical considerations and evaluates the
potential quantum advantages that may manifest in terms of training efficiency
and generative quality. We confront the challenges inherent in the
quantum-classical amalgamation, addressing issues related to quantum hardware
constraints, error correction mechanisms, and scalability considerations. This
research is positioned at the forefront of quantum-enhanced machine learning,
presenting a critical stride towards harnessing the computational power of
quantum systems to expedite the training of Generative Adversarial Networks.
Through our comprehensive examination of the interface between classical and
quantum realms, we aim to uncover transformative insights that will propel the
field forward, fostering innovation and advancing the frontier of quantum
machine learning.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.10024">Accelerating Neural Network Training: A Brief Review. (arXiv:2312.10024v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nokhwal_S/0/1/0/all/0/1">Sahil Nokhwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Chilakalapudi_P/0/1/0/all/0/1">Priyanka Chilakalapudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Donekal_P/0/1/0/all/0/1">Preeti Donekal</a>, <a href="http://arxiv.org/find/cs/1/au:+Nokhwal_S/0/1/0/all/0/1">Suman Nokhwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Pahune_S/0/1/0/all/0/1">Saurabh Pahune</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_A/0/1/0/all/0/1">Ankit Chaudhary</a></p>
<p>The process of training a deep neural network is characterized by significant
time requirements and associated costs. Although researchers have made
considerable progress in this area, further work is still required due to
resource constraints. This study examines innovative approaches to expedite the
training process of deep neural networks (DNN), with specific emphasis on three
state-of-the-art models such as ResNet50, Vision Transformer (ViT), and
EfficientNet. The research utilizes sophisticated methodologies, including
Gradient Accumulation (GA), Automatic Mixed Precision (AMP), and Pin Memory
(PM), in order to optimize performance and accelerate the training procedure.
</p>
<p>The study examines the effects of these methodologies on the DNN models
discussed earlier, assessing their efficacy with regard to training rate and
computational efficacy. The study showcases the efficacy of including GA as a
strategic approach, resulting in a noteworthy decrease in the duration required
for training. This enables the models to converge at a faster pace. The
utilization of AMP enhances the speed of computations by taking advantage of
the advantages offered by lower precision arithmetic while maintaining the
correctness of the model.
</p>
<p>Furthermore, this study investigates the application of Pin Memory as a
strategy to enhance the efficiency of data transmission between the central
processing unit and the graphics processing unit, thereby offering a promising
opportunity for enhancing overall performance. The experimental findings
demonstrate that the combination of these sophisticated methodologies
significantly accelerates the training of DNNs, offering vital insights for
experts seeking to improve the effectiveness of deep learning processes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.11176">Harnessing the Power of Neural Operators with Automatically Encoded Conservation Laws. (arXiv:2312.11176v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1">Ning Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yiming Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1">Xianyi Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Klower_M/0/1/0/all/0/1">Milan Kl&#xf6;wer</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1">Yue Yu</a></p>
<p>Neural operators (NOs) have emerged as effective tools for modeling complex
physical systems in scientific machine learning. In NOs, a central
characteristic is to learn the governing physical laws directly from data. In
contrast to other machine learning applications, partial knowledge is often
known a priori about the physical system at hand whereby quantities such as
mass, energy and momentum are exactly conserved. Currently, NOs have to learn
these conservation laws from data and can only approximately satisfy them due
to finite training data and random noise. In this work, we introduce
conservation law-encoded neural operators (clawNOs), a suite of NOs that endow
inference with automatic satisfaction of such conservation laws. ClawNOs are
built with a divergence-free prediction of the solution field, with which the
continuity equation is automatically guaranteed. As a consequence, clawNOs are
compliant with the most fundamental and ubiquitous conservation laws essential
for correct physical consistency. As demonstrations, we consider a wide variety
of scientific applications ranging from constitutive modeling of material
deformation, incompressible fluid dynamics, to atmospheric simulation. ClawNOs
significantly outperform the state-of-the-art NOs in learning efficacy,
especially in small-data regimes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.12679">Towards Efficient Verification of Quantized Neural Networks. (arXiv:2312.12679v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1">Pei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Haoze Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yuting Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Daukantas_I/0/1/0/all/0/1">Ieva Daukantas</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Min Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yedi Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1">Clark Barrett</a></p>
<p>Quantization replaces floating point arithmetic with integer arithmetic in
deep neural network models, providing more efficient on-device inference with
less power and memory. In this work, we propose a framework for formally
verifying properties of quantized neural networks. Our baseline technique is
based on integer linear programming which guarantees both soundness and
completeness. We then show how efficiency can be improved by utilizing
gradient-based heuristic search methods and also bound-propagation techniques.
We evaluate our approach on perception networks quantized with PyTorch. Our
results show that we can verify quantized networks with better scalability and
efficiency than the previous state of the art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.14276">Deep Neural Networks and Finite Elements of Any Order on Arbitrary Dimensions. (arXiv:2312.14276v2 [math.NA] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/math/1/au:+He_J/0/1/0/all/0/1">Juncai He</a>, <a href="http://arxiv.org/find/math/1/au:+Xu_J/0/1/0/all/0/1">Jinchao Xu</a></p>
<p>In this study, we establish that deep neural networks employing ReLU and
ReLU$^2$ activation functions are capable of representing Lagrange finite
element functions of any order on simplicial meshes across arbitrary
dimensions. We introduce a novel global formulation of the basis functions for
Lagrange elements, grounded in a geometric decomposition of these elements and
leveraging two essential properties of high-dimensional simplicial meshes and
barycentric coordinate functions. This representation theory facilitates a
natural approximation result for such deep neural networks. Our findings
present the first demonstration of how deep neural networks can systematically
generate general continuous piecewise polynomial functions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15566">Deep Copula-Based Survival Analysis for Dependent Censoring with Identifiability Guarantees. (arXiv:2312.15566v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Zhang_W/0/1/0/all/0/1">Weijia Zhang</a>, <a href="http://arxiv.org/find/stat/1/au:+Ling_C/0/1/0/all/0/1">Chun Kai Ling</a>, <a href="http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1">Xuanhui Zhang</a></p>
<p>Censoring is the central problem in survival analysis where either the
time-to-event (for instance, death), or the time-tocensoring (such as loss of
follow-up) is observed for each sample. The majority of existing machine
learning-based survival analysis methods assume that survival is conditionally
independent of censoring given a set of covariates; an assumption that cannot
be verified since only marginal distributions is available from the data. The
existence of dependent censoring, along with the inherent bias in current
estimators has been demonstrated in a variety of applications, accentuating the
need for a more nuanced approach. However, existing methods that adjust for
dependent censoring require practitioners to specify the ground truth copula.
This requirement poses a significant challenge for practical applications, as
model misspecification can lead to substantial bias. In this work, we propose a
flexible deep learning-based survival analysis method that simultaneously
accommodate for dependent censoring and eliminates the requirement for
specifying the ground truth copula. We theoretically prove the identifiability
of our model under a broad family of copulas and survival distributions.
Experiments results from a wide range of datasets demonstrate that our approach
successfully discerns the underlying dependency structure and significantly
reduces survival estimation bias when compared to existing methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15824">Self-Supervised Learning for Few-Shot Bird Sound Classification. (arXiv:2312.15824v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moummad_I/0/1/0/all/0/1">Ilyass Moummad</a>, <a href="http://arxiv.org/find/cs/1/au:+Serizel_R/0/1/0/all/0/1">Romain Serizel</a>, <a href="http://arxiv.org/find/cs/1/au:+Farrugia_N/0/1/0/all/0/1">Nicolas Farrugia</a></p>
<p>Self-supervised learning (SSL) in audio holds significant potential across
various domains, particularly in situations where abundant, unlabeled data is
readily available at no cost. This is particularly pertinent in bioacoustics,
where biologists routinely collect extensive sound datasets from the natural
environment. In this study, we demonstrate that SSL is capable of acquiring
meaningful representations of bird sounds from audio recordings without the
need for annotations. Our experiments showcase that these learned
representations exhibit the capacity to generalize to new bird species in
few-shot learning (FSL) scenarios. Additionally, we show that selecting windows
with high bird activation for self-supervised learning, using a pretrained
audio neural network, significantly enhances the quality of the learned
representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15969">Exploiting the capacity of deep networks only at training stage for nonlinear black-box system identification. (arXiv:2312.15969v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Eivaghi_V/0/1/0/all/0/1">Vahid MohammadZadeh Eivaghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Shooredeli_M/0/1/0/all/0/1">Mahdi Aliyari Shooredeli</a></p>
<p>To benefit from the modeling capacity of deep models in system
identification, without worrying about inference time, this study presents a
novel training strategy that uses deep models only at the training stage. For
this purpose two separate models with different structures and goals are
employed. The first one is a deep generative model aiming at modeling the
distribution of system output(s), called the teacher model, and the second one
is a shallow basis function model, named the student model, fed by system
input(s) to predict the system output(s). That means these isolated paths must
reach the same ultimate target. As deep models show a great performance in
modeling of highly nonlinear systems, aligning the representation space learned
by these two models make the student model to inherit the approximation power
of the teacher model. The proposed objective function consists of the objective
of each student and teacher model adding up with a distance penalty between the
learned latent representations. The simulation results on three nonlinear
benchmarks show a comparative performance with examined deep architectures
applied on the same benchmarks. Algorithmic transparency and structure
efficiency are also achieved as byproducts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15985">Discrete Messages Improve Communication Efficiency among Isolated Intelligent Agents. (arXiv:2312.15985v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1">Yuchuan Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1">Weijie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Meo_C/0/1/0/all/0/1">Cristian Meo</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Ziwei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1">Dianbo Liu</a></p>
<p>Individuals, despite having varied life experiences and learning processes,
can communicate effectively through languages. This study aims to explore the
efficiency of language as a communication medium. We put forth two specific
hypotheses: First, discrete messages are more effective than continuous ones
when agents have diverse personal experiences. Second, communications using
multiple discrete tokens are more advantageous than those using a single token.
To valdate these hypotheses, we designed multi-agent machine learning
experiments to assess communication efficiency using various information
transmission methods between speakers and listeners. Our empirical findings
indicate that, in scenarios where agents are exposed to different data,
communicating through sentences composed of discrete tokens offers the best
inter-agent communication efficiency. The limitations of our finding include
lack of systematic advantages over other more sophisticated encoder-decoder
model such as variational autoencoder and lack of evluation on non-image
dataset, which we will leave for future studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2312.15148">Personalized Federated Learning with Attention-based Client Selection. (arXiv:2312.15148v1 [cs.LG] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zihan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jundong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1">Cong Shen</a></p>
<p>Personalized Federated Learning (PFL) relies on collective data knowledge to
build customized models. However, non-IID data between clients poses
significant challenges, as collaborating with clients who have diverse data
distributions can harm local model performance, especially with limited
training data. To address this issue, we propose FedACS, a new PFL algorithm
with an Attention-based Client Selection mechanism. FedACS integrates an
attention mechanism to enhance collaboration among clients with similar data
distributions and mitigate the data scarcity issue. It prioritizes and
allocates resources based on data similarity. We further establish the
theoretical convergence behavior of FedACS. Experiments on CIFAR10 and FMNIST
validate FedACS's superiority, showcasing its potential to advance personalized
federated learning. By tackling non-IID data challenges and data scarcity,
FedACS offers promising advances in the field of personalized federated
learning.
</p>
</p>
</div>

    </div>
    </body>
    