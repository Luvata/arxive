<!DOCTYPE html>
<html>
<head>
<title>2023-10-14-cs-cv</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2310.07725">Extreme Image Transformations Facilitate Robust Latent Object Representations. (arXiv:2310.07725v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malik_G/0/1/0/all/0/1">Girik Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Crowder_D/0/1/0/all/0/1">Dakarai Crowder</a>, <a href="http://arxiv.org/find/cs/1/au:+Mingolla_E/0/1/0/all/0/1">Ennio Mingolla</a></p>
<p>Adversarial attacks can affect the object recognition capabilities of
machines in wild. These can often result from spurious correlations between
input and class labels, and are prone to memorization in large networks. While
networks are expected to do automated feature selection, it is not effective at
the scale of the object. Humans, however, are able to select the minimum set of
features required to form a robust representation of an object. In this work,
we show that finetuning any pretrained off-the-shelf network with Extreme Image
Transformations (EIT) not only helps in learning a robust latent
representation, it also improves the performance of these networks against
common adversarial attacks of various intensities. Our EIT trained networks
show strong activations in the object regions even when tested with more
intense noise, showing promising generalizations across different kinds of
adversarial attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07726">Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content. (arXiv:2310.07726v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Guanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yifei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jie Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1">Shangwei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianwei Zhang</a></p>
<p>Artificial Intelligence Generated Content (AIGC) is gaining great popularity
in social media, with many commercial services available. These services
leverage advanced generative models, such as latent diffusion models and large
language models, to generate creative content (e.g., realistic images, fluent
sentences) for users. The usage of such generated content needs to be highly
regulated, as the service providers need to ensure the users do not violate the
usage policies (e.g., abuse for commercialization, generating and distributing
unsafe content).
</p>
<p>Numerous watermarking approaches have been proposed recently. However, in
this paper, we show that an adversary can easily break these watermarking
mechanisms. Specifically, we consider two possible attacks. (1) Watermark
removal: the adversary can easily erase the embedded watermark from the
generated content and then use it freely without the regulation of the service
provider. (2) Watermark forge: the adversary can create illegal content with
forged watermarks from another user, causing the service provider to make wrong
attributions. We propose WMaGi, a unified framework to achieve both attacks in
a holistic way. The key idea is to leverage a pre-trained diffusion model for
content processing, and a generative adversarial network for watermark removing
or forging. We evaluate WMaGi on different datasets and embedding setups. The
results prove that it can achieve high success rates while maintaining the
quality of the generated content. Compared with existing diffusion model-based
attacks, WMaGi is 5,050$\sim$11,000$\times$ faster.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07727">Deep Learning based Systems for Crater Detection: A Review. (arXiv:2310.07727v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tewari_A/0/1/0/all/0/1">Atal Tewari</a>, <a href="http://arxiv.org/find/cs/1/au:+Prateek_K/0/1/0/all/0/1">K Prateek</a>, <a href="http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1">Amrita Singh</a>, <a href="http://arxiv.org/find/cs/1/au:+Khanna_N/0/1/0/all/0/1">Nitin Khanna</a></p>
<p>Craters are one of the most prominent features on planetary surfaces, used in
applications such as age estimation, hazard detection, and spacecraft
navigation. Crater detection is a challenging problem due to various aspects,
including complex crater characteristics such as varying sizes and shapes, data
resolution, and planetary data types. Similar to other computer vision tasks,
deep learning-based approaches have significantly impacted research on crater
detection in recent years. This survey aims to assist researchers in this field
by examining the development of deep learning-based crater detection algorithms
(CDAs). The review includes over 140 research works covering diverse crater
detection approaches, including planetary data, craters database, and
evaluation metrics. To be specific, we discuss the challenges in crater
detection due to the complex properties of the craters and survey the DL-based
CDAs by categorizing them into three parts: (a) semantic segmentation-based,
(b) object detection-based, and (c) classification-based. Additionally, we have
conducted training and testing of all the semantic segmentation-based CDAs on a
common dataset to evaluate the effectiveness of each architecture for crater
detection and its potential applications. Finally, we have provided
recommendations for potential future works.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07730">Domain-Controlled Prompt Learning. (arXiv:2310.07730v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1">Qinglong Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zhengqin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuantian Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1">Chao Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1">Xiaokang Yang</a></p>
<p>Large pre-trained vision-language models, such as CLIP, have shown remarkable
generalization capabilities across various tasks when appropriate text prompts
are provided. However, adapting these models to specialized domains, like
remote sensing images (RSIs), medical images, etc, remains unexplored and
challenging. Existing prompt learning methods often lack domain-awareness or
domain-transfer mechanisms, leading to suboptimal performance due to the
misinterpretation of specialized images in natural image patterns. To tackle
this dilemma, we proposed a Domain-Controlled Prompt Learning for the
specialized domains. Specifically, the large-scale specialized domain
foundation model (LSDM) is first introduced to provide essential specialized
domain knowledge. Using lightweight neural networks, we transfer this knowledge
into domain biases, which control both the visual and language branches to
obtain domain-adaptive prompts in a directly incorporating manner.
Simultaneously, to overcome the existing overfitting challenge, we propose a
novel noisy-adding strategy, without extra trainable parameters, to help the
model escape the suboptimal solution in a global domain oscillation manner.
Experimental results show our method achieves state-of-the-art performance in
specialized domain image recognition datasets. Our code is available at
https://anonymous.4open.science/r/DCPL-8588.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07743">PointHR: Exploring High-Resolution Architectures for 3D Point Cloud Segmentation. (arXiv:2310.07743v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1">Haibo Qiu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1">Baosheng Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yixin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Significant progress has been made recently in point cloud segmentation
utilizing an encoder-decoder framework, which initially encodes point clouds
into low-resolution representations and subsequently decodes high-resolution
predictions. Inspired by the success of high-resolution architectures in image
dense prediction, which always maintains a high-resolution representation
throughout the entire learning process, we consider it also highly important
for 3D dense point cloud analysis. Therefore, in this paper, we explore
high-resolution architectures for 3D point cloud segmentation. Specifically, we
generalize high-resolution architectures using a unified pipeline named
PointHR, which includes a knn-based sequence operator for feature extraction
and a differential resampling operator to efficiently communicate different
resolutions. Additionally, we propose to avoid numerous on-the-fly computations
of high-resolution architectures by pre-computing the indices for both sequence
and resampling operators. By doing so, we deliver highly competitive
high-resolution architectures while capitalizing on the benefits of
well-designed point cloud blocks without additional effort. To evaluate these
architectures for dense point cloud analysis, we conduct thorough experiments
using S3DIS and ScanNetV2 datasets, where the proposed PointHR outperforms
recent state-of-the-art methods without any bells and whistles. The source code
is available at \url{https://github.com/haibo-qiu/PointHR}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07749">OpenLEAF: Open-Domain Interleaved Image-Text Generation and Evaluation. (arXiv:2310.07749v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1">Jie An</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhengyuan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linjie Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianfeng Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1">Kevin Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lijuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiebo Luo</a></p>
<p>This work investigates a challenging task named open-domain interleaved
image-text generation, which generates interleaved texts and images following
an input query. We propose a new interleaved generation framework based on
prompting large-language models (LLMs) and pre-trained text-to-image (T2I)
models, namely OpenLEAF. In OpenLEAF, the LLM generates textual descriptions,
coordinates T2I models, creates visual prompts for generating images, and
incorporates global contexts into the T2I models. This global context improves
the entity and style consistencies of images in the interleaved generation. For
model assessment, we first propose to use large multi-modal models (LMMs) to
evaluate the entity and style consistencies of open-domain interleaved
image-text sequences. According to the LMM evaluation on our constructed
evaluation set, the proposed interleaved generation framework can generate
high-quality image-text content for various domains and applications, such as
how-to question answering, storytelling, graphical story rewriting, and
webpage/poster generation tasks. Moreover, we validate the effectiveness of the
proposed LMM evaluation technique with human assessment. We hope our proposed
framework, benchmark, and LMM evaluation could help establish the intriguing
interleaved image-text generation task.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07771">DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. (arXiv:2310.07771v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaofan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifu Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1">Xiaoqing Ye</a></p>
<p>With the increasing popularity of autonomous driving based on the powerful
and unified bird's-eye-view (BEV) representation, a demand for high-quality and
large-scale multi-view video data with accurate annotation is urgently
required. However, such large-scale multi-view data is hard to obtain due to
expensive collection and annotation costs. To alleviate the problem, we propose
a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate
realistic multi-view videos controlled by 3D layout. There are three challenges
when synthesizing multi-view videos given a 3D layout: How to keep 1)
cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the
quality of the generated instances? Our DrivingDiffusion solves the problem by
cascading the multi-view single-frame image generation step, the single-view
video generation step shared by multiple cameras, and post-processing that can
handle long video generation. In the multi-view model, the consistency of
multi-view images is ensured by information exchange between adjacent cameras.
In the temporal model, we mainly query the information that needs attention in
subsequent frame generation from the multi-view images of the first frame. We
also introduce the local prompt to effectively improve the quality of generated
instances. In post-processing, we further enhance the cross-view consistency of
subsequent frames and extend the video length by employing temporal sliding
window algorithm. Without any extra cost, our model can generate large-scale
realistic multi-camera driving videos in complex urban scenes, fueling the
downstream driving tasks. The code will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07781">3D TransUNet: Advancing Medical Image Segmentation through Vision Transformers. (arXiv:2310.07781v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jieneng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1">Jieru Mei</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xianhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yongyi Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qihang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Q/0/1/0/all/0/1">Qingyue Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1">Xiangde Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1">Yutong Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1">Ehsan Adeli</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1">Matthew Lungren</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1">Lei Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1">Le Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1">Alan Yuille</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yuyin Zhou</a></p>
<p>Medical image segmentation plays a crucial role in advancing healthcare
systems for disease diagnosis and treatment planning. The u-shaped
architecture, popularly known as U-Net, has proven highly successful for
various medical image segmentation tasks. However, U-Net's convolution-based
operations inherently limit its ability to model long-range dependencies
effectively. To address these limitations, researchers have turned to
Transformers, renowned for their global self-attention mechanisms, as
alternative architectures. One popular network is our previous TransUNet, which
leverages Transformers' self-attention to complement U-Net's localized
information with the global context. In this paper, we extend the 2D TransUNet
architecture to a 3D network by building upon the state-of-the-art nnU-Net
architecture, and fully exploring Transformers' potential in both the encoder
and decoder design. We introduce two key components: 1) A Transformer encoder
that tokenizes image patches from a convolution neural network (CNN) feature
map, enabling the extraction of global contexts, and 2) A Transformer decoder
that adaptively refines candidate regions by utilizing cross-attention between
candidate proposals and U-Net features. Our investigations reveal that
different medical tasks benefit from distinct architectural designs. The
Transformer encoder excels in multi-organ segmentation, where the relationship
among organs is crucial. On the other hand, the Transformer decoder proves more
beneficial for dealing with small and challenging segmented targets such as
tumor segmentation. Extensive experiments showcase the significant potential of
integrating a Transformer-based encoder and decoder into the u-shaped medical
image segmentation architecture. TransUNet outperforms competitors in various
medical applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07782">An automated approach for improving the inference latency and energy efficiency of pretrained CNNs by removing irrelevant pixels with focused convolutions. (arXiv:2310.07782v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tung_C/0/1/0/all/0/1">Caleb Tung</a>, <a href="http://arxiv.org/find/cs/1/au:+Eliopoulos_N/0/1/0/all/0/1">Nicholas Eliopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Jajal_P/0/1/0/all/0/1">Purvish Jajal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramshankar_G/0/1/0/all/0/1">Gowri Ramshankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1">Chen-Yun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Synovic_N/0/1/0/all/0/1">Nicholas Synovic</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuecen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhary_V/0/1/0/all/0/1">Vipin Chaudhary</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1">George K. Thiruvathukal</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yung-Hsiang Lu</a></p>
<p>Computer vision often uses highly accurate Convolutional Neural Networks
(CNNs), but these deep learning models are associated with ever-increasing
energy and computation requirements. Producing more energy-efficient CNNs often
requires model training which can be cost-prohibitive. We propose a novel,
automated method to make a pretrained CNN more energy-efficient without
re-training. Given a pretrained CNN, we insert a threshold layer that filters
activations from the preceding layers to identify regions of the image that are
irrelevant, i.e. can be ignored by the following layers while maintaining
accuracy. Our modified focused convolution operation saves inference latency
(by up to 25%) and energy costs (by up to 22%) on various popular pretrained
CNNs, with little to no loss in accuracy.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07794">CRITERIA: a New Benchmarking Paradigm for Evaluating Trajectory Prediction Models for Autonomous Driving. (arXiv:2310.07794v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Changhe Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Pourkeshavarz_M/0/1/0/all/0/1">Mozhgan Pourkeshavarz</a>, <a href="http://arxiv.org/find/cs/1/au:+Rasouli_A/0/1/0/all/0/1">Amir Rasouli</a></p>
<p>Benchmarking is a common method for evaluating trajectory prediction models
for autonomous driving. Existing benchmarks rely on datasets, which are biased
towards more common scenarios, such as cruising, and distance-based metrics
that are computed by averaging over all scenarios. Following such a regiment
provides a little insight into the properties of the models both in terms of
how well they can handle different scenarios and how admissible and diverse
their outputs are. There exist a number of complementary metrics designed to
measure the admissibility and diversity of trajectories, however, they suffer
from biases, such as length of trajectories.
</p>
<p>In this paper, we propose a new benChmarking paRadIgm for evaluaTing
trajEctoRy predIction Approaches (CRITERIA). Particularly, we propose 1) a
method for extracting driving scenarios at varying levels of specificity
according to the structure of the roads, models' performance, and data
properties for fine-grained ranking of prediction models; 2) A set of new
bias-free metrics for measuring diversity, by incorporating the characteristics
of a given scenario, and admissibility, by considering the structure of roads
and kinematic compliancy, motivated by real-world driving constraints. 3) Using
the proposed benchmark, we conduct extensive experimentation on a
representative set of the prediction models using the large scale Argoverse
dataset. We show that the proposed benchmark can produce a more accurate
ranking of the models and serve as a means of characterizing their behavior. We
further present ablation studies to highlight contributions of different
elements that are used to compute the proposed metrics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07801">Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation. (arXiv:2310.07801v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1">Elvis Han Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bingbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yanan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1">Weng Kee Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1">Donghui Wang</a></p>
<p>Data augmentation for deep learning benefits model training, image
transformation, medical imaging analysis and many other fields. Many existing
methods generate new samples from a parametric distribution, like the Gaussian,
with little attention to generate samples along the data manifold in either the
input or feature space. In this paper, we verify that there are theoretical and
practical advantages of using the principal manifold hidden in the feature
space than the Gaussian distribution. We then propose a novel trajectory-aware
principal manifold framework to restore the manifold backbone and generate
samples along a specific trajectory. On top of the autoencoder architecture, we
further introduce an intrinsic dimension regularization term to make the
manifold more compact and enable few-shot image generation. Experimental
results show that the novel framework is able to extract more compact manifold
representation, improve classification accuracy and generate smooth
transformation among few samples.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07812">Automatic Identification of Stone-Handling Behaviour in Japanese Macaques Using LabGym Artificial Intelligence. (arXiv:2310.07812v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ardoin_T/0/1/0/all/0/1">Th&#xe9;o Ardoin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sueur_C/0/1/0/all/0/1">C&#xe9;dric Sueur</a> (IPHC, ANTHROPO LAB, IUF)</p>
<p>The latest advancements in artificial intelligence technology have opened
doors to the analysis of intricate behaviours. In light of this, ethologists
are actively exploring the potential of these innovations to streamline the
time-intensive process of behavioural analysis using video data. In the realm
of primatology, several tools have been developed for this purpose.
Nonetheless, each of these tools grapples with technical constraints that we
aim to surmount. To address these limitations, we have established a
comprehensive protocol designed to harness the capabilities of a cutting-edge
tool, LabGym. Our primary objective was to evaluate LabGym's suitability for
the analysis of primate behaviour, with a focus on Japanese macaques as our
model subjects. We have successfully developed a model that demonstrates a high
degree of accuracy in detecting Japanese macaques stone-handling behaviour. Our
behavioural analysis model was completed as per our initial expectations and
LabGym succeed to recognise stone-handling behaviour on videos. However, it is
important to note that our study's ability to draw definitive conclusions
regarding the quality of the behavioural analysis is hampered by the absence of
quantitative data within the specified timeframe. Nevertheless, our model
represents the pioneering endeavour, as far as our knowledge extends, in
leveraging LabGym for the analysis of primate behaviours. It lays the
groundwork for potential future research in this promising field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07814">Explorable Mesh Deformation Subspaces from Unstructured Generative Models. (arXiv:2310.07814v1 [cs.GR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maesumi_A/0/1/0/all/0/1">Arman Maesumi</a>, <a href="http://arxiv.org/find/cs/1/au:+Guerrero_P/0/1/0/all/0/1">Paul Guerrero</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_V/0/1/0/all/0/1">Vladimir G. Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Fisher_M/0/1/0/all/0/1">Matthew Fisher</a>, <a href="http://arxiv.org/find/cs/1/au:+Chaudhuri_S/0/1/0/all/0/1">Siddhartha Chaudhuri</a>, <a href="http://arxiv.org/find/cs/1/au:+Aigerman_N/0/1/0/all/0/1">Noam Aigerman</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritchie_D/0/1/0/all/0/1">Daniel Ritchie</a></p>
<p>Exploring variations of 3D shapes is a time-consuming process in traditional
3D modeling tools. Deep generative models of 3D shapes often feature continuous
latent spaces that can, in principle, be used to explore potential variations
starting from a set of input shapes. In practice, doing so can be problematic:
latent spaces are high dimensional and hard to visualize, contain shapes that
are not relevant to the input shapes, and linear paths through them often lead
to sub-optimal shape transitions. Furthermore, one would ideally be able to
explore variations in the original high-quality meshes used to train the
generative model, not its lower-quality output geometry. In this paper, we
present a method to explore variations among a given set of landmark shapes by
constructing a mapping from an easily-navigable 2D exploration space to a
subspace of a pre-trained generative model. We first describe how to find a
mapping that spans the set of input landmark shapes and exhibits smooth
variations between them. We then show how to turn the variations in this
subspace into deformation fields, to transfer those variations to high-quality
meshes for the landmark shapes. Our results show that our method can produce
visually-pleasing and easily-navigable 2D exploration spaces for several
different shape categories, especially as compared to prior work on learning
deformation spaces for 3D shapes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07855">CrIBo: Self-Supervised Learning via Cross-Image Object-Level Bootstrapping. (arXiv:2310.07855v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lebailly_T/0/1/0/all/0/1">Tim Lebailly</a>, <a href="http://arxiv.org/find/cs/1/au:+Stegmuller_T/0/1/0/all/0/1">Thomas Stegm&#xfc;ller</a>, <a href="http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1">Behzad Bozorgtabar</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1">Jean-Philippe Thiran</a>, <a href="http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1">Tinne Tuytelaars</a></p>
<p>Leveraging nearest neighbor retrieval for self-supervised representation
learning has proven beneficial with object-centric images. However, this
approach faces limitations when applied to scene-centric datasets, where
multiple objects within an image are only implicitly captured in the global
representation. Such global bootstrapping can lead to undesirable entanglement
of object representations. Furthermore, even object-centric datasets stand to
benefit from a finer-grained bootstrapping approach. In response to these
challenges, we introduce a novel Cross-Image Object-Level Bootstrapping method
tailored to enhance dense visual representation learning. By employing
object-level nearest neighbor bootstrapping throughout the training, CrIBo
emerges as a notably strong and adequate candidate for in-context learning,
leveraging nearest neighbor retrieval at test time. CrIBo shows
state-of-the-art performance on the latter task while being highly competitive
in more standard downstream segmentation tasks. Our code and pretrained models
will be publicly available upon acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07886">A Survey of Feature Types and Their Contributions for Camera Tampering Detection. (arXiv:2310.07886v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mantini_P/0/1/0/all/0/1">Pranav Mantini</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_S/0/1/0/all/0/1">Shishir K. Shah</a></p>
<p>Camera tamper detection is the ability to detect unauthorized and
unintentional alterations in surveillance cameras by analyzing the video.
Camera tampering can occur due to natural events or it can be caused
intentionally to disrupt surveillance. We cast tampering detection as a change
detection problem, and perform a review of the existing literature with
emphasis on feature types. We formulate tampering detection as a time series
analysis problem, and design experiments to study the robustness and capability
of various feature types. We compute ten features on real-world surveillance
video and apply time series analysis to ascertain their predictability, and
their capability to detect tampering. Finally, we quantify the performance of
various time series models using each feature type to detect tampering.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07887">Unsupervised Structured Noise Removal with Variational Lossy Autoencoder. (arXiv:2310.07887v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Salmon_B/0/1/0/all/0/1">Benjamin Salmon</a>, <a href="http://arxiv.org/find/eess/1/au:+Krull_A/0/1/0/all/0/1">Alexander Krull</a></p>
<p>Most unsupervised denoising methods are based on the assumption that imaging
noise is either pixel-independent, i.e., spatially uncorrelated, or
signal-independent, i.e., purely additive. However, in practice many imaging
setups, especially in microscopy, suffer from a combination of signal-dependent
noise (e.g. Poisson shot noise) and axis-aligned correlated noise (e.g. stripe
shaped scanning or readout artifacts). In this paper, we present the first
unsupervised deep learning-based denoiser that can remove this type of noise
without access to any clean images or a noise model. Unlike self-supervised
techniques, our method does not rely on removing pixels by masking or
subsampling so can utilize all available information. We implement a
Variational Autoencoder (VAE) with a specially designed autoregressive decoder
capable of modelling the noise component of an image but incapable of
independently modelling the underlying clean signal component. As a
consequence, our VAE's encoder learns to encode only underlying clean signal
content and to discard imaging noise. We also propose an additional decoder for
mapping the encoder's latent variables back into image space, thereby sampling
denoised images. Experimental results demonstrate that our approach surpasses
existing methods for self- and unsupervised image denoising while being robust
with respect to the size of the autoregressive receptive field. Code for this
project can be found at https://github.com/krulllab/DVLAE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07889">LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1">Bowen Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1">Rameswar Panda</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1">SouYoung Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1">Rogerio Feris</a>, <a href="http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1">Aude Oliva</a>, <a href="http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1">Phillip Isola</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yoon Kim</a></p>
<p>We explore the use of language as a perceptual representation for
vision-and-language navigation. Our approach uses off-the-shelf vision systems
(for image captioning and object detection) to convert an agent's egocentric
panoramic view at each time step into natural language descriptions. We then
finetune a pretrained language model to select an action, based on the current
view and the trajectory history, that would best fulfill the navigation
instructions. In contrast to the standard setup which adapts a pretrained
language model to work directly with continuous visual features from pretrained
vision models, our approach instead uses (discrete) language as the perceptual
representation. We explore two use cases of our language-based navigation
(LangNav) approach on the R2R vision-and-language navigation benchmark:
generating synthetic trajectories from a prompted large language model (GPT-4)
with which to finetune a smaller language model; and sim-to-real transfer where
we transfer a policy learned on a simulated environment (ALFRED) to a
real-world environment (R2R). Our approach is found to improve upon strong
baselines that rely on visual features in settings where only a few gold
trajectories (10-100) are available, demonstrating the potential of using
language as a perceptual representation for navigation tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07894">Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1">Kushagra Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1">Maja Rudolph</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1">Stephan Mandt</a></p>
<p>Diffusion models suffer from slow sample generation at inference time.
Therefore, developing a principled framework for fast deterministic/stochastic
sampling for a broader class of diffusion models is a promising direction. We
propose two complementary frameworks for accelerating sample generation in
pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate
integrators generalize DDIM, mapping the reverse diffusion dynamics to a more
amenable space for sampling. In contrast, splitting-based integrators, commonly
used in molecular dynamics, reduce the numerical simulation error by cleverly
alternating between numerical updates involving the data and auxiliary
variables. After extensively studying these methods empirically and
theoretically, we present a hybrid method that leads to the best-reported
performance for diffusion models in augmented spaces. Applied to Phase Space
Langevin Diffusion [Pandey &amp; Mandt, 2023] on CIFAR-10, our deterministic and
stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network
function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing
baselines, respectively. Our code and model checkpoints will be made publicly
available at \url{https://github.com/mandt-lab/PSLD}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07896">NoMaD: Goal Masked Diffusion Policies for Navigation and Exploration. (arXiv:2310.07896v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1">Ajay Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shah_D/0/1/0/all/0/1">Dhruv Shah</a>, <a href="http://arxiv.org/find/cs/1/au:+Glossop_C/0/1/0/all/0/1">Catherine Glossop</a>, <a href="http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1">Sergey Levine</a></p>
<p>Robotic learning for navigation in unfamiliar environments needs to provide
policies for both task-oriented navigation (i.e., reaching a goal that the
robot has located), and task-agnostic exploration (i.e., searching for a goal
in a novel setting). Typically, these roles are handled by separate models, for
example by using subgoal proposals, planning, or separate navigation
strategies. In this paper, we describe how we can train a single unified
diffusion policy to handle both goal-directed navigation and goal-agnostic
exploration, with the latter providing the ability to search novel
environments, and the former providing the ability to reach a user-specified
goal once it has been located. We show that this unified policy results in
better overall performance when navigating to visually indicated goals in novel
environments, as compared to approaches that use subgoal proposals from
generative models, or prior methods based on latent variable models. We
instantiate our method by using a large-scale Transformer-based policy trained
on data from multiple ground robots, with a diffusion model decoder to flexibly
handle both goal-conditioned and goal-agnostic navigation. Our experiments,
conducted on a real-world mobile robot platform, show effective navigation in
unseen environments in comparison with five alternative methods, and
demonstrate significant improvements in performance and lower collision rates,
despite utilizing smaller models than state-of-the-art approaches. For more
videos, code, and pre-trained model checkpoints, see
https://general-navigation-models.github.io/nomad/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07916">Dynamic Appearance Particle Neural Radiance Field. (arXiv:2310.07916v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1">Ancheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jun Li</a></p>
<p>Neural Radiance Fields (NeRFs) have shown great potential in modelling 3D
scenes. Dynamic NeRFs extend this model by capturing time-varying elements,
typically using deformation fields. The existing dynamic NeRFs employ a similar
Eulerian representation for both light radiance and deformation fields. This
leads to a close coupling of appearance and motion and lacks a physical
interpretation. In this work, we propose Dynamic Appearance Particle Neural
Radiance Field (DAP-NeRF), which introduces particle-based representation to
model the motions of visual elements in a dynamic 3D scene. DAP-NeRF consists
of superposition of a static field and a dynamic field. The dynamic field is
quantised as a collection of {\em appearance particles}, which carries the
visual information of a small dynamic element in the scene and is equipped with
a motion model. All components, including the static field, the visual features
and motion models of the particles, are learned from monocular videos without
any prior geometric knowledge of the scene. We develop an efficient
computational framework for the particle-based model. We also construct a new
dataset to evaluate motion modelling. Experimental results show that DAP-NeRF
is an effective technique to capture not only the appearance but also the
physically meaningful motions in a 3D dynamic scene.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07931">D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1">Adyasha Maharana</a>, <a href="http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1">Prateek Yadav</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>Analytical theories suggest that higher-quality data can lead to lower test
errors in models trained on a fixed data budget. Moreover, a model can be
trained on a lower compute budget without compromising performance if a dataset
can be stripped of its redundancies. Coreset selection (or data pruning) seeks
to select a subset of the training data so as to maximize the performance of
models trained on this subset, also referred to as coreset. There are two
dominant approaches: (1) geometry-based data selection for maximizing data
diversity in the coreset, and (2) functions that assign difficulty scores to
samples based on training dynamics. Optimizing for data diversity leads to a
coreset that is biased towards easier samples, whereas, selection by difficulty
ranking omits easy samples that are necessary for the training of deep learning
models. This demonstrates that data diversity and importance scores are two
complementary factors that need to be jointly considered during coreset
selection. We represent a dataset as an undirected graph and propose a novel
pruning algorithm, D2 Pruning, that uses forward and reverse message passing
over this dataset graph for coreset selection. D2 Pruning updates the
difficulty scores of each example by incorporating the difficulty of its
neighboring examples in the dataset graph. Then, these updated difficulty
scores direct a graph-based sampling method to select a coreset that
encapsulates both diverse and difficult regions of the dataset space. We
evaluate supervised and self-supervised versions of our method on various
vision and language datasets. Results show that D2 Pruning improves coreset
selection over previous state-of-the-art methods for up to 70% pruning rates.
Additionally, we find that using D2 Pruning for filtering large multimodal
datasets leads to increased diversity in the dataset and improved
generalization of pretrained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07932">What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1">Ran Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chenfeng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1">Masayoshi Tomizuka</a>, <a href="http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1">Jitendra Malik</a>, <a href="http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1">Andrea Bajcsy</a></p>
<p>When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human's input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL's reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot's.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07969">CleftGAN: Adapting A Style-Based Generative Adversarial Network To Create Images Depicting Cleft Lip Deformity. (arXiv:2310.07969v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayajneh_A/0/1/0/all/0/1">Abdullah Hayajneh</a>, <a href="http://arxiv.org/find/cs/1/au:+Serpedin_E/0/1/0/all/0/1">Erchin Serpedin</a>, <a href="http://arxiv.org/find/cs/1/au:+Shaqfeh_M/0/1/0/all/0/1">Mohammad Shaqfeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Glass_G/0/1/0/all/0/1">Graeme Glass</a>, <a href="http://arxiv.org/find/cs/1/au:+Stotland_M/0/1/0/all/0/1">Mitchell A. Stotland</a></p>
<p>A major obstacle when attempting to train a machine learning system to
evaluate facial clefts is the scarcity of large datasets of high-quality,
ethics board-approved patient images. In response, we have built a deep
learning-based cleft lip generator designed to produce an almost unlimited
number of artificial images exhibiting high-fidelity facsimiles of cleft lip
with wide variation. We undertook a transfer learning protocol testing
different versions of StyleGAN-ADA (a generative adversarial network image
generator incorporating adaptive data augmentation (ADA)) as the base model.
Training images depicting a variety of cleft deformities were pre-processed to
adjust for rotation, scaling, color adjustment and background blurring. The ADA
modification of the primary algorithm permitted construction of our new
generative model while requiring input of a relatively small number of training
images. Adversarial training was carried out using 514 unique frontal
photographs of cleft-affected faces to adapt a pre-trained model based on
70,000 normal faces. The Frechet Inception Distance (FID) was used to measure
the similarity of the newly generated facial images to the cleft training
dataset, while Perceptual Path Length (PPL) and the novel Divergence Index of
Severity Histograms (DISH) measures were also used to assess the performance of
the image generator that we dub CleftGAN. We found that StyleGAN3 with
translation invariance (StyleGAN3-t) performed optimally as a base model.
Generated images achieved a low FID reflecting a close similarity to our
training input dataset of genuine cleft images. Low PPL and DISH measures
reflected a smooth and semantically valid interpolation of images through the
transfer learning process and a similar distribution of severity in the
training and generated images, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07975">Self-supervised visual learning for analyzing firearms trafficking activities on the Web. (arXiv:2310.07975v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Konstantakos_S/0/1/0/all/0/1">Sotirios Konstantakos</a>, <a href="http://arxiv.org/find/cs/1/au:+Chalkiadaki_D/0/1/0/all/0/1">Despina Ioanna Chalkiadaki</a>, <a href="http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1">Ioannis Mademlis</a>, <a href="http://arxiv.org/find/cs/1/au:+Chrysochoou_A/0/1/0/all/0/1">Adamantia Anna Rebolledo Chrysochoou</a>, <a href="http://arxiv.org/find/cs/1/au:+Papadopoulos_G/0/1/0/all/0/1">Georgios Th. Papadopoulos</a></p>
<p>Automated visual firearms classification from RGB images is an important
real-world task with applications in public space security, intelligence
gathering and law enforcement investigations. When applied to images massively
crawled from the World Wide Web (including social media and dark Web sites), it
can serve as an important component of systems that attempt to identify
criminal firearms trafficking networks, by analyzing Big Data from open-source
intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology
for achieving this, with Convolutional Neural Networks (CNN) being typically
employed. The common transfer learning approach consists of pretraining on a
large-scale, generic annotated dataset for whole-image classification, such as
ImageNet-1k, and then finetuning the DNN on a smaller, annotated,
task-specific, downstream dataset for visual firearms classification. Neither
Visual Transformer (ViT) neural architectures nor Self-Supervised Learning
(SSL) approaches have been so far evaluated on this critical task. SSL
essentially consists of replacing the traditional supervised pretraining
objective with an unsupervised pretext task that does not require ground-truth
labels..
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07995">HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images. (arXiv:2310.07995v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhan Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yidan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1">Xiyu Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yongqiang Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xin Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1">Lulu Niu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Hui Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1">Lei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1">Yunping Ge</a></p>
<p>Height estimation has long been a pivotal topic within measurement and remote
sensing disciplines, proving critical for endeavours such as 3D urban
modelling, MR and autonomous driving. Traditional methods utilise stereo
matching or multisensor fusion, both well-established techniques that typically
necessitate multiple images from varying perspectives and adjunct sensors like
SAR, leading to substantial deployment costs. Single image height estimation
has emerged as an attractive alternative, boasting a larger data source variety
and simpler deployment. However, current methods suffer from limitations such
as fixed receptive fields, a lack of global information interaction, leading to
noticeable instance-level height deviations. The inherent complexity of height
prediction can result in a blurry estimation of object edge depth when using
mainstream regression methods based on fixed height division. This paper
presents a comprehensive solution for monocular height estimation in remote
sensing, termed HeightFormer, combining multilevel interactions and
image-adaptive classification-regression. It features the Multilevel
Interaction Backbone (MIB) and Image-adaptive Classification-regression Height
Generator (ICG). MIB supplements the fixed sample grid in CNN of the
conventional backbone network with tokens of different interaction ranges. It
is complemented by a pixel-, patch-, and feature map-level hierarchical
interaction mechanism, designed to relay spatial geometry information across
different scales and introducing a global receptive field to enhance the
quality of instance-level height estimation. The ICG dynamically generates
height partition for each image and reframes the traditional regression task,
using a refinement from coarse to fine classification-regression that
significantly mitigates the innate ill-posedness issue and drastically improves
edge sharpness.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07996">Reset It and Forget It: Relearning Last-Layer Weights Improves Continual and Transfer Learning. (arXiv:2310.07996v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Frati_L/0/1/0/all/0/1">Lapo Frati</a>, <a href="http://arxiv.org/find/cs/1/au:+Traft_N/0/1/0/all/0/1">Neil Traft</a>, <a href="http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1">Jeff Clune</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheney_N/0/1/0/all/0/1">Nick Cheney</a></p>
<p>This work identifies a simple pre-training mechanism that leads to
representations exhibiting better continual and transfer learning. This
mechanism -- the repeated resetting of weights in the last layer, which we
nickname "zapping" -- was originally designed for a meta-continual-learning
procedure, yet we show it is surprisingly applicable in many settings beyond
both meta-learning and continual learning. In our experiments, we wish to
transfer a pre-trained image classifier to a new set of classes, in a few
shots. We show that our zapping procedure results in improved transfer accuracy
and/or more rapid adaptation in both standard fine-tuning and continual
learning settings, while being simple to implement and computationally
efficient. In many cases, we achieve performance on par with state of the art
meta-learning without needing the expensive higher-order gradients, by using a
combination of zapping and sequential learning. An intuitive explanation for
the effectiveness of this zapping procedure is that representations trained
with repeated zapping learn features that are capable of rapidly adapting to
newly initialized classifiers. Such an approach may be considered a
computationally cheaper type of, or alternative to, meta-learning rapidly
adaptable features with higher-order gradients. This adds to recent work on the
usefulness of resetting neural network parameters during training, and invites
further investigation of this mechanism.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07997">Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering. (arXiv:2310.07997v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1">Wanjuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1">Wenbing Tao</a></p>
<p>Recently, learning neural implicit surface by volume rendering has been a
promising way for multi-view reconstruction. However, limited accuracy and
excessive time complexity remain bottlenecks that current methods urgently need
to overcome. To address these challenges, we propose a new method called
Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient
reconstruction. Point modeling is organically embedded into the volume
rendering to enhance and regularize the representation of implicit surface.
Specifically, to achieve precise point guidance and noise robustness, aleatoric
uncertainty of the point cloud is modeled to capture the distribution of noise
and estimate the reliability of points. Additionally, a Neural Projection
module connecting points and images is introduced to add geometric constraints
to the Signed Distance Function (SDF). To better compensate for geometric bias
between volume rendering and point modeling, high-fidelity points are filtered
into an Implicit Displacement Network to improve the representation of SDF.
Benefiting from our effective point guidance, lightweight networks are employed
to achieve an impressive 11x speedup compared to NeuS. Extensive experiments
show that our method yields high-quality surfaces, especially for fine-grained
details and smooth regions. Moreover, it exhibits strong robustness to both
noisy and sparse data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08002">MLP-AMDC: An MLP Architecture for Adaptive-Mask-based Dual-Camera snapshot hyperspectral imaging. (arXiv:2310.08002v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Cai_Z/0/1/0/all/0/1">Zeyu Cai</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1">Can Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1">Xunhao Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1">Shanghuan Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Jin_C/0/1/0/all/0/1">Chengqian Jin</a>, <a href="http://arxiv.org/find/eess/1/au:+Da_F/0/1/0/all/0/1">Feipeng Da</a></p>
<p>Coded Aperture Snapshot Spectral Imaging (CASSI) system has great advantages
over traditional methods in dynamically acquiring Hyper-Spectral Image (HSI),
but there are the following problems. 1) Traditional mask relies on random
patterns or analytical design, both of which limit the performance improvement
of CASSI. 2) Existing high-quality reconstruction algorithms are slow in
reconstruction and can only reconstruct scene information offline. To address
the above two problems, this paper designs the AMDC-CASSI system, introducing
RGB camera with CASSI based on Adaptive-Mask as multimodal input to improve the
reconstruction quality. The existing SOTA reconstruction schemes are based on
transformer, but the operation of self-attention pulls down the operation
efficiency of the network. In order to improve the inference speed of the
reconstruction network, this paper proposes An MLP Architecture for
Adaptive-Mask-based Dual-Camera (MLP-AMDC) to replace the transformer structure
of the network. Numerous experiments have shown that MLP performs no less well
than transformer-based structures for HSI reconstruction, while MLP greatly
improves the network inference speed and has less number of parameters and
operations, our method has a 8 db improvement over SOTA and at least a 5-fold
improvement in reconstruction speed. (https://github.com/caizeyu1992/MLP-AMDC.)
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08009">Dual-Stream Knowledge-Preserving Hashing for Unsupervised Video Retrieval. (arXiv:2310.08009v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Pandeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1">Hongtao Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1">Jiannan Ge</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1">Shaobo Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yongdong Zhang</a></p>
<p>Unsupervised video hashing usually optimizes binary codes by learning to
reconstruct input videos. Such reconstruction constraint spends much effort on
frame-level temporal context changes without focusing on video-level global
semantics that are more useful for retrieval. Hence, we address this problem by
decomposing video information into reconstruction-dependent and
semantic-dependent information, which disentangles the semantic extraction from
reconstruction constraint. Specifically, we first design a simple dual-stream
structure, including a temporal layer and a hash layer. Then, with the help of
semantic similarity knowledge obtained from self-supervision, the hash layer
learns to capture information for semantic retrieval, while the temporal layer
learns to capture the information for reconstruction. In this way, the model
naturally preserves the disentangled semantics into binary codes. Validated by
comprehensive experiments, our method consistently outperforms the
state-of-the-arts on three video benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08026">Beyond Sharing Weights in Decoupling Feature Learning Network for UAV RGB-Infrared Vehicle Re-Identification. (arXiv:2310.08026v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xingyue Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1">Jiahao Qi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Bin_K/0/1/0/all/0/1">Kangcheng Bin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1">Ping Zhong</a></p>
<p>Owing to the capacity of performing full-time target search, cross-modality
vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is
gaining more attention in both video surveillance and public security. However,
this promising and innovative research has not been studied sufficiently due to
the data inadequacy issue. Meanwhile, the cross-modality discrepancy and
orientation discrepancy challenges further aggravate the difficulty of this
task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named
UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with
16015 RGB and 13913 infrared images. Moreover, to meet cross-modality
discrepancy and orientation discrepancy challenges, we present a hybrid weights
decoupling network (HWDNet) to learn the shared discriminative
orientation-invariant features. For the first challenge, we proposed a hybrid
weights siamese network with a well-designed weight restrainer and its
corresponding objective function to learn both modality-specific and modality
shared information. In terms of the second challenge, three effective
decoupling structures with two pretext tasks are investigated to learn
orientation-invariant feature. Comprehensive experiments are carried out to
validate the effectiveness of the proposed method. The dataset and codes will
be released at https://github.com/moonstarL/UAV-CM-VeID.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08027">Exploring Large Language Models for Multi-Modal Out-of-Distribution Detection. (arXiv:2310.08027v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yi Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lang_H/0/1/0/all/0/1">Hao Lang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_K/0/1/0/all/0/1">Kaisheng Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a></p>
<p>Out-of-distribution (OOD) detection is essential for reliable and trustworthy
machine learning. Recent multi-modal OOD detection leverages textual
information from in-distribution (ID) class names for visual OOD detection, yet
it currently neglects the rich contextual information of ID classes. Large
language models (LLMs) encode a wealth of world knowledge and can be prompted
to generate descriptive features for each class. Indiscriminately using such
knowledge causes catastrophic damage to OOD detection due to LLMs'
hallucinations, as is observed by our analysis. In this paper, we propose to
apply world knowledge to enhance OOD detection performance through selective
generation from LLMs. Specifically, we introduce a consistency-based
uncertainty calibration method to estimate the confidence score of each
generation. We further extract visual objects from each image to fully
capitalize on the aforementioned world knowledge. Extensive experiments
demonstrate that our method consistently outperforms the state-of-the-art.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08035">BaSAL: Size Balanced Warm Start Active Learning for LiDAR Semantic Segmentation. (arXiv:2310.08035v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1">Jiarong Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Yancong Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Caesar_H/0/1/0/all/0/1">Holger Caesar</a></p>
<p>Active learning strives to reduce the need for costly data annotation, by
repeatedly querying an annotator to label the most informative samples from a
pool of unlabeled data and retraining a model from these samples. We identify
two problems with existing active learning methods for LiDAR semantic
segmentation. First, they ignore the severe class imbalance inherent in LiDAR
semantic segmentation datasets. Second, to bootstrap the active learning loop,
they train their initial model from randomly selected data samples, which leads
to low performance and is referred to as the cold start problem. To address
these problems we propose BaSAL, a size-balanced warm start active learning
model, based on the observation that each object class has a characteristic
size. By sampling object clusters according to their size, we can thus create a
size-balanced dataset that is also more class-balanced. Furthermore, in
contrast to existing information measures like entropy or CoreSet, size-based
sampling does not require an already trained model and thus can be used to
address the cold start problem. Results show that we are able to improve the
performance of the initial model by a large margin. Combining size-balanced
sampling and warm start with established information measures, our approach
achieves a comparable performance to training on the entire SemanticKITTI
dataset, despite using only 5% of the annotations, which outperforms existing
active learning methods. We also match the existing state-of-the-art in active
learning on nuScenes. Our code will be made available upon paper acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08038">Continual Learning via Manifold Expansion Replay. (arXiv:2310.08038v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1">Zihao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1">Xuan Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yufei Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianfeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jian Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingsong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xian Wei</a></p>
<p>In continual learning, the learner learns multiple tasks in sequence, with
data being acquired only once for each task. Catastrophic forgetting is a major
challenge to continual learning. To reduce forgetting, some existing
rehearsal-based methods use episodic memory to replay samples of previous
tasks. However, in the process of knowledge integration when learning a new
task, this strategy also suffers from catastrophic forgetting due to an
imbalance between old and new knowledge. To address this problem, we propose a
novel replay strategy called Manifold Expansion Replay (MaER). We argue that
expanding the implicit manifold of the knowledge representation in the episodic
memory helps to improve the robustness and expressiveness of the model. To this
end, we propose a greedy strategy to keep increasing the diameter of the
implicit manifold represented by the knowledge in the buffer during memory
management. In addition, we introduce Wasserstein distance instead of cross
entropy as distillation loss to preserve previous knowledge. With extensive
experimental validation on MNIST, CIFAR10, CIFAR100, and TinyImageNet, we show
that the proposed method significantly improves the accuracy in continual
learning setup, outperforming the state of the arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08042">X-HRNet: Towards Lightweight Human Pose Estimation with Spatially Unidimensional Self-Attention. (arXiv:2310.08042v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yixuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xuanhan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1">Xing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1">Lei Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jingkuan Song</a></p>
<p>High-resolution representation is necessary for human pose estimation to
achieve high performance, and the ensuing problem is high computational
complexity. In particular, predominant pose estimation methods estimate human
joints by 2D single-peak heatmaps. Each 2D heatmap can be horizontally and
vertically projected to and reconstructed by a pair of 1D heat vectors.
Inspired by this observation, we introduce a lightweight and powerful
alternative, Spatially Unidimensional Self-Attention (SUSA), to the pointwise
(1x1) convolution that is the main computational bottleneck in the depthwise
separable 3c3 convolution. Our SUSA reduces the computational complexity of the
pointwise (1x1) convolution by 96% without sacrificing accuracy. Furthermore,
we use the SUSA as the main module to build our lightweight pose estimation
backbone X-HRNet, where `X' represents the estimated cross-shape attention
vectors. Extensive experiments on the COCO benchmark demonstrate the
superiority of our X-HRNet, and comprehensive ablation studies show the
effectiveness of the SUSA modules. The code is publicly available at
https://github.com/cool-xuan/x-hrnet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08044">EC-Depth: Exploring the consistency of self-supervised monocular depth estimation under challenging scenes. (arXiv:2310.08044v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1">Ruijie Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Ziyang Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chuxin Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1">Jianfeng He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tianzhu Zhang</a></p>
<p>Self-supervised monocular depth estimation holds significant importance in
the fields of autonomous driving and robotics. However, existing methods are
typically designed to train and test on clear and pristine datasets,
overlooking the impact of various adverse conditions prevalent in real-world
scenarios. As a result, it is commonly observed that most self-supervised
monocular depth estimation methods struggle to perform adequately under
challenging conditions. To address this issue, we present EC-Depth, a novel
self-supervised two-stage training framework to achieve a robust depth
estimation, starting from the foundation of depth prediction consistency under
different perturbations. Leveraging the proposed perturbation-invariant depth
consistency constraint module and the consistency-based pseudo-label selection
module, our model attains accurate and consistent depth predictions in both
standard and challenging scenarios. Extensive experiments substantiate the
effectiveness of the proposed method. Moreover, our method surpasses existing
state-of-the-art methods on KITTI, KITTI-C and DrivingStereo benchmarks,
demonstrating its potential for enhancing the reliability of self-supervised
monocular depth estimation models in real-world applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08064">Age Estimation Based on Graph Convolutional Networks and Multi-head Attention Mechanisms. (arXiv:2310.08064v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1">Miaomiao Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1">Changwei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1">Shijin Yan</a></p>
<p>Age estimation technology is a part of facial recognition and has been
applied to identity authentication. This technology achieves the development
and application of a juvenile anti-addiction system by authenticating users in
the game. Convolutional Neural Network (CNN) and Transformer algorithms are
widely used in this application scenario. However, these two models cannot
flexibly extract and model features of faces with irregular shapes, and they
are ineffective in capturing key information. Furthermore, the above methods
will contain a lot of background information while extracting features, which
will interfere with the model. In consequence, it is easy to extract redundant
information from images. In this paper, a new modeling idea is proposed to
solve this problem, which can flexibly model irregular objects. The Graph
Convolutional Network (GCN) is used to extract features from irregular face
images effectively, and multi-head attention mechanisms are added to avoid
redundant features and capture key region information in the image. This model
can effectively improve the accuracy of age estimation and reduce the MAE error
value to about 3.64, which is better than the effect of today's age estimation
model, to improve the accuracy of face recognition and identity authentication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08068">Frequency-Aware Re-Parameterization for Over-Fitting Based Image Compression. (arXiv:2310.08068v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Ye_Y/0/1/0/all/0/1">Yun Ye</a>, <a href="http://arxiv.org/find/eess/1/au:+Pan_Y/0/1/0/all/0/1">Yanjie Pan</a>, <a href="http://arxiv.org/find/eess/1/au:+Jiang_Q/0/1/0/all/0/1">Qually Jiang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1">Ming Lu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fang_X/0/1/0/all/0/1">Xiaoran Fang</a>, <a href="http://arxiv.org/find/eess/1/au:+Xu_B/0/1/0/all/0/1">Beryl Xu</a></p>
<p>Over-fitting-based image compression requires weights compactness for
compression and fast convergence for practical use, posing challenges for deep
convolutional neural networks (CNNs) based methods. This paper presents a
simple re-parameterization method to train CNNs with reduced weights storage
and accelerated convergence. The convolution kernels are re-parameterized as a
weighted sum of discrete cosine transform (DCT) kernels enabling direct
optimization in the frequency domain. Combined with L1 regularization, the
proposed method surpasses vanilla convolutions by achieving a significantly
improved rate-distortion with low computational cost. The proposed method is
verified with extensive experiments of over-fitting-based image restoration on
various datasets, achieving up to -46.12% BD-rate on top of HEIF with only 200
iterations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08071">Learning Transferable Conceptual Prototypes for Interpretable Unsupervised Domain Adaptation. (arXiv:2310.08071v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1">Junyu Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xinhong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Changsheng Xu</a></p>
<p>Despite the great progress of unsupervised domain adaptation (UDA) with the
deep neural networks, current UDA models are opaque and cannot provide
promising explanations, limiting their applications in the scenarios that
require safe and controllable model decisions. At present, a surge of work
focuses on designing deep interpretable methods with adequate data annotations
and only a few methods consider the distributional shift problem. Most existing
interpretable UDA methods are post-hoc ones, which cannot facilitate the model
learning process for performance enhancement. In this paper, we propose an
inherently interpretable method, named Transferable Conceptual Prototype
Learning (TCPL), which could simultaneously interpret and improve the processes
of knowledge transfer and decision-making in UDA. To achieve this goal, we
design a hierarchically prototypical module that transfers categorical basic
concepts from the source domain to the target domain and learns domain-shared
prototypes for explaining the underlying reasoning process. With the learned
transferable prototypes, a self-predictive consistent pseudo-label strategy
that fuses confidence, predictions, and prototype information, is designed for
selecting suitable target samples for pseudo annotations and gradually
narrowing down the domain gap. Comprehensive experiments show that the proposed
method can not only provide effective and intuitive explanations but also
outperform previous state-of-the-arts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08073">Samples on Thin Ice: Re-Evaluating Adversarial Pruning of Neural Networks. (arXiv:2310.08073v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piras_G/0/1/0/all/0/1">Giorgio Piras</a>, <a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1">Maura Pintor</a>, <a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1">Ambra Demontis</a>, <a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1">Battista Biggio</a></p>
<p>Neural network pruning has shown to be an effective technique for reducing
the network size, trading desirable properties like generalization and
robustness to adversarial attacks for higher sparsity. Recent work has claimed
that adversarial pruning methods can produce sparse networks while also
preserving robustness to adversarial examples. In this work, we first
re-evaluate three state-of-the-art adversarial pruning methods, showing that
their robustness was indeed overestimated. We then compare pruned and dense
versions of the same models, discovering that samples on thin ice, i.e., closer
to the unpruned model's decision boundary, are typically misclassified after
pruning. We conclude by discussing how this intuition may lead to designing
more effective adversarial pruning methods in future work.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08080">RT-SRTS: Angle-Agnostic Real-Time Simultaneous 3D Reconstruction and Tumor Segmentation from Single X-Ray Projection. (arXiv:2310.08080v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Zhu_M/0/1/0/all/0/1">Miao Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Fu_Q/0/1/0/all/0/1">Qiming Fu</a>, <a href="http://arxiv.org/find/eess/1/au:+Liu_B/0/1/0/all/0/1">Bo Liu</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_M/0/1/0/all/0/1">Mengxi Zhang</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_B/0/1/0/all/0/1">Bojian Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Luo_X/0/1/0/all/0/1">Xiaoyan Luo</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhou_F/0/1/0/all/0/1">Fugen Zhou</a></p>
<p>Radiotherapy is one of the primary treatment methods for tumors, but the
organ movement caused by respiratory motion limits its accuracy. Recently, 3D
imaging from single X-ray projection receives extensive attentions as a
promising way to address this issue. However, current methods can only
reconstruct 3D image without direct location of the tumor and are only
validated for fixed-angle imaging, which fails to fully meet the requirement of
motion control in radiotherapy. In this study, we propose a novel imaging
method RT-SRTS which integrates 3D imaging and tumor segmentation into one
network based on the multi-task learning (MTL) and achieves real-time
simultaneous 3D reconstruction and tumor segmentation from single X-ray
projection at any angle. Futhermore, we propose the attention enhanced
calibrator (AEC) and uncertain-region elaboration (URE) modules to aid feature
extraction and improve segmentation accuracy. We evaluated the proposed method
on ten patient cases and compared it with two state-of-the-art methods. Our
approach not only delivered superior 3D reconstruction but also demonstrated
commendable tumor segmentation results. The simultaneous reconstruction and
segmentation could be completed in approximately 70 ms, significantly faster
than the required time threshold for real-time tumor tracking. The efficacy of
both AEC and URE was also validated through ablation studies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08082">Jointly Optimized Global-Local Visual Localization of UAVs. (arXiv:2310.08082v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Haoling Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiuniu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1">Zhiwei Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wenjia Xu</a></p>
<p>Navigation and localization of UAVs present a challenge when global
navigation satellite systems (GNSS) are disrupted and unreliable. Traditional
techniques, such as simultaneous localization and mapping (SLAM) and visual
odometry (VO), exhibit certain limitations in furnishing absolute coordinates
and mitigating error accumulation. Existing visual localization methods achieve
autonomous visual localization without error accumulation by matching with
ortho satellite images. However, doing so cannot guarantee real-time
performance due to the complex matching process. To address these challenges,
we propose a novel Global-Local Visual Localization (GLVL) network. Our GLVL
network is a two-stage visual localization approach, combining a large-scale
retrieval module that finds similar regions with the UAV flight scene, and a
fine-grained matching module that localizes the precise UAV coordinate,
enabling real-time and precise localization. The training process is jointly
optimized in an end-to-end manner to further enhance the model capability.
Experiments on six UAV flight scenes encompassing both texture-rich and
texture-sparse regions demonstrate the ability of our model to achieve the
real-time precise localization requirements of UAVs. Particularly, our method
achieves a localization error of only 2.39 meters in 0.48 seconds in a village
scene with sparse texture features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08084">Volumetric Medical Image Segmentation via Scribble Annotations and Shape Priors. (arXiv:2310.08084v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1">Qiuhui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_H/0/1/0/all/0/1">Haiying Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinyue Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1">Yong Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1">Yi Hong</a></p>
<p>Recently, weakly-supervised image segmentation using weak annotations like
scribbles has gained great attention in computer vision and medical image
analysis, since such annotations are much easier to obtain compared to
time-consuming and labor-intensive labeling at the pixel/voxel level. However,
due to a lack of structure supervision on regions of interest (ROIs), existing
scribble-based methods suffer from poor boundary localization. Furthermore,
most current methods are designed for 2D image segmentation, which do not fully
leverage the volumetric information if directly applied to each image slice. In
this paper, we propose a scribble-based volumetric image segmentation,
Scribble2D5, which tackles 3D anisotropic image segmentation and aims to its
improve boundary prediction. To achieve this, we augment a 2.5D attention UNet
with a proposed label propagation module to extend semantic information from
scribbles and use a combination of static and active boundary prediction to
learn ROI's boundary and regularize its shape. Also, we propose an optional
add-on component, which incorporates the shape prior information from unpaired
segmentation masks to further improve model accuracy. Extensive experiments on
three public datasets and one private dataset demonstrate our Scribble2D5
achieves state-of-the-art performance on volumetric image segmentation using
scribbles and shape prior if available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08092">Consistent123: Improve Consistency for One Image to 3D Object Synthesis. (arXiv:2310.08092v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weng_H/0/1/0/all/0/1">Haohan Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1">Tianyu Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jianan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1">Tong Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">C. L. Philip Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lei Zhang</a></p>
<p>Large image diffusion models enable novel view synthesis with high quality
and excellent zero-shot capability. However, such models based on
image-to-image translation have no guarantee of view consistency, limiting the
performance for downstream tasks like 3D reconstruction and image-to-3D
generation. To empower consistency, we propose Consistent123 to synthesize
novel views simultaneously by incorporating additional cross-view attention
layers and the shared self-attention mechanism. The proposed attention
mechanism improves the interaction across all synthesized views, as well as the
alignment between the condition view and novel views. In the sampling stage,
such architecture supports simultaneously generating an arbitrary number of
views while training at a fixed length. We also introduce a progressive
classifier-free guidance strategy to achieve the trade-off between texture and
geometry for synthesized object views. Qualitative and quantitative experiments
show that Consistent123 outperforms baselines in view consistency by a large
margin. Furthermore, we demonstrate a significant improvement of Consistent123
on varying downstream tasks, showing its great potential in the 3D generation
field. The project page is available at consistent-123.github.io.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08094">SingleInsert: Inserting New Concepts from a Single Image into Text-to-Image Models for Flexible Editing. (arXiv:2310.08094v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zijie Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1">Chaohui Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1">Zhen Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1">Fan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1">Xiang Bai</a></p>
<p>Recent progress in text-to-image (T2I) models enables high-quality image
generation with flexible textual control. To utilize the abundant visual priors
in the off-the-shelf T2I models, a series of methods try to invert an image to
proper embedding that aligns with the semantic space of the T2I model. However,
these image-to-text (I2T) inversion methods typically need multiple source
images containing the same concept or struggle with the imbalance between
editing flexibility and visual fidelity. In this work, we point out that the
critical problem lies in the foreground-background entanglement when learning
an intended concept, and propose a simple and effective baseline for
single-image I2T inversion, named SingleInsert. SingleInsert adopts a two-stage
scheme. In the first stage, we regulate the learned embedding to concentrate on
the foreground area without being associated with the irrelevant background. In
the second stage, we finetune the T2I model for better visual resemblance and
devise a semantic loss to prevent the language drift problem. With the proposed
techniques, SingleInsert excels in single concept generation with high visual
fidelity while allowing flexible editing. Additionally, SingleInsert can
perform single-image novel view synthesis and multiple concepts composition
without requiring joint training. To facilitate evaluation, we design an
editing prompt list and introduce a metric named Editing Success Rate (ESR) for
quantitative assessment of editing flexibility. Our project page is:
https://jarrentwu1031.github.io/SingleInsert-web/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08106">Generalized Logit Adjustment: Calibrating Fine-tuned Models by Removing Label Bias in Foundation Models. (arXiv:2310.08106v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Beier Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1">Kaihua Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1">Qianru Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hanwang Zhang</a></p>
<p>Foundation models like CLIP allow zero-shot transfer on various tasks without
additional training data. Yet, the zero-shot performance is less competitive
than a fully supervised one. Thus, to enhance the performance, fine-tuning and
ensembling are also commonly adopted to better fit the downstream tasks.
However, we argue that such prior work has overlooked the inherent biases in
foundation models. Due to the highly imbalanced Web-scale training set, these
foundation models are inevitably skewed toward frequent semantics, and thus the
subsequent fine-tuning or ensembling is still biased. In this study, we
systematically examine the biases in foundation models and demonstrate the
efficacy of our proposed Generalized Logit Adjustment (GLA) method. Note that
bias estimation in foundation models is challenging, as most pre-train data
cannot be explicitly accessed like in traditional long-tailed classification
tasks. To this end, GLA has an optimization-based bias estimation approach for
debiasing foundation models. As our work resolves a fundamental flaw in the
pre-training, the proposed GLA demonstrates significant improvements across a
diverse range of tasks: it achieves 1.5 pp accuracy gains on ImageNet, an large
average improvement (1.4-4.6 pp) on 11 few-shot datasets, 2.4 pp gains on
long-tailed classification. Codes are in \url{https://github.com/BeierZhu/GLA}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08116">Multimodal Active Measurement for Human Mesh Recovery in Close Proximity. (arXiv:2310.08116v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maeda_T/0/1/0/all/0/1">Takahiro Maeda</a>, <a href="http://arxiv.org/find/cs/1/au:+Takeshita_K/0/1/0/all/0/1">Keisuke Takeshita</a>, <a href="http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1">Kazuhito Tanaka</a></p>
<p>For safe and sophisticated physical human-robot interactions (pHRI), a robot
needs to estimate the accurate body pose or mesh of the target person. However,
in these pHRI scenarios, the robot cannot fully observe the target person's
body with equipped cameras because the target person is usually close to the
robot. This leads to severe truncation and occlusions, and results in poor
accuracy of human pose estimation. For better accuracy of human pose estimation
or mesh recovery on this limited information from cameras, we propose an active
measurement and sensor fusion framework of the equipped cameras and other
sensors such as touch sensors and 2D LiDAR. These touch and LiDAR sensing are
obtained attendantly through pHRI without additional costs. These sensor
measurements are sparse but reliable and informative cues for human mesh
recovery. In our active measurement process, camera viewpoints and sensor
placements are optimized based on the uncertainty of the estimated pose, which
is closely related to the truncated or occluded areas. In our sensor fusion
process, we fuse the sensor measurements to the camera-based estimated pose by
minimizing the distance between the estimated mesh and measured positions. Our
method is agnostic to robot configurations. Experiments were conducted using
the Toyota Human Support Robot, which has a camera, 2D LiDAR, and a touch
sensor on the robot arm. Our proposed method demonstrated the superiority in
the human pose estimation accuracy on the quantitative comparison. Furthermore,
our proposed method reliably estimated the pose of the target person in
practical settings such as target people occluded by a blanket and standing aid
with the robot arm.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08117">DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception. (arXiv:2310.08117v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1">Xianghao Kong</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1">Wentao Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1">Jinrang Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yifeng Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Runsheng Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1">Si Liu</a></p>
<p>Vehicle-to-Everything (V2X) collaborative perception is crucial for
autonomous driving. However, achieving high-precision V2X perception requires a
significant amount of annotated real-world data, which can always be expensive
and hard to acquire. Simulated data have raised much attention since they can
be massively produced at an extremely low cost. Nevertheless, the significant
domain gap between simulated and real-world data, including differences in
sensor type, reflectance patterns, and road surroundings, often leads to poor
performance of models trained on simulated data when evaluated on real-world
data. In addition, there remains a domain gap between real-world collaborative
agents, e.g. different types of sensors may be installed on autonomous vehicles
and roadside infrastructures with different extrinsics, further increasing the
difficulty of sim2real generalization. To take full advantage of simulated
data, we present a new unsupervised sim2real domain adaptation method for V2X
collaborative detection named Decoupled Unsupervised Sim2Real Adaptation
(DUSA). Our new method decouples the V2X collaborative sim2real domain
adaptation problem into two sub-problems: sim2real adaptation and inter-agent
adaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real
Adapter (LSA) module to adaptively aggregate features from critical locations
of the feature map and align the features between simulated data and real-world
data via a sim/real discriminator on the aggregated global feature. For
inter-agent adaptation, we further devise a Confidence-aware Inter-agent
Adapter (CIA) module to align the fine-grained features from heterogeneous
agents under the guidance of agent-wise confidence maps. Experiments
demonstrate the effectiveness of the proposed DUSA approach on unsupervised
sim2real adaptation from the simulated V2XSet dataset to the real-world
DAIR-V2X-C dataset.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08129">Tailored Visions: Enhancing Text-to-Image Generation with Personalized Prompt Rewriting. (arXiv:2310.08129v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zijie Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Lichao Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Weng_F/0/1/0/all/0/1">Fangsheng Weng</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1">Lili Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1">Zhenzhong Lan</a></p>
<p>We propose a novel perspective of viewing large pretrained models as search
engines, thereby enabling the repurposing of techniques previously used to
enhance search engine performance. As an illustration, we employ a personalized
query rewriting technique in the realm of text-to-image generation. Despite
significant progress in the field, it is still challenging to create
personalized visual representations that align closely with the desires and
preferences of individual users. This process requires users to articulate
their ideas in words that are both comprehensible to the models and accurately
capture their vision, posing difficulties for many users. In this paper, we
tackle this challenge by leveraging historical user interactions with the
system to enhance user prompts. We propose a novel approach that involves
rewriting user prompts based a new large-scale text-to-image dataset with over
300k prompts from 3115 users. Our rewriting model enhances the expressiveness
and alignment of user prompts with their intended visual outputs. Experimental
results demonstrate the superiority of our methods over baseline approaches, as
evidenced in our new offline evaluation method and online tests. Our approach
opens up exciting possibilities of applying more search engine techniques to
build truly personalized large pretrained models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08139">DualAug: Exploiting Additional Heavy Augmentation with OOD Data Rejection. (arXiv:2310.08139v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zehao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1">Yiwen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qizhang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1">Guanglei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1">Wangmeng Zuo</a></p>
<p>Data augmentation is a dominant method for reducing model overfitting and
improving generalization. Most existing data augmentation methods tend to find
a compromise in augmenting the data, \textit{i.e.}, increasing the amplitude of
augmentation carefully to avoid degrading some data too much and doing harm to
the model performance. We delve into the relationship between data augmentation
and model performance, revealing that the performance drop with heavy
augmentation comes from the presence of out-of-distribution (OOD) data.
Nonetheless, as the same data transformation has different effects for
different training samples, even for heavy augmentation, there remains part of
in-distribution data which is beneficial to model training. Based on the
observation, we propose a novel data augmentation method, named
\textbf{DualAug}, to keep the augmentation in distribution as much as possible
at a reasonable time and computational cost. We design a data mixing strategy
to fuse augmented data from both the basic- and the heavy-augmentation
branches. Extensive experiments on supervised image classification benchmarks
show that DualAug improve various automated data augmentation method. Moreover,
the experiments on semi-supervised learning and contrastive self-supervised
learning demonstrate that our DualAug can also improve related method. Code is
available at
\href{https://github.com/shuguang99/DualAug}{https://github.com/shuguang99/DualAug}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08142">Fine-Grained Annotation for Face Anti-Spoofing. (arXiv:2310.08142v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1">Xu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yunde Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuwei Wu</a></p>
<p>Face anti-spoofing plays a critical role in safeguarding facial recognition
systems against presentation attacks. While existing deep learning methods show
promising results, they still suffer from the lack of fine-grained annotations,
which lead models to learn task-irrelevant or unfaithful features. In this
paper, we propose a fine-grained annotation method for face anti-spoofing.
Specifically, we first leverage the Segment Anything Model (SAM) to obtain
pixel-wise segmentation masks by utilizing face landmarks as point prompts. The
face landmarks provide segmentation semantics, which segments the face into
regions. We then adopt these regions as masks and assemble them into three
separate annotation maps: spoof, living, and background maps. Finally, we
combine three separate maps into a three-channel map as annotations for model
training. Furthermore, we introduce the Multi-Channel Region Exchange
Augmentation (MCREA) to diversify training data and reduce overfitting.
Experimental results demonstrate that our method outperforms existing
state-of-the-art approaches in both intra-dataset and cross-dataset
evaluations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08143">A Deep Learning Framework for Spatiotemporal Ultrasound Localization Microscopy. (arXiv:2310.08143v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Milecki_L/0/1/0/all/0/1">L&#xe9;o Milecki</a>, <a href="http://arxiv.org/find/cs/1/au:+Poree_J/0/1/0/all/0/1">Jonathan Por&#xe9;e</a>, <a href="http://arxiv.org/find/cs/1/au:+Belgharbi_H/0/1/0/all/0/1">Hatim Belgharbi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bourquin_C/0/1/0/all/0/1">Chlo&#xe9; Bourquin</a>, <a href="http://arxiv.org/find/cs/1/au:+Damseh_R/0/1/0/all/0/1">Rafat Damseh</a>, <a href="http://arxiv.org/find/cs/1/au:+Delafontaine_Martel_P/0/1/0/all/0/1">Patrick Delafontaine-Martel</a>, <a href="http://arxiv.org/find/cs/1/au:+Lesage_F/0/1/0/all/0/1">Fr&#xe9;d&#xe9;ric Lesage</a>, <a href="http://arxiv.org/find/cs/1/au:+Gasse_M/0/1/0/all/0/1">Maxime Gasse</a>, <a href="http://arxiv.org/find/cs/1/au:+Provost_J/0/1/0/all/0/1">Jean Provost</a></p>
<p>Ultrasound Localization Microscopy can resolve the microvascular bed down to
a few micrometers. To achieve such performance microbubble contrast agents must
perfuse the entire microvascular network. Microbubbles are then located
individually and tracked over time to sample individual vessels, typically over
hundreds of thousands of images. To overcome the fundamental limit of
diffraction and achieve a dense reconstruction of the network, low microbubble
concentrations must be used, which lead to acquisitions lasting several
minutes. Conventional processing pipelines are currently unable to deal with
interference from multiple nearby microbubbles, further reducing achievable
concentrations. This work overcomes this problem by proposing a Deep Learning
approach to recover dense vascular networks from ultrasound acquisitions with
high microbubble concentrations. A realistic mouse brain microvascular network,
segmented from 2-photon microscopy, was used to train a three-dimensional
convolutional neural network based on a V-net architecture. Ultrasound data
sets from multiple microbubbles flowing through the microvascular network were
simulated and used as ground truth to train the 3D CNN to track microbubbles.
The 3D-CNN approach was validated in silico using a subset of the data and in
vivo on a rat brain acquisition. In silico, the CNN reconstructed vascular
networks with higher precision (81%) than a conventional ULM framework (70%).
In vivo, the CNN could resolve micro vessels as small as 10 $\mu$m with an
increase in resolution when compared against a conventional approach.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08165">COVID-19 Detection Using Swin Transformer Approach from Computed Tomography Images. (arXiv:2310.08165v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Morani_K/0/1/0/all/0/1">Kenan Morani</a></p>
<p>The accurate and efficient diagnosis of COVID-19 is of paramount importance,
particularly in the context of large-scale medical imaging datasets. In this
preprint paper, we propose a novel approach for COVID-19 diagnosis using CT
images that leverages the power of Swin Transformer models, state-of-the-art
solutions in computer vision tasks. Our method includes a systematic approach
for patient-level predictions, where individual CT slices are classified as
COVID-19 or non-COVID, and the patient's overall diagnosis is determined
through majority voting. The application of the Swin Transformer in this
context results in patient-level predictions that demonstrate exceptional
diagnostic accuracy. In terms of evaluation metrics, our approach consistently
outperforms the baseline, as well as numerous competing methods, showcasing its
effectiveness in COVID-19 diagnosis. The macro F1 score achieved by our model
exceeds the baseline and offers a robust solution for accurate diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08177">Improving Fast Minimum-Norm Attacks with Hyperparameter Optimization. (arXiv:2310.08177v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Floris_G/0/1/0/all/0/1">Giuseppe Floris</a>, <a href="http://arxiv.org/find/cs/1/au:+Mura_R/0/1/0/all/0/1">Raffaele Mura</a>, <a href="http://arxiv.org/find/cs/1/au:+Scionis_L/0/1/0/all/0/1">Luca Scionis</a>, <a href="http://arxiv.org/find/cs/1/au:+Piras_G/0/1/0/all/0/1">Giorgio Piras</a>, <a href="http://arxiv.org/find/cs/1/au:+Pintor_M/0/1/0/all/0/1">Maura Pintor</a>, <a href="http://arxiv.org/find/cs/1/au:+Demontis_A/0/1/0/all/0/1">Ambra Demontis</a>, <a href="http://arxiv.org/find/cs/1/au:+Biggio_B/0/1/0/all/0/1">Battista Biggio</a></p>
<p>Evaluating the adversarial robustness of machine learning models using
gradient-based attacks is challenging. In this work, we show that
hyperparameter optimization can improve fast minimum-norm attacks by automating
the selection of the loss function, the optimizer and the step-size scheduler,
along with the corresponding hyperparameters. Our extensive evaluation
involving several robust models demonstrates the improved efficacy of fast
minimum-norm attacks when hyper-up with hyperparameter optimization. We release
our open-source code at https://github.com/pralab/HO-FMN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08182">XIMAGENET-12: An Explainable AI Benchmark Dataset for Model Robustness Evaluation. (arXiv:2310.08182v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1">Qiang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lei_S/0/1/0/all/0/1">Shengzhao Lei</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xun Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuyan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Kamnoedboon_P/0/1/0/all/0/1">Porawit Kamnoedboon</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1">WeiWei Li</a></p>
<p>The lack of standardized robustness metrics and the widespread reliance on
numerous unrelated benchmark datasets for testing have created a gap between
academically validated robust models and their often problematic practical
adoption. To address this, we introduce XIMAGENET-12, an explainable benchmark
dataset with over 200K images and 15,600 manual semantic annotations. Covering
12 categories from ImageNet to represent objects commonly encountered in
practical life and simulating six diverse scenarios, including overexposure,
blurring, color changing, etc., we further propose a novel robustness criterion
that extends beyond model generation ability assessment. This benchmark
dataset, along with related code, is available at
https://sites.google.com/view/ximagenet-12/home. Researchers and practitioners
can leverage this resource to evaluate the robustness of their visual models
under challenging conditions and ultimately benefit from the demands of
practical computer vision systems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08204">Lifelong Audio-video Masked Autoencoder with Forget-robust Localized Alignments. (arXiv:2310.08204v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaewoo Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1">Jaehong Yoon</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_W/0/1/0/all/0/1">Wonjae Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yunji Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1">Sung Ju Hwang</a></p>
<p>We present a lifelong audio-video masked autoencoder that continually learns
the multimodal representations from a video stream containing audio-video
pairs, while its distribution continually shifts over time. Specifically, we
propose two novel ideas to tackle the problem: (1) Localized Alignment: We
introduce a small trainable multimodal encoder that predicts the audio and
video tokens that are well-aligned with each other. This allows the model to
learn only the highly correlated audiovisual patches with accurate multimodal
relationships. (2) Forget-robust multimodal patch selection: We compare the
relative importance of each audio-video patch between the current and past data
pair to mitigate unintended drift of the previously learned audio-video
representations. Our proposed method, FLAVA (Forget-robust Localized
Audio-Video Alignment), therefore, captures the complex relationships between
the audio and video modalities during training on a sequence of pre-training
tasks while alleviating the forgetting of learned audiovisual correlations. Our
experiments validate that FLAVA outperforms the state-of-the-art continual
learning methods on several benchmark datasets under continual audio-video
representation learning scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08206">Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jinye Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Ji Xu</a></p>
<p>Long-tailed(LT) classification is an unavoidable and challenging problem in
the real world. Most of the existing long-tailed classification methods focus
only on solving the inter-class imbalance in which there are more samples in
the head class than in the tail class, while ignoring the intra-lass imbalance
in which the number of samples of the head attribute within the same class is
much larger than the number of samples of the tail attribute. The deviation in
the model is caused by both of these factors, and due to the fact that
attributes are implicit in most datasets and the combination of attributes is
very complex, the intra-class imbalance is more difficult to handle. For this
purpose, we proposed a long-tailed classification framework, known as
\textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest
(CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint
solution model by means of invariant feature learning. In this method, we
designed an unsupervised learning method, i.e., CLF, to better characterize the
distribution of attributes within a class. Depending on the distribution of
attributes, we can flexibly construct sampling strategies suitable for
different environments. In addition, we introduce a new metric learning loss
(MCL), which aims to gradually eliminate confusing attributes during the
feature learning process. More importantly, this approach does not depend on a
specific model structure and can be integrated with existing LT methods as an
independent component. We have conducted extensive experiments and our approach
has state-of-the-art performance in both existing benchmarks ImageNet-GLT and
MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes
are available on GitHub: \url{https://github.com/jinyery/cognisance}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08217">TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion. (arXiv:2310.08217v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1">Preetha Vijayan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1">Prashant Bhat</a>, <a href="http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1">Elahe Arani</a>, <a href="http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1">Bahram Zonooz</a></p>
<p>Continual learning (CL) has remained a persistent challenge for deep neural
networks due to catastrophic forgetting (CF) of previously learned tasks.
Several techniques such as weight regularization, experience rehearsal, and
parameter isolation have been proposed to alleviate CF. Despite their relative
success, these research directions have predominantly remained orthogonal and
suffer from several shortcomings, while missing out on the advantages of
competing strategies. On the contrary, the brain continually learns,
accommodates, and transfers knowledge across tasks by simultaneously leveraging
several neurophysiological processes, including neurogenesis, active
forgetting, neuromodulation, metaplasticity, experience rehearsal, and
context-dependent gating, rarely resulting in CF. Inspired by how the brain
exploits multiple mechanisms concurrently, we propose TriRE, a novel CL
paradigm that encompasses retaining the most prominent neurons for each task,
revising and solidifying the extracted knowledge of current and past tasks, and
actively promoting less active neurons for subsequent tasks through rewinding
and relearning. Across CL settings, TriRE significantly reduces task
interference and surpasses different CL approaches considered in isolation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08222">Structural analysis of Hindi online handwritten characters for character recognition. (arXiv:2310.08222v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Anand Sharma</a> (MIET, Meerut), <a href="http://arxiv.org/find/cs/1/au:+Ramakrishnan_A/0/1/0/all/0/1">A. G. Ramakrishnan</a> (IISc, Bengaluru)</p>
<p>Direction properties of online strokes are used to analyze them in terms of
homogeneous regions or sub-strokes with points satisfying common geometric
properties. Such sub-strokes are called sub-units. These properties are used to
extract sub-units from Hindi ideal online characters. These properties along
with some heuristics are used to extract sub-units from Hindi online
handwritten characters.\\ A method is developed to extract point stroke,
clockwise curve stroke, counter-clockwise curve stroke and loop stroke segments
as sub-units from Hindi online handwritten characters. These extracted
sub-units are close in structure to the sub-units of the corresponding Hindi
online ideal characters.\\ Importance of local representation of online
handwritten characters in terms of sub-units is assessed by training a
classifier with sub-unit level local and character level global features
extracted from characters for character recognition. The classifier has the
recognition accuracy of 93.5\% on the testing set. This accuracy is the highest
when compared with that of the classifiers trained only with global features
extracted from characters in the same training set and evaluated on the same
testing set.\\ Sub-unit extraction algorithm and the sub-unit based character
classifier are tested on Hindi online handwritten character dataset. This
dataset consists of samples from 96 different characters. There are 12832 and
2821 samples in the training and testing sets, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08230">Fast Discrete Optimisation for Geometrically Consistent 3D Shape Matching. (arXiv:2310.08230v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roetzer_P/0/1/0/all/0/1">Paul Roetzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1">Ahmed Abbas</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_D/0/1/0/all/0/1">Dongliang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernard_F/0/1/0/all/0/1">Florian Bernard</a>, <a href="http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1">Paul Swoboda</a></p>
<p>In this work we propose to combine the advantages of learning-based and
combinatorial formalisms for 3D shape matching. While learning-based shape
matching solutions lead to state-of-the-art matching performance, they do not
ensure geometric consistency, so that obtained matchings are locally unsmooth.
On the contrary, axiomatic methods allow to take geometric consistency into
account by explicitly constraining the space of valid matchings. However,
existing axiomatic formalisms are impractical since they do not scale to
practically relevant problem sizes, or they require user input for the
initialisation of non-convex optimisation problems. In this work we aim to
close this gap by proposing a novel combinatorial solver that combines a unique
set of favourable properties: our approach is (i) initialisation free, (ii)
massively parallelisable powered by a quasi-Newton method, (iii) provides
optimality gaps, and (iv) delivers decreased runtime and globally optimal
results for many instances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08255">Distilling from Vision-Language Models for Improved OOD Generalization in Vision Tasks. (arXiv:2310.08255v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Addepalli_S/0/1/0/all/0/1">Sravanti Addepalli</a>, <a href="http://arxiv.org/find/cs/1/au:+Asokan_A/0/1/0/all/0/1">Ashish Ramayee Asokan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_L/0/1/0/all/0/1">Lakshay Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Babu_R/0/1/0/all/0/1">R. Venkatesh Babu</a></p>
<p>Vision-Language Models (VLMs) such as CLIP are trained on large amounts of
image-text pairs, resulting in remarkable generalization across several data
distributions. The prohibitively expensive training and data
collection/curation costs of these models make them valuable Intellectual
Property (IP) for organizations. This motivates a vendor-client paradigm, where
a vendor trains a large-scale VLM and grants only input-output access to
clients on a pay-per-query basis in a black-box setting. The client aims to
minimize inference cost by distilling the VLM to a student model using the
limited available task-specific data, and further deploying this student model
in the downstream application. While naive distillation largely improves the
In-Domain (ID) accuracy of the student, it fails to transfer the superior
out-of-distribution (OOD) generalization of the VLM teacher using the limited
available labeled images. To mitigate this, we propose Vision-Language to
Vision-Align, Distill, Predict (VL2V-ADiP), which first aligns the vision and
language modalities of the teacher model with the vision modality of a
pre-trained student model, and further distills the aligned VLM embeddings to
the student. This maximally retains the pre-trained features of the student,
while also incorporating the rich representations of the VLM image encoder and
the superior generalization of the text embeddings. The proposed approach
achieves state-of-the-art results on the standard Domain Generalization
benchmarks in a black-box teacher setting, and also when weights of the VLM are
accessible.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08259">Invisible Threats: Backdoor Attack in OCR Systems. (arXiv:2310.08259v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1">Mauro Conti</a>, <a href="http://arxiv.org/find/cs/1/au:+Farronato_N/0/1/0/all/0/1">Nicola Farronato</a>, <a href="http://arxiv.org/find/cs/1/au:+Koffas_S/0/1/0/all/0/1">Stefanos Koffas</a>, <a href="http://arxiv.org/find/cs/1/au:+Pajola_L/0/1/0/all/0/1">Luca Pajola</a>, <a href="http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1">Stjepan Picek</a></p>
<p>Optical Character Recognition (OCR) is a widely used tool to extract text
from scanned documents. Today, the state-of-the-art is achieved by exploiting
deep neural networks. However, the cost of this performance is paid at the
price of system vulnerability. For instance, in backdoor attacks, attackers
compromise the training phase by inserting a backdoor in the victim's model
that will be activated at testing time by specific patterns while leaving the
overall model performance intact. This work proposes a backdoor attack for OCR
resulting in the injection of non-readable characters from malicious input
images. This simple but effective attack exposes the state-of-the-art OCR
weakness, making the extracted text correct to human eyes but simultaneously
unusable for the NLP application that uses OCR as a preprocessing step.
Experimental results show that the attacked models successfully output
non-readable characters for around 90% of the poisoned instances without
harming their performance for the remaining instances.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08261">GraphAlign: Enhancing Accurate Feature Alignment by Graph matching for Multi-Modal 3D Object Detection. (arXiv:2310.08261v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1">Ziying Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1">Haiyue Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1">Lin Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1">Lei Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1">Caiyan Jia</a></p>
<p>LiDAR and cameras are complementary sensors for 3D object detection in
autonomous driving. However, it is challenging to explore the unnatural
interaction between point clouds and images, and the critical factor is how to
conduct feature alignment of heterogeneous modalities. Currently, many methods
achieve feature alignment by projection calibration only, without considering
the problem of coordinate conversion accuracy errors between sensors, leading
to sub-optimal performance. In this paper, we present GraphAlign, a more
accurate feature alignment strategy for 3D object detection by graph matching.
Specifically, we fuse image features from a semantic segmentation encoder in
the image branch and point cloud features from a 3D Sparse CNN in the LiDAR
branch. To save computation, we construct the nearest neighbor relationship by
calculating Euclidean distance within the subspaces that are divided into the
point cloud features. Through the projection calibration between the image and
point cloud, we project the nearest neighbors of point cloud features onto the
image features. Then by matching the nearest neighbors with a single point
cloud to multiple images, we search for a more appropriate feature alignment.
In addition, we provide a self-attention module to enhance the weights of
significant relations to fine-tune the feature alignment between heterogeneous
modalities. Extensive experiments on nuScenes benchmark demonstrate the
effectiveness and efficiency of our GraphAlign.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08276">Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qing Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1">Jiancheng Pan</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1">Cong Bai</a></p>
<p>Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the
distance between the final visual and textual embeddings in the latent semantic
space, oriented by regional visual features. Meanwhile, a lightweight Digging
Text Genome Assistant (DTGA) is designed to expand the range of tractable
textual representation and enhance global word-level semantic connections using
less attention operations. Ultimately, we exploit a global visual-semantic
constraint to reduce single visual dependency and serve as an external
constraint for the final visual and textual representations. The effectiveness
and superiority of our method are verified by extensive experiments including
parameter evaluation, quantitative comparison, ablation studies and visual
analysis, on two benchmark datasets, RSICD and RSITMD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08303">Multimodal Variational Auto-encoder based Audio-Visual Segmentation. (arXiv:2310.08303v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1">Yuxin Mao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jing Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1">Mochu Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1">Yiran Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yuchao Dai</a></p>
<p>We propose an Explicit Conditional Multimodal Variational Auto-Encoder
(ECMVAE) for audio-visual segmentation (AVS), aiming to segment sound sources
in the video sequence. Existing AVS methods focus on implicit feature fusion
strategies, where models are trained to fit the discrete samples in the
dataset. With a limited and less diverse dataset, the resulting performance is
usually unsatisfactory. In contrast, we address this problem from an effective
representation learning perspective, aiming to model the contribution of each
modality explicitly. Specifically, we find that audio contains critical
category information of the sound producers, and visual data provides candidate
sound producer(s). Their shared information corresponds to the target sound
producer(s) shown in the visual data. In this case, cross-modal shared
representation learning is especially important for AVS. To achieve this, our
ECMVAE factorizes the representations of each modality with a modality-shared
representation and a modality-specific representation. An orthogonality
constraint is applied between the shared and specific representations to
maintain the exclusive attribute of the factorized latent code. Further, a
mutual information maximization regularizer is introduced to achieve extensive
exploration of each modality. Quantitative and qualitative evaluations on the
AVSBench demonstrate the effectiveness of our approach, leading to a new
state-of-the-art for AVS, with a 3.84 mIOU performance leap on the challenging
MS3 subset for multiple sound source segmentation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08304">CHIP: Contrastive Hierarchical Image Pretraining. (arXiv:2310.08304v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1">Arpit Mittal</a>, <a href="http://arxiv.org/find/cs/1/au:+Jhaveri_H/0/1/0/all/0/1">Harshil Jhaveri</a>, <a href="http://arxiv.org/find/cs/1/au:+Mallick_S/0/1/0/all/0/1">Swapnil Mallick</a>, <a href="http://arxiv.org/find/cs/1/au:+Ajmera_A/0/1/0/all/0/1">Abhishek Ajmera</a></p>
<p>Few-shot object classification is the task of classifying objects in an image
with limited number of examples as supervision. We propose a one-shot/few-shot
classification model that can classify an object of any unseen class into a
relatively general category in an hierarchically based classification. Our
model uses a three-level hierarchical contrastive loss based ResNet152
classifier for classifying an object based on its features extracted from Image
embedding, not used during the training phase. For our experimentation, we have
used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal
classes for training our model and created our own dataset of unseen classes
for evaluating our trained model. Our model provides satisfactory results in
classifying the unknown objects into a generic category which has been later
discussed in greater detail.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08312">GePSAn: Generative Procedure Step Anticipation in Cooking Videos. (arXiv:2310.08312v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Abdelsalam_M/0/1/0/all/0/1">Mohamed Ashraf Abdelsalam</a>, <a href="http://arxiv.org/find/cs/1/au:+Rangrej_S/0/1/0/all/0/1">Samrudhdhi B. Rangrej</a>, <a href="http://arxiv.org/find/cs/1/au:+Hadji_I/0/1/0/all/0/1">Isma Hadji</a>, <a href="http://arxiv.org/find/cs/1/au:+Dvornik_N/0/1/0/all/0/1">Nikita Dvornik</a>, <a href="http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1">Konstantinos G. Derpanis</a>, <a href="http://arxiv.org/find/cs/1/au:+Fazly_A/0/1/0/all/0/1">Afsaneh Fazly</a></p>
<p>We study the problem of future step anticipation in procedural videos. Given
a video of an ongoing procedural activity, we predict a plausible next
procedure step described in rich natural language. While most previous work
focus on the problem of data scarcity in procedural video datasets, another
core challenge of future anticipation is how to account for multiple plausible
future realizations in natural settings. This problem has been largely
overlooked in previous work. To address this challenge, we frame future step
prediction as modelling the distribution of all possible candidates for the
next step. Specifically, we design a generative model that takes a series of
video clips as input, and generates multiple plausible and diverse candidates
(in natural language) for the next step. Following previous work, we side-step
the video annotation scarcity by pretraining our model on a large text-based
corpus of procedural activities, and then transfer the model to the video
domain. Our experiments, both in textual and video domains, show that our model
captures diversity in the next step prediction and generates multiple plausible
future predictions. Moreover, our model establishes new state-of-the-art
results on YouCookII, where it outperforms existing baselines on the next step
anticipation. Finally, we also show that our model can successfully transfer
from text to the video domain zero-shot, ie, without fine-tuning or adaptation,
and produces good-quality future step predictions from video.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08316">Extended target tracking utilizing machine-learning software -- with applications to animal classification. (arXiv:2310.08316v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Malmstrom_M/0/1/0/all/0/1">Magnus Malmstr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Kullberg_A/0/1/0/all/0/1">Anton Kullberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Skog_I/0/1/0/all/0/1">Isaac Skog</a>, <a href="http://arxiv.org/find/cs/1/au:+Axehill_D/0/1/0/all/0/1">Daniel Axehill</a>, <a href="http://arxiv.org/find/cs/1/au:+Gustafsson_F/0/1/0/all/0/1">Fredrik Gustafsson</a></p>
<p>This paper considers the problem of detecting and tracking objects in a
sequence of images. The problem is formulated in a filtering framework, using
the output of object-detection algorithms as measurements. An extension to the
filtering formulation is proposed that incorporates class information from the
previous frame to robustify the classification, even if the object-detection
algorithm outputs an incorrect prediction. Further, the properties of the
object-detection algorithm are exploited to quantify the uncertainty of the
bounding box detection in each frame. The complete filtering method is
evaluated on camera trap images of the four large Swedish carnivores, bear,
lynx, wolf, and wolverine. The experiments show that the class tracking
formulation leads to a more robust classification.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08320">Defending Our Privacy With Backdoors. (arXiv:2310.08320v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1">Dominik Hintersdorf</a>, <a href="http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1">Lukas Struppek</a>, <a href="http://arxiv.org/find/cs/1/au:+Neider_D/0/1/0/all/0/1">Daniel Neider</a>, <a href="http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1">Kristian Kersting</a></p>
<p>The proliferation of large AI models trained on uncurated, often sensitive
web-scraped data has raised significant privacy concerns. One of the concerns
is that adversaries can extract information about the training data using
privacy attacks. Unfortunately, the task of removing specific information from
the models without sacrificing performance is not straightforward and has
proven to be challenging. We propose a rather easy yet effective defense based
on backdoor attacks to remove private information such as names of individuals
from models, and focus in this work on text encoders. Specifically, through
strategic insertion of backdoors, we align the embeddings of sensitive phrases
with those of neutral terms-"a person" instead of the person's name. Our
empirical results demonstrate the effectiveness of our backdoor-based defense
on CLIP by assessing its performance using a specialized privacy attack for
zero-shot classifiers. Our approach provides not only a new "dual-use"
perspective on backdoor attacks, but also presents a promising avenue to
enhance the privacy of individuals within models trained on uncurated
web-scraped data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08326">NSM4D: Neural Scene Model Based Online 4D Point Cloud Sequence Understanding. (arXiv:2310.08326v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1">Yuhao Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhuoyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yunze Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1">Li Yi</a></p>
<p>Understanding 4D point cloud sequences online is of significant practical
value in various scenarios such as VR/AR, robotics, and autonomous driving. The
key goal is to continuously analyze the geometry and dynamics of a 3D scene as
unstructured and redundant point cloud sequences arrive. And the main challenge
is to effectively model the long-term history while keeping computational costs
manageable. To tackle these challenges, we introduce a generic online 4D
perception paradigm called NSM4D. NSM4D serves as a plug-and-play strategy that
can be adapted to existing 4D backbones, significantly enhancing their online
perception capabilities for both indoor and outdoor scenarios. To efficiently
capture the redundant 4D history, we propose a neural scene model that
factorizes geometry and motion information by constructing geometry tokens
separately storing geometry and motion features. Exploiting the history becomes
as straightforward as querying the neural scene model. As the sequence
progresses, the neural scene model dynamically deforms to align with new
observations, effectively providing the historical context and updating itself
with the new observations. By employing token representation, NSM4D also
exhibits robustness to low-level sensor noise and maintains a compact size
through a geometric sampling scheme. We integrate NSM4D with state-of-the-art
4D perception backbones, demonstrating significant improvements on various
online perception benchmarks in indoor and outdoor settings. Notably, we
achieve a 9.6% accuracy improvement for HOI4D online action segmentation and a
3.4% mIoU improvement for SemanticKITTI online semantic segmentation.
Furthermore, we show that NSM4D inherently offers excellent scalability to
longer sequences beyond the training set, which is crucial for real-world
applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08332">Real-Time Neural BRDF with Spherically Distributed Primitives. (arXiv:2310.08332v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1">Yishun Dou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1">Zhong Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1">Qiaoqiao Jin</a>, <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bingbing Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yugang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Ke_J/0/1/0/all/0/1">Junxiang Ke</a></p>
<p>We propose a novel compact and efficient neural BRDF offering highly
versatile material representation, yet with very-light memory and neural
computation consumption towards achieving real-time rendering. The results in
Figure 1, rendered at full HD resolution on a current desktop machine, show
that our system achieves real-time rendering with a wide variety of
appearances, which is approached by the following two designs. On the one hand,
noting that bidirectional reflectance is distributed in a very sparse
high-dimensional subspace, we propose to project the BRDF into two
low-dimensional components, i.e., two hemisphere feature-grids for incoming and
outgoing directions, respectively. On the other hand, learnable neural
reflectance primitives are distributed on our highly-tailored spherical surface
grid, which offer informative features for each component and alleviate the
conventional heavy feature learning network to a much smaller one, leading to
very fast evaluation. These primitives are centrally stored in a codebook and
can be shared across multiple grids and even across materials, based on the
low-cost indices stored in material-specific spherical surface grids. Our
neural BRDF, which is agnostic to the material, provides a unified framework
that can represent a variety of materials in consistent manner. Comprehensive
experimental results on measured BRDF compression, Monte Carlo simulated BRDF
acceleration, and extension to spatially varying effect demonstrate the
superior quality and generalizability achieved by the proposed scheme.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.08339">A Generic Software Framework for Distributed Topological Analysis Pipelines. (arXiv:2310.08339v1 [cs.DC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guillou_E/0/1/0/all/0/1">Eve Le Guillou</a>, <a href="http://arxiv.org/find/cs/1/au:+Will_M/0/1/0/all/0/1">Michael Will</a>, <a href="http://arxiv.org/find/cs/1/au:+Guillou_P/0/1/0/all/0/1">Pierre Guillou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukasczyk_J/0/1/0/all/0/1">Jonas Lukasczyk</a>, <a href="http://arxiv.org/find/cs/1/au:+Fortin_P/0/1/0/all/0/1">Pierre Fortin</a>, <a href="http://arxiv.org/find/cs/1/au:+Garth_C/0/1/0/all/0/1">Christoph Garth</a>, <a href="http://arxiv.org/find/cs/1/au:+Tierny_J/0/1/0/all/0/1">Julien Tierny</a></p>
<p>This system paper presents a software framework for the support of
topological analysis pipelines in a distributed-memory model. While several
recent papers introduced topology-based approaches for distributed-memory
environments, these were reporting experiments obtained with tailored,
mono-algorithm implementations. In contrast, we describe in this paper a
general-purpose, generic framework for topological analysis pipelines, i.e. a
sequence of topological algorithms interacting together, possibly on distinct
numbers of processes. Specifically, we instantiated our framework with the MPI
model, within the Topology ToolKit (TTK). While developing this framework, we
faced several algorithmic and software engineering challenges, which we
document in this paper. We provide a taxonomy for the distributed-memory
topological algorithms supported by TTK, depending on their communication needs
and provide examples of hybrid MPI+thread parallelizations. Detailed
performance analyses show that parallel efficiencies range from $20\%$ to
$80\%$ (depending on the algorithms), and that the MPI-specific preconditioning
introduced by our framework induces a negligible computation time overhead. We
illustrate the new distributed-memory capabilities of TTK with an example of
advanced analysis pipeline, combining multiple algorithms, run on the largest
publicly available dataset we have found (120 billion vertices) on a standard
cluster with 64 nodes (for a total of 1,536 cores). Finally, we provide a
roadmap for the completion of TTK's MPI extension, along with generic
recommendations for each algorithm communication category.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.09726">Soundify: Matching Sound Effects to Video. (arXiv:2112.09726v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1">David Chuan-En Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Germanidis_A/0/1/0/all/0/1">Anastasis Germanidis</a>, <a href="http://arxiv.org/find/cs/1/au:+Valenzuela_C/0/1/0/all/0/1">Crist&#xf3;bal Valenzuela</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yining Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Martelaro_N/0/1/0/all/0/1">Nikolas Martelaro</a></p>
<p>In the art of video editing, sound helps add character to an object and
immerse the viewer within a space. Through formative interviews with
professional editors (N=10), we found that the task of adding sounds to video
can be challenging. This paper presents Soundify, a system that assists editors
in matching sounds to video. Given a video, Soundify identifies matching
sounds, synchronizes the sounds to the video, and dynamically adjusts panning
and volume to create spatial audio. In a human evaluation study (N=889), we
show that Soundify is capable of matching sounds to video out-of-the-box for a
diverse range of audio categories. In a within-subjects expert study (N=12), we
demonstrate the usefulness of Soundify in helping video editors match sounds to
video with lighter workload, reduced task completion time, and improved
usability.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2112.15202">Visual and Object Geo-localization: A Comprehensive Survey. (arXiv:2112.15202v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wilson_D/0/1/0/all/0/1">Daniel Wilson</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xiaohan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Sultani_W/0/1/0/all/0/1">Waqas Sultani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wshah_S/0/1/0/all/0/1">Safwan Wshah</a></p>
<p>The concept of geo-localization refers to the process of determining where on
earth some `entity' is located, typically using Global Positioning System (GPS)
coordinates. The entity of interest may be an image, sequence of images, a
video, satellite image, or even objects visible within the image. As massive
datasets of GPS tagged media have rapidly become available due to smartphones
and the internet, and deep learning has risen to enhance the performance
capabilities of machine learning models, the fields of visual and object
geo-localization have emerged due to its significant impact on a wide range of
applications such as augmented reality, robotics, self-driving vehicles, road
maintenance, and 3D reconstruction. This paper provides a comprehensive survey
of geo-localization involving images, which involves either determining from
where an image has been captured (Image geo-localization) or geo-locating
objects within an image (Object geo-localization). We will provide an in-depth
study, including a summary of popular algorithms, a description of proposed
datasets, and an analysis of performance results to illustrate the current
state of each field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2202.09348">A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable&#x27;s Clouds More Real than His Contemporaries?. (arXiv:2202.09348v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhuomin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Mansfield_E/0/1/0/all/0/1">Elizabeth C. Mansfield</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jia Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Russell_J/0/1/0/all/0/1">John Russell</a>, <a href="http://arxiv.org/find/cs/1/au:+Young_G/0/1/0/all/0/1">George S. Young</a>, <a href="http://arxiv.org/find/cs/1/au:+Adams_C/0/1/0/all/0/1">Catherine Adams</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">James Z. Wang</a></p>
<p>The British landscape painter John Constable is considered foundational for
the Realist movement in 19th-century European painting. Constable's painted
skies, in particular, were seen as remarkably accurate by his contemporaries,
an impression shared by many viewers today. Yet, assessing the accuracy of
realist paintings like Constable's is subjective or intuitive, even for
professional art historians, making it difficult to say with certainty what set
Constable's skies apart from those of his contemporaries. Our goal is to
contribute to a more objective understanding of Constable's realism. We propose
a new machine-learning-based paradigm for studying pictorial realism in an
explainable way. Our framework assesses realism by measuring the similarity
between clouds painted by artists noted for their skies, like Constable, and
photographs of clouds. The experimental results of cloud classification show
that Constable approximates more consistently than his contemporaries the
formal features of actual clouds in his paintings. The study, as a novel
interdisciplinary approach that combines computer vision and machine learning,
meteorology, and art history, is a springboard for broader and deeper analyses
of pictorial realism.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2205.05188">On Scale Space Radon Transform, Properties and Application in CT Image Reconstruction. (arXiv:2205.05188v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nacereddine_N/0/1/0/all/0/1">Nafaa Nacereddine</a>, <a href="http://arxiv.org/find/cs/1/au:+Ziou_D/0/1/0/all/0/1">Djemel Ziou</a>, <a href="http://arxiv.org/find/cs/1/au:+Goumeidane_A/0/1/0/all/0/1">Aicha Baya Goumeidane</a></p>
<p>Since the Radon transform (RT) consists in a line integral function, some
modeling assumptions are made on Computed Tomography (CT) system, making image
reconstruction analytical methods, such as Filtered Backprojection (FBP),
sensitive to artifacts and noise. In the other hand, recently, a new integral
transform, called Scale Space Radon Transform (SSRT), is introduced where, RT
is a particular case. Thanks to its interesting properties, such as good scale
space behavior, the SSRT has known number of new applications. In this paper,
with the aim to improve the reconstructed image quality for these methods, we
propose to model the X-ray beam with the Scale Space Radon Transform (SSRT)
where, the assumptions done on the physical dimensions of the CT system
elements reflect better the reality. After depicting the basic properties and
the inversion of SSRT, the FBP algorithm is used to reconstruct the image from
the SSRT sinogram where the RT spectrum used in FBP is replaced by SSRT and the
Gaussian kernel, expressed in their frequency domain. PSNR and SSIM, as quality
measures, are used to compare RT and SSRT-based image reconstruction on
Shepp-Logan head and anthropomorphic abdominal phantoms. The first findings
show that the SSRT-based method outperforms the methods based on RT,
especially, when the number of projections is reduced, making it more
appropriate for applications requiring low-dose radiation, such as medical
X-ray CT. While SSRT-FBP and RT-FBP have utmost the same runtime, the
experiments show that SSRT-FBP is more robust to Poisson-Gaussian noise
corrupting CT data.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2206.02136">LDRNet: Enabling Real-time Document Localization on Mobile Devices. (arXiv:2206.02136v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Han Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1">Holland Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1">Huaming Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Moorsel_A/0/1/0/all/0/1">Aad van Moorsel</a></p>
<p>While Identity Document Verification (IDV) technology on mobile devices
becomes ubiquitous in modern business operations, the risk of identity theft
and fraud is increasing. The identity document holder is normally required to
participate in an online video interview to circumvent impostors. However, the
current IDV process depends on an additional human workforce to support online
step-by-step guidance which is inefficient and expensive. The performance of
existing AI-based approaches cannot meet the real-time and lightweight demands
of mobile devices. In this paper, we address those challenges by designing an
edge intelligence-assisted approach for real-time IDV. Aiming at improving the
responsiveness of the IDV process, we propose a new document localization model
for mobile devices, LDRNet, to Localize the identity Document in Real-time. On
the basis of a lightweight backbone network, we build three prediction branches
for LDRNet, the corner points prediction, the line borders prediction and the
document classification. We design novel supplementary targets, the
equal-division points, and use a new loss function named Line Loss, to improve
the speed and accuracy of our approach. In addition to the IDV process, LDRNet
is an efficient and reliable document localization alternative for all kinds of
mobile applications. As a matter of proof, we compare the performance of LDRNet
with other popular approaches on localizing general document datasets. The
experimental results show that LDRNet runs at a speed up to 790 FPS which is
47x faster, while still achieving comparable Jaccard Index(JI) in single-model
and single-scale tests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.09339">Vision Transformers: From Semantic Segmentation to Dense Prediction. (arXiv:2207.09339v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1">Li Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1">Jiachen Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1">Sixiao Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1">Xinxuan Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiatian Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1">Yanwei Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1">Tao Xiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1">Jianfeng Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip H.S. Torr</a></p>
<p>The emergence of vision transformers (ViTs) in image classification has
shifted the methodologies for visual representation learning. In particular,
ViTs learn visual representation at full receptive field per layer across all
the image patches, in comparison to the increasing receptive fields of CNNs
across layers and other alternatives (e.g., large kernels and atrous
convolution). In this work, for the first time we explore the global context
learning potentials of ViTs for dense visual prediction (e.g., semantic
segmentation). Our motivation is that through learning global context at full
receptive field layer by layer, ViTs may capture stronger long-range dependency
information, critical for dense prediction tasks. We first demonstrate that
encoding an image as a sequence of patches, a vanilla ViT without local
convolution and resolution reduction can yield stronger visual representation
for semantic segmentation. For example, our model, termed as SEgmentation
TRansformer (SETR), excels on ADE20K (50.28% mIoU, the first position in the
test leaderboard on the day of submission) and Pascal Context (55.83% mIoU),
and performs competitively on Cityscapes. For tackling general dense visual
prediction tasks in a cost-effective manner, we further formulate a family of
Hierarchical Local-Global (HLG) Transformers, characterized by local attention
within windows and global-attention across windows in a pyramidal architecture.
Extensive experiments show that our methods achieve appealing performance on a
variety of dense prediction tasks (e.g., object detection and instance
segmentation and semantic segmentation) as well as image classification. Our
code and models are available at https://github.com/fudan-zvg/SETR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2207.12389">MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation. (arXiv:2207.12389v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kalluri_T/0/1/0/all/0/1">Tarun Kalluri</a>, <a href="http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1">Astuti Sharma</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1">Manmohan Chandraker</a></p>
<p>Practical real world datasets with plentiful categories introduce new
challenges for unsupervised domain adaptation like small inter-class
discriminability, that existing approaches relying on domain invariance alone
cannot handle sufficiently well. In this work we propose MemSAC, which exploits
sample level similarity across source and target domains to achieve
discriminative transfer, along with architectures that scale to a large number
of categories. For this purpose, we first introduce a memory augmented approach
to efficiently extract pairwise similarity relations between labeled source and
unlabeled target domain instances, suited to handle an arbitrary number of
classes. Next, we propose and theoretically justify a novel variant of the
contrastive loss to promote local consistency among within-class cross domain
samples while enforcing separation between classes, thus preserving
discriminative transfer from source to target. We validate the advantages of
MemSAC with significant improvements over previous state-of-the-art on multiple
challenging transfer tasks designed for large-scale adaptation, such as
DomainNet with 345 classes and fine-grained adaptation on Caltech-UCSD birds
dataset with 200 classes. We also provide in-depth analysis and insights into
the effectiveness of MemSAC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2209.05167">LF-VISLAM: A SLAM Framework for Large Field-of-View Cameras with Negative Imaging Plane on Mobile Agents. (arXiv:2209.05167v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1">Hao Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1">Peng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1">Fei Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1">Jian Bai</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kaiwei Wang</a></p>
<p>Simultaneous Localization And Mapping (SLAM) has become a crucial aspect in
the fields of autonomous driving and robotics. One crucial component of visual
SLAM is the Field-of-View (FoV) of the camera, as a larger FoV allows for a
wider range of surrounding elements and features to be perceived. However, when
the FoV of the camera reaches the negative half-plane, traditional methods for
representing image feature points using [u,v,1]^T become ineffective. While the
panoramic FoV is advantageous for loop closure, its benefits are not easily
realized under large-attitude-angle differences where loop-closure frames
cannot be easily matched by existing methods. As loop closure on wide-FoV
panoramic data further comes with a large number of outliers, traditional
outlier rejection methods are not directly applicable. To address these issues,
we propose LF-VISLAM, a Visual Inertial SLAM framework for cameras with
extremely Large FoV with loop closure. A three-dimensional vector with unit
length is introduced to effectively represent feature points even on the
negative half-plane. The attitude information of the SLAM system is leveraged
to guide the feature point detection of the loop closure. Additionally, a new
outlier rejection method based on the unit length representation is integrated
into the loop closure module. We collect the PALVIO dataset using a Panoramic
Annular Lens (PAL) system with an entire FoV of 360{\deg}x(40{\deg}~120{\deg})
and an Inertial Measurement Unit (IMU) for Visual Inertial Odometry (VIO) to
address the lack of panoramic SLAM datasets. Experiments on the established
PALVIO and public datasets show that the proposed LF-VISLAM outperforms
state-of-the-art SLAM methods. Our code will be open-sourced at
https://github.com/flysoaryun/LF-VISLAM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.01244">Event-based Temporally Dense Optical Flow Estimation with Sequential Learning. (arXiv:2210.01244v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ponghiran_W/0/1/0/all/0/1">Wachirawit Ponghiran</a>, <a href="http://arxiv.org/find/cs/1/au:+Liyanagedera_C/0/1/0/all/0/1">Chamika Mihiranga Liyanagedera</a>, <a href="http://arxiv.org/find/cs/1/au:+Roy_K/0/1/0/all/0/1">Kaushik Roy</a></p>
<p>Event cameras provide an advantage over traditional frame-based cameras when
capturing fast-moving objects without a motion blur. They achieve this by
recording changes in light intensity (known as events), thus allowing them to
operate at a much higher frequency and making them suitable for capturing
motions in a highly dynamic scene. Many recent studies have proposed methods to
train neural networks (NNs) for predicting optical flow from events. However,
they often rely on a spatio-temporal representation constructed from events
over a fixed interval, such as 10Hz used in training on the DSEC dataset. This
limitation restricts the flow prediction to the same interval (10Hz) whereas
the fast speed of event cameras, which can operate up to 3kHz, has not been
effectively utilized. In this work, we show that a temporally dense flow
estimation at 100Hz can be achieved by treating the flow estimation as a
sequential problem using two different variants of recurrent networks -
Long-short term memory (LSTM) and spiking neural network (SNN). First, We
utilize the NN model constructed similar to the popular EV-FlowNet but with
LSTM layers to demonstrate the efficiency of our training method. The model not
only produces 10x more frequent optical flow than the existing ones, but the
estimated flows also have 13% lower errors than predictions from the baseline
EV-FlowNet. Second, we construct an EV-FlowNet SNN but with leaky integrate and
fire neurons to efficiently capture the temporal dynamics. We found that simple
inherent recurrent dynamics of SNN lead to significant parameter reduction
compared to the LSTM model. In addition, because of its event-driven
computation, the spiking model is estimated to consume only 1.5% energy of the
LSTM model, highlighting the efficiency of SNN in processing events and the
potential for achieving temporally dense flow.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.02081">Locate before Answering: Answer Guided Question Localization for Video Question Answering. (arXiv:2210.02081v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qian_T/0/1/0/all/0/1">Tianwen Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_R/0/1/0/all/0/1">Ran Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jingjing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1">Pai Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1">Xiaowei Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a></p>
<p>Video question answering (VideoQA) is an essential task in vision-language
understanding, which has attracted numerous research attention recently.
Nevertheless, existing works mostly achieve promising performances on short
videos of duration within 15 seconds. For VideoQA on minute-level long-term
videos, those methods are likely to fail because of lacking the ability to deal
with noise and redundancy caused by scene changes and multiple actions in the
video. Considering the fact that the question often remains concentrated in a
short temporal range, we propose to first locate the question to a segment in
the video and then infer the answer using the located segment only. Under this
scheme, we propose "Locate before Answering" (LocAns), a novel approach that
integrates a question locator and an answer predictor into an end-to-end model.
During the training phase, the available answer label not only serves as the
supervision signal of the answer predictor, but also is used to generate pseudo
temporal labels for the question locator. Moreover, we design a decoupled
alternative training strategy to update the two modules separately. In the
experiments, LocAns achieves state-of-the-art performance on two modern
long-term VideoQA datasets NExT-QA and ActivityNet-QA, and its qualitative
examples show the reliable performance of the question localization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.09222">MMTSA: Multimodal Temporal Segment Attention Network for Efficient Human Activity Recognition. (arXiv:2210.09222v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Ziqi Gao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuntao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1">Jianguo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1">Junliang Xing</a>, <a href="http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1">Shwetak Patel</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1">Yuanchun Shi</a></p>
<p>Multimodal sensors provide complementary information to develop accurate
machine-learning methods for human activity recognition (HAR), but introduce
significantly higher computational load, which reduces efficiency. This paper
proposes an efficient multimodal neural architecture for HAR using an RGB
camera and inertial measurement units (IMUs) called Multimodal Temporal Segment
Attention Network (MMTSA). MMTSA first transforms IMU sensor data into a
temporal and structure-preserving gray-scale image using the Gramian Angular
Field (GAF), representing the inherent properties of human activities. MMTSA
then applies a multimodal sparse sampling method to reduce data redundancy.
Lastly, MMTSA adopts an inter-segment attention module for efficient multimodal
fusion. Using three well-established public datasets, we evaluated MMTSA's
effectiveness and efficiency in HAR. Results show that our method achieves
superior performance improvements 11.13% of cross-subject F1-score on the MMAct
dataset than the previous state-of-the-art (SOTA) methods. The ablation study
and analysis suggest that MMTSA's effectiveness in fusing multimodal data for
accurate HAR. The efficiency evaluation on an edge device showed that MMTSA
achieved significantly better accuracy, lower computational load, and lower
inference latency than SOTA methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.11318">A Survey of Computer Vision Technologies In Urban and Controlled-environment Agriculture. (arXiv:2210.11318v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1">Jiayun Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Boyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Leung_C/0/1/0/all/0/1">Cyril Leung</a></p>
<p>In the evolution of agriculture to its next stage, Agriculture 5.0,
artificial intelligence will play a central role. Controlled-environment
agriculture, or CEA, is a special form of urban and suburban agricultural
practice that offers numerous economic, environmental, and social benefits,
including shorter transportation routes to population centers, reduced
environmental impact, and increased productivity. Due to its ability to control
environmental factors, CEA couples well with computer vision (CV) in the
adoption of real-time monitoring of the plant conditions and autonomous
cultivation and harvesting. The objective of this paper is to familiarize CV
researchers with agricultural applications and agricultural practitioners with
the solutions offered by CV. We identify five major CV applications in CEA,
analyze their requirements and motivation, and survey the state of the art as
reflected in 68 technical papers using deep learning methods. In addition, we
discuss five key subareas of computer vision and how they related to these CEA
problems, as well as eleven vision-based CEA datasets. We hope the survey will
help researchers quickly gain a bird-eye view of the striving research area and
will spark inspiration for new research and development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.14558">Compressing And Debiasing Vision-Language Pre-Trained Models for Visual Question Answering. (arXiv:2210.14558v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1">Qingyi Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yuanxin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zheng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1">Peng Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Weiping Wang</a></p>
<p>Despite the excellent performance of vision-language pre-trained models
(VLPs) on conventional VQA task, they still suffer from two problems: First,
VLPs tend to rely on language biases in datasets and fail to generalize to
out-of-distribution (OOD) data. Second, they are inefficient in terms of memory
footprint and computation. Although promising progress has been made in both
problems, most existing works tackle them independently. To facilitate the
application of VLP to VQA tasks, it is imperative to jointly study VLP
compression and OOD robustness, which, however, has not yet been explored. This
paper investigates whether a VLP can be compressed and debiased simultaneously
by searching sparse and robust subnetworks. To this end, we systematically
study the design of a training and compression pipeline to search the
subnetworks, as well as the assignment of sparsity to different
modality-specific modules. Our experiments involve 3 VLPs, 2 compression
methods, 4 training methods, 2 datasets and a range of sparsity levels and
random seeds. Our results show that there indeed exist sparse and robust
subnetworks, which are competitive with the debiased full VLP and clearly
outperform the debiasing SoTAs with fewer parameters on OOD datasets VQA-CP v2
and VQA-VS. The codes can be found at
https://github.com/PhoebusSi/Compress-Robust-VQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.15889">Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1">Wenguan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1">Fei Wu</a></p>
<p>Neural-symbolic computing (NeSy), which pursues the integration of the
symbolic and statistical paradigms of cognition, has been an active research
area of Artificial Intelligence (AI) for many years. As NeSy shows promise of
reconciling the advantages of reasoning and interpretability of symbolic
representation and robust learning in neural networks, it may serve as a
catalyst for the next generation of AI. In the present paper, we provide a
systematic overview of the recent developments and important contributions of
NeSy research. Firstly, we introduce study history of this area, covering early
work and foundations. We further discuss background concepts and identify key
driving factors behind the development of NeSy. Afterward, we categorize recent
landmark approaches along several main characteristics that underline this
research paradigm, including neural-symbolic integration, knowledge
representation, knowledge embedding, and functionality. Next, we briefly
discuss the successful application of modern NeSy approaches in several
domains. Then, we benchmark several NeSy methods on three representative
application tasks. Finally, we identify the open problems together with
potential future research directions. This survey is expected to help new
researchers enter this rapidly evolving field and accelerate the progress
towards data-and knowledge-driven AI.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.14108">3DDesigner: Towards Photorealistic 3D Object Generation and Editing with Text-guided Diffusion Models. (arXiv:2211.14108v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1">Gang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1">Heliang Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chaoyue Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Changwen Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1">Dacheng Tao</a></p>
<p>Text-guided diffusion models have shown superior performance in image/video
generation and editing. While few explorations have been performed in 3D
scenarios. In this paper, we discuss three fundamental and interesting problems
on this topic. First, we equip text-guided diffusion models to achieve
3D-consistent generation. Specifically, we integrate a NeRF-like neural field
to generate low-resolution coarse results for a given camera view. Such results
can provide 3D priors as condition information for the following diffusion
process. During denoising diffusion, we further enhance the 3D consistency by
modeling cross-view correspondences with a novel two-stream (corresponding to
two different views) asynchronous diffusion process. Second, we study 3D local
editing and propose a two-step solution that can generate 360-degree
manipulated results by editing an object from a single view. Step 1, we propose
to perform 2D local editing by blending the predicted noises. Step 2, we
conduct a noise-to-text inversion process that maps 2D blended noises into the
view-independent text embedding space. Once the corresponding text embedding is
obtained, 360-degree images can be generated. Last but not least, we extend our
model to perform one-shot novel view synthesis by fine-tuning on a single
image, firstly showing the potential of leveraging text guidance for novel view
synthesis. Extensive experiments and various applications show the prowess of
our 3DDesigner. The project page is available at
https://3ddesigner-diffusion.github.io/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00564">Leveraging Single-View Images for Unsupervised 3D Point Cloud Completion. (arXiv:2212.00564v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1">Lintai Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qijian Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a></p>
<p>Point clouds captured by scanning devices are often incomplete due to
occlusion. To overcome this limitation, point cloud completion methods have
been developed to predict the complete shape of an object based on its partial
input. These methods can be broadly classified as supervised or unsupervised.
However, both categories require a large number of 3D complete point clouds,
which may be difficult to capture. In this paper, we propose Cross-PCC, an
unsupervised point cloud completion method without requiring any 3D complete
point clouds. We only utilize 2D images of the complete objects, which are
easier to capture than 3D complete and clean point clouds. Specifically, to
take advantage of the complementary information from 2D images, we use a
single-view RGB image to extract 2D features and design a fusion module to fuse
the 2D and 3D features extracted from the partial point cloud. To guide the
shape of predicted point clouds, we project the predicted points of the object
to the 2D plane and use the foreground pixels of its silhouette maps to
constrain the position of the projected points. To reduce the outliers of the
predicted point clouds, we propose a view calibrator to move the points
projected to the background into the foreground by the single-view silhouette
image. To the best of our knowledge, our approach is the first point cloud
completion method that does not require any 3D supervision. The experimental
results of our method are superior to those of the state-of-the-art
unsupervised methods by a large margin. Moreover, our method even achieves
comparable performance to some supervised methods. We will make the source code
publicly available at https://github.com/ltwu6/cross-pcc.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.00613">NeuWigs: A Neural Dynamic Model for Volumetric Hair Capture and Animation. (arXiv:2212.00613v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1">Giljoo Nam</a>, <a href="http://arxiv.org/find/cs/1/au:+Stuyck_T/0/1/0/all/0/1">Tuur Stuyck</a>, <a href="http://arxiv.org/find/cs/1/au:+Lombardi_S/0/1/0/all/0/1">Stephen Lombardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1">Chen Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1">Jason Saragih</a>, <a href="http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1">Michael Zollhoefer</a>, <a href="http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1">Jessica Hodgins</a>, <a href="http://arxiv.org/find/cs/1/au:+Lassner_C/0/1/0/all/0/1">Christoph Lassner</a></p>
<p>The capture and animation of human hair are two of the major challenges in
the creation of realistic avatars for the virtual reality. Both problems are
highly challenging, because hair has complex geometry and appearance, as well
as exhibits challenging motion. In this paper, we present a two-stage approach
that models hair independently from the head to address these challenges in a
data-driven manner. The first stage, state compression, learns a
low-dimensional latent space of 3D hair states containing motion and
appearance, via a novel autoencoder-as-a-tracker strategy. To better
disentangle the hair and head in appearance learning, we employ multi-view hair
segmentation masks in combination with a differentiable volumetric renderer.
The second stage learns a novel hair dynamics model that performs temporal hair
transfer based on the discovered latent codes. To enforce higher stability
while driving our dynamics model, we employ the 3D point-cloud autoencoder from
the compression stage for de-noising of the hair state. Our model outperforms
the state of the art in novel view synthesis and is capable of creating novel
hair animations without having to rely on hair observations as a driving
signal. Project page is here https://ziyanw1.github.io/neuwigs/.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.09408">Universal Object Detection with Large Vision Model. (arXiv:2212.09408v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1">Feng Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1">Wenze Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yaowei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1">Yonghong Tian</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1">Guangming Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1">Fanglin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yong Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xiaoyu Wang</a></p>
<p>Over the past few years, there has been growing interest in developing a
broad, universal, and general-purpose computer vision system. Such systems have
the potential to address a wide range of vision tasks simultaneously, without
being limited to specific problems or data domains. This universality is
crucial for practical, real-world computer vision applications. In this study,
our focus is on a specific challenge: the large-scale, multi-domain universal
object detection problem, which contributes to the broader goal of achieving a
universal vision system. This problem presents several intricate challenges,
including cross-dataset category label duplication, label conflicts, and the
necessity to handle hierarchical taxonomies. To address these challenges, we
introduce our approach to label handling, hierarchy-aware loss design, and
resource-efficient model training utilizing a pre-trained large vision model.
Our method has demonstrated remarkable performance, securing a prestigious
second-place ranking in the object detection track of the Robust Vision
Challenge 2022 (RVC 2022) on a million-scale cross-dataset object detection
benchmark. We believe that our comprehensive study will serve as a valuable
reference and offer an alternative approach for addressing similar challenges
within the computer vision community. The source code for our work is openly
available at https://github.com/linfeng93/Large-UniDet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.11514">Deep Industrial Image Anomaly Detection: A Survey. (arXiv:2301.11514v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1">Guoyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinbao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shangnian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaochu Jin</a></p>
<p>The recent rapid development of deep learning has laid a milestone in
industrial Image Anomaly Detection (IAD). In this paper, we provide a
comprehensive review of deep learning-based image anomaly detection techniques,
from the perspectives of neural network architectures, levels of supervision,
loss functions, metrics and datasets. In addition, we extract the new setting
from industrial manufacturing and review the current IAD approaches under our
proposed our new setting. Moreover, we highlight several opening challenges for
image anomaly detection. The merits and downsides of representative network
architectures under varying supervision are discussed. Finally, we summarize
the research findings and point out future research directions. More resources
are available at
https://github.com/M-3LAB/awesome-industrial-anomaly-detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.12082">Pushing the Limits of Fewshot Anomaly Detection in Industry Vision: Graphcore. (arXiv:2301.12082v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1">Guoyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinbao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaochu Jin</a></p>
<p>In the area of fewshot anomaly detection (FSAD), efficient visual feature
plays an essential role in memory bank M-based methods. However, these methods
do not account for the relationship between the visual feature and its rotated
visual feature, drastically limiting the anomaly detection performance. To push
the limits, we reveal that rotation-invariant feature property has a
significant impact in industrial-based FSAD. Specifically, we utilize graph
representation in FSAD and provide a novel visual isometric invariant feature
(VIIF) as anomaly measurement feature. As a result, VIIF can robustly improve
the anomaly discriminating ability and can further reduce the size of redundant
features stored in M by a large amount. Besides, we provide a novel model
GraphCore via VIIFs that can fast implement unsupervised FSAD training and can
improve the performance of anomaly detection. A comprehensive evaluation is
provided for comparing GraphCore and other SOTA anomaly detection models under
our proposed fewshot anomaly detection setting, which shows GraphCore can
increase average AUC by 5.8%, 4.1%, 3.4%, and 1.6% on MVTec AD and by 25.5%,
22.0%, 16.9%, and 14.1% on MPDD for 1, 2, 4, and 8-shot cases, respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.13359">IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1">Guoyang Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jinbao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiaqi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1">Jiayi Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chengjie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Feng Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1">Yaochu Jin</a></p>
<p>Image anomaly detection (IAD) is an urgent issue that needs to be addressed
in modern industrial manufacturing (IM). Recently, many advanced algorithms
have been released, but their performance varies greatly due to non-uniformed
settings. That is, researchers find it difficult to analyze because they are
designed for different or specific cases in IM. To eliminate this problem, we
first propose a uniform IAD setting to systematically assess the effectiveness
of these algorithms, mainly considering three aspects of supervision level
(unsupervised, fully supervised), learning paradigm (few-shot, continual, noisy
label), and efficiency (memory usage, inference speed). Then, we skillfully
construct a comprehensive image anomaly detection benchmark (IM-IAD), which
includes 19 algorithms on 7 major datasets with the same setting. Our extensive
experiments (17,017 total) provide new insights into the redesign or selection
of the IAD algorithm under uniform conditions. Importantly, the proposed IM-IAD
presents feasible challenges and future directions for further work. We believe
that this work can have a significant impact on the IAD field. To foster
reproducibility and accessibility, the source code of IM-IAD is uploaded on the
website, https://github.com/M-3LAB/IM-IAD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08594">TransUPR: A Transformer-based Uncertain Point Refiner for LiDAR Point Cloud Semantic Segmentation. (arXiv:2302.08594v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1">Zifan Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Meida Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhikang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1">Suya You</a>, <a href="http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1">Raghuveer Rao</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Sanjeev Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Ren_F/0/1/0/all/0/1">Fengbo Ren</a></p>
<p>Common image-based LiDAR point cloud semantic segmentation (LiDAR PCSS)
approaches have bottlenecks resulting from the boundary-blurring problem of
convolution neural networks (CNNs) and quantitation loss of spherical
projection. In this work, we propose a transformer-based plug-and-play
uncertain point refiner, i.e., TransUPR, to refine selected uncertain points in
a learnable manner, which leads to an improved segmentation performance.
Uncertain points are sampled from coarse semantic segmentation results of 2D
image segmentation where uncertain points are located close to the object
boundaries in the 2D range image representation and 3D spherical projection
background points. Following that, the geometry and coarse semantic features of
uncertain points are aggregated by neighbor points in 3D space without adding
expensive computation and memory footprint. Finally, the transformer-based
refiner, which contains four stacked self-attention layers, along with an MLP
module, is utilized for uncertain point classification on the concatenated
features of self-attention layers. As the proposed refiner is independent of 2D
CNNs, our TransUPR can be easily integrated into any existing image-based LiDAR
PCSS approaches, e.g., CENet. Our TransUPR with the CENet achieves
state-of-the-art performance, i.e., 68.2% mean Intersection over Union (mIoU)
on the Semantic KITTI benchmark, which provides a performance improvement of
0.6% on the mIoU compared to the original CENet.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.11713">Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Hexiang Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1">Yi Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1">Haitian Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1">Soravit Changpinyo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1">Alan Ritter</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1">Ming-Wei Chang</a></p>
<p>Pre-trained vision and language models have demonstrated state-of-the-art
capabilities over existing tasks involving images and texts, including visual
question answering. However, it remains unclear whether these models possess
the capability to answer questions that are not only querying visual content
but knowledge-intensive and information-seeking. In this study, we introduce
InfoSeek, a visual question answering dataset tailored for information-seeking
questions that cannot be answered with only common sense knowledge. Using
InfoSeek, we analyze various pre-trained visual question answering models and
gain insights into their characteristics. Our findings reveal that
state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)
face challenges in answering visual information-seeking questions, but
fine-tuning on the InfoSeek dataset elicits models to use fine-grained
knowledge that was learned during their pre-training. Furthermore, we show that
accurate visual entity recognition can be used to improve performance on
InfoSeek by retrieving relevant documents, showing a significant space for
improvement.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.01748">A Complete Recipe for Diffusion Generative Models. (arXiv:2303.01748v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1">Kushagra Pandey</a>, <a href="http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1">Stephan Mandt</a></p>
<p>Score-based Generative Models (SGMs) have demonstrated exceptional synthesis
outcomes across various tasks. However, the current design landscape of the
forward diffusion process remains largely untapped and often relies on physical
heuristics or simplifying assumptions. Utilizing insights from the development
of scalable Bayesian posterior samplers, we present a complete recipe for
formulating forward processes in SGMs, ensuring convergence to the desired
target distribution. Our approach reveals that several existing SGMs can be
seen as specific manifestations of our framework. Building upon this method, we
introduce Phase Space Langevin Diffusion (PSLD), which relies on score-based
modeling within an augmented space enriched by auxiliary variables akin to
physical phase space. Empirical results exhibit the superior sample quality and
improved speed-quality trade-off of PSLD compared to various competing
approaches on established image synthesis benchmarks. Remarkably, PSLD achieves
sample quality akin to state-of-the-art SGMs (FID: 2.10 for unconditional
CIFAR-10 generation). Lastly, we demonstrate the applicability of PSLD in
conditional synthesis using pre-trained score networks, offering an appealing
alternative as an SGM backbone for future advancements. Code and model
checkpoints can be accessed at \url{https://github.com/mandt-lab/PSLD}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.07189">Optimizing Convolutional Neural Networks for Chronic Obstructive Pulmonary Disease Detection in Clinical Computed Tomography Imaging. (arXiv:2303.07189v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Dorosti_T/0/1/0/all/0/1">Tina Dorosti</a>, <a href="http://arxiv.org/find/eess/1/au:+Schultheiss_M/0/1/0/all/0/1">Manuel Schultheiss</a>, <a href="http://arxiv.org/find/eess/1/au:+Hofmann_F/0/1/0/all/0/1">Felix Hofmann</a>, <a href="http://arxiv.org/find/eess/1/au:+Thalhammer_J/0/1/0/all/0/1">Johannes Thalhammer</a>, <a href="http://arxiv.org/find/eess/1/au:+Kirchner_L/0/1/0/all/0/1">Luisa Kirchner</a>, <a href="http://arxiv.org/find/eess/1/au:+Urban_T/0/1/0/all/0/1">Theresa Urban</a>, <a href="http://arxiv.org/find/eess/1/au:+Pfeiffer_F/0/1/0/all/0/1">Franz Pfeiffer</a>, <a href="http://arxiv.org/find/eess/1/au:+Schaff_F/0/1/0/all/0/1">Florian Schaff</a>, <a href="http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1">Tobias Lasser</a>, <a href="http://arxiv.org/find/eess/1/au:+Pfeiffer_D/0/1/0/all/0/1">Daniela Pfeiffer</a></p>
<p>We aim to optimize the binary detection of Chronic Obstructive Pulmonary
Disease (COPD) based on emphysema presence in the lung with convolutional
neural networks (CNN) by exploring manually adjusted versus automated
window-setting optimization (WSO) on computed tomography (CT) images. 7,194 CT
images (3,597 with COPD; 3,597 healthy controls) from 78 subjects (43 with
COPD; 35 healthy controls) were selected retrospectively (10.2018-12.2019) and
preprocessed. For each image, intensity values were manually clipped to the
emphysema window setting and a baseline 'full-range' window setting.
Class-balanced train, validation, and test sets contained 3,392, 1,114, and
2,688 images. The network backbone was optimized by comparing various CNN
architectures. Furthermore, automated WSO was implemented by adding a
customized layer to the model. The image-level area under the Receiver
Operating Characteristics curve (AUC) [lower, upper limit 95% confidence] was
utilized to compare model variations. Repeated inference (n=7) on the test set
showed that the DenseNet was the most efficient backbone and achieved a mean
AUC of 0.80 [0.76, 0.85] without WSO. Comparably, with input images manually
adjusted to the emphysema window, the DenseNet model predicted COPD with a mean
AUC of 0.86 [0.82, 0.89]. By adding a customized WSO layer to the DenseNet, an
optimal window in the proximity of the emphysema window setting was learned
automatically, and a mean AUC of 0.82 [0.78, 0.86] was achieved. Detection of
COPD with DenseNet models was improved by WSO of CT data to the emphysema
window setting range.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.12535">An Effective Motion-Centric Paradigm for 3D Single Object Tracking in Point Clouds. (arXiv:2303.12535v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1">Chaoda Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1">Xu Yan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Haiming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baoyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1">Shenghui Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1">Shuguang Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1">Zhen Li</a></p>
<p>3D single object tracking in LiDAR point clouds (LiDAR SOT) plays a crucial
role in autonomous driving. Current approaches all follow the Siamese paradigm
based on appearance matching. However, LiDAR point clouds are usually
textureless and incomplete, which hinders effective appearance matching.
Besides, previous methods greatly overlook the critical motion clues among
targets. In this work, beyond 3D Siamese tracking, we introduce a
motion-centric paradigm to handle LiDAR SOT from a new perspective. Following
this paradigm, we propose a matching-free two-stage tracker M^2-Track. At the
1st-stage, M^2-Track localizes the target within successive frames via motion
transformation. Then it refines the target box through motion-assisted shape
completion at the 2nd-stage. Due to the motion-centric nature, our method shows
its impressive generalizability with limited training labels and provides good
differentiability for end-to-end cycle training. This inspires us to explore
semi-supervised LiDAR SOT by incorporating a pseudo-label-based motion
augmentation and a self-supervised loss term. Under the fully-supervised
setting, extensive experiments confirm that M^2-Track significantly outperforms
previous state-of-the-arts on three large-scale datasets while running at 57FPS
(~3%, ~11% and ~22% precision gains on KITTI, NuScenes, and Waymo Open Dataset
respectively). While under the semi-supervised setting, our method performs on
par with or even surpasses its fully-supervised counterpart using fewer than
half of the labels from KITTI. Further analysis verifies each component's
effectiveness and shows the motion-centric paradigm's promising potential for
auto-labeling and unsupervised domain adaptation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01601">Primitive Simultaneous Optimization of Similarity Metrics for Image Registration. (arXiv:2304.01601v3 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Waldmannstetter_D/0/1/0/all/0/1">Diana Waldmannstetter</a>, <a href="http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1">Benedikt Wiestler</a>, <a href="http://arxiv.org/find/eess/1/au:+Schwarting_J/0/1/0/all/0/1">Julian Schwarting</a>, <a href="http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1">Ivan Ezhov</a>, <a href="http://arxiv.org/find/eess/1/au:+Metz_M/0/1/0/all/0/1">Marie Metz</a>, <a href="http://arxiv.org/find/eess/1/au:+Bakas_S/0/1/0/all/0/1">Spyridon Bakas</a>, <a href="http://arxiv.org/find/eess/1/au:+Baheti_B/0/1/0/all/0/1">Bhakti Baheti</a>, <a href="http://arxiv.org/find/eess/1/au:+Chakrabarty_S/0/1/0/all/0/1">Satrajit Chakrabarty</a>, <a href="http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1">Daniel Rueckert</a>, <a href="http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1">Jan S. Kirschke</a>, <a href="http://arxiv.org/find/eess/1/au:+Heckemann_R/0/1/0/all/0/1">Rolf A. Heckemann</a>, <a href="http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1">Marie Piraud</a>, <a href="http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1">Bjoern H. Menze</a>, <a href="http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1">Florian Kofler</a></p>
<p>Even though simultaneous optimization of similarity metrics is a standard
procedure in the field of semantic segmentation, surprisingly, this is much
less established for image registration. To help closing this gap in the
literature, we investigate in a complex multi-modal 3D setting whether
simultaneous optimization of registration metrics, here implemented by means of
primitive summation, can benefit image registration. We evaluate two
challenging datasets containing collections of pre- to post-operative and pre-
to intra-operative MR images of glioma. Employing the proposed optimization, we
demonstrate improved registration accuracy in terms of TRE on expert
neuroradiologists' landmark annotations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02621">High-fidelity Pseudo-labels for Boosting Weakly-Supervised Segmentation. (arXiv:2304.02621v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jonnarth_A/0/1/0/all/0/1">Arvi Jonnarth</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yushan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1">Michael Felsberg</a></p>
<p>Image-level weakly-supervised semantic segmentation (WSSS) reduces the
usually vast data annotation cost by surrogate segmentation masks during
training. The typical approach involves training an image classification
network using global average pooling (GAP) on convolutional feature maps. This
enables the estimation of object locations based on class activation maps
(CAMs), which identify the importance of image regions. The CAMs are then used
to generate pseudo-labels, in the form of segmentation masks, to supervise a
segmentation model in the absence of pixel-level ground truth. Our work is
based on two techniques for improving CAMs; importance sampling, which is a
substitute for GAP, and the feature similarity loss, which utilizes a heuristic
that object contours almost always align with color edges in images. However,
both are based on the multinomial posterior with softmax, and implicitly assume
that classes are mutually exclusive, which turns out suboptimal in our
experiments. Thus, we reformulate both techniques based on binomial posteriors
of multiple independent binary problems. This has two benefits; their
performance is improved and they become more general, resulting in an add-on
method that can boost virtually any WSSS method. This is demonstrated on a wide
variety of baselines on the PASCAL VOC dataset, improving the region similarity
and contour quality of all implemented state-of-the-art methods. Experiments on
the MS COCO dataset show that our proposed add-on is well-suited for
large-scale settings. Our code is available at https://github.com/arvijj/hfpl.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.03198">RFAConv: Innovating Spatial Attention and Standard Convolutional Operation. (arXiv:2304.03198v5 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1">Degang Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1">Tingting Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1">Yichen Ye</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Ke Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1">Yingze Song</a></p>
<p>Spatial attention has been widely used to improve the performance of
convolutional neural networks. However, it has certain limitations. In this
paper, we propose a new perspective on the effectiveness of spatial attention,
which is that the spatial attention mechanism essentially solves the problem of
convolutional kernel parameter sharing. However, the information contained in
the attention map generated by spatial attention is not sufficient for
large-size convolutional kernels. Therefore, we propose a novel attention
mechanism called Receptive-Field Attention (RFA). Existing spatial attention,
such as Convolutional Block Attention Module (CBAM) and Coordinated Attention
(CA) focus only on spatial features, which does not fully address the problem
of convolutional kernel parameter sharing. In contrast, RFA not only focuses on
the receptive-field spatial feature but also provides effective attention
weights for large-size convolutional kernels. The Receptive-Field Attention
convolutional operation (RFAConv), developed by RFA, represents a new approach
to replace the standard convolution operation. It offers nearly negligible
increment of computational cost and parameters, while significantly improving
network performance. We conducted a series of experiments on ImageNet-1k, COCO,
and VOC datasets to demonstrate the superiority of our approach. Of particular
importance, we believe that it is time to shift focus from spatial features to
receptive-field spatial features for current spatial attention mechanisms. In
this way, we can further improve network performance and achieve even better
results. The code and pre-trained models for the relevant tasks can be found at
https://github.com/Liuchen1997/RFAConv.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13819">MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning. (arXiv:2304.13819v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">Jiaze Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhixiang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1">Tae-Kyun Kim</a></p>
<p>3D pose transfer is a challenging generation task that aims to transfer the
pose of a source geometry onto a target geometry with the target identity
preserved. Many prior methods require keypoint annotations to find
correspondence between the source and target. Current pose transfer methods
allow end-to-end correspondence learning but require the desired final output
as ground truth for supervision. Unsupervised methods have been proposed for
graph convolutional models but they require ground truth correspondence between
the source and target inputs. We present a novel self-supervised framework for
3D pose transfer which can be trained in unsupervised, semi-supervised, or
fully supervised settings without any correspondence labels. We introduce two
contrastive learning constraints in the latent space: a mesh-level loss for
disentangling global patterns including pose and identity, and a point-level
loss for discriminating local semantics. We demonstrate quantitatively and
qualitatively that our method achieves state-of-the-art results in supervised
3D pose transfer, with comparable results in unsupervised and semi-supervised
settings. Our method is also generalisable to unseen human and animal data with
complex topologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.16319">Image as First-Order Norm+Linear Autoregression: Unveiling Mathematical Invariance. (arXiv:2305.16319v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yinpeng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_X/0/1/0/all/0/1">Xiyang Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1">Dongdong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Mengchen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1">Lu Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zicheng Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1">Youzuo Lin</a></p>
<p>This paper introduces a novel mathematical property applicable to diverse
images, referred to as FINOLA (First-Order Norm+Linear Autoregressive). FINOLA
represents each image in the latent space as a first-order autoregressive
process, in which each regression step simply applies a shared linear model on
the normalized value of its immediate neighbor. This intriguing property
reveals a mathematical invariance that transcends individual images. Expanding
from image grids to continuous coordinates, we unveil the presence of two
underlying partial differential equations. We validate the FINOLA property from
two distinct angles: image reconstruction and self-supervised learning.
Firstly, we demonstrate the ability of FINOLA to auto-regress up to a 256x256
feature map (the same resolution to the image) from a single vector placed at
the center, successfully reconstructing the original image by only using three
3x3 convolution layers as decoder. Secondly, we leverage FINOLA for
self-supervised learning by employing a simple masked prediction approach.
Encoding a single unmasked quadrant block, we autoregressively predict the
surrounding masked region. Remarkably, this pre-trained representation proves
highly effective in image classification and object detection tasks, even when
integrated into lightweight networks, all without the need for extensive
fine-tuning. The code will be made publicly available.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.19443">OWAdapt: An adaptive loss function for deep learning using OWA operators. (arXiv:2305.19443v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maldonado_S/0/1/0/all/0/1">Sebasti&#xe1;n Maldonado</a>, <a href="http://arxiv.org/find/cs/1/au:+Vairetti_C/0/1/0/all/0/1">Carla Vairetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Jara_K/0/1/0/all/0/1">Katherine Jara</a>, <a href="http://arxiv.org/find/cs/1/au:+Carrasco_M/0/1/0/all/0/1">Miguel Carrasco</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_J/0/1/0/all/0/1">Julio L&#xf3;pez</a></p>
<p>In this paper, we propose a fuzzy adaptive loss function for enhancing deep
learning performance in classification tasks. Specifically, we redefine the
cross-entropy loss to effectively address class-level noise conditions,
including the challenging problem of class imbalance. Our approach introduces
aggregation operators, leveraging the power of fuzzy logic to improve
classification accuracy. The rationale behind our proposed method lies in the
iterative up-weighting of class-level components within the loss function,
focusing on those with larger errors. To achieve this, we employ the ordered
weighted average (OWA) operator and combine it with an adaptive scheme for
gradient-based learning. Through extensive experimentation, our method
outperforms other commonly used loss functions, such as the standard
cross-entropy or focal loss, across various binary and multiclass
classification tasks. Furthermore, we explore the influence of hyperparameters
associated with the OWA operators and present a default configuration that
performs well across different experimental settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06323">Learning Joint Latent Space EBM Prior Model for Multi-layer Generator. (arXiv:2306.06323v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1">Jiali Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Ying Nian Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_T/0/1/0/all/0/1">Tian Han</a></p>
<p>This paper studies the fundamental problem of learning multi-layer generator
models. The multi-layer generator model builds multiple layers of latent
variables as a prior model on top of the generator, which benefits learning
complex data distribution and hierarchical representations. However, such a
prior model usually focuses on modeling inter-layer relations between latent
variables by assuming non-informative (conditional) Gaussian distributions,
which can be limited in model expressivity. To tackle this issue and learn more
expressive prior models, we propose an energy-based model (EBM) on the joint
latent space over all layers of latent variables with the multi-layer generator
as its backbone. Such joint latent space EBM prior model captures the
intra-layer contextual relations at each layer through layer-wise energy terms,
and latent variables across different layers are jointly corrected. We develop
a joint training scheme via maximum likelihood estimation (MLE), which involves
Markov Chain Monte Carlo (MCMC) sampling for both prior and posterior
distributions of the latent variables from different layers. To ensure
efficient inference and learning, we further propose a variational training
scheme where an inference model is used to amortize the costly posterior MCMC
sampling. Our experiments demonstrate that the learned model can be expressive
in generating high-quality images and capturing hierarchical features for
better outlier detection.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.06599">Variational Imbalanced Regression: Fair Uncertainty Quantification via Probabilistic Smoothing. (arXiv:2306.06599v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziyan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>Existing regression models tend to fall short in both accuracy and
uncertainty estimation when the label distribution is imbalanced. In this
paper, we propose a probabilistic deep learning model, dubbed variational
imbalanced regression (VIR), which not only performs well in imbalanced
regression but naturally produces reasonable uncertainty estimation as a
byproduct. Different from typical variational autoencoders assuming I.I.D.
representations (a data point's representation is not directly affected by
other data points), our VIR borrows data with similar regression labels to
compute the latent representation's variational distribution; furthermore,
different from deterministic regression models producing point estimates, VIR
predicts the entire normal-inverse-gamma distributions and modulates the
associated conjugate distributions to impose probabilistic reweighting on the
imbalanced data, thereby providing better uncertainty estimation. Experiments
in several real-world datasets show that our VIR can outperform
state-of-the-art imbalanced regression models in terms of both accuracy and
uncertainty estimation. Code will soon be available at
\url{https://github.com/Wang-ML-Lab/variational-imbalanced-regression}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.16605">KITE: Keypoint-Conditioned Policies for Semantic Manipulation. (arXiv:2306.16605v4 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sundaresan_P/0/1/0/all/0/1">Priya Sundaresan</a>, <a href="http://arxiv.org/find/cs/1/au:+Belkhale_S/0/1/0/all/0/1">Suneel Belkhale</a>, <a href="http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1">Dorsa Sadigh</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohg_J/0/1/0/all/0/1">Jeannette Bohg</a></p>
<p>While natural language offers a convenient shared interface for humans and
robots, enabling robots to interpret and follow language commands remains a
longstanding challenge in manipulation. A crucial step to realizing a
performant instruction-following robot is achieving semantic manipulation,
where a robot interprets language at different specificities, from high-level
instructions like "Pick up the stuffed animal" to more detailed inputs like
"Grab the left ear of the elephant." To tackle this, we propose Keypoints +
Instructions to Execution (KITE), a two-step framework for semantic
manipulation which attends to both scene semantics (distinguishing between
different objects in a visual scene) and object semantics (precisely localizing
different parts within an object instance). KITE first grounds an input
instruction in a visual scene through 2D image keypoints, providing a highly
accurate object-centric bias for downstream action inference. Provided an RGB-D
scene observation, KITE then executes a learned keypoint-conditioned skill to
carry out the instruction. The combined precision of keypoints and
parameterized skills enables fine-grained manipulation with generalization to
scene and object variations. Empirically, we demonstrate KITE in 3 real-world
environments: long-horizon 6-DoF tabletop manipulation, semantic grasping, and
a high-precision coffee-making task. In these settings, KITE achieves a 75%,
70%, and 71% overall success rate for instruction-following, respectively. KITE
outperforms frameworks that opt for pre-trained visual language models over
keypoint-based grounding, or omit skills in favor of end-to-end visuomotor
control, all while being trained from fewer or comparable amounts of
demonstrations. Supplementary material, datasets, code, and videos can be found
on our website: <a href="http://tinyurl.com/kite-site.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.17046">Spiking Denoising Diffusion Probabilistic Models. (arXiv:2306.17046v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1">Jiahang Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ziqing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Hanzhong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1">Hao Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qiang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Renjing Xu</a></p>
<p>Spiking neural networks (SNNs) have ultra-low energy consumption and high
biological plausibility due to their binary and bio-driven nature compared with
artificial neural networks (ANNs). While previous research has primarily
focused on enhancing the performance of SNNs in classification tasks, the
generative potential of SNNs remains relatively unexplored. In our paper, we
put forward Spiking Denoising Diffusion Probabilistic Models (SDDPM), a new
class of SNN-based generative models that achieve high sample quality. To fully
exploit the energy efficiency of SNNs, we propose a purely Spiking U-Net
architecture, which achieves comparable performance to its ANN counterpart
using only 4 time steps, resulting in significantly reduced energy consumption.
Extensive experimental results reveal that our approach achieves
state-of-the-art on the generative tasks and substantially outperforms other
SNN-based generative models, achieving up to $12\times$ and $6\times$
improvement on the CIFAR-10 and the CelebA datasets, respectively. Moreover, we
propose a threshold-guided strategy that can further improve the performances
by 16.7% in a training-free manner. The SDDPM symbolizes a significant
advancement in the field of SNN generation, injecting new perspectives and
potential avenues of exploration.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03135">Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xuanlin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1">Yunhao Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1">Zhan Ling</a>, <a href="http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1">Zhuowen Tu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a></p>
<p>Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student's OOD
generalization: (1) by better imitating teacher's visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher's language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Poster:
https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf
Code: https://github.com/xuanlinli17/large_vlm_distillation_ood
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.03293">CheXmask: a large-scale dataset of anatomical segmentation masks for multi-center chest x-ray images. (arXiv:2307.03293v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Gaggion_N/0/1/0/all/0/1">Nicol&#xe1;s Gaggion</a>, <a href="http://arxiv.org/find/eess/1/au:+Mosquera_C/0/1/0/all/0/1">Candelaria Mosquera</a>, <a href="http://arxiv.org/find/eess/1/au:+Mansilla_L/0/1/0/all/0/1">Lucas Mansilla</a>, <a href="http://arxiv.org/find/eess/1/au:+Aineseder_M/0/1/0/all/0/1">Martina Aineseder</a>, <a href="http://arxiv.org/find/eess/1/au:+Milone_D/0/1/0/all/0/1">Diego H. Milone</a>, <a href="http://arxiv.org/find/eess/1/au:+Ferrante_E/0/1/0/all/0/1">Enzo Ferrante</a></p>
<p>The development of successful artificial intelligence models for chest X-ray
analysis relies on large, diverse datasets with high-quality annotations. While
several databases of chest X-ray images have been released, most include
disease diagnosis labels but lack detailed pixel-level anatomical segmentation
labels. To address this gap, we introduce an extensive chest X-ray multi-center
segmentation dataset with uniform and fine-grain anatomical annotations for
images coming from six well-known publicly available databases: CANDID-PTX,
ChestX-ray8, Chexpert, MIMIC-CXR-JPG, Padchest, and VinDr-CXR, resulting in
676,803 segmentation masks. Our methodology utilizes the HybridGNet model to
ensure consistent and high-quality segmentations across all datasets. Rigorous
validation, including expert physician evaluation and automatic quality
control, was conducted to validate the resulting masks. Additionally, we
provide individualized quality indices per mask and an overall quality
estimation per dataset. This dataset serves as a valuable resource for the
broader scientific community, streamlining the development and assessment of
innovative methodologies in chest X-ray analysis. The CheXmask dataset is
publicly available at:
https://physionet.org/content/chexmask-cxr-segmentation-data/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.07763">Tightly-Coupled LiDAR-Visual SLAM Based on Geometric Features for Mobile Agents. (arXiv:2307.07763v2 [cs.RO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_K/0/1/0/all/0/1">Ke Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ruiping Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Peng_K/0/1/0/all/0/1">Kunyu Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jiaming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1">Junwei Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Z/0/1/0/all/0/1">Zhifeng Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1">Kailun Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Stiefelhagen_R/0/1/0/all/0/1">Rainer Stiefelhagen</a></p>
<p>The mobile robot relies on SLAM (Simultaneous Localization and Mapping) to
provide autonomous navigation and task execution in complex and unknown
environments. However, it is hard to develop a dedicated algorithm for mobile
robots due to dynamic and challenging situations, such as poor lighting
conditions and motion blur. To tackle this issue, we propose a tightly-coupled
LiDAR-visual SLAM based on geometric features, which includes two sub-systems
(LiDAR and monocular visual SLAM) and a fusion framework. The fusion framework
associates the depth and semantics of the multi-modal geometric features to
complement the visual line landmarks and to add direction optimization in
Bundle Adjustment (BA). This further constrains visual odometry. On the other
hand, the entire line segment detected by the visual subsystem overcomes the
limitation of the LiDAR subsystem, which can only perform the local calculation
for geometric features. It adjusts the direction of linear feature points and
filters out outliers, leading to a higher accurate odometry system. Finally, we
employ a module to detect the subsystem's operation, providing the LiDAR
subsystem's output as a complementary trajectory to our system while visual
subsystem tracking fails. The evaluation results on the public dataset M2DGR,
gathered from ground robots across various indoor and outdoor scenarios, show
that our system achieves more accurate and robust pose estimation compared to
current state-of-the-art multi-modal methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03807">Nest-DGIL: Nesterov-optimized Deep Geometric Incremental Learning for CS Image Reconstruction. (arXiv:2308.03807v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Fan_X/0/1/0/all/0/1">Xiaohong Fan</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1">Yin Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1">Ke Chen</a>, <a href="http://arxiv.org/find/eess/1/au:+Feng_Y/0/1/0/all/0/1">Yujie Feng</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1">Jianping Zhang</a></p>
<p>Proximal gradient-based optimization is one of the most common strategies to
solve inverse problem of images, and it is easy to implement. However, these
techniques often generate heavy artifacts in image reconstruction. One of the
most popular refinement methods is to fine-tune the regularization parameter to
alleviate such artifacts, but it may not always be sufficient or applicable due
to increased computational costs. In this work, we propose a deep geometric
incremental learning framework based on the second Nesterov proximal gradient
optimization. The proposed end-to-end network not only has the powerful
learning ability for high-/low-frequency image features, but also can
theoretically guarantee that geometric texture details will be reconstructed
from preliminary linear reconstruction. Furthermore, it can avoid the risk of
intermediate reconstruction results falling outside the geometric decomposition
domains and achieve fast convergence. Our reconstruction framework is
decomposed into four modules including general linear reconstruction, cascade
geometric incremental restoration, Nesterov acceleration, and post-processing.
In the image restoration step, a cascade geometric incremental learning module
is designed to compensate for missing texture information from different
geometric spectral decomposition domains. Inspired by the overlap-tile
strategy, we also develop a post-processing module to remove the block effect
in patch-wise-based natural image reconstruction. All parameters in the
proposed model are learnable, an adaptive initialization technique of physical
parameters is also employed to make model flexibility and ensure converging
smoothly. We compare the reconstruction performance of the proposed method with
existing state-of-the-art methods to demonstrate its superiority. Our source
codes are available at https://github.com/fanxiaohong/Nest-DGIL.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.03998">Real-time Strawberry Detection Based on Improved YOLOv5s Architecture for Robotic Harvesting in open-field environment. (arXiv:2308.03998v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zixuan He</a> (1) (2), <a href="http://arxiv.org/find/cs/1/au:+Khanal_S/0/1/0/all/0/1">Salik Ram Khanal</a> (1) (2), <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xin Zhang</a> (3), <a href="http://arxiv.org/find/cs/1/au:+Karkee_M/0/1/0/all/0/1">Manoj Karkee</a> (1) (2), <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qin Zhang</a> (1) (2) ((1) Center for Precision and Automated Agricultural Systems, Washington State University, (2) Department of Biological Systems Engineering, Washington State University, (3) Department of Agricultural and Biological Engineering, Mississippi State University)</p>
<p>This study proposed a YOLOv5-based custom object detection model to detect
strawberries in an outdoor environment. The original architecture of the
YOLOv5s was modified by replacing the C3 module with the C2f module in the
backbone network, which provided a better feature gradient flow. Secondly, the
Spatial Pyramid Pooling Fast in the final layer of the backbone network of
YOLOv5s was combined with Cross Stage Partial Net to improve the generalization
ability over the strawberry dataset in this study. The proposed architecture
was named YOLOv5s-Straw. The RGB images dataset of the strawberry canopy with
three maturity classes (immature, nearly mature, and mature) was collected in
open-field environment and augmented through a series of operations including
brightness reduction, brightness increase, and noise adding. To verify the
superiority of the proposed method for strawberry detection in open-field
environment, four competitive detection models (YOLOv3-tiny, YOLOv5s,
YOLOv5s-C2f, and YOLOv8s) were trained, and tested under the same computational
environment and compared with YOLOv5s-Straw. The results showed that the
highest mean average precision of 80.3% was achieved using the proposed
architecture whereas the same was achieved with YOLOv3-tiny, YOLOv5s,
YOLOv5s-C2f, and YOLOv8s were 73.4%, 77.8%, 79.8%, 79.3%, respectively.
Specifically, the average precision of YOLOv5s-Straw was 82.1% in the immature
class, 73.5% in the nearly mature class, and 86.6% in the mature class, which
were 2.3% and 3.7%, respectively, higher than that of the latest YOLOv8s. The
model included 8.6*10^6 network parameters with an inference speed of 18ms per
image while the inference speed of YOLOv8s had a slower inference speed of
21.0ms and heavy parameters of 11.1*10^6, which indicates that the proposed
model is fast enough for real time strawberry detection and localization for
the robotic picking.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.11606">StoryBench: A Multifaceted Benchmark for Continuous Story Visualization. (arXiv:2308.11606v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bugliarello_E/0/1/0/all/0/1">Emanuele Bugliarello</a>, <a href="http://arxiv.org/find/cs/1/au:+Moraldo_H/0/1/0/all/0/1">Hernan Moraldo</a>, <a href="http://arxiv.org/find/cs/1/au:+Villegas_R/0/1/0/all/0/1">Ruben Villegas</a>, <a href="http://arxiv.org/find/cs/1/au:+Babaeizadeh_M/0/1/0/all/0/1">Mohammad Babaeizadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Saffar_M/0/1/0/all/0/1">Mohammad Taghi Saffar</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Han Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Erhan_D/0/1/0/all/0/1">Dumitru Erhan</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1">Vittorio Ferrari</a>, <a href="http://arxiv.org/find/cs/1/au:+Kindermans_P/0/1/0/all/0/1">Pieter-Jan Kindermans</a>, <a href="http://arxiv.org/find/cs/1/au:+Voigtlaender_P/0/1/0/all/0/1">Paul Voigtlaender</a></p>
<p>Generating video stories from text prompts is a complex task. In addition to
having high visual quality, videos need to realistically adhere to a sequence
of text prompts whilst being consistent throughout the frames. Creating a
benchmark for video generation requires data annotated over time, which
contrasts with the single caption used often in video datasets. To fill this
gap, we collect comprehensive human annotations on three existing datasets, and
introduce StoryBench: a new, challenging multi-task benchmark to reliably
evaluate forthcoming text-to-video models. Our benchmark includes three video
generation tasks of increasing difficulty: action execution, where the next
action must be generated starting from a conditioning video; story
continuation, where a sequence of actions must be executed starting from a
conditioning video; and story generation, where a video must be generated from
only text prompts. We evaluate small yet strong text-to-video baselines, and
show the benefits of training on story-like data algorithmically generated from
existing video captions. Finally, we establish guidelines for human evaluation
of video stories, and reaffirm the need of better automatic metrics for video
generation. StoryBench aims at encouraging future research efforts in this
exciting new area.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12364">Saliency-based Video Summarization for Face Anti-spoofing. (arXiv:2308.12364v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Muhammad_U/0/1/0/all/0/1">Usman Muhammad</a>, <a href="http://arxiv.org/find/cs/1/au:+Oussalah_M/0/1/0/all/0/1">Mourad Oussalah</a>, <a href="http://arxiv.org/find/cs/1/au:+Laaksonen_J/0/1/0/all/0/1">Jorma Laaksonen</a></p>
<p>With the growing availability of databases for face presentation attack
detection, researchers are increasingly focusing on video-based face
anti-spoofing methods that involve hundreds to thousands of images for training
the models. However, there is currently no clear consensus on the optimal
number of frames in a video to improve face spoofing detection. Inspired by the
visual saliency theory, we present a video summarization method for face
anti-spoofing detection that aims to enhance the performance and efficiency of
deep learning models by leveraging visual saliency. In particular, saliency
information is extracted from the differences between the Laplacian and Wiener
filter outputs of the source images, enabling identification of the most
visually salient regions within each frame. Subsequently, the source images are
decomposed into base and detail images, enhancing the representation of the
most important information. Weighting maps are then computed based on the
saliency information, indicating the importance of each pixel in the image. By
linearly combining the base and detail images using the weighting maps, the
method fuses the source images to create a single representative image that
summarizes the entire video. The key contribution of the proposed method lies
in demonstrating how visual saliency can be used as a data-centric approach to
improve the performance and efficiency for face presentation attack detection.
By focusing on the most salient images or regions within the images, a more
representative and diverse training set can be created, potentially leading to
more effective models. To validate the method's effectiveness, a simple CNN-RNN
deep learning architecture was used, and the experimental results showcased
state-of-the-art performance on five challenging face anti-spoofing datasets
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.12435">Characterising representation dynamics in recurrent neural networks for object recognition. (arXiv:2308.12435v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thorat_S/0/1/0/all/0/1">Sushrut Thorat</a>, <a href="http://arxiv.org/find/cs/1/au:+Doerig_A/0/1/0/all/0/1">Adrien Doerig</a>, <a href="http://arxiv.org/find/cs/1/au:+Kietzmann_T/0/1/0/all/0/1">Tim C. Kietzmann</a></p>
<p>Recurrent neural networks (RNNs) have yielded promising results for both
recognizing objects in challenging conditions and modeling aspects of primate
vision. However, the representational dynamics of recurrent computations remain
poorly understood, especially in large-scale visual models. Here, we studied
such dynamics in RNNs trained for object classification on MiniEcoset, a novel
subset of ecoset. We report two main insights. First, upon inference,
representations continued to evolve after correct classification, suggesting a
lack of the notion of being ``done with classification''. Second, focusing on
``readout zones'' as a way to characterize the activation trajectories, we
observe that misclassified representations exhibit activation patterns with
lower L2 norm, and are positioned more peripherally in the readout zones. Such
arrangements help the misclassified representations move into the correct zones
as time progresses. Our findings generalize to networks with lateral and
top-down connections, and include both additive and multiplicative interactions
with the bottom-up sweep. The results therefore contribute to a general
understanding of RNN dynamics in naturalistic tasks. We hope that the analysis
framework will aid future investigations of other types of RNNs, including
understanding of representational dynamics in primate vision.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.00848">Bengali Document Layout Analysis -- A YOLOV8 Based Ensembling Approach. (arXiv:2309.00848v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1">Nazmus Sakib Ahmed</a>, <a href="http://arxiv.org/find/cs/1/au:+Noor_S/0/1/0/all/0/1">Saad Sakib Noor</a>, <a href="http://arxiv.org/find/cs/1/au:+Sikder_A/0/1/0/all/0/1">Ashraful Islam Shanto Sikder</a>, <a href="http://arxiv.org/find/cs/1/au:+Paul_A/0/1/0/all/0/1">Abhijit Paul</a></p>
<p>This paper focuses on enhancing Bengali Document Layout Analysis (DLA) using
the YOLOv8 model and innovative post-processing techniques. We tackle
challenges unique to the complex Bengali script by employing data augmentation
for model robustness. After meticulous validation set evaluation, we fine-tune
our approach on the complete dataset, leading to a two-stage prediction
strategy for accurate element segmentation. Our ensemble model, combined with
post-processing, outperforms individual base architectures, addressing issues
identified in the BaDLAD dataset. By leveraging this approach, we aim to
advance Bengali document analysis, contributing to improved OCR and document
comprehension and BaDLAD serves as a foundational resource for this endeavor,
aiding future research in the field. Furthermore, our experiments provided key
insights to incorporate new strategies into the established solution.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.04946">Efficient Emotional Adaptation for Audio-Driven Talking-Head Generation. (arXiv:2309.04946v2 [cs.SD] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gan_Y/0/1/0/all/0/1">Yuan Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zongxin Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1">Xihang Yue</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1">Lingyun Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a></p>
<p>Audio-driven talking-head synthesis is a popular research topic for virtual
human-related applications. However, the inflexibility and inefficiency of
existing methods, which necessitate expensive end-to-end training to transfer
emotions from guidance videos to talking-head predictions, are significant
limitations. In this work, we propose the Emotional Adaptation for Audio-driven
Talking-head (EAT) method, which transforms emotion-agnostic talking-head
models into emotion-controllable ones in a cost-effective and efficient manner
through parameter-efficient adaptations. Our approach utilizes a pretrained
emotion-agnostic talking-head transformer and introduces three lightweight
adaptations (the Deep Emotional Prompts, Emotional Deformation Network, and
Emotional Adaptation Module) from different perspectives to enable precise and
realistic emotion controls. Our experiments demonstrate that our approach
achieves state-of-the-art performance on widely-used benchmarks, including LRW
and MEAD. Additionally, our parameter-efficient adaptations exhibit remarkable
generalization ability, even in scenarios where emotional training videos are
scarce or nonexistent. Project website: https://yuangan.github.io/eat/
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.05173">DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1">Zhengxiang Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1">Aldo Lipani</a></p>
<p>Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer's quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving over 20% memory and time costs compared to vanilla PT
and its variants, without changing trainable parameter sizes. Through extensive
experiments on 23 natural language processing (NLP) and vision-language (VL)
tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,
including the full fine-tuning baseline in some scenarios. Additionally, we
empirically show that DEPT grows more efficient as the model size increases.
Our further study reveals that DePT integrates seamlessly with
parameter-efficient transfer learning in the few-shot learning setting and
highlights its adaptability to various model architectures and sizes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.06188">Computer Vision Pipeline for Automated Antarctic Krill Analysis. (arXiv:2309.06188v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gudelis_M/0/1/0/all/0/1">Mazvydas Gudelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mackiewicz_M/0/1/0/all/0/1">Michal Mackiewicz</a>, <a href="http://arxiv.org/find/cs/1/au:+Bremner_J/0/1/0/all/0/1">Julie Bremner</a>, <a href="http://arxiv.org/find/cs/1/au:+Fielding_S/0/1/0/all/0/1">Sophie Fielding</a></p>
<p>British Antarctic Survey (BAS) researchers launch annual expeditions to the
Antarctic in order to estimate Antarctic Krill biomass and assess the change
from previous years. These comparisons provide insight into the effects of the
current environment on this key component of the marine food chain. In this
work we have developed tools for automating the data collection and analysis
process, using web-based image annotation tools and deep learning image
classification and regression models. We achieve highly accurate krill instance
segmentation results with an average 77.28% AP score, as well as separate
maturity stage and length estimation of krill specimens with 62.99% accuracy
and a 1.98mm length error respectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.11523">RMT: Retentive Networks Meet Vision Transformers. (arXiv:2309.11523v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1">Qihang Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1">Huaibo Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1">Mingrui Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hongmin Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1">Ran He</a></p>
<p>Transformer first appears in the field of natural language processing and is
later migrated to the computer vision domain, where it demonstrates excellent
performance in vision tasks. However, recently, Retentive Network (RetNet) has
emerged as an architecture with the potential to replace Transformer,
attracting widespread attention in the NLP community. Therefore, we raise the
question of whether transferring RetNet's idea to vision can also bring
outstanding performance to vision tasks. To address this, we combine RetNet and
Transformer to propose RMT. Inspired by RetNet, RMT introduces explicit decay
into the vision backbone, bringing prior knowledge related to spatial distances
to the vision model. This distance-related spatial prior allows for explicit
control of the range of tokens that each token can attend to. Additionally, to
reduce the computational cost of global modeling, we decompose this modeling
process along the two coordinate axes of the image. Abundant experiments have
demonstrated that our RMT exhibits exceptional performance across various
computer vision tasks. For example, RMT achieves 84.1% Top1-acc on ImageNet-1k
using merely 4.5G FLOPs. To the best of our knowledge, among all models, RMT
achieves the highest Top1-acc when models are of similar size and trained with
the same strategy. Moreover, RMT significantly outperforms existing vision
backbones in downstream tasks such as object detection, instance segmentation,
and semantic segmentation. Our work is still in progress.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.13336">FedDrive v2: an Analysis of the Impact of Label Skewness in Federated Semantic Segmentation for Autonomous Driving. (arXiv:2309.13336v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fani_E/0/1/0/all/0/1">Eros Fan&#xec;</a>, <a href="http://arxiv.org/find/cs/1/au:+Ciccone_M/0/1/0/all/0/1">Marco Ciccone</a>, <a href="http://arxiv.org/find/cs/1/au:+Caputo_B/0/1/0/all/0/1">Barbara Caputo</a></p>
<p>We propose FedDrive v2, an extension of the Federated Learning benchmark for
Semantic Segmentation in Autonomous Driving. While the first version aims at
studying the effect of domain shift of the visual features across clients, in
this work, we focus on the distribution skewness of the labels. We propose six
new federated scenarios to investigate how label skewness affects the
performance of segmentation models and compare it with the effect of domain
shift. Finally, we study the impact of using the domain information during
testing. Official website: https://feddrive.github.io
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14616">NDC-Scene: Boost Monocular 3D Semantic Scene Completion in Normalized Device Coordinates Space. (arXiv:2309.14616v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1">Jiawei Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Chuming Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1">Keqiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1">Yingjie Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1">Wanli Ouyang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Hongsheng Li</a></p>
<p>Monocular 3D Semantic Scene Completion (SSC) has garnered significant
attention in recent years due to its potential to predict complex semantics and
geometry shapes from a single image, requiring no 3D inputs. In this paper, we
identify several critical issues in current state-of-the-art methods, including
the Feature Ambiguity of projected 2D features in the ray to the 3D space, the
Pose Ambiguity of the 3D convolution, and the Computation Imbalance in the 3D
convolution across different depth levels. To address these problems, we devise
a novel Normalized Device Coordinates scene completion network (NDC-Scene) that
directly extends the 2D feature map to a Normalized Device Coordinates (NDC)
space, rather than to the world space directly, through progressive restoration
of the dimension of depth with deconvolution operations. Experiment results
demonstrate that transferring the majority of computation from the target 3D
space to the proposed normalized device coordinates space benefits monocular
SSC tasks. Additionally, we design a Depth-Adaptive Dual Decoder to
simultaneously upsample and fuse the 2D and 3D feature maps, further improving
overall performance. Our extensive experiments confirm that the proposed method
consistently outperforms state-of-the-art methods on both outdoor SemanticKITTI
and indoor NYUv2 datasets. Our code are available at
https://github.com/Jiawei-Yao0812/NDCScene.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.15505">Finite Scalar Quantization: VQ-VAE Made Simple. (arXiv:2309.15505v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mentzer_F/0/1/0/all/0/1">Fabian Mentzer</a>, <a href="http://arxiv.org/find/cs/1/au:+Minnen_D/0/1/0/all/0/1">David Minnen</a>, <a href="http://arxiv.org/find/cs/1/au:+Agustsson_E/0/1/0/all/0/1">Eirikur Agustsson</a>, <a href="http://arxiv.org/find/cs/1/au:+Tschannen_M/0/1/0/all/0/1">Michael Tschannen</a></p>
<p>We propose to replace vector quantization (VQ) in the latent representation
of VQ-VAEs with a simple scheme termed finite scalar quantization (FSQ), where
we project the VAE representation down to a few dimensions (typically less than
10). Each dimension is quantized to a small set of fixed values, leading to an
(implicit) codebook given by the product of these sets. By appropriately
choosing the number of dimensions and values each dimension can take, we obtain
the same codebook size as in VQ. On top of such discrete representations, we
can train the same models that have been trained on VQ-VAE representations. For
example, autoregressive and masked transformer models for image generation,
multimodal generation, and dense prediction computer vision tasks. Concretely,
we employ FSQ with MaskGIT for image generation, and with UViM for depth
estimation, colorization, and panoptic segmentation. Despite the much simpler
design of FSQ, we obtain competitive performance in all these tasks. We
emphasize that FSQ does not suffer from codebook collapse and does not need the
complex machinery employed in VQ (commitment losses, codebook reseeding, code
splitting, entropy penalties, etc.) to learn expressive discrete
representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00847">Can Pre-trained Networks Detect Familiar Out-of-Distribution Data?. (arXiv:2310.00847v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miyai_A/0/1/0/all/0/1">Atsuyuki Miyai</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1">Qing Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Irie_G/0/1/0/all/0/1">Go Irie</a>, <a href="http://arxiv.org/find/cs/1/au:+Aizawa_K/0/1/0/all/0/1">Kiyoharu Aizawa</a></p>
<p>Out-of-distribution (OOD) detection is critical for safety-sensitive machine
learning applications and has been extensively studied, yielding a plethora of
methods developed in the literature. However, most studies for OOD detection
did not use pre-trained models and trained a backbone from scratch. In recent
years, transferring knowledge from large pre-trained models to downstream tasks
by lightweight tuning has become mainstream for training in-distribution (ID)
classifiers. To bridge the gap between the practice of OOD detection and
current classifiers, the unique and crucial problem is that the samples whose
information networks know often come as OOD input. We consider that such data
may significantly affect the performance of large pre-trained networks because
the discriminability of these OOD data depends on the pre-training algorithm.
Here, we define such OOD data as PT-OOD (Pre-Trained OOD) data. In this paper,
we aim to reveal the effect of PT-OOD on the OOD detection performance of
pre-trained networks from the perspective of pre-training algorithms. To
achieve this, we explore the PT-OOD detection performance of supervised and
self-supervised pre-training algorithms with linear-probing tuning, the most
common efficient tuning method. Through our experiments and analysis, we find
that the low linear separability of PT-OOD in the feature space heavily
degrades the PT-OOD detection performance, and self-supervised models are more
vulnerable to PT-OOD than supervised pre-trained models, even with
state-of-the-art detection methods. To solve this vulnerability, we further
propose a unique solution to large-scale pre-trained models: Leveraging
powerful instance-by-instance discriminative representations of pre-trained
models and detecting OOD in the feature space independent of the ID decision
boundaries. The code will be available via https://github.com/AtsuMiyai/PT-OOD.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01662">SYRAC: Synthesize, Rank, and Count. (arXiv:2310.01662v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+DAlessandro_A/0/1/0/all/0/1">Adriano D&#x27;Alessandro</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahdavi_Amiri_A/0/1/0/all/0/1">Ali Mahdavi-Amiri</a>, <a href="http://arxiv.org/find/cs/1/au:+Hamarneh_G/0/1/0/all/0/1">Ghassan Hamarneh</a></p>
<p>Crowd counting is a critical task in computer vision, with several important
applications. However, existing counting methods rely on labor-intensive
density map annotations, necessitating the manual localization of each
individual pedestrian. While recent efforts have attempted to alleviate the
annotation burden through weakly or semi-supervised learning, these approaches
fall short of significantly reducing the workload. We propose a novel approach
to eliminate the annotation burden by leveraging latent diffusion models to
generate synthetic data. However, these models struggle to reliably understand
object quantities, leading to noisy annotations when prompted to produce images
with a specific quantity of objects. To address this, we use latent diffusion
models to create two types of synthetic data: one by removing pedestrians from
real images, which generates ranked image pairs with a weak but reliable object
quantity signal, and the other by generating synthetic images with a
predetermined number of objects, offering a strong but noisy counting signal.
Our method utilizes the ranking image pairs for pre-training and then fits a
linear layer to the noisy synthetic images using these crowd quantity features.
We report state-of-the-art results for unsupervised crowd counting.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.03270">EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models. (arXiv:2310.03270v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yefei He</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jing Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1">Weijia Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hong Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1">Bohan Zhuang</a></p>
<p>Diffusion models have demonstrated remarkable capabilities in image synthesis
and related generative tasks. Nevertheless, their practicality for low-latency
real-world applications is constrained by substantial computational costs and
latency issues. Quantization is a dominant way to compress and accelerate
diffusion models, where post-training quantization (PTQ) and quantization-aware
training (QAT) are two main approaches, each bearing its own properties. While
PTQ exhibits efficiency in terms of both time and data usage, it may lead to
diminished performance in low bit-width. On the other hand, QAT can alleviate
performance degradation but comes with substantial demands on computational and
data resources. To capitalize on the advantages while avoiding their respective
drawbacks, we introduce a data-free and parameter-efficient fine-tuning
framework for low-bit diffusion models, dubbed EfficientDM, to achieve
QAT-level performance with PTQ-like efficiency. Specifically, we propose a
quantization-aware variant of the low-rank adapter (QALoRA) that can be merged
with model weights and jointly quantized to low bit-width. The fine-tuning
process distills the denoising capabilities of the full-precision model into
its quantized counterpart, eliminating the requirement for training data. We
also introduce scale-aware optimization and employ temporal learned step-size
quantization to further enhance performance. Extensive experimental results
demonstrate that our method significantly outperforms previous PTQ-based
diffusion models while maintaining similar time and data efficiency.
Specifically, there is only a marginal 0.05 sFID increase when quantizing both
weights and activations of LDM-4 to 4-bit on ImageNet 256x256. Compared to
QAT-based methods, our EfficientDM also boasts a 16.2x faster quantization
speed with comparable generation quality.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04099">ClusVPR: Efficient Visual Place Recognition with Clustering-based Weighted Transformer. (arXiv:2310.04099v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1">Yifan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shamsolmoali_P/0/1/0/all/0/1">Pourya Shamsolmoali</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jie Yang</a></p>
<p>Visual place recognition (VPR) is a highly challenging task that has a wide
range of applications, including robot navigation and self-driving vehicles.
VPR is particularly difficult due to the presence of duplicate regions and the
lack of attention to small objects in complex scenes, resulting in recognition
deviations. In this paper, we present ClusVPR, a novel approach that tackles
the specific issues of redundant information in duplicate regions and
representations of small objects. Different from existing methods that rely on
Convolutional Neural Networks (CNNs) for feature map generation, ClusVPR
introduces a unique paradigm called Clustering-based Weighted Transformer
Network (CWTNet). CWTNet leverages the power of clustering-based weighted
feature maps and integrates global dependencies to effectively address visual
deviations encountered in large-scale VPR problems. We also introduce the
optimized-VLAD (OptLAD) layer that significantly reduces the number of
parameters and enhances model efficiency. This layer is specifically designed
to aggregate the information obtained from scale-wise image patches.
Additionally, our pyramid self-supervised strategy focuses on extracting
representative and diverse information from scale-wise image patches instead of
entire images, which is crucial for capturing representative and diverse
information in VPR. Extensive experiments on four VPR datasets show our model's
superior performance compared to existing models while being less complex.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04825">Comparative study of multi-person tracking methods. (arXiv:2310.04825v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akola_D/0/1/0/all/0/1">Denis Mbey Akola</a></p>
<p>This paper presents a study of two tracking algorithms (SORT~\cite{7533003}
and Tracktor++~\cite{2019}) that were ranked first positions on the MOT
Challenge leaderboard (The MOTChallenge web page: https://motchallenge.net ).
The purpose of this study is to discover the techniques used and to provide
useful insights about these algorithms in the tracking pipeline that could
improve the performance of MOT tracking algorithms. To this end, we adopted the
popular tracking-by-detection approach. We trained our own Pedestrian Detection
model using the MOT17Det dataset (MOT17Det :
https://motchallenge.net/data/MOT17Det/ ). We also used a re-identification
model trained on MOT17 dataset (MOT17 : https://motchallenge.net/data/MOT17/ )
for Tracktor++ to reduce the false re-identification alarms. We then present
experimental results which shows that Tracktor++ is a better multi-person
tracking algorithm than SORT. We also performed ablation studies to discover
the contribution of re-identification(RE-ID) network and motion to the results
of Tracktor++. We finally conclude by providing some recommendations for future
research.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.04829">How to effectively train an ensemble of Faster R-CNN object detectors to quantify uncertainty. (arXiv:2310.04829v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Akola_D/0/1/0/all/0/1">Denis Mbey Akola</a>, <a href="http://arxiv.org/find/cs/1/au:+Franchi_G/0/1/0/all/0/1">Gianni Franchi</a></p>
<p>This paper presents a new approach for training two-stage object detection
ensemble models, more specifically, Faster R-CNN models to estimate
uncertainty. We propose training one Region Proposal
Network(RPN)~\cite{https://doi.org/10.48550/arxiv.<a href="/abs/1506.01497">1506.01497</a>} and multiple Fast
R-CNN prediction heads is all you need to build a robust deep ensemble network
for estimating uncertainty in object detection. We present this approach and
provide experiments to show that this approach is much faster than the naive
method of fully training all $n$ models in an ensemble. We also estimate the
uncertainty by measuring this ensemble model's Expected Calibration Error
(ECE). We then further compare the performance of this model with that of
Gaussian YOLOv3, a variant of YOLOv3 that models uncertainty using predicted
bounding box coordinates. The source code is released at
\url{https://github.com/Akola-Mbey-Denis/EfficientEnsemble}
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05624">Locality-Aware Generalizable Implicit Neural Representation. (arXiv:2310.05624v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1">Doyup Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1">Chiheon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1">Minsu Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wook-Shin Han</a></p>
<p>Generalizable implicit neural representation (INR) enables a single
continuous function, i.e., a coordinate-based neural network, to represent
multiple data instances by modulating its weights or intermediate features
using latent codes. However, the expressive power of the state-of-the-art
modulation is limited due to its inability to localize and capture fine-grained
details of data entities such as specific pixels and rays. To address this
issue, we propose a novel framework for generalizable INR that combines a
transformer encoder with a locality-aware INR decoder. The transformer encoder
predicts a set of latent tokens from a data instance to encode local
information into each latent token. The locality-aware INR decoder extracts a
modulation vector by selectively aggregating the latent tokens via
cross-attention for a coordinate input and then predicts the output by
progressively decoding with coarse-to-fine modulation through multiple
frequency bandwidths. The selective token aggregation and the multi-band
feature modulation enable us to learn locality-aware representation in spatial
and spectral aspects, respectively. Our framework significantly outperforms
previous generalizable INRs and validates the usefulness of the locality-aware
latents for downstream tasks such as image generation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.05969">Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v2 [eess.IV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Muharram_A/0/1/0/all/0/1">Arief Purnama Muharram</a>, <a href="http://arxiv.org/find/eess/1/au:+Haryono_H/0/1/0/all/0/1">Hollyana Puteri Haryono</a>, <a href="http://arxiv.org/find/eess/1/au:+Juma_A/0/1/0/all/0/1">Abassi Haji Juma</a>, <a href="http://arxiv.org/find/eess/1/au:+Puspasari_I/0/1/0/all/0/1">Ira Puspasari</a>, <a href="http://arxiv.org/find/eess/1/au:+Utama_N/0/1/0/all/0/1">Nugraha Priya Utama</a></p>
<p>Reading and interpreting chest X-ray images is one of the most radiologist's
routines. However, it still can be challenging, even for the most experienced
ones. Therefore, we proposed a multi-model deep learning-based automated chest
X-ray report generator system designed to assist radiologists in their work.
The basic idea of the proposed system is by utilizing multi
binary-classification models for detecting multi abnormalities, with each model
responsible for detecting one abnormality, in a single image. In this study, we
limited the radiology abnormalities detection to only cardiomegaly, lung
effusion, and consolidation. The system generates a radiology report by
performing the following three steps: image pre-processing, utilizing deep
learning models to detect abnormalities, and producing a report. The aim of the
image pre-processing step is to standardize the input by scaling it to 128x128
pixels and slicing it into three segments, which covers the upper, lower, and
middle parts of the lung. After pre-processing, each corresponding model
classifies the image, resulting in a 0 (zero) for no abnormality detected and a
1 (one) for the presence of an abnormality. The prediction outputs of each
model are then concatenated to form a 'result code'. The 'result code' is used
to construct a report by selecting the appropriate pre-determined sentence for
each detected abnormality in the report generation step. The proposed system is
expected to reduce the workload of radiologists and increase the accuracy of
chest X-ray diagnosis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06488">SpikeCLIP: A Contrastive Language-Image Pretrained Spiking Neural Network. (arXiv:2310.06488v2 [cs.NE] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1">Tianlong Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wenhao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1">Changze Lv</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1">Jianhan Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Cenyuan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1">Muling Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1">Xiaoqing Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1">Xuanjing Huang</a></p>
<p>Spiking neural networks (SNNs) have demonstrated the capability to achieve
comparable performance to deep neural networks (DNNs) in both visual and
linguistic domains while offering the advantages of improved energy efficiency
and adherence to biological plausibility. However, the extension of such
single-modality SNNs into the realm of multimodal scenarios remains an
unexplored territory. Drawing inspiration from the concept of contrastive
language-image pre-training (CLIP), we introduce a novel framework, named
SpikeCLIP, to address the gap between two modalities within the context of
spike-based computing through a two-step recipe involving ``Alignment
Pre-training + Dual-Loss Fine-tuning". Extensive experiments demonstrate that
SNNs achieve comparable results to their DNN counterparts while significantly
reducing energy consumption across a variety of datasets commonly used for
multimodal model evaluation. Furthermore, SpikeCLIP maintains robust
performance in image classification tasks that involve class labels not
predefined within specific categories.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.06823">NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v2 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Ammar_M/0/1/0/all/0/1">Mou&#xef;n Ben Ammar</a>, <a href="http://arxiv.org/find/stat/1/au:+Belkhir_N/0/1/0/all/0/1">Nacim Belkhir</a>, <a href="http://arxiv.org/find/stat/1/au:+Popescu_S/0/1/0/all/0/1">Sebastian Popescu</a>, <a href="http://arxiv.org/find/stat/1/au:+Manzanera_A/0/1/0/all/0/1">Antoine Manzanera</a>, <a href="http://arxiv.org/find/stat/1/au:+Franchi_G/0/1/0/all/0/1">Gianni Franchi</a></p>
<p>Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse'', a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse'' and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. We plan to release the code after the anonymity
period.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07449">PoRF: Pose Residual Field for Accurate Neural Surface Reconstruction. (arXiv:2310.07449v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1">Jia-Wang Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Bian_W/0/1/0/all/0/1">Wenjing Bian</a>, <a href="http://arxiv.org/find/cs/1/au:+Prisacariu_V/0/1/0/all/0/1">Victor Adrian Prisacariu</a>, <a href="http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1">Philip Torr</a></p>
<p>Neural surface reconstruction is sensitive to the camera pose noise, even if
state-of-the-art pose estimators like COLMAP or ARKit are used. More
importantly, existing Pose-NeRF joint optimisation methods have struggled to
improve pose accuracy in challenging real-world scenarios. To overcome the
challenges, we introduce the pose residual field (\textbf{PoRF}), a novel
implicit representation that uses an MLP for regressing pose updates. This is
more robust than the conventional pose parameter optimisation due to parameter
sharing that leverages global information over the entire sequence.
Furthermore, we propose an epipolar geometry loss to enhance the supervision
that leverages the correspondences exported from COLMAP results without the
extra computational overhead. Our method yields promising results. On the DTU
dataset, we reduce the rotation error by 78\% for COLMAP poses, leading to the
decreased reconstruction Chamfer distance from 3.48mm to 0.85mm. On the
MobileBrick dataset that contains casually captured unbounded 360-degree
videos, our method refines ARKit poses and improves the reconstruction F1 score
from 69.18 to 75.67, outperforming that with the dataset provided ground-truth
pose (75.14). These achievements demonstrate the efficacy of our approach in
refining camera poses and improving the accuracy of neural surface
reconstruction in real-world scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07522">S4C: Self-Supervised Semantic Scene Completion with Neural Fields. (arXiv:2310.07522v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hayler_A/0/1/0/all/0/1">Adrian Hayler</a>, <a href="http://arxiv.org/find/cs/1/au:+Wimbauer_F/0/1/0/all/0/1">Felix Wimbauer</a>, <a href="http://arxiv.org/find/cs/1/au:+Muhle_D/0/1/0/all/0/1">Dominik Muhle</a>, <a href="http://arxiv.org/find/cs/1/au:+Rupprecht_C/0/1/0/all/0/1">Christian Rupprecht</a>, <a href="http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1">Daniel Cremers</a></p>
<p>3D semantic scene understanding is a fundamental challenge in computer
vision. It enables mobile agents to autonomously plan and navigate arbitrary
environments. SSC formalizes this challenge as jointly estimating dense
geometry and semantic information from sparse observations of a scene. Current
methods for SSC are generally trained on 3D ground truth based on aggregated
LiDAR scans. This process relies on special sensors and annotation by hand
which are costly and do not scale well. To overcome this issue, our work
presents the first self-supervised approach to SSC called S4C that does not
rely on 3D ground truth data. Our proposed method can reconstruct a scene from
a single image and only relies on videos and pseudo segmentation ground truth
generated from off-the-shelf image segmentation network during training. Unlike
existing methods, which use discrete voxel grids, we represent scenes as
implicit semantic fields. This formulation allows querying any point within the
camera frustum for occupancy and semantic class. Our architecture is trained
through rendering-based self-supervised losses. Nonetheless, our method
achieves performance close to fully supervised state-of-the-art methods.
Additionally, our method demonstrates strong generalization capabilities and
can synthesize accurate segmentation maps for far away viewpoints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.16375">A Comprehensive Review on Tree Detection Methods Using Point Cloud and Aerial Imagery from Unmanned Aerial Vehicles. (arXiv:2309.16375v2 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kuang_W/0/1/0/all/0/1">Weijie Kuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ho_H/0/1/0/all/0/1">Hann Woei Ho</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Ye Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Suandi_S/0/1/0/all/0/1">Shahrel Azmin Suandi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ismail_F/0/1/0/all/0/1">Farzad Ismail</a></p>
<p>Unmanned Aerial Vehicles (UAVs) are considered cutting-edge technology with
highly cost-effective and flexible usage scenarios. Although many papers have
reviewed the application of UAVs in agriculture, the review of the application
for tree detection is still insufficient. This paper focuses on tree detection
methods applied to UAV data collected by UAVs. There are two kinds of data, the
point cloud and the images, which are acquired by the Light Detection and
Ranging (LiDAR) sensor and camera, respectively. Among the detection methods
using point-cloud data, this paper mainly classifies these methods according to
LiDAR and Digital Aerial Photography (DAP). For the detection methods using
images directly, this paper reviews these methods by whether or not to use the
Deep Learning (DL) method. Our review concludes and analyses the comparison and
combination between the application of LiDAR-based and DAP-based point cloud
data. The performance, relative merits, and application fields of the methods
are also introduced. Meanwhile, this review counts the number of tree detection
studies using different methods in recent years. From our statics, the
detection task using DL methods on the image has become a mainstream trend as
the number of DL-based detection researches increases to 45% of the total
number of tree detection studies up to 2022. As a result, this review could
help and guide researchers who want to carry out tree detection on specific
forests and for farmers to use UAVs in managing agriculture production.
</p>
</p>
</div>

    </div>
    </body>
    