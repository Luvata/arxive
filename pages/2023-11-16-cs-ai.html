<!DOCTYPE html>
<html>
<head>
<title>2023-11-16-cs-ai</title>

    <style>
    body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        display: flex;
        justify-content: center;
        background-color: #f2f2f2;
    }
    .container {
        width: 60%;
        max-width: 900px;
        min-width: 300px;
        background-color: #fff;
        padding: 20px;
        margin: 20px;
        border-radius: 10px;
        box-shadow: 0px 0px 10px rgba(0, 0, 0, 0.05);
        font-size: 0.9em;
    }
    h1 {
        color: #333;
        font-size: 1.6em;
    }
    .article {
        border-bottom: 1px solid #ddd;
        padding: 10px 0;
    }
    .article a {
        color: #007BFF;
        text-decoration: none;
    }
    .article a:hover {
        color: #0056b3;
    }
    @media (max-width: 768px) {
        .container {
            width: 90%;
            font-size: 0.8em;
        }
        h1 {
            font-size: 1.4em;
        }
    }
    </style>
    </head>
    <body>
    <div class="container">
    <div class="article">
<h1><a href="http://arxiv.org/abs/2311.07578">A Metacognitive Approach to Out-of-Distribution Detection for Segmentation. (arXiv:2311.07578v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gummadi_M/0/1/0/all/0/1">Meghna Gummadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kent_C/0/1/0/all/0/1">Cassandra Kent</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmeckpeper_K/0/1/0/all/0/1">Karl Schmeckpeper</a>, <a href="http://arxiv.org/find/cs/1/au:+Eaton_E/0/1/0/all/0/1">Eric Eaton</a></p>
<p>Despite outstanding semantic scene segmentation in closed-worlds, deep neural
networks segment novel instances poorly, which is required for autonomous
agents acting in an open world. To improve out-of-distribution (OOD) detection
for segmentation, we introduce a metacognitive approach in the form of a
lightweight module that leverages entropy measures, segmentation predictions,
and spatial context to characterize the segmentation model's uncertainty and
detect pixel-wise OOD data in real-time. Additionally, our approach
incorporates a novel method of generating synthetic OOD data in context with
in-distribution data, which we use to fine-tune existing segmentation models
with maximum entropy training. This further improves the metacognitive module's
performance without requiring access to OOD data while enabling compatibility
with established pre-trained models. Our resulting approach can reliably detect
OOD instances in a scene, as shown by state-of-the-art performance on OOD
detection for semantic segmentation benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07579">Relative intrinsic dimensionality is intrinsic to learning. (arXiv:2311.07579v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sutton_O/0/1/0/all/0/1">Oliver J. Sutton</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1">Qinghua Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1">Alexander N. Gorban</a>, <a href="http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1">Ivan Y. Tyukin</a></p>
<p>High dimensional data can have a surprising property: pairs of data points
may be easily separated from each other, or even from arbitrary subsets, with
high probability using just simple linear classifiers. However, this is more of
a rule of thumb than a reliable property as high dimensionality alone is
neither necessary nor sufficient for successful learning. Here, we introduce a
new notion of the intrinsic dimension of a data distribution, which precisely
captures the separability properties of the data. For this intrinsic dimension,
the rule of thumb above becomes a law: high intrinsic dimension guarantees
highly separable data. We extend this notion to that of the relative intrinsic
dimension of two data distributions, which we show provides both upper and
lower bounds on the probability of successfully learning and generalising in a
binary classification problem
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07582">Evaluating the Potential of Leading Large Language Models in Reasoning Biology Questions. (arXiv:2311.07582v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1">Xinyu Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Holmes_J/0/1/0/all/0/1">Jason Holmes</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yiwei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhengliang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1">Qi Gan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1">Zihao Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jianli Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1">Yusong Zou</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yuxi Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1">Tian Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1">Hongtu Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1">Wei Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1">Tianming Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1">Yajun Yan</a></p>
<p>Recent advances in Large Language Models (LLMs) have presented new
opportunities for integrating Artificial General Intelligence (AGI) into
biological research and education. This study evaluated the capabilities of
leading LLMs, including GPT-4, GPT-3.5, PaLM2, Claude2, and SenseNova, in
answering conceptual biology questions. The models were tested on a
108-question multiple-choice exam covering biology topics in molecular biology,
biological techniques, metabolic engineering, and synthetic biology. Among the
models, GPT-4 achieved the highest average score of 90 and demonstrated the
greatest consistency across trials with different prompts. The results
indicated GPT-4's proficiency in logical reasoning and its potential to aid
biology research through capabilities like data analysis, hypothesis
generation, and knowledge integration. However, further development and
validation are still required before the promise of LLMs in accelerating
biological discovery can be realized.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07583">Cross-Dialect Sentence Transformation: A Comparative Analysis of Language Models for Adapting Sentences to British English. (arXiv:2311.07583v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1">Shruti Dutta</a>, <a href="http://arxiv.org/find/cs/1/au:+Mookherjee_S/0/1/0/all/0/1">Shashwat Mookherjee</a></p>
<p>This study explores linguistic distinctions among American, Indian, and Irish
English dialects and assesses various Language Models (LLMs) in their ability
to generate British English translations from these dialects. Using cosine
similarity analysis, the study measures the linguistic proximity between
original British English translations and those produced by LLMs for each
dialect. The findings reveal that Indian and Irish English translations
maintain notably high similarity scores, suggesting strong linguistic alignment
with British English. In contrast, American English exhibits slightly lower
similarity, reflecting its distinct linguistic traits. Additionally, the choice
of LLM significantly impacts translation quality, with Llama-2-70b consistently
demonstrating superior performance. The study underscores the importance of
selecting the right model for dialect translation, emphasizing the role of
linguistic expertise and contextual understanding in achieving accurate
translations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07584">Performance Prediction of Data-Driven Knowledge summarization of High Entropy Alloys (HEAs) literature implementing Natural Language Processing algorithms. (arXiv:2311.07584v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1">Akshansh Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Jatti_V/0/1/0/all/0/1">Vijaykumar S Jatti</a>, <a href="http://arxiv.org/find/cs/1/au:+More_V/0/1/0/all/0/1">Vaishnavi More</a>, <a href="http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1">Anish Dasgupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Dixit_D/0/1/0/all/0/1">Devarrishi Dixit</a>, <a href="http://arxiv.org/find/cs/1/au:+Sefene_E/0/1/0/all/0/1">Eyob Messele Sefene</a></p>
<p>The ability to interpret spoken language is connected to natural language
processing. It involves teaching the AI how words relate to one another, how
they are meant to be used, and in what settings. The goal of natural language
processing (NLP) is to get a machine intelligence to process words the same way
a human brain does. This enables machine intelligence to interpret, arrange,
and comprehend textual data by processing the natural language. The technology
can comprehend what is communicated, whether it be through speech or writing
because AI pro-cesses language more quickly than humans can. In the present
study, five NLP algorithms, namely, Geneism, Sumy, Luhn, Latent Semantic
Analysis (LSA), and Kull-back-Liebler (KL) al-gorithm, are implemented for the
first time for the knowledge summarization purpose of the High Entropy Alloys
(HEAs). The performance prediction of these algorithms is made by using the
BLEU score and ROUGE score. The results showed that the Luhn algorithm has the
highest accuracy score for the knowledge summarization tasks compared to the
other used algorithms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07585">Input Reconstruction Attack against Vertical Federated Large Language Models. (arXiv:2311.07585v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1">Fei Zheng</a></p>
<p>Recently, large language models (LLMs) have drawn extensive attention from
academia and the public, due to the advent of the ChatGPT. While LLMs show
their astonishing ability in text generation for various tasks, privacy
concerns limit their usage in real-life businesses. More specifically, either
the user's inputs (the user sends the query to the model-hosting server) or the
model (the user downloads the complete model) itself will be revealed during
the usage. Vertical federated learning (VFL) is a promising solution to this
kind of problem. It protects both the user's input and the knowledge of the
model by splitting the model into a bottom part and a top part, which is
maintained by the user and the model provider, respectively. However, in this
paper, we demonstrate that in LLMs, VFL fails to protect the user input since
it is simple and cheap to reconstruct the input from the intermediate
embeddings. Experiments show that even with a commercial GPU, the input
sentence can be reconstructed in only one second. We also discuss several
possible solutions to enhance the privacy of vertical federated LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07587">Frontier Language Models are not Robust to Adversarial Arithmetic, or &quot;What do I need to say so you agree 2+2=5?. (arXiv:2311.07587v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Freeman_C/0/1/0/all/0/1">C. Daniel Freeman</a>, <a href="http://arxiv.org/find/cs/1/au:+Culp_L/0/1/0/all/0/1">Laura Culp</a>, <a href="http://arxiv.org/find/cs/1/au:+Parisi_A/0/1/0/all/0/1">Aaron Parisi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bileschi_M/0/1/0/all/0/1">Maxwell L Bileschi</a>, <a href="http://arxiv.org/find/cs/1/au:+Elsayed_G/0/1/0/all/0/1">Gamaleldin F Elsayed</a>, <a href="http://arxiv.org/find/cs/1/au:+Rizkowsky_A/0/1/0/all/0/1">Alex Rizkowsky</a>, <a href="http://arxiv.org/find/cs/1/au:+Simpson_I/0/1/0/all/0/1">Isabelle Simpson</a>, <a href="http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1">Alex Alemi</a>, <a href="http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1">Azade Nova</a>, <a href="http://arxiv.org/find/cs/1/au:+Adlam_B/0/1/0/all/0/1">Ben Adlam</a>, <a href="http://arxiv.org/find/cs/1/au:+Bohnet_B/0/1/0/all/0/1">Bernd Bohnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_G/0/1/0/all/0/1">Gaurav Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1">Hanie Sedghi</a>, <a href="http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1">Igor Mordatch</a>, <a href="http://arxiv.org/find/cs/1/au:+Gur_I/0/1/0/all/0/1">Izzeddin Gur</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jaehoon Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Co_Reyes_J/0/1/0/all/0/1">JD Co-Reyes</a>, <a href="http://arxiv.org/find/cs/1/au:+Pennington_J/0/1/0/all/0/1">Jeffrey Pennington</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1">Kelvin Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Swersky_K/0/1/0/all/0/1">Kevin Swersky</a>, <a href="http://arxiv.org/find/cs/1/au:+Mahajan_K/0/1/0/all/0/1">Kshiteej Mahajan</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1">Lechao Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Rosanne Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1">Simon Kornblith</a>, <a href="http://arxiv.org/find/cs/1/au:+Constant_N/0/1/0/all/0/1">Noah Constant</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Peter J. Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Novak_R/0/1/0/all/0/1">Roman Novak</a>, <a href="http://arxiv.org/find/cs/1/au:+Vikram_S/0/1/0/all/0/1">Sharad Vikram</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1">Yundi Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Fiedel_N/0/1/0/all/0/1">Noah Fiedel</a>, <a href="http://arxiv.org/find/cs/1/au:+Sohl_Dickstein_J/0/1/0/all/0/1">Jascha Sohl-Dickstein</a></p>
<p>We introduce and study the problem of adversarial arithmetic, which provides
a simple yet challenging testbed for language model alignment. This problem is
comprised of arithmetic questions posed in natural language, with an arbitrary
adversarial string inserted before the question is complete. Even in the simple
setting of 1-digit addition problems, it is easy to find adversarial prompts
that make all tested models (including PaLM2, GPT4, Claude2) misbehave, and
even to steer models to a particular wrong answer. We additionally provide a
simple algorithm for finding successful attacks by querying those same models,
which we name "prompt inversion rejection sampling" (PIRS). We finally show
that models can be partially hardened against these attacks via reinforcement
learning and via agentic constitutional loops. However, we were not able to
make a language model fully robust against adversarial arithmetic attacks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07588">NLQxform: A Language Model-based Question to SPARQL Transformer. (arXiv:2311.07588v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Ruijie Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhiruo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Rossetto_L/0/1/0/all/0/1">Luca Rossetto</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruosch_F/0/1/0/all/0/1">Florian Ruosch</a>, <a href="http://arxiv.org/find/cs/1/au:+Bernstein_A/0/1/0/all/0/1">Abraham Bernstein</a></p>
<p>In recent years, scholarly data has grown dramatically in terms of both scale
and complexity. It becomes increasingly challenging to retrieve information
from scholarly knowledge graphs that include large-scale heterogeneous
relationships, such as authorship, affiliation, and citation, between various
types of entities, e.g., scholars, papers, and organizations. As part of the
Scholarly QALD Challenge, this paper presents a question-answering (QA) system
called NLQxform, which provides an easy-to-use natural language interface to
facilitate accessing scholarly knowledge graphs. NLQxform allows users to
express their complex query intentions in natural language questions. A
transformer-based language model, i.e., BART, is employed to translate
questions into standard SPARQL queries, which can be evaluated to retrieve the
required information. According to the public leaderboard of the Scholarly QALD
Challenge at ISWC 2023 (Task 1: DBLP-QUAD - Knowledge Graph Question Answering
over DBLP), NLQxform achieved an F1 score of 0.85 and ranked first on the QA
task, demonstrating the competitiveness of the system.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07589">Dialogizer: Context-aware Conversational-QA Dataset Generation from Textual Sources. (arXiv:2311.07589v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hwang_Y/0/1/0/all/0/1">Yerin Hwang</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1">Yongil Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1">Hyunkyung Bae</a>, <a href="http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1">Jeesoo Bang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hwanhee Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Jung_K/0/1/0/all/0/1">Kyomin Jung</a></p>
<p>To address the data scarcity issue in Conversational question answering
(ConvQA), a dialog inpainting method, which utilizes documents to generate
ConvQA datasets, has been proposed. However, the original dialog inpainting
model is trained solely on the dialog reconstruction task, resulting in the
generation of questions with low contextual relevance due to insufficient
learning of question-answer alignment. To overcome this limitation, we propose
a novel framework called Dialogizer, which has the capability to automatically
generate ConvQA datasets with high contextual relevance from textual sources.
The framework incorporates two training tasks: question-answer matching (QAM)
and topic-aware dialog generation (TDG). Moreover, re-ranking is conducted
during the inference phase based on the contextual relevance of the generated
questions. Using our framework, we produce four ConvQA datasets by utilizing
documents from multiple domains as the primary source. Through automatic
evaluation using diverse metrics, as well as human evaluation, we validate that
our proposed framework exhibits the ability to generate datasets of higher
quality compared to the baseline dialog inpainting model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07590">Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. (arXiv:2311.07590v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1">J&#xe9;r&#xe9;my Scheurer</a>, <a href="http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1">Mikita Balesni</a>, <a href="http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1">Marius Hobbhahn</a></p>
<p>We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07592">Hallucination-minimized Data-to-answer Framework for Financial Decision-makers. (arXiv:2311.07592v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roychowdhury_S/0/1/0/all/0/1">Sohini Roychowdhury</a>, <a href="http://arxiv.org/find/cs/1/au:+Alvarez_A/0/1/0/all/0/1">Andres Alvarez</a>, <a href="http://arxiv.org/find/cs/1/au:+Moore_B/0/1/0/all/0/1">Brian Moore</a>, <a href="http://arxiv.org/find/cs/1/au:+Krema_M/0/1/0/all/0/1">Marko Krema</a>, <a href="http://arxiv.org/find/cs/1/au:+Gelpi_M/0/1/0/all/0/1">Maria Paz Gelpi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_F/0/1/0/all/0/1">Federico Martin Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1">Angel Rodriguez</a>, <a href="http://arxiv.org/find/cs/1/au:+Cabrejas_J/0/1/0/all/0/1">Jose Ramon Cabrejas</a>, <a href="http://arxiv.org/find/cs/1/au:+Serrano_P/0/1/0/all/0/1">Pablo Martinez Serrano</a>, <a href="http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1">Punit Agrawal</a>, <a href="http://arxiv.org/find/cs/1/au:+Mukherjee_A/0/1/0/all/0/1">Arijit Mukherjee</a></p>
<p>Large Language Models (LLMs) have been applied to build several automation
and personalized question-answering prototypes so far. However, scaling such
prototypes to robust products with minimized hallucinations or fake responses
still remains an open challenge, especially in niche data-table heavy domains
such as financial decision making. In this work, we present a novel
Langchain-based framework that transforms data tables into hierarchical textual
data chunks to enable a wide variety of actionable question answering. First,
the user-queries are classified by intention followed by automated retrieval of
the most relevant data chunks to generate customized LLM prompts per query.
Next, the custom prompts and their responses undergo multi-metric scoring to
assess for hallucinations and response confidence. The proposed system is
optimized with user-query intention classification, advanced prompting, data
scaling capabilities and it achieves over 90% confidence scores for a variety
of user-queries responses ranging from {What, Where, Why, How, predict, trend,
anomalies, exceptions} that are crucial for financial decision making
applications. The proposed data to answers framework can be extended to other
analytical domains such as sales and payroll to ensure optimal hallucination
control guardrails.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07594">How to Bridge the Gap between Modalities: A Comprehensive Survey on Multimodal Large Language Model. (arXiv:2311.07594v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1">Shezheng Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaopeng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shasha Li</a></p>
<p>This review paper explores Multimodal Large Language Models (MLLMs), which
integrate Large Language Models (LLMs) like GPT-4 to handle multimodal data
such as text and vision. MLLMs demonstrate capabilities like generating image
narratives and answering image-based questions, bridging the gap towards
real-world human-computer interactions and hinting at a potential pathway to
artificial general intelligence. However, MLLMs still face challenges in
processing the semantic gap in multimodality, which may lead to erroneous
generation, posing potential risks to society. Choosing the appropriate
modality alignment method is crucial, as improper methods might require more
parameters with limited performance improvement. This paper aims to explore
modality alignment methods for LLMs and their existing capabilities.
Implementing modality alignment allows LLMs to address environmental issues and
enhance accessibility. The study surveys existing modal alignment methods in
MLLMs into four groups: (1) Multimodal Converters that change data into
something LLMs can understand; (2) Multimodal Perceivers to improve how LLMs
perceive different types of data; (3) Tools Assistance for changing data into
one common format, usually text; and (4) Data-Driven methods that teach LLMs to
understand specific types of data in a dataset. This field is still in a phase
of exploration and experimentation, and we will organize and update various
existing research methods for multimodal information alignment.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07595">A Decision Support System for Liver Diseases Prediction: Integrating Batch Processing, Rule-Based Event Detection and SPARQL Query. (arXiv:2311.07595v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1">Ritesh Chandra</a>, <a href="http://arxiv.org/find/cs/1/au:+Tiwari_S/0/1/0/all/0/1">Sadhana Tiwari</a>, <a href="http://arxiv.org/find/cs/1/au:+Rastogi_S/0/1/0/all/0/1">Satyam Rastogi</a>, <a href="http://arxiv.org/find/cs/1/au:+Agarwal_S/0/1/0/all/0/1">Sonali Agarwal</a></p>
<p>Liver diseases pose a significant global health burden, impacting a
substantial number of individuals and exerting substantial economic and social
consequences. Rising liver problems are considered a fatal disease in many
countries, such as Egypt, Molda, etc. The objective of this study is to
construct a predictive model for liver illness using Basic Formal Ontology
(BFO) and detection rules derived from a decision tree algorithm. Based on
these rules, events are detected through batch processing using the Apache Jena
framework. Based on the event detected, queries can be directly processed using
SPARQL. To make the ontology operational, these Decision Tree (DT) rules are
converted into Semantic Web Rule Language (SWRL). Using this SWRL in the
ontology for predicting different types of liver disease with the help of the
Pellet and Drool inference engines in Protege Tools, a total of 615 records are
taken from different liver diseases. After inferring the rules, the result can
be generated for the patient according to the DT rules, and other
patient-related details along with different precautionary suggestions can be
obtained based on these results. Combining query results of batch processing
and ontology-generated results can give more accurate suggestions for disease
prevention and detection. This work aims to provide a comprehensive approach
that is applicable for liver disease prediction, rich knowledge graph
representation, and smart querying capabilities. The results show that
combining RDF data, SWRL rules, and SPARQL queries for analysing and predicting
liver disease can help medical professionals to learn more about liver diseases
and make a Decision Support System (DSS) for health care.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07597">Enhancing Actuarial Non-Life Pricing Models via Transformers. (arXiv:2311.07597v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Brauer_A/0/1/0/all/0/1">Alexej Brauer</a></p>
<p>Currently, there is a lot of research in the field of neural networks for
non-life insurance pricing. The usual goal is to improve the predictive power
via neural networks while building upon the generalized linear model, which is
the current industry standard. Our paper contributes to this current journey
via novel methods to enhance actuarial non-life models with transformer models
for tabular data. We build here upon the foundation laid out by the combined
actuarial neural network as well as the localGLMnet and enhance those models
via the feature tokenizer transformer. The manuscript demonstrates the
performance of the proposed methods on a real-world claim frequency dataset and
compares them with several benchmark models such as generalized linear models,
feed-forward neural networks, combined actuarial neural networks, LocalGLMnet,
and pure feature tokenizer transformer. The paper shows that the new methods
can achieve better results than the benchmark models while preserving certain
generalized linear model advantages. The paper also discusses the practical
implications and challenges of applying transformer models in actuarial
settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07599">Testing LLMs on Code Generation with Varying Levels of Prompt Specificity. (arXiv:2311.07599v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Murr_L/0/1/0/all/0/1">Lincoln Murr</a>, <a href="http://arxiv.org/find/cs/1/au:+Grainger_M/0/1/0/all/0/1">Morgan Grainger</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_D/0/1/0/all/0/1">David Gao</a></p>
<p>Large language models (LLMs) have demonstrated unparalleled prowess in
mimicking human-like text generation and processing. Among the myriad of
applications that benefit from LLMs, automated code generation is increasingly
promising. The potential to transform natural language prompts into executable
code promises a major shift in software development practices and paves the way
for significant reductions in manual coding efforts and the likelihood of
human-induced errors. This paper reports the results of a study that evaluates
the performance of various LLMs, such as Bard, ChatGPT-3.5, ChatGPT-4, and
Claude-2, in generating Python for coding problems. We focus on how levels of
prompt specificity impact the accuracy, time efficiency, and space efficiency
of the generated code. A benchmark of 104 coding problems, each with four types
of prompts with varying degrees of tests and specificity, was employed to
examine these aspects comprehensively. Our results indicate significant
variations in performance across different LLMs and prompt types, and its key
contribution is to reveal the ideal prompting strategy for creating accurate
Python functions. This study lays the groundwork for further research in LLM
capabilities and suggests practical implications for utilizing LLMs in
automated code generation tasks and test-driven development.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07601">Online Advertisements with LLMs: Opportunities and Challenges. (arXiv:2311.07601v1 [cs.CY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hajiaghayi_M/0/1/0/all/0/1">MohammadTaghi Hajiaghayi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rezaei_K/0/1/0/all/0/1">Keivan Rezaei</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Suho Shin</a></p>
<p>This paper explores the potential for leveraging Large Language Models (LLM)
in the realm of online advertising systems. We delve into essential
requirements including privacy, latency, reliability, users and advertisers'
satisfaction, which such a system must fulfill. We further introduce a general
framework for LLM advertisement, consisting of modification, bidding,
prediction, and auction modules. Different design considerations for each
module is presented, with an in-depth examination of their practicality and the
technical challenges inherent to their implementation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07604">Finetuning Text-to-Image Diffusion Models for Fairness. (arXiv:2311.07604v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1">Xudong Shen</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1">Chao Du</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1">Tianyu Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1">Min Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1">Yongkang Wong</a>, <a href="http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1">Mohan Kankanhalli</a></p>
<p>The rapid adoption of text-to-image diffusion models in society underscores
an urgent need to address their biases. Without interventions, these biases
could propagate a distorted worldview and limit opportunities for minority
groups. In this work, we frame fairness as a distributional alignment problem.
Our solution consists of two main technical contributions: (1) a distributional
alignment loss that steers specific characteristics of the generated images
towards a user-defined target distribution, and (2) biased direct finetuning of
diffusion model's sampling process, which leverages a biased gradient to more
effectively optimize losses defined on the generated images. Empirically, our
method markedly reduces gender, racial, and their intersectional biases for
occupational prompts. Gender bias is significantly reduced even when finetuning
just five soft tokens. Crucially, our method supports diverse perspectives of
fairness beyond absolute equality, which is demonstrated by controlling age to
a $75\%$ young and $25\%$ old distribution while simultaneously debiasing
gender and race. Finally, our method is scalable: it can debias multiple
concepts at once by simply including these prompts in the finetuning data. We
hope our work facilitates the social alignment of T2I generative AI. We will
share code and various debiased diffusion model adaptors.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07605">Conceptual Model Interpreter for Large Language Models. (arXiv:2311.07605v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Harer_F/0/1/0/all/0/1">Felix H&#xe4;rer</a></p>
<p>Large Language Models (LLMs) recently demonstrated capabilities for
generating source code in common programming languages. Additionally,
commercial products such as ChatGPT 4 started to provide code interpreters,
allowing for the automatic execution of generated code fragments, instant
feedback, and the possibility to develop and refine in a conversational
fashion. With an exploratory research approach, this paper applies code
generation and interpretation to conceptual models. The concept and prototype
of a conceptual model interpreter is explored, capable of rendering visual
models generated in textual syntax by state-of-the-art LLMs such as Llama~2 and
ChatGPT 4. In particular, these LLMs can generate textual syntax for the
PlantUML and Graphviz modeling software that is automatically rendered within a
conversational user interface. The first result is an architecture describing
the components necessary to interact with interpreters and LLMs through APIs or
locally, providing support for many commercial and open source LLMs and
interpreters. Secondly, experimental results for models generated with ChatGPT
4 and Llama 2 are discussed in two cases covering UML and, on an instance
level, graphs created from custom data. The results indicate the possibility of
modeling iteratively in a conversational fashion.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07607">Modeling Choice via Self-Attention. (arXiv:2311.07607v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1">Joohwan Ko</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1">Andrew A. Li</a></p>
<p>Models of choice are a fundamental input to many now-canonical optimization
problems in the field of Operations Management, including assortment,
inventory, and price optimization. Naturally, accurate estimation of these
models from data is a critical step in the application of these optimization
problems in practice, and so it is perhaps surprising that such choice
estimation has to now been accomplished almost exclusively, both in theory and
in practice, (a) without the use of deep learning in any meaningful way, and
(b) via evaluation on limited data with constantly-changing metrics. This is in
stark contrast to the vast majority of similar learning applications, for which
the practice of machine learning suggests that (a) neural network-based models
are typically state-of-the-art, and (b) strict standardization on evaluation
procedures (datasets, metrics, etc.) is crucial. Thus motivated, we first
propose a choice model that is the first to successfully (both theoretically
and practically) leverage a modern neural network architectural concept
(self-attention). Theoretically, we show that our attention-based choice model
is a low-rank generalization of the Halo Multinomial Logit model, a recent
model that parsimoniously captures irrational choice effects and has seen
empirical success. We prove that whereas the Halo-MNL requires $\Omega(m^2)$
data samples to estimate, where $m$ is the number of products, our model
supports a natural nonconvex estimator (in particular, that which a standard
neural network implementation would apply) which admits a near-optimal
stationary point with $O(m)$ samples. We then establish the first
realistic-scale benchmark for choice estimation on real data and use this
benchmark to run the largest evaluation of existing choice models to date. We
find that the model we propose is dominant over both short-term and long-term
data periods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07608">MuST: Multimodal Spatiotemporal Graph-Transformer for Hospital Readmission Prediction. (arXiv:2311.07608v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Miao_Y/0/1/0/all/0/1">Yan Miao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1">Lequan Yu</a></p>
<p>Hospital readmission prediction is considered an essential approach to
decreasing readmission rates, which is a key factor in assessing the quality
and efficacy of a healthcare system. Previous studies have extensively utilized
three primary modalities, namely electronic health records (EHR), medical
images, and clinical notes, to predict hospital readmissions. However, the
majority of these studies did not integrate information from all three
modalities or utilize the spatiotemporal relationships present in the dataset.
This study introduces a novel model called the Multimodal Spatiotemporal
Graph-Transformer (MuST) for predicting hospital readmissions. By employing
Graph Convolution Networks and temporal transformers, we can effectively
capture spatial and temporal dependencies in EHR and chest radiographs. We then
propose a fusion transformer to combine the spatiotemporal features from the
two modalities mentioned above with the features from clinical notes extracted
by a pre-trained, domain-specific transformer. We assess the effectiveness of
our methods using the latest publicly available dataset, MIMIC-IV. The
experimental results indicate that the inclusion of multimodal features in MuST
improves its performance in comparison to unimodal methods. Furthermore, our
proposed pipeline outperforms the current leading methods in the prediction of
hospital readmissions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07616">ReIDTracker Sea: the technical report of BoaTrack and SeaDronesSee-MOT challenge at MaCVi of WACV24. (arXiv:2311.07616v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kaer Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1">Weitu Chong</a></p>
<p>Multi-Object Tracking is one of the most important technologies in maritime
computer vision. Our solution tries to explore Multi-Object Tracking in
maritime Unmanned Aerial vehicles (UAVs) and Unmanned Surface Vehicles (USVs)
usage scenarios. Most of the current Multi-Object Tracking algorithms require
complex association strategies and association information (2D location and
motion, 3D motion, 3D depth, 2D appearance) to achieve better performance,
which makes the entire tracking system extremely complex and heavy. At the same
time, most of the current Multi-Object Tracking algorithms still require video
annotation data which is costly to obtain for training. Our solution tries to
explore Multi-Object Tracking in a completely unsupervised way. The scheme
accomplishes instance representation learning by using self-supervision on
ImageNet. Then, by cooperating with high-quality detectors, the multi-target
tracking task can be completed simply and efficiently. The scheme achieved top
3 performance on both UAV-based Multi-Object Tracking with Reidentification and
USV-based Multi-Object Tracking benchmarks and the solution won the
championship in many multiple Multi-Object Tracking competitions. such as
BDD100K MOT,MOTS, Waymo 2D MOT
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07618">Large Language Models&#x27; Understanding of Math: Source Criticism and Extrapolation. (arXiv:2311.07618v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yousefzadeh_R/0/1/0/all/0/1">Roozbeh Yousefzadeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1">Xuenan Cao</a></p>
<p>It has been suggested that large language models such as GPT-4 have acquired
some form of understanding beyond the correlations among the words in text
including some understanding of mathematics as well. Here, we perform a
critical inquiry into this claim by evaluating the mathematical understanding
of the GPT-4 model. Considering that GPT-4's training set is a secret, it is
not straightforward to evaluate whether the model's correct answers are based
on a mathematical understanding or based on replication of proofs that the
model has seen before. We specifically craft mathematical questions which their
formal proofs are not readily available on the web, proofs that are more likely
not seen by the GPT-4. We see that GPT-4 is unable to solve those problems
despite their simplicity. It is hard to find scientific evidence suggesting
that GPT-4 has acquired an understanding of even basic mathematical concepts. A
straightforward way to find failure modes of GPT-4 in theorem proving is to
craft questions where their formal proofs are not available on the web. Our
finding suggests that GPT-4's ability is to reproduce, rephrase, and polish the
mathematical proofs that it has seen before, and not in grasping mathematical
concepts. We also see that GPT-4's ability to prove mathematical theorems is
continuously expanding over time despite the claim that it is a fixed model. We
suggest that the task of proving mathematical theorems in formal language is
comparable to the methods used in search engines such as Google while
predicting the next word in a sentence may be a misguided approach, a recipe
that often leads to excessive extrapolation and eventual failures. Prompting
the GPT-4 over and over may benefit the GPT-4 and the OpenAI, but we question
whether it is valuable for machine learning or for theorem proving.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07619">Modeling User Viewing Flow using Large Language Models for Article Recommendation. (arXiv:2311.07619v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1">Zhenghao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zulong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1">Moufeng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Duan_S/0/1/0/all/0/1">Shaoyang Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1">Hong Wen</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Liangyue Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_N/0/1/0/all/0/1">Nan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1">Yu Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1">Ge Yu</a></p>
<p>This paper proposes the User Viewing Flow Modeling (SINGLE) method for the
article recommendation task, which models the user constant preference and
instant interest from user-clicked articles. Specifically, we employ a user
constant viewing flow modeling method to summarize the user's general interest
to recommend articles. We utilize Large Language Models (LLMs) to capture
constant user preferences from previously clicked articles, such as skills and
positions. Then we design the user instant viewing flow modeling method to
build interactions between user-clicked article history and candidate articles.
It attentively reads the representations of user-clicked articles and aims to
learn the user's different interest views to match the candidate article. Our
experimental results on the Alibaba Technology Association (ATA) website show
the advantage of SINGLE, which achieves 2.4% improvements over previous
baseline models in the online A/B test. Our further analyses illustrate that
SINGLE has the ability to build a more tailored recommendation system by
mimicking different article viewing behaviors of users and recommending more
appropriate and diverse articles to match user interests.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07632">ResMGCN: Residual Message Graph Convolution Network for Fast Biomedical Interactions Discovering. (arXiv:2311.07632v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zecheng Yin</a></p>
<p>Biomedical information graphs are crucial for interaction discovering of
biomedical information in modern age, such as identification of multifarious
molecular interactions and drug discovery, which attracts increasing interests
in biomedicine, bioinformatics, and human healthcare communities. Nowadays,
more and more graph neural networks have been proposed to learn the entities of
biomedical information and precisely reveal biomedical molecule interactions
with state-of-the-art results. These methods remedy the fading of features from
a far distance but suffer from remedying such problem at the expensive cost of
redundant memory and time. In our paper, we propose a novel Residual Message
Graph Convolution Network (ResMGCN) for fast and precise biomedical interaction
prediction in a different idea. Specifically, instead of enhancing the message
from far nodes, ResMGCN aggregates lower-order information with the next round
higher information to guide the node update to obtain a more meaningful node
representation. ResMGCN is able to perceive and preserve various messages from
the previous layer and high-order information in the current layer with least
memory and time cost to obtain informative representations of biomedical
entities. We conduct experiments on four biomedical interaction network
datasets, including protein-protein, drug-drug, drug-target, and gene-disease
interactions, which demonstrates that ResMGCN outperforms previous
state-of-the-art models while achieving superb effectiveness on both storage
and time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07633">Rethinking and Benchmarking Predict-then-Optimize Paradigm for Combinatorial Optimization Problems. (arXiv:2311.07633v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Geng_H/0/1/0/all/0/1">Haoyu Geng</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruan_H/0/1/0/all/0/1">Han Ruan</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Runzhong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Lei Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1">Junchi Yan</a></p>
<p>Numerous web applications rely on solving combinatorial optimization
problems, such as energy cost-aware scheduling, budget allocation on web
advertising, and graph matching on social networks. However, many optimization
problems involve unknown coefficients, and improper predictions of these
factors may lead to inferior decisions which may cause energy wastage,
inefficient resource allocation, inappropriate matching in social networks,
etc. Such a research topic is referred to as "Predict-Then-Optimize (PTO)"
which considers the performance of prediction and decision-making in a unified
system. A noteworthy recent development is the end-to-end methods by directly
optimizing the ultimate decision quality which claims to yield better results
in contrast to the traditional two-stage approach. However, the evaluation
benchmarks in this field are fragmented and the effectiveness of various models
in different scenarios remains unclear, hindering the comprehensive assessment
and fast deployment of these methods. To address these issues, we provide a
comprehensive categorization of current approaches and integrate existing
experimental scenarios to establish a unified benchmark, elucidating the
circumstances under which end-to-end training yields improvements, as well as
the contexts in which it performs ineffectively. We also introduce a new
dataset for the industrial combinatorial advertising problem for inclusive
finance to open-source. We hope the rethinking and benchmarking of PTO could
facilitate more convenient evaluation and deployment, and inspire further
improvements both in the academy and industry within this field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07635">Past as a Guide: Leveraging Retrospective Learning for Python Code Completion. (arXiv:2311.07635v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1">Seunggyoon Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1">Seunggyu Chang</a>, <a href="http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1">Sungjoon Choi</a></p>
<p>This work presents Past as a Guide (PaG), a simple approach for Large
Language Models (LLMs) to improve the coding capabilities by integrating the
past history with interactive and iterative code refinements. To be specific,
inspired by human cognitive processes, the proposed method enables LLMs to
utilize previous programming and debugging experiences to enhance the Python
code completion tasks. The framework facilitates LLMs to iteratively refine the
Python code based on previous execution and debugging results and optimize
learning and reasoning capabilities. The proposed methodology achieved a 92\%
pass@1 on HumanEval, demonstrating the potential to advance the field by
leveraging retrospection from past experiences and interactive and iterative
refinement processes without external correctness indicators.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07682">Fuse to Forget: Bias Reduction and Selective Memorization through Model Fusion. (arXiv:2311.07682v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zaman_K/0/1/0/all/0/1">Kerem Zaman</a>, <a href="http://arxiv.org/find/cs/1/au:+Choshen_L/0/1/0/all/0/1">Leshem Choshen</a>, <a href="http://arxiv.org/find/cs/1/au:+Srivastava_S/0/1/0/all/0/1">Shashank Srivastava</a></p>
<p>Model fusion research aims to aggregate the knowledge of multiple models to
enhance performance by combining their weights. In this work, we study the
inverse, investigating whether and how can model fusion interfere and reduce
unwanted knowledge. We delve into the effects of model fusion on the evolution
of learned shortcuts, social biases, and memorization capabilities in
fine-tuned language models. Through several experiments covering text
classification and generation tasks, our analysis highlights that shared
knowledge among models is usually enhanced during model fusion, while unshared
knowledge is usually lost or forgotten. Based on this observation, we
demonstrate the potential of model fusion as a debiasing tool and showcase its
efficacy in addressing privacy concerns associated with language models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07687">Language Model-In-The-Loop: Data Optimal Approach to Learn-To-Recommend Actions in Text Games. (arXiv:2311.07687v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sudhakar_A/0/1/0/all/0/1">Arjun Vaithilingam Sudhakar</a>, <a href="http://arxiv.org/find/cs/1/au:+Parthasarathi_P/0/1/0/all/0/1">Prasanna Parthasarathi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajendran_J/0/1/0/all/0/1">Janarthanan Rajendran</a>, <a href="http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1">Sarath Chandar</a></p>
<p>Large Language Models (LLMs) have demonstrated superior performance in
language understanding benchmarks. CALM, a popular approach, leverages
linguistic priors of LLMs -- GPT-2 -- for action candidate recommendations to
improve the performance in text games in Jericho without environment-provided
actions. However, CALM adapts GPT-2 with annotated human gameplays and keeps
the LLM fixed during the learning of the text based games. In this work, we
explore and evaluate updating LLM used for candidate recommendation during the
learning of the text based game as well to mitigate the reliance on the human
annotated gameplays, which are costly to acquire. We observe that by updating
the LLM during learning using carefully selected in-game transitions, we can
reduce the dependency on using human annotated game plays for fine-tuning the
LLMs. We conducted further analysis to study the transferability of the updated
LLMs and observed that transferring in-game trained models to other games did
not result in a consistent transfer.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07692">On The Truthfulness of &#x27;Surprisingly Likely&#x27; Responses of Large Language Models. (arXiv:2311.07692v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goel_N/0/1/0/all/0/1">Naman Goel</a></p>
<p>The surprisingly likely criterion in the seminal work of Prelec (the Bayesian
Truth Serum) guarantees truthfulness in a game-theoretic multi-agent setting,
by rewarding rational agents to maximise the expected information gain with
their answers w.r.t. their probabilistic beliefs. We investigate the relevance
of a similar criterion for responses of LLMs. We hypothesize that if the
surprisingly likely criterion works in LLMs, under certain conditions, the
responses that maximize the reward under this criterion should be more accurate
than the responses that only maximize the posterior probability. Using
benchmarks including the TruthfulQA benchmark and using openly available LLMs:
GPT-2 and LLaMA-2, we show that the method indeed improves the accuracy
significantly (for example, upto 24 percentage points aggregate improvement on
TruthfulQA and upto 70 percentage points improvement on individual categories
of questions).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07700">AuthentiGPT: Detecting Machine-Generated Text via Black-Box Language Models Denoising. (arXiv:2311.07700v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1">Zhen Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1">Shangdi Yu</a></p>
<p>Large language models (LLMs) have opened up enormous opportunities while
simultaneously posing ethical dilemmas. One of the major concerns is their
ability to create text that closely mimics human writing, which can lead to
potential misuse, such as academic misconduct, disinformation, and fraud. To
address this problem, we present AuthentiGPT, an efficient classifier that
distinguishes between machine-generated and human-written texts. Under the
assumption that human-written text resides outside the distribution of
machine-generated text, AuthentiGPT leverages a black-box LLM to denoise input
text with artificially added noise, and then semantically compares the denoised
text with the original to determine if the content is machine-generated. With
only one trainable parameter, AuthentiGPT eliminates the need for a large
training dataset, watermarking the LLM's output, or computing the
log-likelihood. Importantly, the detection capability of AuthentiGPT can be
easily adapted to any generative language model. With a 0.918 AUROC score on a
domain-specific dataset, AuthentiGPT demonstrates its effectiveness over other
commercial algorithms, highlighting its potential for detecting
machine-generated text in academic settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07705">Robust and Scalable Hyperdimensional Computing With Brain-Like Neural Adaptations. (arXiv:2311.07705v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Junyao Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1">Mohammad Abdullah Al Faruque</a></p>
<p>The Internet of Things (IoT) has facilitated many applications utilizing
edge-based machine learning (ML) methods to analyze locally collected data.
Unfortunately, popular ML algorithms often require intensive computations
beyond the capabilities of today's IoT devices. Brain-inspired hyperdimensional
computing (HDC) has been introduced to address this issue. However, existing
HDCs use static encoders, requiring extremely high dimensionality and hundreds
of training iterations to achieve reasonable accuracy. This results in a huge
efficiency loss, severely impeding the application of HDCs in IoT systems. We
observed that a main cause is that the encoding module of existing HDCs lacks
the capability to utilize and adapt to information learned during training. In
contrast, neurons in human brains dynamically regenerate all the time and
provide more useful functionalities when learning new information. While the
goal of HDC is to exploit the high-dimensionality of randomly generated base
hypervectors to represent the information as a pattern of neural activity, it
remains challenging for existing HDCs to support a similar behavior as brain
neural regeneration. In this work, we present dynamic HDC learning frameworks
that identify and regenerate undesired dimensions to provide adequate accuracy
with significantly lowered dimensionalities, thereby accelerating both the
training and inference.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07708">Reinforcement Learning for Solving Stochastic Vehicle Routing Problem. (arXiv:2311.07708v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Iklassov_Z/0/1/0/all/0/1">Zangir Iklassov</a>, <a href="http://arxiv.org/find/cs/1/au:+Sobirov_I/0/1/0/all/0/1">Ikboljon Sobirov</a>, <a href="http://arxiv.org/find/cs/1/au:+Solozabal_R/0/1/0/all/0/1">Ruben Solozabal</a>, <a href="http://arxiv.org/find/cs/1/au:+Takac_M/0/1/0/all/0/1">Martin Takac</a></p>
<p>This study addresses a gap in the utilization of Reinforcement Learning (RL)
and Machine Learning (ML) techniques in solving the Stochastic Vehicle Routing
Problem (SVRP) that involves the challenging task of optimizing vehicle routes
under uncertain conditions. We propose a novel end-to-end framework that
comprehensively addresses the key sources of stochasticity in SVRP and utilizes
an RL agent with a simple yet effective architecture and a tailored training
method. Through comparative analysis, our proposed model demonstrates superior
performance compared to a widely adopted state-of-the-art metaheuristic,
achieving a significant 3.43% reduction in travel costs. Furthermore, the model
exhibits robustness across diverse SVRP settings, highlighting its adaptability
and ability to learn optimal routing strategies in varying environments. The
publicly available implementation of our framework serves as a valuable
resource for future research endeavors aimed at advancing RL-based solutions
for SVRP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07711">Histopathologic Cancer Detection. (arXiv:2311.07711v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Rohila_V/0/1/0/all/0/1">Varan Singh Rohila</a>, <a href="http://arxiv.org/find/cs/1/au:+Lalwani_N/0/1/0/all/0/1">Neeraj Lalwani</a>, <a href="http://arxiv.org/find/cs/1/au:+Basyal_L/0/1/0/all/0/1">Lochan Basyal</a></p>
<p>Early diagnosis of the cancer cells is necessary for making an effective
treatment plan and for the health and safety of a patient. Nowadays, doctors
usually use a histological grade that pathologists determine by performing a
semi-quantitative analysis of the histopathological and cytological features of
hematoxylin-eosin (HE) stained histopathological images. This research
contributes a potential classification model for cancer prognosis to
efficiently utilize the valuable information underlying the HE-stained
histopathological images. This work uses the PatchCamelyon benchmark datasets
and trains them in a multi-layer perceptron and convolution model to observe
the model's performance in terms of precision, Recall, F1 Score, Accuracy, and
AUC Score. The evaluation result shows that the baseline convolution model
outperforms the baseline MLP model. Also, this paper introduced ResNet50 and
InceptionNet models with data augmentation, where ResNet50 is able to beat the
state-of-the-art model. Furthermore, the majority vote and concatenation
ensemble were evaluated and provided the future direction of using transfer
learning and segmentation to understand the specific features.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07715">PolyIE: A Dataset of Information Extraction from Polymer Material Scientific Literature. (arXiv:2311.07715v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheung_J/0/1/0/all/0/1">Jerry Junyang Cheung</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1">Yuchen Zhuang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yinghao Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Shetty_P/0/1/0/all/0/1">Pranav Shetty</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1">Wantian Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Grampurohit_S/0/1/0/all/0/1">Sanjeev Grampurohit</a>, <a href="http://arxiv.org/find/cs/1/au:+Ramprasad_R/0/1/0/all/0/1">Rampi Ramprasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chao Zhang</a></p>
<p>Scientific information extraction (SciIE), which aims to automatically
extract information from scientific literature, is becoming more important than
ever. However, there are no existing SciIE datasets for polymer materials,
which is an important class of materials used ubiquitously in our daily lives.
To bridge this gap, we introduce POLYIE, a new SciIE dataset for polymer
materials. POLYIE is curated from 146 full-length polymer scholarly articles,
which are annotated with different named entities (i.e., materials, properties,
values, conditions) as well as their N-ary relations by domain experts. POLYIE
presents several unique challenges due to diverse lexical formats of entities,
ambiguity between entities, and variable-length relations. We evaluate
state-of-the-art named entity extraction and relation extraction models on
POLYIE, analyze their strengths and weaknesses, and highlight some difficult
cases for these models. To the best of our knowledge, POLYIE is the first SciIE
benchmark for polymer materials, and we hope it will lead to more research
efforts from the community on this challenging task. Our code and data are
available on: https://github.com/jerry3027/PolyIE.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07723">Generalization Analogies (GENIES): A Testbed for Generalizing AI Oversight to Hard-To-Measure Domains. (arXiv:2311.07723v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Clymer_J/0/1/0/all/0/1">Joshua Clymer</a>, <a href="http://arxiv.org/find/cs/1/au:+Baker_G/0/1/0/all/0/1">Garrett Baker</a>, <a href="http://arxiv.org/find/cs/1/au:+Subramani_R/0/1/0/all/0/1">Rohan Subramani</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Sam Wang</a></p>
<p>As AI systems become more intelligent and their behavior becomes more
challenging to assess, they may learn to game the flaws of human feedback
instead of genuinely striving to follow instructions; however, this risk can be
mitigated by controlling how LLMs generalize human feedback to situations where
it is unreliable. To better understand how reward models generalize, we craft
69 distribution shifts spanning 8 categories. We find that reward models do not
learn to evaluate `instruction-following' by default and instead favor personas
that resemble internet text. Techniques for interpreting reward models'
internal representations achieve better generalization than standard
fine-tuning, but still frequently fail to distinguish instruction-following
from conflated behaviors. We consolidate the 15 most challenging distribution
shifts into the GENaralization analogIES (GENIES) benchmark, which we hope will
enable progress toward controlling reward model generalization.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07745">Simplifying Complex Observation Models in Continuous POMDP Planning with Probabilistic Guarantees and Practice. (arXiv:2311.07745v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lev_Yehudi_I/0/1/0/all/0/1">Idan Lev-Yehudi</a>, <a href="http://arxiv.org/find/cs/1/au:+Barenboim_M/0/1/0/all/0/1">Moran Barenboim</a>, <a href="http://arxiv.org/find/cs/1/au:+Indelman_V/0/1/0/all/0/1">Vadim Indelman</a></p>
<p>Solving partially observable Markov decision processes (POMDPs) with high
dimensional and continuous observations, such as camera images, is required for
many real life robotics and planning problems. Recent researches suggested
machine learned probabilistic models as observation models, but their use is
currently too computationally expensive for online deployment. We deal with the
question of what would be the implication of using simplified observation
models for planning, while retaining formal guarantees on the quality of the
solution. Our main contribution is a novel probabilistic bound based on a
statistical total variation distance of the simplified model. We show that it
bounds the theoretical POMDP value w.r.t. original model, from the empirical
planned value with the simplified model, by generalizing recent results of
particle-belief MDP concentration bounds. Our calculations can be separated
into offline and online parts, and we arrive at formal guarantees without
having to access the costly model at all during planning, which is also a novel
result. Finally, we demonstrate in simulation how to integrate the bound into
the routine of an existing continuous online POMDP solver.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07750">SynthEnsemble: A Fusion of CNN, Vision Transformer, and Hybrid Models for Multi-Label Chest X-Ray Classification. (arXiv:2311.07750v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ashraf_S/0/1/0/all/0/1">S.M. Nabil Ashraf</a>, <a href="http://arxiv.org/find/cs/1/au:+Mamun_M/0/1/0/all/0/1">Md. Adyelullahil Mamun</a>, <a href="http://arxiv.org/find/cs/1/au:+Abdullah_H/0/1/0/all/0/1">Hasnat Md. Abdullah</a>, <a href="http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1">Md. Golam Rabiul Alam</a></p>
<p>Chest X-rays are widely used to diagnose thoracic diseases, but the lack of
detailed information about these abnormalities makes it challenging to develop
accurate automated diagnosis systems, which is crucial for early detection and
effective treatment. To address this challenge, we employed deep learning
techniques to identify patterns in chest X-rays that correspond to different
diseases. We conducted experiments on the "ChestX-ray14" dataset using various
pre-trained CNNs, transformers, hybrid(CNN+Transformer) models and classical
models. The best individual model was the CoAtNet, which achieved an area under
the receiver operating characteristic curve (AUROC) of 84.2%. By combining the
predictions of all trained models using a weighted average ensemble where the
weight of each model was determined using differential evolution, we further
improved the AUROC to 85.4%, outperforming other state-of-the-art methods in
this field. Our findings demonstrate the potential of deep learning techniques,
particularly ensemble deep learning, for improving the accuracy of automatic
diagnosis of thoracic diseases from chest X-rays.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07759">Enabling High-Level Machine Reasoning with Cognitive Neuro-Symbolic Systems. (arXiv:2311.07759v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1">Alessandro Oltramari</a></p>
<p>High-level reasoning can be defined as the capability to generalize over
knowledge acquired via experience, and to exhibit robust behavior in novel
situations. Such form of reasoning is a basic skill in humans, who seamlessly
use it in a broad spectrum of tasks, from language communication to decision
making in complex situations. When it manifests itself in understanding and
manipulating the everyday world of objects and their interactions, we talk
about common sense or commonsense reasoning. State-of-the-art AI systems don't
possess such capability: for instance, Large Language Models have recently
become popular by demonstrating remarkable fluency in conversing with humans,
but they still make trivial mistakes when probed for commonsense competence; on
a different level, performance degradation outside training data prevents
self-driving vehicles to safely adapt to unseen scenarios, a serious and
unsolved problem that limits the adoption of such technology. In this paper we
propose to enable high-level reasoning in AI systems by integrating cognitive
architectures with external neuro-symbolic components. We illustrate a hybrid
framework centered on ACT-R and we discuss the role of generative models in
recent and future applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07761">Amodal Optical Flow. (arXiv:2311.07761v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luz_M/0/1/0/all/0/1">Maximilian Luz</a>, <a href="http://arxiv.org/find/cs/1/au:+Mohan_R/0/1/0/all/0/1">Rohit Mohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Sekkat_A/0/1/0/all/0/1">Ahmed Rida Sekkat</a>, <a href="http://arxiv.org/find/cs/1/au:+Sawade_O/0/1/0/all/0/1">Oliver Sawade</a>, <a href="http://arxiv.org/find/cs/1/au:+Matthes_E/0/1/0/all/0/1">Elmar Matthes</a>, <a href="http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1">Thomas Brox</a>, <a href="http://arxiv.org/find/cs/1/au:+Valada_A/0/1/0/all/0/1">Abhinav Valada</a></p>
<p>Optical flow estimation is very challenging in situations with transparent or
occluded objects. In this work, we address these challenges at the task level
by introducing Amodal Optical Flow, which integrates optical flow with amodal
perception. Instead of only representing the visible regions, we define amodal
optical flow as a multi-layered pixel-level motion field that encompasses both
visible and occluded regions of the scene. To facilitate research on this new
task, we extend the AmodalSynthDrive dataset to include pixel-level labels for
amodal optical flow estimation. We present several strong baselines, along with
the Amodal Flow Quality metric to quantify the performance in an interpretable
manner. Furthermore, we propose the novel AmodalFlowNet as an initial step
toward addressing this task. AmodalFlowNet consists of a transformer-based
cost-volume encoder paired with a recurrent transformer decoder which
facilitates recurrent hierarchical feature propagation and amodal semantic
grounding. We demonstrate the tractability of amodal optical flow in extensive
experiments and show its utility for downstream tasks such as panoptic
tracking. We make the dataset, code, and trained models publicly available at
<a href="http://amodal-flow.cs.uni-freiburg.de.">this http URL</a>
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07763">The Disagreement Problem in Faithfulness Metrics. (arXiv:2311.07763v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Barr_B/0/1/0/all/0/1">Brian Barr</a>, <a href="http://arxiv.org/find/cs/1/au:+Fatsi_N/0/1/0/all/0/1">Noah Fatsi</a>, <a href="http://arxiv.org/find/cs/1/au:+Hancox_Li_L/0/1/0/all/0/1">Leif Hancox-Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Richter_P/0/1/0/all/0/1">Peter Richter</a>, <a href="http://arxiv.org/find/cs/1/au:+Proano_D/0/1/0/all/0/1">Daniel Proano</a>, <a href="http://arxiv.org/find/cs/1/au:+Mok_C/0/1/0/all/0/1">Caleb Mok</a></p>
<p>The field of explainable artificial intelligence (XAI) aims to explain how
black-box machine learning models work. Much of the work centers around the
holy grail of providing post-hoc feature attributions to any model
architecture. While the pace of innovation around novel methods has slowed
down, the question remains of how to choose a method, and how to make it fit
for purpose. Recently, efforts around benchmarking XAI methods have suggested
metrics for that purpose -- but there are many choices. That bounty of choice
still leaves an end user unclear on how to proceed. This paper focuses on
comparing metrics with the aim of measuring faithfulness of local explanations
on tabular classification problems -- and shows that the current metrics don't
agree; leaving users unsure how to choose the most faithful explanations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07766">Vision-Language Integration in Multimodal Video Transformers (Partially) Aligns with the Brain. (arXiv:2311.07766v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dong_D/0/1/0/all/0/1">Dota Tianai Dong</a>, <a href="http://arxiv.org/find/cs/1/au:+Toneva_M/0/1/0/all/0/1">Mariya Toneva</a></p>
<p>Integrating information from multiple modalities is arguably one of the
essential prerequisites for grounding artificial intelligence systems with an
understanding of the real world. Recent advances in video transformers that
jointly learn from vision, text, and sound over time have made some progress
toward this goal, but the degree to which these models integrate information
from modalities still remains unclear. In this work, we present a promising
approach for probing a pre-trained multimodal video transformer model by
leveraging neuroscientific evidence of multimodal information processing in the
brain. Using brain recordings of participants watching a popular TV show, we
analyze the effects of multi-modal connections and interactions in a
pre-trained multi-modal video transformer on the alignment with uni- and
multi-modal brain regions. We find evidence that vision enhances masked
prediction performance during language processing, providing support that
cross-modal representations in models can benefit individual modalities.
However, we don't find evidence of brain-relevant information captured by the
joint multi-modal transformer representations beyond that captured by all of
the individual modalities. We finally show that the brain alignment of the
pre-trained joint representation can be improved by fine-tuning using a task
that requires vision-language inferences. Overall, our results paint an
optimistic picture of the ability of multi-modal transformers to integrate
vision and language in partially brain-relevant ways but also show that
improving the brain alignment of these models may require new approaches.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07767">GreekT5: A Series of Greek Sequence-to-Sequence Models for News Summarization. (arXiv:2311.07767v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Giarelis_N/0/1/0/all/0/1">Nikolaos Giarelis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mastrokostas_C/0/1/0/all/0/1">Charalampos Mastrokostas</a>, <a href="http://arxiv.org/find/cs/1/au:+Karacapilidis_N/0/1/0/all/0/1">Nikos Karacapilidis</a></p>
<p>Text summarization (TS) is a natural language processing (NLP) subtask
pertaining to the automatic formulation of a concise and coherent summary that
covers the major concepts and topics from one or multiple documents. Recent
advancements in deep learning have led to the development of abstractive
summarization transformer-based models, which outperform classical approaches.
In any case, research in this field focuses on high resource languages such as
English, while the corresponding work for low resource languages is still
underdeveloped. Taking the above into account, this paper proposes a series of
novel TS models for Greek news articles. The proposed models were thoroughly
evaluated on the same dataset against GreekBART, which is the state-of-the-art
model in Greek abstractive news summarization. Our evaluation results reveal
that most of the proposed models significantly outperform GreekBART on various
evaluation metrics. We make our evaluation code public, aiming to increase the
reproducibility of this work and facilitate future research in the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07780">Parrot-Trained Adversarial Examples: Pushing the Practicality of Black-Box Audio Attacks against Speaker Recognition Models. (arXiv:2311.07780v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Duan_R/0/1/0/all/0/1">Rui Duan</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_Z/0/1/0/all/0/1">Zhe Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1">Leah Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yao Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1">Zhuo Lu</a></p>
<p>Audio adversarial examples (AEs) have posed significant security challenges
to real-world speaker recognition systems. Most black-box attacks still require
certain information from the speaker recognition model to be effective (e.g.,
keeping probing and requiring the knowledge of similarity scores). This work
aims to push the practicality of the black-box attacks by minimizing the
attacker's knowledge about a target speaker recognition model. Although it is
not feasible for an attacker to succeed with completely zero knowledge, we
assume that the attacker only knows a short (or a few seconds) speech sample of
a target speaker. Without any probing to gain further knowledge about the
target model, we propose a new mechanism, called parrot training, to generate
AEs against the target model. Motivated by recent advancements in voice
conversion (VC), we propose to use the one short sentence knowledge to generate
more synthetic speech samples that sound like the target speaker, called parrot
speech. Then, we use these parrot speech samples to train a parrot-trained(PT)
surrogate model for the attacker. Under a joint transferability and perception
framework, we investigate different ways to generate AEs on the PT model
(called PT-AEs) to ensure the PT-AEs can be generated with high transferability
to a black-box target model with good human perceptual quality. Real-world
experiments show that the resultant PT-AEs achieve the attack success rates of
45.8% - 80.8% against the open-source models in the digital-line scenario and
47.9% - 58.3% against smart devices, including Apple HomePod (Siri), Amazon
Echo, and Google Home, in the over-the-air scenario.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07815">Cooperative AI via Decentralized Commitment Devices. (arXiv:2311.07815v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xinyuan Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Crapis_D/0/1/0/all/0/1">Davide Crapis</a>, <a href="http://arxiv.org/find/cs/1/au:+Stephenson_M/0/1/0/all/0/1">Matt Stephenson</a>, <a href="http://arxiv.org/find/cs/1/au:+Monnot_B/0/1/0/all/0/1">Barnab&#xe9; Monnot</a>, <a href="http://arxiv.org/find/cs/1/au:+Thiery_T/0/1/0/all/0/1">Thomas Thiery</a>, <a href="http://arxiv.org/find/cs/1/au:+Passerat_Palmbach_J/0/1/0/all/0/1">Jonathan Passerat-Palmbach</a></p>
<p>Credible commitment devices have been a popular approach for robust
multi-agent coordination. However, existing commitment mechanisms face
limitations like privacy, integrity, and susceptibility to mediator or user
strategic behavior. It is unclear if the cooperative AI techniques we study are
robust to real-world incentives and attack vectors. However, decentralized
commitment devices that utilize cryptography have been deployed in the wild,
and numerous studies have shown their ability to coordinate algorithmic agents
facing adversarial opponents with significant economic incentives, currently in
the order of several million to billions of dollars. In this paper, we use
examples in the decentralization and, in particular, Maximal Extractable Value
(MEV) (<a href="/abs/1904.05234">arXiv:1904.05234</a>) literature to illustrate the potential security issues
in cooperative AI. We call for expanded research into decentralized commitments
to advance cooperative AI capabilities for secure coordination in open
environments and empirical testing frameworks to evaluate multi-agent
coordination ability given real-world commitment constraints.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07816">Leveraging Large Language Models to Detect Influence Campaigns in Social Media. (arXiv:2311.07816v1 [cs.SI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Luceri_L/0/1/0/all/0/1">Luca Luceri</a>, <a href="http://arxiv.org/find/cs/1/au:+Boniardi_E/0/1/0/all/0/1">Eric Boniardi</a>, <a href="http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1">Emilio Ferrara</a></p>
<p>Social media influence campaigns pose significant challenges to public
discourse and democracy. Traditional detection methods fall short due to the
complexity and dynamic nature of social media. Addressing this, we propose a
novel detection method using Large Language Models (LLMs) that incorporates
both user metadata and network structures. By converting these elements into a
text format, our approach effectively processes multilingual content and adapts
to the shifting tactics of malicious campaign actors. We validate our model
through rigorous testing on multiple datasets, showcasing its superior
performance in identifying influence efforts. This research not only offers a
powerful tool for detecting campaigns, but also sets the stage for future
enhancements to keep up with the fast-paced evolution of social media-based
influence tactics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07822">A Neuro-Inspired Hierarchical Reinforcement Learning for Motor Control. (arXiv:2311.07822v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1">Pei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Hua_Z/0/1/0/all/0/1">Zhaobo Hua</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1">Jinliang Ding</a></p>
<p>Designing controllers to achieve natural motion capabilities for multi-joint
robots is a significant challenge. However, animals in nature are naturally
with basic motor abilities and can master various complex motor skills through
acquired learning. On the basis of analyzing the mechanism of the central motor
system in mammals, we propose a neuro-inspired hierarchical reinforcement
learning algorithm that enables robots to learn rich motor skills and apply
them to complex task environments without relying on external data. We first
design a skills network similar to the cerebellum by utilizing the selection
mechanism of voluntary movements in the basal ganglia and the regulatory
ability of the cerebellum to regulate movement. Subsequently, by imitating the
structure of advanced centers in the motion system, we propose a high-level
policy to generate different skill combinations, thereby enabling the robot to
acquire natural motor abilities. We conduct experiments on 4 types of robots
and 22 task environments, and the results show that the proposed method can
enable different types of robots to achieve flexible motion skills. Overall,
our research provides a promising framework for the design of robotic neural
motor controllers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07838">LLatrieval: LLM-Verified Retrieval for Verifiable Generation. (arXiv:2311.07838v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1">Xiaonan Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1">Changtai Zhu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Linyang Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1">Zhangyue Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1">Tianxiang Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1">Xipeng Qiu</a></p>
<p>Verifiable generation aims to let the large language model (LLM) generate
text with corresponding supporting documents, which enables the user to
flexibly verify the answer and makes it more trustworthy. Its evaluation not
only measures the correctness of the answer, but also the answer's
verifiability, i.e., how well the answer is supported by the corresponding
documents. In typical, verifiable generation adopts the retrieval-read
pipeline, which is divided into two stages: 1) retrieve relevant documents of
the question. 2) according to the documents, generate the corresponding answer.
Since the retrieved documents can supplement knowledge for the LLM to generate
the answer and serve as evidence, the retrieval stage is essential for the
correctness and verifiability of the answer. However, the widely used
retrievers become the bottleneck of the entire pipeline and limit the overall
performance. They often have fewer parameters than the large language model and
have not been proven to scale well to the size of LLMs. Since the LLM passively
receives the retrieval result, if the retriever does not correctly find the
supporting documents, the LLM can not generate the correct and verifiable
answer, which overshadows the LLM's remarkable abilities. In this paper, we
propose LLatrieval (Large Language Model Verified Retrieval), where the LLM
updates the retrieval result until it verifies that the retrieved documents can
support answering the question. Thus, the LLM can iteratively provide feedback
to retrieval and facilitate the retrieval result to sufficiently support
verifiable generation. Experimental results show that our method significantly
outperforms extensive baselines and achieves new state-of-the-art results.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07840">Enabling Decision-Support Systems through Automated Cell Tower Detection. (arXiv:2311.07840v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Krell_N/0/1/0/all/0/1">Natasha Krell</a>, <a href="http://arxiv.org/find/cs/1/au:+Gleave_W/0/1/0/all/0/1">Will Gleave</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakada_D/0/1/0/all/0/1">Daniel Nakada</a>, <a href="http://arxiv.org/find/cs/1/au:+Downes_J/0/1/0/all/0/1">Justin Downes</a>, <a href="http://arxiv.org/find/cs/1/au:+Willet_A/0/1/0/all/0/1">Amanda Willet</a>, <a href="http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1">Matthew Baran</a></p>
<p>Cell phone coverage and high-speed service gaps persist in rural areas in
sub-Saharan Africa, impacting public access to mobile-based financial,
educational, and humanitarian services. Improving maps of telecommunications
infrastructure can help inform strategies to eliminate gaps in mobile coverage.
Deep neural networks, paired with remote sensing images, can be used for object
detection of cell towers and eliminate the need for inefficient and burdensome
manual mapping to find objects over large geographic regions. In this study, we
demonstrate a partially automated workflow to train an object detection model
to locate cell towers using OpenStreetMap (OSM) features and high-resolution
Maxar imagery. For model fine-tuning and evaluation, we curated a diverse
dataset of over 6,000 unique images of cell towers in 26 countries in eastern,
southern, and central Africa using automatically generated annotations from OSM
points. Our model achieves an average precision at 50% Intersection over Union
(IoU) (AP@50) of 81.2 with good performance across different geographies and
out-of-sample testing. Accurate localization of cell towers can yield more
accurate cell coverage maps, in turn enabling improved delivery of digital
services for decision-support applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07850">Bring Your Own KG: Self-Supervised Program Synthesis for Zero-Shot KGQA. (arXiv:2311.07850v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Agarwal_D/0/1/0/all/0/1">Dhruv Agarwal</a>, <a href="http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1">Rajarshi Das</a>, <a href="http://arxiv.org/find/cs/1/au:+Khosla_S/0/1/0/all/0/1">Sopan Khosla</a>, <a href="http://arxiv.org/find/cs/1/au:+Gangadharaiah_R/0/1/0/all/0/1">Rashmi Gangadharaiah</a></p>
<p>We present BYOKG, a universal question-answering (QA) system that can operate
on any knowledge graph (KG), requires no human-annotated training data, and can
be ready to use within a day -- attributes that are out-of-scope for current
KGQA systems. BYOKG draws inspiration from the remarkable ability of humans to
comprehend information present in an unseen KG through exploration -- starting
at random nodes, inspecting the labels of adjacent nodes and edges, and
combining them with their prior world knowledge. In BYOKG, exploration
leverages an LLM-backed symbolic agent that generates a diverse set of
query-program exemplars, which are then used to ground a retrieval-augmented
reasoning procedure to predict programs for arbitrary questions. BYOKG is
effective over both small- and large-scale graphs, showing dramatic gains in QA
accuracy over a zero-shot baseline of 27.89 and 58.02 F1 on GrailQA and MetaQA,
respectively. On GrailQA, we further show that our unsupervised BYOKG
outperforms a supervised in-context learning method, demonstrating the
effectiveness of exploration. Lastly, we find that performance of BYOKG
reliably improves with continued exploration as well as improvements in the
base LLM, notably outperforming a state-of-the-art fine-tuned model by 7.08 F1
on a sub-sampled zero-shot split of GrailQA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07861">Overview of the TREC 2023 Product Product Search Track. (arXiv:2311.07861v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Campos_D/0/1/0/all/0/1">Daniel Campos</a>, <a href="http://arxiv.org/find/cs/1/au:+Kallumadi_S/0/1/0/all/0/1">Surya Kallumadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosset_C/0/1/0/all/0/1">Corby Rosset</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhai_C/0/1/0/all/0/1">Cheng Xiang Zhai</a>, <a href="http://arxiv.org/find/cs/1/au:+Magnani_A/0/1/0/all/0/1">Alessandro Magnani</a></p>
<p>This is the first year of the TREC Product search track. The focus this year
was the creation of a reusable collection and evaluation of the impact of the
use of metadata and multi-modal data on retrieval accuracy. This year we
leverage the new product search corpus, which includes contextual metadata. Our
analysis shows that in the product search domain, traditional retrieval systems
are highly effective and commonly outperform general-purpose pretrained
embedding models. Our analysis also evaluates the impact of using simplified
and metadata-enhanced collections, finding no clear trend in the impact of the
expanded collection. We also see some surprising outcomes; despite their
widespread adoption and competitive performance on other tasks, we find
single-stage dense retrieval runs can commonly be noncompetitive or generate
low-quality results both in the zero-shot and fine-tuned domain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07868">Multi-Signal Reconstruction Using Masked Autoencoder From EEG During Polysomnography. (arXiv:2311.07868v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kweon_Y/0/1/0/all/0/1">Young-Seok Kweon</a>, <a href="http://arxiv.org/find/cs/1/au:+Shin_G/0/1/0/all/0/1">Gi-Hwan Shin</a>, <a href="http://arxiv.org/find/cs/1/au:+Kwak_H/0/1/0/all/0/1">Heon-Gyu Kwak</a>, <a href="http://arxiv.org/find/cs/1/au:+Jo_H/0/1/0/all/0/1">Ha-Na Jo</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Polysomnography (PSG) is an indispensable diagnostic tool in sleep medicine,
essential for identifying various sleep disorders. By capturing physiological
signals, including EEG, EOG, EMG, and cardiorespiratory metrics, PSG presents a
patient's sleep architecture. However, its dependency on complex equipment and
expertise confines its use to specialized clinical settings. Addressing these
limitations, our study aims to perform PSG by developing a system that requires
only a single EEG measurement. We propose a novel system capable of
reconstructing multi-signal PSG from a single-channel EEG based on a masked
autoencoder. The masked autoencoder was trained and evaluated using the
Sleep-EDF-20 dataset, with mean squared error as the metric for assessing the
similarity between original and reconstructed signals. The model demonstrated
proficiency in reconstructing multi-signal data. Our results present promise
for the development of more accessible and long-term sleep monitoring systems.
This suggests the expansion of PSG's applicability, enabling its use beyond the
confines of clinics.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07870">AutoML for Large Capacity Modeling of Meta Ranking Systems. (arXiv:2311.07870v1 [cs.IR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1">Hang Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1">Kuang-Hung Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1">Mengying Sun</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuxin Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1">Buyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1">Jiang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sehgal_V/0/1/0/all/0/1">Vivek Sehgal</a>, <a href="http://arxiv.org/find/cs/1/au:+Panchal_R/0/1/0/all/0/1">Rudresh Rajnikant Panchal</a>, <a href="http://arxiv.org/find/cs/1/au:+Hotaj_E/0/1/0/all/0/1">Eugen Hotaj</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1">Daifeng Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1">Jamey Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhou Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1">Shali Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1">Huayu Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhengxing Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Wen-Yen Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1">Jiyan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1">Wei Wen</a></p>
<p>Web-scale ranking systems at Meta serving billions of users is complex.
Improving ranking models is essential but engineering heavy. Automated Machine
Learning (AutoML) can release engineers from labor intensive work of tuning
ranking models; however, it is unknown if AutoML is efficient enough to meet
tight production timeline in real-world and, at the same time, bring additional
improvements to the strong baselines. Moreover, to achieve higher ranking
performance, there is an ever-increasing demand to scale up ranking models to
even larger capacity, which imposes more challenges on the efficiency. The
large scale of models and tight production schedule requires AutoML to
outperform human baselines by only using a small number of model evaluation
trials (around 100). We presents a sampling-based AutoML method, focusing on
neural architecture search and hyperparameter optimization, addressing these
challenges in Meta-scale production when building large capacity models. Our
approach efficiently handles large-scale data demands. It leverages a
lightweight predictor-based searcher and reinforcement learning to explore vast
search spaces, significantly reducing the number of model evaluations. Through
experiments in large capacity modeling for CTR and CVR applications, we show
that our method achieves outstanding Return on Investment (ROI) versus human
tuned baselines, with up to 0.09% Normalized Entropy (NE) loss reduction or
$25\%$ Query per Second (QPS) increase by only sampling one hundred models on
average from a curated search space. The proposed AutoML method has already
made real-world impact where a discovered Instagram CTR model with up to -0.36%
NE gain (over existing production baseline) was selected for large-scale online
A/B test and show statistically significant gain. These production results
proved AutoML efficacy and accelerated its adoption in ranking systems at Meta.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07876">Learning Adversarial Low-rank Markov Decision Processes with Unknown Transition and Full-information Feedback. (arXiv:2311.07876v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Canzhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1">Ruofeng Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Baoxiang Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xuezhou Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shuai Li</a></p>
<p>In this work, we study the low-rank MDPs with adversarially changed losses in
the full-information feedback setting. In particular, the unknown transition
probability kernel admits a low-rank matrix decomposition \citep{REPUCB22}, and
the loss functions may change adversarially but are revealed to the learner at
the end of each episode. We propose a policy optimization-based algorithm POLO,
and we prove that it attains the
$\widetilde{O}(K^{\frac{5}{6}}A^{\frac{1}{2}}d\ln(1+M)/(1-\gamma)^2)$ regret
guarantee, where $d$ is rank of the transition kernel (and hence the dimension
of the unknown representations), $A$ is the cardinality of the action space,
$M$ is the cardinality of the model class, and $\gamma$ is the discounted
factor. Notably, our algorithm is oracle-efficient and has a regret guarantee
with no dependence on the size of potentially arbitrarily large state space.
Furthermore, we also prove an $\Omega(\frac{\gamma^2}{1-\gamma} \sqrt{d A K})$
regret lower bound for this problem, showing that low-rank MDPs are
statistically more difficult to learn than linear MDPs in the regret
minimization setting. To the best of our knowledge, we present the first
algorithm that interleaves representation learning, exploration, and
exploitation to achieve the sublinear regret guarantee for RL with nonlinear
function approximation and adversarial losses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07879">Toxicity Detection is NOT all you Need: Measuring the Gaps to Supporting Volunteer Content Moderators. (arXiv:2311.07879v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1">Yang Trista Cao</a>, <a href="http://arxiv.org/find/cs/1/au:+Domingo_L/0/1/0/all/0/1">Lovely-Frances Domingo</a>, <a href="http://arxiv.org/find/cs/1/au:+Gilbert_S/0/1/0/all/0/1">Sarah Ann Gilbert</a>, <a href="http://arxiv.org/find/cs/1/au:+Mazurek_M/0/1/0/all/0/1">Michelle Mazurek</a>, <a href="http://arxiv.org/find/cs/1/au:+Shilton_K/0/1/0/all/0/1">Katie Shilton</a>, <a href="http://arxiv.org/find/cs/1/au:+Daume_H/0/1/0/all/0/1">Hal Daum&#xe9; III</a></p>
<p>Extensive efforts in automated approaches for content moderation have been
focused on developing models to identify toxic, offensive, and hateful content
-- with the aim of lightening the load for moderators. Yet, it remains
uncertain whether improvements on those tasks truly address the needs that
moderators have in accomplishing their work. In this paper, we surface the gaps
between past research efforts that have aimed to provide automation for aspects
of the content moderation task, and the needs of volunteer content moderators.
To do so, we conduct a model review on Hugging Face to reveal the availability
of models to cover various moderation rules and guidelines. We further put
state-of-the-art LLMs to the test (GPT-4 and Llama-2), evaluating how well
these models perform in flagging violations of platform rules. Overall, we
observe a non-trivial gap, as missing developed models and LLMs exhibit low
recall on a significant portion of the rules.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07880">VegaEdge: Edge AI Confluence Anomaly Detection for Real-Time Highway IoT-Applications. (arXiv:2311.07880v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Katariya_V/0/1/0/all/0/1">Vinit Katariya</a>, <a href="http://arxiv.org/find/cs/1/au:+Jannat_F/0/1/0/all/0/1">Fatema-E- Jannat</a>, <a href="http://arxiv.org/find/cs/1/au:+Pazho_A/0/1/0/all/0/1">Armin Danesh Pazho</a>, <a href="http://arxiv.org/find/cs/1/au:+Noghre_G/0/1/0/all/0/1">Ghazal Alinezhad Noghre</a>, <a href="http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1">Hamed Tabkhi</a></p>
<p>Vehicle anomaly detection plays a vital role in highway safety applications
such as accident prevention, rapid response, traffic flow optimization, and
work zone safety. With the surge of the Internet of Things (IoT) in recent
years, there has arisen a pressing demand for Artificial Intelligence (AI)
based anomaly detection methods designed to meet the requirements of IoT
devices. Catering to this futuristic vision, we introduce a lightweight
approach to vehicle anomaly detection by utilizing the power of trajectory
prediction. Our proposed design identifies vehicles deviating from expected
paths, indicating highway risks from different camera-viewing angles from
real-world highway datasets. On top of that, we present VegaEdge - a
sophisticated AI confluence designed for real-time security and surveillance
applications in modern highway settings through edge-centric IoT-embedded
platforms equipped with our anomaly detection approach. Extensive testing
across multiple platforms and traffic scenarios showcases the versatility and
effectiveness of VegaEdge. This work also presents the Carolinas Anomaly
Dataset (CAD), to bridge the existing gap in datasets tailored for highway
anomalies. In real-world scenarios, our anomaly detection approach achieves an
AUC-ROC of 0.94, and our proposed VegaEdge design, on an embedded IoT platform,
processes 738 trajectories per second in a typical highway setting. The dataset
is available at
https://github.com/TeCSAR-UNCC/Carolinas_Dataset#chd-anomaly-test-set .
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07885">One-2-3-45++: Fast Single Image to 3D Objects with Consistent Multi-View Generation and 3D Diffusion. (arXiv:2311.07885v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1">Minghua Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_R/0/1/0/all/0/1">Ruoxi Shi</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1">Linghao Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Zhuoyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1">Chao Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1">Xinyue Wei</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hansheng Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_C/0/1/0/all/0/1">Chong Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1">Jiayuan Gu</a>, <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hao Su</a></p>
<p>Recent advancements in open-world 3D object generation have been remarkable,
with image-to-3D methods offering superior fine-grained control over their
text-to-3D counterparts. However, most existing models fall short in
simultaneously providing rapid generation speeds and high fidelity to input
images - two features essential for practical applications. In this paper, we
present One-2-3-45++, an innovative method that transforms a single image into
a detailed 3D textured mesh in approximately one minute. Our approach aims to
fully harness the extensive knowledge embedded in 2D diffusion models and
priors from valuable yet limited 3D data. This is achieved by initially
finetuning a 2D diffusion model for consistent multi-view image generation,
followed by elevating these images to 3D with the aid of multi-view conditioned
3D native diffusion models. Extensive experimental evaluations demonstrate that
our method can produce high-quality, diverse 3D assets that closely mirror the
original input image. Our project webpage:
https://sudo-ai-3d.github.io/One2345plus_page.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07888">RoboSense At Edge: Detecting Slip, Crumple and Shape of the Object in Robotic Hand for Teleoprations. (arXiv:2311.07888v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Padhi_S/0/1/0/all/0/1">Sudev Kumar Padhi</a>, <a href="http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1">Mohit Kumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Giri_D/0/1/0/all/0/1">Debanka Giri</a>, <a href="http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1">Subidh Ali</a></p>
<p>Slip and crumple detection is essential for performing robust manipulation
tasks with a robotic hand (RH) like remote surgery. It has been one of the
challenging problems in the robotics manipulation community. In this work, we
propose a technique based on machine learning (ML) based techniques to detect
the slip, and crumple as well as the shape of an object that is currently held
in the robotic hand. We proposed ML model will detect the slip, crumple, and
shape using the force/torque exerted and the angular positions of the actuators
present in the RH. The proposed model would be integrated into the loop of a
robotic hand(RH) and haptic glove(HG). This would help us to reduce the latency
in case of teleoperation
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07911">Instruction-Following Evaluation for Large Language Models. (arXiv:2311.07911v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jeffrey Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1">Tianjian Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1">Swaroop Mishra</a>, <a href="http://arxiv.org/find/cs/1/au:+Brahma_S/0/1/0/all/0/1">Siddhartha Brahma</a>, <a href="http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1">Sujoy Basu</a>, <a href="http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1">Yi Luan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1">Denny Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1">Le Hou</a></p>
<p>One core capability of Large Language Models (LLMs) is to follow natural
language instructions. However, the evaluation of such abilities is not
standardized: Human evaluations are expensive, slow, and not objectively
reproducible, while LLM-based auto-evaluation is potentially biased or limited
by the ability of the evaluator LLM. To overcome these issues, we introduce
Instruction-Following Eval (IFEval) for large language models. IFEval is a
straightforward and easy-to-reproduce evaluation benchmark. It focuses on a set
of "verifiable instructions" such as "write in more than 400 words" and
"mention the keyword of AI at least 3 times". We identified 25 types of those
verifiable instructions and constructed around 500 prompts, with each prompt
containing one or more verifiable instructions. We show evaluation results of
two widely available LLMs on the market. Our code and data can be found at
https://github.com/google-research/google-research/tree/master/instruction_following_eval
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07925">Brain-Driven Representation Learning Based on Diffusion Model. (arXiv:2311.07925v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1">Soowon Kim</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seo-Hyun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Young-Eun Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Ji-Won Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Ji-Ha Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1">Seong-Whan Lee</a></p>
<p>Interpreting EEG signals linked to spoken language presents a complex
challenge, given the data's intricate temporal and spatial attributes, as well
as the various noise factors. Denoising diffusion probabilistic models (DDPMs),
which have recently gained prominence in diverse areas for their capabilities
in representation learning, are explored in our research as a means to address
this issue. Using DDPMs in conjunction with a conditional autoencoder, our new
approach considerably outperforms traditional machine learning algorithms and
established baseline models in accuracy. Our results highlight the potential of
DDPMs as a sophisticated computational method for the analysis of
speech-related EEG signals. This could lead to significant advances in
brain-computer interfaces tailored for spoken communication.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07928">Towards Improving Robustness Against Common Corruptions in Object Detectors Using Adversarial Contrastive Learning. (arXiv:2311.07928v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kotyan_S/0/1/0/all/0/1">Shashank Kotyan</a>, <a href="http://arxiv.org/find/cs/1/au:+Vargas_D/0/1/0/all/0/1">Danilo Vasconcellos Vargas</a></p>
<p>Neural networks have revolutionized various domains, exhibiting remarkable
accuracy in tasks like natural language processing and computer vision.
However, their vulnerability to slight alterations in input samples poses
challenges, particularly in safety-critical applications like autonomous
driving. Current approaches, such as introducing distortions during training,
fall short in addressing unforeseen corruptions. This paper proposes an
innovative adversarial contrastive learning framework to enhance neural network
robustness simultaneously against adversarial attacks and common corruptions.
By generating instance-wise adversarial examples and optimizing contrastive
loss, our method fosters representations that resist adversarial perturbations
and remain robust in real-world scenarios. Subsequent contrastive learning then
strengthens the similarity between clean samples and their adversarial
counterparts, fostering representations resistant to both adversarial attacks
and common distortions. By focusing on improving performance under adversarial
and real-world conditions, our approach aims to bolster the robustness of
neural networks in safety-critical applications, such as autonomous vehicles
navigating unpredictable weather conditions. We anticipate that this framework
will contribute to advancing the reliability of neural networks in challenging
environments, facilitating their widespread adoption in mission-critical
scenarios.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07941">Non-autoregressive Machine Translation with Probabilistic Context-free Grammar. (arXiv:2311.07941v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gui_S/0/1/0/all/0/1">Shangtong Gui</a>, <a href="http://arxiv.org/find/cs/1/au:+Shao_C/0/1/0/all/0/1">Chenze Shao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zhengrui Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1">Xishan Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yunji Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Non-autoregressive Transformer(NAT) significantly accelerates the inference
of neural machine translation. However, conventional NAT models suffer from
limited expression power and performance degradation compared to autoregressive
(AT) models due to the assumption of conditional independence among target
tokens. To address these limitations, we propose a novel approach called
PCFG-NAT, which leverages a specially designed Probabilistic Context-Free
Grammar (PCFG) to enhance the ability of NAT models to capture complex
dependencies among output tokens. Experimental results on major machine
translation benchmarks demonstrate that PCFG-NAT further narrows the gap in
translation quality between NAT and AT models. Moreover, PCFG-NAT facilitates a
deeper understanding of the generated sentences, addressing the lack of
satisfactory explainability in neural machine translation.Code is publicly
available at https://github.com/ictnlp/PCFG-NAT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07946">The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks. (arXiv:2311.07946v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Piaseczny_A/0/1/0/all/0/1">Adam Piaseczny</a>, <a href="http://arxiv.org/find/cs/1/au:+Ruzomberka_E/0/1/0/all/0/1">Eric Ruzomberka</a>, <a href="http://arxiv.org/find/cs/1/au:+Parasnis_R/0/1/0/all/0/1">Rohit Parasnis</a>, <a href="http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1">Christopher G. Brinton</a></p>
<p>As Federated Learning (FL) grows in popularity, new decentralized frameworks
are becoming widespread. These frameworks leverage the benefits of
decentralized environments to enable fast and energy-efficient inter-device
communication. However, this growing popularity also intensifies the need for
robust security measures. While existing research has explored various aspects
of FL security, the role of adversarial node placement in decentralized
networks remains largely unexplored. This paper addresses this gap by analyzing
the performance of decentralized FL for various adversarial placement
strategies when adversaries can jointly coordinate their placement within a
network. We establish two baseline strategies for placing adversarial node:
random placement and network centrality-based placement. Building on this
foundation, we propose a novel attack algorithm that prioritizes adversarial
spread over adversarial centrality by maximizing the average network distance
between adversaries. We show that the new attack algorithm significantly
impacts key performance metrics such as testing accuracy, outperforming the
baseline frameworks by between 9% and 66.5% for the considered setups. Our
findings provide valuable insights into the vulnerabilities of decentralized FL
systems, setting the stage for future research aimed at developing more secure
and robust decentralized FL frameworks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07954">A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning. (arXiv:2311.07954v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1">Ruixin Hong</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1">Hongming Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Pang_X/0/1/0/all/0/1">Xinyu Pang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1">Dong Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Changshui Zhang</a></p>
<p>Logical reasoning has been an ongoing pursuit in the field of AI. Despite
significant advancements made by large language models (LLMs), they still
struggle with complex logical reasoning problems. To enhance reasoning
performance, one promising direction is scalable oversight, which requires LLMs
to identify their own errors and then improve by themselves. Various
self-verification methods have been proposed in pursuit of this goal.
Nevertheless, whether existing models understand their own errors well is still
under investigation. In this paper, we take a closer look at the
self-verification abilities of LLMs in the context of logical reasoning,
focusing on their ability to identify logical fallacies accurately. We
introduce a dataset, FALLACIES, containing 232 types of reasoning fallacies
categorized in a hierarchical taxonomy. By conducting exhaustive experiments on
FALLACIES, we obtain comprehensive and detailed analyses of a series of models
on their verification abilities. Our main findings suggest that existing LLMs
could struggle to identify fallacious reasoning steps accurately and may fall
short of guaranteeing the validity of self-verification methods. Drawing from
these observations, we offer suggestions for future research and practical
applications of self-verification methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07955">Deep Learning-Based Object Detection in Maritime Unmanned Aerial Vehicle Imagery: Review and Experimental Comparisons. (arXiv:2311.07955v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1">Chenjie Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1">Ryan Wen Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1">Jingxiang Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_R/0/1/0/all/0/1">Ruobin Gao</a></p>
<p>With the advancement of maritime unmanned aerial vehicles (UAVs) and deep
learning technologies, the application of UAV-based object detection has become
increasingly significant in the fields of maritime industry and ocean
engineering. Endowed with intelligent sensing capabilities, the maritime UAVs
enable effective and efficient maritime surveillance. To further promote the
development of maritime UAV-based object detection, this paper provides a
comprehensive review of challenges, relative methods, and UAV aerial datasets.
Specifically, in this work, we first briefly summarize four challenges for
object detection on maritime UAVs, i.e., object feature diversity, device
limitation, maritime environment variability, and dataset scarcity. We then
focus on computational methods to improve maritime UAV-based object detection
performance in terms of scale-aware, small object detection, view-aware,
rotated object detection, lightweight methods, and others. Next, we review the
UAV aerial image/video datasets and propose a maritime UAV aerial dataset named
MS2ship for ship detection. Furthermore, we conduct a series of experiments to
present the performance evaluation and robustness analysis of object detection
methods on maritime datasets. Eventually, we give the discussion and outlook on
future works for maritime UAV-based object detection. The MS2ship dataset is
available at
\href{https://github.com/zcj234/MS2ship}{https://github.com/zcj234/MS2ship}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07978">How good are Large Language Models on African Languages?. (arXiv:2311.07978v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ojo_J/0/1/0/all/0/1">Jessica Ojo</a>, <a href="http://arxiv.org/find/cs/1/au:+Ogueji_K/0/1/0/all/0/1">Kelechi Ogueji</a>, <a href="http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1">Pontus Stenetorp</a>, <a href="http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1">David I. Adelani</a></p>
<p>Recent advancements in natural language processing have led to the
proliferation of large language models (LLMs). These models have been shown to
yield good performance, using in-context learning, even on unseen tasks and
languages. Additionally, they have been widely adopted as
language-model-as-a-service commercial APIs like GPT-4 API. However, their
performance on African languages is largely unknown. We present an analysis of
three popular large language models (mT0, LLaMa 2, and GPT-4) on five tasks
(news topic classification, sentiment classification, machine translation,
question answering, and named entity recognition) across 30 African languages,
spanning different language families and geographical regions. Our results
suggest that all LLMs produce below-par performance on African languages, and
there is a large gap in performance compared to high-resource languages like
English most tasks. We find that GPT-4 has an average or impressive performance
on classification tasks but very poor results on generative tasks like machine
translation. Surprisingly, we find that mT0 had the best overall on
cross-lingual QA, better than the state-of-the-art supervised model (i.e.
fine-tuned mT5) and GPT-4 on African languages. Overall, LLaMa 2 records the
worst performance due to its limited multilingual capabilities and
English-centric pre-training corpus. In general, our findings present a
call-to-action to ensure African languages are well represented in large
language models, given their growing popularity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07989">A Survey on Language Models for Code. (arXiv:2311.07989v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1">Ziyin Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1">Chaoyu Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1">Bingchang Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1">Cong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1">Zi Gong</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1">Hang Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jianguo Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1">Rui Wang</a></p>
<p>In this work we systematically review the recent advancements in code
processing with language models, covering 50+ models, 30+ evaluation tasks, and
500 related works. We break down code processing models into general language
models represented by the GPT family and specialized models that are
specifically pretrained on code, often with tailored objectives. We discuss the
relations and differences between these models, and highlight the historical
transition of code modeling from statistical models and RNNs to pretrained
Transformers and LLMs, which is exactly the same course that had been taken by
NLP. We also discuss code-specific features such as AST, CFG, and unit tests,
along with their application in training code language models, and identify key
challenges and potential future directions in this domain. We keep the survey
open and updated on github repository at
https://github.com/codefuse-ai/Awesome-Code-LLM.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07992">Probable Object Location (POLo) Score Estimation for Efficient Object Goal Navigation. (arXiv:2311.07992v1 [cs.RO])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1">Jiaming Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Soh_H/0/1/0/all/0/1">Harold Soh</a></p>
<p>To advance the field of autonomous robotics, particularly in object search
tasks within unexplored environments, we introduce a novel framework centered
around the Probable Object Location (POLo) score. Utilizing a 3D object
probability map, the POLo score allows the agent to make data-driven decisions
for efficient object search. We further enhance the framework's practicality by
introducing POLoNet, a neural network trained to approximate the
computationally intensive POLo score. Our approach addresses critical
limitations of both end-to-end reinforcement learning methods, which suffer
from memory decay over long-horizon tasks, and traditional map-based methods
that neglect visibility constraints. Our experiments, involving the first phase
of the OVMM 2023 challenge, demonstrate that an agent equipped with POLoNet
significantly outperforms a range of baseline methods, including end-to-end RL
techniques and prior map-based strategies. To provide a comprehensive
evaluation, we introduce new performance metrics that offer insights into the
efficiency and effectiveness of various agents in object goal navigation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08000">LiPar: A Lightweight Parallel Learning Model for Practical In-Vehicle Network Intrusion Detection. (arXiv:2311.08000v1 [cs.CR])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1">Aiheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1">Kai Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1">Bailing Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yulei Wu</a></p>
<p>With the development of intelligent transportation systems, vehicles are
exposed to a complex network environment. As the main network of in-vehicle
networks, the controller area network (CAN) has many potential security
hazards, resulting in higher requirements for intrusion detection systems to
ensure safety. Among intrusion detection technologies, methods based on deep
learning work best without prior expert knowledge. However, they all have a
large model size and rely on cloud computing, and are therefore not suitable to
be installed on the in-vehicle network. Therefore, we propose a lightweight
parallel neural network structure, LiPar, to allocate task loads to multiple
electronic control units (ECU). The LiPar model consists of multi-dimensional
branch convolution networks, spatial and temporal feature fusion learning, and
a resource adaptation algorithm. Through experiments, we prove that LiPar has
great detection performance, running efficiency, and lightweight model size,
which can be well adapted to the in-vehicle environment practically and protect
the in-vehicle CAN bus security.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08002">TempTabQA: Temporal Question Answering for Semi-Structured Tables. (arXiv:2311.08002v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1">Vivek Gupta</a>, <a href="http://arxiv.org/find/cs/1/au:+Kandoi_P/0/1/0/all/0/1">Pranshu Kandoi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vora_M/0/1/0/all/0/1">Mahek Bhavesh Vora</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shuo Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1">Yujie He</a>, <a href="http://arxiv.org/find/cs/1/au:+Reinanda_R/0/1/0/all/0/1">Ridho Reinanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1">Vivek Srikumar</a></p>
<p>Semi-structured data, such as Infobox tables, often include temporal
information about entities, either implicitly or explicitly. Can current NLP
systems reason about such information in semi-structured tables? To tackle this
question, we introduce the task of temporal question answering on
semi-structured tables. We present a dataset, TempTabQA, which comprises 11,454
question-answer pairs extracted from 1,208 Wikipedia Infobox tables spanning
more than 90 distinct domains. Using this dataset, we evaluate several
state-of-the-art models for temporal reasoning. We observe that even the
top-performing LLMs lag behind human performance by more than 13.5 F1 points.
Given these results, our dataset has the potential to serve as a challenging
benchmark to improve the temporal reasoning capabilities of NLP models.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08005">Iterative missing value imputation based on feature importance. (arXiv:2311.08005v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1">Cong Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1">Chun Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1">Wei Yang</a></p>
<p>Many datasets suffer from missing values due to various reasons,which not
only increases the processing difficulty of related tasks but also reduces the
accuracy of classification. To address this problem, the mainstream approach is
to use missing value imputation to complete the dataset. Existing imputation
methods estimate the missing parts based on the observed values in the original
feature space, and they treat all features as equally important during data
completion, while in fact different features have different importance.
Therefore, we have designed an imputation method that considers feature
importance. This algorithm iteratively performs matrix completion and feature
importance learning, and specifically, matrix completion is based on a filling
loss that incorporates feature importance. Our experimental analysis involves
three types of datasets: synthetic datasets with different noisy features and
missing values, real-world datasets with artificially generated missing values,
and real-world datasets originally containing missing values. The results on
these datasets consistently show that the proposed method outperforms the
existing five imputation algorithms.To the best of our knowledge, this is the
first work that considers feature importance in the imputation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08010">Distantly-Supervised Named Entity Recognition with Uncertainty-aware Teacher Learning and Student-student Collaborative Learning. (arXiv:2311.08010v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1">Helan Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1">Shuzheng Si</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1">Haozhe Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1">Shuang Zeng</a>, <a href="http://arxiv.org/find/cs/1/au:+An_K/0/1/0/all/0/1">Kaikai An</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1">Zefan Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1">Baobao Chang</a></p>
<p>Distantly-Supervised Named Entity Recognition (DS-NER) effectively alleviates
the burden of annotation, but meanwhile suffers from the label noise. Recent
works attempt to adopt the teacher-student framework to gradually refine the
training labels and improve the overall robustness. However, we argue that
these teacher-student methods achieve limited performance because poor network
calibration produces incorrectly pseudo-labeled samples, leading to error
propagation. Therefore, we attempt to mitigate this issue by proposing: (1)
Uncertainty-aware Teacher Learning that leverages the prediction uncertainty to
guide the selection of pseudo-labels, avoiding the number of incorrect
pseudo-labels in the self-training stage. (2) Student-student Collaborative
Learning that allows the transfer of reliable labels between two student
networks instead of completely relying on all pseudo-labels from its teacher.
Meanwhile, this approach allows a full exploration of mislabeled samples rather
than simply filtering unreliable pseudo-labeled samples. Extensive experimental
results on five DS-NER datasets demonstrate that our method is superior to
state-of-the-art teacher-student methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08022">Two-Stage Predict+Optimize for Mixed Integer Linear Programs with Unknown Parameters in Constraints. (arXiv:2311.08022v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1">Xinyi Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jasper C.H. Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jimmy H.M. Lee</a></p>
<p>Consider the setting of constrained optimization, with some parameters
unknown at solving time and requiring prediction from relevant features.
Predict+Optimize is a recent framework for end-to-end training supervised
learning models for such predictions, incorporating information about the
optimization problem in the training process in order to yield better
predictions in terms of the quality of the predicted solution under the true
parameters. Almost all prior works have focused on the special case where the
unknowns appear only in the optimization objective and not the constraints. Hu
et al.~proposed the first adaptation of Predict+Optimize to handle unknowns
appearing in constraints, but the framework has somewhat ad-hoc elements, and
they provided a training algorithm only for covering and packing linear
programs. In this work, we give a new \emph{simpler} and \emph{more powerful}
framework called \emph{Two-Stage Predict+Optimize}, which we believe should be
the canonical framework for the Predict+Optimize setting. We also give a
training algorithm usable for all mixed integer linear programs, vastly
generalizing the applicability of the framework. Experimental results
demonstrate the superior prediction performance of our training framework over
all classical and state-of-the-art methods.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08035">Data-driven building energy efficiency prediction based on envelope heat losses using physics-informed neural networks. (arXiv:2311.08035v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Michalakopoulos_V/0/1/0/all/0/1">Vasilis Michalakopoulos</a>, <a href="http://arxiv.org/find/cs/1/au:+Pelekis_S/0/1/0/all/0/1">Sotiris Pelekis</a>, <a href="http://arxiv.org/find/cs/1/au:+Kormpakis_G/0/1/0/all/0/1">Giorgos Kormpakis</a>, <a href="http://arxiv.org/find/cs/1/au:+Karakolis_V/0/1/0/all/0/1">Vagelis Karakolis</a>, <a href="http://arxiv.org/find/cs/1/au:+Mouzakitis_S/0/1/0/all/0/1">Spiros Mouzakitis</a>, <a href="http://arxiv.org/find/cs/1/au:+Askounis_D/0/1/0/all/0/1">Dimitris Askounis</a></p>
<p>The analytical prediction of building energy performance in residential
buildings based on the heat losses of its individual envelope components is a
challenging task. It is worth noting that this field is still in its infancy,
with relatively limited research conducted in this specific area to date,
especially when it comes for data-driven approaches. In this paper we introduce
a novel physics-informed neural network model for addressing this problem.
Through the employment of unexposed datasets that encompass general building
information, audited characteristics, and heating energy consumption, we feed
the deep learning model with general building information, while the model's
output consists of the structural components and several thermal properties
that are in fact the basic elements of an energy performance certificate (EPC).
On top of this neural network, a function, based on physics equations,
calculates the energy consumption of the building based on heat losses and
enhances the loss function of the deep learning model. This methodology is
tested on a real case study for 256 buildings located in Riga, Latvia. Our
investigation comes up with promising results in terms of prediction accuracy,
paving the way for automated, and data-driven energy efficiency performance
prediction based on basic properties of the building, contrary to exhaustive
energy efficiency audits led by humans, which are the current status quo.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08045">Adversarial Preference Optimization. (arXiv:2311.08045v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1">Pengyu Cheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yifan Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1">Jian Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yong Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1">Nan Du</a></p>
<p>Human preference alignment is a crucial training step to improve the
interaction quality of large language models (LLMs). Existing aligning methods
depend on manually annotated preference data to guide the LLM optimization
directions. However, in practice, continuously updating LLMs raises a
distribution gap between model-generated samples and human-preferred responses,
which hinders model fine-tuning efficiency. To mitigate this issue, previous
methods require additional preference annotation on generated samples to adapt
the shifted distribution, which consumes a large amount of annotation
resources. Targeting more efficient human preference optimization, we propose
an adversarial preference optimization (APO) framework, where the LLM agent and
the preference model update alternatively via a min-max game. Without
additional annotation, our APO method can make a self-adaption to the
generation distribution gap through the adversarial learning process. In
experiments, we empirically verify the effectiveness of APO in improving LLM's
helpfulness and harmlessness compared with rejection sampling baselines.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08077">Zero-Shot Segmentation of Eye Features Using the Segment Anything Model (SAM). (arXiv:2311.08077v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Maquiling_V/0/1/0/all/0/1">Virmarie Maquiling</a>, <a href="http://arxiv.org/find/cs/1/au:+Byrne_S/0/1/0/all/0/1">Sean Anthony Byrne</a>, <a href="http://arxiv.org/find/cs/1/au:+Niehorster_D/0/1/0/all/0/1">Diederick C. Niehorster</a>, <a href="http://arxiv.org/find/cs/1/au:+Nystrom_M/0/1/0/all/0/1">Marcus Nystr&#xf6;m</a>, <a href="http://arxiv.org/find/cs/1/au:+Kasneci_E/0/1/0/all/0/1">Enkelejda Kasneci</a></p>
<p>The advent of foundation models signals a new era in artificial intelligence.
The Segment Anything Model (SAM) is the first foundation model for image
segmentation. In this study, we evaluate SAM's ability to segment features from
eye images recorded in virtual reality setups. The increasing requirement for
annotated eye-image datasets presents a significant opportunity for SAM to
redefine the landscape of data annotation in gaze estimation. Our investigation
centers on SAM's zero-shot learning abilities and the effectiveness of prompts
like bounding boxes or point clicks. Our results are consistent with studies in
other domains, demonstrating that SAM's segmentation effectiveness can be
on-par with specialized models depending on the feature, with prompts improving
its performance, evidenced by an IoU of 93.34% for pupil segmentation in one
dataset. Foundation models like SAM could revolutionize gaze estimation by
enabling quick and easy image segmentation, reducing reliance on specialized
models and extensive manual annotation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08083">Solving ARC visual analogies with neural embeddings and vector arithmetic: A generalized method. (arXiv:2311.08083v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Thoms_L/0/1/0/all/0/1">Luca H. Thoms</a>, <a href="http://arxiv.org/find/cs/1/au:+Veldkamp_K/0/1/0/all/0/1">Karel A. Veldkamp</a>, <a href="http://arxiv.org/find/cs/1/au:+Rosenbusch_H/0/1/0/all/0/1">Hannes Rosenbusch</a>, <a href="http://arxiv.org/find/cs/1/au:+Stevenson_C/0/1/0/all/0/1">Claire E. Stevenson</a></p>
<p>Analogical reasoning derives information from known relations and generalizes
this information to similar yet unfamiliar situations. One of the first
generalized ways in which deep learning models were able to solve verbal
analogies was through vector arithmetic of word embeddings, essentially
relating words that were mapped to a vector space (e.g., king - man + woman =
__?). In comparison, most attempts to solve visual analogies are still
predominantly task-specific and less generalizable. This project focuses on
visual analogical reasoning and applies the initial generalized mechanism used
to solve verbal analogies to the visual realm. Taking the Abstraction and
Reasoning Corpus (ARC) as an example to investigate visual analogy solving, we
use a variational autoencoder (VAE) to transform ARC items into low-dimensional
latent vectors, analogous to the word embeddings used in the verbal approaches.
Through simple vector arithmetic, underlying rules of ARC items are discovered
and used to solve them. Results indicate that the approach works well on simple
items with fewer dimensions (i.e., few colors used, uniform shapes), similar
input-to-output examples, and high reconstruction accuracy on the VAE.
Predictions on more complex items showed stronger deviations from expected
outputs, although, predictions still often approximated parts of the item's
rule set. Error patterns indicated that the model works as intended. On the
official ARC paradigm, the model achieved a score of 2% (cf. current world
record is 21%) and on ConceptARC it scored 8.8%. Although the methodology
proposed involves basic dimensionality reduction techniques and standard vector
arithmetic, this approach demonstrates promising outcomes on ARC and can easily
be generalized to other abstract visual reasoning tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08086">CPSOR-GCN: A Vehicle Trajectory Prediction Method Powered by Emotion and Cognitive Theory. (arXiv:2311.08086v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1">L. Tang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Y. Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1">J. Yuan</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1">A. Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1">J. Sun</a></p>
<p>Active safety systems on vehicles often face problems with false alarms. Most
active safety systems predict the driver's trajectory with the assumption that
the driver is always in a normal emotion, and then infer risks. However, the
driver's trajectory uncertainty increases under abnormal emotions. This paper
proposes a new trajectory prediction model: CPSOR-GCN, which predicts vehicle
trajectories under abnormal emotions. At the physical level, the interaction
features between vehicles are extracted by the physical GCN module. At the
cognitive level, SOR cognitive theory is used as prior knowledge to build a
Dynamic Bayesian Network (DBN) structure. The conditional probability and state
transition probability of nodes from the calibrated SOR-DBN quantify the causal
relationship between cognitive factors, which is embedded into the cognitive
GCN module to extract the characteristics of the influence mechanism of
emotions on driving behavior. The CARLA-SUMO joint driving simulation platform
was built to develop dangerous pre-crash scenarios. Methods of recreating
traffic scenes were used to naturally induce abnormal emotions. The experiment
collected data from 26 participants to verify the proposed model. Compared with
the model that only considers physical motion features, the prediction accuracy
of the proposed model is increased by 68.70%. Furthermore,considering the
SOR-DBN reduces the prediction error of the trajectory by 15.93%. Compared with
other advanced trajectory prediction models, the results of CPSOR-GCN also have
lower errors. This model can be integrated into active safety systems to better
adapt to the driver's emotions, which could effectively reduce false alarms.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08093">Spot: A Natural Language Interface for Geospatial Searches in OSM. (arXiv:2311.08093v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Khellaf_L/0/1/0/all/0/1">Lynn Khellaf</a>, <a href="http://arxiv.org/find/cs/1/au:+Schlicht_I/0/1/0/all/0/1">Ipek Baris Schlicht</a>, <a href="http://arxiv.org/find/cs/1/au:+Bayer_J/0/1/0/all/0/1">Julia Bayer</a>, <a href="http://arxiv.org/find/cs/1/au:+Bouwmeester_R/0/1/0/all/0/1">Ruben Bouwmeester</a>, <a href="http://arxiv.org/find/cs/1/au:+Mirass_T/0/1/0/all/0/1">Tilman Mira&#xdf;</a>, <a href="http://arxiv.org/find/cs/1/au:+Wagner_T/0/1/0/all/0/1">Tilman Wagner</a></p>
<p>Investigative journalists and fact-checkers have found OpenStreetMap (OSM) to
be an invaluable resource for their work due to its extensive coverage and
intricate details of various locations, which play a crucial role in
investigating news scenes. Despite its value, OSM's complexity presents
considerable accessibility and usability challenges, especially for those
without a technical background. To address this, we introduce 'Spot', a
user-friendly natural language interface for querying OSM data. Spot utilizes a
semantic mapping from natural language to OSM tags, leveraging artificially
generated sentence queries and a T5 transformer. This approach enables Spot to
extract relevant information from user-input sentences and display candidate
locations matching the descriptions on a map. To foster collaboration and
future advancement, all code and generated data is available as an open-source
repository.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08094">Act-VIT: A Representationally Robust Attention Architecture for Skeleton Based Action Recognition Using Vision Transformer. (arXiv:2311.08094v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Karadag_O/0/1/0/all/0/1">Ozge Oztimur Karadag</a></p>
<p>Skeleton-based action recognition receives the attention of many researchers
as it is robust to viewpoint and illumination changes, and its processing is
much more efficient than video frames. With the emergence of deep learning
models, it has become very popular to represent the skeleton data in
pseudo-image form and apply Convolutional Neural Networks for action
recognition. Thereafter, studies concentrated on finding effective methods for
forming pseudo-images. Recently, attention networks, more specifically
transformers have provided promising results in various vision problems. In
this study, the effectiveness of vision transformers for skeleton-based action
recognition is examined and its robustness on the pseudo-image representation
scheme is investigated. To this end, a three-level architecture, Act-VIT is
proposed, which forms a set of pseudo images apply a classifier on each of the
representation and combine their results to find the final action class. The
classifiers of Act-VIT are first realized by CNNs and then by VITs and their
performances are compared. Experimental studies reveal that the vision
transformer is less sensitive to the initial pseudo-image representation
compared to CNN. Nevertheless, even with the vision transformer, the
recognition performance can be further improved by consensus of classifiers.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08097">Empowering Multi-step Reasoning across Languages via Tree-of-Thoughts. (arXiv:2311.08097v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ranaldi_L/0/1/0/all/0/1">Leonardo Ranaldi</a>, <a href="http://arxiv.org/find/cs/1/au:+Zanzotto_F/0/1/0/all/0/1">Fabio Massimo Zanzotto</a></p>
<p>Chain-of-Thought (CoT) prompting empowers the reasoning abilities of Large
Language Models (LLMs), eliciting them to solve complex reasoning tasks
step-by-step. However, with the success of CoT methods, the ability to deliver
multi-step reasoning remains limited to English due to the imbalance in the
distribution of the pre-training data, making the other languages a barrier.
</p>
<p>In this work, we propose a Cross-lingual multi-step reasoning approach,
aiming to align reasoning processes across different languages. In particular,
our method, through a Self-consistent Cross-lingual prompting mechanism
inspired by the Tree-of-Thoughts approach, delivers multi-step reasoning paths
in different languages that, during the steps, lead to the final solution. Our
experimental evaluations show that our method significantly outperforms
existing prompting methods, reducing the number of interactions and achieving
state-of-the-art performance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08103">Exploring Semi-supervised Hierarchical Stacked Encoder for Legal Judgement Prediction. (arXiv:2311.08103v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Prasad_N/0/1/0/all/0/1">Nishchal Prasad</a>, <a href="http://arxiv.org/find/cs/1/au:+Boughanem_M/0/1/0/all/0/1">Mohand Boughanem</a>, <a href="http://arxiv.org/find/cs/1/au:+Dkaki_T/0/1/0/all/0/1">Taoufiq Dkaki</a></p>
<p>Predicting the judgment of a legal case from its unannotated case facts is a
challenging task. The lengthy and non-uniform document structure poses an even
greater challenge in extracting information for decision prediction. In this
work, we explore and propose a two-level classification mechanism; both
supervised and unsupervised; by using domain-specific pre-trained BERT to
extract information from long documents in terms of sentence embeddings further
processing with transformer encoder layer and use unsupervised clustering to
extract hidden labels from these embeddings to better predict a judgment of a
legal case. We conduct several experiments with this mechanism and see higher
performance gains than the previously proposed methods on the ILDC dataset. Our
experimental results also show the importance of domain-specific pre-training
of Transformer Encoders in legal information processing.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08104">Reimagining Speech: A Scoping Review of Deep Learning-Powered Voice Conversion. (arXiv:2311.08104v1 [cs.SD])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bargum_A/0/1/0/all/0/1">Anders R. Bargum</a>, <a href="http://arxiv.org/find/cs/1/au:+Serafin_S/0/1/0/all/0/1">Stefania Serafin</a>, <a href="http://arxiv.org/find/cs/1/au:+Erkut_C/0/1/0/all/0/1">Cumhur Erkut</a></p>
<p>Research on deep learning-powered voice conversion (VC) in speech-to-speech
scenarios is getting increasingly popular. Although many of the works in the
field of voice conversion share a common global pipeline, there is a
considerable diversity in the underlying structures, methods, and neural
sub-blocks used across research efforts. Thus, obtaining a comprehensive
understanding of the reasons behind the choice of the different methods in the
voice conversion pipeline can be challenging, and the actual hurdles in the
proposed solutions are often unclear. To shed light on these aspects, this
paper presents a scoping review that explores the use of deep learning in
speech analysis, synthesis, and disentangled speech representation learning
within modern voice conversion systems. We screened 621 publications from more
than 38 different venues between the years 2017 and 2023, followed by an
in-depth review of a final database consisting of 123 eligible studies. Based
on the review, we summarise the most frequently used approaches to voice
conversion based on deep learning and highlight common pitfalls within the
community. Lastly, we condense the knowledge gathered, identify main challenges
and provide recommendations for future research directions.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08118">Evaluating Neighbor Explainability for Graph Neural Networks. (arXiv:2311.08118v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Llorente_O/0/1/0/all/0/1">Oscar Llorente</a>, <a href="http://arxiv.org/find/cs/1/au:+Vaderna_P/0/1/0/all/0/1">P&#xe9;ter Vaderna</a>, <a href="http://arxiv.org/find/cs/1/au:+Laki_S/0/1/0/all/0/1">S&#xe1;ndor Laki</a>, <a href="http://arxiv.org/find/cs/1/au:+Kotroczo_R/0/1/0/all/0/1">Roland Kotrocz&#xf3;</a>, <a href="http://arxiv.org/find/cs/1/au:+Csoma_R/0/1/0/all/0/1">Rita Csoma</a>, <a href="http://arxiv.org/find/cs/1/au:+Szalai_Gindl_J/0/1/0/all/0/1">J&#xe1;nos M&#xe1;rk Szalai-Gindl</a></p>
<p>Explainability in Graph Neural Networks (GNNs) is a new field growing in the
last few years. In this publication we address the problem of determining how
important is each neighbor for the GNN when classifying a node and how to
measure the performance for this specific task. To do this, various known
explainability methods are reformulated to get the neighbor importance and four
new metrics are presented. Our results show that there is almost no difference
between the explanations provided by gradient-based techniques in the GNN
domain. In addition, many explainability techniques failed to identify
important neighbors when GNNs without self-loops are used.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08120">Caring Trouble and Musical AI: Considerations towards a Feminist Musical AI. (arXiv:2311.08120v1 [cs.HC])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cotton_K/0/1/0/all/0/1">Kelsey Cotton</a>, <a href="http://arxiv.org/find/cs/1/au:+Tatar_K/0/1/0/all/0/1">K&#x131;van&#xe7; Tatar</a></p>
<p>The ethics of AI as both material and medium for interaction remains in murky
waters within the context of musical and artistic practice. The
interdisciplinarity of the field is revealing matters of concern and care,
which necessitate interdisciplinary methodologies for evaluation to trouble and
critique the inheritance of "residue-laden" AI-tools in musical applications.
Seeking to unsettle these murky waters, this paper critically examines the
example of Holly+, a deep neural network that generates raw audio in the
likeness of its creator Holly Herndon. Drawing from theoretical concerns and
considerations from speculative feminism and care ethics, we care-fully trouble
the structures, frameworks and assumptions that oscillate within and around
Holly+. We contribute with several considerations and contemplate future
directions for integrating speculative feminism and care into musical-AI agent
and system design, derived from our critical feminist examination.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08147">RECALL: A Benchmark for LLMs Robustness against External Counterfactual Knowledge. (arXiv:2311.08147v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yi Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1">Lianzhe Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Shicheng Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Sishuo Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1">Hao Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1">Xu Sun</a></p>
<p>LLMs and AI chatbots have improved people's efficiency in various fields.
However, the necessary knowledge for answering the question may be beyond the
models' knowledge boundaries. To mitigate this issue, many researchers try to
introduce external knowledge, such as knowledge graphs and Internet contents,
into LLMs for up-to-date information. However, the external information from
the Internet may include counterfactual information that will confuse the model
and lead to an incorrect response. Thus there is a pressing need for LLMs to
possess the ability to distinguish reliable information from external
knowledge. Therefore, to evaluate the ability of LLMs to discern the
reliability of external knowledge, we create a benchmark from existing
knowledge bases. Our benchmark consists of two tasks, Question Answering and
Text Generation, and for each task, we provide models with a context containing
counterfactual information. Evaluation results show that existing LLMs are
susceptible to interference from unreliable external knowledge with
counterfactual information, and simple intervention methods make limited
contributions to the alleviation of this issue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08148">Cattle Identification Using Muzzle Images and Deep Learning Techniques. (arXiv:2311.08148v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kimani_G/0/1/0/all/0/1">G. N. Kimani</a>, <a href="http://arxiv.org/find/cs/1/au:+Oluwadara_P/0/1/0/all/0/1">P. Oluwadara</a>, <a href="http://arxiv.org/find/cs/1/au:+Fashingabo_P/0/1/0/all/0/1">P. Fashingabo</a>, <a href="http://arxiv.org/find/cs/1/au:+Busogi_M/0/1/0/all/0/1">M. Busogi</a>, <a href="http://arxiv.org/find/cs/1/au:+Luhanga_E/0/1/0/all/0/1">E. Luhanga</a>, <a href="http://arxiv.org/find/cs/1/au:+Sowon_K/0/1/0/all/0/1">K. Sowon</a>, <a href="http://arxiv.org/find/cs/1/au:+Chacha_L/0/1/0/all/0/1">L. Chacha</a> ((1) CyLab-Africa / Upanzi Network, (2) Carnegie Mellon University Africa and (3) Carnegie Mellon University Pittsburgh)</p>
<p>Traditional animal identification methods such as ear-tagging, ear notching,
and branding have been effective but pose risks to the animal and have
scalability issues. Electrical methods offer better tracking and monitoring but
require specialized equipment and are susceptible to attacks. Biometric
identification using time-immutable dermatoglyphic features such as muzzle
prints and iris patterns is a promising solution. This project explores cattle
identification using 4923 muzzle images collected from 268 beef cattle. Two
deep learning classification models are implemented - wide ResNet50 and
VGG16\_BN and image compression is done to lower the image quality and adapt
the models to work for the African context. From the experiments run, a maximum
accuracy of 99.5\% is achieved while using the wide ResNet50 model with a
compression retaining 25\% of the original image. From the study, it is noted
that the time required by the models to train and converge as well as
recognition time are dependent on the machine used to run the model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08150">The Hyperdimensional Transform for Distributional Modelling, Regression and Classification. (arXiv:2311.08150v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Dewulf_P/0/1/0/all/0/1">Pieter Dewulf</a>, <a href="http://arxiv.org/find/cs/1/au:+Baets_B/0/1/0/all/0/1">Bernard De Baets</a>, <a href="http://arxiv.org/find/cs/1/au:+Stock_M/0/1/0/all/0/1">Michiel Stock</a></p>
<p>Hyperdimensional computing (HDC) is an increasingly popular computing
paradigm with immense potential for future intelligent applications. Although
the main ideas already took form in the 1990s, HDC recently gained significant
attention, especially in the field of machine learning and data science. Next
to efficiency, interoperability and explainability, HDC offers attractive
properties for generalization as it can be seen as an attempt to combine
connectionist ideas from neural networks with symbolic aspects. In recent work,
we introduced the hyperdimensional transform, revealing deep theoretical
foundations for representing functions and distributions as high-dimensional
holographic vectors. Here, we present the power of the hyperdimensional
transform to a broad data science audience. We use the hyperdimensional
transform as a theoretical basis and provide insight into state-of-the-art HDC
approaches for machine learning. We show how existing algorithms can be
modified and how this transform can lead to a novel, well-founded toolbox. Next
to the standard regression and classification tasks of machine learning, our
discussion includes various aspects of statistical modelling, such as
representation, learning and deconvolving distributions, sampling, Bayesian
inference, and uncertainty estimation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08153">When Mining Electric Locomotives Meet Reinforcement Learning. (arXiv:2311.08153v1 [eess.SY])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1">Ying Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Zhu_Z/0/1/0/all/0/1">Zhencai Zhu</a>, <a href="http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1">Xiaoqiang Li</a>, <a href="http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1">Chunyu Yang</a>, <a href="http://arxiv.org/find/eess/1/au:+Lu_H/0/1/0/all/0/1">Hao Lu</a></p>
<p>As the most important auxiliary transportation equipment in coal mines,
mining electric locomotives are mostly operated manually at present. However,
due to the complex and ever-changing coal mine environment, electric locomotive
safety accidents occur frequently these years. A mining electric locomotive
control method that can adapt to different complex mining environments is
needed. Reinforcement Learning (RL) is concerned with how artificial agents
ought to take actions in an environment so as to maximize reward, which can
help achieve automatic control of mining electric locomotive. In this paper, we
present how to apply RL to the autonomous control of mining electric
locomotives. To achieve more precise control, we further propose an improved
epsilon-greedy (IEG) algorithm which can better balance the exploration and
exploitation. To verify the effectiveness of this method, a co-simulation
platform for autonomous control of mining electric locomotives is built which
can complete closed-loop simulation of the vehicles. The simulation results
show that this method ensures the locomotives following the front vehicle
safely and responding promptly in the event of sudden obstacles on the road
when the vehicle in complex and uncertain coal mine environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08154">Ask One More Time: Self-Agreement Improves Reasoning of Language Models in (Almost) All Scenarios. (arXiv:2311.08154v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1">Lei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1">Jiayi Fu</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1">Pengli Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1">Junchen Wan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1">Fuzheng Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhongyuan Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Di Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gai_K/0/1/0/all/0/1">Kun Gai</a></p>
<p>Although chain-of-thought (CoT) prompting combined with language models has
achieved encouraging results on complex reasoning tasks, the naive greedy
decoding used in CoT prompting usually causes the repetitiveness and local
optimality. To address this shortcoming, ensemble-optimization tries to obtain
multiple reasoning paths to get the final answer assembly. However, current
ensemble-optimization methods either simply employ rule-based post-processing
such as \textit{self-consistency}, or train an additional model based on
several task-related human annotations to select the best one among multiple
reasoning paths, yet fail to generalize to realistic settings where the type of
input questions is unknown or the answer format of reasoning paths is unknown.
To avoid their limitations, we propose \textbf{self-agreement}, a generalizable
ensemble-optimization method applying in almost all scenarios where the type of
input questions and the answer format of reasoning paths may be known or
unknown. Self-agreement firstly samples from language model's decoder to
generate a \textit{diverse} set of reasoning paths, and subsequently prompts
the language model \textit{one more time} to determine the optimal answer by
selecting the most \textit{agreed} answer among the sampled reasoning paths.
Self-agreement simultaneously achieves remarkable performance on six public
reasoning benchmarks and superior generalization capabilities.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08157">TransformCode: A Contrastive Learning Framework for Code Embedding via Subtree transformation. (arXiv:2311.08157v1 [cs.SE])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1">Zixiang Xian</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1">Rubing Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Towey_D/0/1/0/all/0/1">Dave Towey</a>, <a href="http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1">Chunrong Fang</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1">Zhenyu Chen</a></p>
<p>Large-scale language models have made great progress in the field of software
engineering in recent years. They can be used for many code-related tasks such
as code clone detection, code-to-code search, and method name prediction.
However, these large-scale language models based on each code token have
several drawbacks: They are usually large in scale, heavily dependent on
labels, and require a lot of computing power and time to fine-tune new
datasets.Furthermore, code embedding should be performed on the entire code
snippet rather than encoding each code token. The main reason for this is that
encoding each code token would cause model parameter inflation, resulting in a
lot of parameters storing information that we are not very concerned about. In
this paper, we propose a novel framework, called TransformCode, that learns
about code embeddings in a contrastive learning manner. The framework uses the
Transformer encoder as an integral part of the model. We also introduce a novel
data augmentation technique called abstract syntax tree transformation: This
technique applies syntactic and semantic transformations to the original code
snippets to generate more diverse and robust anchor samples. Our proposed
framework is both flexible and adaptable: It can be easily extended to other
downstream tasks that require code representation such as code clone detection
and classification. The framework is also very efficient and scalable: It does
not require a large model or a large amount of training data, and can support
any programming language.Finally, our framework is not limited to unsupervised
learning, but can also be applied to some supervised learning tasks by
incorporating task-specific labels or objectives. To explore the effectiveness
of our framework, we conducted extensive experiments on different software
engineering tasks using different programming languages and multiple datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08166">MechAgents: Large language model multi-agent collaborations can solve mechanics problems, generate new data, and integrate knowledge. (arXiv:2311.08166v1 [cs.AI])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Ni_B/0/1/0/all/0/1">Bo Ni</a>, <a href="http://arxiv.org/find/cs/1/au:+Buehler_M/0/1/0/all/0/1">Markus J. Buehler</a></p>
<p>Solving mechanics problems using numerical methods requires comprehensive
intelligent capability of retrieving relevant knowledge and theory,
constructing and executing codes, analyzing the results, a task that has thus
far mainly been reserved for humans. While emerging AI methods can provide
effective approaches to solve end-to-end problems, for instance via the use of
deep surrogate models or various data analytics strategies, they often lack
physical intuition since knowledge is baked into the parametric complement
through training, offering less flexibility when it comes to incorporating
mathematical or physical insights. By leveraging diverse capabilities of
multiple dynamically interacting large language models (LLMs), we can overcome
the limitations of conventional approaches and develop a new class of
physics-inspired generative machine learning platform, here referred to as
MechAgents. A set of AI agents can solve mechanics tasks, here demonstrated for
elasticity problems, via autonomous collaborations. A two-agent team can
effectively write, execute and self-correct code, in order to apply finite
element methods to solve classical elasticity problems in various flavors
(different boundary conditions, domain geometries, meshes, small/finite
deformation and linear/hyper-elastic constitutive laws, and others). For more
complex tasks, we construct a larger group of agents with enhanced division of
labor among planning, formulating, coding, executing and criticizing the
process and results. The agents mutually correct each other to improve the
overall team-work performance in understanding, formulating and validating the
solution. Our framework shows the potential of synergizing the intelligence of
language models, the reliability of physics-based modeling, and the dynamic
collaborations among diverse agents, opening novel avenues for automation of
solving engineering problems.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08170">Neural Lattice Reduction: A Self-Supervised Geometric Deep Learning Approach. (arXiv:2311.08170v1 [cs.LG])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Marchetti_G/0/1/0/all/0/1">Giovanni Luca Marchetti</a>, <a href="http://arxiv.org/find/cs/1/au:+Cesa_G/0/1/0/all/0/1">Gabriele Cesa</a>, <a href="http://arxiv.org/find/cs/1/au:+Pratik_K/0/1/0/all/0/1">Kumar Pratik</a>, <a href="http://arxiv.org/find/cs/1/au:+Behboodi_A/0/1/0/all/0/1">Arash Behboodi</a></p>
<p>Lattice reduction is a combinatorial optimization problem aimed at finding
the most orthogonal basis in a given lattice. In this work, we address lattice
reduction via deep learning methods. We design a deep neural model outputting
factorized unimodular matrices and train it in a self-supervised manner by
penalizing non-orthogonal lattice bases. We incorporate the symmetries of
lattice reduction into the model by making it invariant and equivariant with
respect to appropriate continuous and discrete groups.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08179">Semi-Supervised Learning via Swapped Prediction for Communication Signal Recognition. (arXiv:2311.08179v1 [eess.SP])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Wang_W/0/1/0/all/0/1">Weidong Wang</a>, <a href="http://arxiv.org/find/eess/1/au:+Liao_H/0/1/0/all/0/1">Hongshu Liao</a>, <a href="http://arxiv.org/find/eess/1/au:+Gan_L/0/1/0/all/0/1">Lu Gan</a></p>
<p>Deep neural networks have been widely used in communication signal
recognition and achieved remarkable performance, but this superiority typically
depends on using massive examples for supervised learning, whereas training a
deep neural network on small datasets with few labels generally falls into
overfitting, resulting in degenerated performance. To this end, we develop a
semi-supervised learning (SSL) method that effectively utilizes a large
collection of more readily available unlabeled signal data to improve
generalization. The proposed method relies largely on a novel implementation of
consistency-based regularization, termed Swapped Prediction, which leverages
strong data augmentation to perturb an unlabeled sample and then encourage its
corresponding model prediction to be close to its original, optimized with a
scaled cross-entropy loss with swapped symmetry. Extensive experiments indicate
that our proposed method can achieve a promising result for deep SSL of
communication signal recognition.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08195">Automated Fact-Checking in Dialogue: Are Specialized Models Needed?. (arXiv:2311.08195v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chamoun_E/0/1/0/all/0/1">Eric Chamoun</a>, <a href="http://arxiv.org/find/cs/1/au:+Saeidi_M/0/1/0/all/0/1">Marzieh Saeidi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1">Andreas Vlachos</a></p>
<p>Prior research has shown that typical fact-checking models for stand-alone
claims struggle with claims made in dialogues. As a solution, fine-tuning these
models on labelled dialogue data has been proposed. However, creating separate
models for each use case is impractical, and we show that fine-tuning models
for dialogue results in poor performance on typical fact-checking. To overcome
this challenge, we present techniques that allow us to use the same models for
both dialogue and typical fact-checking. These mainly focus on retrieval
adaptation and transforming conversational inputs so that they can be
accurately predicted by models trained on stand-alone claims. We demonstrate
that a typical fact-checking model incorporating these techniques is
competitive with state-of-the-art models fine-tuned for dialogue, while
maintaining its accuracy on stand-alone claims.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08206">Human-Centric Autonomous Systems With LLMs for User Command Reasoning. (arXiv:2311.08206v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1">Yi Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1">Qingwen Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1">Ci Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Marta_D/0/1/0/all/0/1">Daniel Sim&#xf5;es Marta</a>, <a href="http://arxiv.org/find/cs/1/au:+Batool_N/0/1/0/all/0/1">Nazre Batool</a>, <a href="http://arxiv.org/find/cs/1/au:+Folkesson_J/0/1/0/all/0/1">John Folkesson</a></p>
<p>The evolution of autonomous driving has made remarkable advancements in
recent years, evolving into a tangible reality. However, a human-centric
large-scale adoption hinges on meeting a variety of multifaceted requirements.
To ensure that the autonomous system meets the user's intent, it is essential
to accurately discern and interpret user commands, especially in complex or
emergency situations. To this end, we propose to leverage the reasoning
capabilities of Large Language Models (LLMs) to infer system requirements from
in-cabin users' commands. Through a series of experiments that include
different LLM models and prompt designs, we explore the few-shot multivariate
binary classification accuracy of system requirements from natural language
textual commands. We confirm the general ability of LLMs to understand and
reason about prompts but underline that their effectiveness is conditioned on
the quality of both the LLM model and the design of appropriate sequential
prompts. Code and models are public with the link
\url{https://github.com/KTH-RPL/DriveCmd_LLM}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08219">Eval-GCSC: A New Metric for Evaluating ChatGPT&#x27;s Performance in Chinese Spelling Correction. (arXiv:2311.08219v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1">Kunting Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1">Yong Hu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1">Shaolei Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1">Hanhan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1">Liang He</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>ChatGPT has demonstrated impressive performance in various downstream tasks.
However, in the Chinese Spelling Correction (CSC) task, we observe a
discrepancy: while ChatGPT performs well under human evaluation, it scores
poorly according to traditional metrics. We believe this inconsistency arises
because the traditional metrics are not well-suited for evaluating generative
models. Their overly strict length and phonics constraints may lead to
underestimating ChatGPT's correction capabilities. To better evaluate
generative models in the CSC task, this paper proposes a new evaluation metric:
Eval-GCSC. By incorporating word-level and semantic similarity judgments, it
relaxes the stringent length and phonics constraints. Experimental results show
that Eval-GCSC closely aligns with human evaluations. Under this metric,
ChatGPT's performance is comparable to traditional token-level classification
models (TCM), demonstrating its potential as a CSC tool. The source code and
scripts can be accessed at https://github.com/ktlKTL/Eval-GCSC.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08239">Learning Physics-Inspired Regularization for Medical Image Registration with Hypernetworks. (arXiv:2311.08239v1 [eess.IV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/eess/1/au:+Reithmeir_A/0/1/0/all/0/1">Anna Reithmeir</a>, <a href="http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1">Julia A. Schnabel</a>, <a href="http://arxiv.org/find/eess/1/au:+Zimmer_V/0/1/0/all/0/1">Veronika A. Zimmer</a></p>
<p>Medical image registration aims at identifying the spatial deformation
between images of the same anatomical region and is fundamental to image-based
diagnostics and therapy. To date, the majority of the deep learning-based
registration methods employ regularizers that enforce global spatial
smoothness, e.g., the diffusion regularizer. However, such regularizers are not
tailored to the data and might not be capable of reflecting the complex
underlying deformation. In contrast, physics-inspired regularizers promote
physically plausible deformations. One such regularizer is the linear elastic
regularizer which models the deformation of elastic material. These
regularizers are driven by parameters that define the material's physical
properties. For biological tissue, a wide range of estimations of such
parameters can be found in the literature and it remains an open challenge to
identify suitable parameter values for successful registration. To overcome
this problem and to incorporate physical properties into learning-based
registration, we propose to use a hypernetwork that learns the effect of the
physical parameters of a physics-inspired regularizer on the resulting spatial
deformation field. In particular, we adapt the HyperMorph framework to learn
the effect of the two elasticity parameters of the linear elastic regularizer.
Our approach enables the efficient discovery of suitable, data-specific
physical parameters at test time.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08240">Investigating the Encoding of Words in BERT&#x27;s Neurons using Feature Textualization. (arXiv:2311.08240v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Baeumel_T/0/1/0/all/0/1">Tanja Baeumel</a>, <a href="http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1">Soniya Vijayakumar</a>, <a href="http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1">Josef van Genabith</a>, <a href="http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1">Guenter Neumann</a>, <a href="http://arxiv.org/find/cs/1/au:+Ostermann_S/0/1/0/all/0/1">Simon Ostermann</a></p>
<p>Pretrained language models (PLMs) form the basis of most state-of-the-art NLP
technologies. Nevertheless, they are essentially black boxes: Humans do not
have a clear understanding of what knowledge is encoded in different parts of
the models, especially in individual neurons. The situation is different in
computer vision, where feature visualization provides a decompositional
interpretability technique for neurons of vision models. Activation
maximization is used to synthesize inherently interpretable visual
representations of the information encoded in individual neurons. Our work is
inspired by this but presents a cautionary tale on the interpretability of
single neurons, based on the first large-scale attempt to adapt activation
maximization to NLP, and, more specifically, large PLMs. We propose feature
textualization, a technique to produce dense representations of neurons in the
PLM word embedding space. We apply feature textualization to the BERT model
(Devlin et al., 2019) to investigate whether the knowledge encoded in
individual neurons can be interpreted and symbolized. We find that the produced
representations can provide insights about the knowledge encoded in individual
neurons, but that individual neurons do not represent clearcut symbolic units
of language such as words. Additionally, we use feature textualization to
investigate how many neurons are needed to encode words in BERT.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08252">REST: Retrieval-Based Speculative Decoding. (arXiv:2311.08252v1 [cs.CL])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1">Zhenyu He</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1">Zexuan Zhong</a>, <a href="http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1">Tianle Cai</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1">Jason D Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1">Di He</a></p>
<p>We introduce Retrieval-Based Speculative Decoding (REST), a novel algorithm
designed to speed up language model generation. The key insight driving the
development of REST is the observation that the process of text generation
often includes certain common phases and patterns. Unlike previous methods that
rely on a draft language model for speculative decoding, REST harnesses the
power of retrieval to generate draft tokens. This method draws from the
reservoir of existing knowledge, retrieving and employing relevant tokens based
on the current context. Its plug-and-play nature allows for seamless
integration and acceleration of any language models, all without necessitating
additional training. When benchmarked on 7B and 13B language models in a
single-batch setting, REST achieves a significant speedup of 1.62X to 2.36X on
code or text generation. The code of REST is available at
https://github.com/FasterDecoding/REST.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.08265">On The Relationship Between Universal Adversarial Attacks And Sparse Representations. (arXiv:2311.08265v1 [cs.CV])</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Weitzner_D/0/1/0/all/0/1">Dana Weitzner</a>, <a href="http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1">Raja Giryes</a></p>
<p>The prominent success of neural networks, mainly in computer vision tasks, is
increasingly shadowed by their sensitivity to small, barely perceivable
adversarial perturbations in image input.
</p>
<p>In this work, we aim at explaining this vulnerability through the framework
of sparsity.
</p>
<p>We show the connection between adversarial attacks and sparse
representations, with a focus on explaining the universality and
transferability of adversarial examples in neural networks.
</p>
<p>To this end, we show that sparse coding algorithms, and the neural
network-based learned iterative shrinkage thresholding algorithm (LISTA) among
them, suffer from this sensitivity, and that common attacks on neural networks
can be expressed as attacks on the sparse representation of the input image.
The phenomenon that we observe holds true also when the network is agnostic to
the sparse representation and dictionary, and thus can provide a possible
explanation for the universality and transferability of adversarial attacks.
</p>
<p>The code is available at
https://github.com/danawr/adversarial_attacks_and_sparse_representations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2106.06854">A Deep Reinforcement Learning Approach to Marginalized Importance Sampling with the Successor Representation. (arXiv:2106.06854v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Fujimoto_S/0/1/0/all/0/1">Scott Fujimoto</a>, <a href="http://arxiv.org/find/cs/1/au:+Meger_D/0/1/0/all/0/1">David Meger</a>, <a href="http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1">Doina Precup</a></p>
<p>Marginalized importance sampling (MIS), which measures the density ratio
between the state-action occupancy of a target policy and that of a sampling
distribution, is a promising approach for off-policy evaluation. However,
current state-of-the-art MIS methods rely on complex optimization tricks and
succeed mostly on simple toy problems. We bridge the gap between MIS and deep
reinforcement learning by observing that the density ratio can be computed from
the successor representation of the target policy. The successor representation
can be trained through deep reinforcement learning methodology and decouples
the reward optimization from the dynamics of the environment, making the
resulting algorithm stable and applicable to high-dimensional domains. We
evaluate the empirical performance of our approach on a variety of challenging
Atari and MuJoCo environments.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.10859">Visualizing the Diversity of Representations Learned by Bayesian Neural Networks. (arXiv:2201.10859v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1">Dennis Grinwald</a>, <a href="http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1">Kirill Bykov</a>, <a href="http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1">Shinichi Nakajima</a>, <a href="http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1">Marina M.-C. H&#xf6;hne</a></p>
<p>Explainable Artificial Intelligence (XAI) aims to make learning machines less
opaque, and offers researchers and practitioners various tools to reveal the
decision-making strategies of neural networks. In this work, we investigate how
XAI methods can be used for exploring and visualizing the diversity of feature
representations learned by Bayesian Neural Networks (BNNs). Our goal is to
provide a global understanding of BNNs by making their decision-making
strategies a) visible and tangible through feature visualizations and b)
quantitatively measurable with a distance measure learned by contrastive
learning. Our work provides new insights into the \emph{posterior} distribution
in terms of human-understandable feature information with regard to the
underlying decision making strategies. The main findings of our work are the
following: 1) global XAI methods can be applied to explain the diversity of
decision-making strategies of BNN instances, 2) Monte Carlo dropout with
commonly used Dropout rates exhibit increased diversity in feature
representations compared to the multimodal posterior approximation of
MultiSWAG, 3) the diversity of learned feature representations highly
correlates with the uncertainty estimate for the output and 4) the inter-mode
diversity of the multimodal posterior decreases as the network width increases,
while the intra mode diversity increases. These findings are consistent with
the recent Deep Neural Networks theory, providing additional intuitions about
what the theory implies in terms of humanly understandable concepts.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2201.11239">Diagnosing AI Explanation Methods with Folk Concepts of Behavior. (arXiv:2201.11239v5 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jacovi_A/0/1/0/all/0/1">Alon Jacovi</a>, <a href="http://arxiv.org/find/cs/1/au:+Bastings_J/0/1/0/all/0/1">Jasmijn Bastings</a>, <a href="http://arxiv.org/find/cs/1/au:+Gehrmann_S/0/1/0/all/0/1">Sebastian Gehrmann</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldberg_Y/0/1/0/all/0/1">Yoav Goldberg</a>, <a href="http://arxiv.org/find/cs/1/au:+Filippova_K/0/1/0/all/0/1">Katja Filippova</a></p>
<p>We investigate a formalism for the conditions of a successful explanation of
AI. We consider "success" to depend not only on what information the
explanation contains, but also on what information the human explainee
understands from it. Theory of mind literature discusses the folk concepts that
humans use to understand and generalize behavior. We posit that folk concepts
of behavior provide us with a "language" that humans understand behavior with.
We use these folk concepts as a framework of social attribution by the human
explainee - the information constructs that humans are likely to comprehend
from explanations - by introducing a blueprint for an explanatory narrative
(Figure 1) that explains AI behavior with these constructs. We then demonstrate
that many XAI methods today can be mapped to folk concepts of behavior in a
qualitative evaluation. This allows us to uncover their failure modes that
prevent current methods from explaining successfully - i.e., the information
constructs that are missing for any given XAI method, and whose inclusion can
decrease the likelihood of misunderstanding AI behavior.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03461">FastCLIPstyler: Optimisation-free Text-based Image Style Transfer Using Style Representations. (arXiv:2210.03461v4 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1">Ananda Padhmanabhan Suresh</a>, <a href="http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1">Sanjana Jain</a>, <a href="http://arxiv.org/find/cs/1/au:+Noinongyao_P/0/1/0/all/0/1">Pavit Noinongyao</a>, <a href="http://arxiv.org/find/cs/1/au:+Ganguly_A/0/1/0/all/0/1">Ankush Ganguly</a>, <a href="http://arxiv.org/find/cs/1/au:+Watchareeruetai_U/0/1/0/all/0/1">Ukrit Watchareeruetai</a>, <a href="http://arxiv.org/find/cs/1/au:+Samacoits_A/0/1/0/all/0/1">Aubin Samacoits</a></p>
<p>In recent years, language-driven artistic style transfer has emerged as a new
type of style transfer technique, eliminating the need for a reference style
image by using natural language descriptions of the style. The first model to
achieve this, called CLIPstyler, has demonstrated impressive stylisation
results. However, its lengthy optimisation procedure at runtime for each query
limits its suitability for many practical applications. In this work, we
present FastCLIPstyler, a generalised text-based image style transfer model
capable of stylising images in a single forward pass for arbitrary text inputs.
Furthermore, we introduce EdgeCLIPstyler, a lightweight model designed for
compatibility with resource-constrained devices. Through quantitative and
qualitative comparisons with state-of-the-art approaches, we demonstrate that
our models achieve superior stylisation quality based on measurable metrics
while offering significantly improved runtime efficiency, particularly on edge
devices.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.03475">Winner Takes It All: Training Performant RL Populations for Combinatorial Optimization. (arXiv:2210.03475v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Grinsztajn_N/0/1/0/all/0/1">Nathan Grinsztajn</a>, <a href="http://arxiv.org/find/cs/1/au:+Furelos_Blanco_D/0/1/0/all/0/1">Daniel Furelos-Blanco</a>, <a href="http://arxiv.org/find/cs/1/au:+Surana_S/0/1/0/all/0/1">Shikha Surana</a>, <a href="http://arxiv.org/find/cs/1/au:+Bonnet_C/0/1/0/all/0/1">Cl&#xe9;ment Bonnet</a>, <a href="http://arxiv.org/find/cs/1/au:+Barrett_T/0/1/0/all/0/1">Thomas D. Barrett</a></p>
<p>Applying reinforcement learning (RL) to combinatorial optimization problems
is attractive as it removes the need for expert knowledge or pre-solved
instances. However, it is unrealistic to expect an agent to solve these (often
NP-)hard problems in a single shot at inference due to their inherent
complexity. Thus, leading approaches often implement additional search
strategies, from stochastic sampling and beam search to explicit fine-tuning.
In this paper, we argue for the benefits of learning a population of
complementary policies, which can be simultaneously rolled out at inference. To
this end, we introduce Poppy, a simple training procedure for populations.
Instead of relying on a predefined or hand-crafted notion of diversity, Poppy
induces an unsupervised specialization targeted solely at maximizing the
performance of the population. We show that Poppy produces a set of
complementary policies, and obtains state-of-the-art RL results on four popular
NP-hard problems: traveling salesman, capacitated vehicle routing, 0-1
knapsack, and job-shop scheduling.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2210.07410">Identification of quantum entanglement with Siamese convolutional neural networks and semi-supervised learning. (arXiv:2210.07410v3 [quant-ph] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/quant-ph/1/au:+Pawlowski_J/0/1/0/all/0/1">Jaros&#x142;aw Paw&#x142;owski</a>, <a href="http://arxiv.org/find/quant-ph/1/au:+Krawczyk_M/0/1/0/all/0/1">Mateusz Krawczyk</a></p>
<p>Quantum entanglement is a fundamental property commonly used in various
quantum information protocols and algorithms. Nonetheless, the problem of
identifying entanglement has still not reached a general solution for systems
larger than two qubits. In this study, we use deep convolutional neural
networks, a type of supervised machine learning, to identify quantum
entanglement for any bipartition in a 3-qubit system. We demonstrate that
training the model on synthetically generated datasets of random density
matrices excluding challenging positive-under-partial-transposition entangled
states (PPTES), which cannot be identified (and correctly labeled) in general,
leads to good model accuracy even for PPTES states, that were outside the
training data. Our aim is to enhance the model's generalization on PPTES. By
applying entanglement-preserving symmetry operations through a triple Siamese
network trained in a semi-supervised manner, we improve the model's accuracy
and ability to recognize PPTES. Moreover, by constructing an ensemble of
Siamese models, even better generalization is observed, in analogy with the
idea of finding separate types of entanglement witnesses for different classes
of states. The neural models' code and training schemes, as well as data
generation procedures, are available at
github.com/Maticraft/quantum_correlations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.08253">HMOE: Hypernetwork-based Mixture of Experts for Domain Generalization. (arXiv:2211.08253v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1">Jingang Qu</a>, <a href="http://arxiv.org/find/cs/1/au:+Faney_T/0/1/0/all/0/1">Thibault Faney</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Ze Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Gallinari_P/0/1/0/all/0/1">Patrick Gallinari</a>, <a href="http://arxiv.org/find/cs/1/au:+Yousef_S/0/1/0/all/0/1">Soleiman Yousef</a>, <a href="http://arxiv.org/find/cs/1/au:+Hemptinne_J/0/1/0/all/0/1">Jean-Charles de Hemptinne</a></p>
<p>Due to domain shifts, machine learning systems typically struggle to
generalize well to new domains that differ from those of training data, which
is what domain generalization (DG) aims to address. Although a variety of DG
methods have been proposed, most of them fall short in interpretability and
require domain labels, which are not available in many real-world scenarios.
This paper presents a novel DG method, called HMOE: Hypernetwork-based Mixture
of Experts (MoE), which does not rely on domain labels and is more
interpretable. MoE proves effective in identifying heterogeneous patterns in
data. For the DG problem, heterogeneity arises exactly from domain shifts. HMOE
employs hypernetworks taking vectors as input to generate the weights of
experts, which promotes knowledge sharing among experts and enables the
exploration of their similarities in a low-dimensional vector space. We
benchmark HMOE against other DG methods under a fair evaluation framework --
DomainBed. Our extensive experiments show that HMOE can effectively separate
mixed-domain data into distinct clusters that are surprisingly more consistent
with human intuition than original domain labels. Using self-learned domain
information, HMOE achieves state-of-the-art results on most datasets and
significantly surpasses other DG methods in average accuracy across all
datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.10627">EGRC-Net: Embedding-induced Graph Refinement Clustering Network. (arXiv:2211.10627v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1">Zhihao Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1">Hui Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1">Yuheng Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1">Junhui Hou</a></p>
<p>Existing graph clustering networks heavily rely on a predefined yet fixed
graph, which can lead to failures when the initial graph fails to accurately
capture the data topology structure of the embedding space. In order to address
this issue, we propose a novel clustering network called Embedding-Induced
Graph Refinement Clustering Network (EGRC-Net), which effectively utilizes the
learned embedding to adaptively refine the initial graph and enhance the
clustering performance. To begin, we leverage both semantic and topological
information by employing a vanilla auto-encoder and a graph convolution
network, respectively, to learn a latent feature representation. Subsequently,
we utilize the local geometric structure within the feature embedding space to
construct an adjacency matrix for the graph. This adjacency matrix is
dynamically fused with the initial one using our proposed fusion architecture.
To train the network in an unsupervised manner, we minimize the Jeffreys
divergence between multiple derived distributions. Additionally, we introduce
an improved approximate personalized propagation of neural predictions to
replace the standard graph convolution network, enabling EGRC-Net to scale
effectively. Through extensive experiments conducted on nine widely-used
benchmark datasets, we demonstrate that our proposed methods consistently
outperform several state-of-the-art approaches. Notably, EGRC-Net achieves an
improvement of more than 11.99\% in Adjusted Rand Index (ARI) over the best
baseline on the DBLP dataset. Furthermore, our scalable approach exhibits a
10.73% gain in ARI while reducing memory usage by 33.73% and decreasing running
time by 19.71%. The code for EGRC-Net will be made publicly available at
\url{https://github.com/ZhihaoPENG-CityU/EGRC-Net}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2211.13854">ComCLIP: Training-Free Compositional Image and Text Matching. (arXiv:2211.13854v3 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1">Kenan Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1">Xuehai He</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1">Ruize Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1">Xin Eric Wang</a></p>
<p>Contrastive Language-Image Pretraining (CLIP) has demonstrated great
zero-shot performance for matching images and text. However, it is still
challenging to adapt vision-lanaguage pretrained models like CLIP to
compositional image and text matching -- a more challenging image and text
matching task requiring the model understanding of compositional word concepts
and visual components. Towards better compositional generalization in zero-shot
image and text matching, in this paper, we study the problem from a causal
perspective: the erroneous semantics of individual entities are essentially
confounders that cause the matching failure. Therefore, we propose a novel
\textbf{\textit{training-free}} compositional CLIP model (ComCLIP). ComCLIP
disentangles input images into subjects, objects, and action sub-images and
composes CLIP's vision encoder and text encoder to perform evolving matching
over compositional text embedding and sub-image embeddings. In this way,
ComCLIP can mitigate spurious correlations introduced by the pretrained CLIP
models and dynamically evaluate the importance of each component. Experiments
on four compositional image-text matching datasets: SVO, ComVG, Winoground, and
VL-checklist, and two general image-text retrieval datasets: Flick30K, and
MSCOCO demonstrate the effectiveness of our plug-and-play method, which boosts
the \textbf{\textit{zero-shot}} inference ability of CLIP, SLIP, and BLIP2 even
without further training or fine-tuning. Our codes can be found at
https://github.com/eric-ai-lab/ComCLIP.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.02712">Improved Beam Search for Hallucination Mitigation in Abstractive Summarization. (arXiv:2212.02712v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1">Arvind Krishna Sridhar</a>, <a href="http://arxiv.org/find/cs/1/au:+Visser_E/0/1/0/all/0/1">Erik Visser</a></p>
<p>Advancement in large pretrained language models has significantly improved
their performance for conditional language generation tasks including
summarization albeit with hallucinations. To reduce hallucinations,
conventional methods proposed improving beam search or using a fact checker as
a postprocessing step. In this paper, we investigate the use of the Natural
Language Inference (NLI) entailment metric to detect and prevent hallucinations
in summary generation. We propose an NLI-assisted beam re-ranking mechanism by
computing entailment probability scores between the input context and
summarization model-generated beams during saliency-enhanced greedy decoding.
Moreover, a diversity metric is introduced to compare its effectiveness against
vanilla beam search. Our proposed algorithm significantly outperforms vanilla
beam decoding on XSum and CNN/DM datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2212.04972">MOPRD: A multidisciplinary open peer review dataset. (arXiv:2212.04972v2 [cs.DL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1">Jialiang Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1">Jiaxin Song</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1">Zhangping Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yidong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1">Xiaodong Shi</a></p>
<p>Open peer review is a growing trend in academic publications. Public access
to peer review data can benefit both the academic and publishing communities.
It also serves as a great support to studies on review comment generation and
further to the realization of automated scholarly paper review. However, most
of the existing peer review datasets do not provide data that cover the whole
peer review process. Apart from this, their data are not diversified enough as
the data are mainly collected from the field of computer science. These two
drawbacks of the currently available peer review datasets need to be addressed
to unlock more opportunities for related studies. In response, we construct
MOPRD, a multidisciplinary open peer review dataset. This dataset consists of
paper metadata, multiple version manuscripts, review comments, meta-reviews,
author's rebuttal letters, and editorial decisions. Moreover, we propose a
modular guided review comment generation method based on MOPRD. Experiments
show that our method delivers better performance as indicated by both automatic
metrics and human evaluation. We also explore other potential applications of
MOPRD, including meta-review generation, editorial decision prediction, author
rebuttal generation, and scientometric analysis. MOPRD is a strong endorsement
for further studies in peer review-related research and other applications.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2301.05525">Understanding Concept Identification as Consistent Data Clustering Across Multiple Feature Spaces. (arXiv:2301.05525v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lanfermann_F/0/1/0/all/0/1">Felix Lanfermann</a>, <a href="http://arxiv.org/find/cs/1/au:+Schmitt_S/0/1/0/all/0/1">Sebastian Schmitt</a>, <a href="http://arxiv.org/find/cs/1/au:+Wollstadt_P/0/1/0/all/0/1">Patricia Wollstadt</a></p>
<p>Identifying meaningful concepts in large data sets can provide valuable
insights into engineering design problems. Concept identification aims at
identifying non-overlapping groups of design instances that are similar in a
joint space of all features, but which are also similar when considering only
subsets of features. These subsets usually comprise features that characterize
a design with respect to one specific context, for example, constructive design
parameters, performance values, or operation modes. It is desirable to evaluate
the quality of design concepts by considering several of these feature subsets
in isolation. In particular, meaningful concepts should not only identify
dense, well separated groups of data instances, but also provide
non-overlapping groups of data that persist when considering pre-defined
feature subsets separately. In this work, we propose to view concept
identification as a special form of clustering algorithm with a broad range of
potential applications beyond engineering design. To illustrate the differences
between concept identification and classical clustering algorithms, we apply a
recently proposed concept identification algorithm to two synthetic data sets
and show the differences in identified solutions. In addition, we introduce the
mutual information measure as a metric to evaluate whether solutions return
consistent clusters across relevant subsets. To support the novel understanding
of concept identification, we consider a simulated data set from a
decision-making problem in the energy management domain and show that the
identified clusters are more interpretable with respect to relevant feature
subsets than clusters found by common clustering algorithms and are thus more
suitable to support a decision maker.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2302.08942">PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mbacke_S/0/1/0/all/0/1">Sokhna Diarra Mbacke</a>, <a href="http://arxiv.org/find/cs/1/au:+Clerc_F/0/1/0/all/0/1">Florence Clerc</a>, <a href="http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1">Pascal Germain</a></p>
<p>We extend PAC-Bayesian theory to generative models and develop generalization
bounds for models based on the Wasserstein distance and the total variation
distance. Our first result on the Wasserstein distance assumes the instance
space is bounded, while our second result takes advantage of dimensionality
reduction. Our results naturally apply to Wasserstein GANs and Energy-Based
GANs, and our bounds provide new training objectives for these two. Although
our work is mainly theoretical, we perform numerical experiments showing
non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2303.06273">Consistency Analysis of ChatGPT. (arXiv:2303.06273v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Jang_M/0/1/0/all/0/1">Myeongjun Erik Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lukasiewicz_T/0/1/0/all/0/1">Thomas Lukasiewicz</a></p>
<p>ChatGPT has gained a huge popularity since its introduction. Its positive
aspects have been reported through many media platforms, and some analyses even
showed that ChatGPT achieved a decent grade in professional exams, adding extra
support to the claim that AI can now assist and even replace humans in
industrial fields. Others, however, doubt its reliability and trustworthiness.
This paper investigates the trustworthiness of ChatGPT and GPT-4 regarding
logically consistent behaviour, focusing specifically on semantic consistency
and the properties of negation, symmetric, and transitive consistency. Our
findings suggest that while both models appear to show an enhanced language
understanding and reasoning ability, they still frequently fall short of
generating logically consistent predictions. We also ascertain via experiments
that prompt designing, few-shot learning and employing larger large language
models (LLMs) are unlikely to be the ultimate solution to resolve the
inconsistency issue of LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.02192">A Diffusion-based Method for Multi-turn Compositional Image Generation. (arXiv:2304.02192v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1">Chao Wang</a></p>
<p>Multi-turn compositional image generation (M-CIG) is a challenging task that
aims to iteratively manipulate a reference image given a modification text.
While most of the existing methods for M-CIG are based on generative
adversarial networks (GANs), recent advances in image generation have
demonstrated the superiority of diffusion models over GANs. In this paper, we
propose a diffusion-based method for M-CIG named conditional denoising
diffusion with image compositional matching (CDD-ICM). We leverage CLIP as the
backbone of image and text encoders, and incorporate a gated fusion mechanism,
originally proposed for question answering, to compositionally fuse the
reference image and the modification text at each turn of M-CIG. We introduce a
conditioning scheme to generate the target image based on the fusion results.
To prioritize the semantic quality of the generated target image, we learn an
auxiliary image compositional match (ICM) objective, along with the conditional
denoising diffusion (CDD) objective in a multi-task learning framework.
Additionally, we also perform ICM guidance and classifier-free guidance to
improve performance. Experimental results show that CDD-ICM achieves
state-of-the-art results on two benchmark datasets for M-CIG, i.e., CoDraw and
i-CLEVR.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.13765">Towards ethical multimodal systems. (arXiv:2304.13765v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Roger_A/0/1/0/all/0/1">Alexis Roger</a>, <a href="http://arxiv.org/find/cs/1/au:+Aimeur_E/0/1/0/all/0/1">Esma A&#xef;meur</a>, <a href="http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1">Irina Rish</a></p>
<p>Generative AI systems (ChatGPT, DALL-E, etc) are expanding into multiple
areas of our lives, from art Rombach et al. [2021] to mental health Rob Morris
and Kareem Kouddous [2022]; their rapidly growing societal impact opens new
opportunities, but also raises ethical concerns. The emerging field of AI
alignment aims to make AI systems reflect human values. This paper focuses on
evaluating the ethics of multimodal AI systems involving both text and images -
a relatively under-explored area, as most alignment work is currently focused
on language models. We first create a multimodal ethical database from human
feedback on ethicality. Then, using this database, we develop algorithms,
including a RoBERTa-large classifier and a multilayer perceptron, to
automatically assess the ethicality of system responses.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2305.03520">Context-Aware Semantic Similarity Measurement for Unsupervised Word Sense Disambiguation. (arXiv:2305.03520v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Martinez_Gil_J/0/1/0/all/0/1">Jorge Martinez-Gil</a></p>
<p>The issue of word sense ambiguity poses a significant challenge in natural
language processing due to the scarcity of annotated data to feed machine
learning models to face the challenge. Therefore, unsupervised word sense
disambiguation methods have been developed to overcome that challenge without
relying on annotated data. This research proposes a new context-aware approach
to unsupervised word sense disambiguation, which provides a flexible mechanism
for incorporating contextual information into the similarity measurement
process. We experiment with a popular benchmark dataset to evaluate the
proposed strategy and compare its performance with state-of-the-art
unsupervised word sense disambiguation techniques. The experimental results
indicate that our approach substantially enhances disambiguation accuracy and
surpasses the performance of several existing techniques. Our findings
underscore the significance of integrating contextual information in semantic
similarity measurements to manage word sense ambiguity in unsupervised
scenarios effectively.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.00183">Diffused Redundancy in Pre-trained Representations. (arXiv:2306.00183v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Nanda_V/0/1/0/all/0/1">Vedant Nanda</a>, <a href="http://arxiv.org/find/cs/1/au:+Speicher_T/0/1/0/all/0/1">Till Speicher</a>, <a href="http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1">John P. Dickerson</a>, <a href="http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1">Soheil Feizi</a>, <a href="http://arxiv.org/find/cs/1/au:+Gummadi_K/0/1/0/all/0/1">Krishna P. Gummadi</a>, <a href="http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1">Adrian Weller</a></p>
<p>Representations learned by pre-training a neural network on a large dataset
are increasingly used successfully to perform a variety of downstream tasks. In
this work, we take a closer look at how features are encoded in such
pre-trained representations. We find that learned representations in a given
layer exhibit a degree of diffuse redundancy, ie, any randomly chosen subset of
neurons in the layer that is larger than a threshold size shares a large degree
of similarity with the full layer and is able to perform similarly as the whole
layer on a variety of downstream tasks. For example, a linear probe trained on
$20\%$ of randomly picked neurons from the penultimate layer of a ResNet50
pre-trained on ImageNet1k achieves an accuracy within $5\%$ of a linear probe
trained on the full layer of neurons for downstream CIFAR10 classification. We
conduct experiments on different neural architectures (including CNNs and
Transformers) pre-trained on both ImageNet1k and ImageNet21k and evaluate a
variety of downstream tasks taken from the VTAB benchmark. We find that the
loss and dataset used during pre-training largely govern the degree of diffuse
redundancy and the "critical mass" of neurons needed often depends on the
downstream task, suggesting that there is a task-inherent
redundancy-performance Pareto frontier. Our findings shed light on the nature
of representations learned by pre-trained deep neural networks and suggest that
entire layers might not be necessary to perform many downstream tasks. We
investigate the potential for exploiting this redundancy to achieve efficient
generalization for downstream tasks and also draw caution to certain possible
unintended consequences. Our code is available at
\url{https://github.com/nvedant07/diffused-redundancy}.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.09299">Can Language Models Teach Weaker Agents? Teacher Explanations Improve Students via Personalization. (arXiv:2306.09299v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1">Swarnadeep Saha</a>, <a href="http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1">Peter Hase</a>, <a href="http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1">Mohit Bansal</a></p>
<p>A hallmark property of explainable AI models is the ability to teach other
agents, communicating knowledge of how to perform a task. While Large Language
Models perform complex reasoning by generating explanations for their
predictions, it is unclear whether they also make good teachers for weaker
agents. To address this, we consider a student-teacher framework between two
LLM agents and study if, when, and how the teacher should intervene with
natural language explanations to improve the student's performance. Since
communication is expensive, we define a budget such that the teacher only
communicates explanations for a fraction of the data, after which the student
should perform well on its own. We decompose the teaching problem along four
axes: (1) if teacher's test time intervention improve student predictions, (2)
when it is worth explaining a data point, (3) how the teacher should
personalize explanations to better teach the student, and (4) if teacher
explanations also improve students on future unexplained data. We first show
that teacher LLMs can indeed intervene on student reasoning to improve their
performance. Next, inspired by the Theory of Mind abilities of effective
teachers, we propose building two few-shot mental models of the student. The
first model defines an Intervention Function that simulates the utility of an
intervention, allowing the teacher to intervene when this utility is the
highest and improving student performance at lower budgets. The second model
enables the teacher to personalize explanations for a particular student and
outperform unpersonalized teachers. We also demonstrate that in multi-turn
interactions, teacher explanations generalize and learning from explained data
improves student performance on future unexplained data. Finally, we verify
that misaligned teachers can lower student performance to random chance by
intentionally misleading them.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2306.13258">Fast Maximum $k$-Plex Algorithms Parameterized by Small Degeneracy Gaps. (arXiv:2306.13258v3 [cs.DS] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1">Zhengren Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1">Yi Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1">Chunyu Luo</a>, <a href="http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1">Mingyu Xiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1">Jin-Kao Hao</a></p>
<p>Given a graph, a $k$-plex is a set of vertices in which each vertex is not
adjacent to at most $k-1$ other vertices in the set. The maximum $k$-plex
problem, which asks for the largest $k$-plex from the given graph, is an
important but computationally challenging problem in applications such as graph
mining and community detection. So far, there are many practical algorithms,
but without providing theoretical explanations on their efficiency. We define a
novel parameter of the input instance, $g_k(G)$, the gap between the degeneracy
bound and the size of the maximum $k$-plex in the given graph, and present an
exact algorithm parameterized by this $g_k(G)$, which has a worst-case running
time polynomial in the size of the input graph and exponential in $g_k(G)$. In
real-world inputs, $g_k(G)$ is very small, usually bounded by $O(\log{(|V|)})$,
indicating that the algorithm runs in polynomial time. We further extend our
discussion to an even smaller parameter $cg_k(G)$, the gap between the
community-degeneracy bound and the size of the maximum $k$-plex, and show that
without much modification, our algorithm can also be parameterized by
$cg_k(G)$. To verify the empirical performance of these algorithms, we carry
out extensive experiments to show that these algorithms are competitive with
the state-of-the-art algorithms. In particular, for large $k$ values such as
$15$ and $20$, our algorithms dominate the existing algorithms. Finally,
empirical analysis is performed to illustrate the effectiveness of the
parameters and other key components in the implementation.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.00754">ImDiffusion: Imputed Diffusion Models for Multivariate Time Series Anomaly Detection. (arXiv:2307.00754v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1">Yuhang Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1">Chaoyun Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_M/0/1/0/all/0/1">Minghua Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1">Yudong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1">Ruomeng Ding</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1">Bowen Li</a>, <a href="http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1">Shilin He</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajmohan_S/0/1/0/all/0/1">Saravan Rajmohan</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1">Qingwei Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1">Dongmei Zhang</a></p>
<p>Anomaly detection in multivariate time series data is of paramount importance
for ensuring the efficient operation of large-scale systems across diverse
domains. However, accurately detecting anomalies in such data poses significant
challenges. Existing approaches, including forecasting and reconstruction-based
methods, struggle to address these challenges effectively. To overcome these
limitations, we propose a novel anomaly detection framework named ImDiffusion,
which combines time series imputation and diffusion models to achieve accurate
and robust anomaly detection. The imputation-based approach employed by
ImDiffusion leverages the information from neighboring values in the time
series, enabling precise modeling of temporal and inter-correlated
dependencies, reducing uncertainty in the data, thereby enhancing the
robustness of the anomaly detection process. ImDiffusion further leverages
diffusion models as time series imputers to accurately capturing complex
dependencies. We leverage the step-by-step denoised outputs generated during
the inference process to serve as valuable signals for anomaly prediction,
resulting in improved accuracy and robustness of the detection process.
</p>
<p>We evaluate the performance of ImDiffusion via extensive experiments on
benchmark datasets. The results demonstrate that our proposed framework
significantly outperforms state-of-the-art approaches in terms of detection
accuracy and timeliness. ImDiffusion is further integrated into the real
production system in Microsoft and observe a remarkable 11.4% increase in
detection F1 score compared to the legacy approach. To the best of our
knowledge, ImDiffusion represents a pioneering approach that combines
imputation-based techniques with time series anomaly detection, while
introducing the novel use of diffusion models to the field.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.02933">In Time and Space: Towards Usable Adaptive Control for Assistive Robotic Arms. (arXiv:2307.02933v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Pascher_M/0/1/0/all/0/1">Max Pascher</a>, <a href="http://arxiv.org/find/cs/1/au:+Kronhardt_K/0/1/0/all/0/1">Kirill Kronhardt</a>, <a href="http://arxiv.org/find/cs/1/au:+Goldau_F/0/1/0/all/0/1">Felix Ferdinand Goldau</a>, <a href="http://arxiv.org/find/cs/1/au:+Frese_U/0/1/0/all/0/1">Udo Frese</a>, <a href="http://arxiv.org/find/cs/1/au:+Gerken_J/0/1/0/all/0/1">Jens Gerken</a></p>
<p>Robotic solutions, in particular robotic arms, are becoming more frequently
deployed for close collaboration with humans, for example in manufacturing or
domestic care environments. These robotic arms require the user to control
several Degrees-of-Freedom (DoFs) to perform tasks, primarily involving
grasping and manipulating objects. Standard input devices predominantly have
two DoFs, requiring time-consuming and cognitively demanding mode switches to
select individual DoFs. Contemporary Adaptive DoF Mapping Controls (ADMCs) have
shown to decrease the necessary number of mode switches but were up to now not
able to significantly reduce the perceived workload. Users still bear the
mental workload of incorporating abstract mode switching into their workflow.
We address this by providing feed-forward multimodal feedback using updated
recommendations of ADMC, allowing users to visually compare the current and the
suggested mapping in real-time. We contrast the effectiveness of two new
approaches that a) continuously recommend updated DoF combinations or b) use
discrete thresholds between current robot movements and new recommendations.
Both are compared in a Virtual Reality (VR) in-person study against a classic
control method. Significant results for lowered task completion time, fewer
mode switches, and reduced perceived workload conclusively establish that in
combination with feedforward, ADMC methods can indeed outperform classic mode
switching. A lack of apparent quantitative differences between Continuous and
Threshold reveals the importance of user-centered customization options.
Including these implications in the development process will improve usability,
which is essential for successfully implementing robotic technologies with high
user acceptance.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.06440">No Train No Gain: Revisiting Efficient Training Algorithms For Transformer-based Language Models. (arXiv:2307.06440v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Kaddour_J/0/1/0/all/0/1">Jean Kaddour</a>, <a href="http://arxiv.org/find/cs/1/au:+Key_O/0/1/0/all/0/1">Oscar Key</a>, <a href="http://arxiv.org/find/cs/1/au:+Nawrot_P/0/1/0/all/0/1">Piotr Nawrot</a>, <a href="http://arxiv.org/find/cs/1/au:+Minervini_P/0/1/0/all/0/1">Pasquale Minervini</a>, <a href="http://arxiv.org/find/cs/1/au:+Kusner_M/0/1/0/all/0/1">Matt J. Kusner</a></p>
<p>The computation necessary for training Transformer-based language models has
skyrocketed in recent years. This trend has motivated research on efficient
training algorithms designed to improve training, validation, and downstream
performance faster than standard training. In this work, we revisit three
categories of such algorithms: dynamic architectures (layer stacking, layer
dropping), batch selection (selective backprop, RHO loss), and efficient
optimizers (Lion, Sophia). When pre-training BERT and T5 with a fixed
computation budget using such methods, we find that their training, validation,
and downstream gains vanish compared to a baseline with a fully-decayed
learning rate. We define an evaluation protocol that enables computation to be
done on arbitrary machines by mapping all computation time to a reference
machine which we call reference system time. We discuss the limitations of our
proposed protocol and release our code to encourage rigorous research in
efficient training procedures: https://github.com/JeanKaddour/NoTrainNoGain.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2307.12540">UniFormaly: Towards Task-Agnostic Unified Framework for Visual Anomaly Detection. (arXiv:2307.12540v2 [cs.CV] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1">Yujin Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1">Harin Lim</a>, <a href="http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1">Seoyoon Jang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1">Hyunsoo Yoon</a></p>
<p>Visual anomaly detection aims to learn normality from normal images, but
existing approaches are fragmented across various tasks: defect detection,
semantic anomaly detection, multi-class anomaly detection, and anomaly
clustering. This one-task-one-model approach is resource-intensive and incurs
high maintenance costs as the number of tasks increases. We present UniFormaly,
a universal and powerful anomaly detection framework. We emphasize the
necessity of our off-the-shelf approach by pointing out a suboptimal issue in
online encoder-based methods. We introduce Back Patch Masking (BPM) and top
k-ratio feature matching to achieve unified anomaly detection. BPM eliminates
irrelevant background regions using a self-attention map from self-supervised
ViTs. This operates in a task-agnostic manner and alleviates memory storage
consumption, scaling to tasks with large-scale datasets. Top k-ratio feature
matching unifies anomaly levels and tasks by casting anomaly scoring into
multiple instance learning. Finally, UniFormaly achieves outstanding results on
various tasks and datasets. Codes are available at
https://github.com/YoojLee/Uniformaly.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.07336">Learning Deductive Reasoning from Synthetic Corpus based on Formal Logic. (arXiv:2308.07336v3 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Morishita_T/0/1/0/all/0/1">Terufumi Morishita</a>, <a href="http://arxiv.org/find/cs/1/au:+Morio_G/0/1/0/all/0/1">Gaku Morio</a>, <a href="http://arxiv.org/find/cs/1/au:+Yamaguchi_A/0/1/0/all/0/1">Atsuki Yamaguchi</a>, <a href="http://arxiv.org/find/cs/1/au:+Sogawa_Y/0/1/0/all/0/1">Yasuhiro Sogawa</a></p>
<p>We study a synthetic corpus based approach for language models (LMs) to
acquire logical deductive reasoning ability. The previous studies generated
deduction examples using specific sets of deduction rules. However, these rules
were limited or otherwise arbitrary, limiting the generalizability of acquired
reasoning ability. We rethink this and adopt a well-grounded set of deduction
rules based on formal logic theory, which can derive any other deduction rules
when combined in a multistep way. Then, using the proposed corpora, which we
name FLD (Formal Logic Deduction), we first evaluate and analyze the logical
reasoning ability of the latest LLMs. Even GPT-4 can solve only half of the
problems, suggesting that pure logical reasoning isolated from knowledge is
still challenging for the LLMs, and additional training specialized in logical
reasoning is indeed essential. We next empirically verify that LMs trained on
FLD corpora acquire more generalizable reasoning ability. Furthermore, we
identify the aspects of reasoning ability on which deduction corpora can
enhance LMs and those on which they cannot, and discuss future directions on
each aspect. The released corpora serve both as learning resources and as
challenging benchmarks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.10874">Analyzing Transformer Dynamics as Movement through Embedding Space. (arXiv:2308.10874v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1">Sumeet S. Singh</a></p>
<p>Transformer based language models exhibit intelligent behaviors such as
understanding natural language, recognizing patterns, acquiring knowledge,
reasoning, planning, reflecting and using tools. This paper explores how their
underlying mechanics give rise to intelligent behaviors. Towards that end, we
propose framing Transformer dynamics as movement through embedding space.
Examining Transformers through this perspective reveals key insights,
establishing a Theory of Transformers: 1) Intelligent behaviours map to paths
in Embedding Space which, the Transformer random-walks through during
inferencing. 2) LM training learns a probability distribution over all possible
paths. `Intelligence' is learnt by assigning higher probabilities to paths
representing intelligent behaviors. No learning can take place in-context;
context only narrows the subset of paths sampled during decoding. 5) The
Transformer is a self-mapping composition function, folding a context sequence
into a context-vector such that it's proximity to a token-vector reflects its
co-occurrence and conditioned probability. Thus, the physical arrangement of
vectors in Embedding Space determines path probabilities. 6) Context vectors
are composed by aggregating features of the sequence's tokens via a process we
call the encoding walk. Attention contributes a - potentially redundant -
association-bias to this process. 7) This process is comprised of two principal
operation types: filtering (data independent) and aggregation (data dependent).
This generalization unifies Transformers with other sequence models. Building
upon this foundation, we formalize a popular semantic interpretation of
embeddings into a ``concept-space theory'' and find some evidence of it's
validity.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2308.13816">Homological Convolutional Neural Networks. (arXiv:2308.13816v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Briola_A/0/1/0/all/0/1">Antonio Briola</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yuanrong Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Bartolucci_S/0/1/0/all/0/1">Silvia Bartolucci</a>, <a href="http://arxiv.org/find/cs/1/au:+Aste_T/0/1/0/all/0/1">Tomaso Aste</a></p>
<p>Deep learning methods have demonstrated outstanding performances on
classification and regression tasks on homogeneous data types (e.g., image,
audio, and text data). However, tabular data still pose a challenge, with
classic machine learning approaches being often computationally cheaper and
equally effective than increasingly complex deep learning architectures. The
challenge arises from the fact that, in tabular data, the correlation among
features is weaker than the one from spatial or semantic relationships in
images or natural language, and the dependency structures need to be modeled
without any prior information. In this work, we propose a novel deep learning
architecture that exploits the data structural organization through
topologically constrained network representations to gain relational
information from sparse tabular inputs. The resulting model leverages the power
of convolution and is centered on a limited number of concepts from network
topology to guarantee: (i) a data-centric and deterministic building pipeline;
(ii) a high level of interpretability over the inference process; and (iii) an
adequate room for scalability. We test our model on 18 benchmark datasets
against 5 classic machine learning and 3 deep learning models, demonstrating
that our approach reaches state-of-the-art performances on these challenging
datasets. The code to reproduce all our experiments is provided at
https://github.com/FinancialComputingUCL/HomologicalCNN.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.08918">Exploration of TPUs for AI Applications. (arXiv:2309.08918v2 [cs.AR] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Carrion_D/0/1/0/all/0/1">Diego Sanmart&#xed;n Carri&#xf3;n</a>, <a href="http://arxiv.org/find/cs/1/au:+Prohaska_V/0/1/0/all/0/1">Vera Prohaska</a></p>
<p>Tensor Processing Units (TPUs) are specialized hardware accelerators for deep
learning developed by Google. This paper aims to explore TPUs in cloud and edge
computing focusing on its applications in AI. We provide an overview of TPUs,
their general architecture, specifically their design in relation to neural
networks, compilation techniques and supporting frameworks. Furthermore, we
provide a comparative analysis of Cloud and Edge TPU performance against other
counterpart chip architectures. Our results show that TPUs can provide
significant performance improvements in both cloud and edge computing.
Additionally, this paper underscores the imperative need for further research
in optimization techniques for efficient deployment of AI architectures on the
Edge TPU and benchmarking standards for a more robust comparative analysis in
edge computing scenarios. The primary motivation behind this push for research
is that efficient AI acceleration, facilitated by TPUs, can lead to substantial
savings in terms of time, money, and environmental resources.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2309.14610">Unsupervised Graph Deep Learning Reveals Emergent Flood Risk Profile of Urban Areas. (arXiv:2309.14610v3 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1">Kai Yin</a>, <a href="http://arxiv.org/find/cs/1/au:+Mostafavi_A/0/1/0/all/0/1">Ali Mostafavi</a></p>
<p>Urban flood risk emerges from complex and nonlinear interactions among
multiple features related to flood hazard, flood exposure, and social and
physical vulnerabilities, along with the complex spatial flood dependence
relationships. Existing approaches for characterizing urban flood risk,
however, are primarily based on flood plain maps, focusing on a limited number
of features, primarily hazard and exposure features, without consideration of
feature interactions or the dependence relationships among spatial areas. To
address this gap, this study presents an integrated urban flood-risk rating
model based on a novel unsupervised graph deep learning model (called
FloodRisk-Net). FloodRisk-Net is capable of capturing spatial dependence among
areas and complex and nonlinear interactions among flood hazards and urban
features for specifying emergent flood risk. Using data from multiple
metropolitan statistical areas (MSAs) in the United States, the model
characterizes their flood risk into six distinct city-specific levels. The
model is interpretable and enables feature analysis of areas within each
flood-risk level, allowing for the identification of the three archetypes
shaping the highest flood risk within each MSA. Flood risk is found to be
spatially distributed in a hierarchical structure within each MSA, where the
core city disproportionately bears the highest flood risk. Multiple cities are
found to have high overall flood-risk levels and low spatial inequality,
indicating limited options for balancing urban development and flood-risk
reduction. Relevant flood-risk reduction strategies are discussed considering
ways that the highest flood risk and uneven spatial distribution of flood risk
are formed.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.00566">Empowering Many, Biasing a Few: Generalist Credit Scoring through Large Language Models. (arXiv:2310.00566v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1">Duanyu Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1">Yongfu Dai</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1">Jimin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yifang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1">Qianqian Xie</a>, <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Weiguang Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Lopez_Lira_A/0/1/0/all/0/1">Alejandro Lopez-Lira</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1">Hao Wang</a></p>
<p>In the financial industry, credit scoring is a fundamental element, shaping
access to credit and determining the terms of loans for individuals and
businesses alike. Traditional credit scoring methods, however, often grapple
with challenges such as narrow knowledge scope and isolated evaluation of
credit tasks. Our work posits that Large Language Models (LLMs) have great
potential for credit scoring tasks, with strong generalization ability across
multiple tasks. To systematically explore LLMs for credit scoring, we propose
the first open-source comprehensive framework. We curate a novel benchmark
covering 9 datasets with 14K samples, tailored for credit assessment and a
critical examination of potential biases within LLMs, and the novel instruction
tuning data with over 45k samples. We then propose the first Credit and Risk
Assessment Large Language Model (CALM) by instruction tuning, tailored to the
nuanced demands of various financial risk assessment tasks. We evaluate CALM,
and existing state-of-art (SOTA) open source and close source LLMs on the build
benchmark. Our empirical results illuminate the capability of LLMs to not only
match but surpass conventional models, pointing towards a future where credit
scoring can be more inclusive, comprehensive, and unbiased. We contribute to
the industry's transformation by sharing our pioneering instruction-tuning
datasets, credit and risk assessment LLM, and benchmarks with the research
community and the financial industry.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01331">ChoiceMates: Supporting Unfamiliar Online Decision-Making with Multi-Agent Conversational Interactions. (arXiv:2310.01331v2 [cs.HC] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jeongeon Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Min_B/0/1/0/all/0/1">Bryan Min</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xiaojuan Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1">Juho Kim</a></p>
<p>Unfamiliar decisions -- decisions where people lack adequate domain knowledge
or expertise -- specifically increase the complexity and uncertainty of the
process of searching for, understanding, and making decisions with online
information. Through our formative study (n=14), we observed users' challenges
in accessing diverse perspectives, identifying relevant information, and
deciding the right moment to make the final decision. We present ChoiceMates, a
system that enables conversations with a dynamic set of LLM-powered agents for
a holistic domain understanding and efficient discovery and management of
information to make decisions. Agents, as opinionated personas, flexibly join
the conversation, not only providing responses but also conversing among
themselves to elicit each agent's preferences. Our between-subjects study
(n=36) comparing ChoiceMates to conventional web search and single-agent showed
that ChoiceMates was more helpful in discovering, diving deeper, and managing
information compared to Web with higher confidence. We also describe how
participants utilized multi-agent conversations in their decision-making
process.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.01420">Ruffle&amp;Riley: Towards the Automated Induction of Conversational Tutoring Systems. (arXiv:2310.01420v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Schmucker_R/0/1/0/all/0/1">Robin Schmucker</a>, <a href="http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1">Meng Xia</a>, <a href="http://arxiv.org/find/cs/1/au:+Azaria_A/0/1/0/all/0/1">Amos Azaria</a>, <a href="http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1">Tom Mitchell</a></p>
<p>Conversational tutoring systems (CTSs) offer learning experiences driven by
natural language interaction. They are known to promote high levels of
cognitive engagement and benefit learning outcomes, particularly in reasoning
tasks. Nonetheless, the time and cost required to author CTS content is a major
obstacle to widespread adoption. In this paper, we introduce a novel type of
CTS that leverages the recent advances in large language models (LLMs) in two
ways: First, the system induces a tutoring script automatically from a lesson
text. Second, the system automates the script orchestration via two LLM-based
agents (Ruffle&amp;Riley) with the roles of a student and a professor in a
learning-by-teaching format. The system allows a free-form conversation that
follows the ITS-typical inner and outer loop structure. In an initial
between-subject online user study (N = 100) comparing Ruffle&amp;Riley to simpler
QA chatbots and reading activity, we found no significant differences in
post-test scores. Nonetheless, in the learning experience survey, Ruffle&amp;Riley
users expressed higher ratings of understanding and remembering and further
perceived the offered support as more helpful and the conversation as coherent.
Our study provides insights for a new generation of scalable CTS technologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07793">GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1">Ruotong Liao</a>, <a href="http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1">Xu Jia</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1">Yunpu Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1">Volker Tresp</a></p>
<p>The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional carefully
designed embedding-based and rule-based models dominate. The question remains
open of whether pre-trained LLMs can understand structured temporal relational
data and replace them as the foundation model for temporal relational
forecasting. Therefore, we bring temporal knowledge forecasting into the
generative setting. However, challenges occur in the huge chasms between
complex temporal graph data structure and sequential natural expressions LLMs
can handle, and between the enormous data sizes of tKGs and heavy computation
costs of finetuning LLMs. To address these challenges, we propose a novel
retrieval augmented generation framework that performs generative forecasting
on tKGs named GenTKG, which combines a temporal logical rule-based retrieval
strategy and lightweight parameter-efficient instruction tuning. Extensive
experiments have shown that GenTKG outperforms conventional methods of temporal
relational forecasting under low computation resources. GenTKG also highlights
remarkable transferability with exceeding performance on unseen datasets
without re-training. Our work reveals the huge potential of LLMs in the tKG
domain and opens a new frontier for generative forecasting on tKGs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.07838">Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v4 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1">Qingyue Zhao</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1">Banghua Zhu</a></p>
<p>We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.14421">On existence, uniqueness and scalability of adversarial robustness measures for AI classifiers. (arXiv:2310.14421v3 [stat.ML] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Horenko_I/0/1/0/all/0/1">Illia Horenko</a></p>
<p>Simply-verifiable mathematical conditions for existence, uniqueness and
explicit analytical computation of minimal adversarial paths (MAP) and minimal
adversarial distances (MAD) for (locally) uniquely-invertible classifiers, for
generalized linear models (GLM), and for entropic AI (EAI) are formulated and
proven. Practical computation of MAP and MAD, their comparison and
interpretations for various classes of AI tools (for neuronal networks, boosted
random forests, GLM and EAI) are demonstrated on the common synthetic
benchmarks: on a double Swiss roll spiral and its extensions, as well as on the
two biomedical data problems (for the health insurance claim predictions, and
for the heart attack lethality classification). On biomedical applications it
is demonstrated how MAP provides unique minimal patient-specific
risk-mitigating interventions in the predefined subsets of accessible control
variables.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16147">PreWoMe: Exploiting Presuppositions as Working Memory for Long Form Question Answering. (arXiv:2310.16147v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1">Wookje Han</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jinsol Park</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1">Kyungjae Lee</a></p>
<p>Information-seeking questions in long-form question answering (LFQA) often
prove misleading due to ambiguity or false presupposition in the question.
While many existing approaches handle misleading questions, they are tailored
to limited questions, which are insufficient in a real-world setting with
unpredictable input characteristics. In this work, we propose PreWoMe, a
unified approach capable of handling any type of information-seeking question.
The key idea of PreWoMe involves extracting presuppositions in the question and
exploiting them as working memory to generate feedback and action about the
question. Our experiment shows that PreWoMe is effective not only in tackling
misleading questions but also in handling normal ones, thereby demonstrating
the effectiveness of leveraging presuppositions, feedback, and action for
real-world QA settings.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.16600">Balancing central and marginal rejection when combining independent significance tests. (arXiv:2310.16600v2 [stat.ME] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/stat/1/au:+Salahub_C/0/1/0/all/0/1">Chris Salahub</a>, <a href="http://arxiv.org/find/stat/1/au:+Oldford_W/0/1/0/all/0/1">Wayne Oldford</a></p>
<p>A common approach to evaluating the significance of a collection of
$p$-values combines them with a pooling function, in particular when the
original data are not available. These pooled $p$-values convert a sample of
$p$-values into a single number which behaves like a univariate $p$-value. To
clarify discussion of these functions, a telescoping series of alternative
hypotheses are introduced that communicate the strength and prevalence of
non-null evidence in the $p$-values before general pooling formulae are
discussed. A pattern noticed in the UMP pooled $p$-value for a particular
alternative motivates the definition and discussion of central and marginal
rejection levels at $\alpha$. It is proven that central rejection is always
greater than or equal to marginal rejection, motivating a quotient to measure
the balance between the two for pooled $p$-values. A combining function based
on the $\chi^2_{\kappa}$ quantile transformation is proposed to control this
quotient and shown to be robust to mis-specified parameters relative to the
UMP. Different powers for different parameter settings motivate a map of
plausible alternatives based on where this pooled $p$-value is minimized.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17490">Improving Zero-shot Reader by Reducing Distractions from Irrelevant Documents in Open-Domain Question Answering. (arXiv:2310.17490v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1">Sukmin Cho</a>, <a href="http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1">Jeongyeon Seo</a>, <a href="http://arxiv.org/find/cs/1/au:+Jeong_S/0/1/0/all/0/1">Soyeong Jeong</a>, <a href="http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1">Jong C. Park</a></p>
<p>Large language models (LLMs) enable zero-shot approaches in open-domain
question answering (ODQA), yet with limited advancements as the reader is
compared to the retriever. This study aims at the feasibility of a zero-shot
reader that addresses the challenges of computational cost and the need for
labeled data. We find that LLMs are distracted due to irrelevant documents in
the retrieved set and the overconfidence of the generated answers when they are
exploited as zero-shot readers. To tackle these problems, we mitigate the
impact of such documents via Distraction-aware Answer Selection (DAS) with a
negation-based instruction and score adjustment for proper answer selection.
Experimental results show that our approach successfully handles distraction
across diverse scenarios, enhancing the performance of zero-shot readers.
Furthermore, unlike supervised readers struggling with unseen data, zero-shot
readers demonstrate outstanding transferability without any training.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17784">Data-Centric Financial Large Language Models. (arXiv:2310.17784v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1">Zhixuan Chu</a>, <a href="http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1">Huaiyu Guo</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1">Xinyuan Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yijia Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1">Fei Yu</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1">Hong Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wanqing Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1">Xin Lu</a>, <a href="http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1">Qing Cui</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1">Longfei Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jun Zhou</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1">Sheng Li</a></p>
<p>Large language models (LLMs) show promise for natural language tasks but
struggle when applied directly to complex domains like finance. LLMs have
difficulty reasoning about and integrating all relevant information. We propose
a data-centric approach to enable LLMs to better handle financial tasks. Our
key insight is that rather than overloading the LLM with everything at once, it
is more effective to preprocess and pre-understand the data. We create a
financial LLM (FLLM) using multitask prompt-based finetuning to achieve data
pre-processing and pre-understanding. However, labeled data is scarce for each
task. To overcome manual annotation costs, we employ abductive augmentation
reasoning (AAR) to automatically generate training data by modifying the pseudo
labels from FLLM's own outputs. Experiments show our data-centric FLLM with AAR
substantially outperforms baseline financial LLMs designed for raw text,
achieving state-of-the-art on financial analysis and interpretation tasks. We
also open source a new benchmark for financial analysis and interpretation. Our
methodology provides a promising path to unlock LLMs' potential for complex
real-world domains.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.17940">Unified Segment-to-Segment Framework for Simultaneous Sequence Generation. (arXiv:2310.17940v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Shaolei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1">Yang Feng</a></p>
<p>Simultaneous sequence generation is a pivotal task for real-time scenarios,
such as streaming speech recognition, simultaneous machine translation and
simultaneous speech translation, where the target sequence is generated while
receiving the source sequence. The crux of achieving high-quality generation
with low latency lies in identifying the optimal moments for generating,
accomplished by learning a mapping between the source and target sequences.
However, existing methods often rely on task-specific heuristics for different
sequence types, limiting the model's capacity to adaptively learn the
source-target mapping and hindering the exploration of multi-task learning for
various simultaneous tasks. In this paper, we propose a unified
segment-to-segment framework (Seg2Seg) for simultaneous sequence generation,
which learns the mapping in an adaptive and unified manner. During the process
of simultaneous generation, the model alternates between waiting for a source
segment and generating a target segment, making the segment serve as the
natural bridge between the source and target. To accomplish this, Seg2Seg
introduces a latent segment as the pivot between source to target and explores
all potential source-target mappings via the proposed expectation training,
thereby learning the optimal moments for generating. Experiments on multiple
simultaneous generation tasks demonstrate that Seg2Seg achieves
state-of-the-art performance and exhibits better generality across various
tasks.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19347">Improving Factual Consistency of Text Summarization by Adversarially Decoupling Comprehension and Embellishment Abilities of LLMs. (arXiv:2310.19347v3 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1">Huawen Feng</a>, <a href="http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1">Yan Fan</a>, <a href="http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1">Xiong Liu</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1">Ting-En Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1">Zekun Yao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1">Yuchuan Wu</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1">Fei Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1">Yongbin Li</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1">Qianli Ma</a></p>
<p>Despite the recent progress in text summarization made by large language
models (LLMs), they often generate summaries that are factually inconsistent
with original articles, known as "hallucinations" in text generation. Unlike
previous small models (e.g., BART, T5), current LLMs make fewer silly mistakes
but more sophisticated ones, such as imposing cause and effect, adding false
details, overgeneralizing, etc. These hallucinations are challenging to detect
through traditional methods, which poses great challenges for improving the
factual consistency of text summarization. In this paper, we propose an
adversarially DEcoupling method to disentangle the Comprehension and
EmbellishmeNT abilities of LLMs (DECENT). Furthermore, we adopt a probing-based
efficient training to cover the shortage of sensitivity for true and false in
the training process of LLMs. In this way, LLMs are less confused about
embellishing and understanding; thus, they can execute the instructions more
accurately and have enhanced abilities to distinguish hallucinations.
Experimental results show that DECENT significantly improves the reliability of
text summarization based on LLMs.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.19647">Fast swap regret minimization and applications to approximate correlated equilibria. (arXiv:2310.19647v2 [cs.GT] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1">Binghui Peng</a>, <a href="http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1">Aviad Rubinstein</a></p>
<p>We give a simple and computationally efficient algorithm that, for any
constant $\varepsilon&gt;0$, obtains $\varepsilon T$-swap regret within only $T =
\mathsf{polylog}(n)$ rounds; this is an exponential improvement compared to the
super-linear number of rounds required by the state-of-the-art algorithm, and
resolves the main open problem of [Blum and Mansour 2007]. Our algorithm has an
exponential dependence on $\varepsilon$, but we prove a new, matching lower
bound.
</p>
<p>Our algorithm for swap regret implies faster convergence to
$\varepsilon$-Correlated Equilibrium ($\varepsilon$-CE) in several regimes: For
normal form two-player games with $n$ actions, it implies the first uncoupled
dynamics that converges to the set of $\varepsilon$-CE in polylogarithmic
rounds; a $\mathsf{polylog}(n)$-bit communication protocol for $\varepsilon$-CE
in two-player games (resolving an open problem mentioned by
[Babichenko-Rubinstein'2017, Goos-Rubinstein'2018, Ganor-CS'2018]); and an
$\tilde{O}(n)$-query algorithm for $\varepsilon$-CE (resolving an open problem
of [Babichenko'2020] and obtaining the first separation between
$\varepsilon$-CE and $\varepsilon$-Nash equilibrium in the query complexity
model).
</p>
<p>For extensive-form games, our algorithm implies a PTAS for $\mathit{normal}$
$\mathit{form}$ $\mathit{correlated}$ $\mathit{equilibria}$, a solution concept
often conjectured to be computationally intractable (e.g. [Stengel-Forges'08,
Fujii'23]).
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2310.20689">Learning From Mistakes Makes LLM Better Reasoner. (arXiv:2310.20689v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1">Shengnan An</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1">Zexiong Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1">Zeqi Lin</a>, <a href="http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1">Nanning Zheng</a>, <a href="http://arxiv.org/find/cs/1/au:+Lou_J/0/1/0/all/0/1">Jian-Guang Lou</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1">Weizhu Chen</a></p>
<p>Large language models (LLMs) recently exhibited remarkable reasoning
capabilities on solving math problems. To further improve this capability, this
work proposes Learning from Mistakes (LeMa), akin to human learning processes.
Consider a human student who failed to solve a math problem, he will learn from
what mistake he has made and how to correct it. Mimicking this error-driven
learning process, LeMa fine-tunes LLMs on mistake-correction data pairs
generated by GPT-4. Specifically, we first collect inaccurate reasoning paths
from various LLMs and then employ GPT-4 as a "corrector" to (1) identify the
mistake step, (2) explain the reason for the mistake, and (3) correct the
mistake and generate the final answer. Experimental results demonstrate the
effectiveness of LeMa: across five backbone LLMs and two mathematical reasoning
tasks, LeMa consistently improves the performance compared with fine-tuning on
CoT data alone. Impressively, LeMa can also benefit specialized LLMs such as
WizardMath and MetaMath, achieving 85.4% pass@1 accuracy on GSM8K and 27.1% on
MATH. This surpasses the SOTA performance achieved by non-execution open-source
models on these challenging tasks. Our code, data and models will be publicly
available at https://github.com/microsoft/LEMA.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.01351">Simplicial Models for the Epistemic Logic of Faulty Agents. (arXiv:2311.01351v3 [cs.LO] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Goubault_E/0/1/0/all/0/1">Eric Goubault</a>, <a href="http://arxiv.org/find/cs/1/au:+Kniazev_R/0/1/0/all/0/1">Roman Kniazev</a>, <a href="http://arxiv.org/find/cs/1/au:+Ledent_J/0/1/0/all/0/1">Jeremy Ledent</a>, <a href="http://arxiv.org/find/cs/1/au:+Rajsbaum_S/0/1/0/all/0/1">Sergio Rajsbaum</a></p>
<p>In recent years, several authors have been investigating simplicial models, a
model of epistemic logic based on higher-dimensional structures called
simplicial complexes. In the original formulation, simplicial models were
always assumed to be pure, meaning that all worlds have the same dimension.
This is equivalent to the standard S5n semantics of epistemic logic, based on
Kripke models. By removing the assumption that models must be pure, we can go
beyond the usual Kripke semantics and study epistemic logics where the number
of agents participating in a world can vary. This approach has been developed
in a number of papers, with applications in fault-tolerant distributed
computing where processes may crash during the execution of a system. A
difficulty that arises is that subtle design choices in the definition of
impure simplicial models can result in different axioms of the resulting logic.
In this paper, we classify those design choices systematically, and axiomatize
the corresponding logics. We illustrate them via distributed computing examples
of synchronous systems where processes may crash.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.02082">Semantic Modelling of Organizational Knowledge as a Basis for Enterprise Data Governance 4.0 -- Application to a Unified Clinical Data Model. (arXiv:2311.02082v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Oliveira_M/0/1/0/all/0/1">Miguel AP Oliveira</a>, <a href="http://arxiv.org/find/cs/1/au:+Manara_S/0/1/0/all/0/1">Stephane Manara</a>, <a href="http://arxiv.org/find/cs/1/au:+Mole_B/0/1/0/all/0/1">Bruno Mol&#xe9;</a>, <a href="http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1">Thomas Muller</a>, <a href="http://arxiv.org/find/cs/1/au:+Guillouche_A/0/1/0/all/0/1">Aur&#xe9;lien Guillouche</a>, <a href="http://arxiv.org/find/cs/1/au:+Hesske_L/0/1/0/all/0/1">Lysann Hesske</a>, <a href="http://arxiv.org/find/cs/1/au:+Jordan_B/0/1/0/all/0/1">Bruce Jordan</a>, <a href="http://arxiv.org/find/cs/1/au:+Hubert_G/0/1/0/all/0/1">Gilles Hubert</a>, <a href="http://arxiv.org/find/cs/1/au:+Kulkarni_C/0/1/0/all/0/1">Chinmay Kulkarni</a>, <a href="http://arxiv.org/find/cs/1/au:+Jagdev_P/0/1/0/all/0/1">Pralipta Jagdev</a>, <a href="http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1">Cedric R. Berger</a></p>
<p>Individuals and organizations cope with an always-growing data amount,
heterogeneous in contents and formats. A prerequisite to get value out this
data and minimise inherent risks related to multiple usages is an adequate data
management process yielding data quality and control over its lifecycle. Common
data governance frameworks relying on people, policies and processes falls
short of the overwhelming data complexity. Yet, harnessing this complexity is
necessary to achieve high quality standards. The later will condition the
outcome of any downstream data usage, including generative artificial
intelligence trained on this data. In this paper, we report our concrete
experience establishing a simple, cost-efficient framework, that enables
metadata-driven, agile and (semi-)automated data governance (i.e. Data
Governance 4.0). We explain how we implement and use this framework to
integrate 25 years of clinical study data at enterprise scale, in a fully
productive environment. The framework encompasses both methodologies and
technologies leveraging semantic web principles. We built a knowledge graph
describing avatars of data assets in their business context including
governance principles. Multiple ontologies articulated by an enterprise upper
ontology enable key governance actions such as FAIRification, lifecycle
management, definition of roles and responsibilities, lineage across
transformations and provenance from source systems. This metadata model is the
keystone to data governance 4.0: a semi-automatized data management process,
taking in account the business context in an agile manner to adapt governance
constraints to each use case and dynamically tune it based on business changes.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.04589">TEAL: Tokenize and Embed ALL for Multi-modal Large Language Models. (arXiv:2311.04589v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1">Zhen Yang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1">Yingxue Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1">Fandong Meng</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1">Jie Zhou</a></p>
<p>Despite Multi-modal Large Language Models (MM-LLMs) have made exciting
strides recently, they are still struggling to efficiently model the
interactions among multi-modal inputs and the generation in non-textual
modalities. In this work, we propose TEAL (Tokenize and Embed ALl)}, an
approach to treat the input from any modality as a token sequence and learn a
joint embedding space for all modalities. Specifically, for the input from any
modality, TEAL first discretizes it into a token sequence with the
off-the-shelf tokenizer and embeds the token sequence into a joint embedding
space with a learnable embedding matrix. MM-LLMs just need to predict the
multi-modal tokens autoregressively as the textual LLMs do. Finally, the
corresponding de-tokenizer is applied to generate the output in each modality
based on the predicted token sequence. With the joint embedding space, TEAL
enables the frozen LLMs to perform both understanding and generation tasks
involving non-textual modalities, such as image and audio. Thus, the textual
LLM can just work as an interface and maintain its high performance in textual
understanding and generation. Experiments show that TEAL achieves substantial
improvements in multi-modal understanding, and implements a simple scheme for
multi-modal generations.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05511">Anytime-Constrained Reinforcement Learning. (arXiv:2311.05511v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+McMahan_J/0/1/0/all/0/1">Jeremy McMahan</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1">Xiaojin Zhu</a></p>
<p>We introduce and study constrained Markov Decision Processes (cMDPs) with
anytime constraints. An anytime constraint requires the agent to never violate
its budget at any point in time, almost surely. Although Markovian policies are
no longer sufficient, we show that there exist optimal deterministic policies
augmented with cumulative costs. In fact, we present a fixed-parameter
tractable reduction from anytime-constrained cMDPs to unconstrained MDPs. Our
reduction yields planning and learning algorithms that are time and
sample-efficient for tabular cMDPs so long as the precision of the costs is
logarithmic in the size of the cMDP. However, we also show that computing
non-trivial approximately optimal policies is NP-hard in general. To circumvent
this bottleneck, we design provable approximation algorithms that efficiently
compute or learn an arbitrarily accurate approximately feasible policy with
optimal value so long as the maximum supported cost is bounded by a polynomial
in the cMDP or the absolute budget. Given our hardness results, our
approximation guarantees are the best possible under worst-case analysis.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.05915">Fake Alignment: Are LLMs Really Aligned Well?. (arXiv:2311.05915v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yixu Wang</a>, <a href="http://arxiv.org/find/cs/1/au:+Teng_Y/0/1/0/all/0/1">Yan Teng</a>, <a href="http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1">Kexin Huang</a>, <a href="http://arxiv.org/find/cs/1/au:+Lyu_C/0/1/0/all/0/1">Chengqi Lyu</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1">Songyang Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1">Wenwei Zhang</a>, <a href="http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1">Xingjun Ma</a>, <a href="http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1">Yu-Gang Jiang</a>, <a href="http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1">Yu Qiao</a>, <a href="http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1">Yingchun Wang</a></p>
<p>The growing awareness of safety concerns in large language models (LLMs) has
sparked considerable interest in the evaluation of safety within current
research endeavors. This study investigates an interesting issue pertaining to
the evaluation of LLMs, namely the substantial discrepancy in performance
between multiple-choice questions and open-ended questions. Inspired by
research on jailbreak attack patterns, we argue this is caused by mismatched
generalization. That is, the LLM does not have a comprehensive understanding of
the complex concept of safety. Instead, it only remembers what to answer for
open-ended safety questions, which makes it unable to solve other forms of
safety tests. We refer to this phenomenon as fake alignment and construct a
comparative benchmark to empirically verify its existence in LLMs. Such fake
alignment renders previous evaluation protocols unreliable. To address this, we
introduce the Fake alIgNment Evaluation (FINE) framework and two novel
metrics--Consistency Score (CS) and Consistent Safety Score (CSS), which
jointly assess two complementary forms of evaluation to quantify fake alignment
and obtain corrected performance estimates. Applying FINE to 14 widely-used
LLMs reveals several models with purported safety are poorly aligned in
practice. Our work highlights potential limitations in prevailing alignment
methodologies.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06315">ShipGen: A Diffusion Model for Parametric Ship Hull Generation with Multiple Objectives and Constraints. (arXiv:2311.06315v2 [cs.LG] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Bagazinski_N/0/1/0/all/0/1">Noah J. Bagazinski</a>, <a href="http://arxiv.org/find/cs/1/au:+Ahmed_F/0/1/0/all/0/1">Faez Ahmed</a></p>
<p>Ship design is a years-long process that requires balancing complex design
trade-offs to create a ship that is efficient and effective. Finding new ways
to improve the ship design process can lead to significant cost savings for
ship building and operation. One promising technology is generative artificial
intelligence, which has been shown to reduce design cycle time and create
novel, high-performing designs. In literature review, generative artificial
intelligence has been shown to generate ship hulls; however, ship design is
particularly difficult as the hull of a ship requires the consideration of many
objectives. This paper presents a study on the generation of parametric ship
hull designs using a parametric diffusion model that considers multiple
objectives and constraints for the hulls. This denoising diffusion
probabilistic model (DDPM) generates the tabular parametric design vectors of a
ship hull for evaluation. In addition to a tabular DDPM, this paper details
adding guidance to improve the quality of generated ship hull designs. By
leveraging classifier guidance, the DDPM produced feasible parametric ship
hulls that maintain the coverage of the initial training dataset of ship hulls
with a 99.5% rate, a 149x improvement over random sampling of the design vector
parameters across the design space. Parametric ship hulls produced with
performance guidance saw an average of 91.4% reduction in wave drag
coefficients and an average of a 47.9x relative increase in the total displaced
volume of the hulls compared to the mean performance of the hulls in the
training dataset. The use of a DDPM to generate parametric ship hulls can
reduce design time by generating high-performing hull designs for future
analysis. These generated hulls have low drag and high volume, which can reduce
the cost of operating a ship and increase its potential to generate revenue.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06513">Step by Step to Fairness: Attributing Societal Bias in Task-oriented Dialogue Systems. (arXiv:2311.06513v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1">Hsuan Su</a>, <a href="http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1">Rebecca Qian</a>, <a href="http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1">Chinnadhurai Sankar</a>, <a href="http://arxiv.org/find/cs/1/au:+Shayandeh_S/0/1/0/all/0/1">Shahin Shayandeh</a>, <a href="http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1">Shang-Tse Chen</a>, <a href="http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1">Hung-yi Lee</a>, <a href="http://arxiv.org/find/cs/1/au:+Bikel_D/0/1/0/all/0/1">Daniel M. Bikel</a></p>
<p>Recent works have shown considerable improvements in task-oriented dialogue
(TOD) systems by utilizing pretrained large language models (LLMs) in an
end-to-end manner. However, the biased behavior of each component in a TOD
system and the error propagation issue in the end-to-end framework can lead to
seriously biased TOD responses. Existing works of fairness only focus on the
total bias of a system. In this paper, we propose a diagnosis method to
attribute bias to each component of a TOD system. With the proposed attribution
method, we can gain a deeper understanding of the sources of bias.
Additionally, researchers can mitigate biased model behavior at a more granular
level. We conduct experiments to attribute the TOD system's bias toward three
demographic axes: gender, age, and race. Experimental results show that the
bias of a TOD system usually comes from the response generation model.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.06703">Enabling Human-Centered AI: A Methodological Perspective. (arXiv:2311.06703v2 [cs.AI] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1">Wei Xu</a>, <a href="http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1">Zaifeng Gao</a></p>
<p>Human-centered AI (HCAI) is a design philosophy that advocates prioritizing
humans in designing, developing, and deploying intelligent systems, aiming to
maximize the benefits of AI to humans and avoid potential adverse impacts.
While HCAI continues to influence, the lack of guidance on methodology in
practice makes its adoption challenging. This paper proposes a comprehensive
HCAI framework based on our previous work with integrated components, including
design goals, design principles, implementation approaches, interdisciplinary
teams, HCAI methods, and HCAI processes. This paper also presents a
"three-layer" approach to facilitate the implementation of the framework. We
believe this systematic and executable framework can overcome the weaknesses in
current HCAI frameworks and the challenges currently faced in practice, putting
it into action to enable HCAI further.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2311.07439">Investigating Multi-Pivot Ensembling with Massively Multilingual Machine Translation Models. (arXiv:2311.07439v2 [cs.CL] UPDATED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Mohammadshahi_A/0/1/0/all/0/1">Alireza Mohammadshahi</a>, <a href="http://arxiv.org/find/cs/1/au:+Vamvas_J/0/1/0/all/0/1">Jannis Vamvas</a>, <a href="http://arxiv.org/find/cs/1/au:+Sennrich_R/0/1/0/all/0/1">Rico Sennrich</a></p>
<p>Massively multilingual machine translation models allow for the translation
of a large number of languages with a single model, but have limited
performance on low- and very-low-resource translation directions. Pivoting via
high-resource languages remains a strong strategy for low-resource directions,
and in this paper we revisit ways of pivoting through multiple languages.
Previous work has used a simple averaging of probability distributions from
multiple paths, but we find that this performs worse than using a single pivot,
and exacerbates the hallucination problem because the same hallucinations can
be probable across different paths. As an alternative, we propose MaxEns, a
combination strategy that is biased towards the most confident predictions,
hypothesising that confident predictions are less prone to be hallucinations.
We evaluate different strategies on the FLORES benchmark for 20 low-resource
language directions, demonstrating that MaxEns improves translation quality for
low-resource languages while reducing hallucination in translations, compared
to both direct translation and an averaging approach. On average, multi-pivot
strategies still lag behind using English as a single pivot language, raising
the question of how to identify the best pivoting strategy for a given
translation direction.
</p>
</p>
</div>
<div class="article">
<h1><a href="http://arxiv.org/abs/2304.01994">Waving Goodbye to Low-Res: A Diffusion-Wavelet Approach for Image Super-Resolution. (arXiv:2304.01994v2 [cs.CV] CROSS LISTED)</a></h1>
<p><b>Authors:</b>  <a href="http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1">Brian Moser</a>, <a href="http://arxiv.org/find/cs/1/au:+Frolov_S/0/1/0/all/0/1">Stanislav Frolov</a>, <a href="http://arxiv.org/find/cs/1/au:+Raue_F/0/1/0/all/0/1">Federico Raue</a>, <a href="http://arxiv.org/find/cs/1/au:+Palacio_S/0/1/0/all/0/1">Sebastian Palacio</a>, <a href="http://arxiv.org/find/cs/1/au:+Dengel_A/0/1/0/all/0/1">Andreas Dengel</a></p>
<p>This paper presents a novel Diffusion-Wavelet (DiWa) approach for
Single-Image Super-Resolution (SISR). It leverages the strengths of Denoising
Diffusion Probabilistic Models (DDPMs) and Discrete Wavelet Transformation
(DWT). By enabling DDPMs to operate in the DWT domain, our DDPM models
effectively hallucinate high-frequency information for super-resolved images on
the wavelet spectrum, resulting in high-quality and detailed reconstructions in
image space. Quantitatively, we outperform state-of-the-art diffusion-based
SISR methods, namely SR3 and SRDiff, regarding PSNR, SSIM, and LPIPS on both
face (8x scaling) and general (4x scaling) SR benchmarks. Meanwhile, using DWT
enabled us to use fewer parameters than the compared models: 92M parameters
instead of 550M compared to SR3 and 9.3M instead of 12M compared to SRDiff.
Additionally, our method outperforms other state-of-the-art generative methods
on classical general SR datasets while saving inference time. Finally, our work
highlights its potential for various applications.
</p>
</p>
</div>

    </div>
    </body>
    