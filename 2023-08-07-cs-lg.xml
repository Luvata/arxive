<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02145" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02180" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02182" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02261" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02427" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02460" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2003.09168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.00596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.09195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.07871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.15076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.09919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.03890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.08549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07264" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01867" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.01910">
<title>Deep Policy Gradient Methods in Commodity Markets. (arXiv:2308.01910v1 [q-fin.TR])</title>
<link>http://arxiv.org/abs/2308.01910</link>
<description rdf:parseType="Literal">&lt;p&gt;The energy transition has increased the reliance on intermittent energy
sources, destabilizing energy markets and causing unprecedented volatility,
culminating in the global energy crisis of 2021. In addition to harming
producers and consumers, volatile energy markets may jeopardize vital
decarbonization efforts. Traders play an important role in stabilizing markets
by providing liquidity and reducing volatility. Several mathematical and
statistical models have been proposed for forecasting future returns. However,
developing such models is non-trivial due to financial markets&apos; low
signal-to-noise ratios and nonstationary dynamics.
&lt;/p&gt;
&lt;p&gt;This thesis investigates the effectiveness of deep reinforcement learning
methods in commodities trading. It formalizes the commodities trading problem
as a continuing discrete-time stochastic dynamical system. This system employs
a novel time-discretization scheme that is reactive and adaptive to market
volatility, providing better statistical properties for the sub-sampled
financial time series. Two policy gradient algorithms, an actor-based and an
actor-critic-based, are proposed for optimizing a transaction-cost- and
risk-sensitive trading agent. The agent maps historical price observations to
market positions through parametric function approximators utilizing deep
neural network architectures, specifically CNNs and LSTMs.
&lt;/p&gt;
&lt;p&gt;On average, the deep reinforcement learning models produce an 83 percent
higher Sharpe ratio than the buy-and-hold baseline when backtested on
front-month natural gas futures from 2017 to 2022. The backtests demonstrate
that the risk tolerance of the deep reinforcement learning agents can be
adjusted using a risk-sensitivity term. The actor-based policy gradient
algorithm performs significantly better than the actor-critic-based algorithm,
and the CNN-based models perform slightly better than those based on the LSTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Hanetho_J/0/1/0/all/0/1&quot;&gt;Jonas Hanetho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01915">
<title>LOB-Based Deep Learning Models for Stock Price Trend Prediction: A Benchmark Study. (arXiv:2308.01915v1 [q-fin.TR])</title>
<link>http://arxiv.org/abs/2308.01915</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advancements in Deep Learning (DL) research have notably
influenced the finance sector. We examine the robustness and generalizability
of fifteen state-of-the-art DL models focusing on Stock Price Trend Prediction
(SPTP) based on Limit Order Book (LOB) data. To carry out this study, we
developed LOBCAST, an open-source framework that incorporates data
preprocessing, DL model training, evaluation and profit analysis. Our extensive
experiments reveal that all models exhibit a significant performance drop when
exposed to new data, thereby raising questions about their real-world market
applicability. Our work serves as a benchmark, illuminating the potential and
the limitations of current approaches and providing insight for innovative
solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Prata_M/0/1/0/all/0/1&quot;&gt;Matteo Prata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Masi_G/0/1/0/all/0/1&quot;&gt;Giuseppe Masi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Berti_L/0/1/0/all/0/1&quot;&gt;Leonardo Berti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Arrigoni_V/0/1/0/all/0/1&quot;&gt;Viviana Arrigoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Coletta_A/0/1/0/all/0/1&quot;&gt;Andrea Coletta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Cannistraci_I/0/1/0/all/0/1&quot;&gt;Irene Cannistraci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Vyetrenko_S/0/1/0/all/0/1&quot;&gt;Svitlana Vyetrenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Velardi_P/0/1/0/all/0/1&quot;&gt;Paola Velardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Bartolini_N/0/1/0/all/0/1&quot;&gt;Novella Bartolini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01916">
<title>Semi Supervised Meta Learning for Spatiotemporal Learning. (arXiv:2308.01916v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.01916</link>
<description rdf:parseType="Literal">&lt;p&gt;We approached the goal of applying meta-learning to self-supervised masked
autoencoders for spatiotemporal learning in three steps. Broadly, we seek to
understand the impact of applying meta-learning to existing state-of-the-art
representation learning architectures. Thus, we test spatiotemporal learning
through: a meta-learning architecture only, a representation learning
architecture only, and an architecture applying representation learning
alongside a meta learning architecture. We utilize the Memory Augmented Neural
Network (MANN) architecture to apply meta-learning to our framework.
Specifically, we first experiment with applying a pre-trained MAE and
fine-tuning on our small-scale spatiotemporal dataset for video reconstruction
tasks. Next, we experiment with training an MAE encoder and applying a
classification head for action classification tasks. Finally, we experiment
with applying a pre-trained MAE and fine-tune with MANN backbone for action
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waseem_F/0/1/0/all/0/1&quot;&gt;Faraz Waseem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muthukumar_P/0/1/0/all/0/1&quot;&gt;Pratyush Muthukumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01917">
<title>PePNet: A Periodicity-Perceived Workload Prediction Network Supporting Rare Occurrence of Heavy Workload. (arXiv:2308.01917v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2308.01917</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud providers can greatly benefit from accurate workload prediction.
However, the workload of cloud servers is highly variable, with occasional
heavy workload bursts. This makes workload prediction challenging.
&lt;/p&gt;
&lt;p&gt;There are mainly two categories of workload prediction methods: statistical
methods and neural-network-based ones. The former ones rely on strong
mathematical assumptions and have reported low accuracy when predicting highly
variable workload. The latter ones offer higher overall accuracy, yet they are
vulnerable to data imbalance between heavy workload and common one. This
impairs the prediction accuracy of neural network-based models on heavy
workload.
&lt;/p&gt;
&lt;p&gt;Either the overall inaccuracy of statistic methods or the heavy-workload
inaccuracy of neural-network-based models can cause service level agreement
violations.
&lt;/p&gt;
&lt;p&gt;Thus, we propose PePNet to improve overall especially heavy workload
prediction accuracy. It has two distinctive characteristics:
&lt;/p&gt;
&lt;p&gt;(i) A Periodicity-Perceived Mechanism to detect the existence of periodicity
and the length of one period automatically, without any priori knowledge.
Furthermore, it fuses periodic information adaptively, which is suitable for
periodic, lax periodic and aperiodic time series.
&lt;/p&gt;
&lt;p&gt;(ii) An Achilles&apos; Heel Loss Function iteratively optimizing the most
under-fitting part in predicting sequence for each step, which significantly
improves the prediction accuracy of heavy load.
&lt;/p&gt;
&lt;p&gt;Extensive experiments conducted on Alibaba2018, SMD dataset and Dinda&apos;s
dataset demonstrate that PePNet improves MAPE for overall workload by 20.0% on
average, compared with state-of-the-art methods. Especially, PePNet improves
MAPE for heavy workload by 23.9% on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hailiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mengchu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shuiguang Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01920">
<title>Sequence-Based Nanobody-Antigen Binding Prediction. (arXiv:2308.01920v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2308.01920</link>
<description rdf:parseType="Literal">&lt;p&gt;Nanobodies (Nb) are monomeric heavy-chain fragments derived from heavy-chain
only antibodies naturally found in Camelids and Sharks. Their considerably
small size (~3-4 nm; 13 kDa) and favorable biophysical properties make them
attractive targets for recombinant production. Furthermore, their unique
ability to bind selectively to specific antigens, such as toxins, chemicals,
bacteria, and viruses, makes them powerful tools in cell biology, structural
biology, medical diagnostics, and future therapeutic agents in treating cancer
and other serious illnesses. However, a critical challenge in nanobodies
production is the unavailability of nanobodies for a majority of antigens.
Although some computational methods have been proposed to screen potential
nanobodies for given target antigens, their practical application is highly
restricted due to their reliance on 3D structures. Moreover, predicting
nanobodyantigen interactions (binding) is a time-consuming and labor-intensive
task. This study aims to develop a machine-learning method to predict
Nanobody-Antigen binding solely based on the sequence data. We curated a
comprehensive dataset of Nanobody-Antigen binding and nonbinding data and
devised an embedding method based on gapped k-mers to predict binding based
only on sequences of nanobody and antigen. Our approach achieves up to 90%
accuracy in binding prediction and is significantly more efficient compared to
the widely-used computational docking technique.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sardar_U/0/1/0/all/0/1&quot;&gt;Usama Sardar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Sarwan Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ayub_M/0/1/0/all/0/1&quot;&gt;Muhammad Sohaib Ayub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shoaib_M/0/1/0/all/0/1&quot;&gt;Muhammad Shoaib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bashir_K/0/1/0/all/0/1&quot;&gt;Khurram Bashir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Khan_I/0/1/0/all/0/1&quot;&gt;Imdad Ullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Patterson_M/0/1/0/all/0/1&quot;&gt;Murray Patterson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01921">
<title>Transferable Graph Neural Fingerprint Models for Quick Response to Future Bio-Threats. (arXiv:2308.01921v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2308.01921</link>
<description rdf:parseType="Literal">&lt;p&gt;Fast screening of drug molecules based on the ligand binding affinity is an
important step in the drug discovery pipeline. Graph neural fingerprint is a
promising method for developing molecular docking surrogates with high
throughput and great fidelity. In this study, we built a COVID-19 drug docking
dataset of about 300,000 drug candidates on 23 coronavirus protein targets.
With this dataset, we trained graph neural fingerprint docking models for
high-throughput virtual COVID-19 drug screening. The graph neural fingerprint
models yield high prediction accuracy on docking scores with the mean squared
error lower than $0.21$ kcal/mol for most of the docking targets, showing
significant improvement over conventional circular fingerprint methods. To make
the neural fingerprints transferable for unknown targets, we also propose a
transferable graph neural fingerprint method trained on multiple targets. With
comparable accuracy to target-specific graph neural fingerprint models, the
transferable model exhibits superb training and data efficiency. We highlight
that the impact of this study extends beyond COVID-19 dataset, as our approach
for fast virtual ligand screening can be easily adapted and integrated into a
general machine learning-accelerated pipeline to battle future bio-threats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yihui Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kagawa_A/0/1/0/all/0/1&quot;&gt;Ai Kagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Carbone_M/0/1/0/all/0/1&quot;&gt;Matthew R. Carbone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Samuel Yen-Chi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yoo_S/0/1/0/all/0/1&quot;&gt;Shinjae Yoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Clyde_A/0/1/0/all/0/1&quot;&gt;Austin Clyde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ramanathan_A/0/1/0/all/0/1&quot;&gt;Arvind Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Stevens_R/0/1/0/all/0/1&quot;&gt;Rick L. Stevens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dam_H/0/1/0/all/0/1&quot;&gt;Hubertus J. J. van Dam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Deyu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01923">
<title>An Empirical Study on Fairness Improvement with Multiple Protected Attributes. (arXiv:2308.01923v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01923</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing research mostly improves the fairness of Machine Learning (ML)
software regarding a single protected attribute at a time, but this is
unrealistic given that many users have multiple protected attributes. This
paper conducts an extensive study of fairness improvement regarding multiple
protected attributes, covering 11 state-of-the-art fairness improvement
methods. We analyze the effectiveness of these methods with different datasets,
metrics, and ML models when considering multiple protected attributes. The
results reveal that improving fairness for a single protected attribute can
largely decrease fairness regarding unconsidered protected attributes. This
decrease is observed in up to 88.3% of scenarios (57.5% on average). More
surprisingly, we find little difference in accuracy loss when considering
single and multiple protected attributes, indicating that accuracy can be
maintained in the multiple-attribute paradigm. However, the effect on precision
and recall when handling multiple protected attributes is about 5 times and 8
times that of a single attribute. This has important implications for future
fairness research: reporting only accuracy as the ML performance metric, which
is currently common in the literature, is inadequate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenpeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie M. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarro_F/0/1/0/all/0/1&quot;&gt;Federica Sarro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harman_M/0/1/0/all/0/1&quot;&gt;Mark Harman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01926">
<title>Are Easy Data Easy (for K-Means). (arXiv:2308.01926v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01926</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the capability of correctly recovering well-separated
clusters by various brands of the $k$-means algorithm. The concept of
well-separatedness used here is derived directly from the common definition of
clusters, which imposes an interplay between the requirements of
within-cluster-homogenicity and between-clusters-diversity. Conditions are
derived for a special case of well-separated clusters such that the global
minimum of $k$-means cost function coincides with the well-separatedness. An
experimental investigation is performed to find out whether or no various
brands of $k$-means are actually capable of discovering well separated
clusters. It turns out that they are not. A new algorithm is proposed that is a
variation of $k$-means++ via repeated {sub}sampling when choosing a seed. The
new algorithm outperforms four other algorithms from $k$-means family on the
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klopotek_M/0/1/0/all/0/1&quot;&gt;Mieczys&amp;#x142;aw A. K&amp;#x142;opotek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01929">
<title>A Transformer-based Prediction Method for Depth of Anesthesia During Target-controlled Infusion of Propofol and Remifentanil. (arXiv:2308.01929v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01929</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately predicting anesthetic effects is essential for target-controlled
infusion systems. The traditional (PK-PD) models for Bispectral index (BIS)
prediction require manual selection of model parameters, which can be
challenging in clinical settings. Recently proposed deep learning methods can
only capture general trends and may not predict abrupt changes in BIS. To
address these issues, we propose a transformer-based method for predicting the
depth of anesthesia (DOA) using drug infusions of propofol and remifentanil.
Our method employs long short-term memory (LSTM) and gate residual network
(GRN) networks to improve the efficiency of feature fusion and applies an
attention mechanism to discover the interactions between the drugs. We also use
label distribution smoothing and reweighting losses to address data imbalance.
Experimental results show that our proposed method outperforms traditional
PK-PD models and previous deep learning methods, effectively predicting
anesthetic depth under sudden and deep anesthesia conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongkang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Siyuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingjin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhijing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanhui Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01930">
<title>Machine Learning-Based Diabetes Detection Using Photoplethysmography Signal Features. (arXiv:2308.01930v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01930</link>
<description rdf:parseType="Literal">&lt;p&gt;Diabetes is a prevalent chronic condition that compromises the health of
millions of people worldwide. Minimally invasive methods are needed to prevent
and control diabetes but most devices for measuring glucose levels are invasive
and not amenable for continuous monitoring. Here, we present an alternative
method to overcome these shortcomings based on non-invasive optical
photoplethysmography (PPG) for detecting diabetes. We classify non-Diabetic and
Diabetic patients using the PPG signal and metadata for training Logistic
Regression (LR) and eXtreme Gradient Boosting (XGBoost) algorithms. We used PPG
signals from a publicly available dataset. To prevent overfitting, we divided
the data into five folds for cross-validation. By ensuring that patients in the
training set are not in the testing set, the model&apos;s performance can be
evaluated on unseen subjects&apos; data, providing a more accurate assessment of its
generalization. Our model achieved an F1-Score and AUC of $58.8\pm20.0\%$ and
$79.2\pm15.0\%$ for LR and $51.7\pm16.5\%$ and $73.6\pm17.0\%$ for XGBoost,
respectively. Feature analysis suggested that PPG morphological features
contains diabetes-related information alongside metadata. Our findings are
within the same range reported in the literature, indicating that machine
learning methods are promising for developing remote, non-invasive, and
continuous measurement devices for detecting and preventing diabetes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_F/0/1/0/all/0/1&quot;&gt;Filipe A. C. Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dias_F/0/1/0/all/0/1&quot;&gt;Felipe M. Dias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toledo_M/0/1/0/all/0/1&quot;&gt;Marcelo A. F. Toledo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardenas_D/0/1/0/all/0/1&quot;&gt;Diego A. C. Cardenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_D/0/1/0/all/0/1&quot;&gt;Douglas A. Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_E/0/1/0/all/0/1&quot;&gt;Estela Ribeiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krieger_J/0/1/0/all/0/1&quot;&gt;Jose E. Krieger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_M/0/1/0/all/0/1&quot;&gt;Marco A. Gutierrez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01932">
<title>Investigation on Machine Learning Based Approaches for Estimating the Critical Temperature of Superconductors. (arXiv:2308.01932v1 [cond-mat.supr-con])</title>
<link>http://arxiv.org/abs/2308.01932</link>
<description rdf:parseType="Literal">&lt;p&gt;Superconductors have been among the most fascinating substances, as the
fundamental concept of superconductivity as well as the correlation of critical
temperature and superconductive materials have been the focus of extensive
investigation since their discovery. However, superconductors at normal
temperatures have yet to be identified. Additionally, there are still many
unknown factors and gaps of understanding regarding this unique phenomenon,
particularly the connection between superconductivity and the fundamental
criteria to estimate the critical temperature. To bridge the gap, numerous
machine learning techniques have been established to estimate critical
temperatures as it is extremely challenging to determine. Furthermore, the need
for a sophisticated and feasible method for determining the temperature range
that goes beyond the scope of the standard empirical formula appears to be
strongly emphasized by various machine-learning approaches. This paper uses a
stacking machine learning approach to train itself on the complex
characteristics of superconductive materials in order to accurately predict
critical temperatures. In comparison to other previous accessible research
investigations, this model demonstrated a promising performance with an RMSE of
9.68 and an R2 score of 0.922. The findings presented here could be a viable
technique to shed new insight on the efficient implementation of the stacking
ensemble method with hyperparameter optimization (HPO).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Shams_F/0/1/0/all/0/1&quot;&gt;Fatin Abrar Shams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Ratul_R/0/1/0/all/0/1&quot;&gt;Rashed Hasan Ratul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Naf_A/0/1/0/all/0/1&quot;&gt;Ahnaf Islam Naf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Samir_S/0/1/0/all/0/1&quot;&gt;Syed Shaek Hossain Samir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Nishat_M/0/1/0/all/0/1&quot;&gt;Mirza Muntasir Nishat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Faisal_F/0/1/0/all/0/1&quot;&gt;Fahim Faisal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hoque_M/0/1/0/all/0/1&quot;&gt;Md. Ashraful Hoque&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01937">
<title>Training Data Protection with Compositional Diffusion Models. (arXiv:2308.01937v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01937</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Compartmentalized Diffusion Models (CDM), a method to train
different diffusion models (or prompts) on distinct data sources and
arbitrarily compose them at inference time. The individual models can be
trained in isolation, at different times, and on different distributions and
domains and can be later composed to achieve performance comparable to a
paragon model trained on all data simultaneously. Furthermore, each model only
contains information about the subset of the data it was exposed to during
training, enabling several forms of training data protection. In particular,
CDMs are the first method to enable both selective forgetting and continual
learning for large-scale diffusion models, as well as allowing serving
customized models based on the user&apos;s access rights. CDMs also allow
determining the importance of a subset of the data in generating particular
samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golatkar_A/0/1/0/all/0/1&quot;&gt;Aditya Golatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaminathan_A/0/1/0/all/0/1&quot;&gt;Ashwin Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01938">
<title>Online Multi-Task Learning with Recursive Least Squares and Recursive Kernel Methods. (arXiv:2308.01938v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.01938</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces two novel approaches for Online Multi-Task Learning
(MTL) Regression Problems. We employ a high performance graph-based MTL
formulation and develop its recursive versions based on the Weighted Recursive
Least Squares (WRLS) and the Online Sparse Least Squares Support Vector
Regression (OSLSSVR). Adopting task-stacking transformations, we demonstrate
the existence of a single matrix incorporating the relationship of multiple
tasks and providing structural information to be embodied by the MT-WRLS method
in its initialization procedure and by the MT-OSLSSVR in its multi-task kernel
function. Contrasting the existing literature, which is mostly based on Online
Gradient Descent (OGD) or cubic inexact approaches, we achieve exact and
approximate recursions with quadratic per-instance cost on the dimension of the
input space (MT-WRLS) or on the size of the dictionary of instances
(MT-OSLSSVR). We compare our online MTL methods to other contenders in a
real-world wind speed forecasting case study, evidencing the significant gain
in performance of both proposed approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lencione_G/0/1/0/all/0/1&quot;&gt;Gabriel R. Lencione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zuben_F/0/1/0/all/0/1&quot;&gt;Fernando J. Von Zuben&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01946">
<title>Experimental Results regarding multiple Machine Learning via Quaternions. (arXiv:2308.01946v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01946</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an experimental study on the application of quaternions
in several machine learning algorithms. Quaternion is a mathematical
representation of rotation in three-dimensional space, which can be used to
represent complex data transformations. In this study, we explore the use of
quaternions to represent and classify rotation data, using randomly generated
quaternion data and corresponding labels, converting quaternions to rotation
matrices, and using them as input features. Based on quaternions and multiple
machine learning algorithms, it has shown higher accuracy and significantly
improved performance in prediction tasks. Overall, this study provides an
empirical basis for exploiting quaternions for machine learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tianlei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Renzhe Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01947">
<title>Discriminative Graph-level Anomaly Detection via Dual-students-teacher Model. (arXiv:2308.01947v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;Different from the current node-level anomaly detection task, the goal of
graph-level anomaly detection is to find abnormal graphs that significantly
differ from others in a graph set. Due to the scarcity of research on the work
of graph-level anomaly detection, the detailed description of graph-level
anomaly is insufficient. Furthermore, existing works focus on capturing
anomalous graph information to learn better graph representations, but they
ignore the importance of an effective anomaly score function for evaluating
abnormal graphs. Thus, in this work, we first define anomalous graph
information including node and graph property anomalies in a graph set and
adopt node-level and graph-level information differences to identify them,
respectively. Then, we introduce a discriminative graph-level anomaly detection
framework with dual-students-teacher model, where the teacher model with a
heuristic loss are trained to make graph representations more divergent. Then,
two competing student models trained by normal and abnormal graphs respectively
fit graph representations of the teacher model in terms of node-level and
graph-level representation perspectives. Finally, we combine representation
errors between two student models to discriminatively distinguish anomalous
graphs. Extensive experiment analysis demonstrates that our method is effective
for the graph-level anomaly detection task on graph datasets in the real world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xuexiong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_S/0/1/0/all/0/1&quot;&gt;Shan Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_H/0/1/0/all/0/1&quot;&gt;Haonan Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01954">
<title>Bringing Chemistry to Scale: Loss Weight Adjustment for Multivariate Regression in Deep Learning of Thermochemical Processes. (arXiv:2308.01954v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01954</link>
<description rdf:parseType="Literal">&lt;p&gt;Flamelet models are widely used in computational fluid dynamics to simulate
thermochemical processes in turbulent combustion. These models typically employ
memory-expensive lookup tables that are predetermined and represent the
combustion process to be simulated. Artificial neural networks (ANNs) offer a
deep learning approach that can store this tabular data using a small number of
network weights, potentially reducing the memory demands of complex simulations
by orders of magnitude. However, ANNs with standard training losses often
struggle with underrepresented targets in multivariate regression tasks, e.g.,
when learning minor species mass fractions as part of lookup tables. This paper
seeks to improve the accuracy of an ANN when learning multiple species mass
fractions of a hydrogen (\ce{H2}) combustion lookup table. We assess a simple,
yet effective loss weight adjustment that outperforms the standard mean-squared
error optimization and enables accurate learning of all species mass fractions,
even of minor species where the standard optimization completely fails.
Furthermore, we find that the loss weight adjustment leads to more balanced
gradients in the network training, which explains its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohrhofer_F/0/1/0/all/0/1&quot;&gt;Franz M. Rohrhofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posch_S/0/1/0/all/0/1&quot;&gt;Stefan Posch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gossnitzer_C/0/1/0/all/0/1&quot;&gt;Clemens G&amp;#xf6;&amp;#xdf;nitzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Oliver_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; M. Garc&amp;#xed;a-Oliver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_B/0/1/0/all/0/1&quot;&gt;Bernhard C. Geiger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01966">
<title>DCTM: Dilated Convolutional Transformer Model for Multimodal Engagement Estimation in Conversation. (arXiv:2308.01966v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2308.01966</link>
<description rdf:parseType="Literal">&lt;p&gt;Conversational engagement estimation is posed as a regression problem,
entailing the identification of the favorable attention and involvement of the
participants in the conversation. This task arises as a crucial pursuit to gain
insights into human&apos;s interaction dynamics and behavior patterns within a
conversation. In this research, we introduce a dilated convolutional
Transformer for modeling and estimating human engagement in the MULTIMEDIATE
2023 competition. Our proposed system surpasses the baseline models, exhibiting
a noteworthy $7$\% improvement on test set and $4$\% on validation set.
Moreover, we employ different modality fusion mechanism and show that for this
type of data, a simple concatenated method with self-attention fusion gains the
best performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_V/0/1/0/all/0/1&quot;&gt;Vu Ngoc Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1&quot;&gt;Van Thong Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hyung-Jeong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaheer_M/0/1/0/all/0/1&quot;&gt;M. Zaigham Zaheer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nawaz_S/0/1/0/all/0/1&quot;&gt;Shah Nawaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandakumar_K/0/1/0/all/0/1&quot;&gt;Karthik Nandakumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Soo-Hyung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01976">
<title>Domain specificity and data efficiency in typo tolerant spell checkers: the case of search in online marketplaces. (arXiv:2308.01976v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.01976</link>
<description rdf:parseType="Literal">&lt;p&gt;Typographical errors are a major source of frustration for visitors of online
marketplaces. Because of the domain-specific nature of these marketplaces and
the very short queries users tend to search for, traditional spell cheking
solutions do not perform well in correcting typos. We present a data
augmentation method to address the lack of annotated typo data and train a
recurrent neural network to learn context-limited domain-specific embeddings.
Those embeddings are deployed in a real-time inferencing API for the Microsoft
AppSource marketplace to find the closest match between a misspelled user query
and the available product names. Our data efficient solution shows that
controlled high quality synthetic data may be a powerful tool especially
considering the current climate of large language models which rely on
prohibitively huge and often uncontrolled datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ubrangala_D/0/1/0/all/0/1&quot;&gt;Dayananda Ubrangala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_J/0/1/0/all/0/1&quot;&gt;Juhi Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondapalli_R/0/1/0/all/0/1&quot;&gt;Ravi Prasad Kondapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_K/0/1/0/all/0/1&quot;&gt;Kiran R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwala_A/0/1/0/all/0/1&quot;&gt;Amit Agarwala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boue_L/0/1/0/all/0/1&quot;&gt;Laurent Bou&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01981">
<title>CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01981</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce CartiMorph, a framework for automated knee articular cartilage
morphometrics. It takes an image as input and generates quantitative metrics
for cartilage subregions, including the percentage of full-thickness cartilage
loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the
power of deep learning models for hierarchical image feature representation.
Deep learning models were trained and validated for tissue segmentation,
template construction, and template-to-image registration. We established
methods for surface-normal-based cartilage thickness mapping, FCL estimation,
and rule-based cartilage parcellation. Our cartilage thickness map showed less
error in thin and peripheral regions. We evaluated the effectiveness of the
adopted segmentation model by comparing the quantitative metrics obtained from
model segmentation and those from manual segmentation. The root-mean-squared
deviation of the FCL measurements was less than 8%, and strong correlations
were observed for the mean thickness (Pearson&apos;s correlation coefficient $\rho
\in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in
[0.89,0.98]$) measurements. We compared our FCL measurements with those from a
previous study and found that our measurements deviated less from the ground
truths. We observed superior performance of the proposed rule-based cartilage
parcellation method compared with the atlas-based approach. CartiMorph has the
potential to promote imaging biomarkers discovery for knee osteoarthritis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yongcheng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Junru Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Sheheryar Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01994">
<title>Explainable unsupervised multi-modal image registration using deep networks. (arXiv:2308.01994v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.01994</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical decision making from magnetic resonance imaging (MRI) combines
complementary information from multiple MRI sequences (defined as
&apos;modalities&apos;). MRI image registration aims to geometrically &apos;pair&apos; diagnoses
from different modalities, time points and slices. Both intra- and
inter-modality MRI registration are essential components in clinical MRI
settings. Further, an MRI image processing pipeline that can address both afine
and non-rigid registration is critical, as both types of deformations may be
occuring in real MRI data scenarios. Unlike image classification,
explainability is not commonly addressed in image registration deep learning
(DL) methods, as it is challenging to interpet model-data behaviours against
transformation fields. To properly address this, we incorporate Grad-CAM-based
explainability frameworks in each major component of our unsupervised
multi-modal and multi-organ image registration DL methodology. We previously
demonstrated that we were able to reach superior performance (against the
current standard Syn method). In this work, we show that our DL model becomes
fully explainable, setting the framework to generalise our approach on further
medical imaging data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Papanastasiou_G/0/1/0/all/0/1&quot;&gt;Giorgos Papanastasiou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02000">
<title>On the Transition from Neural Representation to Symbolic Knowledge. (arXiv:2308.02000v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.02000</link>
<description rdf:parseType="Literal">&lt;p&gt;Bridging the huge disparity between neural and symbolic representation can
potentially enable the incorporation of symbolic thinking into neural networks
from essence. Motivated by how human gradually builds complex symbolic
representation from the prototype symbols that are learned through perception
and environmental interactions. We propose a Neural-Symbolic Transitional
Dictionary Learning (TDL) framework that employs an EM algorithm to learn a
transitional representation of data that compresses high-dimension information
of visual parts of an input into a set of tensors as neural variables and
discover the implicit predicate structure in a self-supervised way. We
implement the framework with a diffusion model by regarding the decomposition
of input as a cooperative game, then learn predicates by prototype clustering.
We additionally use RL enabled by the Markovian of diffusion models to further
tune the learned prototypes by incorporating subjective factors. Extensive
experiments on 3 abstract compositional visual objects datasets that require
the model to segment parts without any visual features like texture, color, or
shadows apart from shape and 3 neural/symbolic downstream tasks demonstrate the
learned representation enables interpretable decomposition of visual input and
smooth adaption to downstream tasks which are not available by existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junyan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_P/0/1/0/all/0/1&quot;&gt;Peter Chin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02001">
<title>Memory capacity of two layer neural networks with smooth activations. (arXiv:2308.02001v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02001</link>
<description rdf:parseType="Literal">&lt;p&gt;Determining the memory capacity of two-layer neural networks with m hidden
neurons and input dimension d (i.e., md+m total trainable parameters), which
refers to the largest size of general data the network can memorize, is a
fundamental machine-learning question. For non-polynomial real analytic
activation functions, such as sigmoids and smoothed rectified linear units
(smoothed ReLUs), we establish a lower bound of md/2 and optimality up to a
factor of approximately 2. Analogous prior results were limited to Heaviside
and ReLU activations, with results for smooth activations suffering from
logarithmic factors and requiring random data. To analyze the memory capacity,
we examine the rank of the network&apos;s Jacobian by computing the rank of matrices
involving both Hadamard powers and the Khati-Rao product. Our computation
extends classical linear algebraic facts about the rank of Hadamard powers.
Overall, our approach differs from previous works on memory capacity and holds
promise for extending to deeper models and other architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madden_L/0/1/0/all/0/1&quot;&gt;Liam Madden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02013">
<title>Federated Representation Learning for Automatic Speech Recognition. (arXiv:2308.02013v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.02013</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) is a privacy-preserving paradigm, allowing edge
devices to learn collaboratively without sharing data. Edge devices like Alexa
and Siri are prospective sources of unlabeled audio data that can be tapped to
learn robust audio representations. In this work, we bring Self-supervised
Learning (SSL) and FL together to learn representations for Automatic Speech
Recognition respecting data privacy constraints. We use the speaker and chapter
information in the unlabeled speech dataset, Libri-Light, to simulate non-IID
speaker-siloed data distributions and pre-train an LSTM encoder with the
Contrastive Predictive Coding framework with FedSGD. We show that the
pre-trained ASR encoder in FL performs as well as a centrally pre-trained model
and produces an improvement of 12-15% (WER) compared to no pre-training. We
further adapt the federated pre-trained models to a new language, French, and
show a 20% (WER) improvement over no pre-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rames_G/0/1/0/all/0/1&quot;&gt;Guruprasad V Rames&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chennupati_G/0/1/0/all/0/1&quot;&gt;Gopinath Chennupati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_M/0/1/0/all/0/1&quot;&gt;Milind Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_A/0/1/0/all/0/1&quot;&gt;Anit Kumar Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastrow_A/0/1/0/all/0/1&quot;&gt;Ariya Rastrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Droppo_J/0/1/0/all/0/1&quot;&gt;Jasha Droppo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02029">
<title>Deep Maxout Network-based Feature Fusion and Political Tangent Search Optimizer enabled Transfer Learning for Thalassemia Detection. (arXiv:2308.02029v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02029</link>
<description rdf:parseType="Literal">&lt;p&gt;Thalassemia is a heritable blood disorder which is the outcome of a genetic
defect causing lack of production of hemoglobin polypeptide chains. However,
there is less understanding of the precise frequency as well as sharing in
these areas. Knowing about the frequency of thalassemia occurrence and
dependable mutations is thus a significant step in preventing, controlling, and
treatment planning. Here, Political Tangent Search Optimizer based Transfer
Learning (PTSO_TL) is introduced for thalassemia detection. Initially, input
data obtained from a particular dataset is normalized in the data normalization
stage. Quantile normalization is utilized in the data normalization stage, and
the data are then passed to the feature fusion phase, in which Weighted
Euclidean Distance with Deep Maxout Network (DMN) is utilized. Thereafter, data
augmentation is performed using the oversampling method to increase data
dimensionality. Lastly, thalassemia detection is carried out by TL, wherein a
convolutional neural network (CNN) is utilized with hyperparameters from a
trained model such as Xception. TL is tuned by PTSO, and the training algorithm
PTSO is presented by merging of Political Optimizer (PO) and Tangent Search
Algorithm (TSA). Furthermore, PTSO_TL obtained maximal precision, recall, and
f-measure values of about 94.3%, 96.1%, and 95.2%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdalla_H/0/1/0/all/0/1&quot;&gt;Hemn Barzan Abdalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Awder Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_N/0/1/0/all/0/1&quot;&gt;Nasser Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangi_A/0/1/0/all/0/1&quot;&gt;Abdur Rashid Sangi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02031">
<title>Knowledge-enhanced Neuro-Symbolic AI for Cybersecurity and Privacy. (arXiv:2308.02031v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2308.02031</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuro-Symbolic Artificial Intelligence (AI) is an emerging and quickly
advancing field that combines the subsymbolic strengths of (deep) neural
networks and explicit, symbolic knowledge contained in knowledge graphs to
enhance explainability and safety in AI systems. This approach addresses a key
criticism of current generation systems, namely their inability to generate
human-understandable explanations for their outcomes and ensure safe behaviors,
especially in scenarios with \textit{unknown unknowns} (e.g. cybersecurity,
privacy). The integration of neural networks, which excel at exploring complex
data spaces, and symbolic knowledge graphs, which represent domain knowledge,
allows AI systems to reason, learn, and generalize in a manner understandable
to experts. This article describes how applications in cybersecurity and
privacy, two most demanding domains in terms of the need for AI to be
explainable while being highly accurate in complex environments, can benefit
from Neuro-Symbolic AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piplai_A/0/1/0/all/0/1&quot;&gt;Aritran Piplai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotal_A/0/1/0/all/0/1&quot;&gt;Anantaa Kotal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohseni_S/0/1/0/all/0/1&quot;&gt;Seyedreza Mohseni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1&quot;&gt;Manas Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sudip Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Anupam Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02034">
<title>The Growth of E-Bike Use: A Machine Learning Approach. (arXiv:2308.02034v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2308.02034</link>
<description rdf:parseType="Literal">&lt;p&gt;We present our work on electric bicycles (e-bikes) and their implications for
policymakers in the United States. E-bikes have gained significant popularity
as a fast and eco-friendly transportation option. As we strive for a
sustainable energy plan, understanding the growth and impact of e-bikes is
crucial for policymakers. Our mathematical modeling offers insights into the
value of e-bikes and their role in the future. Using an ARIMA model, a
supervised machine-learning algorithm, we predicted the growth of e-bike sales
in the U.S. Our model, trained on historical sales data from January 2006 to
December 2022, projected sales of 1.3 million units in 2025 and 2.113 million
units in 2028. To assess the factors contributing to e-bike usage, we employed
a Random Forest regression model. The most significant factors influencing
e-bike sales growth were disposable personal income and popularity.
Furthermore, we examined the environmental and health impacts of e-bikes.
Through Monte Carlo simulations, we estimated the reduction in carbon emissions
due to e-bike use and the calories burned through e-biking. Our findings
revealed that e-bike usage in the U.S. resulted in a reduction of 15,737.82
kilograms of CO2 emissions in 2022. Additionally, e-bike users burned
approximately 716,630.727 kilocalories through their activities in the same
year. Our research provides valuable insights for policymakers, emphasizing the
potential of e-bikes as a sustainable transportation solution. By understanding
the growth factors and quantifying the environmental and health benefits,
policymakers can make informed decisions about integrating e-bikes into future
energy and transportation strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Aditya Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitgopekar_S/0/1/0/all/0/1&quot;&gt;Samarth Chitgopekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Alexander Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Joseph Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Megan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grattoni_C/0/1/0/all/0/1&quot;&gt;Christopher Grattoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02035">
<title>What Twitter Data Tell Us about the Future?. (arXiv:2308.02035v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2308.02035</link>
<description rdf:parseType="Literal">&lt;p&gt;Anticipation is a fundamental human cognitive ability that involves thinking
about and living towards the future. While language markers reflect
anticipatory thinking, research on anticipation from the perspective of natural
language processing is limited. This study aims to investigate the futures
projected by futurists on Twitter and explore the impact of language cues on
anticipatory thinking among social media users. We address the research
questions of what futures Twitter&apos;s futurists anticipate and share, and how
these anticipated futures can be modeled from social data. To investigate this,
we review related works on anticipation, discuss the influence of language
markers and prestigious individuals on anticipatory thinking, and present a
taxonomy system categorizing futures into &quot;present futures&quot; and &quot;future
present&quot;. This research presents a compiled dataset of over 1 million publicly
shared tweets by future influencers and develops a scalable NLP pipeline using
SOTA models. The study identifies 15 topics from the LDA approach and 100
distinct topics from the BERTopic approach within the futurists&apos; tweets. These
findings contribute to the research on topic modelling and provide insights
into the futures anticipated by Twitter&apos;s futurists. The research demonstrates
the futurists&apos; language cues signals futures-in-the-making that enhance social
media users to anticipate their own scenarios and respond to them in present.
The fully open-sourced dataset, interactive analysis, and reproducible source
code are available for further exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landowska_A/0/1/0/all/0/1&quot;&gt;Alina Landowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robak_M/0/1/0/all/0/1&quot;&gt;Marek Robak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skorski_M/0/1/0/all/0/1&quot;&gt;Maciej Skorski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02040">
<title>Learning Regionalization within a Differentiable High-Resolution Hydrological Model using Accurate Spatial Cost Gradients. (arXiv:2308.02040v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02040</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating spatially distributed hydrological parameters in ungauged
catchments poses a challenging regionalization problem and requires imposing
spatial constraints given the sparsity of discharge data. A possible approach
is to search for a transfer function that quantitatively relates physical
descriptors to conceptual model parameters. This paper introduces a Hybrid Data
Assimilation and Parameter Regionalization (HDA-PR) approach incorporating
learnable regionalization mappings, based on either multivariate regressions or
neural networks, into a differentiable hydrological model. It enables the
exploitation of heterogeneous datasets across extensive spatio-temporal
computational domains within a high-dimensional regionalization context, using
accurate adjoint-based gradients. The inverse problem is tackled with a
multi-gauge calibration cost function accounting for information from multiple
observation sites. HDA-PR was tested on high-resolution, hourly and kilometric
regional modeling of two flash-flood-prone areas located in the South of
France. In both study areas, the median Nash-Sutcliffe efficiency (NSE) scores
ranged from 0.52 to 0.78 at pseudo-ungauged sites over calibration and
validation periods. These results highlight a strong regionalization
performance of HDA-PR, improving NSE by up to 0.57 compared to the baseline
model calibrated with lumped parameters, and achieving a performance comparable
to the reference solution obtained with local uniform calibration (median NSE
from 0.59 to 0.79). Multiple evaluation metrics based on flood-oriented
hydrological signatures are also employed to assess the accuracy and robustness
of the approach. The regionalization method is amenable to state-parameter
correction from multi-source data over a range of time scales needed for
operational data assimilation, and it is adaptable to other differentiable
geophysical models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1&quot;&gt;Ngo Nghi Truyen Huynh&lt;/a&gt; (INRAE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garambois_P/0/1/0/all/0/1&quot;&gt;Pierre-Andr&amp;#xe9; Garambois&lt;/a&gt; (INRAE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colleoni_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Colleoni&lt;/a&gt; (INRAE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renard_B/0/1/0/all/0/1&quot;&gt;Benjamin Renard&lt;/a&gt; (INRAE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roux_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;l&amp;#xe8;ne Roux&lt;/a&gt; (IMFT), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demargne_J/0/1/0/all/0/1&quot;&gt;Julie Demargne&lt;/a&gt; (HYDRIS), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javelle_P/0/1/0/all/0/1&quot;&gt;Pierre Javelle&lt;/a&gt; (INRAE)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02050">
<title>FuNToM: Functional Modeling of RF Circuits Using a Neural Network Assisted Two-Port Analysis Method. (arXiv:2308.02050v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02050</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic synthesis of analog and Radio Frequency (RF) circuits is a trending
approach that requires an efficient circuit modeling method. This is due to the
expensive cost of running a large number of simulations at each synthesis
cycle. Artificial intelligence methods are promising approaches for circuit
modeling due to their speed and relative accuracy. However, existing approaches
require a large amount of training data, which is still collected using
simulation runs. In addition, such approaches collect a whole separate dataset
for each circuit topology even if a single element is added or removed. These
matters are only exacerbated by the need for post-layout modeling simulations,
which take even longer. To alleviate these drawbacks, in this paper, we present
FuNToM, a functional modeling method for RF circuits. FuNToM leverages the
two-port analysis method for modeling multiple topologies using a single main
dataset and multiple small datasets. It also leverages neural networks which
have shown promising results in predicting the behavior of circuits. Our
results show that for multiple RF circuits, in comparison to the
state-of-the-art works, while maintaining the same accuracy, the required
training data is reduced by 2.8x - 10.9x. In addition, FuNToM needs 176.8x -
188.6x less time for collecting the training set in post-layout modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayazi_M/0/1/0/all/0/1&quot;&gt;Morteza Fayazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taba_M/0/1/0/all/0/1&quot;&gt;Morteza Tavakoli Taba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabatabavakili_A/0/1/0/all/0/1&quot;&gt;Amirata Tabatabavakili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afshari_E/0/1/0/all/0/1&quot;&gt;Ehsan Afshari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dreslinski_R/0/1/0/all/0/1&quot;&gt;Ronald Dreslinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02051">
<title>A Graphical Approach to Document Layout Analysis. (arXiv:2308.02051v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02051</link>
<description rdf:parseType="Literal">&lt;p&gt;Document layout analysis (DLA) is the task of detecting the distinct,
semantic content within a document and correctly classifying these items into
an appropriate category (e.g., text, title, figure). DLA pipelines enable users
to convert documents into structured machine-readable formats that can then be
used for many useful downstream tasks. Most existing state-of-the-art (SOTA)
DLA models represent documents as images, discarding the rich metadata
available in electronically generated PDFs. Directly leveraging this metadata,
we represent each PDF page as a structured graph and frame the DLA problem as a
graph segmentation and classification problem. We introduce the Graph-based
Layout Analysis Model (GLAM), a lightweight graph neural network competitive
with SOTA models on two challenging DLA datasets - while being an order of
magnitude smaller than existing models. In particular, the 4-million parameter
GLAM model outperforms the leading 140M+ parameter computer vision-based model
on 5 of the 11 classes on the DocLayNet dataset. A simple ensemble of these two
models achieves a new state-of-the-art on DocLayNet, increasing mAP from 76.8
to 80.8. Overall, GLAM is over 5 times more efficient than SOTA models, making
GLAM a favorable engineering choice for DLA tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jilin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krumdick_M/0/1/0/all/0/1&quot;&gt;Michael Krumdick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_B/0/1/0/all/0/1&quot;&gt;Baojia Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halim_H/0/1/0/all/0/1&quot;&gt;Hamima Halim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolov_M/0/1/0/all/0/1&quot;&gt;Maxim Sokolov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barda_V/0/1/0/all/0/1&quot;&gt;Vadym Barda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vendryes_D/0/1/0/all/0/1&quot;&gt;Delphine Vendryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanner_C/0/1/0/all/0/1&quot;&gt;Chris Tanner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02054">
<title>Robust Independence Tests with Finite Sample Guarantees for Synchronous Stochastic Linear Systems. (arXiv:2308.02054v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.02054</link>
<description rdf:parseType="Literal">&lt;p&gt;The paper introduces robust independence tests with non-asymptotically
guaranteed significance levels for stochastic linear time-invariant systems,
assuming that the observed outputs are synchronous, which means that the
systems are driven by jointly i.i.d. noises. Our method provides bounds for the
type I error probabilities that are distribution-free, i.e., the innovations
can have arbitrary distributions. The algorithm combines confidence region
estimates with permutation tests and general dependence measures, such as the
Hilbert-Schmidt independence criterion and the distance covariance, to detect
any nonlinear dependence between the observed systems. We also prove the
consistency of our hypothesis tests under mild assumptions and demonstrate the
ideas through the example of autoregressive systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tamas_A/0/1/0/all/0/1&quot;&gt;Ambrus Tam&amp;#xe1;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Balint_D/0/1/0/all/0/1&quot;&gt;D&amp;#xe1;niel &amp;#xc1;goston B&amp;#xe1;lint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Csaji_B/0/1/0/all/0/1&quot;&gt;Bal&amp;#xe1;zs Csan&amp;#xe1;d Cs&amp;#xe1;ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02055">
<title>Seasonality Based Reranking of E-commerce Autocomplete Using Natural Language Queries. (arXiv:2308.02055v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.02055</link>
<description rdf:parseType="Literal">&lt;p&gt;Query autocomplete (QAC) also known as typeahead, suggests list of complete
queries as user types prefix in the search box. It is one of the key features
of modern search engines specially in e-commerce. One of the goals of typeahead
is to suggest relevant queries to users which are seasonally important. In this
paper we propose a neural network based natural language processing (NLP)
algorithm to incorporate seasonality as a signal and present end to end
evaluation of the QAC ranking model. Incorporating seasonality into
autocomplete ranking model can improve autocomplete relevance and business
metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_P/0/1/0/all/0/1&quot;&gt;Prateek Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_A/0/1/0/all/0/1&quot;&gt;Adithya Rajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02058">
<title>Incorporating Recklessness to Collaborative Filtering based Recommender Systems. (arXiv:2308.02058v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2308.02058</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems that include some reliability measure of their
predictions tend to be more conservative in forecasting, due to their
constraint to preserve reliability. This leads to a significant drop in the
coverage and novelty that these systems can provide. In this paper, we propose
the inclusion of a new term in the learning process of matrix
factorization-based recommender systems, called recklessness, which enables the
control of the risk level desired when making decisions about the reliability
of a prediction. Experimental results demonstrate that recklessness not only
allows for risk regulation but also improves the quantity and quality of
predictions provided by the recommender system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Lopez_D/0/1/0/all/0/1&quot;&gt;Diego P&amp;#xe9;rez-L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_F/0/1/0/all/0/1&quot;&gt;Fernando Ortega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Prieto_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngel Gonz&amp;#xe1;lez-Prieto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duenas_Lerin_J/0/1/0/all/0/1&quot;&gt;Jorge Due&amp;#xf1;as-Ler&amp;#xed;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02060">
<title>Accurate Neural Network Pruning Requires Rethinking Sparse Optimization. (arXiv:2308.02060v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02060</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining versions of deep neural networks that are both highly-accurate and
highly-sparse is one of the main challenges in the area of model compression,
and several high-performance pruning techniques have been investigated by the
community. Yet, much less is known about the interaction between sparsity and
the standard stochastic optimization techniques used for training sparse
networks, and most existing work uses standard dense schedules and
hyperparameters for training sparse networks. In this work, we examine the
impact of high sparsity on model training using the standard computer vision
and natural language processing sparsity benchmarks. We begin by showing that
using standard dense training recipes for sparse training is suboptimal, and
results in under-training. We provide new approaches for mitigating this issue
for both sparse pre-training of vision models (e.g. ResNet50/ImageNet) and
sparse fine-tuning of language models (e.g. BERT/GLUE), achieving
state-of-the-art results in both settings in the high-sparsity regime, and
providing detailed analyses for the difficulty of sparse training in both
scenarios. Our work sets a new threshold in terms of the accuracies that can be
achieved under high sparsity, and should inspire further research into
improving sparse model training, to reach higher accuracies under high
sparsity, but also to do so efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuznedelev_D/0/1/0/all/0/1&quot;&gt;Denis Kuznedelev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurtic_E/0/1/0/all/0/1&quot;&gt;Eldar Kurtic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iofinova_E/0/1/0/all/0/1&quot;&gt;Eugenia Iofinova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frantar_E/0/1/0/all/0/1&quot;&gt;Elias Frantar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peste_A/0/1/0/all/0/1&quot;&gt;Alexandra Peste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02065">
<title>On the Biometric Capacity of Generative Face Models. (arXiv:2308.02065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02065</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been tremendous progress in generating realistic faces with high
fidelity over the past few years. Despite this progress, a crucial question
remains unanswered: &quot;Given a generative face model, how many unique identities
can it generate?&quot; In other words, what is the biometric capacity of the
generative face model? A scientific basis for answering this question will
benefit evaluating and comparing different generative face models and establish
an upper bound on their scalability. This paper proposes a statistical approach
to estimate the biometric capacity of generated face images in a hyperspherical
feature space. We employ our approach on multiple generative models, including
unconditional generators like StyleGAN, Latent Diffusion Model, and &quot;Generated
Photos,&quot; as well as DCFace, a class-conditional generator. We also estimate
capacity w.r.t. demographic attributes such as gender and age. Our capacity
estimates indicate that (a) under ArcFace representation at a false acceptance
rate (FAR) of 0.1%, StyleGAN3 and DCFace have a capacity upper bound of
$1.43\times10^6$ and $1.190\times10^4$, respectively; (b) the capacity reduces
drastically as we lower the desired FAR with an estimate of $1.796\times10^4$
and $562$ at FAR of 1% and 10%, respectively, for StyleGAN3; (c) there is no
discernible disparity in the capacity w.r.t gender; and (d) for some generative
models, there is an appreciable disparity in the capacity w.r.t age. Code is
available at https://github.com/human-analysis/capacity-generative-face-models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1&quot;&gt;Vishnu Naresh Boddeti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreekumar_G/0/1/0/all/0/1&quot;&gt;Gautam Sreekumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02066">
<title>Mitigating Task Interference in Multi-Task Learning via Explicit Task Routing with Non-Learnable Primitives. (arXiv:2308.02066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.02066</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-task learning (MTL) seeks to learn a single model to accomplish
multiple tasks by leveraging shared information among the tasks. Existing MTL
models, however, have been known to suffer from negative interference among
tasks. Efforts to mitigate task interference have focused on either
loss/gradient balancing or implicit parameter partitioning with partial
overlaps among the tasks. In this paper, we propose ETR-NLP to mitigate task
interference through a synergistic combination of non-learnable primitives
(NLPs) and explicit task routing (ETR). Our key idea is to employ non-learnable
primitives to extract a diverse set of task-agnostic features and recombine
them into a shared branch common to all tasks and explicit task-specific
branches reserved for each task. The non-learnable primitives and the explicit
decoupling of learnable parameters into shared and task-specific ones afford
the flexibility needed for minimizing task interference. We evaluate the
efficacy of ETR-NLP networks for both image-level classification and
pixel-level dense prediction MTL problems. Experimental results indicate that
ETR-NLP significantly outperforms state-of-the-art baselines with fewer
learnable parameters and similar FLOPs across all datasets. Code is available
at this \href{https://github.com/zhichao-lu/etr-nlp-mtl}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Chuntao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhichao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ran Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boddeti_V/0/1/0/all/0/1&quot;&gt;Vishnu Naresh Boddeti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02068">
<title>Specious Sites: Tracking the Spread and Sway of Spurious News Stories at Scale. (arXiv:2308.02068v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2308.02068</link>
<description rdf:parseType="Literal">&lt;p&gt;Misinformation, propaganda, and outright lies proliferate on the web, with
some narratives having dangerous real-world consequences on public health,
elections, and individual safety. However, despite the impact of
misinformation, the research community largely lacks automated and programmatic
approaches for tracking news narratives across online platforms. In this work,
utilizing daily scrapes of 1,404 unreliable news websites, the large-language
model MPNet, and DP-Means clustering, we introduce a system to automatically
isolate and analyze the narratives spread within online ecosystems. Identifying
55,301 narratives on these 1,404 websites, we describe the most prevalent
narratives spread in 2022 and identify the most influential websites that
originate and magnify narratives. Finally, we show how our system can be
utilized to detect new narratives originating from unreliable news websites and
aid fact-checkers like Politifact, Reuters, and AP News in more quickly
addressing misinformation stories.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanley_H/0/1/0/all/0/1&quot;&gt;Hans W. A. Hanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Deepak Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durumeric_Z/0/1/0/all/0/1&quot;&gt;Zakir Durumeric&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02080">
<title>Causality Guided Disentanglement for Cross-Platform Hate Speech Detection. (arXiv:2308.02080v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.02080</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media platforms, despite their value in promoting open discourse, are
often exploited to spread harmful content. Current deep learning and natural
language processing models used for detecting this harmful content overly rely
on domain-specific terms affecting their capabilities to adapt to generalizable
hate speech detection. This is because they tend to focus too narrowly on
particular linguistic signals or the use of certain categories of words.
Another significant challenge arises when platforms lack high-quality annotated
data for training, leading to a need for cross-platform models that can adapt
to different distribution shifts. Our research introduces a cross-platform hate
speech detection model capable of being trained on one platform&apos;s data and
generalizing to multiple unseen platforms. To achieve good generalizability
across platforms, one way is to disentangle the input representations into
invariant and platform-dependent features. We also argue that learning causal
relationships, which remain constant across diverse environments, can
significantly aid in understanding invariant representations in hate speech. By
disentangling input into platform-dependent features (useful for predicting
hate targets) and platform-independent features (used to predict the presence
of hate), we learn invariant representations resistant to distribution shifts.
These features are then used to predict hate speech across unseen platforms.
Our extensive experiments across four platforms highlight our model&apos;s enhanced
efficacy compared to existing state-of-the-art methods in detecting generalized
hate speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_P/0/1/0/all/0/1&quot;&gt;Paras Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumarage_T/0/1/0/all/0/1&quot;&gt;Tharindu Kumarage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moraffah_R/0/1/0/all/0/1&quot;&gt;Raha Moraffah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02081">
<title>Target specification bias, counterfactual prediction, and algorithmic fairness in healthcare. (arXiv:2308.02081v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02081</link>
<description rdf:parseType="Literal">&lt;p&gt;Bias in applications of machine learning (ML) to healthcare is usually
attributed to unrepresentative or incomplete data, or to underlying health
disparities. This article identifies a more pervasive source of bias that
affects the clinical utility of ML-enabled prediction tools: target
specification bias. Target specification bias arises when the
operationalization of the target variable does not match its definition by
decision makers. The mismatch is often subtle, and stems from the fact that
decision makers are typically interested in predicting the outcomes of
counterfactual, rather than actual, healthcare scenarios. Target specification
bias persists independently of data limitations and health disparities. When
left uncorrected, it gives rise to an overestimation of predictive accuracy, to
inefficient utilization of medical resources, and to suboptimal decisions that
can harm patients. Recent work in metrology - the science of measurement -
suggests ways of counteracting target specification bias and avoiding its
harmful consequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tal_E/0/1/0/all/0/1&quot;&gt;Eran Tal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02084">
<title>Efficient Model Adaptation for Continual Learning at the Edge. (arXiv:2308.02084v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02084</link>
<description rdf:parseType="Literal">&lt;p&gt;Most machine learning (ML) systems assume stationary and matching data
distributions during training and deployment. This is often a false assumption.
When ML models are deployed on real devices, data distributions often shift
over time due to changes in environmental factors, sensor characteristics, and
task-of-interest. While it is possible to have a human-in-the-loop to monitor
for distribution shifts and engineer new architectures in response to these
shifts, such a setup is not cost-effective. Instead, non-stationary automated
ML (AutoML) models are needed. This paper presents the
Encoder-Adaptor-Reconfigurator (EAR) framework for efficient continual learning
under domain shifts. The EAR framework uses a fixed deep neural network (DNN)
feature encoder and trains shallow networks on top of the encoder to handle
novel data. The EAR framework is capable of 1) detecting when new data is
out-of-distribution (OOD) by combining DNNs with hyperdimensional computing
(HDC), 2) identifying low-parameter neural adaptors to adapt the model to the
OOD data using zero-shot neural architecture search (ZS-NAS), and 3) minimizing
catastrophic forgetting on previous tasks by progressively growing the neural
architecture as needed and dynamically routing data through the appropriate
adaptors and reconfigurators for handling domain-incremental and
class-incremental continual learning. We systematically evaluate our approach
on several benchmark datasets for domain adaptation and demonstrate strong
performance compared to state-of-the-art algorithms for OOD detection and
few-/zero-shot NAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daniels_Z/0/1/0/all/0/1&quot;&gt;Zachary A. Daniels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomnitz_M/0/1/0/all/0/1&quot;&gt;Michael Lomnitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_P/0/1/0/all/0/1&quot;&gt;Phil Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_A/0/1/0/all/0/1&quot;&gt;Aswin Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Joe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piacentino_M/0/1/0/all/0/1&quot;&gt;Michael Piacentino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02101">
<title>Breast Ultrasound Tumor Classification Using a Hybrid Multitask CNN-Transformer Network. (arXiv:2308.02101v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.02101</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing global contextual information plays a critical role in breast
ultrasound (BUS) image classification. Although convolutional neural networks
(CNNs) have demonstrated reliable performance in tumor classification, they
have inherent limitations for modeling global and long-range dependencies due
to the localized nature of convolution operations. Vision Transformers have an
improved capability of capturing global contextual information but may distort
the local image patterns due to the tokenization operations. In this study, we
proposed a hybrid multitask deep neural network called Hybrid-MT-ESTAN,
designed to perform BUS tumor classification and segmentation using a hybrid
architecture composed of CNNs and Swin Transformer components. The proposed
approach was compared to nine BUS classification methods and evaluated using
seven quantitative metrics on a dataset of 3,320 BUS images. The results
indicate that Hybrid-MT-ESTAN achieved the highest accuracy, sensitivity, and
F1 score of 82.7%, 86.4%, and 86.0%, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shareef_B/0/1/0/all/0/1&quot;&gt;Bryar Shareef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xian_M/0/1/0/all/0/1&quot;&gt;Min Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vakanski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Vakanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haotian Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02117">
<title>VQGraph: Graph Vector-Quantization for Bridging GNNs and MLPs. (arXiv:2308.02117v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02117</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) conduct message passing which aggregates local
neighbors to update node representations. Such message passing leads to
scalability issues in practical latency-constrained applications. To address
this issue, recent methods adopt knowledge distillation (KD) to learn
computationally-efficient multi-layer perceptron (MLP) by mimicking the output
of GNN. However, the existing GNN representation space may not be expressive
enough for representing diverse local structures of the underlying graph, which
limits the knowledge transfer from GNN to MLP. Here we present a novel
framework VQGraph to learn a powerful graph representation space for bridging
GNNs and MLPs. We adopt the encoder of a variant of a vector-quantized
variational autoencoder (VQ-VAE) as a structure-aware graph tokenizer, which
explicitly represents the nodes of diverse local structures as numerous
discrete tokens and constitutes a meaningful codebook. Equipped with the
learned codebook, we propose a new token-based distillation objective based on
soft token assignments to sufficiently transfer the structural knowledge from
GNN to MLP. Extensive experiments and analyses demonstrate the strong
performance of VQGraph, where we achieve new state-of-the-art performance on
GNN-MLP distillation in both transductive and inductive settings across seven
graph datasets. We show that VQGraph with better performance infers faster than
GNNs by 828x, and also achieves accuracy improvement over GNNs and stand-alone
MLPs by 3.90% and 28.05% on average, respectively. Code:
https://github.com/YangLing0818/VQGraph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Ling Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minkai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shenda Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_W/0/1/0/all/0/1&quot;&gt;Wei Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wentao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Bin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leskovec_J/0/1/0/all/0/1&quot;&gt;Jure Leskovec&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02121">
<title>Model Provenance via Model DNA. (arXiv:2308.02121v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02121</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the life cycle of the machine learning (ML) model is an
intriguing area of research (e.g., understanding where the model comes from,
how it is trained, and how it is used). This paper focuses on a novel problem
within this field, namely Model Provenance (MP), which concerns the
relationship between a target model and its pre-training model and aims to
determine whether a source model serves as the provenance for a target model.
This is an important problem that has significant implications for ensuring the
security and intellectual property of machine learning models but has not
received much attention in the literature. To fill in this gap, we introduce a
novel concept of Model DNA which represents the unique characteristics of a
machine learning model. We utilize a data-driven and model-driven
representation learning method to encode the model&apos;s training data and
input-output information as a compact and comprehensive representation (i.e.,
DNA) of the model. Using this model DNA, we develop an efficient framework for
model provenance identification, which enables us to identify whether a source
model is a pre-training model of a target model. We conduct evaluations on both
computer vision and natural language processing tasks using various models,
datasets, and scenarios to demonstrate the effectiveness of our approach in
accurately identifying model provenance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_X/0/1/0/all/0/1&quot;&gt;Xin Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yehong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yang Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yue Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02123">
<title>Eva: A General Vectorized Approximation Framework for Second-order Optimization. (arXiv:2308.02123v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02123</link>
<description rdf:parseType="Literal">&lt;p&gt;Second-order optimization algorithms exhibit excellent convergence properties
for training deep learning models, but often incur significant computation and
memory overheads. This can result in lower training efficiency than the
first-order counterparts such as stochastic gradient descent (SGD). In this
work, we present a memory- and time-efficient second-order algorithm named Eva
with two novel techniques: 1) we construct the second-order information with
the Kronecker factorization of small stochastic vectors over a mini-batch of
training data to reduce memory consumption, and 2) we derive an efficient
update formula without explicitly computing the inverse of matrices using the
Sherman-Morrison formula. We further extend Eva to a general vectorized
approximation framework to improve the compute and memory efficiency of two
existing second-order algorithms (FOOF and Shampoo) without affecting their
convergence performance. Extensive experimental results on different models and
datasets show that Eva reduces the end-to-end training time up to 2.05x and
2.42x compared to first-order SGD and second-order algorithms (K-FAC and
Shampoo), respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shaohuai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02137">
<title>Learning the solution operator of two-dimensional incompressible Navier-Stokes equations using physics-aware convolutional neural networks. (arXiv:2308.02137v1 [math.NA])</title>
<link>http://arxiv.org/abs/2308.02137</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the concept of introducing physics to machine learning has
become widely popular. Most physics-inclusive ML-techniques however are still
limited to a single geometry or a set of parametrizable geometries. Thus, there
remains the need to train a new model for a new geometry, even if it is only
slightly modified. With this work we introduce a technique with which it is
possible to learn approximate solutions to the steady-state Navier--Stokes
equations in varying geometries without the need of parametrization. This
technique is based on a combination of a U-Net-like CNN and well established
discretization methods from the field of the finite difference method.The
results of our physics-aware CNN are compared to a state-of-the-art data-based
approach. Additionally, it is also shown how our approach performs when
combined with the data-based approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grimm_V/0/1/0/all/0/1&quot;&gt;Viktor Grimm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Heinlein_A/0/1/0/all/0/1&quot;&gt;Alexander Heinlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Klawonn_A/0/1/0/all/0/1&quot;&gt;Axel Klawonn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02145">
<title>Optimization on Pareto sets: On a theory of multi-objective optimization. (arXiv:2308.02145v1 [math.OC])</title>
<link>http://arxiv.org/abs/2308.02145</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-objective optimization, a single decision vector must balance the
trade-offs between many objectives. Solutions achieving an optimal trade-off
are said to be Pareto optimal: these are decision vectors for which improving
any one objective must come at a cost to another. But as the set of Pareto
optimal vectors can be very large, we further consider a more practically
significant Pareto-constrained optimization problem, where the goal is to
optimize a preference function constrained to the Pareto set.
&lt;/p&gt;
&lt;p&gt;We investigate local methods for solving this constrained optimization
problem, which poses significant challenges because the constraint set is (i)
implicitly defined, and (ii) generally non-convex and non-smooth, even when the
objectives are. We define notions of optimality and stationarity, and provide
an algorithm with a last-iterate convergence rate of $O(K^{-1/2})$ to
stationarity when the objectives are strongly convex and Lipschitz smooth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Roy_A/0/1/0/all/0/1&quot;&gt;Abhishek Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+So_G/0/1/0/all/0/1&quot;&gt;Geelon So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yi-An Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02157">
<title>Improved Order Analysis and Design of Exponential Integrator for Diffusion Models Sampling. (arXiv:2308.02157v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02157</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient differential equation solvers have significantly reduced the
sampling time of diffusion models (DMs) while retaining high sampling quality.
Among these solvers, exponential integrators (EI) have gained prominence by
demonstrating state-of-the-art performance. However, existing high-order
EI-based sampling algorithms rely on degenerate EI solvers, resulting in
inferior error bounds and reduced accuracy in contrast to the theoretically
anticipated results under optimal settings. This situation makes the sampling
quality extremely vulnerable to seemingly innocuous design choices such as
timestep schedules. For example, an inefficient timestep scheduler might
necessitate twice the number of steps to achieve a quality comparable to that
obtained through carefully optimized timesteps. To address this issue, we
reevaluate the design of high-order differential solvers for DMs. Through a
thorough order analysis, we reveal that the degeneration of existing high-order
EI solvers can be attributed to the absence of essential order conditions. By
reformulating the differential equations in DMs and capitalizing on the theory
of exponential integrators, we propose refined EI solvers that fulfill all the
order conditions, which we designate as Refined Exponential Solver (RES).
Utilizing these improved solvers, RES exhibits more favorable error bounds
theoretically and achieves superior sampling efficiency and stability in
practical applications. For instance, a simple switch from the single-step
DPM-Solver++ to our order-satisfied RES solver when Number of Function
Evaluations (NFE) $=9$, results in a reduction of numerical defects by $25.2\%$
and FID improvement of $25.4\%$ (16.77 vs 12.51) on a pre-trained ImageNet
diffusion model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongxin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02160">
<title>Speaker Diarization of Scripted Audiovisual Content. (arXiv:2308.02160v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.02160</link>
<description rdf:parseType="Literal">&lt;p&gt;The media localization industry usually requires a verbatim script of the
final film or TV production in order to create subtitles or dubbing scripts in
a foreign language. In particular, the verbatim script (i.e. as-broadcast
script) must be structured into a sequence of dialogue lines each including
time codes, speaker name and transcript. Current speech recognition technology
alleviates the transcription step. However, state-of-the-art speaker
diarization models still fall short on TV shows for two main reasons: (i) their
inability to track a large number of speakers, (ii) their low accuracy in
detecting frequent speaker changes. To mitigate this problem, we present a
novel approach to leverage production scripts used during the shooting process,
to extract pseudo-labeled data for the speaker diarization task. We propose a
novel semi-supervised approach and demonstrate improvements of 51.7% relative
to two unsupervised baseline models on our metrics on a 66 show test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virkar_Y/0/1/0/all/0/1&quot;&gt;Yogesh Virkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thompson_B/0/1/0/all/0/1&quot;&gt;Brian Thompson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paturi_R/0/1/0/all/0/1&quot;&gt;Rohit Paturi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srinivasan_S/0/1/0/all/0/1&quot;&gt;Sundararajan Srinivasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Federico_M/0/1/0/all/0/1&quot;&gt;Marcello Federico&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02165">
<title>Diffusion probabilistic models enhance variational autoencoder for crystal structure generative modeling. (arXiv:2308.02165v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02165</link>
<description rdf:parseType="Literal">&lt;p&gt;The crystal diffusion variational autoencoder (CDVAE) is a machine learning
model that leverages score matching to generate realistic crystal structures
that preserve crystal symmetry. In this study, we leverage novel diffusion
probabilistic (DP) models to denoise atomic coordinates rather than adopting
the standard score matching approach in CDVAE. Our proposed DP-CDVAE model can
reconstruct and generate crystal structures whose qualities are statistically
comparable to those of the original CDVAE. Furthermore, notably, when comparing
the carbon structures generated by the DP-CDVAE model with relaxed structures
obtained from density functional theory calculations, we find that the DP-CDVAE
generated structures are remarkably closer to their respective ground states.
The energy differences between these structures and the true ground states are,
on average, 68.1 meV/atom lower than those generated by the original CDVAE.
This significant improvement in the energy accuracy highlights the
effectiveness of the DP-CDVAE model in generating crystal structures that
better represent their ground-state configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pakornchote_T/0/1/0/all/0/1&quot;&gt;Teerachote Pakornchote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choomphon_anomakhun_N/0/1/0/all/0/1&quot;&gt;Natthaphon Choomphon-anomakhun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arrerut_S/0/1/0/all/0/1&quot;&gt;Sorrjit Arrerut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atthapak_C/0/1/0/all/0/1&quot;&gt;Chayanon Atthapak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khamkaeo_S/0/1/0/all/0/1&quot;&gt;Sakarn Khamkaeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chotibut_T/0/1/0/all/0/1&quot;&gt;Thiparat Chotibut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovornratanaraks_T/0/1/0/all/0/1&quot;&gt;Thiti Bovornratanaraks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02180">
<title>Scaling Clinical Trial Matching Using Large Language Models: A Case Study in Oncology. (arXiv:2308.02180v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.02180</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical trial matching is a key process in health delivery and discovery. In
practice, it is plagued by overwhelming unstructured data and unscalable manual
processing. In this paper, we conduct a systematic study on scaling clinical
trial matching using large language models (LLMs), with oncology as the focus
area. Our study is grounded in a clinical trial matching system currently in
test deployment at a large U.S. health network. Initial findings are promising:
out of box, cutting-edge LLMs, such as GPT-4, can already structure elaborate
eligibility criteria of clinical trials and extract complex matching logic
(e.g., nested AND/OR/NOT). While still far from perfect, LLMs substantially
outperform prior strong baselines and may serve as a preliminary solution to
help triage patient-trial candidates with humans in the loop. Our study also
reveals a few significant growth areas for applying LLMs to end-to-end clinical
trial matching, such as context limitation and accuracy, especially in
structuring patient information from longitudinal medical records.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Cliff Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Sheng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yu Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moung_C/0/1/0/all/0/1&quot;&gt;Christine Moung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abel_J/0/1/0/all/0/1&quot;&gt;Jacob Abel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usuyama_N/0/1/0/all/0/1&quot;&gt;Naoto Usuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weerasinghe_R/0/1/0/all/0/1&quot;&gt;Roshanthi Weerasinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piening_B/0/1/0/all/0/1&quot;&gt;Brian Piening&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumann_T/0/1/0/all/0/1&quot;&gt;Tristan Naumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bifulco_C/0/1/0/all/0/1&quot;&gt;Carlo Bifulco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poon_H/0/1/0/all/0/1&quot;&gt;Hoifung Poon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02182">
<title>AutoML4ETC: Automated Neural Architecture Search for Real-World Encrypted Traffic Classification. (arXiv:2308.02182v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2308.02182</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) has been successfully applied to encrypted network traffic
classification in experimental settings. However, in production use, it has
been shown that a DL classifier&apos;s performance inevitably decays over time.
Re-training the model on newer datasets has been shown to only partially
improve its performance. Manually re-tuning the model architecture to meet the
performance expectations on newer datasets is time-consuming and requires
domain expertise. We propose AutoML4ETC, a novel tool to automatically design
efficient and high-performing neural architectures for encrypted traffic
classification. We define a novel, powerful search space tailored specifically
for the near real-time classification of encrypted traffic using packet header
bytes. We show that with different search strategies over our search space,
AutoML4ETC generates neural architectures that outperform the state-of-the-art
encrypted traffic classifiers on several datasets, including public benchmark
datasets and real-world TLS and QUIC traffic collected from the Orange mobile
network. In addition to being more accurate, AutoML4ETC&apos;s architectures are
significantly more efficient and lighter in terms of the number of parameters.
Finally, we make AutoML4ETC publicly available for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malekghaini_N/0/1/0/all/0/1&quot;&gt;Navid Malekghaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbari_E/0/1/0/all/0/1&quot;&gt;Elham Akbari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salahuddin_M/0/1/0/all/0/1&quot;&gt;Mohammad A. Salahuddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Limam_N/0/1/0/all/0/1&quot;&gt;Noura Limam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutaba_R/0/1/0/all/0/1&quot;&gt;Raouf Boutaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathieu_B/0/1/0/all/0/1&quot;&gt;Bertrand Mathieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moteau_S/0/1/0/all/0/1&quot;&gt;Stephanie Moteau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuffin_S/0/1/0/all/0/1&quot;&gt;Stephane Tuffin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02199">
<title>A Survey of Spanish Clinical Language Models. (arXiv:2308.02199v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.02199</link>
<description rdf:parseType="Literal">&lt;p&gt;This survey focuses in encoder Language Models for solving tasks in the
clinical domain in the Spanish language. We review the contributions of 17
corpora focused mainly in clinical tasks, then list the most relevant Spanish
Language Models and Spanish Clinical Language models. We perform a thorough
comparison of these models by benchmarking them over a curated subset of the
available corpora, in order to find the best-performing ones; in total more
than 3000 models were fine-tuned for this study. All the tested corpora and the
best models are made publically available in an accessible way, so that the
results can be reproduced by independent teams or challenged in the future when
new Spanish Clinical Language models are created.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subies_G/0/1/0/all/0/1&quot;&gt;Guillem Garc&amp;#xed;a Subies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro Barbero Jim&amp;#xe9;nez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1&quot;&gt;Paloma Mart&amp;#xed;nez Fern&amp;#xe1;ndez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02221">
<title>Likelihood-ratio-based confidence intervals for neural networks. (arXiv:2308.02221v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.02221</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a first implementation of a novel
likelihood-ratio-based approach for constructing confidence intervals for
neural networks. Our method, called DeepLR, offers several qualitative
advantages: most notably, the ability to construct asymmetric intervals that
expand in regions with a limited amount of data, and the inherent incorporation
of factors such as the amount of training time, network architecture, and
regularization techniques. While acknowledging that the current implementation
of the method is prohibitively expensive for many deep-learning applications,
the high cost may already be justified in specific fields like medical
predictions or astrophysics, where a reliable uncertainty estimate for a single
prediction is essential. This work highlights the significant potential of a
likelihood-ratio-based uncertainty estimate and establishes a promising avenue
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sluijterman_L/0/1/0/all/0/1&quot;&gt;Laurens Sluijterman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cator_E/0/1/0/all/0/1&quot;&gt;Eric Cator&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heskes_T/0/1/0/all/0/1&quot;&gt;Tom Heskes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02233">
<title>Self-Normalizing Neural Network, Enabling One Shot Transfer Learning for Modeling EDFA Wavelength Dependent Gain. (arXiv:2308.02233v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2308.02233</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel ML framework for modeling the wavelength-dependent gain of
multiple EDFAs, based on semi-supervised, self-normalizing neural networks,
enabling one-shot transfer learning. Our experiments on 22 EDFAs in Open
Ireland and COSMOS testbeds show high-accuracy transfer-learning even when
operated across different amplifier types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_A/0/1/0/all/0/1&quot;&gt;Agastya Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slyne_F/0/1/0/all/0/1&quot;&gt;Frank Slyne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tingjun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kilper_D/0/1/0/all/0/1&quot;&gt;Dan Kilper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruffini_M/0/1/0/all/0/1&quot;&gt;Marco Ruffini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02249">
<title>Finding Tori: Self-supervised Learning for Analyzing Korean Folk Song. (arXiv:2308.02249v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2308.02249</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a computational analysis of the field recording
dataset of approximately 700 hours of Korean folk songs, which were recorded
around 1980-90s. Because most of the songs were sung by non-expert musicians
without accompaniment, the dataset provides several challenges. To address this
challenge, we utilized self-supervised learning with convolutional neural
network based on pitch contour, then analyzed how the musical concept of tori,
a classification system defined by a specific scale, ornamental notes, and an
idiomatic melodic contour, is captured by the model. The experimental result
shows that our approach can better capture the characteristics of tori compared
to traditional pitch histograms. Using our approaches, we have examined how
musical discussions proposed in existing academia manifest in the actual field
recordings of Korean folk songs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Danbinaerin Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Repetto_R/0/1/0/all/0/1&quot;&gt;Rafael Caro Repetto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_D/0/1/0/all/0/1&quot;&gt;Dasaem Jeong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02261">
<title>Adaptive Proximal Gradient Method for Convex Optimization. (arXiv:2308.02261v1 [math.OC])</title>
<link>http://arxiv.org/abs/2308.02261</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore two fundamental first-order algorithms in convex
optimization, namely, gradient descent (GD) and proximal gradient method
(ProxGD). Our focus is on making these algorithms entirely adaptive by
leveraging local curvature information of smooth functions. We propose adaptive
versions of GD and ProxGD that are based on observed gradient differences and,
thus, have no added computational costs. Moreover, we prove convergence of our
methods assuming only local Lipschitzness of the gradient. In addition, the
proposed versions allow for even larger stepsizes than those initially
suggested in [MM20].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Malitsky_Y/0/1/0/all/0/1&quot;&gt;Yura Malitsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mishchenko_K/0/1/0/all/0/1&quot;&gt;Konstantin Mishchenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02282">
<title>DIVERSIFY: A General Framework for Time Series Out-of-distribution Detection and Generalization. (arXiv:2308.02282v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02282</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series remains one of the most challenging modalities in machine
learning research. The out-of-distribution (OOD) detection and generalization
on time series tend to suffer due to its non-stationary property, i.e., the
distribution changes over time. The dynamic distributions inside time series
pose great challenges to existing algorithms to identify invariant
distributions since they mainly focus on the scenario where the domain
information is given as prior knowledge. In this paper, we attempt to exploit
subdomains within a whole dataset to counteract issues induced by
non-stationary for generalized representation learning. We propose DIVERSIFY, a
general framework, for OOD detection and generalization on dynamic
distributions of time series. DIVERSIFY takes an iterative process: it first
obtains the &quot;worst-case&quot; latent distribution scenario via adversarial training,
then reduces the gap between these latent distributions. We implement DIVERSIFY
via combining existing OOD detection methods according to either extracted
features or outputs of models for detection while we also directly utilize
outputs for classification. In addition, theoretical insights illustrate that
DIVERSIFY is theoretically supported. Extensive experiments are conducted on
seven datasets with different OOD settings across gesture recognition, speech
commands recognition, wearable stress and affect detection, and sensor-based
human activity recognition. Qualitative and quantitative results demonstrate
that DIVERSIFY learns more generalized features and significantly outperforms
other baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinwei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiqiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_X/0/1/0/all/0/1&quot;&gt;Xiangyang Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02287">
<title>Frustratingly Easy Model Generalization by Dummy Risk Minimization. (arXiv:2308.02287v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02287</link>
<description rdf:parseType="Literal">&lt;p&gt;Empirical risk minimization (ERM) is a fundamental machine learning paradigm.
However, its generalization ability is limited in various tasks. In this paper,
we devise Dummy Risk Minimization (DuRM), a frustratingly easy and general
technique to improve the generalization of ERM. DuRM is extremely simple to
implement: just enlarging the dimension of the output logits and then
optimizing using standard gradient descent. Moreover, we validate the efficacy
of DuRM on both theoretical and empirical analysis. Theoretically, we show that
DuRM derives greater variance of the gradient, which facilitates model
generalization by observing better flat local minima. Empirically, we conduct
evaluations of DuRM across different datasets, modalities, and network
architectures on diverse tasks, including conventional classification, semantic
segmentation, out-of-distribution generalization, adverserial training, and
long-tailed recognition. Results demonstrate that DuRM could consistently
improve the performance under all tasks with an almost free lunch manner.
Furthermore, we show that DuRM is compatible with existing generalization
techniques and we discuss possible limitations. We hope that DuRM could trigger
new interest in the fundamental research on risk minimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Juncheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xixu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shujun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02293">
<title>A stochastic optimization approach to train non-linear neural networks with regularization of higher-order total variation. (arXiv:2308.02293v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2308.02293</link>
<description rdf:parseType="Literal">&lt;p&gt;While highly expressive parametric models including deep neural networks have
an advantage to model complicated concepts, training such highly non-linear
models is known to yield a high risk of notorious overfitting. To address this
issue, this study considers a $k$th order total variation ($k$-TV)
regularization, which is defined as the squared integral of the $k$th order
derivative of the parametric models to be trained; penalizing the $k$-TV is
expected to yield a smoother function, which is expected to avoid overfitting.
While the $k$-TV terms applied to general parametric models are computationally
intractable due to the integration, this study provides a stochastic
optimization algorithm, that can efficiently train general models with the
$k$-TV regularization without conducting explicit numerical integration. The
proposed approach can be applied to the training of even deep neural networks
whose structure is arbitrary, as it can be implemented by only a simple
stochastic gradient descent algorithm and automatic differentiation. Our
numerical experiments demonstrate that the neural networks trained with the
$K$-TV terms are more ``resilient&apos;&apos; than those with the conventional parameter
regularization. The proposed algorithm also can be extended to the
physics-informed training of neural networks (PINNs).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Okuno_A/0/1/0/all/0/1&quot;&gt;Akifumi Okuno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02335">
<title>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02335</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph classification is a crucial task in many real-world multimedia
applications, where graphs can represent various multimedia data types such as
images, videos, and social networks. Previous efforts have applied graph neural
networks (GNNs) in balanced situations where the class distribution is
balanced. However, real-world data typically exhibit long-tailed class
distributions, resulting in a bias towards the head classes when using GNNs and
limited generalization ability over the tail classes. Recent approaches mainly
focus on re-balancing different classes during model training, which fails to
explicitly introduce new knowledge and sacrifices the performance of the head
classes. To address these drawbacks, we propose a novel framework called
Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature
extractor and an unbiased classifier in a decoupled manner. In the feature
extractor training stage, we develop a graph retrieval module to search for
relevant graphs that directly enrich the intra-class diversity for the tail
classes. Moreover, we innovatively optimize a category-centered supervised
contrastive loss to obtain discriminative representations, which is more
suitable for long-tailed scenarios. In the classifier fine-tuning stage, we
balance the classifier weights with two weight regularization techniques, i.e.,
Max-norm and weight decay. Experiments on various popular benchmarks verify the
superiority of the proposed method against state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1&quot;&gt;Wei Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yifang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02344">
<title>Learning Networks from Gaussian Graphical Models and Gaussian Free Fields. (arXiv:2308.02344v1 [math.ST])</title>
<link>http://arxiv.org/abs/2308.02344</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of estimating the structure of a weighted network
from repeated measurements of a Gaussian Graphical Model (GGM) on the network.
In this vein, we consider GGMs whose covariance structures align with the
geometry of the weighted network on which they are based. Such GGMs have been
of longstanding interest in statistical physics, and are referred to as the
Gaussian Free Field (GFF). In recent years, they have attracted considerable
interest in the machine learning and theoretical computer science. In this
work, we propose a novel estimator for the weighted network (equivalently, its
Laplacian) from repeated measurements of a GFF on the network, based on the
Fourier analytic properties of the Gaussian distribution. In this pursuit, our
approach exploits complex-valued statistics constructed from observed data,
that are of interest on their own right. We demonstrate the effectiveness of
our estimator with concrete recovery guarantees and bounds on the required
sample complexity. In particular, we show that the proposed statistic achieves
the parametric rate of estimation for fixed network size. In the setting of
networks growing with sample size, our results show that for Erdos-Renyi random
graphs $G(d,p)$ above the connectivity threshold, we demonstrate that network
recovery takes place with high probability as soon as the sample size $n$
satisfies $n \gg d^4 \log d \cdot p^{-2}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Subhro Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Soumendu Sundar Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tran_H/0/1/0/all/0/1&quot;&gt;Hoang-Son Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gangopadhyay_U/0/1/0/all/0/1&quot;&gt;Ujan Gangopadhyay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02347">
<title>Stability and Generalization of Hypergraph Collaborative Networks. (arXiv:2308.02347v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02347</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks have been shown to be very effective in utilizing
pairwise relationships across samples. Recently, there have been several
successful proposals to generalize graph neural networks to hypergraph neural
networks to exploit more complex relationships. In particular, the hypergraph
collaborative networks yield superior results compared to other hypergraph
neural networks for various semi-supervised learning tasks. The collaborative
network can provide high quality vertex embeddings and hyperedge embeddings
together by formulating them as a joint optimization problem and by using their
consistency in reconstructing the given hypergraph. In this paper, we aim to
establish the algorithmic stability of the core layer of the collaborative
network and provide generalization guarantees. The analysis sheds light on the
design of hypergraph filters in collaborative networks, for instance, how the
data and hypergraph filters should be scaled to achieve uniform stability of
the learning process. Some experimental results on real-world datasets are
presented to illustrate the theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_M/0/1/0/all/0/1&quot;&gt;Michael Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hanrui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yip_A/0/1/0/all/0/1&quot;&gt;Andy Yip&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02350">
<title>RobustMQ: Benchmarking Robustness of Quantized Models. (arXiv:2308.02350v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02350</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantization has emerged as an essential technique for deploying deep neural
networks (DNNs) on devices with limited resources. However, quantized models
exhibit vulnerabilities when exposed to various noises in real-world
applications. Despite the importance of evaluating the impact of quantization
on robustness, existing research on this topic is limited and often disregards
established principles of robustness evaluation, resulting in incomplete and
inconclusive findings. To address this gap, we thoroughly evaluated the
robustness of quantized models against various noises (adversarial attacks,
natural corruptions, and systematic noises) on ImageNet. The comprehensive
evaluation results empirically provide valuable insights into the robustness of
quantized models in various scenarios, for example: (1) quantized models
exhibit higher adversarial robustness than their floating-point counterparts,
but are more vulnerable to natural corruptions and systematic noises; (2) in
general, increasing the quantization bit-width results in a decrease in
adversarial robustness, an increase in natural robustness, and an increase in
systematic robustness; (3) among corruption methods, \textit{impulse noise} and
\textit{glass blur} are the most harmful to quantized models, while
\textit{brightness} has the least impact; (4) among systematic noises, the
\textit{nearest neighbor interpolation} has the highest impact, while bilinear
interpolation, cubic interpolation, and area interpolation are the three least
harmful. Our research contributes to advancing the robust quantization of
models and their deployment in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yisong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Haotong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jinyang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianglong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02353">
<title>Adapting to Change: Robust Counterfactual Explanations in Dynamic Data Landscapes. (arXiv:2308.02353v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02353</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel semi-supervised Graph Counterfactual Explainer (GCE)
methodology, Dynamic GRAph Counterfactual Explainer (DyGRACE). It leverages
initial knowledge about the data distribution to search for valid
counterfactuals while avoiding using information from potentially outdated
decision functions in subsequent time steps. Employing two graph autoencoders
(GAEs), DyGRACE learns the representation of each class in a binary
classification scenario. The GAEs minimise the reconstruction error between the
original graph and its learned representation during training. The method
involves (i) optimising a parametric density function (implemented as a
logistic regression function) to identify counterfactuals by maximising the
factual autoencoder&apos;s reconstruction error, (ii) minimising the counterfactual
autoencoder&apos;s error, and (iii) maximising the similarity between the factual
and counterfactual graphs. This semi-supervised approach is independent of an
underlying black-box oracle. A logistic regression model is trained on a set of
graph pairs to learn weights that aid in finding counterfactuals. At inference,
for each unseen graph, the logistic regressor identifies the best
counterfactual candidate using these learned weights, while the GAEs can be
iteratively updated to represent the continual adaptation of the learned graph
representation over iterations. DyGRACE is quite effective and can act as a
drift detector, identifying distributional drift based on differences in
reconstruction errors between iterations. It avoids reliance on the oracle&apos;s
predictions in successive iterations, thereby increasing the efficiency of
counterfactual discovery. DyGRACE, with its capacity for contrastive learning
and drift detection, will offer new avenues for semi-supervised learning and
explanation generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenkaj_B/0/1/0/all/0/1&quot;&gt;Bardh Prenkaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villaizan_Vallelado_M/0/1/0/all/0/1&quot;&gt;Mario Villaizan-Vallelado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leemann_T/0/1/0/all/0/1&quot;&gt;Tobias Leemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasneci_G/0/1/0/all/0/1&quot;&gt;Gjergji Kasneci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02360">
<title>Intensity-free Integral-based Learning of Marked Temporal Point Processes. (arXiv:2308.02360v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02360</link>
<description rdf:parseType="Literal">&lt;p&gt;In the marked temporal point processes (MTPP), a core problem is to
parameterize the conditional joint PDF (probability distribution function)
$p^*(m,t)$ for inter-event time $t$ and mark $m$, conditioned on the history.
The majority of existing studies predefine intensity functions. Their utility
is challenged by specifying the intensity function&apos;s proper form, which is
critical to balance expressiveness and processing efficiency. Recently, there
are studies moving away from predefining the intensity function -- one models
$p^*(t)$ and $p^*(m)$ separately, while the other focuses on temporal point
processes (TPPs), which do not consider marks. This study aims to develop
high-fidelity $p^*(m,t)$ for discrete events where the event marks are either
categorical or numeric in a multi-dimensional continuous space. We propose a
solution framework IFIB (\underline{I}ntensity-\underline{f}ree
\underline{I}ntegral-\underline{b}ased process) that models conditional joint
PDF $p^*(m,t)$ directly without intensity functions. It remarkably simplifies
the process to compel the essential mathematical restrictions. We show the
desired properties of IFIB and the superior experimental results of IFIB on
real-world and synthetic datasets. The code is available at
\url{https://github.com/StepinSilence/IFIB}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sishun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_K/0/1/0/all/0/1&quot;&gt;Ke Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jenny Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yongli Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02362">
<title>Flexible Differentially Private Vertical Federated Learning with Adaptive Feature Embeddings. (arXiv:2308.02362v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.02362</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of vertical federated learning (VFL) has stimulated concerns
about the imperfection in privacy protection, as shared feature embeddings may
reveal sensitive information under privacy attacks. This paper studies the
delicate equilibrium between data privacy and task utility goals of VFL under
differential privacy (DP). To address the generality issue of prior arts, this
paper advocates a flexible and generic approach that decouples the two goals
and addresses them successively. Specifically, we initially derive a rigorous
privacy guarantee by applying norm clipping on shared feature embeddings, which
is applicable across various datasets and models. Subsequently, we demonstrate
that task utility can be optimized via adaptive adjustments on the scale and
distribution of feature embeddings in an accuracy-appreciative way, without
compromising established DP mechanisms. We concretize our observation into the
proposed VFL-AFE framework, which exhibits effectiveness against privacy
attacks and the capacity to retain favorable task utility, as substantiated by
extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_Y/0/1/0/all/0/1&quot;&gt;Yuxi Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongquan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yewei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yiheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1&quot;&gt;Jihong Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuigeng Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02370">
<title>A Machine Learning Method for Predicting Traffic Signal Timing from Probe Vehicle Data. (arXiv:2308.02370v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02370</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic signals play an important role in transportation by enabling traffic
flow management, and ensuring safety at intersections. In addition, knowing the
traffic signal phase and timing data can allow optimal vehicle routing for time
and energy efficiency, eco-driving, and the accurate simulation of signalized
road networks. In this paper, we present a machine learning (ML) method for
estimating traffic signal timing information from vehicle probe data. To the
authors best knowledge, very few works have presented ML techniques for
determining traffic signal timing parameters from vehicle probe data. In this
work, we develop an Extreme Gradient Boosting (XGBoost) model to estimate
signal cycle lengths and a neural network model to determine the corresponding
red times per phase from probe data. The green times are then be derived from
the cycle length and red times. Our results show an error of less than 0.56 sec
for cycle length, and red times predictions within 7.2 sec error on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugirumurera_J/0/1/0/all/0/1&quot;&gt;Juliette Ugirumurera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Severino_J/0/1/0/all/0/1&quot;&gt;Joseph Severino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bensen_E/0/1/0/all/0/1&quot;&gt;Erik A. Bensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qichao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macfarlane_J/0/1/0/all/0/1&quot;&gt;Jane Macfarlane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02382">
<title>Scaling Survival Analysis in Healthcare with Federated Survival Forests: A Comparative Study on Heart Failure and Breast Cancer Genomics. (arXiv:2308.02382v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02382</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival analysis is a fundamental tool in medicine, modeling the time until
an event of interest occurs in a population. However, in real-world
applications, survival data are often incomplete, censored, distributed, and
confidential, especially in healthcare settings where privacy is critical. The
scarcity of data can severely limit the scalability of survival models to
distributed applications that rely on large data pools. Federated learning is a
promising technique that enables machine learning models to be trained on
multiple datasets without compromising user privacy, making it particularly
well-suited for addressing the challenges of survival data and large-scale
survival applications. Despite significant developments in federated learning
for classification and regression, many directions remain unexplored in the
context of survival analysis. In this work, we propose an extension of the
Federated Survival Forest algorithm, called FedSurF++. This federated ensemble
method constructs random survival forests in heterogeneous federations.
Specifically, we investigate several new tree sampling methods from client
forests and compare the results with state-of-the-art survival models based on
neural networks. The key advantage of FedSurF++ is its ability to achieve
comparable performance to existing methods while requiring only a single
communication round to complete. The extensive empirical investigation results
in a significant improvement from the algorithmic and privacy preservation
perspectives, making the original FedSurF algorithm more efficient, robust, and
private. We also present results on two real-world datasets demonstrating the
success of FedSurF++ in real-world healthcare studies. Our results underscore
the potential of FedSurF++ to improve the scalability and effectiveness of
survival analysis in distributed settings while preserving user privacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Archetti_A/0/1/0/all/0/1&quot;&gt;Alberto Archetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ieva_F/0/1/0/all/0/1&quot;&gt;Francesca Ieva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1&quot;&gt;Matteo Matteucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02391">
<title>Learning Optimal Admission Control in Partially Observable Queueing Networks. (arXiv:2308.02391v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02391</link>
<description rdf:parseType="Literal">&lt;p&gt;We present an efficient reinforcement learning algorithm that learns the
optimal admission control policy in a partially observable queueing network.
Specifically, only the arrival and departure times from the network are
observable, and optimality refers to the average holding/rejection cost in
infinite horizon.
&lt;/p&gt;
&lt;p&gt;While reinforcement learning in Partially Observable Markov Decision
Processes (POMDP) is prohibitively expensive in general, we show that our
algorithm has a regret that only depends sub-linearly on the maximal number of
jobs in the network, $S$. In particular, in contrast with existing regret
analyses, our regret bound does not depend on the diameter of the underlying
Markov Decision Process (MDP), which in most queueing systems is at least
exponential in $S$.
&lt;/p&gt;
&lt;p&gt;The novelty of our approach is to leverage Norton&apos;s equivalent theorem for
closed product-form queueing networks and an efficient reinforcement learning
algorithm for MDPs with the structure of birth-and-death processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anselmi_J/0/1/0/all/0/1&quot;&gt;Jonatha Anselmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaujal_B/0/1/0/all/0/1&quot;&gt;Bruno Gaujal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebuffi_L/0/1/0/all/0/1&quot;&gt;Louis-S&amp;#xe9;bastien Rebuffi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02395">
<title>ECG classification using Deep CNN and Gramian Angular Field. (arXiv:2308.02395v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02395</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper study provides a novel contribution to the field of signal
processing and DL for ECG signal analysis by introducing a new feature
representation method for ECG signals. The proposed method is based on
transforming time frequency 1D vectors into 2D images using Gramian Angular
Field transform. Moving on, the classification of the transformed ECG signals
is performed using Convolutional Neural Networks (CNN). The obtained results
show a classification accuracy of 97.47% and 98.65% for anomaly detection.
Accordingly, in addition to improving the classification performance compared
to the state-of-the-art, the feature representation helps identify and
visualize temporal patterns in the ECG signal, such as changes in heart rate,
rhythm, and morphology, which may not be apparent in the original signal. This
has significant implications in the diagnosis and treatment of cardiovascular
diseases and detection of anomalies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Elmir_Y/0/1/0/all/0/1&quot;&gt;Youssef Elmir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Amira_A/0/1/0/all/0/1&quot;&gt;Abbes Amira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02396">
<title>HOOD: Real-Time Robust Human Presence and Out-of-Distribution Detection with Low-Cost FMCW Radar. (arXiv:2308.02396v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02396</link>
<description rdf:parseType="Literal">&lt;p&gt;Human presence detection in indoor environments using millimeter-wave
frequency-modulated continuous-wave (FMCW) radar is challenging due to the
presence of moving and stationary clutters in indoor places. This work proposes
&quot;HOOD&quot; as a real-time robust human presence and out-of-distribution (OOD)
detection method by exploiting 60 GHz short-range FMCW radar. We approach the
presence detection application as an OOD detection problem and solve the two
problems simultaneously using a single pipeline. Our solution relies on a
reconstruction-based architecture and works with radar macro and micro
range-Doppler images (RDIs). HOOD aims to accurately detect the &quot;presence&quot; of
humans in the presence or absence of moving and stationary disturbers. Since it
is also an OOD detector, it aims to detect moving or stationary clutters as OOD
in humans&apos; absence and predicts the current scene&apos;s output as &quot;no presence.&quot;
HOOD is an activity-free approach that performs well in different human
scenarios. On our dataset collected with a 60 GHz short-range FMCW Radar, we
achieve an average AUROC of 94.36%. Additionally, our extensive evaluations and
experiments demonstrate that HOOD outperforms state-of-the-art (SOTA) OOD
detection methods in terms of common OOD detection metrics. Our real-time
experiments are available at: https://muskahya.github.io/HOOD
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kahya_S/0/1/0/all/0/1&quot;&gt;Sabri Mustafa Kahya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Muhammet Sami Yavuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Steinbach_E/0/1/0/all/0/1&quot;&gt;Eckehard Steinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02397">
<title>Design Space Exploration on Efficient and Accurate Human Pose Estimation from Sparse IMU-Sensing. (arXiv:2308.02397v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02397</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Pose Estimation (HPE) to assess human motion in sports, rehabilitation
or work safety requires accurate sensing without compromising the sensitive
underlying personal data. Therefore, local processing is necessary and the
limited energy budget in such systems can be addressed by Inertial Measurement
Units (IMU) instead of common camera sensing. The central trade-off between
accuracy and efficient use of hardware resources is rarely discussed in
research. We address this trade-off by a simulative Design Space Exploration
(DSE) of a varying quantity and positioning of IMU-sensors. First, we generate
IMU-data from a publicly available body model dataset for different sensor
configurations and train a deep learning model with this data. Additionally, we
propose a combined metric to assess the accuracy-resource trade-off. We used
the DSE as a tool to evaluate sensor configurations and identify beneficial
ones for a specific use case. Exemplary, for a system with equal importance of
accuracy and resources, we identify an optimal sensor configuration of 4
sensors with a mesh error of 6.03 cm, increasing the accuracy by 32.7% and
reducing the hardware effort by two sensors compared to state of the art. Our
work can be used to design health applications with well-suited sensor
positioning and attention to data privacy and resource-awareness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Furst_Walter_I/0/1/0/all/0/1&quot;&gt;Iris F&amp;#xfc;rst-Walter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nappi_A/0/1/0/all/0/1&quot;&gt;Antonio Nappi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harbaum_T/0/1/0/all/0/1&quot;&gt;Tanja Harbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Becker_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Becker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02408">
<title>Evaluating the structure of cognitive tasks with transfer learning. (arXiv:2308.02408v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02408</link>
<description rdf:parseType="Literal">&lt;p&gt;Electroencephalography (EEG) decoding is a challenging task due to the
limited availability of labelled data. While transfer learning is a promising
technique to address this challenge, it assumes that transferable data domains
and task are known, which is not the case in this setting. This study
investigates the transferability of deep learning representations between
different EEG decoding tasks. We conduct extensive experiments using
state-of-the-art decoding models on two recently released EEG datasets, ERP
CORE and M$^3$CV, containing over 140 subjects and 11 distinct cognitive tasks.
We measure the transferability of learned representations by pre-training deep
neural networks on one task and assessing their ability to decode subsequent
tasks. Our experiments demonstrate that, even with linear probing transfer,
significant improvements in decoding performance can be obtained, with gains of
up to 28% compare with the pure supervised approach. Additionally, we discover
evidence that certain decoding paradigms elicit specific and narrow brain
activities, while others benefit from pre-training on a broad range of
representations. By revealing which tasks transfer well and demonstrating the
benefits of transfer learning for EEG decoding, our findings have practical
implications for mitigating data scarcity in this setting. The transfer maps
generated also provide insights into the hierarchical relations between
cognitive tasks, hence enhancing our understanding of how these tasks are
connected from a neuroscientific standpoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aristimunha_B/0/1/0/all/0/1&quot;&gt;Bruno Aristimunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Camargo_R/0/1/0/all/0/1&quot;&gt;Raphael Y. de Camargo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pinaya_W/0/1/0/all/0/1&quot;&gt;Walter H. Lopez Pinaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chevallier_S/0/1/0/all/0/1&quot;&gt;Sylvain Chevallier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gramfort_A/0/1/0/all/0/1&quot;&gt;Alexandre Gramfort&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rommel_C/0/1/0/all/0/1&quot;&gt;Cedric Rommel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02409">
<title>Mental Workload Estimation with Electroencephalogram Signals by Combining Multi-Space Deep Models. (arXiv:2308.02409v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02409</link>
<description rdf:parseType="Literal">&lt;p&gt;The human brain is in a continuous state of activity during both work and
rest. Mental activity is a daily process, and when the brain is overworked, it
can have negative effects on human health. In recent years, great attention has
been paid to early detection of mental health problems because it can help
prevent serious health problems and improve quality of life. Several signals
are used to assess mental state, but the electroencephalogram (EEG) is widely
used by researchers because of the large amount of information it provides
about the brain. This paper aims to classify mental workload into three states
and estimate continuum levels. Our method combines multiple dimensions of space
to achieve the best results for mental estimation. In the time domain approach,
we use Temporal Convolutional Networks, and in the frequency domain, we propose
a new architecture called the Multi-Dimensional Residual Block, which combines
residual blocks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hong-Hai Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iyortsuun_N/0/1/0/all/0/1&quot;&gt;Ngumimi Karen Iyortsuun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hyung-Jeong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Guee-Sang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Soo-Hyung Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02410">
<title>RFID-Assisted Indoor Localization Using Hybrid Wireless Data Fusion. (arXiv:2308.02410v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02410</link>
<description rdf:parseType="Literal">&lt;p&gt;Wireless localization is essential for tracking objects in indoor
environments. Internet of Things (IoT) enables localization through its diverse
wireless communication protocols. In this paper, a hybrid section-based indoor
localization method using a developed Radio Frequency Identification (RFID)
tracking device and multiple IoT wireless technologies is proposed. In order to
reduce the cost of the RFID tags, the tags are installed only on the borders of
each section. The RFID tracking device identifies the section, and the proposed
wireless hybrid method finds the location of the object inside the section. The
proposed hybrid method is analytically driven by linear location estimates
obtained from different IoT wireless technologies. The experimental results
using developed RFID tracking device and RSSI-based localization for Bluetooth,
WiFi and ZigBee technologies verifies the analytical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghavami_A/0/1/0/all/0/1&quot;&gt;Abouzar Ghavami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Abedi_A/0/1/0/all/0/1&quot;&gt;Ali Abedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02412">
<title>Self-Supervised Learning for WiFi CSI-Based Human Activity Recognition: A Systematic Study. (arXiv:2308.02412v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02412</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, with the advancement of the Internet of Things (IoT), WiFi
CSI-based HAR has gained increasing attention from academic and industry
communities. By integrating the deep learning technology with CSI-based HAR,
researchers achieve state-of-the-art performance without the need of expert
knowledge. However, the scarcity of labeled CSI data remains the most prominent
challenge when applying deep learning models in the context of CSI-based HAR
due to the privacy and incomprehensibility of CSI-based HAR data. On the other
hand, SSL has emerged as a promising approach for learning meaningful
representations from data without heavy reliance on labeled examples.
Therefore, considerable efforts have been made to address the challenge of
insufficient data in deep learning by leveraging SSL algorithms. In this paper,
we undertake a comprehensive inventory and analysis of the potential held by
different categories of SSL algorithms, including those that have been
previously studied and those that have not yet been explored, within the field.
We provide an in-depth investigation of SSL algorithms in the context of WiFi
CSI-based HAR. We evaluate four categories of SSL algorithms using three
publicly available CSI HAR datasets, each encompassing different tasks and
environmental settings. To ensure relevance to real-world applications, we
design performance metrics that align with specific requirements. Furthermore,
our experimental findings uncover several limitations and blind spots in
existing work, highlighting the barriers that need to be addressed before SSL
can be effectively deployed in real-world WiFi-based HAR applications. Our
results also serve as a practical guideline for industry practitioners and
provide valuable insights for future research endeavors in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Ke Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiangtao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dingchang Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02416">
<title>Local-Global Temporal Fusion Network with an Attention Mechanism for Multiple and Multiclass Arrhythmia Classification. (arXiv:2308.02416v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02416</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical decision support systems (CDSSs) have been widely utilized to
support the decisions made by cardiologists when detecting and classifying
arrhythmia from electrocardiograms (ECGs). However, forming a CDSS for the
arrhythmia classification task is challenging due to the varying lengths of
arrhythmias. Although the onset time of arrhythmia varies, previously developed
methods have not considered such conditions. Thus, we propose a framework that
consists of (i) local temporal information extraction, (ii) global pattern
extraction, and (iii) local-global information fusion with attention to perform
arrhythmia detection and classification with a constrained input length. The
10-class and 4-class performances of our approach were assessed by detecting
the onset and offset of arrhythmia as an episode and the duration of arrhythmia
based on the MIT-BIH arrhythmia database (MITDB) and MIT-BIH atrial
fibrillation database (AFDB), respectively. The results were statistically
superior to those achieved by the comparison models. To check the
generalization ability of the proposed method, an AFDB-trained model was tested
on the MITDB, and superior performance was attained compared with that of a
state-of-the-art model. The proposed method can capture local-global
information and dynamics without incurring information losses. Therefore,
arrhythmias can be recognized more accurately, and their occurrence times can
be calculated; thus, the clinical field can create more accurate treatment
plans by using the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yun Kwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minji Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jo_K/0/1/0/all/0/1&quot;&gt;Kunwook Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hee Seok Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02418">
<title>Differentiable adaptive short-time Fourier transform with respect to the window length. (arXiv:2308.02418v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02418</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a gradient-based method for on-the-fly optimization for
both per-frame and per-frequency window length of the short-time Fourier
transform (STFT), related to previous work in which we developed a
differentiable version of STFT by making the window length a continuous
parameter. The resulting differentiable adaptive STFT possesses commendable
properties, such as the ability to adapt in the same time-frequency
representation to both transient and stationary components, while being easily
optimized by gradient descent. We validate the performance of our method in
vibration analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leiber_M/0/1/0/all/0/1&quot;&gt;Maxime Leiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marnissi_Y/0/1/0/all/0/1&quot;&gt;Yosra Marnissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barrau_A/0/1/0/all/0/1&quot;&gt;Axel Barrau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Badaoui_M/0/1/0/all/0/1&quot;&gt;Mohammed El Badaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02419">
<title>Multimodal Indoor Localisation in Parkinson&apos;s Disease for Detecting Medication Use: Observational Pilot Study in a Free-Living Setting. (arXiv:2308.02419v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02419</link>
<description rdf:parseType="Literal">&lt;p&gt;Parkinson&apos;s disease (PD) is a slowly progressive, debilitating
neurodegenerative disease which causes motor symptoms including gait
dysfunction. Motor fluctuations are alterations between periods with a positive
response to levodopa therapy (&quot;on&quot;) and periods marked by re-emergency of PD
symptoms (&quot;off&quot;) as the response to medication wears off. These fluctuations
often affect gait speed and they increase in their disabling impact as PD
progresses. To improve the effectiveness of current indoor localisation
methods, a transformer-based approach utilising dual modalities which provide
complementary views of movement, Received Signal Strength Indicator (RSSI) and
accelerometer data from wearable devices, is proposed. A sub-objective aims to
evaluate whether indoor localisation, including its in-home gait speed features
(i.e. the time taken to walk between rooms), could be used to evaluate motor
fluctuations by detecting whether the person with PD is taking levodopa
medications or withholding them. To properly evaluate our proposed method, we
use a free-living dataset where the movements and mobility are greatly varied
and unstructured as expected in real-world conditions. 24 participants lived in
pairs (consisting of one person with PD, one control) for five days in a smart
home with various sensors. Our evaluation on the resulting dataset demonstrates
that our proposed network outperforms other methods for indoor localisation.
The sub-objective evaluation shows that precise room-level localisation
predictions, transformed into in-home gait speed features, produce accurate
predictions on whether the PD participant is taking or withholding their
medications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jovan_F/0/1/0/all/0/1&quot;&gt;Ferdian Jovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morgan_C/0/1/0/all/0/1&quot;&gt;Catherine Morgan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McConville_R/0/1/0/all/0/1&quot;&gt;Ryan McConville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tonkin_E/0/1/0/all/0/1&quot;&gt;Emma L. Tonkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Craddock_I/0/1/0/all/0/1&quot;&gt;Ian Craddock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Whone_A/0/1/0/all/0/1&quot;&gt;Alan Whone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02420">
<title>P\=uioio: On-device Real-Time Smartphone-Based Automated Exercise Repetition Counting System. (arXiv:2308.02420v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02420</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated exercise repetition counting has applications across the physical
fitness realm, from personal health to rehabilitation. Motivated by the
ubiquity of mobile phones and the benefits of tracking physical activity, this
study explored the feasibility of counting exercise repetitions in real-time,
using only on-device inference, on smartphones. In this work, after providing
an extensive overview of the state-of-the-art automatic exercise repetition
counting methods, we introduce a deep learning based exercise repetition
counting system for smartphones consisting of five components: (1) Pose
estimation, (2) Thresholding, (3) Optical flow, (4) State machine, and (5)
Counter. The system is then implemented via a cross-platform mobile application
named P\=uioio that uses only the smartphone camera to track repetitions in
real time for three standard exercises: Squats, Push-ups, and Pull-ups. The
proposed system was evaluated via a dataset of pre-recorded videos of
individuals exercising as well as testing by subjects exercising in real time.
Evaluation results indicated the system was 98.89% accurate in real-world tests
and up to 98.85% when evaluated via the pre-recorded dataset. This makes it an
effective, low-cost, and convenient alternative to existing solutions since the
proposed system has minimal hardware requirements without requiring any
wearable or specific sensors or network connectivity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinclair_A/0/1/0/all/0/1&quot;&gt;Adam Sinclair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kautai_K/0/1/0/all/0/1&quot;&gt;Kayla Kautai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shahamiri_S/0/1/0/all/0/1&quot;&gt;Seyed Reza Shahamiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02421">
<title>Differentiable short-time Fourier transform with respect to the hop length. (arXiv:2308.02421v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02421</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a differentiable version of the short-time Fourier
transform (STFT) that allows for gradient-based optimization of the hop length
or the frame temporal position by making these parameters continuous. Our
approach provides improved control over the temporal positioning of frames, as
the continuous nature of the hop length allows for a more finely-tuned
optimization. Furthermore, our contribution enables the use of optimization
methods such as gradient descent, which are more computationally efficient than
conventional discrete optimization methods. Our differentiable STFT can also be
easily integrated into existing algorithms and neural networks. We present a
simulated illustration to demonstrate the efficacy of our approach and to
garner interest from the research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leiber_M/0/1/0/all/0/1&quot;&gt;Maxime Leiber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marnissi_Y/0/1/0/all/0/1&quot;&gt;Yosra Marnissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barrau_A/0/1/0/all/0/1&quot;&gt;Axel Barrau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Badaoui_M/0/1/0/all/0/1&quot;&gt;Mohammed El Badaoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02425">
<title>Hypertension Detection From High-Dimensional Representation of Photoplethysmogram Signals. (arXiv:2308.02425v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02425</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypertension is commonly referred to as the &quot;silent killer&quot;, since it can
lead to severe health complications without any visible symptoms. Early
detection of hypertension is crucial in preventing significant health issues.
Although some studies suggest a relationship between blood pressure and certain
vital signals, such as Photoplethysmogram (PPG), reliable generalization of the
proposed blood pressure estimation methods is not yet guaranteed. This lack of
certainty has resulted in some studies doubting the existence of such
relationships, or considering them weak and limited to heart rate and blood
pressure. In this paper, a high-dimensional representation technique based on
random convolution kernels is proposed for hypertension detection using PPG
signals. The results show that this relationship extends beyond heart rate and
blood pressure, demonstrating the feasibility of hypertension detection with
generalization. Additionally, the utilized transform using convolution kernels,
as an end-to-end time-series feature extractor, outperforms the methods
proposed in the previous studies and state-of-the-art deep learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hasanzadeh_N/0/1/0/all/0/1&quot;&gt;Navid Hasanzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Valaee_S/0/1/0/all/0/1&quot;&gt;Shahrokh Valaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Salehinejad_H/0/1/0/all/0/1&quot;&gt;Hojjat Salehinejad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02427">
<title>Unlocking the Potential of Similarity Matching: Scalability, Supervision and Pre-training. (arXiv:2308.02427v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2308.02427</link>
<description rdf:parseType="Literal">&lt;p&gt;While effective, the backpropagation (BP) algorithm exhibits limitations in
terms of biological plausibility, computational cost, and suitability for
online learning. As a result, there has been a growing interest in developing
alternative biologically plausible learning approaches that rely on local
learning rules. This study focuses on the primarily unsupervised similarity
matching (SM) framework, which aligns with observed mechanisms in biological
systems and offers online, localized, and biologically plausible algorithms. i)
To scale SM to large datasets, we propose an implementation of Convolutional
Nonnegative SM using PyTorch. ii) We introduce a localized supervised SM
objective reminiscent of canonical correlation analysis, facilitating stacking
SM layers. iii) We leverage the PyTorch implementation for pre-training
architectures such as LeNet and compare the evaluation of features against
BP-trained models. This work combines biologically plausible algorithms with
computational efficiency opening multiple avenues for further explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahroun_Y/0/1/0/all/0/1&quot;&gt;Yanis Bahroun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1&quot;&gt;Shagesh Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Atithi Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chklovskii_D/0/1/0/all/0/1&quot;&gt;Dmitri B. Chklovskii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1&quot;&gt;Anirvan M. Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02433">
<title>Contrastive Self-Supervised Learning Based Approach for Patient Similarity: A Case Study on Atrial Fibrillation Detection from PPG Signal. (arXiv:2308.02433v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02433</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel contrastive learning based deep learning
framework for patient similarity search using physiological signals. We use a
contrastive learning based approach to learn similar embeddings of patients
with similar physiological signal data. We also introduce a number of neighbor
selection algorithms to determine the patients with the highest similarity on
the generated embeddings. To validate the effectiveness of our framework for
measuring patient similarity, we select the detection of Atrial Fibrillation
(AF) through photoplethysmography (PPG) signals obtained from smartwatch
devices as our case study. We present extensive experimentation of our
framework on a dataset of over 170 individuals and compare the performance of
our framework with other baseline methods on this dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shanto_S/0/1/0/all/0/1&quot;&gt;Subangkar Karmaker Shanto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Shoumik Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahman_A/0/1/0/all/0/1&quot;&gt;Atif Hasan Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Masud_M/0/1/0/all/0/1&quot;&gt;Mohammad Mehedy Masud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mohammed Eunus Ali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02437">
<title>Noise removal methods on ambulatory EEG: A Survey. (arXiv:2308.02437v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02437</link>
<description rdf:parseType="Literal">&lt;p&gt;Over many decades, research is being attempted for the removal of noise in
the ambulatory EEG. In this respect, an enormous number of research papers is
published for identification of noise removal, It is difficult to present a
detailed review of all these literature. Therefore, in this paper, an attempt
has been made to review the detection and removal of an noise. More than 100
research papers have been discussed to discern the techniques for detecting and
removal the ambulatory EEG. Further, the literature survey shows that the
pattern recognition required to detect ambulatory method, eye open and close,
varies with different conditions of EEG datasets. This is mainly due to the
fact that EEG detected under different conditions has different
characteristics. This is, in turn, necessitates the identification of pattern
recognition technique to effectively distinguish EEG noise data from a various
condition of EEG data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johari_S/0/1/0/all/0/1&quot;&gt;Sarthak Johari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meedinti_G/0/1/0/all/0/1&quot;&gt;Gowri Namratha Meedinti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Delhibabu_R/0/1/0/all/0/1&quot;&gt;Radhakrishnan Delhibabu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Joshi_D/0/1/0/all/0/1&quot;&gt;Deepak Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02442">
<title>Adaptive Preferential Attached kNN Graph With Distribution-Awareness. (arXiv:2308.02442v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02442</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-based kNN algorithms have garnered widespread popularity for machine
learning tasks, due to their simplicity and effectiveness. However, the
conventional kNN graph&apos;s reliance on a fixed value of k can hinder its
performance, especially in scenarios involving complex data distributions.
Moreover, like other classification models, the presence of ambiguous samples
along decision boundaries often presents a challenge, as they are more prone to
incorrect classification. To address these issues, we propose the Preferential
Attached k-Nearest Neighbors Graph (paNNG), which combines adaptive kNN with
distribution-based graph construction. By incorporating distribution
information, paNNG can significantly improve performance for ambiguous samples
by &quot;pulling&quot; them towards their original classes and hence enable enhanced
overall accuracy and generalization capability. Through rigorous evaluations on
diverse benchmark datasets, paNNG outperforms state-of-the-art algorithms,
showcasing its adaptability and efficacy across various real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_S/0/1/0/all/0/1&quot;&gt;Shaojie Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02448">
<title>From Military to Healthcare: Adopting and Expanding Ethical Principles for Generative Artificial Intelligence. (arXiv:2308.02448v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2308.02448</link>
<description rdf:parseType="Literal">&lt;p&gt;In 2020, the U.S. Department of Defense officially disclosed a set of ethical
principles to guide the use of Artificial Intelligence (AI) technologies on
future battlefields. Despite stark differences, there are core similarities
between the military and medical service. Warriors on battlefields often face
life-altering circumstances that require quick decision-making. Medical
providers experience similar challenges in a rapidly changing healthcare
environment, such as in the emergency department or during surgery treating a
life-threatening condition. Generative AI, an emerging technology designed to
efficiently generate valuable information, holds great promise. As computing
power becomes more accessible and the abundance of health data, such as
electronic health records, electrocardiograms, and medical images, increases,
it is inevitable that healthcare will be revolutionized by this technology.
Recently, generative AI has captivated the research community, leading to
debates about its application in healthcare, mainly due to concerns about
transparency and related issues. Meanwhile, concerns about the potential
exacerbation of health disparities due to modeling biases have raised notable
ethical concerns regarding the use of this technology in healthcare. However,
the ethical principles for generative AI in healthcare have been understudied,
and decision-makers often fail to consider the significance of generative AI.
In this paper, we propose GREAT PLEA ethical principles, encompassing
governance, reliability, equity, accountability, traceability, privacy,
lawfulness, empathy, and autonomy, for generative AI in healthcare. We aim to
proactively address the ethical dilemmas and challenges posed by the
integration of generative AI in healthcare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oniani_D/0/1/0/all/0/1&quot;&gt;David Oniani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsman_J/0/1/0/all/0/1&quot;&gt;Jordan Hilsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;, COL (Ret.) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poropatich_R/0/1/0/all/0/1&quot;&gt;Ronald K. Poropatich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pamplin_C/0/1/0/all/0/1&quot;&gt;COL Jeremy C. Pamplin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Legault_L/0/1/0/all/0/1&quot;&gt;LTC Gary L. Legault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02451">
<title>Pruning a neural network using Bayesian inference. (arXiv:2308.02451v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.02451</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network pruning is a highly effective technique aimed at reducing the
computational and memory demands of large neural networks. In this research
paper, we present a novel approach to pruning neural networks utilizing
Bayesian inference, which can seamlessly integrate into the training procedure.
Our proposed method leverages the posterior probabilities of the neural network
prior to and following pruning, enabling the calculation of Bayes factors. The
calculated Bayes factors guide the iterative pruning. Through comprehensive
evaluations conducted on multiple benchmarks, we demonstrate that our method
achieves desired levels of sparsity while maintaining competitive accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mathew_S/0/1/0/all/0/1&quot;&gt;Sunil Mathew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowe_D/0/1/0/all/0/1&quot;&gt;Daniel B. Rowe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02452">
<title>Generative Modelling of L\&apos;{e}vy Area for High Order SDE Simulation. (arXiv:2308.02452v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2308.02452</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well known that, when numerically simulating solutions to SDEs,
achieving a strong convergence rate better than O(\sqrt{h}) (where h is the
step size) requires the use of certain iterated integrals of Brownian motion,
commonly referred to as its &quot;L\&apos;{e}vy areas&quot;. However, these stochastic
integrals are difficult to simulate due to their non-Gaussian nature and for a
d-dimensional Brownian motion with d &amp;gt; 2, no fast almost-exact sampling
algorithm is known.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose L\&apos;{e}vyGAN, a deep-learning-based model for
generating approximate samples of L\&apos;{e}vy area conditional on a Brownian
increment. Due to our &quot;Bridge-flipping&quot; operation, the output samples match all
joint and conditional odd moments exactly. Our generator employs a tailored
GNN-inspired architecture, which enforces the correct dependency structure
between the output distribution and the conditioning variable. Furthermore, we
incorporate a mathematically principled characteristic-function based
discriminator. Lastly, we introduce a novel training mechanism termed
&quot;Chen-training&quot;, which circumvents the need for expensive-to-generate training
data-sets. This new training procedure is underpinned by our two main
theoretical results.
&lt;/p&gt;
&lt;p&gt;For 4-dimensional Brownian motion, we show that L\&apos;{e}vyGAN exhibits
state-of-the-art performance across several metrics which measure both the
joint and marginal distributions. We conclude with a numerical experiment on
the log-Heston model, a popular SDE in mathematical finance, demonstrating that
high-quality synthetic L\&apos;{e}vy area can lead to high order weak convergence
and variance reduction when using multilevel Monte Carlo (MLMC).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Jelincic_A/0/1/0/all/0/1&quot;&gt;Andra&amp;#x17e; Jelin&amp;#x10d;i&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tao_J/0/1/0/all/0/1&quot;&gt;Jiajie Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Turner_W/0/1/0/all/0/1&quot;&gt;William F. Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cass_T/0/1/0/all/0/1&quot;&gt;Thomas Cass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Foster_J/0/1/0/all/0/1&quot;&gt;James Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ni_H/0/1/0/all/0/1&quot;&gt;Hao Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02454">
<title>SoK: Assessing the State of Applied Federated Machine Learning. (arXiv:2308.02454v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02454</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine Learning (ML) has shown significant potential in various
applications; however, its adoption in privacy-critical domains has been
limited due to concerns about data privacy. A promising solution to this issue
is Federated Machine Learning (FedML), a model-to-data approach that
prioritizes data privacy. By enabling ML algorithms to be applied directly to
distributed data sources without sharing raw data, FedML offers enhanced
privacy protections, making it suitable for privacy-critical environments.
Despite its theoretical benefits, FedML has not seen widespread practical
implementation. This study aims to explore the current state of applied FedML
and identify the challenges hindering its practical adoption. Through a
comprehensive systematic literature review, we assess 74 relevant papers to
analyze the real-world applicability of FedML. Our analysis focuses on the
characteristics and emerging trends of FedML implementations, as well as the
motivational drivers and application domains. We also discuss the encountered
challenges in integrating FedML into real-life settings. By shedding light on
the existing landscape and potential obstacles, this research contributes to
the further development and implementation of FedML in privacy-critical
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_T/0/1/0/all/0/1&quot;&gt;Tobias M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stabler_M/0/1/0/all/0/1&quot;&gt;Maximilian St&amp;#xe4;bler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gascon_H/0/1/0/all/0/1&quot;&gt;Hugo Gasc&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koster_F/0/1/0/all/0/1&quot;&gt;Frank K&amp;#xf6;ster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1&quot;&gt;Florian Matthes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02459">
<title>Nonprehensile Planar Manipulation through Reinforcement Learning with Multimodal Categorical Exploration. (arXiv:2308.02459v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.02459</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing robot controllers capable of achieving dexterous nonprehensile
manipulation, such as pushing an object on a table, is challenging. The
underactuated and hybrid-dynamics nature of the problem, further complicated by
the uncertainty resulting from the frictional interactions, requires
sophisticated control behaviors. Reinforcement Learning (RL) is a powerful
framework for developing such robot controllers. However, previous RL
literature addressing the nonprehensile pushing task achieves low accuracy,
non-smooth trajectories, and only simple motions, i.e. without rotation of the
manipulated object. We conjecture that previously used unimodal exploration
strategies fail to capture the inherent hybrid-dynamics of the task, arising
from the different possible contact interaction modes between the robot and the
object, such as sticking, sliding, and separation. In this work, we propose a
multimodal exploration approach through categorical distributions, which
enables us to train planar pushing RL policies for arbitrary starting and
target object poses, i.e. positions and orientations, and with improved
accuracy. We show that the learned policies are robust to external disturbances
and observation noise, and scale to tasks with multiple pushers. Furthermore,
we validate the transferability of the learned policies, trained entirely in
simulation, to a physical robot hardware using the KUKA iiwa robot arm. See our
supplemental video: https://youtu.be/vTdva1mgrk4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrandis_J/0/1/0/all/0/1&quot;&gt;Juan Del Aguila Ferrandis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moura_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Moura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayakumar_S/0/1/0/all/0/1&quot;&gt;Sethu Vijayakumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02460">
<title>Sea level Projections with Machine Learning using Altimetry and Climate Model ensembles. (arXiv:2308.02460v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2308.02460</link>
<description rdf:parseType="Literal">&lt;p&gt;Satellite altimeter observations retrieved since 1993 show that the global
mean sea level is rising at an unprecedented rate (3.4mm/year). With almost
three decades of observations, we can now investigate the contributions of
anthropogenic climate-change signals such as greenhouse gases, aerosols, and
biomass burning in this rising sea level. We use machine learning (ML) to
investigate future patterns of sea level change. To understand the extent of
contributions from the climate-change signals, and to help in forecasting sea
level change in the future, we turn to climate model simulations. This work
presents a machine learning framework that exploits both satellite observations
and climate model simulations to generate sea level rise projections at a
2-degree resolution spatial grid, 30 years into the future. We train fully
connected neural networks (FCNNs) to predict altimeter values through a
non-linear fusion of the climate model hindcasts (for 1993-2019). The learned
FCNNs are then applied to future climate model projections to predict future
sea level patterns. We propose segmenting our spatial dataset into meaningful
clusters and show that clustering helps to improve predictions of our ML model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sinha_S/0/1/0/all/0/1&quot;&gt;Saumya Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fasullo_J/0/1/0/all/0/1&quot;&gt;John Fasullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nerem_R/0/1/0/all/0/1&quot;&gt;R. Steven Nerem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Monteleoni_C/0/1/0/all/0/1&quot;&gt;Claire Monteleoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02462">
<title>Fast and Accurate Reduced-Order Modeling of a MOOSE-based Additive Manufacturing Model with Operator Learning. (arXiv:2308.02462v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02462</link>
<description rdf:parseType="Literal">&lt;p&gt;One predominant challenge in additive manufacturing (AM) is to achieve
specific material properties by manipulating manufacturing process parameters
during the runtime. Such manipulation tends to increase the computational load
imposed on existing simulation tools employed in AM. The goal of the present
work is to construct a fast and accurate reduced-order model (ROM) for an AM
model developed within the Multiphysics Object-Oriented Simulation Environment
(MOOSE) framework, ultimately reducing the time/cost of AM control and
optimization processes. Our adoption of the operator learning (OL) approach
enabled us to learn a family of differential equations produced by altering
process variables in the laser&apos;s Gaussian point heat source. More specifically,
we used the Fourier neural operator (FNO) and deep operator network (DeepONet)
to develop ROMs for time-dependent responses. Furthermore, we benchmarked the
performance of these OL methods against a conventional deep neural network
(DNN)-based ROM. Ultimately, we found that OL methods offer comparable
performance and, in terms of accuracy and generalizability, even outperform DNN
at predicting scalar model responses. The DNN-based ROM afforded the fastest
training time. Furthermore, all the ROMs were faster than the original MOOSE
model yet still provided accurate predictions. FNO had a smaller mean
prediction error than DeepONet, with a larger variance for time-dependent
responses. Unlike DNN, both FNO and DeepONet were able to simulate time series
data without the need for dimensionality reduction techniques. The present work
can help facilitate the AM optimization process by enabling faster execution of
simulation tools while still preserving evaluation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaseen_M/0/1/0/all/0/1&quot;&gt;Mahmoud Yaseen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yushu_D/0/1/0/all/0/1&quot;&gt;Dewen Yushu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+German_P/0/1/0/all/0/1&quot;&gt;Peter German&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xu Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02464">
<title>Universal Approximation of Linear Time-Invariant (LTI) Systems through RNNs: Power of Randomness in Reservoir Computing. (arXiv:2308.02464v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.02464</link>
<description rdf:parseType="Literal">&lt;p&gt;Recurrent neural networks (RNNs) are known to be universal approximators of
dynamic systems under fairly mild and general assumptions, making them good
tools to process temporal information. However, RNNs usually suffer from the
issues of vanishing and exploding gradients in the standard RNN training.
Reservoir computing (RC), a special RNN where the recurrent weights are
randomized and left untrained, has been introduced to overcome these issues and
has demonstrated superior empirical performance in fields as diverse as natural
language processing and wireless communications especially in scenarios where
training samples are extremely limited. On the contrary, the theoretical
grounding to support this observed performance has not been fully developed at
the same pace. In this work, we show that RNNs can provide universal
approximation of linear time-invariant (LTI) systems. Specifically, we show
that RC can universally approximate a general LTI system. We present a clear
signal processing interpretation of RC and utilize this understanding in the
problem of simulating a generic LTI system through RC. Under this setup, we
analytically characterize the optimal probability distribution function for
generating the recurrent weights of the underlying RNN of the RC. We provide
extensive numerical evaluations to validate the optimality of the derived
optimum distribution of the recurrent weights of the RC for the LTI system
simulation problem. Our work results in clear signal processing-based model
interpretability of RC and provides theoretical explanation for the power of
randomness in setting instead of training RC&apos;s recurrent weights. It further
provides a complete optimum analytical characterization for the untrained
recurrent weights, marking an important step towards explainable machine
learning (XML) which is extremely important for applications where training
samples are limited.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jere_S/0/1/0/all/0/1&quot;&gt;Shashank Jere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lizhong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Said_K/0/1/0/all/0/1&quot;&gt;Karim Said&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjia Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02465">
<title>BlindSage: Label Inference Attacks against Node-level Vertical Federated Graph Neural Networks. (arXiv:2308.02465v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.02465</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning enables collaborative training of machine learning models
by keeping the raw data of the involved workers private. One of its main
objectives is to improve the models&apos; privacy, security, and scalability.
Vertical Federated Learning (VFL) offers an efficient cross-silo setting where
a few parties collaboratively train a model without sharing the same features.
In such a scenario, classification labels are commonly considered sensitive
information held exclusively by one (active) party, while other (passive)
parties use only their local information. Recent works have uncovered important
flaws of VFL, leading to possible label inference attacks under the assumption
that the attacker has some, even limited, background knowledge on the relation
between labels and data. In this work, we are the first (to the best of our
knowledge) to investigate label inference attacks on VFL using a
zero-background knowledge strategy. To concretely formulate our proposal, we
focus on Graph Neural Networks (GNNs) as a target model for the underlying VFL.
In particular, we refer to node classification tasks, which are widely studied,
and GNNs have shown promising results. Our proposed attack, BlindSage, provides
impressive results in the experiments, achieving nearly 100% accuracy in most
cases. Even when the attacker has no information about the used architecture or
the number of classes, the accuracy remained above 85% in most instances.
Finally, we observe that well-known defenses cannot mitigate our attack without
affecting the model&apos;s performance on the main classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arazzi_M/0/1/0/all/0/1&quot;&gt;Marco Arazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_M/0/1/0/all/0/1&quot;&gt;Mauro Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koffas_S/0/1/0/all/0/1&quot;&gt;Stefanos Koffas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krcek_M/0/1/0/all/0/1&quot;&gt;Marina Krcek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nocera_A/0/1/0/all/0/1&quot;&gt;Antonino Nocera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1&quot;&gt;Stjepan Picek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02490">
<title>MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities. (arXiv:2308.02490v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2308.02490</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MM-Vet, an evaluation benchmark that examines large multimodal
models (LMMs) on complicated multimodal tasks. Recent LMMs have shown various
intriguing abilities, such as solving math problems written on the blackboard,
reasoning about events and celebrities in news images, and explaining visual
jokes. Rapid model advancements pose challenges to evaluation benchmark
development. Problems include: (1) How to systematically structure and evaluate
the complicated multimodal tasks; (2) How to design evaluation metrics that
work well across question and answer types; and (3) How to give model insights
beyond a simple performance ranking. To this end, we present MM-Vet, designed
based on the insight that the intriguing ability to solve complicated tasks is
often achieved by a generalist model being able to integrate different core
vision-language (VL) capabilities. MM-Vet defines 6 core VL capabilities and
examines the 16 integrations of interest derived from the capability
combination. For evaluation metrics, we propose an LLM-based evaluator for
open-ended outputs. The evaluator enables the evaluation across different
question types and answer styles, resulting in a unified scoring metric. We
evaluate representative LMMs on MM-Vet, providing insights into the
capabilities of different LMM system paradigms and models. Code and data are
available at https://github.com/yuweihao/MM-Vet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weihao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2003.09168">
<title>Fine-grained Species Recognition with Privileged Pooling: Better Sample Efficiency Through Supervised Attention. (arXiv:2003.09168v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2003.09168</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a scheme for supervised image classification that uses privileged
information, in the form of keypoint annotations for the training data, to
learn strong models from small and/or biased training sets. Our main motivation
is the recognition of animal species for ecological applications such as
biodiversity modelling, which is challenging because of long-tailed species
distributions due to rare species, and strong dataset biases such as repetitive
scene background in camera traps. To counteract these challenges, we propose a
visual attention mechanism that is supervised via keypoint annotations that
highlight important object parts. This privileged information, implemented as a
novel privileged pooling operation, is only required during training and helps
the model to focus on regions that are discriminative. In experiments with
three different animal species datasets, we show that deep networks with
privileged pooling can use small training sets more efficiently and generalize
better.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andres C. Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAronco_S/0/1/0/all/0/1&quot;&gt;Stefano D&amp;#x27;Aronco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schindler_K/0/1/0/all/0/1&quot;&gt;Konrad Schindler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wegner_J/0/1/0/all/0/1&quot;&gt;Jan Dirk Wegner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.00596">
<title>A New Basis for Sparse Principal Component Analysis. (arXiv:2007.00596v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2007.00596</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous versions of sparse principal component analysis (PCA) have presumed
that the eigen-basis (a $p \times k$ matrix) is approximately sparse. We
propose a method that presumes the $p \times k$ matrix becomes approximately
sparse after a $k \times k$ rotation. The simplest version of the algorithm
initializes with the leading $k$ principal components. Then, the principal
components are rotated with an $k \times k$ orthogonal rotation to make them
approximately sparse. Finally, soft-thresholding is applied to the rotated
principal components. This approach differs from prior approaches because it
uses an orthogonal rotation to approximate a sparse basis. One consequence is
that a sparse component need not to be a leading eigenvector, but rather a
mixture of them. In this way, we propose a new (rotated) basis for sparse PCA.
In addition, our approach avoids &quot;deflation&quot; and multiple tuning parameters
required for that. Our sparse PCA framework is versatile; for example, it
extends naturally to a two-way analysis of a data matrix for simultaneous
dimensionality reduction of rows and columns. We provide evidence showing that
for the same level of sparsity, the proposed sparse PCA method is more stable
and can explain more variance compared to alternative methods. Through three
applications -- sparse coding of images, analysis of transcriptome sequencing
data, and large-scale clustering of social networks, we demonstrate the modern
usefulness of sparse PCA in exploring multivariate data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rohe_K/0/1/0/all/0/1&quot;&gt;Karl Rohe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.09195">
<title>Mitigating the Bias of Centered Objects in Common Datasets. (arXiv:2112.09195v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.09195</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional networks are considered shift invariant, but it was
demonstrated that their response may vary according to the exact location of
the objects. In this paper we will demonstrate that most commonly investigated
datasets have a bias, where objects are over-represented at the center of the
image during training. This bias and the boundary condition of these networks
can have a significant effect on the performance of these architectures and
their accuracy drops significantly as an object approaches the boundary. We
will also demonstrate how this effect can be mitigated with data augmentation
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szabo_G/0/1/0/all/0/1&quot;&gt;Gergely Szabo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvath_A/0/1/0/all/0/1&quot;&gt;Andras Horvath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.07871">
<title>Mondrian Forest for Data Stream Classification Under Memory Constraints. (arXiv:2205.07871v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning algorithms generally assume the availability of enough
memory to store their data model during the training and test phases. However,
in the Internet of Things, this assumption is unrealistic when data comes in
the form of infinite data streams, or when learning algorithms are deployed on
devices with reduced amounts of memory. In this paper, we adapt the online
Mondrian forest classification algorithm to work with memory constraints on
data streams. In particular, we design five out-of-memory strategies to update
Mondrian trees with new data points when the memory limit is reached. Moreover,
we design trimming mechanisms to make Mondrian trees more robust to concept
drifts under memory constraints. We evaluate our algorithms on a variety of
real and simulated datasets, and we conclude with recommendations on their use
in different situations: the Extend Node strategy appears as the best
out-of-memory strategy in all configurations, whereas different trimming
mechanisms should be adopted depending on whether a concept drift is expected.
All our methods are implemented in the OrpailleCC open-source library and are
ready to be used on embedded systems and connected objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khannouz_M/0/1/0/all/0/1&quot;&gt;Martin Khannouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glatard_T/0/1/0/all/0/1&quot;&gt;Tristan Glatard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13687">
<title>Statistical Inference of Constrained Stochastic Optimization via Sketched Sequential Quadratic Programming. (arXiv:2205.13687v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13687</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider statistical inference of equality-constrained stochastic
nonlinear optimization problems. We develop a fully online stochastic
sequential quadratic programming (StoSQP) method to solve the problems, which
can be regarded as applying Newton&apos;s method to the first-order optimality
conditions (i.e., the KKT conditions). Motivated by recent designs of numerical
second-order methods, we allow StoSQP to adaptively select any random stepsize
$\bar{\alpha}_t$, as long as $\beta_t\leq \bar{\alpha}_t \leq \beta_t+\chi_t$,
for some control sequences $\beta_t$ and $\chi_t=o(\beta_t)$. To reduce the
dominant computational cost of second-order methods, we additionally allow
StoSQP to inexactly solve quadratic programs via efficient randomized iterative
solvers that utilize sketching techniques. Notably, we do not require the
approximation error to diminish as iteration proceeds. For the developed
method, we show that under mild assumptions (i) computationally, it can take at
most $O(1/\epsilon^4)$ iterations (same as samples) to attain
$\epsilon$-stationarity; (ii) statistically, its primal-dual sequence
$1/\sqrt{\beta_t}\cdot (x_t - x^\star, \lambda_t - \lambda^\star)$ converges to
a mean-zero Gaussian distribution with a nontrivial covariance matrix depending
on the underlying sketching distribution. Additionally, we establish the
almost-sure convergence rate of the iterate $(x_t, \lambda_t)$ along with the
Berry-Esseen bound; the latter quantitatively measures the convergence rate of
the distribution function. We analyze a plug-in limiting covariance matrix
estimator, and demonstrate the performance of the method both on benchmark
nonlinear problems in CUTEst test set and on linearly/nonlinearly constrained
regression problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Na_S/0/1/0/all/0/1&quot;&gt;Sen Na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.15076">
<title>Improved Algorithms for Bandit with Graph Feedback via Regret Decomposition. (arXiv:2205.15076v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.15076</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of bandit with graph feedback generalizes both the multi-armed
bandit (MAB) problem and the learning with expert advice problem by encoding in
a directed graph how the loss vector can be observed in each round of the game.
The mini-max regret is closely related to the structure of the feedback graph
and their connection is far from being fully understood. We propose a new
algorithmic framework for the problem based on a partition of the feedback
graph. Our analysis reveals the interplay between various parts of the graph by
decomposing the regret to the sum of the regret caused by small parts and the
regret caused by their interaction. As a result, our algorithm can be viewed as
an interpolation and generalization of the optimal algorithms for MAB and
learning with expert advice. Our framework unifies previous algorithms for both
strongly observable graphs and weakly observable graphs, resulting in improved
and optimal regret bounds on a wide range of graph families including graphs of
bounded degree and strongly observable graphs with a few corrupted arms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuchen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chihao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.09919">
<title>Inference-Based Quantum Sensing. (arXiv:2206.09919v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2206.09919</link>
<description rdf:parseType="Literal">&lt;p&gt;In a standard Quantum Sensing (QS) task one aims at estimating an unknown
parameter $\theta$, encoded into an $n$-qubit probe state, via measurements of
the system. The success of this task hinges on the ability to correlate changes
in the parameter to changes in the system response $\mathcal{R}(\theta)$ (i.e.,
changes in the measurement outcomes). For simple cases the form of
$\mathcal{R}(\theta)$ is known, but the same cannot be said for realistic
scenarios, as no general closed-form expression exists. In this work we present
an inference-based scheme for QS. We show that, for a general class of unitary
families of encoding, $\mathcal{R}(\theta)$ can be fully characterized by only
measuring the system response at $2n+1$ parameters. This allows us to infer the
value of an unknown parameter given the measured response, as well as to
determine the sensitivity of the scheme, which characterizes its overall
performance. We show that inference error is, with high probability, smaller
than $\delta$, if one measures the system response with a number of shots that
scales only as $\Omega(\log^3(n)/\delta^2)$. Furthermore, the framework
presented can be broadly applied as it remains valid for arbitrary probe states
and measurement schemes, and, even holds in the presence of quantum noise. We
also discuss how to extend our results beyond unitary families. Finally, to
showcase our method we implement it for a QS task on real quantum hardware, and
in numerical simulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Alderete_C/0/1/0/all/0/1&quot;&gt;C. Huerta Alderete&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gordon_M/0/1/0/all/0/1&quot;&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sauvage_F/0/1/0/all/0/1&quot;&gt;Frederic Sauvage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sone_A/0/1/0/all/0/1&quot;&gt;Akira Sone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sornborger_A/0/1/0/all/0/1&quot;&gt;Andrew T. Sornborger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Coles_P/0/1/0/all/0/1&quot;&gt;Patrick J. Coles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Cerezo_M/0/1/0/all/0/1&quot;&gt;M. Cerezo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.03890">
<title>ENCODE: Encoding NetFlows for Network Anomaly Detection. (arXiv:2207.03890v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.03890</link>
<description rdf:parseType="Literal">&lt;p&gt;NetFlow data is a popular network log format used by many network analysts
and researchers. The advantages of using NetFlow over deep packet inspection
are that it is easier to collect and process, and it is less privacy intrusive.
Many works have used machine learning to detect network attacks using NetFlow
data. The first step for these machine learning pipelines is to pre-process the
data before it is given to the machine learning algorithm. Many approaches
exist to pre-process NetFlow data; however, these simply apply existing methods
to the data, not considering the specific properties of network data. We argue
that for data originating from software systems, such as NetFlow or software
logs, similarities in frequency and contexts of feature values are more
important than similarities in the value itself. In this work, we propose an
encoding algorithm that directly takes the frequency and the context of the
feature values into account when the data is being processed. Different types
of network behaviours can be clustered using this encoding, thus aiding the
process of detecting anomalies within the network. We train several machine
learning models for anomaly detection using the data that has been encoded with
our encoding algorithm. We evaluate the effectiveness of our encoding on a new
dataset that we created for network attacks on Kubernetes clusters and two
well-known public NetFlow datasets. We empirically demonstrate that the machine
learning models benefit from using our encoding for anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Clinton Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panichella_A/0/1/0/all/0/1&quot;&gt;Annibale Panichella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verwer_S/0/1/0/all/0/1&quot;&gt;Sicco Verwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blaise_A/0/1/0/all/0/1&quot;&gt;Agathe Blaise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rebecchi_F/0/1/0/all/0/1&quot;&gt;Filippo Rebecchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07243">
<title>Exponential Concentration of Stochastic Approximation with Non-vanishing Gradient. (arXiv:2208.07243v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07243</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze the behavior of stochastic approximation algorithms where
iterates, in expectation, make progress towards an objective at each step. When
progress is proportional to the step size of the algorithm, we prove
exponential concentration bounds. These tail-bounds contrast asymptotic
normality results which are more frequently associated with stochastic
approximation. The methods that we develop rely on a geometric ergodicity
proof. This extends a result on Markov chains due to Hajek (1982) to the area
of stochastic approximation algorithms. For Projected Stochastic Gradient
Descent with a non-vanishing gradient, our results can be used to prove
$O(1/t)$ and linear convergence rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Law_K/0/1/0/all/0/1&quot;&gt;Kody Law&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Walton_N/0/1/0/all/0/1&quot;&gt;Neil Walton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shangda Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.08549">
<title>Automatic Emergency Dust-Free solution on-board International Space Station with Bi-GRU (AED-ISS). (arXiv:2210.08549v3 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2210.08549</link>
<description rdf:parseType="Literal">&lt;p&gt;With a rising attention for the issue of PM2.5 or PM0.3, particulate matters
have become not only a potential threat to both the environment and human, but
also a harming existence to instruments onboard International Space Station
(ISS). Our team is aiming to relate various concentration of particulate
matters to magnetic fields, humidity, acceleration, temperature, pressure and
CO2 concentration. Our goal is to establish an early warning system (EWS),
which is able to forecast the levels of particulate matters and provides ample
reaction time for astronauts to protect their instruments in some experiments
or increase the accuracy of the measurements; In addition, the constructed
model can be further developed into a prototype of a remote-sensing smoke alarm
for applications related to fires. In this article, we will implement the
Bi-GRU (Bidirectional Gated Recurrent Unit) algorithms that collect data for
past 90 minutes and predict the levels of particulates which over 2.5
micrometer per 0.1 liter for the next 1 minute, which is classified as an early
warning
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_P/0/1/0/all/0/1&quot;&gt;Po-Han Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wei-Chih Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hou_H/0/1/0/all/0/1&quot;&gt;Hong-Chun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu-Hao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shue_J/0/1/0/all/0/1&quot;&gt;Jih-Hong Shue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13179">
<title>A simple probabilistic neural networks for machine understanding. (arXiv:2210.13179v3 [cond-mat.dis-nn] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13179</link>
<description rdf:parseType="Literal">&lt;p&gt;We discuss probabilistic neural networks for unsupervised learning with a
fixed internal representation as models for machine understanding. Here
understanding is intended as mapping data to an already existing representation
which encodes an {\em a priori} organisation of the feature space. We derive
the internal representation by requiring that it satisfies the principles of
maximal relevance and of maximal ignorance about how different features are
combined. We show that, when hidden units are binary variables, these two
principles identify a unique model -- the Hierarchical Feature Model (HFM) --
which is fully solvable and provides a natural interpretation in terms of
features. We argue that learning machines with this architecture enjoy a number
of interesting properties, like the continuity of the representation with
respect to changes in parameters and data, the possibility to control the level
of compression and the ability to support functions that go beyond
generalisation. We explore the behaviour of the model with extensive numerical
experiments and argue that models where the internal representation is fixed
reproduce a learning modality which is qualitatively different from that of
more traditional models such as Restricted Boltzmann Machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xie_R/0/1/0/all/0/1&quot;&gt;Rongrong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Marsili_M/0/1/0/all/0/1&quot;&gt;Matteo Marsili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01021">
<title>Data-driven modeling of Landau damping by physics-informed neural networks. (arXiv:2211.01021v3 [physics.plasm-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01021</link>
<description rdf:parseType="Literal">&lt;p&gt;Kinetic approaches are generally accurate in dealing with microscale plasma
physics problems but are computationally expensive for large-scale or
multiscale systems. One of the long-standing problems in plasma physics is the
integration of kinetic physics into fluid models, which is often achieved
through sophisticated analytical closure terms. In this paper, we successfully
construct a multi-moment fluid model with an implicit fluid closure included in
the neural network using machine learning. The multi-moment fluid model is
trained with a small fraction of sparsely sampled data from kinetic simulations
of Landau damping, using the physics-informed neural network (PINN) and the
gradient-enhanced physics-informed neural network (gPINN). The multi-moment
fluid model constructed using either PINN or gPINN reproduces the time
evolution of the electric field energy, including its damping rate, and the
plasma dynamics from the kinetic simulations. In addition, we introduce a
variant of the gPINN architecture, namely, gPINN$p$ to capture the Landau
damping process. Instead of including the gradients of all the equation
residuals, gPINN$p$ only adds the gradient of the pressure equation residual as
one additional constraint. Among the three approaches, the gPINN$p$-constructed
multi-moment fluid model offers the most accurate results. This work sheds
light on the accurate and efficient modeling of large-scale systems, which can
be extended to complex multiscale laboratory, space, and astrophysical plasma
physics problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yilan Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiayu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Mingle Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chuanfei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haiyang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wenjie Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaqiu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07206">
<title>Scalable PAC-Bayesian Meta-Learning via the PAC-Optimal Hyper-Posterior: From Theory to Practice. (arXiv:2211.07206v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07206</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-Learning aims to speed up the learning process on new tasks by acquiring
useful inductive biases from datasets of related learning tasks. While, in
practice, the number of related tasks available is often small, most of the
existing approaches assume an abundance of tasks; making them unrealistic and
prone to overfitting. A central question in the meta-learning literature is how
to regularize to ensure generalization to unseen tasks. In this work, we
provide a theoretical analysis using the PAC-Bayesian theory and present a
generalization bound for meta-learning, which was first derived by Rothfuss et
al. (2021). Crucially, the bound allows us to derive the closed form of the
optimal hyper-posterior, referred to as PACOH, which leads to the best
performance guarantees. We provide a theoretical analysis and empirical case
study under which conditions and to what extent these guarantees for
meta-learning improve upon PAC-Bayesian per-task learning bounds. The
closed-form PACOH inspires a practical meta-learning approach that avoids the
reliance on bi-level optimization, giving rise to a stochastic optimization
problem that is amenable to standard variational methods that scale well. Our
experiments show that, when instantiating the PACOH with Gaussian processes and
Bayesian Neural Networks models, the resulting methods are more scalable, and
yield state-of-the-art performance, both in terms of predictive accuracy and
the quality of uncertainty estimates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rothfuss_J/0/1/0/all/0/1&quot;&gt;Jonas Rothfuss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Josifoski_M/0/1/0/all/0/1&quot;&gt;Martin Josifoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Krause_A/0/1/0/all/0/1&quot;&gt;Andreas Krause&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09034">
<title>Graph Neural Networks are Inherently Good Generalizers: Insights by Bridging GNNs and MLPs. (arXiv:2212.09034v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09034</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs), as the de-facto model class for representation
learning on graphs, are built upon the multi-layer perceptrons (MLP)
architecture with additional message passing layers to allow features to flow
across nodes. While conventional wisdom commonly attributes the success of GNNs
to their advanced expressivity, we conjecture that this is not the main cause
of GNNs&apos; superiority in node-level prediction tasks. This paper pinpoints the
major source of GNNs&apos; performance gain to their intrinsic generalization
capability, by introducing an intermediate model class dubbed as
P(ropagational)MLP, which is identical to standard MLP in training, but then
adopts GNN&apos;s architecture in testing. Intriguingly, we observe that PMLPs
consistently perform on par with (or even exceed) their GNN counterparts, while
being much more efficient in training. This finding sheds new insights into
understanding the learning behavior of GNNs, and can be used as an analytic
tool for dissecting various GNN-related research problems. As an initial step
to analyze the inherent generalizability of GNNs, we show the essential
difference between MLP and PMLP at infinite-width limit lies in the NTK feature
map in the post-training stage. Moreover, by examining their extrapolation
behavior, we find that though many GNNs and their PMLP counterparts cannot
extrapolate non-linear functions for extremely out-of-distribution samples,
they have greater potential to generalize to testing samples near the training
data range as natural advantages of GNN architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenxiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qitian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiahua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12794">
<title>GraphCast: Learning skillful medium-range global weather forecasting. (arXiv:2212.12794v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12794</link>
<description rdf:parseType="Literal">&lt;p&gt;Global medium-range weather forecasting is critical to decision-making across
many social and economic domains. Traditional numerical weather prediction uses
increased compute resources to improve forecast accuracy, but cannot directly
use historical weather data to improve the underlying model. We introduce a
machine learning-based method called &quot;GraphCast&quot;, which can be trained directly
from reanalysis data. It predicts hundreds of weather variables, over 10 days
at 0.25 degree resolution globally, in under one minute. We show that GraphCast
significantly outperforms the most accurate operational deterministic systems
on 90% of 1380 verification targets, and its forecasts support better severe
event prediction, including tropical cyclones, atmospheric rivers, and extreme
temperatures. GraphCast is a key advance in accurate and efficient weather
forecasting, and helps realize the promise of machine learning for modeling
complex dynamical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_R/0/1/0/all/0/1&quot;&gt;Remi Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_Gonzalez_A/0/1/0/all/0/1&quot;&gt;Alvaro Sanchez-Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willson_M/0/1/0/all/0/1&quot;&gt;Matthew Willson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirnsberger_P/0/1/0/all/0/1&quot;&gt;Peter Wirnsberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortunato_M/0/1/0/all/0/1&quot;&gt;Meire Fortunato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alet_F/0/1/0/all/0/1&quot;&gt;Ferran Alet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravuri_S/0/1/0/all/0/1&quot;&gt;Suman Ravuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ewalds_T/0/1/0/all/0/1&quot;&gt;Timo Ewalds&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eaton_Rosen_Z/0/1/0/all/0/1&quot;&gt;Zach Eaton-Rosen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Weihua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merose_A/0/1/0/all/0/1&quot;&gt;Alexander Merose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoyer_S/0/1/0/all/0/1&quot;&gt;Stephan Hoyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holland_G/0/1/0/all/0/1&quot;&gt;George Holland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinyals_O/0/1/0/all/0/1&quot;&gt;Oriol Vinyals&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stott_J/0/1/0/all/0/1&quot;&gt;Jacklynn Stott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritzel_A/0/1/0/all/0/1&quot;&gt;Alexander Pritzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_S/0/1/0/all/0/1&quot;&gt;Shakir Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battaglia_P/0/1/0/all/0/1&quot;&gt;Peter Battaglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08780">
<title>SE(3) symmetry lets graph neural networks learn arterial velocity estimation from small datasets. (arXiv:2302.08780v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08780</link>
<description rdf:parseType="Literal">&lt;p&gt;Hemodynamic velocity fields in coronary arteries could be the basis of
valuable biomarkers for diagnosis, prognosis and treatment planning in
cardiovascular disease. Velocity fields are typically obtained from
patient-specific 3D artery models via computational fluid dynamics (CFD).
However, CFD simulation requires meticulous setup by experts and is
time-intensive, which hinders large-scale acceptance in clinical practice. To
address this, we propose graph neural networks (GNN) as an efficient black-box
surrogate method to estimate 3D velocity fields mapped to the vertices of
tetrahedral meshes of the artery lumen. We train these GNNs on synthetic artery
models and CFD-based ground truth velocity fields. Once the GNN is trained,
velocity estimates in a new and unseen artery can be obtained with 36-fold
speed-up compared to CFD. We demonstrate how to construct an SE(3)-equivariant
GNN that is independent of the spatial orientation of the input mesh and show
how this reduces the necessary amount of training data compared to a baseline
neural network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suk_J/0/1/0/all/0/1&quot;&gt;Julian Suk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1&quot;&gt;Christoph Brune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolterink_J/0/1/0/all/0/1&quot;&gt;Jelmer M. Wolterink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11239">
<title>Explainable Contextual Anomaly Detection using Quantile Regression Forests. (arXiv:2302.11239v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11239</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional anomaly detection methods aim to identify objects that deviate
from most other objects by treating all features equally. In contrast,
contextual anomaly detection methods aim to detect objects that deviate from
other objects within a context of similar objects by dividing the features into
contextual features and behavioral features. In this paper, we develop
connections between dependency-based traditional anomaly detection methods and
contextual anomaly detection methods. Based on resulting insights, we propose a
novel approach to inherently interpretable contextual anomaly detection that
uses Quantile Regression Forests to model dependencies between features.
Extensive experiments on various synthetic and real-world datasets demonstrate
that our method outperforms state-of-the-art anomaly detection methods in
identifying contextual anomalies in terms of accuracy and interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leeuwen_M/0/1/0/all/0/1&quot;&gt;Matthijs van Leeuwen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00516">
<title>Exploiting Multiple Abstractions in Episodic RL via Reward Shaping. (arXiv:2303.00516v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00516</link>
<description rdf:parseType="Literal">&lt;p&gt;One major limitation to the applicability of Reinforcement Learning (RL) to
many practical domains is the large number of samples required to learn an
optimal policy. To address this problem and improve learning efficiency, we
consider a linear hierarchy of abstraction layers of the Markov Decision
Process (MDP) underlying the target domain. Each layer is an MDP representing a
coarser model of the one immediately below in the hierarchy. In this work, we
propose a novel form of Reward Shaping where the solution obtained at the
abstract level is used to offer rewards to the more concrete MDP, in such a way
that the abstract solution guides the learning in the more complex domain. In
contrast with other works in Hierarchical RL, our technique has few
requirements in the design of the abstract models and it is also tolerant to
modeling errors, thus making the proposed approach practical. We formally
analyze the relationship between the abstract models and the exploration
heuristic induced in the lower-level domain. Moreover, we prove that the method
guarantees optimal convergence and we demonstrate its effectiveness
experimentally.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cipollone_R/0/1/0/all/0/1&quot;&gt;Roberto Cipollone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomo_G/0/1/0/all/0/1&quot;&gt;Giuseppe De Giacomo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favorito_M/0/1/0/all/0/1&quot;&gt;Marco Favorito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iocchi_L/0/1/0/all/0/1&quot;&gt;Luca Iocchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrizi_F/0/1/0/all/0/1&quot;&gt;Fabio Patrizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08757">
<title>CT Perfusion is All We Need: 4D CNN Segmentation of Penumbra and Core in Patients With Suspected Ischemic Stroke. (arXiv:2303.08757v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08757</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise and fast prediction methods for ischemic areas comprised of dead
tissue, core, and salvageable tissue, penumbra, in acute ischemic stroke (AIS)
patients are of significant clinical interest. They play an essential role in
improving diagnosis and treatment planning. Computed Tomography (CT) scan is
one of the primary modalities for early assessment in patients with suspected
AIS. CT Perfusion (CTP) is often used as a primary assessment to determine
stroke location, severity, and volume of ischemic lesions. Current automatic
segmentation methods for CTP mostly use already processed 3D parametric maps
conventionally used for clinical interpretation by radiologists as input.
Alternatively, the raw CTP data is used on a slice-by-slice basis as 2D+time
input, where the spatial information over the volume is ignored. In addition,
these methods are only interested in segmenting core regions, while predicting
penumbra can be essential for treatment planning. This paper investigates
different methods to utilize the entire 4D CTP as input to fully exploit the
spatio-temporal information, leading us to propose a novel 4D convolution
layer. Our comprehensive experiments on a local dataset of 152 patients divided
into three groups show that our proposed models generate more precise results
than other methods explored. Adopting the proposed 4D mJ-Net, a Dice
Coefficient of 0.53 and 0.23 is achieved for segmenting penumbra and core
areas, respectively. The code is available on
https://github.com/Biomedical-Data-Analysis-Laboratory/4D-mJ-Net.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tomasetti_L/0/1/0/all/0/1&quot;&gt;Luca Tomasetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Engan_K/0/1/0/all/0/1&quot;&gt;Kjersti Engan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollesli_L/0/1/0/all/0/1&quot;&gt;Liv Jorunn H&amp;#xf8;llesli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kurz_K/0/1/0/all/0/1&quot;&gt;Kathinka D&amp;#xe6;hli Kurz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanmohammadi_M/0/1/0/all/0/1&quot;&gt;Mahdieh Khanmohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13035">
<title>SPeC: A Soft Prompt-Based Calibration on Performance Variability of Large Language Model in Clinical Notes Summarization. (arXiv:2303.13035v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13035</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic health records (EHRs) store an extensive array of patient
information, encompassing medical histories, diagnoses, treatments, and test
outcomes. These records are crucial for enabling healthcare providers to make
well-informed decisions regarding patient care. Summarizing clinical notes
further assists healthcare professionals in pinpointing potential health risks
and making better-informed decisions. This process contributes to reducing
errors and enhancing patient outcomes by ensuring providers have access to the
most pertinent and current patient data. Recent research has shown that
incorporating prompts with large language models (LLMs) substantially boosts
the efficacy of summarization tasks. However, we show that this approach also
leads to increased output variance, resulting in notably divergent outputs even
when prompts share similar meanings. To tackle this challenge, we introduce a
model-agnostic Soft Prompt-Based Calibration (SPeC) pipeline that employs soft
prompts to diminish variance while preserving the advantages of prompt-based
summarization. Experimental findings on multiple clinical note tasks and LLMs
indicate that our method not only bolsters performance but also effectively
curbs variance for various LLMs, providing a more uniform and dependable
solution for summarizing vital medical information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yu-Neng Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Ruixiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02978">
<title>Simplifying Low-Light Image Enhancement Networks with Relative Loss Functions. (arXiv:2304.02978v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02978</link>
<description rdf:parseType="Literal">&lt;p&gt;Image enhancement is a common technique used to mitigate issues such as
severe noise, low brightness, low contrast, and color deviation in low-light
images. However, providing an optimal high-light image as a reference for
low-light image enhancement tasks is impossible, which makes the learning
process more difficult than other image processing tasks. As a result, although
several low-light image enhancement methods have been proposed, most of them
are either too complex or insufficient in addressing all the issues in
low-light images. In this paper, to make the learning easier in low-light image
enhancement, we introduce FLW-Net (Fast and LightWeight Network) and two
relative loss functions. Specifically, we first recognize the challenges of the
need for a large receptive field to obtain global contrast and the lack of an
absolute reference, which limits the simplification of network structures in
this task. Then, we propose an efficient global feature information extraction
component and two loss functions based on relative information to overcome
these challenges. Finally, we conducted comparative experiments to demonstrate
the effectiveness of the proposed method, and the results confirm that the
proposed method can significantly reduce the complexity of supervised low-light
image enhancement networks while improving processing effect. The code is
available at \url{https://github.com/hitzhangyu/FLW-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junde Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Rao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guohui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunhui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09981">
<title>Interpretable (not just posthoc-explainable) heterogeneous survivor bias-corrected treatment effects for assignment of postdischarge interventions to prevent readmissions. (arXiv:2304.09981v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09981</link>
<description rdf:parseType="Literal">&lt;p&gt;We used survival analysis to quantify the impact of postdischarge evaluation
and management (E/M) services in preventing hospital readmission or death. Our
approach avoids a specific pitfall of applying machine learning to this
problem, which is an inflated estimate of the effect of interventions, due to
survivors bias -- where the magnitude of inflation may be conditional on
heterogeneous confounders in the population. This bias arises simply because in
order to receive an intervention after discharge, a person must not have been
readmitted in the intervening period. After deriving an expression for this
phantom effect, we controlled for this and other biases within an inherently
interpretable Bayesian survival framework. We identified case management
services as being the most impactful for reducing readmissions overall.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xia_H/0/1/0/all/0/1&quot;&gt;Hongjing Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Joshua C. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nowak_S/0/1/0/all/0/1&quot;&gt;Sarah Nowak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahajan_S/0/1/0/all/0/1&quot;&gt;Sonya Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahajan_R/0/1/0/all/0/1&quot;&gt;Rohit Mahajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chang_T/0/1/0/all/0/1&quot;&gt;Ted L. Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chow_C/0/1/0/all/0/1&quot;&gt;Carson C. Chow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05237">
<title>Traffic Forecasting on New Roads Unseen in the Training Data Using Spatial Contrastive Pre-Training. (arXiv:2305.05237v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05237</link>
<description rdf:parseType="Literal">&lt;p&gt;New roads are being constructed all the time. However, the capabilities of
previous deep forecasting models to generalize to new roads not seen in the
training data (unseen roads) are rarely explored. In this paper, we introduce a
novel setup called a spatio-temporal (ST) split to evaluate the models&apos;
capabilities to generalize to unseen roads. In this setup, the models are
trained on data from a sample of roads, but tested on roads not seen in the
training data. Moreover, we also present a novel framework called Spatial
Contrastive Pre-Training (SCPT) where we introduce a spatial encoder module to
extract latent features from unseen roads during inference time. This spatial
encoder is pre-trained using contrastive learning. During inference, the
spatial encoder only requires two days of traffic data on the new roads and
does not require any re-training. We also show that the output from the spatial
encoder can be used effectively to infer latent node embeddings on unseen roads
during inference time. The SCPT framework also incorporates a new layer, named
the spatially gated addition (SGA) layer, to effectively combine the latent
features from the output of the spatial encoder to existing backbones.
Additionally, since there is limited data on the unseen roads, we argue that it
is better to decouple traffic signals to trivial-to-capture periodic signals
and difficult-to-capture Markovian signals, and for the spatial encoder to only
learn the Markovian signals. Finally, we empirically evaluated SCPT using the
ST split setup on four real-world datasets. The results showed that adding SCPT
to a backbone consistently improves forecasting performance on unseen roads.
More importantly, the improvements are greater when forecasting further into
the future. The codes are available on GitHub:
\burl{https://github.com/cruiseresearchgroup/forecasting-on-new-roads}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabowo_A/0/1/0/all/0/1&quot;&gt;Arian Prabowo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_W/0/1/0/all/0/1&quot;&gt;Wei Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koniusz_P/0/1/0/all/0/1&quot;&gt;Piotr Koniusz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salim_F/0/1/0/all/0/1&quot;&gt;Flora D. Salim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05318">
<title>How Informative is the Approximation Error from Tensor Decomposition for Neural Network Compression?. (arXiv:2305.05318v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05318</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor decompositions have been successfully applied to compress neural
networks. The compression algorithms using tensor decompositions commonly
minimize the approximation error on the weights. Recent work assumes the
approximation error on the weights is a proxy for the performance of the model
to compress multiple layers and fine-tune the compressed model. Surprisingly,
little research has systematically evaluated which approximation errors can be
used to make choices regarding the layer, tensor decomposition method, and
level of compression. To close this gap, we perform an experimental study to
test if this assumption holds across different layers and types of
decompositions, and what the effect of fine-tuning is. We include the
approximation error on the features resulting from a compressed layer in our
analysis to test if this provides a better proxy, as it explicitly takes the
data into account. We find the approximation error on the weights has a
positive correlation with the performance error, before as well as after
fine-tuning. Basing the approximation error on the features does not improve
the correlation significantly. While scaling the approximation error commonly
is used to account for the different sizes of layers, the average correlation
across layers is smaller than across all choices (i.e. layers, decompositions,
and level of compression) before fine-tuning. When calculating the correlation
across the different decompositions, the average rank correlation is larger
than across all choices. This means multiple decompositions can be considered
for compression and the approximation error can be used to choose between them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_J/0/1/0/all/0/1&quot;&gt;Jetze T. Schuurmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batselier_K/0/1/0/all/0/1&quot;&gt;Kim Batselier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kooij_J/0/1/0/all/0/1&quot;&gt;Julian F. P. Kooij&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13498">
<title>Parameter estimation from an Ornstein-Uhlenbeck process with measurement noise. (arXiv:2305.13498v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13498</link>
<description rdf:parseType="Literal">&lt;p&gt;This article aims to investigate the impact of noise on parameter fitting for
an Ornstein-Uhlenbeck process, focusing on the effects of multiplicative and
thermal noise on the accuracy of signal separation. To address these issues, we
propose algorithms and methods that can effectively distinguish between thermal
and multiplicative noise and improve the precision of parameter estimation for
optimal data analysis. Specifically, we explore the impact of both
multiplicative and thermal noise on the obfuscation of the actual signal and
propose methods to resolve them. Firstly, we present an algorithm that can
effectively separate thermal noise with comparable performance to Hamilton
Monte Carlo (HMC) but with significantly improved speed. Subsequently, we
analyze multiplicative noise and demonstrate that HMC is insufficient for
isolating thermal and multiplicative noise. However, we show that, with
additional knowledge of the ratio between thermal and multiplicative noise, we
can accurately distinguish between the two types of noise when provided with a
sufficiently large sampling rate or an amplitude of multiplicative noise
smaller than thermal noise. This finding results in a situation that initially
seems counterintuitive. When multiplicative noise dominates the noise spectrum,
we can successfully estimate the parameters for such systems after adding
additional white noise to shift the noise balance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Carter_S/0/1/0/all/0/1&quot;&gt;Simon Carter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Strey_H/0/1/0/all/0/1&quot;&gt;Helmut H. Strey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16729">
<title>Evaluating generation of chaotic time series by convolutional generative adversarial networks. (arXiv:2305.16729v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16729</link>
<description rdf:parseType="Literal">&lt;p&gt;To understand the ability and limitations of convolutional neural networks to
generate time series that mimic complex temporal signals, we trained a
generative adversarial network consisting of deep convolutional networks to
generate chaotic time series and used nonlinear time series analysis to
evaluate the generated time series. A numerical measure of determinism and the
Lyapunov exponent, a measure of trajectory instability, showed that the
generated time series well reproduce the chaotic properties of the original
time series. However, error distribution analyses showed that large errors
appeared at a low but non-negligible rate. Such errors would not be expected if
the distribution were assumed to be exponential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_Y/0/1/0/all/0/1&quot;&gt;Yuki Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguti_Y/0/1/0/all/0/1&quot;&gt;Yutaka Yamaguti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19148">
<title>Mitigating Label Biases for In-context Learning. (arXiv:2305.19148v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19148</link>
<description rdf:parseType="Literal">&lt;p&gt;Various design settings for in-context learning (ICL), such as the choice and
order of the in-context examples, can bias a model toward a particular
prediction without being reflective of an understanding of the task. While many
studies discuss these design choices, there have been few systematic
investigations into categorizing them and mitigating their impact. In this
work, we define a typology for three types of label biases in ICL for text
classification: vanilla-label bias, context-label bias, and domain-label bias
(which we conceptualize and detect for the first time).
&lt;/p&gt;
&lt;p&gt;Our analysis demonstrates that prior label bias calibration methods fall
short of addressing all three types of biases. Specifically, domain-label bias
restricts LLMs to random-level performance on many tasks regardless of the
choice of in-context examples. To mitigate the effect of these biases, we
propose a simple bias calibration method that estimates a language model&apos;s
label bias using random in-domain words from the task corpus. After controlling
for this estimated bias when making predictions, our novel domain-context
calibration significantly improves the ICL performance of GPT-J and GPT-3 on a
wide range of tasks. The gain is substantial on tasks with large domain-label
bias (up to 37% in Macro-F1). Furthermore, our results generalize to models
with different scales, pretraining methods, and manually-designed task
instructions, showing the prevalence of label biases in ICL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Y/0/1/0/all/0/1&quot;&gt;Yu Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yifan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zeming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosselut_A/0/1/0/all/0/1&quot;&gt;Antoine Bosselut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19158">
<title>Competing for Shareable Arms in Multi-Player Multi-Armed Bandits. (arXiv:2305.19158v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19158</link>
<description rdf:parseType="Literal">&lt;p&gt;Competitions for shareable and limited resources have long been studied with
strategic agents. In reality, agents often have to learn and maximize the
rewards of the resources at the same time. To design an individualized
competing policy, we model the competition between agents in a novel
multi-player multi-armed bandit (MPMAB) setting where players are selfish and
aim to maximize their own rewards. In addition, when several players pull the
same arm, we assume that these players averagely share the arms&apos; rewards by
expectation. Under this setting, we first analyze the Nash equilibrium when
arms&apos; rewards are known. Subsequently, we propose a novel Selfish MPMAB with
Averaging Allocation (SMAA) approach based on the equilibrium. We theoretically
demonstrate that SMAA could achieve a good regret guarantee for each player
when all players follow the algorithm. Additionally, we establish that no
single selfish player can significantly increase their rewards through
deviation, nor can they detrimentally affect other players&apos; rewards without
incurring substantial losses for themselves. We finally validate the
effectiveness of the method in extensive synthetic experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renzhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haotian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingxuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_P/0/1/0/all/0/1&quot;&gt;Peng Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02715">
<title>Federated Deep Learning for Intrusion Detection in IoT Networks. (arXiv:2306.02715v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02715</link>
<description rdf:parseType="Literal">&lt;p&gt;The vast increase of Internet of Things (IoT) technologies and the
ever-evolving attack vectors have increased cyber-security risks dramatically.
A common approach to implementing AI-based Intrusion Detection systems (IDSs)
in distributed IoT systems is in a centralised manner. However, this approach
may violate data privacy and prohibit IDS scalability. Therefore, intrusion
detection solutions in IoT ecosystems need to move towards a decentralised
direction. Federated Learning (FL) has attracted significant interest in recent
years due to its ability to perform collaborative learning while preserving
data confidentiality and locality. Nevertheless, most FL-based IDS for IoT
systems are designed under unrealistic data distribution conditions. To that
end, we design an experiment representative of the real world and evaluate the
performance of an FL-based IDS. For our experiments, we rely on TON-IoT, a
realistic IoT network traffic dataset, associating each IP address with a
single FL client. Additionally, we explore pre-training and investigate various
aggregation methods to mitigate the impact of data heterogeneity. Lastly, we
benchmark our approach against a centralised solution. The comparison shows
that the heterogeneous nature of the data has a considerable negative impact on
the model&apos;s performance when trained in a distributed manner. However, in the
case of a pre-trained initial global FL model, we demonstrate a performance
improvement of over 20% (F1-score) compared to a randomly initiated global
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belarbi_O/0/1/0/all/0/1&quot;&gt;Othmane Belarbi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spyridopoulos_T/0/1/0/all/0/1&quot;&gt;Theodoros Spyridopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthi_E/0/1/0/all/0/1&quot;&gt;Eirini Anthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavromatis_I/0/1/0/all/0/1&quot;&gt;Ioannis Mavromatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carnelli_P/0/1/0/all/0/1&quot;&gt;Pietro Carnelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Aftab Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05722">
<title>Ridge Estimation with Nonlinear Transformations. (arXiv:2306.05722v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05722</link>
<description rdf:parseType="Literal">&lt;p&gt;Ridge estimation is an important manifold learning technique. The goal of
this paper is to examine the effects of nonlinear transformations on the ridge
sets. The main result proves the inclusion relationship between ridges:
$\cR(f\circ p)\subseteq \cR(p)$, provided that the transformation $f$ is
strictly increasing and concave on the range of the function $p$. Additionally,
given an underlying true manifold $\cM$, we show that the Hausdorff distance
between $\cR(f\circ p)$ and its projection onto $\cM$ is smaller than the
Hausdorff distance between $\cR(p)$ and the corresponding projection. This
motivates us to apply an increasing and concave transformation before the ridge
estimation. In specific, we show that the power transformations
$f^{q}(y)=y^q/q,-\infty&amp;lt;q\leq 1$ are increasing and concave on $\RR_+$, and
thus we can use such power transformations when $p$ is strictly positive.
Numerical experiments demonstrate the advantages of the proposed methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hengchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Z/0/1/0/all/0/1&quot;&gt;Zhigang Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06125">
<title>Joint Channel Estimation and Feedback with Masked Token Transformers in Massive MIMO Systems. (arXiv:2306.06125v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06125</link>
<description rdf:parseType="Literal">&lt;p&gt;The downlink channel state information (CSI) estimation and low overhead
acquisition are the major challenges for massive MIMO systems in frequency
division duplex to enable high MIMO gain. Recently, numerous studies have been
conducted to harness the power of deep neural networks for better channel
estimation and feedback. However, existing methods have yet to fully exploit
the intrinsic correlation features present in CSI. As a consequence, distinct
network structures are utilized for handling these two tasks separately. To
achieve joint channel estimation and feedback, this paper proposes an
encoder-decoder based network that unveils the intrinsic frequency-domain
correlation within the CSI matrix. The entire encoder-decoder network is
utilized for channel compression. To effectively capture and restructure
correlation features, a self-mask-attention coding is proposed, complemented by
an active masking strategy designed to improve efficiency. The channel
estimation is achieved through the decoder part, wherein a lightweight
multilayer perceptron denoising module is utilized for further accurate
estimation. Extensive experiments demonstrate that our method not only
outperforms state-of-the-art channel estimation and feedback techniques in
joint tasks but also achieves beneficial performance in individual tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mingming Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lifu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15374">
<title>LeCo: Lightweight Compression via Learning Serial Correlations. (arXiv:2306.15374v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15374</link>
<description rdf:parseType="Literal">&lt;p&gt;Lightweight data compression is a key technique that allows column stores to
exhibit superior performance for analytical queries. Despite a comprehensive
study on dictionary-based encodings to approach Shannon&apos;s entropy, few prior
works have systematically exploited the serial correlation in a column for
compression. In this paper, we propose LeCo (i.e., Learned Compression), a
framework that uses machine learning to remove the serial redundancy in a value
sequence automatically to achieve an outstanding compression ratio and
decompression performance simultaneously. LeCo presents a general approach to
this end, making existing (ad-hoc) algorithms such as Frame-of-Reference (FOR),
Delta Encoding, and Run-Length Encoding (RLE) special cases under our
framework. Our microbenchmark with three synthetic and six real-world data sets
shows that a prototype of LeCo achieves a Pareto improvement on both
compression ratio and random access speed over the existing solutions. When
integrating LeCo into widely-used applications, we observe up to 3.9x speed up
in filter-scanning a Parquet file and a 16% increase in Rocksdb&apos;s throughput.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yihao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xinyu Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huanchen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07264">
<title>On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07264</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning with expert advice and multi-armed bandit are two classic online
decision problems which differ on how the information is observed in each round
of the game. We study a family of problems interpolating the two. For a vector
$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB
indicates that the arms are partitioned into $K$ groups and the $i$-th group
contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same
group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB
and design an optimal PAC algorithm for its pure exploration version,
$\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with
as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB
is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number
of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is
$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Both
our upper bounds and lower bounds for $\mathbf{m}$-MAB can be extended to a
more general setting, namely the bandit with graph feedback, in terms of the
clique cover and related graph parameters. As consequences, we obtained tight
minimax regret bounds for several families of feedback graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Houshuang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuchen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chihao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11608">
<title>Learning minimal representations of stochastic processes with variational autoencoders. (arXiv:2307.11608v2 [cond-mat.soft] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11608</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic processes have found numerous applications in science, as they are
broadly used to model a variety of natural phenomena. Due to their intrinsic
randomness and uncertainty, they are however difficult to characterize. Here,
we introduce an unsupervised machine learning approach to determine the minimal
set of parameters required to effectively describe the dynamics of a stochastic
process. Our method builds upon an extended $\beta$-variational autoencoder
architecture. By means of simulated datasets corresponding to paradigmatic
diffusion models, we showcase its effectiveness in extracting the minimal
relevant parameters that accurately describe these dynamics. Furthermore, the
method enables the generation of new trajectories that faithfully replicate the
expected stochastic behavior. Overall, our approach enables for the autonomous
discovery of unknown parameters describing stochastic processes, hence
enhancing our comprehension of complex phenomena across various fields.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Fernandez_Fernandez_G/0/1/0/all/0/1&quot;&gt;Gabriel Fern&amp;#xe1;ndez-Fern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Manzo_C/0/1/0/all/0/1&quot;&gt;Carlo Manzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Lewenstein_M/0/1/0/all/0/1&quot;&gt;Maciej Lewenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Dauphin_A/0/1/0/all/0/1&quot;&gt;Alexandre Dauphin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Munoz_Gil_G/0/1/0/all/0/1&quot;&gt;Gorka Mu&amp;#xf1;oz-Gil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15299">
<title>Differential Evolution Algorithm based Hyper-Parameters Selection of Transformer Neural Network Model for Load Forecasting. (arXiv:2307.15299v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15299</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate load forecasting plays a vital role in numerous sectors, but
accurately capturing the complex dynamics of dynamic power systems remains a
challenge for traditional statistical models. For these reasons, time-series
models (ARIMA) and deep-learning models (ANN, LSTM, GRU, etc.) are commonly
deployed and often experience higher success. In this paper, we analyze the
efficacy of the recently developed Transformer-based Neural Network model in
Load forecasting. Transformer models have the potential to improve Load
forecasting because of their ability to learn long-range dependencies derived
from their Attention Mechanism. We apply several metaheuristics namely
Differential Evolution to find the optimal hyperparameters of the
Transformer-based Neural Network to produce accurate forecasts. Differential
Evolution provides scalable, robust, global solutions to non-differentiable,
multi-objective, or constrained optimization problems. Our work compares the
proposed Transformer based Neural Network model integrated with different
metaheuristic algorithms by their performance in Load forecasting based on
numerical metrics such as Mean Squared Error (MSE) and Mean Absolute Percentage
Error (MAPE). Our findings demonstrate the potential of metaheuristic-enhanced
Transformer-based Neural Network models in Load forecasting accuracy and
provide optimal hyperparameters for each model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1&quot;&gt;Anuvab Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumder_A/0/1/0/all/0/1&quot;&gt;Arul Rhik Mazumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_U/0/1/0/all/0/1&quot;&gt;Udayon Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15539">
<title>Beating Backdoor Attack at Its Own Game. (arXiv:2307.15539v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15539</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are vulnerable to backdoor attack, which does not
affect the network&apos;s performance on clean data but would manipulate the network
behavior once a trigger pattern is added. Existing defense methods have greatly
reduced attack success rate, but their prediction accuracy on clean data still
lags behind a clean model by a large margin. Inspired by the stealthiness and
effectiveness of backdoor attack, we propose a simple but highly effective
defense framework which injects non-adversarial backdoors targeting poisoned
samples. Following the general steps in backdoor attack, we detect a small set
of suspected samples and then apply a poisoning strategy to them. The
non-adversarial backdoor, once triggered, suppresses the attacker&apos;s backdoor on
poisoned data, but has limited influence on clean data. The defense can be
carried out during data preprocessing, without any modification to the standard
end-to-end training pipeline. We conduct extensive experiments on multiple
benchmarks with different architectures and representative attacks. Results
demonstrate that our method achieves state-of-the-art defense effectiveness
with by far the lowest performance drop on clean data. Considering the
surprising defense ability displayed by our framework, we call for more
attention to utilizing backdoor for backdoor defense. Code is available at
https://github.com/damianliumin/non-adversarial_backdoor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangiovanni_Vincentelli_A/0/1/0/all/0/1&quot;&gt;Alberto Sangiovanni-Vincentelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15980">
<title>Initial State Interventions for Deconfounded Imitation Learning. (arXiv:2307.15980v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15980</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning suffers from causal confusion. This phenomenon occurs when
learned policies attend to features that do not causally influence the expert
actions but are instead spuriously correlated. Causally confused agents produce
low open-loop supervised loss but poor closed-loop performance upon deployment.
We consider the problem of masking observed confounders in a disentangled
representation of the observation space. Our novel masking algorithm leverages
the usual ability to intervene in the initial system state, avoiding any
requirement involving expert querying, expert reward functions, or causal graph
specification. Under certain assumptions, we theoretically prove that this
algorithm is conservative in the sense that it does not incorrectly mask
observations that causally influence the expert; furthermore, intervening on
the initial state serves to strictly reduce excess conservatism. The masking
algorithm is applied to behavior cloning for two illustrative control systems:
CartPole and Reacher.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfrommer_S/0/1/0/all/0/1&quot;&gt;Samuel Pfrommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yatong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1&quot;&gt;Somayeh Sojoudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16104">
<title>AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16104</link>
<description rdf:parseType="Literal">&lt;p&gt;Floods are one of the most common and impactful natural disasters, with a
disproportionate impact in developing countries that often lack dense
streamflow monitoring networks. Accurate and timely warnings are critical for
mitigating flood risks, but accurate hydrological simulation models typically
must be calibrated to long data records in each watershed where they are
applied. We developed an Artificial Intelligence (AI) model to predict extreme
hydrological events at timescales up to 7 days in advance. This model
significantly outperforms current state of the art global hydrology models (the
Copernicus Emergency Management Service Global Flood Awareness System) across
all continents, lead times, and return periods. AI is especially effective at
forecasting in ungauged basins, which is important because only a few percent
of the world&apos;s watersheds have stream gauges, with a disproportionate number of
ungauged basins in developing countries that are especially vulnerable to the
human impacts of flooding. We produce forecasts of extreme events in South
America and Africa that achieve reliability approaching the current state of
the art in Europe and North America, and we achieve reliability at between 4
and 6-day lead times that are similar to current state of the art nowcasts
(0-day lead time). Additionally, we achieve accuracies over 10-year return
period events that are similar to current accuracies over 2-year return period
events, meaning that AI can provide warnings earlier and over larger and more
impactful events. The model that we develop in this paper has been incorporated
into an operational early warning system that produces publicly available (free
and open) forecasts in real time in over 80 countries. This work using AI and
open data highlights a need for increasing the availability of hydrological
data to continue to improve global access to reliable flood warnings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nearing_G/0/1/0/all/0/1&quot;&gt;Grey Nearing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1&quot;&gt;Deborah Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dube_V/0/1/0/all/0/1&quot;&gt;Vusumuzi Dube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_M/0/1/0/all/0/1&quot;&gt;Martin Gauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilon_O/0/1/0/all/0/1&quot;&gt;Oren Gilon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrigan_S/0/1/0/all/0/1&quot;&gt;Shaun Harrigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1&quot;&gt;Avinatan Hassidim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratzert_F/0/1/0/all/0/1&quot;&gt;Frederik Kratzert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzger_A/0/1/0/all/0/1&quot;&gt;Asher Metzger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevo_S/0/1/0/all/0/1&quot;&gt;Sella Nevo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappenberger_F/0/1/0/all/0/1&quot;&gt;Florian Pappenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prudhomme_C/0/1/0/all/0/1&quot;&gt;Christel Prudhomme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalev_G/0/1/0/all/0/1&quot;&gt;Guy Shalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenzis_S/0/1/0/all/0/1&quot;&gt;Shlomo Shenzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekalign_T/0/1/0/all/0/1&quot;&gt;Tadele Tekalign&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weitzner_D/0/1/0/all/0/1&quot;&gt;Dana Weitzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yoss Matias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01119">
<title>Unlearning Spurious Correlations in Chest X-ray Classification. (arXiv:2308.01119v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01119</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image classification models are frequently trained using training
datasets derived from multiple data sources. While leveraging multiple data
sources is crucial for achieving model generalization, it is important to
acknowledge that the diverse nature of these sources inherently introduces
unintended confounders and other challenges that can impact both model accuracy
and transparency. A notable confounding factor in medical image classification,
particularly in musculoskeletal image classification, is skeletal
maturation-induced bone growth observed during adolescence. We train a deep
learning model using a Covid-19 chest X-ray dataset and we showcase how this
dataset can lead to spurious correlations due to unintended confounding
regions. eXplanation Based Learning (XBL) is a deep learning approach that goes
beyond interpretability by utilizing model explanations to interactively
unlearn spurious correlations. This is achieved by integrating interactive user
feedback, specifically feature annotations. In our study, we employed two
non-demanding manual feedback mechanisms to implement an XBL-based approach for
effectively eliminating these spurious correlations. Our results underscore the
promising potential of XBL in constructing robust models even in the presence
of confounding factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hagos_M/0/1/0/all/0/1&quot;&gt;Misgina Tsighe Hagos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Curran_K/0/1/0/all/0/1&quot;&gt;Kathleen M. Curran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Namee_B/0/1/0/all/0/1&quot;&gt;Brian Mac Namee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01404">
<title>Hoodwinked: Deception and Cooperation in a Text-Based Game for Language Models. (arXiv:2308.01404v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01404</link>
<description rdf:parseType="Literal">&lt;p&gt;Are current language models capable of deception and lie detection? We study
this question by introducing a text-based game called $\textit{Hoodwinked}$,
inspired by Mafia and Among Us. Players are locked in a house and must find a
key to escape, but one player is tasked with killing the others. Each time a
murder is committed, the surviving players have a natural language discussion
then vote to banish one player from the game. We conduct experiments with
agents controlled by GPT-3, GPT-3.5, and GPT-4 and find evidence of deception
and lie detection capabilities. The killer often denies their crime and accuses
others, leading to measurable effects on voting outcomes. More advanced models
are more effective killers, outperforming smaller models in 18 of 24 pairwise
comparisons. Secondary metrics provide evidence that this improvement is not
mediated by different actions, but rather by stronger persuasive skills during
discussions. To evaluate the ability of AI agents to deceive humans, we make
this game publicly available at h https://hoodwinked.ai/ .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OGara_A/0/1/0/all/0/1&quot;&gt;Aidan O&amp;#x27;Gara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01867">
<title>MRQ:Support Multiple Quantization Schemes through Model Re-Quantization. (arXiv:2308.01867v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01867</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the proliferation of diverse hardware accelerators (e.g., NPU, TPU,
DPU), deploying deep learning models on edge devices with fixed-point hardware
is still challenging due to complex model quantization and conversion. Existing
model quantization frameworks like Tensorflow QAT [1], TFLite PTQ [2], and
Qualcomm AIMET [3] supports only a limited set of quantization schemes (e.g.,
only asymmetric per-tensor quantization in TF1.x QAT [4]). Accordingly, deep
learning models cannot be easily quantized for diverse fixed-point hardwares,
mainly due to slightly different quantization requirements. In this paper, we
envision a new type of model quantization approach called MRQ (model
re-quantization), which takes existing quantized models and quickly transforms
the models to meet different quantization requirements (e.g., asymmetric -&amp;gt;
symmetric, non-power-of-2 scale -&amp;gt; power-of-2 scale). Re-quantization is much
simpler than quantizing from scratch because it avoids costly re-training and
provides support for multiple quantization schemes simultaneously. To minimize
re-quantization error, we developed a new set of re-quantization algorithms
including weight correction and rounding error folding. We have demonstrated
that MobileNetV2 QAT model [7] can be quickly re-quantized into two different
quantization schemes (i.e., symmetric and symmetric+power-of-2 scale) with less
than 0.64 units of accuracy loss. We believe our work is the first to leverage
this concept of re-quantization for model quantization and models obtained from
the re-quantization process have been successfully deployed on NNA in the Echo
Show devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manohara_M/0/1/0/all/0/1&quot;&gt;Manasa Manohara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayal_S/0/1/0/all/0/1&quot;&gt;Sankalp Dayal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afzal_T/0/1/0/all/0/1&quot;&gt;Tariq Afzal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakshi_R/0/1/0/all/0/1&quot;&gt;Rahul Bakshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kahkuen Fu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>