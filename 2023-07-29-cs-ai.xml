<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14346" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14355" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1912.13122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2005.00695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.08960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.10206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.02626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.11277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.03622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.12672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.00052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.11838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.06589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02109" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13494" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13813" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.14343">
<title>Pruning Distorted Images in MNIST Handwritten Digits. (arXiv:2307.14343v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14343</link>
<description rdf:parseType="Literal">&lt;p&gt;Recognizing handwritten digits is a challenging task primarily due to the
diversity of writing styles and the presence of noisy images. The widely used
MNIST dataset, which is commonly employed as a benchmark for this task,
includes distorted digits with irregular shapes, incomplete strokes, and
varying skew in both the training and testing datasets. Consequently, these
factors contribute to reduced accuracy in digit recognition. To overcome this
challenge, we propose a two-stage deep learning approach. In the first stage,
we create a simple neural network to identify distorted digits within the
training set. This model serves to detect and filter out such distorted and
ambiguous images. In the second stage, we exclude these identified images from
the training dataset and proceed to retrain the model using the filtered
dataset. This process aims to improve the classification accuracy and
confidence levels while mitigating issues of underfitting and overfitting. Our
experimental results demonstrate the effectiveness of the proposed approach,
achieving an accuracy rate of over 99.5% on the testing dataset. This
significant improvement showcases the potential of our method in enhancing
digit classification accuracy. In our future work, we intend to explore the
scalability of this approach and investigate techniques to further enhance
accuracy by reducing the size of the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_A/0/1/0/all/0/1&quot;&gt;Amarnath R&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_V/0/1/0/all/0/1&quot;&gt;Vinay Kumar V&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14344">
<title>Strictly Low Rank Constraint Optimization -- An Asymptotically $\mathcal{O}(\frac{1}{t^2})$ Method. (arXiv:2307.14344v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.14344</link>
<description rdf:parseType="Literal">&lt;p&gt;We study a class of non-convex and non-smooth problems with \textit{rank}
regularization to promote sparsity in optimal solution. We propose to apply the
proximal gradient descent method to solve the problem and accelerate the
process with a novel support set projection operation on the singular values of
the intermediate update. We show that our algorithms are able to achieve a
convergence rate of $O(\frac{1}{t^2})$, which is exactly same as Nesterov&apos;s
optimal convergence rate for first-order methods on smooth and convex problems.
Strict sparsity can be expected and the support set of singular values during
each update is monotonically shrinking, which to our best knowledge, is novel
in momentum-based algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kai Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14346">
<title>Multi-objective Deep Reinforcement Learning for Mobile Edge Computing. (arXiv:2307.14346v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.14346</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile edge computing (MEC) is essential for next-generation mobile network
applications that prioritize various performance metrics, including delays and
energy consumption. However, conventional single-objective scheduling solutions
cannot be directly applied to practical systems in which the preferences of
these applications (i.e., the weights of different objectives) are often
unknown or challenging to specify in advance. In this study, we address this
issue by formulating a multi-objective offloading problem for MEC with multiple
edges to minimize expected long-term energy consumption and transmission delay
while considering unknown preferences as parameters. To address the challenge
of unknown preferences, we design a multi-objective (deep) reinforcement
learning (MORL)-based resource scheduling scheme with proximal policy
optimization (PPO). In addition, we introduce a well-designed state encoding
method for constructing features for multiple edges in MEC systems, a
sophisticated reward function for accurately computing the utilities of delay
and energy consumption. Simulation results demonstrate that our proposed MORL
scheme enhances the hypervolume of the Pareto front by up to 233.1% compared to
benchmarks. Our full framework is available at
https://github.com/gracefulning/mec_morl_multipolicy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Ning Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Junrui Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Ming Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14349">
<title>Copilot for Xcode: Exploring AI-Assisted Programming by Prompting Cloud-based Large Language Models. (arXiv:2307.14349v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.14349</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an AI-assisted programming tool called Copilot for Xcode
for program composition and design to support human software developers. By
seamlessly integrating cloud-based Large Language Models (LLM) with Apple&apos;s
local development environment, Xcode, this tool enhances productivity and
unleashes creativity for software development in Apple software ecosystem
(e.g., iOS apps, macOS). Leveraging advanced natural language processing (NLP)
techniques, Copilot for Xcode effectively processes source code tokens and
patterns within code repositories, enabling features such as code generation,
autocompletion, documentation, and error detection. Software developers can
also query and make &quot;small&quot; decisions for program composition, some of which
can be made simultaneously, and this is facilitated through prompt engineering
in a chat interface of Copilot for Xcode. Finally, we present simple case
studies as evidence of the effectiveness of utilizing NLP in Xcode to prompt
popular LLM services like OpenAI ChatGPT for program composition and design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chee Wei Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shangxin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_M/0/1/0/all/0/1&quot;&gt;Man Fai Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1&quot;&gt;Ching Nam Hang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14355">
<title>Framing Relevance for Safety-Critical Autonomous Systems. (arXiv:2307.14355v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14355</link>
<description rdf:parseType="Literal">&lt;p&gt;We are in the process of building complex highly autonomous systems that have
build-in beliefs, perceive their environment and exchange information. These
systems construct their respective world view and based on it they plan their
future manoeuvres, i.e., they choose their actions in order to establish their
goals based on their prediction of the possible futures. Usually these systems
face an overwhelming flood of information provided by a variety of sources
where by far not everything is relevant. The goal of our work is to develop a
formal approach to determine what is relevant for a safety critical autonomous
system at its current mission, i.e., what information suffices to build an
appropriate world view to accomplish its mission goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakow_A/0/1/0/all/0/1&quot;&gt;Astrid Rakow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14361">
<title>A Hybrid Machine Learning Model for Classifying Gene Mutations in Cancer using LSTM, BiLSTM, CNN, GRU, and GloVe. (arXiv:2307.14361v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.14361</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents an ensemble model combining LSTM, BiLSTM, CNN, GRU, and
GloVe to classify gene mutations using Kaggle&apos;s Personalized Medicine:
Redefining Cancer Treatment dataset. The results were compared against
well-known transformers like as BERT, Electra, Roberta, XLNet, Distilbert, and
their LSTM ensembles. Our model outperformed all other models in terms of
accuracy, precision, recall, F1 score, and Mean Squared Error. Surprisingly, it
also needed less training time, resulting in a perfect combination of
performance and efficiency. This study demonstrates the utility of ensemble
models for difficult tasks such as gene mutation classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aburass_S/0/1/0/all/0/1&quot;&gt;Sanad Aburass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dorgham_O/0/1/0/all/0/1&quot;&gt;Osama Dorgham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shaqsi_J/0/1/0/all/0/1&quot;&gt;Jamil Al Shaqsi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14364">
<title>Federated Distributionally Robust Optimization with Non-Convex Objectives: Algorithm and Analysis. (arXiv:2307.14364v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.14364</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributionally Robust Optimization (DRO), which aims to find an optimal
decision that minimizes the worst case cost over the ambiguity set of
probability distribution, has been widely applied in diverse applications,
e.g., network behavior analysis, risk management, etc. However, existing DRO
techniques face three key challenges: 1) how to deal with the asynchronous
updating in a distributed environment; 2) how to leverage the prior
distribution effectively; 3) how to properly adjust the degree of robustness
according to different scenarios. To this end, we propose an asynchronous
distributed algorithm, named Asynchronous Single-looP alternatIve gRadient
projEction (ASPIRE) algorithm with the itErative Active SEt method (EASE) to
tackle the federated distributionally robust optimization (FDRO) problem.
Furthermore, a new uncertainty set, i.e., constrained D-norm uncertainty set,
is developed to effectively leverage the prior distribution and flexibly
control the degree of robustness. Finally, our theoretical analysis elucidates
that the proposed algorithm is guaranteed to converge and the iteration
complexity is also analyzed. Extensive empirical studies on real-world datasets
demonstrate that the proposed method can not only achieve fast convergence, and
remain robust against data heterogeneity as well as malicious attacks, but also
tradeoff robustness with performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yang Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dongjin Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14368">
<title>Synthesis of Procedural Models for Deterministic Transition Systems. (arXiv:2307.14368v1 [cs.FL])</title>
<link>http://arxiv.org/abs/2307.14368</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a general approach for synthesizing procedural models
of the state-transitions of a given discrete system. The approach is general in
that it accepts different target languages for modeling the state-transitions
of a discrete system; different model acquisition tasks with different target
languages, such as the synthesis of STRIPS action models, or the update rule of
a cellular automaton, fit as particular instances of our general approach. We
follow an inductive approach to synthesis meaning that a set of examples of
state-transitions, represented as (pre-state, action, post-state) tuples, are
given as input. The goal is to synthesize a structured program that, when
executed on a given pre-state, outputs its associated post-state. Our synthesis
method implements a combinatorial search in the space of well-structured
terminating programs that can be built using a Random-Access Machine (RAM),
with a minimalist instruction set, and a finite amount of memory. The
combinatorial search is guided with functions that asses the complexity of the
candidate programs, as well as their fitness to the given input set of
examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segovia_Aguas_J/0/1/0/all/0/1&quot;&gt;Javier Segovia-Aguas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_Mestres_J/0/1/0/all/0/1&quot;&gt;Jonathan Ferrer-Mestres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_S/0/1/0/all/0/1&quot;&gt;Sergio Jim&amp;#xe9;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14377">
<title>How Can Large Language Models Help Humans in Design and Manufacturing?. (arXiv:2307.14377v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14377</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement of Large Language Models (LLMs), including GPT-4, provides
exciting new opportunities for generative design. We investigate the
application of this tool across the entire design and manufacturing workflow.
Specifically, we scrutinize the utility of LLMs in tasks such as: converting a
text-based prompt into a design specification, transforming a design into
manufacturing instructions, producing a design space and design variations,
computing the performance of a design, and searching for designs predicated on
performance. Through a series of examples, we highlight both the benefits and
the limitations of the current LLMs. By exposing these limitations, we aspire
to catalyze the continued improvement and progression of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makatura_L/0/1/0/all/0/1&quot;&gt;Liane Makatura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foshey_M/0/1/0/all/0/1&quot;&gt;Michael Foshey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+HahnLein_F/0/1/0/all/0/1&quot;&gt;Felix H&amp;#xe4;hnLein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_P/0/1/0/all/0/1&quot;&gt;Pingchuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Bolei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjandrasuwita_M/0/1/0/all/0/1&quot;&gt;Megan Tjandrasuwita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spielberg_A/0/1/0/all/0/1&quot;&gt;Andrew Spielberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Owens_C/0/1/0/all/0/1&quot;&gt;Crystal Elaine Owens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peter Yichen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_A/0/1/0/all/0/1&quot;&gt;Allan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1&quot;&gt;Amy Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norton_W/0/1/0/all/0/1&quot;&gt;Wil J Norton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_E/0/1/0/all/0/1&quot;&gt;Edward Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacob_J/0/1/0/all/0/1&quot;&gt;Joshua Jacob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_A/0/1/0/all/0/1&quot;&gt;Adriana Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1&quot;&gt;Wojciech Matusik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14381">
<title>EdgeConvEns: Convolutional Ensemble Learning for Edge Intelligence. (arXiv:2307.14381v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14381</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep edge intelligence aims to deploy deep learning models that demand
computationally expensive training in the edge network with limited
computational power. Moreover, many deep edge intelligence applications require
handling distributed data that cannot be transferred to a central server due to
privacy concerns. Decentralized learning methods, such as federated learning,
offer solutions where models are learned collectively by exchanging learned
weights. However, they often require complex models that edge devices may not
handle and multiple rounds of network communication to achieve state-of-the-art
performances. This study proposes a convolutional ensemble learning approach,
coined EdgeConvEns, that facilitates training heterogeneous weak models on edge
and learning to ensemble them where data on edge are heterogeneously
distributed. Edge models are implemented and trained independently on
Field-Programmable Gate Array (FPGA) devices with various computational
capacities. Learned data representations are transferred to a central server
where the ensemble model is trained with the learned features received from the
edge devices to boost the overall prediction performance. Extensive experiments
demonstrate that the EdgeConvEns can outperform the state-of-the-art
performance with fewer communications and less data in various training
scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikdokur_I/0/1/0/all/0/1&quot;&gt;Ilkay Sikdokur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baytas_I/0/1/0/all/0/1&quot;&gt;&amp;#x130;nci M. Bayta&amp;#x15f;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yurdakul_A/0/1/0/all/0/1&quot;&gt;Arda Yurdakul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14382">
<title>When Multi-Task Learning Meets Partial Supervision: A Computer Vision Review. (arXiv:2307.14382v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14382</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-Task Learning (MTL) aims to learn multiple tasks simultaneously while
exploiting their mutual relationships. By using shared resources to
simultaneously calculate multiple outputs, this learning paradigm has the
potential to have lower memory requirements and inference times compared to the
traditional approach of using separate methods for each task. Previous work in
MTL has mainly focused on fully-supervised methods, as task relationships can
not only be leveraged to lower the level of data-dependency of those methods
but they can also improve performance. However, MTL introduces a set of
challenges due to a complex optimisation scheme and a higher labeling
requirement. This review focuses on how MTL could be utilised under different
partial supervision settings to address these challenges. First, this review
analyses how MTL traditionally uses different parameter sharing techniques to
transfer knowledge in between tasks. Second, it presents the different
challenges arising from such a multi-objective optimisation scheme. Third, it
introduces how task groupings can be achieved by analysing task relationships.
Fourth, it focuses on how partially supervised methods applied to MTL can
tackle the aforementioned challenges. Lastly, this review presents the
available datasets, tools and benchmarking results of such methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fontana_M/0/1/0/all/0/1&quot;&gt;Maxime Fontana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spratling_M/0/1/0/all/0/1&quot;&gt;Michael Spratling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Miaojing Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14384">
<title>HyperFed: Hyperbolic Prototypes Exploration with Consistent Aggregation for Non-IID Data in Federated Learning. (arXiv:2307.14384v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14384</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) collaboratively models user data in a decentralized
way. However, in the real world, non-identical and independent data
distributions (non-IID) among clients hinder the performance of FL due to three
issues, i.e., (1) the class statistics shifting, (2) the insufficient
hierarchical information utilization, and (3) the inconsistency in aggregating
clients. To address the above issues, we propose HyperFed which contains three
main modules, i.e., hyperbolic prototype Tammes initialization (HPTI),
hyperbolic prototype learning (HPL), and consistent aggregation (CA). Firstly,
HPTI in the server constructs uniformly distributed and fixed class prototypes,
and shares them with clients to match class statistics, further guiding
consistent feature representation for local clients. Secondly, HPL in each
client captures the hierarchical information in local data with the supervision
of shared class prototypes in the hyperbolic model space. Additionally, CA in
the server mitigates the impact of the inconsistent deviations from clients to
server. Extensive studies of four datasets prove that HyperFed is effective in
enhancing the performance of FL under the non-IID set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_X/0/1/0/all/0/1&quot;&gt;Xinting Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weiming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaochao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huabin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yanchao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yue Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14387">
<title>Dual-Space Attacks against Random-Walk-based Anomaly Detection. (arXiv:2307.14387v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.14387</link>
<description rdf:parseType="Literal">&lt;p&gt;Random Walks-based Anomaly Detection (RWAD) is commonly used to identify
anomalous patterns in various applications. An intriguing characteristic of
RWAD is that the input graph can either be pre-existing or constructed from raw
features. Consequently, there are two potential attack surfaces against RWAD:
graph-space attacks and feature-space attacks. In this paper, we explore this
vulnerability by designing practical dual-space attacks, investigating the
interplay between graph-space and feature-space attacks. To this end, we
conduct a thorough complexity analysis, proving that attacking RWAD is NP-hard.
Then, we proceed to formulate the graph-space attack as a bi-level optimization
problem and propose two strategies to solve it: alternative iteration
(alterI-attack) or utilizing the closed-form solution of the random walk model
(cf-attack). Finally, we utilize the results from the graph-space attacks as
guidance to design more powerful feature-space attacks (i.e., graph-guided
attacks). Comprehensive experiments demonstrate that our proposed attacks are
effective in enabling the target nodes from RWAD with a limited attack budget.
In addition, we conduct transfer attack experiments in a black-box setting,
which show that our feature attack significantly decreases the anomaly scores
of target nodes. Our study opens the door to studying the dual-space attack
against graph anomaly detection in which the graph space relies on the feature
space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yuni Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waniek_M/0/1/0/all/0/1&quot;&gt;Marcin Waniek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yulin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michalak_T/0/1/0/all/0/1&quot;&gt;Tomasz P. Michalak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahwan_T/0/1/0/all/0/1&quot;&gt;Talal Rahwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kai Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14394">
<title>Hypergraph Isomorphism Computation. (arXiv:2307.14394v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2307.14394</link>
<description rdf:parseType="Literal">&lt;p&gt;The isomorphism problem is a fundamental problem in network analysis, which
involves capturing both low-order and high-order structural information. In
terms of extracting low-order structural information, graph isomorphism
algorithms analyze the structural equivalence to reduce the solver space
dimension, which demonstrates its power in many applications, such as protein
design, chemical pathways, and community detection. For the more commonly
occurring high-order relationships in real-life scenarios, the problem of
hypergraph isomorphism, which effectively captures these high-order structural
relationships, cannot be straightforwardly addressed using graph isomorphism
methods. Besides, the existing hypergraph kernel methods may suffer from high
memory consumption or inaccurate sub-structure identification, thus yielding
sub-optimal performance. In this paper, to address the abovementioned problems,
we first propose the hypergraph Weisfiler-Lehman test algorithm for the
hypergraph isomorphism test problem by generalizing the Weisfiler-Lehman test
algorithm from graphs to hypergraphs. Secondly, based on the presented
algorithm, we propose a general hypergraph Weisfieler-Lehman kernel framework
and implement two instances, which are Hypergraph Weisfeiler-Lehamn Subtree
Kernel and Hypergraph Weisfeiler-Lehamn Hyperedge Kernel. In order to fulfill
our research objectives, a comprehensive set of experiments was meticulously
designed, including seven graph classification datasets and 12 hypergraph
classification datasets. Results on hypergraph classification datasets show
significant improvements compared to other typical kernel-based methods, which
demonstrates the effectiveness of the proposed methods. In our evaluation, we
found that our proposed methods outperform the second-best method in terms of
runtime, running over 80 times faster when handling complex hypergraph
structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yifan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiashu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_S/0/1/0/all/0/1&quot;&gt;Shihui Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yue Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14395">
<title>Learning to simulate partially known spatio-temporal dynamics with trainable difference operators. (arXiv:2307.14395v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14395</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, using neural networks to simulate spatio-temporal dynamics has
received a lot of attention. However, most existing methods adopt pure
data-driven black-box models, which have limited accuracy and interpretability.
By combining trainable difference operators with black-box models, we propose a
new hybrid architecture explicitly embedded with partial prior knowledge of the
underlying PDEs named PDE-Net++. Furthermore, we introduce two distinct options
called the trainable flipping difference layer (TFDL) and the trainable dynamic
difference layer (TDDL) for the difference operators. Numerous numerical
experiments have demonstrated that PDE-Net++ has superior prediction accuracy
and better extrapolation performance than black-box models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuoyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongsheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongye Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1&quot;&gt;Bei Hua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14398">
<title>Non-Linear Self Augmentation Deep Pipeline for Cancer Treatment outcome Prediction. (arXiv:2307.14398v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14398</link>
<description rdf:parseType="Literal">&lt;p&gt;Immunotherapy emerges as promising approach for treating cancer. Encouraging
findings have validated the efficacy of immunotherapy medications in addressing
tumors, resulting in prolonged survival rates and notable reductions in
toxicity compared to conventional chemotherapy methods. However, the pool of
eligible patients for immunotherapy remains relatively small, indicating a lack
of comprehensive understanding regarding the physiological mechanisms
responsible for favorable treatment response in certain individuals while
others experience limited benefits. To tackle this issue, the authors present
an innovative strategy that harnesses a non-linear cellular architecture in
conjunction with a deep downstream classifier. This approach aims to carefully
select and enhance 2D features extracted from chest-abdomen CT images, thereby
improving the prediction of treatment outcomes. The proposed pipeline has been
meticulously designed to seamlessly integrate with an advanced embedded Point
of Care system. In this context, the authors present a compelling case study
focused on Metastatic Urothelial Carcinoma (mUC), a particularly aggressive
form of cancer. Performance evaluation of the proposed approach underscores its
effectiveness, with an impressive overall accuracy of approximately 93%
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rundo_F/0/1/0/all/0/1&quot;&gt;Francesco Rundo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Spampinato_C/0/1/0/all/0/1&quot;&gt;Concetto Spampinato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rundo_M/0/1/0/all/0/1&quot;&gt;Michael Rundo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14403">
<title>Unsupervised Deep Learning-based Pansharpening with Jointly-Enhanced Spectral and Spatial Fidelity. (arXiv:2307.14403v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14403</link>
<description rdf:parseType="Literal">&lt;p&gt;In latest years, deep learning has gained a leading role in the pansharpening
of multiresolution images. Given the lack of ground truth data, most deep
learning-based methods carry out supervised training in a reduced-resolution
domain. However, models trained on downsized images tend to perform poorly on
high-resolution target images. For this reason, several research groups are now
turning to unsupervised training in the full-resolution domain, through the
definition of appropriate loss functions and training paradigms. In this
context, we have recently proposed a full-resolution training framework which
can be applied to many existing architectures.
&lt;/p&gt;
&lt;p&gt;Here, we propose a new deep learning-based pansharpening model that fully
exploits the potential of this approach and provides cutting-edge performance.
Besides architectural improvements with respect to previous work, such as the
use of residual attention modules, the proposed model features a novel loss
function that jointly promotes the spectral and spatial quality of the
pansharpened data. In addition, thanks to a new fine-tuning strategy, it
improves inference-time adaptation to target images. Experiments on a large
variety of test images, performed in challenging scenarios, demonstrate that
the proposed method compares favorably with the state of the art both in terms
of numerical results and visual output. Code is available online at
https://github.com/matciotola/Lambda-PNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ciotola_M/0/1/0/all/0/1&quot;&gt;Matteo Ciotola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poggi_G/0/1/0/all/0/1&quot;&gt;Giovanni Poggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Scarpa_G/0/1/0/all/0/1&quot;&gt;Giuseppe Scarpa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14448">
<title>VISPUR: Visual Aids for Identifying and Interpreting Spurious Associations in Data-Driven Decisions. (arXiv:2307.14448v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.14448</link>
<description rdf:parseType="Literal">&lt;p&gt;Big data and machine learning tools have jointly empowered humans in making
data-driven decisions. However, many of them capture empirical associations
that might be spurious due to confounding factors and subgroup heterogeneity.
The famous Simpson&apos;s paradox is such a phenomenon where aggregated and
subgroup-level associations contradict with each other, causing cognitive
confusions and difficulty in making adequate interpretations and decisions.
Existing tools provide little insights for humans to locate, reason about, and
prevent pitfalls of spurious association in practice. We propose VISPUR, a
visual analytic system that provides a causal analysis framework and a
human-centric workflow for tackling spurious associations. These include a
CONFOUNDER DASHBOARD, which can automatically identify possible confounding
factors, and a SUBGROUP VIEWER, which allows for the visualization and
comparison of diverse subgroup patterns that likely or potentially result in a
misinterpretation of causality. Additionally, we propose a REASONING
STORYBOARD, which uses a flow-based approach to illustrate paradoxical
phenomena, as well as an interactive DECISION DIAGNOSIS panel that helps ensure
accountable decision-making. Through an expert interview and a controlled user
experiment, our qualitative and quantitative results demonstrate that the
proposed &quot;de-paradox&quot; workflow and the designed visual analytic system are
effective in helping human users to identify and understand spurious
associations, as well as to make accountable causal decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_X/0/1/0/all/0/1&quot;&gt;Xian Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_Y/0/1/0/all/0/1&quot;&gt;Yongsu Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu-Ru Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14464">
<title>Single Channel Speech Enhancement Using U-Net Spiking Neural Networks. (arXiv:2307.14464v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.14464</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech enhancement (SE) is crucial for reliable communication devices or
robust speech recognition systems. Although conventional artificial neural
networks (ANN) have demonstrated remarkable performance in SE, they require
significant computational power, along with high energy costs. In this paper,
we propose a novel approach to SE using a spiking neural network (SNN) based on
a U-Net architecture. SNNs are suitable for processing data with a temporal
dimension, such as speech, and are known for their energy-efficient
implementation on neuromorphic hardware. As such, SNNs are thus interesting
candidates for real-time applications on devices with limited resources. The
primary objective of the current work is to develop an SNN-based model with
comparable performance to a state-of-the-art ANN model for SE. We train a deep
SNN using surrogate-gradient-based optimization and evaluate its performance
using perceptual objective tests under different signal-to-noise ratios and
real-world noise conditions. Our results demonstrate that the proposed
energy-efficient SNN model outperforms the Intel Neuromorphic Deep Noise
Suppression Challenge (Intel N-DNS Challenge) baseline solution and achieves
acceptable performance compared to an equivalent ANN model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riahi_A/0/1/0/all/0/1&quot;&gt;Abir Riahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plourde_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;ric Plourde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14487">
<title>Technical note: ShinyAnimalCV: open-source cloud-based web application for object detection, segmentation, and three-dimensional visualization of animals using computer vision. (arXiv:2307.14487v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14487</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer vision (CV), a non-intrusive and cost-effective technology, has
furthered the development of precision livestock farming by enabling optimized
decision-making through timely and individualized animal care. The availability
of affordable two- and three-dimensional camera sensors, combined with various
machine learning and deep learning algorithms, has provided a valuable
opportunity to improve livestock production systems. However, despite the
availability of various CV tools in the public domain, applying these tools to
animal data can be challenging, often requiring users to have programming and
data analysis skills, as well as access to computing resources. Moreover, the
rapid expansion of precision livestock farming is creating a growing need to
educate and train animal science students in CV. This presents educators with
the challenge of efficiently demonstrating the complex algorithms involved in
CV. Thus, the objective of this study was to develop ShinyAnimalCV, an
open-source cloud-based web application. This application provides a
user-friendly interface for performing CV tasks, including object segmentation,
detection, three-dimensional surface visualization, and extraction of two- and
three-dimensional morphological features. Nine pre-trained CV models using
top-view animal data are included in the application. ShinyAnimalCV has been
deployed online using cloud computing platforms. The source code of
ShinyAnimalCV is available on GitHub, along with detailed documentation on
training CV models using custom data and deploying ShinyAnimalCV locally to
allow users to fully leverage the capabilities of the application.
ShinyAnimalCV can contribute to CV research and teaching in the animal science
community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_L/0/1/0/all/0/1&quot;&gt;Lirong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morota_G/0/1/0/all/0/1&quot;&gt;Gota Morota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_S/0/1/0/all/0/1&quot;&gt;Samantha A. Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wickens_C/0/1/0/all/0/1&quot;&gt;Carissa L. Wickens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_Cushon_E/0/1/0/all/0/1&quot;&gt;Emily K. Miller-Cushon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haipeng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14501">
<title>Improving Reliable Navigation under Uncertainty via Predictions Informed by Non-Local Information. (arXiv:2307.14501v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.14501</link>
<description rdf:parseType="Literal">&lt;p&gt;We improve reliable, long-horizon, goal-directed navigation in
partially-mapped environments by using non-locally available information to
predict the goodness of temporally-extended actions that enter unseen space.
Making predictions about where to navigate in general requires non-local
information: any observations the robot has seen so far may provide information
about the goodness of a particular direction of travel. Building on recent work
in learning-augmented model-based planning under uncertainty, we present an
approach that can both rely on non-local information to make predictions (via a
graph neural network) and is reliable by design: it will always reach its goal,
even when learning does not provide accurate predictions. We conduct
experiments in three simulated environments in which non-local information is
needed to perform well. In our large scale university building environment,
generated from real-world floorplans to the scale, we demonstrate a 9.3\%
reduction in cost-to-go compared to a non-learned baseline and a 14.9\%
reduction compared to a learning-informed planner that can only use local
information to inform its predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnob_R/0/1/0/all/0/1&quot;&gt;Raihan Islam Arnob&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_G/0/1/0/all/0/1&quot;&gt;Gregory J. Stein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14510">
<title>Attention of Robot Touch: Tactile Saliency Prediction for Robust Sim-to-Real Tactile Control. (arXiv:2307.14510v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.14510</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution tactile sensing can provide accurate information about local
contact in contact-rich robotic tasks. However, the deployment of such tasks in
unstructured environments remains under-investigated. To improve the robustness
of tactile robot control in unstructured environments, we propose and study a
new concept: \textit{tactile saliency} for robot touch, inspired by the human
touch attention mechanism from neuroscience and the visual saliency prediction
problem from computer vision. In analogy to visual saliency, this concept
involves identifying key information in tactile images captured by a tactile
sensor. While visual saliency datasets are commonly annotated by humans,
manually labelling tactile images is challenging due to their counterintuitive
patterns. To address this challenge, we propose a novel approach comprised of
three interrelated networks: 1) a Contact Depth Network (ConDepNet), which
generates a contact depth map to localize deformation in a real tactile image
that contains target and noise features; 2) a Tactile Saliency Network
(TacSalNet), which predicts a tactile saliency map to describe the target areas
for an input contact depth map; 3) and a Tactile Noise Generator (TacNGen),
which generates noise features to train the TacSalNet. Experimental results in
contact pose estimation and edge-following in the presence of distractors
showcase the accurate prediction of target features from real tactile images.
Overall, our tactile saliency prediction approach gives robust sim-to-real
tactile control in environments with unknown distractors. Project page:
https://sites.google.com/view/tactile-saliency/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yijiong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comi_M/0/1/0/all/0/1&quot;&gt;Mauro Comi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Church_A/0/1/0/all/0/1&quot;&gt;Alex Church&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dandan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepora_N/0/1/0/all/0/1&quot;&gt;Nathan F. Lepora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14511">
<title>Words That Stick: Predicting Decision Making and Synonym Engagement Using Cognitive Biases and Computational Linguistics. (arXiv:2307.14511v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.14511</link>
<description rdf:parseType="Literal">&lt;p&gt;This research draws upon cognitive psychology and information systems studies
to anticipate user engagement and decision-making on digital platforms. By
employing natural language processing (NLP) techniques and insights from
cognitive bias research, we delve into user interactions with synonyms within
digital content. Our methodology synthesizes four cognitive
biasesRepresentativeness, Ease-of-use, Affect, and Distributioninto the READ
model. Through a comprehensive user survey, we assess the model&apos;s ability to
predict user engagement, discovering that synonyms that accurately represent
core ideas, are easy to understand, elicit emotional responses, and are
commonly encountered, promote greater user engagement. Crucially, our work
offers a fresh lens on human-computer interaction, digital behaviors, and
decision-making processes. Our results highlight the promise of cognitive
biases as potent indicators of user engagement, underscoring their significance
in designing effective digital content across fields like education and
marketing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvir_N/0/1/0/all/0/1&quot;&gt;Nimrod Dvir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedman_E/0/1/0/all/0/1&quot;&gt;Elaine Friedman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Commuri_S/0/1/0/all/0/1&quot;&gt;Suraj Commuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romano_J/0/1/0/all/0/1&quot;&gt;Jennifer Romano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14517">
<title>The Co-12 Recipe for Evaluating Interpretable Part-Prototype Image Classifiers. (arXiv:2307.14517v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14517</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable part-prototype models are computer vision models that are
explainable by design. The models learn prototypical parts and recognise these
components in an image, thereby combining classification and explanation.
Despite the recent attention for intrinsically interpretable models, there is
no comprehensive overview on evaluating the explanation quality of
interpretable part-prototype models. Based on the Co-12 properties for
explanation quality as introduced in &lt;a href=&quot;/abs/2201.08164&quot;&gt;arXiv:2201.08164&lt;/a&gt; (e.g., correctness,
completeness, compactness), we review existing work that evaluates
part-prototype models, reveal research gaps and outline future approaches for
evaluation of the explanation quality of part-prototype models. This paper,
therefore, contributes to the progression and maturity of this relatively new
research field on interpretable part-prototype models. We additionally provide
a ``Co-12 cheat sheet&apos;&apos; that acts as a concise summary of our findings on
evaluating part-prototype models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nauta_M/0/1/0/all/0/1&quot;&gt;Meike Nauta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_C/0/1/0/all/0/1&quot;&gt;Christin Seifert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14521">
<title>Patterns of Vehicle Lights: Addressing Complexities in Curation and Annotation of Camera-Based Vehicle Light Datasets and Metrics. (arXiv:2307.14521v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14521</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the representation of vehicle lights in computer vision
and its implications for various tasks in the field of autonomous driving.
Different specifications for representing vehicle lights, including bounding
boxes, center points, corner points, and segmentation masks, are discussed in
terms of their strengths and weaknesses. Three important tasks in autonomous
driving that can benefit from vehicle light detection are identified: nighttime
vehicle detection, 3D vehicle orientation estimation, and dynamic trajectory
cues. Each task may require a different representation of the light. The
challenges of collecting and annotating large datasets for training data-driven
models are also addressed, leading to introduction of the LISA Vehicle Lights
Dataset and associated Light Visibility Model, which provides light annotations
specifically designed for downstream applications in vehicle detection, intent
and trajectory prediction, and safe path planning. A comparison of existing
vehicle light datasets is provided, highlighting the unique features and
limitations of each dataset. Overall, this paper provides insights into the
representation of vehicle lights and the importance of accurate annotations for
training effective detection models in autonomous driving applications. Our
dataset and model are made available at
https://cvrr.ucsd.edu/vehicle-lights-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greer_R/0/1/0/all/0/1&quot;&gt;Ross Greer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalkrishnan_A/0/1/0/all/0/1&quot;&gt;Akshay Gopalkrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keskar_M/0/1/0/all/0/1&quot;&gt;Maitrayee Keskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_M/0/1/0/all/0/1&quot;&gt;Mohan Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14527">
<title>Open Problems in Computer Vision for Wilderness SAR and The Search for Patricia Wu-Murad. (arXiv:2307.14527v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14527</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper details the challenges in applying two computer vision systems, an
EfficientDET supervised learning model and the unsupervised RX spectral
classifier, to 98.9 GB of drone imagery from the Wu-Murad wilderness search and
rescue (WSAR) effort in Japan and identifies 3 directions for future research.
There have been at least 19 proposed approaches and 3 datasets aimed at
locating missing persons in drone imagery, but only 3 approaches (2
unsupervised and 1 of an unknown structure) are referenced in the literature as
having been used in an actual WSAR operation. Of these proposed approaches, the
EfficientDET architecture and the unsupervised spectral RX classifier were
selected as the most appropriate for this setting. The EfficientDET model was
applied to the HERIDAL dataset and despite achieving performance that is
statistically equivalent to the state-of-the-art, the model fails to translate
to the real world in terms of false positives (e.g., identifying tree limbs and
rocks as people), and false negatives (e.g., failing to identify members of the
search team). The poor results in practice for algorithms that showed good
results on datasets suggest 3 areas of future research: more realistic datasets
for wilderness SAR, computer vision models that are capable of seamlessly
handling the variety of imagery that can be collected during actual WSAR
operations, and better alignment on performance measures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzini_T/0/1/0/all/0/1&quot;&gt;Thomas Manzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphy_R/0/1/0/all/0/1&quot;&gt;Robin Murphy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14544">
<title>Speed Reading Tool Powered by Artificial Intelligence for Students with ADHD, Dyslexia, or Short Attention Span. (arXiv:2307.14544v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14544</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach to assist students with dyslexia, ADHD,
and short attention span in digesting any text-based information more
efficiently. The proposed solution utilizes the Multilayer Perceptron (MLP)
algorithm for complex text processing and summarization tasks. The tool
leverages the T5 (Text-to-Text Transfer Transformer) model from Hugging Face,
which treats every NLP task as a text generation task. The model is fine-tuned
on specific tasks using a smaller dataset. The NLTK&apos;s Punkt Sentence Tokenizer
is used to divide a text into a list of sentences. The application is served
using Flask, a lightweight web server and framework. The tool also applies
principles from Bionic Reading to enhance readability, which includes a bolding
function and adjustments to line, word, and character spacing. The paper
discusses the methodology, implementation, and results of the AI-based speed
reading tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamarozaman_M/0/1/0/all/0/1&quot;&gt;Megat Irfan Zackry Bin Ismail Ahmad Nazran bin Yusri Muhammad Hafizzul Bin Abdul Manap Muhammad Muizzuddin Bin Kamarozaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14549">
<title>Adversarial Sleeping Bandit Problems with Multiple Plays: Algorithm and Ranking Application. (arXiv:2307.14549v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14549</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an efficient algorithm to solve the sleeping bandit with
multiple plays problem in the context of an online recommendation system. The
problem involves bounded, adversarial loss and unknown i.i.d. distributions for
arm availability. The proposed algorithm extends the sleeping bandit algorithm
for single arm selection and is guaranteed to achieve theoretical performance
with regret upper bounded by $\bigO(kN^2\sqrt{T\log T})$, where $k$ is the
number of arms selected per time step, $N$ is the total number of arms, and $T$
is the time horizon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jianjun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woon_W/0/1/0/all/0/1&quot;&gt;Wei Lee Woon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coba_L/0/1/0/all/0/1&quot;&gt;Ludovik Coba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14556">
<title>Reinforcement learning guided fuzz testing for a browser&apos;s HTML rendering engine. (arXiv:2307.14556v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14556</link>
<description rdf:parseType="Literal">&lt;p&gt;Generation-based fuzz testing can uncover various bugs and security
vulnerabilities. However, compared to mutation-based fuzz testing, it takes
much longer to develop a well-balanced generator that produces good test cases
and decides where to break the underlying structure to exercise new code paths.
We propose a novel approach to combine a trained test case generator deep
learning model with a double deep Q-network (DDQN) for the first time. The DDQN
guides test case creation based on a code coverage signal. Our approach
improves the code coverage performance of the underlying generator model by up
to 18.5\% for the Firefox HTML rendering engine compared to the baseline
grammar based fuzzer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sablotny_M/0/1/0/all/0/1&quot;&gt;Martin Sablotny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf8;rn Sand Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singer_J/0/1/0/all/0/1&quot;&gt;Jeremy Singer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14568">
<title>Evaluation of Safety Constraints in Autonomous Navigation with Deep Reinforcement Learning. (arXiv:2307.14568v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.14568</link>
<description rdf:parseType="Literal">&lt;p&gt;While reinforcement learning algorithms have had great success in the field
of autonomous navigation, they cannot be straightforwardly applied to the real
autonomous systems without considering the safety constraints. The later are
crucial to avoid unsafe behaviors of the autonomous vehicle on the road. To
highlight the importance of these constraints, in this study, we compare two
learnable navigation policies: safe and unsafe. The safe policy takes the
constraints into account, while the other does not. We show that the safe
policy is able to generate trajectories with more clearance (distance to the
obstacles) and makes less collisions while training without sacrificing the
overall performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angulo_B/0/1/0/all/0/1&quot;&gt;Brian Angulo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorbov_G/0/1/0/all/0/1&quot;&gt;Gregory Gorbov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panov_A/0/1/0/all/0/1&quot;&gt;Aleksandr Panov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakovlev_K/0/1/0/all/0/1&quot;&gt;Konstantin Yakovlev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14575">
<title>A Memory-Augmented Multi-Task Collaborative Framework for Unsupervised Traffic Accident Detection in Driving Videos. (arXiv:2307.14575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14575</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying traffic accidents in driving videos is crucial to ensuring the
safety of autonomous driving and driver assistance systems. To address the
potential danger caused by the long-tailed distribution of driving events,
existing traffic accident detection (TAD) methods mainly rely on unsupervised
learning. However, TAD is still challenging due to the rapid movement of
cameras and dynamic scenes in driving scenarios. Existing unsupervised TAD
methods mainly rely on a single pretext task, i.e., an appearance-based or
future object localization task, to detect accidents. However, appearance-based
approaches are easily disturbed by the rapid movement of the camera and changes
in illumination, which significantly reduce the performance of traffic accident
detection. Methods based on future object localization may fail to capture
appearance changes in video frames, making it difficult to detect ego-involved
accidents (e.g., out of control of the ego-vehicle). In this paper, we propose
a novel memory-augmented multi-task collaborative framework (MAMTCF) for
unsupervised traffic accident detection in driving videos. Different from
previous approaches, our method can more accurately detect both ego-involved
and non-ego accidents by simultaneously modeling appearance changes and object
motions in video frames through the collaboration of optical flow
reconstruction and future object localization tasks. Further, we introduce a
memory-augmented motion representation mechanism to fully explore the
interrelation between different types of motion representations and exploit the
high-level features of normal traffic patterns stored in memory to augment
motion representations, thus enlarging the difference from anomalies.
Experimental results on recently published large-scale dataset demonstrate that
our method achieves better performance compared to previous state-of-the-art
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_R/0/1/0/all/0/1&quot;&gt;Rongqin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanman Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1&quot;&gt;Yingxin Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xia Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14581">
<title>Explainable Techniques for Analyzing Flow Cytometry Cell Transformers. (arXiv:2307.14581v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.14581</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability for Deep Learning Models is especially important for clinical
applications, where decisions of automated systems have far-reaching
consequences.
&lt;/p&gt;
&lt;p&gt;While various post-hoc explainable methods, such as attention visualization
and saliency maps, already exist for common data modalities, including natural
language and images, little work has been done to adapt them to the modality of
Flow CytoMetry (FCM) data.
&lt;/p&gt;
&lt;p&gt;In this work, we evaluate the usage of a transformer architecture called
ReluFormer that ease attention visualization as well as we propose a gradient-
and an attention-based visualization technique tailored for FCM. We
qualitatively evaluate the visualization techniques for cell classification and
polygon regression on pediatric Acute Lymphoblastic Leukemia (ALL) FCM samples.
The results outline the model&apos;s decision process and demonstrate how to utilize
the proposed techniques to inspect the trained model. The gradient-based
visualization not only identifies cells that are most significant for a
particular prediction but also indicates the directions in the FCM feature
space in which changes have the most impact on the prediction. The attention
visualization provides insights on the transformer&apos;s decision process when
handling FCM data. We show that different attention heads specialize by
attending to different biologically meaningful sub-populations in the data,
even though the model retrieved solely supervised binary classification signals
during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kowarsch_F/0/1/0/all/0/1&quot;&gt;Florian Kowarsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Weijler_L/0/1/0/all/0/1&quot;&gt;Lisa Weijler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kleber_F/0/1/0/all/0/1&quot;&gt;FLorian Kleber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wodlinger_M/0/1/0/all/0/1&quot;&gt;Matthias W&amp;#xf6;dlinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Reiter_M/0/1/0/all/0/1&quot;&gt;Michael Reiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maurer_Granofszky_M/0/1/0/all/0/1&quot;&gt;Margarita Maurer-Granofszky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dworzak_M/0/1/0/all/0/1&quot;&gt;Michael Dworzak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14591">
<title>The detection and rectification for identity-switch based on unfalsified control. (arXiv:2307.14591v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14591</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of multi-object tracking (MOT) is to continuously track and
identify objects detected in videos. Currently, most methods for multi-object
tracking model the motion information and combine it with appearance
information to determine and track objects. In this paper, unfalsified control
is employed to address the ID-switch problem in multi-object tracking. We
establish sequences of appearance information variations for the trajectories
during the tracking process and design a detection and rectification module
specifically for ID-switch detection and recovery. We also propose a simple and
effective strategy to address the issue of ambiguous matching of appearance
information during the data association process. Experimental results on
publicly available MOT datasets demonstrate that the tracker exhibits excellent
effectiveness and robustness in handling tracking errors caused by occlusions
and rapid movements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Junchao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sheng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14605">
<title>Clustering based Point Cloud Representation Learning for 3D Analysis. (arXiv:2307.14605v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14605</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud analysis (such as 3D segmentation and detection) is a challenging
task, because of not only the irregular geometries of many millions of
unordered points, but also the great variations caused by depth, viewpoint,
occlusion, etc. Current studies put much focus on the adaption of neural
networks to the complex geometries of point clouds, but are blind to a
fundamental question: how to learn an appropriate point embedding space that is
aware of both discriminative semantics and challenging variations? As a
response, we propose a clustering based supervised learning scheme for point
cloud analysis. Unlike current de-facto, scene-wise training paradigm, our
algorithm conducts within-class clustering on the point embedding space for
automatically discovering subclass patterns which are latent yet representative
across scenes. The mined patterns are, in turn, used to repaint the embedding
space, so as to respect the underlying distribution of the entire training
dataset and improve the robustness to the variations. Our algorithm is
principled and readily pluggable to modern point cloud segmentation networks
during training, without extra overhead during testing. With various 3D network
architectures (i.e., voxel-based, point-based, Transformer-based, automatically
searched), our algorithm shows notable improvements on famous point cloud
segmentation datasets (i.e.,2.0-2.6% on single-scan and 2.0-2.2% multi-scan of
SemanticKITTI, 1.8-1.9% on S3DIS, in terms of mIoU). Our algorithm also
demonstrates utility in 3D detection, showing 2.0-3.4% mAP gains on KITTI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tuo Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14613">
<title>Self-Contrastive Graph Diffusion Network. (arXiv:2307.14613v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14613</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmentation techniques and sampling strategies are crucial in contrastive
learning, but in most existing works, augmentation techniques require careful
design, and their sampling strategies can only capture a small amount of
intrinsic supervision information. Additionally, the existing methods require
complex designs to obtain two different representations of the data. To
overcome these limitations, we propose a novel framework called the
Self-Contrastive Graph Diffusion Network (SCGDN). Our framework consists of two
main components: the Attentional Module (AttM) and the Diffusion Module (DiFM).
AttM aggregates higher-order structure and feature information to get an
excellent embedding, while DiFM balances the state of each node in the graph
through Laplacian diffusion learning and allows the cooperative evolution of
adjacency and feature information in the graph. Unlike existing methodologies,
SCGDN is an augmentation-free approach that avoids &quot;sampling bias&quot; and semantic
drift, without the need for pre-training. We conduct a high-quality sampling of
samples based on structure and feature information. If two nodes are neighbors,
they are considered positive samples of each other. If two disconnected nodes
are also unrelated on $k$NN graph, they are considered negative samples for
each other. The contrastive objective reasonably uses our proposed sampling
strategies, and the redundancy reduction term minimizes redundant information
in the embedding and can well retain more discriminative information. In this
novel framework, the graph self-contrastive learning paradigm gives expression
to a powerful force. SCGDN effectively balances between preserving high-order
structure information and avoiding overfitting. The results manifest that SCGDN
can consistently generate outperformance over both the contrastive methods and
the classical methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yixian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1&quot;&gt;Kun Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14623">
<title>BubbleML: A Multi-Physics Dataset and Benchmarks for Machine Learning. (arXiv:2307.14623v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14623</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of phase change phenomena, the lack of accessible and diverse
datasets suitable for machine learning (ML) training poses a significant
challenge. Existing experimental datasets are often restricted, with limited
availability and sparse ground truth data, impeding our understanding of this
complex multi-physics phenomena. To bridge this gap, we present the BubbleML
Dataset(https://github.com/HPCForge/BubbleML) which leverages physics-driven
simulations to provide accurate ground truth information for various boiling
scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled
boiling. This extensive dataset covers a wide range of parameters, including
varying gravity conditions, flow rates, sub-cooling levels, and wall superheat,
comprising 51 simulations. BubbleML is validated against experimental
observations and trends, establishing it as an invaluable resource for ML
research. Furthermore, we showcase its potential to facilitate exploration of
diverse downstream tasks by introducing two benchmarks: (a) optical flow
analysis to capture bubble dynamics, and (b) operator networks for learning
temperature dynamics. The BubbleML dataset and its benchmarks serve as a
catalyst for advancements in ML-driven research on multi-physics phase change
phenomena, enabling the development and comparison of state-of-the-art
techniques and models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_S/0/1/0/all/0/1&quot;&gt;Sheikh Md Shakeel Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feeney_A/0/1/0/all/0/1&quot;&gt;Arthur Feeney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhruv_A/0/1/0/all/0/1&quot;&gt;Akash Dhruv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jihoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_Y/0/1/0/all/0/1&quot;&gt;Youngjoon Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_J/0/1/0/all/0/1&quot;&gt;Jaiyoung Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Won_Y/0/1/0/all/0/1&quot;&gt;Yoonjin Won&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandramowlishwaran_A/0/1/0/all/0/1&quot;&gt;Aparna Chandramowlishwaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14632">
<title>Metric-Based In-context Learning: A Case Study in Text Simplification. (arXiv:2307.14632v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14632</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) for large language models has proven to be a
powerful approach for many natural language processing tasks. However,
determining the best method to select examples for ICL is nontrivial as the
results can vary greatly depending on the quality, quantity, and order of
examples used. In this paper, we conduct a case study on text simplification
(TS) to investigate how to select the best and most robust examples for ICL. We
propose Metric-Based in-context Learning (MBL) method that utilizes commonly
used TS metrics such as SARI, compression ratio, and BERT-Precision for
selection. Through an extensive set of experiments with various-sized GPT
models on standard TS benchmarks such as TurkCorpus and ASSET, we show that
examples selected by the top SARI scores perform the best on larger models such
as GPT-175B, while the compression ratio generally performs better on smaller
models such as GPT-13B and GPT-6.7B. Furthermore, we demonstrate that MBL is
generally robust to example orderings and out-of-domain test sets, and
outperforms strong baselines and state-of-the-art finetuned language models.
Finally, we show that the behaviour of large GPT models can be implicitly
controlled by the chosen metric. Our research provides a new framework for
selecting examples in ICL, and demonstrates its effectiveness in text
simplification tasks, breaking new ground for more accurate and efficient NLG
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadlamannati_S/0/1/0/all/0/1&quot;&gt;Subha Vadlamannati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;zde G&amp;#xfc;l &amp;#x15e;ahin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14634">
<title>Fact-Checking of AI-Generated Reports. (arXiv:2307.14634v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14634</link>
<description rdf:parseType="Literal">&lt;p&gt;With advances in generative artificial intelligence (AI), it is now possible
to produce realistic-looking automated reports for preliminary reads of
radiology images. This can expedite clinical workflows, improve accuracy and
reduce overall costs. However, it is also well-known that such models often
hallucinate, leading to false findings in the generated reports. In this paper,
we propose a new method of fact-checking of AI-generated reports using their
associated images. Specifically, the developed examiner differentiates real and
fake sentences in reports by learning the association between an image and
sentences describing real or potentially fake findings. To train such an
examiner, we first created a new dataset of fake reports by perturbing the
findings in the original ground truth radiology reports associated with images.
Text encodings of real and fake sentences drawn from these reports are then
paired with image encodings to learn the mapping to real/fake labels. The
utility of such an examiner is demonstrated for verifying automatically
generated reports by detecting and removing fake sentences. Future generative
AI approaches can use the resulting tool to validate their reports leading to a
more responsible use of AI in expediting clinical workflows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_R/0/1/0/all/0/1&quot;&gt;Razi Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ge Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalra_M/0/1/0/all/0/1&quot;&gt;Mannudeep Kalra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Pingkun Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14660">
<title>Multi-Valued Partial Order Plans in Numeric Planning. (arXiv:2307.14660v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14660</link>
<description rdf:parseType="Literal">&lt;p&gt;Many planning formalisms allow for mixing numeric with Boolean effects.
However, most of these formalisms are undecidable. In this paper, we will
analyze possible causes for this undecidability by studying the number of
different occurrences of actions, an approach that proved useful for metric
fluents before. We will start by reformulating a numeric planning problem known
as restricted tasks as a search problem. We will then show how an NP-complete
fragment of numeric planning can be found by using heuristics. To achieve this,
we will develop the idea of multi-valued partial order plans, a least
committing compact representation for (sequential and parallel) plans. Finally,
we will study optimization techniques for this representation to incorporate
soft preconditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helal_H/0/1/0/all/0/1&quot;&gt;Hayyan Helal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakemeyer_G/0/1/0/all/0/1&quot;&gt;Gerhard Lakemeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14669">
<title>Fuzzy order-sorted feature logic. (arXiv:2307.14669v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14669</link>
<description rdf:parseType="Literal">&lt;p&gt;Order-Sorted Feature (OSF) logic is a knowledge representation and reasoning
language based on function-denoting feature symbols and set-denoting sort
symbols ordered in a subsumption lattice. OSF logic allows the construction of
record-like terms that represent classes of entities and that are themselves
ordered in a subsumption relation. The unification algorithm for such
structures provides an efficient calculus of type subsumption, which has been
applied in computational linguistics and implemented in constraint logic
programming languages such as LOGIN and LIFE and automated reasoners such as
CEDAR. This work generalizes OSF logic to a fuzzy setting. We give a flexible
definition of a fuzzy subsumption relation which generalizes Zadeh&apos;s inclusion
between fuzzy sets. Based on this definition we define a fuzzy semantics of OSF
logic where sort symbols and OSF terms denote fuzzy sets. We extend the
subsumption relation to OSF terms and prove that it constitutes a fuzzy partial
order with the property that two OSF terms are subsumed by one another in the
crisp sense if and only if their subsumption degree is greater than 0. We show
how to find the greatest lower bound of two OSF terms by unifying them and how
to compute the subsumption degree between two OSF terms, and we provide the
complexity of these operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milanese_G/0/1/0/all/0/1&quot;&gt;Gian Carlo Milanese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pasi_G/0/1/0/all/0/1&quot;&gt;Gabriella Pasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14675">
<title>Prediction of wind turbines power with physics-informed neural networks and evidential uncertainty quantification. (arXiv:2307.14675v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14675</link>
<description rdf:parseType="Literal">&lt;p&gt;The ever-growing use of wind energy makes necessary the optimization of
turbine operations through pitch angle controllers and their maintenance with
early fault detection. It is crucial to have accurate and robust models
imitating the behavior of wind turbines, especially to predict the generated
power as a function of the wind speed. Existing empirical and physics-based
models have limitations in capturing the complex relations between the input
variables and the power, aggravated by wind variability. Data-driven methods
offer new opportunities to enhance wind turbine modeling of large datasets by
improving accuracy and efficiency. In this study, we used physics-informed
neural networks to reproduce historical data coming from 4 turbines in a wind
farm, while imposing certain physical constraints to the model. The developed
models for regression of the power, torque, and power coefficient as output
variables showed great accuracy for both real data and physical equations
governing the system. Lastly, introducing an efficient evidential layer
provided uncertainty estimations of the predictions, proved to be consistent
with the absolute error, and made possible the definition of a confidence
interval in the power curve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gijon_A/0/1/0/all/0/1&quot;&gt;Alfonso Gij&amp;#xf3;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pujana_Goitia_A/0/1/0/all/0/1&quot;&gt;Ainhoa Pujana-Goitia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perea_E/0/1/0/all/0/1&quot;&gt;Eugenio Perea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molina_Solana_M/0/1/0/all/0/1&quot;&gt;Miguel Molina-Solana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Romero_J/0/1/0/all/0/1&quot;&gt;Juan G&amp;#xf3;mez-Romero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14712">
<title>Evaluating Generative Models for Graph-to-Text Generation. (arXiv:2307.14712v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14712</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have been widely employed for graph-to-text
generation tasks. However, the process of finetuning LLMs requires significant
training resources and annotation work. In this paper, we explore the
capability of generative models to generate descriptive text from graph data in
a zero-shot setting. Specifically, we evaluate GPT-3 and ChatGPT on two
graph-to-text datasets and compare their performance with that of finetuned LLM
models such as T5 and BART. Our results demonstrate that generative models are
capable of generating fluent and coherent text, achieving BLEU scores of 10.57
and 11.08 for the AGENDA and WebNLG datasets, respectively. However, our error
analysis reveals that generative models still struggle with understanding the
semantic relations between entities, and they also tend to generate text with
hallucinations or irrelevant information. As a part of error analysis, we
utilize BERT to detect machine-generated text and achieve high macro-F1 scores.
We have made the text generated by generative models publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_S/0/1/0/all/0/1&quot;&gt;Shuzhou Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farber_M/0/1/0/all/0/1&quot;&gt;Michael F&amp;#xe4;rber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14740">
<title>New Interaction Paradigm for Complex EDA Software Leveraging GPT. (arXiv:2307.14740v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.14740</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly growing field of electronic design automation (EDA),
professional software such as KiCad, Cadence , and Altium Designer provide
increasingly extensive design functionalities. However, the intricate command
structure and high learning curve create a barrier, particularly for novice
printed circuit board (PCB) designers. This results in difficulties in
selecting appropriate functions or plugins for varying design purposes,
compounded by the lack of intuitive learning methods beyond traditional
documentation, videos, and online forums. To address this challenge, an
artificial intelligence (AI) interaction assist plugin for EDA software named
SmartonAl is developed here, also KiCad is taken as the first example.
SmartonAI is inspired by the HuggingGPT framework and employs large language
models, such as GPT and BERT, to facilitate task planning and execution. On
receiving a designer request, SmartonAI conducts a task breakdown and
efficiently executes relevant subtasks, such as analysis of help documentation
paragraphs and execution of different plugins, along with leveraging the
built-in schematic and PCB manipulation functions in both SmartonAl itself and
software. Our preliminary results demonstrate that SmartonAI can significantly
streamline the PCB design process by simplifying complex commands into
intuitive language-based interactions. By harnessing the powerful language
capabilities of ChatGPT and the rich design functions of KiCad, the plugin
effectively bridges the gap between complex EDA software and user-friendly
interaction. Meanwhile, the new paradigm behind SmartonAI can also extend to
other complex software systems, illustrating the immense potential of
AI-assisted user interfaces in advancing digital interactions across various
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Boyu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yidong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14750">
<title>Exploring Annotation-free Image Captioning with Retrieval-augmented Pseudo Sentence Generation. (arXiv:2307.14750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14750</link>
<description rdf:parseType="Literal">&lt;p&gt;Training an image captioner without annotated image-sentence pairs has gained
traction in recent years. Previous approaches can be categorized into two
strategies: crawling sentences from mismatching corpora and aligning them with
the given images as pseudo annotations, or pre-training the captioner using
external image-text pairs. However, the aligning setting seems to reach its
performance limit due to the quality problem of pairs, and pre-training
requires significant computational resources. To address these challenges, we
propose a new strategy ``LPM + retrieval-augmented learning&quot; where the prior
knowledge from large pre-trained models (LPMs) is leveraged as supervision, and
a retrieval process is integrated to further reinforce its effectiveness.
Specifically, we introduce Retrieval-augmented Pseudo Sentence Generation
(RaPSG), which adopts an efficient approach to retrieve highly relevant short
region descriptions from the mismatching corpora and use them to generate a
variety of pseudo sentences with distinct representations as well as high
quality via LPMs. In addition, a fluency filter and a CLIP-guided training
objective are further introduced to facilitate model optimization. Experimental
results demonstrate that our method surpasses the SOTA pre-training model
(Flamingo3B) by achieving a CIDEr score of 78.1 (+5.1) while utilizing only
0.3% of its trainable parameters (1.3B VS 33M). Importantly, our approach
eliminates the need of computationally expensive pre-training processes on
external datasets (e.g., the requirement of 312M image-text pairs for
Flamingo3B). We further show that with a simple extension, the generated pseudo
sentences can be deployed as weak supervision to boost the 1% semi-supervised
image caption benchmark up to 93.4 CIDEr score (+8.9) which showcases the
versatility and effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongnan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14754">
<title>Fair Machine Unlearning: Data Removal while Mitigating Disparities. (arXiv:2307.14754v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14754</link>
<description rdf:parseType="Literal">&lt;p&gt;As public consciousness regarding the collection and use of personal
information by corporations grows, it is of increasing importance that
consumers be active participants in the curation of corporate datasets. In
light of this, data governance frameworks such as the General Data Protection
Regulation (GDPR) have outlined the right to be forgotten as a key principle
allowing individuals to request that their personal data be deleted from the
databases and models used by organizations. To achieve forgetting in practice,
several machine unlearning methods have been proposed to address the
computational inefficiencies of retraining a model from scratch with each
unlearning request. While efficient online alternatives to retraining, it is
unclear how these methods impact other properties critical to real-world
applications, such as fairness. In this work, we propose the first fair machine
unlearning method that can provably and efficiently unlearn data instances
while preserving group fairness. We derive theoretical results which
demonstrate that our method can provably unlearn data instances while
maintaining fairness objectives. Extensive experimentation with real-world
datasets highlight the efficacy of our method at unlearning data instances
while preserving fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oesterling_A/0/1/0/all/0/1&quot;&gt;Alex Oesterling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calmon_F/0/1/0/all/0/1&quot;&gt;Flavio P. Calmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1&quot;&gt;Hima Lakkaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14783">
<title>Emotion4MIDI: a Lyrics-based Emotion-Labeled Symbolic Music Dataset. (arXiv:2307.14783v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.14783</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new large-scale emotion-labeled symbolic music dataset
consisting of 12k MIDI songs. To create this dataset, we first trained emotion
classification models on the GoEmotions dataset, achieving state-of-the-art
results with a model half the size of the baseline. We then applied these
models to lyrics from two large-scale MIDI datasets. Our dataset covers a wide
range of fine-grained emotions, providing a valuable resource to explore the
connection between music and emotions and, especially, to develop models that
can generate music based on specific emotions. Our code for inference, trained
models, and datasets are available online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sulun_S/0/1/0/all/0/1&quot;&gt;Serkan Sulun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oliveira_P/0/1/0/all/0/1&quot;&gt;Pedro Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Viana_P/0/1/0/all/0/1&quot;&gt;Paula Viana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14799">
<title>Hybrid ASP-based multi-objective scheduling of semiconductor manufacturing processes (Extended version). (arXiv:2307.14799v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14799</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern semiconductor manufacturing involves intricate production processes
consisting of hundreds of operations, which can take several months from lot
release to completion. The high-tech machines used in these processes are
diverse, operate on individual wafers, lots, or batches in multiple stages, and
necessitate product-specific setups and specialized maintenance procedures.
This situation is different from traditional job-shop scheduling scenarios,
which have less complex production processes and machines, and mainly focus on
solving highly combinatorial but abstract scheduling problems. In this work, we
address the scheduling of realistic semiconductor manufacturing processes by
modeling their specific requirements using hybrid Answer Set Programming with
difference logic, incorporating flexible machine processing, setup, batching
and maintenance operations. Unlike existing methods that schedule semiconductor
manufacturing processes locally with greedy heuristics or by independently
optimizing specific machine group allocations, we examine the potentials of
large-scale scheduling subject to multiple optimization objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Kholany_M/0/1/0/all/0/1&quot;&gt;Mohammed M. S. El-Kholany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_R/0/1/0/all/0/1&quot;&gt;Ramsha Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebser_M/0/1/0/all/0/1&quot;&gt;Martin Gebser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14849">
<title>Counterfactual Explanations for Graph Classification Through the Lenses of Density. (arXiv:2307.14849v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14849</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual examples have emerged as an effective approach to produce
simple and understandable post-hoc explanations. In the context of graph
classification, previous work has focused on generating counterfactual
explanations by manipulating the most elementary units of a graph, i.e.,
removing an existing edge, or adding a non-existing one. In this paper, we
claim that such language of explanation might be too fine-grained, and turn our
attention to some of the main characterizing features of real-world complex
networks, such as the tendency to close triangles, the existence of recurring
motifs, and the organization into dense modules. We thus define a general
density-based counterfactual search framework to generate instance-level
counterfactual explanations for graph classifiers, which can be instantiated
with different notions of dense substructures. In particular, we show two
specific instantiations of this general framework: a method that searches for
counterfactual graphs by opening or closing triangles, and a method driven by
maximal cliques. We also discuss how the general method can be instantiated to
exploit any other notion of dense substructures, including, for instance, a
given taxonomy of nodes. We evaluate the effectiveness of our approaches in 7
brain network datasets and compare the counterfactual statements generated
according to several widely-used metrics. Results confirm that adopting a
semantic-relevant unit of change like density is essential to define versatile
and interpretable counterfactual explanation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrate_C/0/1/0/all/0/1&quot;&gt;Carlo Abrate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preti_G/0/1/0/all/0/1&quot;&gt;Giulia Preti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonchi_F/0/1/0/all/0/1&quot;&gt;Francesco Bonchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14856">
<title>Exploiting the Potential of Seq2Seq Models as Robust Few-Shot Learners. (arXiv:2307.14856v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14856</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning, which offers substantial advantages over fine-tuning, is
predominantly observed in decoder-only models, while encoder-decoder (i.e.,
seq2seq) models excel in methods that rely on weight updates. Recently, a few
studies have demonstrated the feasibility of few-shot learning with seq2seq
models; however, this has been limited to tasks that align well with the
seq2seq architecture, such as summarization and translation. Inspired by these
initial studies, we provide a first-ever extensive experiment comparing the
in-context few-shot learning capabilities of decoder-only and encoder-decoder
models on a broad range of tasks. Furthermore, we propose two methods to more
effectively elicit in-context learning ability in seq2seq models:
objective-aligned prompting and a fusion-based approach. Remarkably, our
approach outperforms a decoder-only model that is six times larger and exhibits
significant performance improvements compared to conventional seq2seq models
across a variety of settings. We posit that, with the right configuration and
prompt design, seq2seq models can be highly effective few-shot learners for a
wide spectrum of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jihyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dain Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1&quot;&gt;Doohae Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Boseop Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+On_K/0/1/0/all/0/1&quot;&gt;Kyoung-Woon On&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14889">
<title>Weakly Supervised Multi-Modal 3D Human Body Pose Estimation for Autonomous Driving. (arXiv:2307.14889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14889</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate 3D human pose estimation (3D HPE) is crucial for enabling autonomous
vehicles (AVs) to make informed decisions and respond proactively in critical
road scenarios. Promising results of 3D HPE have been gained in several domains
such as human-computer interaction, robotics, sports and medical analytics,
often based on data collected in well-controlled laboratory environments.
Nevertheless, the transfer of 3D HPE methods to AVs has received limited
research attention, due to the challenges posed by obtaining accurate 3D pose
annotations and the limited suitability of data from other domains.
&lt;/p&gt;
&lt;p&gt;We present a simple yet efficient weakly supervised approach for 3D HPE in
the AV context by employing a high-level sensor fusion between camera and LiDAR
data. The weakly supervised setting enables training on the target datasets
without any 2D/3D keypoint labels by using an off-the-shelf 2D joint extractor
and pseudo labels generated from LiDAR to image projections. Our approach
outperforms state-of-the-art results by up to $\sim$ 13% on the Waymo Open
Dataset in the weakly supervised setting and achieves state-of-the-art results
in the supervised setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_P/0/1/0/all/0/1&quot;&gt;Peter Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouazizi_A/0/1/0/all/0/1&quot;&gt;Arij Bouazizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kressel_U/0/1/0/all/0/1&quot;&gt;Ulrich Kressel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flohr_F/0/1/0/all/0/1&quot;&gt;Fabian B. Flohr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14893">
<title>Base-based Model Checking for Multi-Agent Only Believing (long version). (arXiv:2307.14893v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14893</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel semantics for the language of multi-agent only believing
exploiting belief bases, and show how to use it for automatically checking
formulas of this language and of its dynamic extension with private belief
expansion operators. We provide a PSPACE algorithm for model checking relying
on a reduction to QBF and alternative dedicated algorithm relying on the
exploration of the state space. We present an implementation of the QBF-based
algorithm and some experimental results on computation time in a concrete
example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_T/0/1/0/all/0/1&quot;&gt;Tiago de Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorini_E/0/1/0/all/0/1&quot;&gt;Emiliano Lorini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarzentruber_F/0/1/0/all/0/1&quot;&gt;Fran&amp;#xe7;ois Schwarzentruber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14901">
<title>Text-guided Foundation Model Adaptation for Pathological Image Classification. (arXiv:2307.14901v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14901</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent surge of foundation models in computer vision and natural language
processing opens up perspectives in utilizing multi-modal clinical data to
train large models with strong generalizability. Yet pathological image
datasets often lack biomedical text annotation and enrichment. Guiding
data-efficient image diagnosis from the use of biomedical text knowledge
becomes a substantial interest. In this paper, we propose to Connect Image and
Text Embeddings (CITE) to enhance pathological image classification. CITE
injects text insights gained from language models pre-trained with a broad
range of biomedical texts, leading to adapt foundation models towards
pathological image understanding. Through extensive experiments on the
PatchGastric stomach tumor pathological image dataset, we demonstrate that CITE
achieves leading performance compared with various baselines especially when
training data is scarce. CITE offers insights into leveraging in-domain text
knowledge to reinforce data-efficient pathological image classification. Code
is available at https://github.com/Yunkun-Zhang/CITE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunkun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Mu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dequan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14902">
<title>CodeLens: An Interactive Tool for Visualizing Code Representations. (arXiv:2307.14902v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.14902</link>
<description rdf:parseType="Literal">&lt;p&gt;Representing source code in a generic input format is crucial to automate
software engineering tasks, e.g., applying machine learning algorithms to
extract information. Visualizing code representations can further enable human
experts to gain an intuitive insight into the code. Unfortunately, as of today,
there is no universal tool that can simultaneously visualise different types of
code representations. In this paper, we introduce a tool, CodeLens, which
provides a visual interaction environment that supports various representation
methods and helps developers understand and explore them. CodeLens is designed
to support multiple programming languages, such as Java, Python, and
JavaScript, and four types of code representations, including sequence of
tokens, abstract syntax tree (AST), data flow graph (DFG), and control flow
graph (CFG). By using CodeLens, developers can quickly visualize the specific
code representation and also obtain the represented inputs for models of code.
The Web-based interface of CodeLens is available at &lt;a href=&quot;http://www.codelens.org.&quot;&gt;this http URL&lt;/a&gt;
The demonstration video can be found at &lt;a href=&quot;http://www.codelens.org/demo.&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuejun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bettaieb_S/0/1/0/all/0/1&quot;&gt;Seifeddine Bettaieb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Traon_Y/0/1/0/all/0/1&quot;&gt;Yves Le Traon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qiang Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14906">
<title>Scaling Session-Based Transformer Recommendations using Optimized Negative Sampling and Loss Functions. (arXiv:2307.14906v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.14906</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces TRON, a scalable session-based Transformer Recommender
using Optimized Negative-sampling. Motivated by the scalability and performance
limitations of prevailing models such as SASRec and GRU4Rec+, TRON integrates
top-k negative sampling and listwise loss functions to enhance its
recommendation accuracy. Evaluations on relevant large-scale e-commerce
datasets show that TRON improves upon the recommendation quality of current
methods while maintaining training speeds similar to SASRec. A live A/B test
yielded an 18.14% increase in click-through rate over SASRec, highlighting the
potential of TRON in practical settings. For further research, we provide
access to our source code at https://github.com/otto-de/TRON and an anonymized
dataset at https://github.com/otto-de/recsys-dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilm_T/0/1/0/all/0/1&quot;&gt;Timo Wilm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Normann_P/0/1/0/all/0/1&quot;&gt;Philipp Normann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumeister_S/0/1/0/all/0/1&quot;&gt;Sophie Baumeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobow_P/0/1/0/all/0/1&quot;&gt;Paul-Vincent Kobow&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14935">
<title>Solving Data Quality Problems with Desbordante: a Demo. (arXiv:2307.14935v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2307.14935</link>
<description rdf:parseType="Literal">&lt;p&gt;Data profiling is an essential process in modern data-driven industries. One
of its critical components is the discovery and validation of complex
statistics, including functional dependencies, data constraints, association
rules, and others.
&lt;/p&gt;
&lt;p&gt;However, most existing data profiling systems that focus on complex
statistics do not provide proper integration with the tools used by
contemporary data scientists. This creates a significant barrier to the
adoption of these tools in the industry. Moreover, existing systems were not
created with industrial-grade workloads in mind. Finally, they do not aim to
provide descriptive explanations, i.e. why a given pattern is not found. It is
a significant issue as it is essential to understand the underlying reasons for
a specific pattern&apos;s absence to make informed decisions based on the data.
&lt;/p&gt;
&lt;p&gt;Because of that, these patterns are effectively rest in thin air: their
application scope is rather limited, they are rarely used by the broader
public. At the same time, as we are going to demonstrate in this presentation,
complex statistics can be efficiently used to solve many classic data quality
problems.
&lt;/p&gt;
&lt;p&gt;Desbordante is an open-source data profiler that aims to close this gap. It
is built with emphasis on industrial application: it is efficient, scalable,
resilient to crashes, and provides explanations. Furthermore, it provides
seamless Python integration by offloading various costly operations to the C++
core, not only mining.
&lt;/p&gt;
&lt;p&gt;In this demonstration, we show several scenarios that allow end users to
solve different data quality problems. Namely, we showcase typo detection, data
deduplication, and data anomaly detection scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernishev_G/0/1/0/all/0/1&quot;&gt;George Chernishev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polyntsov_M/0/1/0/all/0/1&quot;&gt;Michael Polyntsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chizhov_A/0/1/0/all/0/1&quot;&gt;Anton Chizhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stupakov_K/0/1/0/all/0/1&quot;&gt;Kirill Stupakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shchuckin_I/0/1/0/all/0/1&quot;&gt;Ilya Shchuckin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_A/0/1/0/all/0/1&quot;&gt;Alexander Smirnov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strutovsky_M/0/1/0/all/0/1&quot;&gt;Maxim Strutovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlyonskikh_A/0/1/0/all/0/1&quot;&gt;Alexey Shlyonskikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Firsov_M/0/1/0/all/0/1&quot;&gt;Mikhail Firsov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manannikov_S/0/1/0/all/0/1&quot;&gt;Stepan Manannikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bobrov_N/0/1/0/all/0/1&quot;&gt;Nikita Bobrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncharov_D/0/1/0/all/0/1&quot;&gt;Daniil Goncharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barutkin_I/0/1/0/all/0/1&quot;&gt;Ilia Barutkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalnev_V/0/1/0/all/0/1&quot;&gt;Vladislav Shalnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muraviev_K/0/1/0/all/0/1&quot;&gt;Kirill Muraviev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhmukova_A/0/1/0/all/0/1&quot;&gt;Anna Rakhmukova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shcheka_D/0/1/0/all/0/1&quot;&gt;Dmitriy Shcheka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chernikov_A/0/1/0/all/0/1&quot;&gt;Anton Chernikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyrodov_M/0/1/0/all/0/1&quot;&gt;Mikhail Vyrodov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaroslav_K/0/1/0/all/0/1&quot;&gt;Kurbatov Yaroslav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fofanov_M/0/1/0/all/0/1&quot;&gt;Maxim Fofanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sergei_B/0/1/0/all/0/1&quot;&gt;Belokonnyi Sergei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavel_A/0/1/0/all/0/1&quot;&gt;Anosov Pavel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saliou_A/0/1/0/all/0/1&quot;&gt;Arthur Saliou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaisin_E/0/1/0/all/0/1&quot;&gt;Eduard Gaisin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_K/0/1/0/all/0/1&quot;&gt;Kirill Smirnov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14936">
<title>PanGu-Coder2: Boosting Large Language Models for Code with Ranking Feedback. (arXiv:2307.14936v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14936</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models for Code (Code LLM) are flourishing. New and powerful
models are released on a weekly basis, demonstrating remarkable performance on
the code generation task. Various approaches have been proposed to boost the
code generation performance of pre-trained Code LLMs, such as supervised
fine-tuning, instruction tuning, reinforcement learning, etc. In this paper, we
propose a novel RRTF (Rank Responses to align Test&amp;amp;Teacher Feedback) framework,
which can effectively and efficiently boost pre-trained large language models
for code generation. Under this framework, we present PanGu-Coder2, which
achieves 62.20% pass@1 on the OpenAI HumanEval benchmark. Furthermore, through
an extensive evaluation on CoderEval and LeetCode benchmarks, we show that
PanGu-Coder2 consistently outperforms all previous Code LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_B/0/1/0/all/0/1&quot;&gt;Bo Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Taihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zan_D/0/1/0/all/0/1&quot;&gt;Daoguang Zan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_B/0/1/0/all/0/1&quot;&gt;Bing Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_A/0/1/0/all/0/1&quot;&gt;An Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Muhan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_A/0/1/0/all/0/1&quot;&gt;Ailun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jichuan Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jingyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuenan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qianxiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14953">
<title>Multi-Source Domain Adaptation through Dataset Dictionary Learning in Wasserstein Space. (arXiv:2307.14953v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14953</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper seeks to solve Multi-Source Domain Adaptation (MSDA), which aims
to mitigate data distribution shifts when transferring knowledge from multiple
labeled source domains to an unlabeled target domain. We propose a novel MSDA
framework based on dictionary learning and optimal transport. We interpret each
domain in MSDA as an empirical distribution. As such, we express each domain as
a Wasserstein barycenter of dictionary atoms, which are empirical
distributions. We propose a novel algorithm, DaDiL, for learning via
mini-batches: (i) atom distributions; (ii) a matrix of barycentric coordinates.
Based on our dictionary, we propose two novel methods for MSDA: DaDil-R, based
on the reconstruction of labeled samples in the target domain, and DaDiL-E,
based on the ensembling of classifiers learned on atom distributions. We
evaluate our methods in 3 benchmarks: Caltech-Office, Office 31, and CRWU,
where we improved previous state-of-the-art by 3.15%, 2.29%, and 7.71% in
classification performance. Finally, we show that interpolations in the
Wasserstein hull of learned atoms provide data that can generalize to the
target domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montesuma_E/0/1/0/all/0/1&quot;&gt;Eduardo Fernandes Montesuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mboula_F/0/1/0/all/0/1&quot;&gt;Fred Ngol&amp;#xe8; Mboula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Souloumiac_A/0/1/0/all/0/1&quot;&gt;Antoine Souloumiac&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14971">
<title>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14971</link>
<description rdf:parseType="Literal">&lt;p&gt;With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xumin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1&quot;&gt;Yongming Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14991">
<title>Multilingual Code Co-Evolution Using Large Language Models. (arXiv:2307.14991v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.14991</link>
<description rdf:parseType="Literal">&lt;p&gt;Many software projects implement APIs and algorithms in multiple programming
languages. Maintaining such projects is tiresome, as developers have to ensure
that any change (e.g., a bug fix or a new feature) is being propagated, timely
and without errors, to implementations in other programming languages. In the
world of ever-changing software, using rule-based translation tools (i.e.,
transpilers) or machine learning models for translating code from one language
to another provides limited value. Translating each time the entire codebase
from one language to another is not the way developers work. In this paper, we
target a novel task: translating code changes from one programming language to
another using large language models (LLMs). We design and implement the first
LLM, dubbed Codeditor, to tackle this task. Codeditor explicitly models code
changes as edit sequences and learns to correlate changes across programming
languages. To evaluate Codeditor, we collect a corpus of 6,613 aligned code
changes from 8 pairs of open-source software projects implementing similar
functionalities in two programming languages (Java and C#). Results show that
Codeditor outperforms the state-of-the-art approaches by a large margin on all
commonly used automatic metrics. Our work also reveals that Codeditor is
complementary to the existing generation-based models, and their combination
ensures even greater performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_P/0/1/0/all/0/1&quot;&gt;Pengyu Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junyi Jessy Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gligoric_M/0/1/0/all/0/1&quot;&gt;Milos Gligoric&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14993">
<title>Thinker: Learning to Plan and Act. (arXiv:2307.14993v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.14993</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the Thinker algorithm, a novel approach that enables reinforcement
learning agents to autonomously interact with and utilize a learned world
model. The Thinker algorithm wraps the environment with a world model and
introduces new actions designed for interacting with the world model. These
model-interaction actions enable agents to perform planning by proposing
alternative plans to the world model before selecting a final action to execute
in the environment. This approach eliminates the need for hand-crafted planning
algorithms by enabling the agent to learn how to plan autonomously and allows
for easy interpretation of the agent&apos;s plan with visualization. We demonstrate
the algorithm&apos;s effectiveness through experimental results in the game of
Sokoban and the Atari 2600 benchmark, where the Thinker algorithm achieves
state-of-the-art performance and competitive results, respectively.
Visualizations of agents trained with the Thinker algorithm demonstrate that
they have learned to plan effectively with the world model to select better
actions. The algorithm&apos;s generality opens a new research direction on how a
world model can be used in reinforcement learning and how planning can be
seamlessly integrated into an agent&apos;s decision-making process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;Stephen Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anokhin_I/0/1/0/all/0/1&quot;&gt;Ivan Anokhin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krueger_D/0/1/0/all/0/1&quot;&gt;David Krueger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15008">
<title>A LLM Assisted Exploitation of AI-Guardian. (arXiv:2307.15008v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.15008</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are now highly capable at a diverse range of
tasks. This paper studies whether or not GPT-4, one such LLM, is capable of
assisting researchers in the field of adversarial machine learning. As a case
study, we evaluate the robustness of AI-Guardian, a recent defense to
adversarial examples published at IEEE S&amp;amp;P 2023, a top computer security
conference. We completely break this defense: the proposed scheme does not
increase robustness compared to an undefended baseline.
&lt;/p&gt;
&lt;p&gt;We write none of the code to attack this model, and instead prompt GPT-4 to
implement all attack algorithms following our instructions and guidance. This
process was surprisingly effective and efficient, with the language model at
times producing code from ambiguous instructions faster than the author of this
paper could have done. We conclude by discussing (1) the warning signs present
in the evaluation that suggested to us AI-Guardian would be broken, and (2) our
experience with designing attacks and performing novel research using the most
recent advances in language modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlini_N/0/1/0/all/0/1&quot;&gt;Nicholas Carlini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15016">
<title>How Good is Google Bard&apos;s Visual Understanding? An Empirical Study on Open Challenges. (arXiv:2307.15016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.15016</link>
<description rdf:parseType="Literal">&lt;p&gt;Google&apos;s Bard has emerged as a formidable competitor to OpenAI&apos;s ChatGPT in
the field of conversational AI. Notably, Bard has recently been updated to
handle visual inputs alongside text prompts during conversations. Given Bard&apos;s
impressive track record in handling textual inputs, we explore its capabilities
in understanding and interpreting visual data (images) conditioned by text
questions. This exploration holds the potential to unveil new insights and
challenges for Bard and other forthcoming multi-modal Generative models,
especially in addressing complex computer vision problems that demand accurate
visual and language understanding. Specifically, in this study, we focus on 15
diverse task scenarios encompassing regular, camouflaged, medical, under-water
and remote sensing data to comprehensively evaluate Bard&apos;s performance. Our
primary finding indicates that Bard still struggles in these vision scenarios,
highlighting the significant gap in vision-based understanding that needs to be
bridged in future developments. We expect that this empirical study will prove
valuable in advancing future models, leading to enhanced capabilities in
comprehending and interpreting fine-grained visual data. Our project is
released on https://github.com/htqin/GoogleBard-VisUnderstand
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_H/0/1/0/all/0/1&quot;&gt;Haotong Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Ge-Peng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15020">
<title>SuperCLUE: A Comprehensive Chinese Large Language Model Benchmark. (arXiv:2307.15020v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15020</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown the potential to be integrated into
human daily lives. Therefore, user preference is the most critical criterion
for assessing LLMs&apos; performance in real-world scenarios. However, existing
benchmarks mainly focus on measuring models&apos; accuracy using multi-choice
questions, which limits the understanding of their capabilities in real
applications. We fill this gap by proposing a comprehensive Chinese benchmark
SuperCLUE, named after another popular Chinese LLM benchmark CLUE. SuperCLUE
encompasses three sub-tasks: actual users&apos; queries and ratings derived from an
LLM battle platform (CArena), open-ended questions with single and
multiple-turn dialogues (OPEN), and closed-ended questions with the same stems
as open-ended single-turn ones (CLOSE). Our study shows that accuracy on
closed-ended questions is insufficient to reflect human preferences achieved on
open-ended ones. At the same time, they can complement each other to predict
actual user preferences. We also demonstrate that GPT-4 is a reliable judge to
automatically evaluate human preferences on open-ended questions in a Chinese
context. Our benchmark will be released at https://www.CLUEbenchmarks.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Liang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Anqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Hang Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Changtai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kangkang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haonan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuanwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qiyue Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15043">
<title>Universal and Transferable Adversarial Attacks on Aligned Language Models. (arXiv:2307.15043v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15043</link>
<description rdf:parseType="Literal">&lt;p&gt;Because &quot;out-of-the-box&quot; large language models are capable of generating a
great deal of objectionable content, recent work has focused on aligning these
models in an attempt to prevent undesirable generation. While there has been
some success at circumventing these measures -- so-called &quot;jailbreaks&quot; against
LLMs -- these attacks have required significant human ingenuity and are brittle
in practice. In this paper, we propose a simple and effective attack method
that causes aligned language models to generate objectionable behaviors.
Specifically, our approach finds a suffix that, when attached to a wide range
of queries for an LLM to produce objectionable content, aims to maximize the
probability that the model produces an affirmative response (rather than
refusing to answer). However, instead of relying on manual engineering, our
approach automatically produces these adversarial suffixes by a combination of
greedy and gradient-based search techniques, and also improves over past
automatic prompt generation methods.
&lt;/p&gt;
&lt;p&gt;Surprisingly, we find that the adversarial prompts generated by our approach
are quite transferable, including to black-box, publicly released LLMs.
Specifically, we train an adversarial attack suffix on multiple prompts (i.e.,
queries asking for many different types of objectionable content), as well as
multiple models (in our case, Vicuna-7B and 13B). When doing so, the resulting
attack suffix is able to induce objectionable content in the public interfaces
to ChatGPT, Bard, and Claude, as well as open source LLMs such as LLaMA-2-Chat,
Pythia, Falcon, and others. In total, this work significantly advances the
state-of-the-art in adversarial attacks against aligned language models,
raising important questions about how such systems can be prevented from
producing objectionable information. Code is available at
github.com/llm-attacks/llm-attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_A/0/1/0/all/0/1&quot;&gt;Andy Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fredrikson_M/0/1/0/all/0/1&quot;&gt;Matt Fredrikson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15051">
<title>Matching Patients to Clinical Trials with Large Language Models. (arXiv:2307.15051v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.15051</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical trials are vital in advancing drug development and evidence-based
medicine, but their success is often hindered by challenges in patient
recruitment. In this work, we investigate the potential of large language
models (LLMs) to assist individual patients and referral physicians in
identifying suitable clinical trials from an extensive selection. Specifically,
we introduce TrialGPT, a novel architecture employing LLMs to predict
criterion-level eligibility with detailed explanations, which are then
aggregated for ranking and excluding candidate clinical trials based on
free-text patient notes. We evaluate TrialGPT on three publicly available
cohorts of 184 patients and 18,238 annotated clinical trials. The experimental
results demonstrate several key findings: First, TrialGPT achieves high
criterion-level prediction accuracy with faithful explanations. Second, the
aggregated trial-level TrialGPT scores are highly correlated with expert
eligibility annotations. Third, these scores prove effective in ranking
clinical trials and exclude ineligible candidates. Our error analysis suggests
that current LLMs still make some mistakes due to limited medical knowledge and
domain-specific context understanding. Nonetheless, we believe the explanatory
capabilities of LLMs are highly valuable. Future research is warranted on how
such AI assistants can be integrated into the routine trial matching workflow
in real-world settings to improve its efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qiao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zifeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Floudas_C/0/1/0/all/0/1&quot;&gt;Charalampos S. Floudas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1912.13122">
<title>Towards Regulated Deep Learning. (arXiv:1912.13122v7 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/1912.13122</link>
<description rdf:parseType="Literal">&lt;p&gt;Regulation of Multi-Agent Systems (MAS) and Declarative Electronic
Institutions (DEIs) was a multidisciplinary research topic of the past decade
involving (Physical and Software) Agents and Law since the beginning, but
recently evolved towards News-claimed Robot Lawyer since 2016. One of these
first proposals of restricting the behaviour of Software Agents was Electronic
Institutions.However, with the recent reformulation of Artificial Neural
Networks (ANNs) as Deep Learning (DL), Security, Privacy,Ethical and Legal
issues regarding the use of DL has raised concerns in the Artificial
Intelligence (AI) Community. Now that the Regulation of MAS is almost correctly
addressed, we propose the Regulation of Artificial Neural Networks as
Agent-based Training of a special type of regulated Artificial Neural Network
that we call Institutional Neural Network (INN).The main purpose of this paper
is to bring attention to Artificial Teaching (AT) and to give a tentative
answer showing a proof-of-concept implementation of Regulated Deep Learning
(RDL). This paper introduces the former concept and provide $I^*$, a language
previously used to model declaratively and extend Electronic Institutions, as a
means to regulate the execution of Artificial Neural Networks and their
interactions with Artificial Teachers (ATs)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Camino_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Garc&amp;#xed;a-Camino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2005.00695">
<title>On the Generalization Effects of Linear Transformations in Data Augmentation. (arXiv:2005.00695v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2005.00695</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is a powerful technique to improve performance in
applications such as image and text classification tasks. Yet, there is little
rigorous understanding of why and how various augmentations work. In this work,
we consider a family of linear transformations and study their effects on the
ridge estimator in an over-parametrized linear regression setting. First, we
show that transformations that preserve the labels of the data can improve
estimation by enlarging the span of the training data. Second, we show that
transformations that mix data can improve estimation by playing a
regularization effect. Finally, we validate our theoretical insights on MNIST.
Based on the insights, we propose an augmentation scheme that searches over the
space of transformations by how uncertain the model is about the transformed
data. We validate our proposed scheme on image and text datasets. For example,
our method outperforms random sampling methods by 1.24% on CIFAR-100 using
Wide-ResNet-28-10. Furthermore, we achieve comparable accuracy to the SoTA
Adversarial AutoAugment on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyang R. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valiant_G/0/1/0/all/0/1&quot;&gt;Gregory Valiant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Re_C/0/1/0/all/0/1&quot;&gt;Christopher R&amp;#xe9;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.08960">
<title>Deep Serial Number: Computational Watermarking for DNN Intellectual Property Protection. (arXiv:2011.08960v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2011.08960</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present DSN (Deep Serial Number), a simple yet effective
watermarking algorithm designed specifically for deep neural networks (DNNs).
Unlike traditional methods that incorporate identification signals into DNNs,
our approach explores a novel Intellectual Property (IP) protection mechanism
for DNNs, effectively thwarting adversaries from using stolen networks.
Inspired by the success of serial numbers in safeguarding conventional software
IP, we propose the first implementation of serial number embedding within DNNs.
To achieve this, DSN is integrated into a knowledge distillation framework, in
which a private teacher DNN is initially trained. Subsequently, its knowledge
is distilled and imparted to a series of customized student DNNs. Each customer
DNN functions correctly only upon input of a valid serial number. Experimental
results across various applications demonstrate DSN&apos;s efficacy in preventing
unauthorized usage without compromising the original DNN performance. The
experiments further show that DSN is resistant to different categories of
watermark attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Ruixiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.10206">
<title>DanceFormer: Music Conditioned 3D Dance Generation with Parametric Motion Transformer. (arXiv:2103.10206v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2103.10206</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating 3D dances from music is an emerged research task that benefits a
lot of applications in vision and graphics. Previous works treat this task as
sequence generation, however, it is challenging to render a music-aligned
long-term sequence with high kinematic complexity and coherent movements. In
this paper, we reformulate it by a two-stage process, ie, a key pose generation
and then an in-between parametric motion curve prediction, where the key poses
are easier to be synchronized with the music beats and the parametric curves
can be efficiently regressed to render fluent rhythm-aligned movements. We
named the proposed method as DanceFormer, which includes two cascading
kinematics-enhanced transformer-guided networks (called DanTrans) that tackle
each stage, respectively. Furthermore, we propose a large-scale music
conditioned 3D dance dataset, called PhantomDance, that is accurately labeled
by experienced animators rather than reconstruction or motion capture. This
dataset also encodes dances as key poses and parametric motion curves apart
from pose sequences, thus benefiting the training of our DanceFormer. Extensive
experiments demonstrate that the proposed method, even trained by existing
datasets, can generate fluent, performative, and music-matched 3D dances that
surpass previous works quantitatively and qualitatively. Moreover, the proposed
DanceFormer, together with the PhantomDance dataset
(https://github.com/libuyu/PhantomDanceDataset), are seamlessly compatible with
industrial animation software, thus facilitating the adaptation for various
downstream applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Buyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yongchi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhelun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lu Sheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.02626">
<title>Dynamics of specialization in neural modules under resource constraints. (arXiv:2106.02626v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2106.02626</link>
<description rdf:parseType="Literal">&lt;p&gt;It has long been believed that the brain is highly modular both in terms of
structure and function, although recent evidence has led some to question the
extent of both types of modularity. We used artificial neural networks to test
the hypothesis that structural modularity is sufficient to guarantee functional
specialization, and find that in general, this doesn&apos;t necessarily hold except
at extreme levels. We then systematically tested which features of the
environment and network do lead to the emergence of specialization. We used a
simple toy environment, task and network, allowing us precise control, and show
that in this setup, several distinct measures of specialization give
qualitatively similar results. We further find that (1) specialization can only
emerge in environments where features of that environment are meaningfully
separable, (2) specialization preferentially emerges when the network is
strongly resource-constrained, and (3) these findings are qualitatively similar
across different network architectures, but the quantitative relationships
depends on the architecture type. Finally, we show that functional
specialization varies dynamically across time, and demonstrate that these
dynamics depend on both the timing and bandwidth of information flow in the
network. We conclude that a static notion of specialization, based on
structural modularity, is likely too simple a framework for understanding
intelligent systems in situations of real-world complexity. We propose that
thoroughly stress testing candidate definitions of functional modularity in
simplified scenarios before extending to more complex data, network models and
electrophysiological recordings is likely to be a fruitful approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bena_G/0/1/0/all/0/1&quot;&gt;Gabriel B&amp;#xe9;na&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Goodman_D/0/1/0/all/0/1&quot;&gt;Dan F. M. Goodman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.11277">
<title>Machine Learning with a Reject Option: A survey. (arXiv:2107.11277v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2107.11277</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models always make a prediction, even when it is likely to
be inaccurate. This behavior should be avoided in many decision support
applications, where mistakes can have severe consequences. Albeit already
studied in 1970, machine learning with rejection recently gained interest. This
machine learning subfield enables machine learning models to abstain from
making a prediction when likely to make a mistake.
&lt;/p&gt;
&lt;p&gt;This survey aims to provide an overview on machine learning with rejection.
We introduce the conditions leading to two types of rejection, ambiguity and
novelty rejection, which we carefully formalize. Moreover, we review and
categorize strategies to evaluate a model&apos;s predictive and rejective quality.
Additionally, we define the existing architectures for models with rejection
and describe the standard techniques for learning such models. Finally, we
provide examples of relevant application domains and show how machine learning
with rejection relates to other machine learning research areas.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrickx_K/0/1/0/all/0/1&quot;&gt;Kilian Hendrickx&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perini_L/0/1/0/all/0/1&quot;&gt;Lorenzo Perini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plas_D/0/1/0/all/0/1&quot;&gt;Dries Van der Plas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1&quot;&gt;Wannes Meert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jesse Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.03622">
<title>Graph-Based Recommendation System Enhanced with Community Detection. (arXiv:2201.03622v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2201.03622</link>
<description rdf:parseType="Literal">&lt;p&gt;Many researchers have used tag information to improve the performance of
recommendation techniques in recommender systems. Examining the tags of users
will help to get their interests and leads to more accuracy in the
recommendations. Since user-defined tags are chosen freely and without any
restrictions, problems arise in determining their exact meaning and the
similarity of tags. However, using thesaurus and ontologies to find the meaning
of tags is not very efficient due to their free definition by users and the use
of different languages in many data sets. Therefore, this article uses
mathematical and statistical methods to determine lexical similarity and
co-occurrence tags solution to assign semantic similarity. On the other hand,
due to the change of users&apos; interests over time this article has considered the
time of tag assignments in co-occurrence tags for determining similarity of
tags. Then the graph is created based on similarity of tags. For modeling the
interests of the users, the communities of tags are determined by using
community detection methods. So, recommendations based on the communities of
tags and similarity between resources are done. The performance of the proposed
method has been evaluated using two criteria of precision and recall through
evaluations on two public datasets. The evaluation results show that the
precision and recall of the proposed method have significantly improved,
compared to the other methods. According to the experimental results, the
criteria of recall and precision have been improved, on average by 5% and 7%
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shokrzadeh_Z/0/1/0/all/0/1&quot;&gt;Zeinab Shokrzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1&quot;&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balafar_M/0/1/0/all/0/1&quot;&gt;Mohammad-Ali Balafar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagherzadeh_Mohasefi_J/0/1/0/all/0/1&quot;&gt;Jamshid Bagherzadeh-Mohasefi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14704">
<title>Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning. (arXiv:2205.14704v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14704</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt learning approaches have made waves in natural language processing by
inducing better few-shot performance while they still follow a parametric-based
learning paradigm; the oblivion and rote memorization problems in learning may
encounter unstable generalization issues. Specifically, vanilla prompt learning
may struggle to utilize atypical instances by rote during fully-supervised
training or overfit shallow patterns with low-shot data. To alleviate such
limitations, we develop RetroPrompt with the motivation of decoupling knowledge
from memorization to help the model strike a balance between generalization and
memorization. In contrast with vanilla prompt learning, RetroPrompt constructs
an open-book knowledge-store from training instances and implements a retrieval
mechanism during the process of input, training and inference, thus equipping
the model with the ability to retrieve related contexts from the training
corpus as cues for enhancement. Extensive experiments demonstrate that
RetroPrompt can obtain better performance in both few-shot and zero-shot
settings. Besides, we further illustrate that our proposed RetroPrompt can
yield better generalization abilities with new datasets. Detailed analysis of
memorization indeed reveals RetroPrompt can reduce the reliance of language
models on memorization; thus, improving generalization for downstream tasks.
Code is available in
https://github.com/zjunlp/PromptKG/tree/main/research/RetroPrompt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaozhuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chuanqi Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1&quot;&gt;Luo Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.12672">
<title>Trace Recovery from Stochastically Known Logs. (arXiv:2206.12672v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.12672</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we propose an algorithm for trace recovery from stochastically
known logs, a setting that is becoming more common with the increasing number
of sensors and predictive models that generate uncertain data. The suggested
approach calculates the conformance between a process model and a
stochastically known trace and recovers the best alignment within this
stochastic trace as the true trace. The paper offers an analysis of the impact
of various cost models on trace recovery accuracy and makes use of a product
multi-graph to compare alternative trace recovery options. The average accuracy
of our approach, evaluated using two publicly available datasets, is
impressive, with an average recovery accuracy score of 90-97%, significantly
improving a common heuristic that chooses the most likely value for each
uncertain activity. We believe that the effectiveness of the proposed algorithm
in recovering correct traces from stochastically known logs may be a powerful
aid for developing credible decision-making tools in uncertain settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdanov_E/0/1/0/all/0/1&quot;&gt;Eli Bogdanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_I/0/1/0/all/0/1&quot;&gt;Izack Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gal_A/0/1/0/all/0/1&quot;&gt;Avigdor Gal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.00052">
<title>Visual Pre-training for Navigation: What Can We Learn from Noise?. (arXiv:2207.00052v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.00052</link>
<description rdf:parseType="Literal">&lt;p&gt;One powerful paradigm in visual navigation is to predict actions from
observations directly. Training such an end-to-end system allows
representations useful for downstream tasks to emerge automatically. However,
the lack of inductive bias makes this system data inefficient. We hypothesize a
sufficient representation of the current view and the goal view for a
navigation policy can be learned by predicting the location and size of a crop
of the current view that corresponds to the goal. We further show that training
such random crop prediction in a self-supervised fashion purely on synthetic
noise images transfers well to natural home images. The learned representation
can then be bootstrapped to learn a navigation policy efficiently with little
interaction data. The code is available at https://yanweiw.github.io/noise2ptz
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_C/0/1/0/all/0/1&quot;&gt;Ching-Yun Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04045">
<title>Runtime Analysis for Permutation-based Evolutionary Algorithms. (arXiv:2207.04045v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04045</link>
<description rdf:parseType="Literal">&lt;p&gt;While the theoretical analysis of evolutionary algorithms (EAs) has made
significant progress for pseudo-Boolean optimization problems in the last 25
years, only sporadic theoretical results exist on how EAs solve
permutation-based problems.
&lt;/p&gt;
&lt;p&gt;To overcome the lack of permutation-based benchmark problems, we propose a
general way to transfer the classic pseudo-Boolean benchmarks into benchmarks
defined on sets of permutations. We then conduct a rigorous runtime analysis of
the permutation-based $(1+1)$ EA proposed by Scharnow, Tinnefeld, and Wegener
(2004) on the analogues of the LeadingOnes and Jump benchmarks. The latter
shows that, different from bit-strings, it is not only the Hamming distance
that determines how difficult it is to mutate a permutation $\sigma$ into
another one $\tau$, but also the precise cycle structure of $\sigma \tau^{-1}$.
For this reason, we also regard the more symmetric scramble mutation operator.
We observe that it not only leads to simpler proofs, but also reduces the
runtime on jump functions with odd jump size by a factor of $\Theta(n)$.
Finally, we show that a heavy-tailed version of the scramble operator, as in
the bit-string case, leads to a speed-up of order $m^{\Theta(m)}$ on jump
functions with jump size $m$. A short empirical analysis confirms these
findings, but also reveals that small implementation details like the rate of
void mutations can make an important difference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghannane_Y/0/1/0/all/0/1&quot;&gt;Yassine Ghannane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahim_M/0/1/0/all/0/1&quot;&gt;Marouane Ibn Brahim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10255">
<title>On the non-efficient PAC learnability of conjunctive queries. (arXiv:2208.10255v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10255</link>
<description rdf:parseType="Literal">&lt;p&gt;This note serves three purposes: (i) we provide a self-contained exposition
of the fact that conjunctive queries are not efficiently learnable in the
Probably-Approximately-Correct (PAC) model, paying clear attention to the
complicating fact that this concept class lacks the polynomial-size fitting
property, a property that is tacitly assumed in much of the computational
learning theory literature; (ii) we establish a strong negative PAC
learnability result that applies to many restricted classes of conjunctive
queries (CQs), including acyclic CQs for a wide range of notions of
&quot;acyclicity&quot;; (iii) we show that CQs (and UCQs) are efficiently PAC learnable
with membership queries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cate_B/0/1/0/all/0/1&quot;&gt;Balder ten Cate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funk_M/0/1/0/all/0/1&quot;&gt;Maurice Funk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jean Christoph Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lutz_C/0/1/0/all/0/1&quot;&gt;Carsten Lutz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.11838">
<title>Learning Task Automata for Reinforcement Learning using Hidden Markov Models. (arXiv:2208.11838v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.11838</link>
<description rdf:parseType="Literal">&lt;p&gt;Training reinforcement learning (RL) agents using scalar reward signals is
often infeasible when an environment has sparse and non-Markovian rewards.
Moreover, handcrafting these reward functions before training is prone to
misspecification, especially when the environment&apos;s dynamics are only partially
known. This paper proposes a novel pipeline for learning non-Markovian task
specifications as succinct finite-state `task automata&apos; from episodes of agent
experience within unknown environments. We leverage two key algorithmic
insights. First, we learn a product MDP, a model composed of the
specification&apos;s automaton and the environment&apos;s MDP (both initially unknown),
by treating the product MDP as a partially observable MDP and using the
well-known Baum-Welch algorithm for learning hidden Markov models. Second, we
propose a novel method for distilling the task automaton (assumed to be a
deterministic finite automaton) from the learnt product MDP. Our learnt task
automaton enables the decomposition of a task into its constituent sub-tasks,
which improves the rate at which an RL agent can later synthesise an optimal
policy. It also provides an interpretable encoding of high-level environmental
and task features, so a human can readily verify that the agent has learnt
coherent tasks with no misspecifications. In addition, we take steps towards
ensuring that the learnt automaton is environment-agnostic, making it
well-suited for use in transfer learning. Finally, we provide experimental
results compared with two baselines to illustrate our algorithm&apos;s performance
in different environments and tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abate_A/0/1/0/all/0/1&quot;&gt;Alessandro Abate&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almulla_Y/0/1/0/all/0/1&quot;&gt;Yousif Almulla&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_J/0/1/0/all/0/1&quot;&gt;James Fox&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyland_D/0/1/0/all/0/1&quot;&gt;David Hyland&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wooldridge_M/0/1/0/all/0/1&quot;&gt;Michael Wooldridge&lt;/a&gt; (1) ((1) University of Oxford)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.06589">
<title>Towards Better Generalization with Flexible Representation of Multi-Module Graph Neural Networks. (arXiv:2209.06589v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.06589</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have become compelling models designed to
perform learning and inference on graph-structured data. However, little work
has been done to understand the fundamental limitations of GNNs for scaling to
larger graphs and generalizing to out-of-distribution (OOD) inputs. In this
paper, we use a random graph generator to systematically investigate how the
graph size and structural properties affect the predictive performance of GNNs.
We present specific evidence that the average node degree is a key feature in
determining whether GNNs can generalize to unseen graphs, and that the use of
multiple node update functions can improve the generalization performance of
GNNs when dealing with graphs of multimodal degree distributions. Accordingly,
we propose a multi-module GNN framework that allows the network to adapt
flexibly to new graphs by generalizing a single canonical nonlinear
transformation over aggregated inputs. Our results show that the multi-module
GNNs improve the OOD generalization on a variety of inference tasks in the
direction of diverse structural features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungeun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kijung Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01913">
<title>Learning a Generic Value-Selection Heuristic Inside a Constraint Programming Solver. (arXiv:2301.01913v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01913</link>
<description rdf:parseType="Literal">&lt;p&gt;Constraint programming is known for being an efficient approach for solving
combinatorial problems. Important design choices in a solver are the branching
heuristics, which are designed to lead the search to the best solutions in a
minimum amount of time. However, developing these heuristics is a
time-consuming process that requires problem-specific expertise. This
observation has motivated many efforts to use machine learning to automatically
learn efficient heuristics without expert intervention. To the best of our
knowledge, it is still an open research question. Although several generic
variable-selection heuristics are available in the literature, the options for
a generic value-selection heuristic are more scarce. In this paper, we propose
to tackle this issue by introducing a generic learning procedure that can be
used to obtain a value-selection heuristic inside a constraint programming
solver. This has been achieved thanks to the combination of a deep Q-learning
algorithm, a tailored reward signal, and a heterogeneous graph neural network
architecture. Experiments on graph coloring, maximum independent set, and
maximum cut problems show that our framework is able to find better solutions
close to optimality without requiring a large amounts of backtracks while being
generic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marty_T/0/1/0/all/0/1&quot;&gt;Tom Marty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francois_T/0/1/0/all/0/1&quot;&gt;Tristan Fran&amp;#xe7;ois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessier_P/0/1/0/all/0/1&quot;&gt;Pierre Tessier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauthier_L/0/1/0/all/0/1&quot;&gt;Louis Gauthier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseau_L/0/1/0/all/0/1&quot;&gt;Louis-Martin Rousseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cappart_Q/0/1/0/all/0/1&quot;&gt;Quentin Cappart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11596">
<title>ThoughtSource: A central hub for large language model reasoning data. (arXiv:2301.11596v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11596</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) such as GPT-4 have recently demonstrated
impressive results across a wide range of tasks. LLMs are still limited,
however, in that they frequently fail at complex reasoning, their reasoning
processes are opaque, they are prone to &apos;hallucinate&apos; facts, and there are
concerns about their underlying biases. Letting models verbalize reasoning
steps as natural language, a technique known as chain-of-thought prompting, has
recently been proposed as a way to address some of these issues. Here we
present ThoughtSource, a meta-dataset and software library for chain-of-thought
(CoT) reasoning. The goal of ThoughtSource is to improve future artificial
intelligence systems by facilitating qualitative understanding of CoTs,
enabling empirical evaluations, and providing training data. This first release
of ThoughtSource integrates seven scientific/medical, three general-domain and
five math word question answering datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ott_S/0/1/0/all/0/1&quot;&gt;Simon Ott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebenstreit_K/0/1/0/all/0/1&quot;&gt;Konstantin Hebenstreit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lievin_V/0/1/0/all/0/1&quot;&gt;Valentin Li&amp;#xe9;vin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hother_C/0/1/0/all/0/1&quot;&gt;Christoffer Egeberg Hother&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradi_M/0/1/0/all/0/1&quot;&gt;Milad Moradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayrhauser_M/0/1/0/all/0/1&quot;&gt;Maximilian Mayrhauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Praas_R/0/1/0/all/0/1&quot;&gt;Robert Praas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winther_O/0/1/0/all/0/1&quot;&gt;Ole Winther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samwald_M/0/1/0/all/0/1&quot;&gt;Matthias Samwald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01198">
<title>Causal Lifting and Link Prediction. (arXiv:2302.01198v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01198</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing causal models for link prediction assume an underlying set of
inherent node factors -- an innate characteristic defined at the node&apos;s birth
-- that governs the causal evolution of links in the graph. In some causal
tasks, however, link formation is path-dependent: The outcome of link
interventions depends on existing links. Unfortunately, these existing causal
methods are not designed for path-dependent link formation, as the cascading
functional dependencies between links (arising from path dependence) are either
unidentifiable or require an impractical number of control variables. To
overcome this, we develop the first causal model capable of dealing with path
dependencies in link prediction. In this work we introduce the concept of
causal lifting, an invariance in causal models of independent interest that, on
graphs, allows the identification of causal link prediction queries using
limited interventional data. Further, we show how structural pairwise
embeddings exhibit lower bias and correctly represent the task&apos;s causal
structure, as opposed to existing node embeddings, e.g., graph neural network
node embeddings and matrix factorization. Finally, we validate our theoretical
findings on three scenarios for causal link prediction tasks: knowledge base
completion, covariance matrix estimation and consumer-product recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotta_L/0/1/0/all/0/1&quot;&gt;Leonardo Cotta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1&quot;&gt;Beatrice Bevilacqua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1&quot;&gt;Bruno Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08720">
<title>Algorithmic Hallucinations of Near-Surface Winds: Statistical Downscaling with Generative Adversarial Networks to Convection-Permitting Scales. (arXiv:2302.08720v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08720</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the application of emerging machine learning methods from
image super-resolution (SR) to the task of statistical downscaling. We
specifically focus on convolutional neural network-based Generative Adversarial
Networks (GANs). Our GANs are conditioned on low-resolution (LR) inputs to
generate high-resolution (HR) surface winds emulating Weather Research and
Forecasting (WRF) model simulations over North America. Unlike traditional SR
models, where LR inputs are idealized coarsened versions of the HR images, WRF
emulation involves using non-idealized LR and HR pairs resulting in
shared-scale mismatches due to internal variability. Our study builds upon
current SR-based statistical downscaling by experimenting with a novel
frequency-separation (FS) approach from the computer vision field. To assess
the skill of SR models, we carefully select evaluation metrics, and focus on
performance measures based on spatial power spectra. Our analyses reveal how
GAN configurations influence spatial structures in the generated fields,
particularly biases in spatial variability spectra. Using power spectra to
evaluate the FS experiments reveals that successful applications of FS in
computer vision do not translate to climate fields. However, the FS experiments
demonstrate the sensitivity of power spectra to a commonly used GAN-based SR
objective function, which helps interpret and understand its role in
determining spatial structures. This result motivates the development of a
novel partial frequency-separation scheme as a promising configuration option.
We also quantify the influence on GAN performance of non-idealized LR fields
resulting from internal variability. Furthermore, we conduct a spectra-based
feature-importance experiment allowing us to explore the dependence of the
spatial structure of generated fields on different physically relevant LR
covariates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Annau_N/0/1/0/all/0/1&quot;&gt;Nicolaas J. Annau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cannon_A/0/1/0/all/0/1&quot;&gt;Alex J. Cannon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Monahan_A/0/1/0/all/0/1&quot;&gt;Adam H. Monahan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12012">
<title>Empirical analysis of Different Dimensionality Reduction and classification Techniques for Epileptic Seizure detection. (arXiv:2302.12012v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12012</link>
<description rdf:parseType="Literal">&lt;p&gt;An Electroencephalogram (EEG) is a non-invasive exam that records the
electrical activity of the brain. This exam is used to help diagnose conditions
such as different brain problems. EEG signals are taken for the purpose of
epilepsy detection and with Discrete Wavelet Transform (DWT) and machine
learning classifier, they perform epilepsy detection. In Epilepsy seizure
detection, mainly machine learning classifiers and statistical features are
used. The hidden information in the EEG signal is useful for detecting diseases
affecting the brain. Sometimes it is very difficult to identify the minimum
changes in the EEG in the time and frequency domains purpose. The DWT can give
a good decomposition of the signals in different frequency bands and feature
extraction. We use the tri-dimensionality reduction algorithm.; Principal
Component Analysis (PCA), Independent Component Analysis (ICA), and Linear
Discriminant Analysis (LDA). Finally, features are selected by using a fusion
rule and at the last step three different classifiers Support Vector Machine
(SVM), Naive Bayes (NB) and K-Nearest-Neighbor(KNN) have been used individually
for the classification. The proposed framework is tested on the Bonn dataset
and the simulation results provide the accuracy for the combination of LDA and
SVM 89.17%, LDA and KNN 80.42%, PCA and NB 89.92%, PCA and SVM 85.58%, PCA and
KNN 80.42%, ICA and NB 82.33%, ICA and SVM 90.42%, and ICA and KNN 90%, LDA and
NB 100%, accuracy. It shows the sensitivity, specificity, accuracy, Precision,
and Recall of 100%, 100%, 100%, 100%, and 100%. This combination of LDA with NB
method provides the accuracy of 100% outperforming all existing methods. The
results prove the effectiveness of this model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guharoy_R/0/1/0/all/0/1&quot;&gt;Rabel Guharoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jana_N/0/1/0/all/0/1&quot;&gt;Nanda Dulal Jana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biswas_S/0/1/0/all/0/1&quot;&gt;Suparna Biswas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_L/0/1/0/all/0/1&quot;&gt;Lalit Garg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06848">
<title>CAR-DESPOT: Causally-Informed Online POMDP Planning for Robots in Confounded Environments. (arXiv:2304.06848v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06848</link>
<description rdf:parseType="Literal">&lt;p&gt;Robots operating in real-world environments must reason about possible
outcomes of stochastic actions and make decisions based on partial observations
of the true world state. A major challenge for making accurate and robust
action predictions is the problem of confounding, which if left untreated can
lead to prediction errors. The partially observable Markov decision process
(POMDP) is a widely-used framework to model these stochastic and
partially-observable decision-making problems. However, due to a lack of
explicit causal semantics, POMDP planning methods are prone to confounding bias
and thus in the presence of unobserved confounders may produce underperforming
policies. This paper presents a novel causally-informed extension of &quot;anytime
regularized determinized sparse partially observable tree&quot; (AR-DESPOT), a
modern anytime online POMDP planner, using causal modelling and inference to
eliminate errors caused by unmeasured confounder variables. We further propose
a method to learn offline the partial parameterisation of the causal model for
planning, from ground truth model data. We evaluate our methods on a toy
problem with an unobserved confounder and show that the learned causal model is
highly accurate, while our planning method is more robust to confounding and
produces overall higher performing policies than AR-DESPOT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cannizzaro_R/0/1/0/all/0/1&quot;&gt;Ricardo Cannizzaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunze_L/0/1/0/all/0/1&quot;&gt;Lars Kunze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02109">
<title>Synergies Between Federated Learning and O-RAN: Towards an Elastic Virtualized Architecture for Multiple Distributed Machine Learning Services. (arXiv:2305.02109v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02109</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is the most popular distributed machine learning
technique. However, implementation of FL over modern wireless networks faces
key challenges caused by (i) dynamics of the network conditions and (ii) the
coexistence of multiple FL services/tasks and other network services in the
system, which are not jointly considered in prior works. Motivated by these
challenges, we introduce a generic FL paradigm over NextG networks, called
dynamic multi-service FL (DMS-FL). We identify three unexplored design
considerations in DMS-FL: (i) FL service operator accumulation, (ii) wireless
resource fragmentation, and (iii) signal strength fluctuations. We take the
first steps towards addressing these design considerations by proposing a novel
distributed ML architecture called elastic virtualized FL (EV-FL). EV-FL
unleashes the full potential of Open RAN (O-RAN) systems and introduces an
elastic resource provisioning methodology to execute FL services. It further
constitutes a multi-time-scale FL management system that introduces three
dimensions into existing FL architectures: (i) virtualization, (ii)
scalability, and (iii) elasticity. Through investigating EV-FL, we reveal a
series of open research directions for future work. We finally simulate EV-FL
to demonstrate its potential in saving wireless resources and increasing
fairness among FL services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdisarabshali_P/0/1/0/all/0/1&quot;&gt;Payam Abdisarabshali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Accurso_N/0/1/0/all/0/1&quot;&gt;Nicholas Accurso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malandra_F/0/1/0/all/0/1&quot;&gt;Filippo Malandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weifeng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseinalipour_S/0/1/0/all/0/1&quot;&gt;Seyyedali Hosseinalipour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08234">
<title>Introducing Tales of Tribute AI Competition. (arXiv:2305.08234v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08234</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new AI challenge, the Tales of Tribute AI Competition
(TOTAIC), based on a two-player deck-building card game released with the High
Isle chapter of The Elder Scrolls Online. Currently, there is no other AI
competition covering Collectible Card Games (CCG) genre, and there has never
been one that targets a deck-building game. Thus, apart from usual CCG-related
obstacles to overcome, like randomness, hidden information, and large branching
factor, the successful approach additionally requires long-term planning and
versatility. The game can be tackled with multiple approaches, including
classic adversarial search, single-player planning, and Neural Networks-based
algorithms. This paper introduces the competition framework, describes the
rules of the game, and presents the results of a tournament between sample AI
agents. The first edition of TOTAIC is hosted at the IEEE Conference on Games
2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowalski_J/0/1/0/all/0/1&quot;&gt;Jakub Kowalski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miernik_R/0/1/0/all/0/1&quot;&gt;Rados&amp;#x142;aw Miernik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Polak_K/0/1/0/all/0/1&quot;&gt;Katarzyna Polak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budzki_D/0/1/0/all/0/1&quot;&gt;Dominik Budzki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowalik_D/0/1/0/all/0/1&quot;&gt;Damian Kowalik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08890">
<title>Differential Convolutional Fuzzy Time Series Forecasting. (arXiv:2305.08890v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08890</link>
<description rdf:parseType="Literal">&lt;p&gt;Fuzzy time series forecasting (FTSF) is a typical forecasting method with
wide application. Traditional FTSF is regarded as an expert system which leads
to loss of the ability to recognize undefined features. The mentioned is the
main reason for poor forecasting with FTSF. To solve the problem, the proposed
model Differential Fuzzy Convolutional Neural Network (DFCNN) utilizes a
convolution neural network to re-implement FTSF with learnable ability. DFCNN
is capable of recognizing potential information and improving forecasting
accuracy. Thanks to the learnable ability of the neural network, the length of
fuzzy rules established in FTSF is expended to an arbitrary length that the
expert is not able to handle by the expert system. At the same time, FTSF
usually cannot achieve satisfactory performance of non-stationary time series
due to the trend of non-stationary time series. The trend of non-stationary
time series causes the fuzzy set established by FTSF to be invalid and causes
the forecasting to fail. DFCNN utilizes the Difference algorithm to weaken the
non-stationary of time series so that DFCNN can forecast the non-stationary
time series with a low error that FTSF cannot forecast in satisfactory
performance. After the mass of experiments, DFCNN has an excellent prediction
effect, which is ahead of the existing FTSF and common time series forecasting
algorithms. Finally, DFCNN provides further ideas for improving FTSF and holds
continued research value.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_T/0/1/0/all/0/1&quot;&gt;Tianxiang Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuanpeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09160">
<title>SUG: Single-dataset Unified Generalization for 3D Point Cloud Classification. (arXiv:2305.09160v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09160</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Domain Generalization (DG) problem has been fast-growing in the 2D
image tasks, its exploration on 3D point cloud data is still insufficient and
challenged by more complex and uncertain cross-domain variances with uneven
inter-class modality distribution. In this paper, different from previous 2D DG
works, we focus on the 3D DG problem and propose a Single-dataset Unified
Generalization (SUG) framework that only leverages a single source dataset to
alleviate the unforeseen domain differences faced by a well-trained source
model. Specifically, we first design a Multi-grained Sub-domain Alignment (MSA)
method, which can constrain the learned representations to be domain-agnostic
and discriminative, by performing a multi-grained feature alignment process
between the splitted sub-domains from the single source dataset. Then, a
Sample-level Domain-aware Attention (SDA) strategy is presented, which can
selectively enhance easy-to-adapt samples from different sub-domains according
to the sample-level inter-domain distance to avoid the negative transfer.
Experiments demonstrate that our SUG can boost the generalization ability for
unseen target domains, even outperforming the existing unsupervised domain
adaptation methods that have to access extensive target domain data. Our code
is available at https://github.com/SiyuanHuang95/SUG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Siyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12485">
<title>A Confidence-based Partial Label Learning Model for Crowd-Annotated Named Entity Recognition. (arXiv:2305.12485v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12485</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing models for named entity recognition (NER) are mainly based on
large-scale labeled datasets, which always obtain using crowdsourcing. However,
it is hard to obtain a unified and correct label via majority voting from
multiple annotators for NER due to the large labeling space and complexity of
this task. To address this problem, we aim to utilize the original
multi-annotator labels directly. Particularly, we propose a Confidence-based
Partial Label Learning (CPLL) method to integrate the prior confidence (given
by annotators) and posterior confidences (learned by models) for
crowd-annotated NER. This model learns a token- and content-dependent
confidence via an Expectation-Maximization (EM) algorithm by minimizing
empirical risk. The true posterior estimator and confidence estimator perform
iteratively to update the true posterior and confidence respectively. We
conduct extensive experimental results on both real-world and synthetic
datasets, which show that our model can improve performance effectively
compared with strong baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_L/0/1/0/all/0/1&quot;&gt;Limao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qunxi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuanbin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15299">
<title>Science in the Era of ChatGPT, Large Language Models and Generative AI: Challenges for Research Ethics and How to Respond. (arXiv:2305.15299v3 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15299</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models of artificial intelligence (AI), such as ChatGPT, find
remarkable but controversial applicability in science and research. This paper
reviews epistemological challenges, ethical and integrity risks in science
conduct in the advent of generative AI. This is with the aim to lay new timely
foundations for a high-quality research ethics review. The role of AI language
models as a research instrument and subject is scrutinized along with ethical
implications for scientists, participants and reviewers. New emerging practices
for research ethics review are discussed, concluding with ten recommendations
that shape a response for a more responsible research conduct in the era of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pournaras_E/0/1/0/all/0/1&quot;&gt;Evangelos Pournaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19472">
<title>PlaSma: Making Small Language Models Better Procedural Knowledge Models for (Counterfactual) Planning. (arXiv:2305.19472v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19472</link>
<description rdf:parseType="Literal">&lt;p&gt;Procedural planning, which entails decomposing a high-level goal into a
sequence of temporally ordered steps, is an important yet intricate task for
machines. It involves integrating common-sense knowledge to reason about
complex contextualized situations that are often counterfactual, e.g.
&quot;scheduling a doctor&apos;s appointment without a phone&quot;. While current approaches
show encouraging results using large language models (LLMs), they are hindered
by drawbacks such as costly API calls and reproducibility issues. In this
paper, we advocate planning using smaller language models. We present PlaSma, a
novel two-pronged approach to endow small language models with procedural
knowledge and (counterfactual) planning capabilities. More concretely, we
develop symbolic procedural knowledge distillation to enhance the implicit
knowledge in small language models and an inference-time algorithm to
facilitate more structured and accurate reasoning. In addition, we introduce a
novel task, Counterfactual Planning, that requires a revision of a plan to cope
with a counterfactual situation. In both the original and counterfactual
setting, we show that orders-of-magnitude smaller models (770M-11B parameters)
can compete and often surpass their larger teacher models&apos; capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1&quot;&gt;Faeze Brahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1&quot;&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyatkin_V/0/1/0/all/0/1&quot;&gt;Valentina Pyatkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jena D. Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Lorraine Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arai_H/0/1/0/all/0/1&quot;&gt;Hirona J. Arai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanyal_S/0/1/0/all/0/1&quot;&gt;Soumya Sanyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaguchi_K/0/1/0/all/0/1&quot;&gt;Keisuke Sakaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00017">
<title>Towards Explainable and Language-Agnostic LLMs: Symbolic Reverse Engineering of Language at Scale. (arXiv:2306.00017v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00017</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have achieved a milestone that undenia-bly
changed many held beliefs in artificial intelligence (AI). However, there
remains many limitations of these LLMs when it comes to true language
understanding, limitations that are a byproduct of the under-lying architecture
of deep neural networks. Moreover, and due to their subsymbolic nature,
whatever knowledge these models acquire about how language works will always be
buried in billions of microfeatures (weights), none of which is meaningful on
its own, making such models hopelessly unexplainable. To address these
limitations, we suggest com-bining the strength of symbolic representations
with what we believe to be the key to the success of LLMs, namely a successful
bottom-up re-verse engineering of language at scale. As such we argue for a
bottom-up reverse engineering of language in a symbolic setting. Hints on what
this project amounts to have been suggested by several authors, and we discuss
in some detail here how this project could be accomplished.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saba_W/0/1/0/all/0/1&quot;&gt;Walid S. Saba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03753">
<title>AI Art Curation: Re-imagining the city of Helsinki in occasion of its Biennial. (arXiv:2306.03753v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03753</link>
<description rdf:parseType="Literal">&lt;p&gt;Art curatorial practice is characterized by the presentation of an art
collection in a knowledgeable way. Machine processes are characterized by their
capacity to manage and analyze large amounts of data. This paper envisages AI
curation and audience interaction to explore the implications of contemporary
machine learning models for the curatorial world. This project was developed
for the occasion of the 2023 Helsinki Art Biennial, entitled New Directions May
Emerge. We use the Helsinki Art Museum (HAM) collection to re-imagine the city
of Helsinki through the lens of machine perception. We use visual-textual
models to place indoor artworks in public spaces, assigning fictional
coordinates based on similarity scores. We transform the space that each
artwork inhabits in the city by generating synthetic 360 art panoramas. We
guide the generation estimating depth values from 360 panoramas at each artwork
location, and machine-generated prompts of the artworks. The result of this
project is an AI curation that places the artworks in their imagined physical
space, blurring the lines of artwork, context, and machine perception. The work
is virtually presented as a web-based installation on this link
&lt;a href=&quot;http://newlyformedcity.net/&quot;&gt;this http URL&lt;/a&gt;, where users can navigate an alternative version of
the city while exploring and interacting with its cultural heritage at scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ballesteros_P/0/1/0/all/0/1&quot;&gt;Pepe Ballesteros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernasconi_V/0/1/0/all/0/1&quot;&gt;Valentine Bernasconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neri_I/0/1/0/all/0/1&quot;&gt;Iacopo Neri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_D/0/1/0/all/0/1&quot;&gt;Dario Negueruela del Castillo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05965">
<title>Automating Model Comparison in Factor Graphs. (arXiv:2306.05965v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05965</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian state and parameter estimation have been automated effectively in a
variety of probabilistic programming languages. The process of model comparison
on the other hand, which still requires error-prone and time-consuming manual
derivations, is often overlooked despite its importance. This paper efficiently
automates Bayesian model averaging, selection, and combination by message
passing on a Forney-style factor graph with a custom mixture node. Parameter
and state inference, and model comparison can then be executed simultaneously
using message passing with scale factors. This approach shortens the model
design cycle and allows for the straightforward extension to hierarchical and
temporal model priors to accommodate for modeling complicated time-varying
processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erp_B/0/1/0/all/0/1&quot;&gt;Bart van Erp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nuijten_W/0/1/0/all/0/1&quot;&gt;Wouter W. L. Nuijten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laar_T/0/1/0/all/0/1&quot;&gt;Thijs van de Laar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vries_B/0/1/0/all/0/1&quot;&gt;Bert de Vries&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08541">
<title>Fine-Tuned but Zero-Shot 3D Shape Sketch View Similarity and Retrieval. (arXiv:2306.08541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08541</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, encoders like ViT (vision transformer) and ResNet have been trained
on vast datasets and utilized as perceptual metrics for comparing sketches and
images, as well as multi-domain encoders in a zero-shot setting. However, there
has been limited effort to quantify the granularity of these encoders. Our work
addresses this gap by focusing on multi-modal 2D projections of individual 3D
instances. This task holds crucial implications for retrieval and sketch-based
modeling. We show that in a zero-shot setting, the more abstract the sketch,
the higher the likelihood of incorrect image matches. Even within the same
sketch domain, sketches of the same object drawn in different styles, for
example by distinct individuals, might not be accurately matched. One of the
key findings of our research is that meticulous fine-tuning on one class of 3D
shapes can lead to improved performance on other shape classes, reaching or
surpassing the accuracy of supervised methods. We compare and discuss several
fine-tuning strategies. Additionally, we delve deeply into how the scale of an
object in a sketch influences the similarity of features at different network
layers, helping us identify which network layers provide the most accurate
matching. Significantly, we discover that ViT and ResNet perform best when
dealing with similar object scales. We believe that our work will have a
significant impact on research in the sketch domain, providing insights and
guidance on how to adopt large pretrained models as perceptual losses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berardi_G/0/1/0/all/0/1&quot;&gt;Gianluca Berardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gryaditskaya_Y/0/1/0/all/0/1&quot;&gt;Yulia Gryaditskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16958">
<title>Identifiability of direct effects from summary causal graphs. (arXiv:2306.16958v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16958</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic structural causal models (SCMs) are a powerful framework for
reasoning in dynamic systems about direct effects which measure how a change in
one variable affects another variable while holding all other variables
constant. The causal relations in a dynamic structural causal model can be
qualitatively represented with a full-time causal graph. Assuming linearity and
causal sufficiency and given the full-time causal graph, the direct causal
effect is always identifiable and can be estimated from data by adjusting on
any set of variables given by the so-called single-door criterion. However, in
many application such a graph is not available for various reasons but
nevertheless experts have access to an abstraction of the full-time causal
graph which represents causal relations between time series while omitting
temporal information. This paper presents a complete identifiability result
which characterizes all cases for which the direct effect is graphically
identifiable from summary causal graphs and gives two sound finite adjustment
sets that can be used to estimate the direct effect whenever it is
identifiable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_S/0/1/0/all/0/1&quot;&gt;Simon Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assaad_C/0/1/0/all/0/1&quot;&gt;Charles K. Assaad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00866">
<title>Mining Clues from Incomplete Utterance: A Query-enhanced Network for Incomplete Utterance Rewriting. (arXiv:2307.00866v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00866</link>
<description rdf:parseType="Literal">&lt;p&gt;Incomplete utterance rewriting has recently raised wide attention. However,
previous works do not consider the semantic structural information between
incomplete utterance and rewritten utterance or model the semantic structure
implicitly and insufficiently. To address this problem, we propose a
QUEry-Enhanced Network (QUEEN). Firstly, our proposed query template explicitly
brings guided semantic structural knowledge between the incomplete utterance
and the rewritten utterance making model perceive where to refer back to or
recover omitted tokens. Then, we adopt a fast and effective edit operation
scoring network to model the relation between two tokens. Benefiting from
proposed query template and the well-designed edit operation scoring network,
QUEEN achieves state-of-the-art performance on several public datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_S/0/1/0/all/0/1&quot;&gt;Shuzheng Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shuang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_B/0/1/0/all/0/1&quot;&gt;Baobao Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04019">
<title>GP-guided MPPI for Efficient Navigation in Complex Unknown Cluttered Environments. (arXiv:2307.04019v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04019</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic navigation in unknown, cluttered environments with limited sensing
capabilities poses significant challenges in robotics. Local trajectory
optimization methods, such as Model Predictive Path Intergal (MPPI), are a
promising solution to this challenge. However, global guidance is required to
ensure effective navigation, especially when encountering challenging
environmental conditions or navigating beyond the planning horizon. This study
presents the GP-MPPI, an online learning-based control strategy that integrates
MPPI with a local perception model based on Sparse Gaussian Process (SGP). The
key idea is to leverage the learning capability of SGP to construct a variance
(uncertainty) surface, which enables the robot to learn about the navigable
space surrounding it, identify a set of suggested subgoals, and ultimately
recommend the optimal subgoal that minimizes a predefined cost function to the
local MPPI planner. Afterward, MPPI computes the optimal control sequence that
satisfies the robot and collision avoidance constraints. Such an approach
eliminates the necessity of a global map of the environment or an offline
training process. We validate the efficiency and robustness of our proposed
control strategy through both simulated and real-world experiments of 2D
autonomous navigation tasks in complex unknown environments, demonstrating its
superiority in guiding the robot safely towards its desired goal while avoiding
obstacles and escaping entrapment in local minima. The GPU implementation of
GP-MPPI, including the supplementary video, is available at
https://github.com/IhabMohamed/GP-MPPI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamed_I/0/1/0/all/0/1&quot;&gt;Ihab S. Mohamed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Mahmoud Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lantao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07515">
<title>Artificial intelligence is algorithmic mimicry: why artificial &quot;agents&quot; are not (and won&apos;t be) proper agents. (arXiv:2307.07515v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07515</link>
<description rdf:parseType="Literal">&lt;p&gt;What is the prospect of developing artificial general intelligence (AGI)? I
investigate this question by systematically comparing living and algorithmic
systems, with a special focus on the notion of &quot;agency.&quot; There are three
fundamental differences to consider: (1) Living systems are autopoietic, that
is, self-manufacturing, and therefore able to set their own intrinsic goals,
while algorithms exist in a computational environment with target functions
that are both provided by an external agent. (2) Living systems are embodied in
the sense that there is no separation between their symbolic and physical
aspects, while algorithms run on computational architectures that maximally
isolate software from hardware. (3) Living systems experience a large world, in
which most problems are ill-defined (and not all definable), while algorithms
exist in a small world, in which all problems are well-defined. These three
differences imply that living and algorithmic systems have very different
capabilities and limitations. In particular, it is extremely unlikely that true
AGI (beyond mere mimicry) can be developed in the current algorithmic framework
of AI research. Consequently, discussions about the proper development and
deployment of algorithmic tools should be shaped around the dangers and
opportunities of current narrow AI, not the extremely unlikely prospect of the
emergence of true agency in artificial systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_J/0/1/0/all/0/1&quot;&gt;Johannes Jaeger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09751">
<title>Information Retrieval Meets Large Language Models: A Strategic Report from Chinese IR Community. (arXiv:2307.09751v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09751</link>
<description rdf:parseType="Literal">&lt;p&gt;The research field of Information Retrieval (IR) has evolved significantly,
expanding beyond traditional search to meet diverse user information needs.
Recently, Large Language Models (LLMs) have demonstrated exceptional
capabilities in text understanding, generation, and knowledge inference,
opening up exciting avenues for IR research. LLMs not only facilitate
generative retrieval but also offer improved solutions for user understanding,
model evaluation, and user-system interactions. More importantly, the
synergistic relationship among IR models, LLMs, and humans forms a new
technical paradigm that is more powerful for information seeking. IR models
provide real-time and relevant information, LLMs contribute internal knowledge,
and humans play a central role of demanders and evaluators to the reliability
of information services. Nevertheless, significant challenges exist, including
computational costs, credibility concerns, domain-specific limitations, and
ethical considerations. To thoroughly discuss the transformative impact of LLMs
on IR research, the Chinese IR community conducted a strategic workshop in
April 2023, yielding valuable insights. This paper provides a summary of the
workshop&apos;s outcomes, including the rethinking of IR&apos;s core values, the mutual
enhancement of LLMs and IR, the proposal of a novel IR technical paradigm, and
open challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1&quot;&gt;Qingyao Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_T/0/1/0/all/0/1&quot;&gt;Ting Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhumin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shoubin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Fuli Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shen Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiafeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiangnan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yanyan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiqun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1&quot;&gt;Ziyu Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Weizhi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhaochun Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Pengjie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Le Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_X/0/1/0/all/0/1&quot;&gt;Xin Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaofei Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11411">
<title>Deep Directly-Trained Spiking Neural Networks for Object Detection. (arXiv:2307.11411v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11411</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking neural networks (SNNs) are brain-inspired energy-efficient models
that encode information in spatiotemporal dynamics. Recently, deep SNNs trained
directly have shown great success in achieving high performance on
classification tasks with very few time steps. However, how to design a
directly-trained SNN for the regression task of object detection still remains
a challenging problem. To address this problem, we propose EMS-YOLO, a novel
directly-trained SNN framework for object detection, which is the first trial
to train a deep SNN with surrogate gradients for object detection rather than
ANN-SNN conversion strategies. Specifically, we design a full-spike residual
block, EMS-ResNet, which can effectively extend the depth of the
directly-trained SNN with low power consumption. Furthermore, we theoretically
analyze and prove the EMS-ResNet could avoid gradient vanishing or exploding.
The results demonstrate that our approach outperforms the state-of-the-art
ANN-SNN conversion methods (at least 500 time steps) in extremely fewer time
steps (only 4 time steps). It is shown that our model could achieve comparable
performance to the ANN with the same architecture while consuming 5.83 times
less energy on the frame-based COCO Dataset and the event-based Gen1 Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qiaoyi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1&quot;&gt;Yuhong Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yifan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Shijie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Ziyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoqi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11892">
<title>On the Vulnerability of Fairness Constrained Learning to Malicious Noise. (arXiv:2307.11892v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11892</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the vulnerability of fairness-constrained learning to small
amounts of malicious noise in the training data. Konstantinov and Lampert
(2021) initiated the study of this question and presented negative results
showing there exist data distributions where for several fairness constraints,
any proper learner will exhibit high vulnerability when group sizes are
imbalanced. Here, we present a more optimistic view, showing that if we allow
randomized classifiers, then the landscape is much more nuanced. For example,
for Demographic Parity we show we can incur only a $\Theta(\alpha)$ loss in
accuracy, where $\alpha$ is the malicious noise rate, matching the best
possible even without fairness constraints. For Equal Opportunity, we show we
can incur an $O(\sqrt{\alpha})$ loss, and give a matching
$\Omega(\sqrt{\alpha})$lower bound. In contrast, Konstantinov and Lampert
(2021) showed for proper learners the loss in accuracy for both notions is
$\Omega(1)$. The key technical novelty of our work is how randomization can
bypass simple &quot;tricks&quot; an adversary can use to amplify his power. We also
consider additional fairness notions including Equalized Odds and Calibration.
For these fairness notions, the excess accuracy clusters into three natural
regimes $O(\alpha)$,$O(\sqrt{\alpha})$ and $O(1)$. These results provide a more
fine-grained view of the sensitivity of fairness-constrained learning to
adversarial noise in training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blum_A/0/1/0/all/0/1&quot;&gt;Avrim Blum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okoroafor_P/0/1/0/all/0/1&quot;&gt;Princewill Okoroafor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stangl_K/0/1/0/all/0/1&quot;&gt;Kevin Stangl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13365">
<title>Empower Your Model with Longer and Better Context Comprehension. (arXiv:2307.13365v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13365</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, with the emergence of numerous Large Language Models (LLMs), the
implementation of AI has entered a new era. Irrespective of these models&apos; own
capacity and structure, there is a growing demand for LLMs to possess enhanced
comprehension of longer and more complex contexts with relatively smaller
sizes. Models often encounter an upper limit when processing sequences of
sentences that extend beyond their comprehension capacity and result in
off-topic or even chaotic responses. While several recent works attempt to
address this issue in various ways, they rarely focus on &quot;why models are unable
to compensate or strengthen their capabilities on their own&quot;. In this paper, we
thoroughly investigate the nature of information transfer within LLMs and
propose a novel technique called Attention Transition. This technique empowers
models to achieve longer and better context comprehension with minimal
additional training or impact on generation fluency. Our experiments are
conducted on the challenging XSum dataset using LLaMa-7b model with context
token length ranging from 800 to 1900. Results demonstrate that we achieve
substantial improvements compared with the original generation results
evaluated by GPT4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yifei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Longhua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13494">
<title>Duet: efficient and scalable hybriD neUral rElation undersTanding. (arXiv:2307.13494v3 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13494</link>
<description rdf:parseType="Literal">&lt;p&gt;Learned cardinality estimation methods have achieved high precision compared
to traditional methods. Among learned methods, query-driven approaches face the
data and workload drift problem for a long time. Although both query-driven and
hybrid methods are proposed to avoid this problem, even the state-of-art of
them suffer from high training and estimation costs, limited scalability,
instability, and long-tailed distribution problem on high cardinality and high
dimensional tables, which seriously affects the practical application of
learned cardinality estimators. In this paper, we prove that most of these
problems are directly caused by the widely used progressive sampling. We solve
this problem by introducing predicates into the autoregressive model and
propose Duet, a stable, efficient, and scalable hybrid method to estimate
cardinality directly without sampling or any non-differentiable process, which
can not only reduces the inference complexity from $O(n)$ to $O(1)$ compared to
Naru and UAE but also achieve higher accuracy on high cardinality and high
dimensional tables. Experimental results show that Duet can achieve all the
design goals above and be much more practical and even has a lower inference
cost on CPU than that of most learned methods on GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yabin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1&quot;&gt;Chang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Donghua Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13709">
<title>Deep Bradley-Terry Rating: Estimate Properties Without Metric of Unseen Items. (arXiv:2307.13709v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13709</link>
<description rdf:parseType="Literal">&lt;p&gt;Many properties in the real world, such as desirability or strength in
competitive environment, can&apos;t be directly observed, which makes them difficult
to evaluate. To deal with this challenging problem, prior works have primarily
focused on estimating those properties of known items, especially the strength
of sports players, only of those who appears in paired comparison dataset. In
this paper, we introduce Deep Bradley-Terry Rating (DBTR), a novel ML framework
to evaluate any properties of unknown items, not necessarily present in the
training data. Our method seamlessly integrates traditional Bradley-Terry model
with a neural network structure. We also generalizes this architecture further
for asymmetric environment with unfairness, which is much more common in real
world settings. In our experimental analysis, DBTR successfully learned desired
quantification of those properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_S/0/1/0/all/0/1&quot;&gt;Satoru Fujii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13813">
<title>How to Scale Your EMA. (arXiv:2307.13813v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13813</link>
<description rdf:parseType="Literal">&lt;p&gt;Preserving training dynamics across batch sizes is an important tool for
practical machine learning as it enables the trade-off between batch size and
wall-clock time. This trade-off is typically enabled by a scaling rule, for
example, in stochastic gradient descent, one should scale the learning rate
linearly with the batch size. Another important tool for practical machine
learning is the model Exponential Moving Average (EMA), which is a model copy
that does not receive gradient information, but instead follows its target
model with some momentum. This model EMA can improve the robustness and
generalization properties of supervised learning, stabilize pseudo-labeling,
and provide a learning signal for Self-Supervised Learning (SSL). Prior works
have treated the model EMA separately from optimization, leading to different
training dynamics across batch sizes and lower model performance. In this work,
we provide a scaling rule for optimization in the presence of model EMAs and
demonstrate its validity across a range of architectures, optimizers, and data
modalities. We also show the rule&apos;s validity where the model EMA contributes to
the optimization of the target model, enabling us to train EMA-based
pseudo-labeling and SSL methods at small and large batch sizes. For SSL, we
enable training of BYOL up to batch size 24,576 without sacrificing
performance, optimally a 6$\times$ wall-clock time reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Busbridge_D/0/1/0/all/0/1&quot;&gt;Dan Busbridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ramapuram_J/0/1/0/all/0/1&quot;&gt;Jason Ramapuram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ablin_P/0/1/0/all/0/1&quot;&gt;Pierre Ablin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Likhomanenko_T/0/1/0/all/0/1&quot;&gt;Tatiana Likhomanenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dhekane_E/0/1/0/all/0/1&quot;&gt;Eeshan Gunesh Dhekane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Suau_X/0/1/0/all/0/1&quot;&gt;Xavier Suau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Webb_R/0/1/0/all/0/1&quot;&gt;Russ Webb&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>