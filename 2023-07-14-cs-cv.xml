<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06343" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06505" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06625" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2005.04490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.13445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.09405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.02819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08249" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06065" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.06338">
<title>Denoising Simulated Low-Field MRI (70mT) using Denoising Autoencoders (DAE) and Cycle-Consistent Generative Adversarial Networks (Cycle-GAN). (arXiv:2307.06338v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.06338</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, a denoising Cycle-GAN (Cycle Consistent Generative Adversarial
Network) is implemented to yield high-field, high resolution, high
signal-to-noise ratio (SNR) Magnetic Resonance Imaging (MRI) images from
simulated low-field, low resolution, low SNR MRI images. Resampling and
additive Rician noise were used to simulate low-field MRI. Images were utilized
to train a Denoising Autoencoder (DAE) and a Cycle-GAN, with paired and
unpaired cases. Both networks were evaluated using SSIM and PSNR image quality
metrics. This work demonstrates the use of a generative deep learning model
that can outperform classical DAEs to improve low-field MRI images and does not
require image pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vega_F/0/1/0/all/0/1&quot;&gt;Fernando Vega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Addeh_A/0/1/0/all/0/1&quot;&gt;Abdoljalil Addeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+MacDonald_M/0/1/0/all/0/1&quot;&gt;M. Ethan MacDonald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06342">
<title>ConvNeXt-ChARM: ConvNeXt-based Transform for Efficient Neural Image Compression. (arXiv:2307.06342v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06342</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the last few years, neural image compression has gained wide attention
from research and industry, yielding promising end-to-end deep neural codecs
outperforming their conventional counterparts in rate-distortion performance.
Despite significant advancement, current methods, including attention-based
transform coding, still need to be improved in reducing the coding rate while
preserving the reconstruction fidelity, especially in non-homogeneous textured
image areas. Those models also require more parameters and a higher decoding
time. To tackle the above challenges, we propose ConvNeXt-ChARM, an efficient
ConvNeXt-based transform coding framework, paired with a compute-efficient
channel-wise auto-regressive prior to capturing both global and local contexts
from the hyper and quantized latent representations. The proposed architecture
can be optimized end-to-end to fully exploit the context information and
extract compact latent representation while reconstructing higher-quality
images. Experimental results on four widely-used datasets showed that
ConvNeXt-ChARM brings consistent and significant BD-rate (PSNR) reductions
estimated on average to 5.24% and 1.22% over the versatile video coding (VVC)
reference encoder (VTM-18.0) and the state-of-the-art learned image compression
method SwinT-ChARM, respectively. Moreover, we provide model scaling studies to
verify the computational efficiency of our approach and conduct several
objective and subjective analyses to bring to the fore the performance gap
between the next generation ConvNet, namely ConvNeXt, and Swin Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghorbel_A/0/1/0/all/0/1&quot;&gt;Ahmed Ghorbel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamidouche_W/0/1/0/all/0/1&quot;&gt;Wassim Hamidouche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morin_L/0/1/0/all/0/1&quot;&gt;Luce Morin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06343">
<title>Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning. (arXiv:2307.06343v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.06343</link>
<description rdf:parseType="Literal">&lt;p&gt;In X-ray Computed Tomography (CT), projections from many angles are acquired
and used for 3D reconstruction. To make CT suitable for in-line quality
control, reducing the number of angles while maintaining reconstruction quality
is necessary. Sparse-angle tomography is a popular approach for obtaining 3D
reconstructions from limited data. To optimize its performance, one can adapt
scan angles sequentially to select the most informative angles for each scanned
object. Mathematically, this corresponds to solving and optimal experimental
design (OED) problem. OED problems are high-dimensional, non-convex, bi-level
optimization problems that cannot be solved online, i.e., during the scan. To
address these challenges, we pose the OED problem as a partially observable
Markov decision process in a Bayesian framework, and solve it through deep
reinforcement learning. The approach learns efficient non-greedy policies to
solve a given class of OED problems through extensive offline training rather
than solving a given OED problem directly via numerical optimization. As such,
the trained policy can successfully find the most informative scan angles
online. We use a policy training method based on the Actor-Critic approach and
evaluate its performance on 2D tomography with synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lucka_F/0/1/0/all/0/1&quot;&gt;Felix Lucka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leeuwen_T/0/1/0/all/0/1&quot;&gt;Tristan van Leeuwen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06344">
<title>The Whole Pathological Slide Classification via Weakly Supervised Learning. (arXiv:2307.06344v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.06344</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to its superior efficiency in utilizing annotations and addressing
gigapixel-sized images, multiple instance learning (MIL) has shown great
promise as a framework for whole slide image (WSI) classification in digital
pathology diagnosis. However, existing methods tend to focus on advanced
aggregators with different structures, often overlooking the intrinsic features
of H\&amp;amp;E pathological slides. To address this limitation, we introduced two
pathological priors: nuclear heterogeneity of diseased cells and spatial
correlation of pathological tiles. Leveraging the former, we proposed a data
augmentation method that utilizes stain separation during extractor training
via a contrastive learning strategy to obtain instance-level representations.
We then described the spatial relationships between the tiles using an
adjacency matrix. By integrating these two views, we designed a multi-instance
framework for analyzing H\&amp;amp;E-stained tissue images based on pathological
inductive bias, encompassing feature extraction, filtering, and aggregation.
Extensive experiments on the Camelyon16 breast dataset and TCGA-NSCLC Lung
dataset demonstrate that our proposed framework can effectively handle tasks
related to cancer detection and differentiation of subtypes, outperforming
state-of-the-art medical image classification methods based on MIL. The code
will be released later.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qiehe Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiawen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Junru Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Guan_T/0/1/0/all/0/1&quot;&gt;Tian Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yonghong He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06350">
<title>T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation. (arXiv:2307.06350v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06350</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the stunning ability to generate high-quality images by recent
text-to-image models, current approaches often struggle to effectively compose
objects with different attributes and relationships into a complex and coherent
scene. We propose T2I-CompBench, a comprehensive benchmark for open-world
compositional text-to-image generation, consisting of 6,000 compositional text
prompts from 3 categories (attribute binding, object relationships, and complex
compositions) and 6 sub-categories (color binding, shape binding, texture
binding, spatial relationships, non-spatial relationships, and complex
compositions). We further propose several evaluation metrics specifically
designed to evaluate compositional text-to-image generation. We introduce a new
approach, Generative mOdel fine-tuning with Reward-driven Sample selection
(GORS), to boost the compositional text-to-image generation abilities of
pretrained text-to-image models. Extensive experiments and evaluations are
conducted to benchmark previous methods on T2I-CompBench, and to validate the
effectiveness of our proposed evaluation metrics and GORS approach. Project
page is available at https://karine-h.github.io/T2I-CompBench/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaiyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Kaiyue Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06385">
<title>Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06385</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-Visual Event Localization (AVEL) is the task of temporally localizing
and classifying \emph{audio-visual events}, i.e., events simultaneously visible
and audible in a video. In this paper, we solve AVEL in a weakly-supervised
setting, where only video-level event labels (their presence/absence, but not
their locations in time) are available as supervision for training. Our idea is
to use a base model to estimate labels on the training data at a finer temporal
resolution than at the video level and re-train the model with these labels.
I.e., we determine the subset of labels for each \emph{slice} of frames in a
training video by (i) replacing the frames outside the slice with those from a
second video having no overlap in video-level labels, and (ii) feeding this
synthetic video into the base model to extract labels for just the slice in
question. To handle the out-of-distribution nature of our synthetic videos, we
propose an auxiliary objective for the base model that induces more reliable
predictions of the localized event labels as desired. Our three-stage pipeline
outperforms several existing AVEL methods with no architectural changes and
improves performance on a related weakly-supervised task as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_K/0/1/0/all/0/1&quot;&gt;Kalyan Ramakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06392">
<title>Deep learning-based Segmentation of Rabbit fetal skull with limited and sub-optimal annotations. (arXiv:2307.06392v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.06392</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a deep learning-based method to segment the
skeletal structures in the micro-CT images of Dutch-Belted rabbit fetuses which
can assist in the assessment of drug-induced skeletal abnormalities as a
required study in developmental and reproductive toxicology (DART). Our
strategy leverages sub-optimal segmentation labels of 22 skull bones from 26
micro-CT volumes and maps them to 250 unlabeled volumes on which a deep
CNN-based segmentation model is trained. In the experiments, our model was able
to achieve an average Dice Similarity Coefficient (DSC) of 0.89 across all
bones on the testing set, and 14 out of the 26 skull bones reached average DSC
&amp;gt;0.93. Our next steps are segmenting the whole body followed by developing a
model to classify abnormalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Soans_R/0/1/0/all/0/1&quot;&gt;Rajath Soans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gleason_A/0/1/0/all/0/1&quot;&gt;Alexa Gleason&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shah_T/0/1/0/all/0/1&quot;&gt;Tosha Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Miller_C/0/1/0/all/0/1&quot;&gt;Corey Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Robinson_B/0/1/0/all/0/1&quot;&gt;Barbara Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Brannen_K/0/1/0/all/0/1&quot;&gt;Kimberly Brannen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Antong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06396">
<title>Introduction to Facial Micro Expressions Analysis Using Color and Depth Images: A Matlab Coding Approach (Second Edition, 2023). (arXiv:2307.06396v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06396</link>
<description rdf:parseType="Literal">&lt;p&gt;The book attempts to introduce a gentle introduction to the field of Facial
Micro Expressions Recognition (FMER) using Color and Depth images, with the aid
of MATLAB programming environment. FMER is a subset of image processing and it
is a multidisciplinary topic to analysis. So, it requires familiarity with
other topics of Artifactual Intelligence (AI) such as machine learning, digital
image processing, psychology and more. So, it is a great opportunity to write a
book which covers all of these topics for beginner to professional readers in
the field of AI and even without having background of AI. Our goal is to
provide a standalone introduction in the field of MFER analysis in the form of
theorical descriptions for readers with no background in image processing with
reproducible Matlab practical examples. Also, we describe any basic definitions
for FMER analysis and MATLAB library which is used in the text, that helps
final reader to apply the experiments in the real-world applications. We
believe that this book is suitable for students, researchers, and professionals
alike, who need to develop practical skills, along with a basic understanding
of the field. We expect that, after reading this book, the reader feels
comfortable with different key stages such as color and depth image processing,
color and depth image representation, classification, machine learning, facial
micro-expressions recognition, feature extraction and dimensionality reduction.
The book attempts to introduce a gentle introduction to the field of Facial
Micro Expressions Recognition (FMER) using Color and Depth images, with the aid
of MATLAB programming environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Seyed Muhammad Hossein Mousavi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06420">
<title>RaBiT: An Efficient Transformer using Bidirectional Feature Pyramid Network with Reverse Attention for Colon Polyp Segmentation. (arXiv:2307.06420v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06420</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic and accurate segmentation of colon polyps is essential for early
diagnosis of colorectal cancer. Advanced deep learning models have shown
promising results in polyp segmentation. However, they still have limitations
in representing multi-scale features and generalization capability. To address
these issues, this paper introduces RaBiT, an encoder-decoder model that
incorporates a lightweight Transformer-based architecture in the encoder to
model multiple-level global semantic relationships. The decoder consists of
several bidirectional feature pyramid layers with reverse attention modules to
better fuse feature maps at various levels and incrementally refine polyp
boundaries. We also propose ideas to lighten the reverse attention module and
make it more suitable for multi-class segmentation. Extensive experiments on
several benchmark datasets show that our method outperforms existing methods
across all datasets while maintaining low computational complexity. Moreover,
our method demonstrates high generalization capability in cross-dataset
experiments, even when the training and test sets have different
characteristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thuan_N/0/1/0/all/0/1&quot;&gt;Nguyen Hoang Thuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oanh_N/0/1/0/all/0/1&quot;&gt;Nguyen Thi Oanh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thuy_N/0/1/0/all/0/1&quot;&gt;Nguyen Thi Thuy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perry_S/0/1/0/all/0/1&quot;&gt;Stuart Perry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_D/0/1/0/all/0/1&quot;&gt;Dinh Viet Sang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06443">
<title>Efficient Convolution and Transformer-Based Network for Video Frame Interpolation. (arXiv:2307.06443v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06443</link>
<description rdf:parseType="Literal">&lt;p&gt;Video frame interpolation is an increasingly important research task with
several key industrial applications in the video coding, broadcast and
production sectors. Recently, transformers have been introduced to the field
resulting in substantial performance gains. However, this comes at a cost of
greatly increased memory usage, training and inference time. In this paper, a
novel method integrating a transformer encoder and convolutional features is
proposed. This network reduces the memory burden by close to 50% and runs up to
four times faster during inference time compared to existing transformer-based
interpolation methods. A dual-encoder architecture is introduced which combines
the strength of convolutions in modelling local correlations with those of the
transformer for long-range dependencies. Quantitative evaluations are conducted
on various benchmarks with complex motion to showcase the robustness of the
proposed method, achieving competitive performance compared to state-of-the-art
interpolation networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalifeh_I/0/1/0/all/0/1&quot;&gt;Issa Khalifeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murn_L/0/1/0/all/0/1&quot;&gt;Luka Murn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mrak_M/0/1/0/all/0/1&quot;&gt;Marta Mrak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izquierdo_E/0/1/0/all/0/1&quot;&gt;Ebroul Izquierdo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06458">
<title>Discovering Image Usage Online: A Case Study With &quot;Flatten the Curve&apos;&apos;. (arXiv:2307.06458v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2307.06458</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the spread of images across the web helps us understand the
reuse of scientific visualizations and their relationship with the public. The
&quot;Flatten the Curve&quot; graphic was heavily used during the COVID-19 pandemic to
convey a complex concept in a simple form. It displays two curves comparing the
impact on case loads for medical facilities if the populace either adopts or
fails to adopt protective measures during a pandemic. We use five variants of
the &quot;Flatten the Curve&quot; image as a case study for viewing the spread of an
image online. To evaluate its spread, we leverage three information channels:
reverse image search engines, social media, and web archives. Reverse image
searches give us a current view into image reuse. Social media helps us
understand a variant&apos;s popularity over time. Web archives help us see when it
was preserved, highlighting a view of popularity for future researchers. Our
case study leverages document URLs can be used as a proxy for images when
studying the spread of images online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_S/0/1/0/all/0/1&quot;&gt;Shawn M. Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oyen_D/0/1/0/all/0/1&quot;&gt;Diane Oyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06472">
<title>Early Autism Diagnosis based on Path Signature and Siamese Unsupervised Feature Compressor. (arXiv:2307.06472v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06472</link>
<description rdf:parseType="Literal">&lt;p&gt;Autism Spectrum Disorder (ASD) has been emerging as a growing public health
threat. Early diagnosis of ASD is crucial for timely, effective intervention
and treatment. However, conventional diagnosis methods based on communications
and behavioral patterns are unreliable for children younger than 2 years of
age. Given evidences of neurodevelopmental abnormalities in ASD infants, we
resort to a novel deep learning-based method to extract key features from the
inherently scarce, class-imbalanced, and heterogeneous structural MR images for
early autism diagnosis. Specifically, we propose a Siamese verification
framework to extend the scarce data, and an unsupervised compressor to
alleviate data imbalance by extracting key features. We also proposed weight
constraints to cope with sample heterogeneity by giving different samples
different voting weights during validation, and we used Path Signature to
unravel meaningful developmental features from the two-time point data
longitudinally. Extensive experiments have shown that our method performed well
under practical scenarios, transcending existing machine learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xinyao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhengwang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Li Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06484">
<title>Single-Class Target-Specific Attack against Interpretable Deep Learning Systems. (arXiv:2307.06484v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06484</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel Single-class target-specific Adversarial
attack called SingleADV. The goal of SingleADV is to generate a universal
perturbation that deceives the target model into confusing a specific category
of objects with a target category while ensuring highly relevant and accurate
interpretations. The universal perturbation is stochastically and iteratively
optimized by minimizing the adversarial loss that is designed to consider both
the classifier and interpreter costs in targeted and non-targeted categories.
In this optimization framework, ruled by the first- and second-moment
estimations, the desired loss surface promotes high confidence and
interpretation score of adversarial samples. By avoiding unintended
misclassification of samples from other categories, SingleADV enables more
effective targeted attacks on interpretable deep learning systems in both
white-box and black-box scenarios. To evaluate the effectiveness of SingleADV,
we conduct experiments using four different model architectures (ResNet-50,
VGG-16, DenseNet-169, and Inception-V3) coupled with three interpretation
models (CAM, Grad, and MASK). Through extensive empirical evaluation, we
demonstrate that SingleADV effectively deceives the target deep learning models
and their associated interpreters under various conditions and settings. Our
experimental results show that the performance of SingleADV is effective, with
an average fooling ratio of 0.74 and an adversarial confidence level of 0.78 in
generating deceptive adversarial samples. Furthermore, we discuss several
countermeasures against SingleADV, including a transfer-based learning approach
and existing preprocessing defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdukhamidov_E/0/1/0/all/0/1&quot;&gt;Eldor Abdukhamidov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuhamad_M/0/1/0/all/0/1&quot;&gt;Mohammed Abuhamad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiruvathukal_G/0/1/0/all/0/1&quot;&gt;George K. Thiruvathukal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyoungshick Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuhmed_T/0/1/0/all/0/1&quot;&gt;Tamer Abuhmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06496">
<title>Microbial Genetic Algorithm-based Black-box Attack against Interpretable Deep Learning Systems. (arXiv:2307.06496v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06496</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are susceptible to adversarial samples in white and
black-box environments. Although previous studies have shown high attack
success rates, coupling DNN models with interpretation models could offer a
sense of security when a human expert is involved, who can identify whether a
given sample is benign or malicious. However, in white-box environments,
interpretable deep learning systems (IDLSes) have been shown to be vulnerable
to malicious manipulations. In black-box settings, as access to the components
of IDLSes is limited, it becomes more challenging for the adversary to fool the
system. In this work, we propose a Query-efficient Score-based black-box attack
against IDLSes, QuScore, which requires no knowledge of the target model and
its coupled interpretation model. QuScore is based on transfer-based and
score-based methods by employing an effective microbial genetic algorithm. Our
method is designed to reduce the number of queries necessary to carry out
successful attacks, resulting in a more efficient process. By continuously
refining the adversarial samples created based on feedback scores from the
IDLS, our approach effectively navigates the search space to identify
perturbations that can fool the system. We evaluate the attack&apos;s effectiveness
on four CNN models (Inception, ResNet, VGG, DenseNet) and two interpretation
models (CAM, Grad), using both ImageNet and CIFAR datasets. Our results show
that the proposed approach is query-efficient with a high attack success rate
that can reach between 95% and 100% and transferability with an average success
rate of 69% in the ImageNet and CIFAR datasets. Our attack method generates
adversarial examples with attribution maps that resemble benign samples. We
have also demonstrated that our attack is resilient against various
preprocessing defense techniques and can easily be transferred to different DNN
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdukhamidov_E/0/1/0/all/0/1&quot;&gt;Eldor Abdukhamidov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuhamad_M/0/1/0/all/0/1&quot;&gt;Mohammed Abuhamad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Simon S. Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_Tin_E/0/1/0/all/0/1&quot;&gt;Eric Chan-Tin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuhmed_T/0/1/0/all/0/1&quot;&gt;Tamer Abuhmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06500">
<title>On the ability of CNNs to extract color invariant intensity based features for image classification. (arXiv:2307.06500v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06500</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) have demonstrated remarkable success in
vision-related tasks. However, their susceptibility to failing when inputs
deviate from the training distribution is well-documented. Recent studies
suggest that CNNs exhibit a bias toward texture instead of object shape in
image classification tasks, and that background information may affect
predictions. This paper investigates the ability of CNNs to adapt to different
color distributions in an image while maintaining context and background. The
results of our experiments on modified MNIST and FashionMNIST data demonstrate
that changes in color can substantially affect classification accuracy. The
paper explores the effects of various regularization techniques on
generalization error across datasets and proposes a minor architectural
modification utilizing the dropout regularization in a novel way that enhances
model reliance on color-invariant intensity-based features for improved
classification accuracy. Overall, this work contributes to ongoing efforts to
understand the limitations and challenges of CNNs in image classification tasks
and offers potential solutions to enhance their performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elavarthi_P/0/1/0/all/0/1&quot;&gt;Pradyumna Elavarthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;James Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ralescu_A/0/1/0/all/0/1&quot;&gt;Anca Ralescu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06505">
<title>WaterScenes: A Multi-Task 4D Radar-Camera Fusion Dataset and Benchmark for Autonomous Driving on Water Surfaces. (arXiv:2307.06505v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06505</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving on water surfaces plays an essential role in executing
hazardous and time-consuming missions, such as maritime surveillance, survivors
rescue, environmental monitoring, hydrography mapping and waste cleaning. This
work presents WaterScenes, the first multi-task 4D radar-camera fusion dataset
for autonomous driving on water surfaces. Equipped with a 4D radar and a
monocular camera, our Unmanned Surface Vehicle (USV) proffers all-weather
solutions for discerning object-related information, including color, shape,
texture, range, velocity, azimuth, and elevation. Focusing on typical static
and dynamic objects on water surfaces, we label the camera images and radar
point clouds at pixel-level and point-level, respectively. In addition to basic
perception tasks, such as object detection, instance segmentation and semantic
segmentation, we also provide annotations for free-space segmentation and
waterline segmentation. Leveraging the multi-task and multi-modal data, we
conduct numerous experiments on the single modality of radar and camera, as
well as the fused modalities. Results demonstrate that 4D radar-camera fusion
can considerably enhance the robustness of perception on water surfaces,
especially in adverse lighting and weather conditions. WaterScenes dataset is
public on https://waterscenes.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shanliang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_R/0/1/0/all/0/1&quot;&gt;Runwei Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhaodong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yi Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zile Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yutao Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1&quot;&gt;Hyungjoon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_K/0/1/0/all/0/1&quot;&gt;Ka Lok Man&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06507">
<title>Improving Nonalcoholic Fatty Liver Disease Classification Performance With Latent Diffusion Models. (arXiv:2307.06507v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06507</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating deep learning with clinical expertise holds great potential for
addressing healthcare challenges and empowering medical professionals with
improved diagnostic tools. However, the need for annotated medical images is
often an obstacle to leveraging the full power of machine learning models. Our
research demonstrates that by combining synthetic images, generated using
diffusion models, with real images, we can enhance nonalcoholic fatty liver
disease (NAFLD) classification performance. We evaluate the quality of the
synthetic images by comparing two metrics: Inception Score (IS) and Fr\&apos;{e}chet
Inception Distance (FID), computed on diffusion-generated images and generative
adversarial networks (GANs)-generated images. Our results show superior
performance for the diffusion-generated images, with a maximum IS score of
$1.90$ compared to $1.67$ for GANs, and a minimum FID score of $69.45$ compared
to $99.53$ for GANs. Utilizing a partially frozen CNN backbone (EfficientNet
v1), our synthetic augmentation method achieves a maximum image-level ROC AUC
of $0.904$ on a NAFLD prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardy_R/0/1/0/all/0/1&quot;&gt;Romain Hardy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_C/0/1/0/all/0/1&quot;&gt;Cornelia Ilin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klepich_J/0/1/0/all/0/1&quot;&gt;Joe Klepich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_R/0/1/0/all/0/1&quot;&gt;Ryan Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_S/0/1/0/all/0/1&quot;&gt;Steve Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villareal_J/0/1/0/all/0/1&quot;&gt;Jericho Villareal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06526">
<title>AvatarFusion: Zero-shot Generation of Clothing-Decoupled 3D Avatars Using 2D Diffusion. (arXiv:2307.06526v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06526</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-trained vision-language models allow for the zero-shot
text-based generation of 3D avatars. The previous state-of-the-art method
utilized CLIP to supervise neural implicit models that reconstructed a human
body mesh. However, this approach has two limitations. Firstly, the lack of
avatar-specific models can cause facial distortion and unrealistic clothing in
the generated avatars. Secondly, CLIP only provides optimization direction for
the overall appearance, resulting in less impressive results. To address these
limitations, we propose AvatarFusion, the first framework to use a latent
diffusion model to provide pixel-level guidance for generating human-realistic
avatars while simultaneously segmenting clothing from the avatar&apos;s body.
AvatarFusion includes the first clothing-decoupled neural implicit avatar model
that employs a novel Dual Volume Rendering strategy to render the decoupled
skin and clothing sub-models in one space. We also introduce a novel
optimization method, called Pixel-Semantics Difference-Sampling (PS-DS), which
semantically separates the generation of body and clothes, and generates a
variety of clothing styles. Moreover, we establish the first benchmark for
zero-shot text-to-avatar generation. Our experimental results demonstrate that
our framework outperforms previous approaches, with significant improvements
observed in all metrics. Additionally, since our model is clothing-decoupled,
we can exchange the clothes of avatars. Code will be available on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shuo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jia Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06527">
<title>Free-Form Composition Networks for Egocentric Action Recognition. (arXiv:2307.06527v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06527</link>
<description rdf:parseType="Literal">&lt;p&gt;Egocentric action recognition is gaining significant attention in the field
of human action recognition. In this paper, we address data scarcity issue in
egocentric action recognition from a compositional generalization perspective.
To tackle this problem, we propose a free-form composition network (FFCN) that
can simultaneously learn disentangled verb, preposition, and noun
representations, and then use them to compose new samples in the feature space
for rare classes of action videos. First, we use a graph to capture the
spatial-temporal relations among different hand/object instances in each action
video. We thus decompose each action into a set of verb and preposition
spatial-temporal representations using the edge features in the graph. The
temporal decomposition extracts verb and preposition representations from
different video frames, while the spatial decomposition adaptively learns verb
and preposition representations from action-related instances in each frame.
With these spatial-temporal representations of verbs and prepositions, we can
compose new samples for those rare classes in a free-form manner, which is not
restricted to a rigid form of a verb and a noun. The proposed FFCN can directly
generate new training data samples for rare classes, hence significantly
improve action recognition performance. We evaluated our method on three
popular egocentric action recognition datasets, Something-Something V2, H2O,
and EPIC-KITCHENS-100, and the experimental results demonstrate the
effectiveness of the proposed method for handling data scarcity problems,
including long-tailed and few-shot egocentric action recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Baosheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yibing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dapeng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Liang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Haibin Ling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06528">
<title>Optimised Least Squares Approach for Accurate Rectangle Fitting. (arXiv:2307.06528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06528</link>
<description rdf:parseType="Literal">&lt;p&gt;This study introduces a novel and efficient least squares based method for
rectangle fitting, using a continuous fitness function that approximates a unit
square accurately. The proposed method is compared with the existing method in
the literature using both simulated data and real data. The real data is
derived from aerial photogrammetry point clouds of a rectangular building. The
simulated tests show that the proposed method performs better than the
reference method, reducing the root-mean-square error by about 93% and 14% for
clean datasets and noisy point clouds, respectively. The proposed method also
improves the fitting of the real dataset by about 81%, achieving centimetre
level accuracy. Furthermore, the test results show that the proposed method
converges in fewer than 10 iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Y/0/1/0/all/0/1&quot;&gt;Yiming Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06533">
<title>Domain-adaptive Person Re-identification without Cross-camera Paired Samples. (arXiv:2307.06533v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06533</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing person re-identification (re-ID) research mainly focuses on
pedestrian identity matching across cameras in adjacent areas. However, in
reality, it is inevitable to face the problem of pedestrian identity matching
across long-distance scenes. The cross-camera pedestrian samples collected from
long-distance scenes often have no positive samples. It is extremely
challenging to use cross-camera negative samples to achieve cross-region
pedestrian identity matching. Therefore, a novel domain-adaptive person re-ID
method that focuses on cross-camera consistent discriminative feature learning
under the supervision of unpaired samples is proposed. This method mainly
includes category synergy co-promotion module (CSCM) and cross-camera
consistent feature learning module (CCFLM). In CSCM, a task-specific feature
recombination (FRT) mechanism is proposed. This mechanism first groups features
according to their contributions to specific tasks. Then an interactive
promotion learning (IPL) scheme between feature groups is developed and
embedded in this mechanism to enhance feature discriminability. Since the
control parameters of the specific task model are reduced after division by
task, the generalization ability of the model is improved. In CCFLM,
instance-level feature distribution alignment and cross-camera identity
consistent learning methods are constructed. Therefore, the supervised model
training is achieved under the style supervision of the target domain by
exchanging styles between source-domain samples and target-domain samples, and
the challenges caused by the lack of cross-camera paired samples are solved by
utilizing cross-camera similar samples. In experiments, three challenging
datasets are used as target domains, and the effectiveness of the proposed
method is demonstrated through four experimental settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huafeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yanmei Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yafei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guanqiu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06542">
<title>An Image-Denoising Framework Fit for Quantum Annealing via QUBO and Restricted Boltzmann Machines. (arXiv:2307.06542v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.06542</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate a framework for binary image denoising via restricted
Boltzmann machines (RBMs) that introduces a denoising objective in quadratic
unconstrained binary optimization (QUBO) form and is well-suited for quantum
annealing. The denoising objective is attained by balancing the distribution
learned by a trained RBM with a penalty term for derivations from the noisy
image. We derive the statistically optimal choice of the penalty parameter
assuming the target distribution has been well-approximated, and further
suggest an empirically supported modification to make the method robust to that
idealistic assumption. We also show under additional assumptions that the
denoised images attained by our method are, in expectation, strictly closer to
the noise-free images than the noisy images are. While we frame the model as an
image denoising model, it can be applied to any binary data. As the QUBO
formulation is well-suited for implementation on quantum annealers, we test the
model on a D-Wave Advantage machine, and also test on data too large for
current quantum annealers by approximating QUBO solutions through classical
heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kerger_P/0/1/0/all/0/1&quot;&gt;Phillip Kerger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Miyazaki_R/0/1/0/all/0/1&quot;&gt;Ryoji Miyazaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06547">
<title>Full-resolution Lung Nodule Segmentation from Chest X-ray Images using Residual Encoder-Decoder Networks. (arXiv:2307.06547v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.06547</link>
<description rdf:parseType="Literal">&lt;p&gt;Lung cancer is the leading cause of cancer death and early diagnosis is
associated with a positive prognosis. Chest X-ray (CXR) provides an inexpensive
imaging mode for lung cancer diagnosis. Suspicious nodules are difficult to
distinguish from vascular and bone structures using CXR. Computer vision has
previously been proposed to assist human radiologists in this task, however,
leading studies use down-sampled images and computationally expensive methods
with unproven generalization. Instead, this study localizes lung nodules using
efficient encoder-decoder neural networks that process full resolution images
to avoid any signal loss resulting from down-sampling. Encoder-decoder networks
are trained and tested using the JSRT lung nodule dataset. The networks are
used to localize lung nodules from an independent external CXR dataset.
Sensitivity and false positive rates are measured using an automated framework
to eliminate any observer subjectivity. These experiments allow for the
determination of the optimal network depth, image resolution and pre-processing
pipeline for generalized lung nodule localization. We find that nodule
localization is influenced by subtlety, with more subtle nodules being detected
in earlier training epochs. Therefore, we propose a novel self-ensemble model
from three consecutive epochs centered on the validation optimum. This ensemble
achieved a sensitivity of 85% in 10-fold internal testing with false positives
of 8 per image. A sensitivity of 81% is achieved at a false positive rate of 6
following morphological false positive reduction. This result is comparable to
more computationally complex systems based on linear and spatial filtering, but
with a sub-second inference time that is faster than other methods. The
proposed algorithm achieved excellent generalization results against an
external dataset with sensitivity of 77% at a false positive rate of 7.6.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Horry_M/0/1/0/all/0/1&quot;&gt;Michael James Horry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Subrata Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pradhan_B/0/1/0/all/0/1&quot;&gt;Biswajeet Pradhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Paul_M/0/1/0/all/0/1&quot;&gt;Manoranjan Paul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barua_P/0/1/0/all/0/1&quot;&gt;Prabal Datta Barua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Acharya_U/0/1/0/all/0/1&quot;&gt;U. Rajendra Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jianlong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06548">
<title>Multi-objective Evolutionary Search of Variable-length Composite Semantic Perturbations. (arXiv:2307.06548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06548</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have proven to be vulnerable to adversarial attacks in
the form of adding specific perturbations on images to make wrong outputs.
Designing stronger adversarial attack methods can help more reliably evaluate
the robustness of DNN models. To release the harbor burden and improve the
attack performance, auto machine learning (AutoML) has recently emerged as one
successful technique to help automatically find the near-optimal adversarial
attack strategy. However, existing works about AutoML for adversarial attacks
only focus on $L_{\infty}$-norm-based perturbations. In fact, semantic
perturbations attract increasing attention due to their naturalnesses and
physical realizability. To bridge the gap between AutoML and semantic
adversarial attacks, we propose a novel method called multi-objective
evolutionary search of variable-length composite semantic perturbations
(MES-VCSP). Specifically, we construct the mathematical model of
variable-length composite semantic perturbations, which provides five
gradient-based semantic attack methods. The same type of perturbation in an
attack sequence is allowed to be performed multiple times. Besides, we
introduce the multi-objective evolutionary search consisting of NSGA-II and
neighborhood search to find near-optimal variable-length attack sequences.
Experimental results on CIFAR10 and ImageNet datasets show that compared with
existing methods, MES-VCSP can obtain adversarial examples with a higher attack
success rate, more naturalness, and less time cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suna_J/0/1/0/all/0/1&quot;&gt;Jialiang Suna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jianga_T/0/1/0/all/0/1&quot;&gt;Tingsong Jianga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chena_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Chena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06566">
<title>Regression-Oriented Knowledge Distillation for Lightweight Ship Orientation Angle Prediction with Optical Remote Sensing Images. (arXiv:2307.06566v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06566</link>
<description rdf:parseType="Literal">&lt;p&gt;Ship orientation angle prediction (SOAP) with optical remote sensing images
is an important image processing task, which often relies on deep convolutional
neural networks (CNNs) to make accurate predictions. This paper proposes a
novel framework to reduce the model sizes and computational costs of SOAP
models without harming prediction accuracy. First, a new SOAP model called
Mobile-SOAP is designed based on MobileNetV2, achieving state-of-the-art
prediction accuracy. Four tiny SOAP models are also created by replacing the
convolutional blocks in Mobile-SOAP with four small-scale networks,
respectively. Then, to transfer knowledge from Mobile-SOAP to four lightweight
models, we propose a novel knowledge distillation (KD) framework termed SOAP-KD
consisting of a novel feature-based guidance loss and an optimized synthetic
samples-based knowledge transfer mechanism. Lastly, extensive experiments on
the FGSC-23 dataset confirm the superiority of Mobile-SOAP over existing models
and also demonstrate the effectiveness of SOAP-KD in improving the prediction
performance of four specially designed tiny models. Notably, by using SOAP-KD,
the test mean absolute error of the ShuffleNetV2x1.0-based model is only 8%
higher than that of Mobile-SOAP, but its number of parameters and
multiply-accumulate operations (MACs) are respectively 61.6% and 60.8% less.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_P/0/1/0/all/0/1&quot;&gt;Peng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ru Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06569">
<title>A Study on Differentiable Logic and LLMs for EPIC-KITCHENS-100 Unsupervised Domain Adaptation Challenge for Action Recognition 2023. (arXiv:2307.06569v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06569</link>
<description rdf:parseType="Literal">&lt;p&gt;In this technical report, we present our findings from a study conducted on
the EPIC-KITCHENS-100 Unsupervised Domain Adaptation task for Action
Recognition. Our research focuses on the innovative application of a
differentiable logic loss in the training to leverage the co-occurrence
relations between verb and noun, as well as the pre-trained Large Language
Models (LLMs) to generate the logic rules for the adaptation to unseen action
labels. Specifically, the model&apos;s predictions are treated as the truth
assignment of a co-occurrence logic formula to compute the logic loss, which
measures the consistency between the predictions and the logic constraints. By
using the verb-noun co-occurrence matrix generated from the dataset, we observe
a moderate improvement in model performance compared to our baseline framework.
To further enhance the model&apos;s adaptability to novel action labels, we
experiment with rules generated using GPT-3.5, which leads to a slight decrease
in performance. These findings shed light on the potential and challenges of
incorporating differentiable logic and LLMs for knowledge extraction in
unsupervised domain adaptation for action recognition. Our final submission
(entitled `NS-LLM&apos;) achieved the first place in terms of top-1 action
recognition accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_F/0/1/0/all/0/1&quot;&gt;Fen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dongyun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Hehe Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_Y/0/1/0/all/0/1&quot;&gt;Yongkang Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Ying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kankanhalli_M/0/1/0/all/0/1&quot;&gt;Mohan Kankanhalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06577">
<title>RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation. (arXiv:2307.06577v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06577</link>
<description rdf:parseType="Literal">&lt;p&gt;Retinal vessel segmentation is generally grounded in image-based datasets
collected with bench-top devices. The static images naturally lose the dynamic
characteristics of retina fluctuation, resulting in diminished dataset
richness, and the usage of bench-top devices further restricts dataset
scalability due to its limited accessibility. Considering these limitations, we
introduce the first video-based retinal dataset by employing handheld devices
for data acquisition. The dataset comprises 635 smartphone-based fundus videos
collected from four different clinics, involving 415 patients from 50 to 75
years old. It delivers comprehensive and precise annotations of retinal
structures in both spatial and temporal dimensions, aiming to advance the
landscape of vasculature segmentation. Specifically, the dataset provides three
levels of spatial annotations: binary vessel masks for overall retinal
structure delineation, general vein-artery masks for distinguishing the vein
and artery, and fine-grained vein-artery masks for further characterizing the
granularities of each artery and vein. In addition, the dataset offers temporal
annotations that capture the vessel pulsation characteristics, assisting in
detecting ocular diseases that require fine-grained recognition of hemodynamic
fluctuation. In application, our dataset exhibits a significant domain shift
with respect to data captured by bench-top devices, thus posing great
challenges to existing methods. In the experiments, we provide evaluation
metrics and benchmark results on our dataset, reflecting both the potential and
challenges it offers for vessel segmentation tasks. We hope this challenging
dataset would significantly contribute to the development of eye disease
diagnosis and early prevention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_M/0/1/0/all/0/1&quot;&gt;MD Wahiduzzaman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_H/0/1/0/all/0/1&quot;&gt;Hongwei Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Heming Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coroneo_M/0/1/0/all/0/1&quot;&gt;Minas Theodore Coroneo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajati_F/0/1/0/all/0/1&quot;&gt;Farshid Hajati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shariflou_S/0/1/0/all/0/1&quot;&gt;Sahar Shariflou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalloniatis_M/0/1/0/all/0/1&quot;&gt;Michael Kalloniatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phu_J/0/1/0/all/0/1&quot;&gt;Jack Phu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agar_A/0/1/0/all/0/1&quot;&gt;Ashish Agar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golzan_M/0/1/0/all/0/1&quot;&gt;Mojtaba Golzan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06607">
<title>Image Denoising and the Generative Accumulation of Photons. (arXiv:2307.06607v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.06607</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a fresh perspective on shot noise corrupted images and noise
removal. By viewing image formation as the sequential accumulation of photons
on a detector grid, we show that a network trained to predict where the next
photon could arrive is in fact solving the minimum mean square error (MMSE)
denoising task. This new perspective allows us to make three contributions: We
present a new strategy for self-supervised denoising, We present a new method
for sampling from the posterior of possible solutions by iteratively sampling
and adding small numbers of photons to the image. We derive a full generative
model by starting this process from an empty canvas. We evaluate our method
quantitatively and qualitatively on 4 new fluorescence microscopy datasets,
which will be made available to the community. We find that it outperforms
supervised, self-supervised and unsupervised baselines or performs on-par.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Krull_A/0/1/0/all/0/1&quot;&gt;Alexander Krull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Basevi_H/0/1/0/all/0/1&quot;&gt;Hector Basevi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Salmon_B/0/1/0/all/0/1&quot;&gt;Benjamin Salmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeug_A/0/1/0/all/0/1&quot;&gt;Andre Zeug&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_F/0/1/0/all/0/1&quot;&gt;Franziska M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tonks_S/0/1/0/all/0/1&quot;&gt;Samuel Tonks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muppala_L/0/1/0/all/0/1&quot;&gt;Leela Muppala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leonardis_A/0/1/0/all/0/1&quot;&gt;Ales Leonardis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06614">
<title>Explainable 2D Vision Models for 3D Medical Data. (arXiv:2307.06614v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.06614</link>
<description rdf:parseType="Literal">&lt;p&gt;Training Artificial Intelligence (AI) models on three-dimensional image data
presents unique challenges compared to the two-dimensional case: Firstly, the
computational resources are significantly higher, and secondly, the
availability of large pretraining datasets is often limited, impeding training
success. In this study, we propose a simple approach of adapting 2D networks
with an intermediate feature representation for processing 3D volumes. Our
method involves sequentially applying these networks to slices of a 3D volume
from all orientations. Subsequently, a feature reduction module combines the
extracted slice features into a single representation, which is then used for
classification. We evaluate our approach on medical classification benchmarks
and a real-world clinical dataset, demonstrating comparable results to existing
methods. Furthermore, by employing attention pooling as a feature reduction
module we obtain weighted importance values for each slice during the forward
pass. We show that slices deemed important by our approach allow the inspection
of the basis of a model&apos;s prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ziller_A/0/1/0/all/0/1&quot;&gt;Alexander Ziller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guvenir_A/0/1/0/all/0/1&quot;&gt;Alp G&amp;#xfc;venir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Erdur_A/0/1/0/all/0/1&quot;&gt;Ayhan Can Erdur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mueller_T/0/1/0/all/0/1&quot;&gt;Tamara T. Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_P/0/1/0/all/0/1&quot;&gt;Philip M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jungmann_F/0/1/0/all/0/1&quot;&gt;Friederike Jungmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Brandt_J/0/1/0/all/0/1&quot;&gt;Johannes Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peeken_J/0/1/0/all/0/1&quot;&gt;Jan Peeken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kaissis_G/0/1/0/all/0/1&quot;&gt;Georgios Kaissis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06615">
<title>NLOS Dies Twice: Challenges and Solutions of V2X for Cooperative Perception. (arXiv:2307.06615v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2307.06615</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent multi-lidar sensor fusion between connected vehicles for
cooperative perception has recently been recognized as the best technique for
minimizing the blind zone of individual vehicular perception systems and
further enhancing the overall safety of autonomous driving systems. This
technique relies heavily on the reliability and availability of
vehicle-to-everything (V2X) communication. In practical sensor fusion
application scenarios, the non-line-of-sight (NLOS) issue causes blind zones
for not only the perception system but also V2X direct communication. To
counteract underlying communication issues, we introduce an abstract perception
matrix matching method for quick sensor fusion matching procedures and
mobility-height hybrid relay determination procedures, proactively improving
the efficiency and performance of V2X communication to serve the upper layer
application fusion requirements. To demonstrate the effectiveness of our
solution, we design a new simulation framework to consider autonomous driving,
sensor fusion and V2X communication in general, paving the way for end-to-end
performance evaluation and further solution derivation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lantao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chen Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06625">
<title>Automated Deception Detection from Videos: Using End-to-End Learning Based High-Level Features and Classification Approaches. (arXiv:2307.06625v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06625</link>
<description rdf:parseType="Literal">&lt;p&gt;Deception detection is an interdisciplinary field attracting researchers from
psychology, criminology, computer science, and economics. We propose a
multimodal approach combining deep learning and discriminative models for
automated deception detection. Using video modalities, we employ convolutional
end-to-end learning to analyze gaze, head pose, and facial expressions,
achieving promising results compared to state-of-the-art methods. Due to
limited training data, we also utilize discriminative models for deception
detection. Although sequence-to-class approaches are explored, discriminative
models outperform them due to data scarcity. Our approach is evaluated on five
datasets, including a new Rolling-Dice Experiment motivated by economic
factors. Results indicate that facial expressions outperform gaze and head
pose, and combining modalities with feature selection enhances detection
performance. Differences in expressed features across datasets emphasize the
importance of scenario-specific training data and the influence of context on
deceptive behavior. Cross-dataset experiments reinforce these findings. Despite
the challenges posed by low-stake datasets, including the Rolling-Dice
Experiment, deception detection performance exceeds chance levels. Our proposed
multimodal approach and comprehensive evaluation shed light on the potential of
automating deception detection from video modalities, opening avenues for
future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinges_L/0/1/0/all/0/1&quot;&gt;Laslo Dinges&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiedler_M/0/1/0/all/0/1&quot;&gt;Marc-Andr&amp;#xe9; Fiedler&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Hamadi_A/0/1/0/all/0/1&quot;&gt;Ayoub Al-Hamadi&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hempel_T/0/1/0/all/0/1&quot;&gt;Thorsten Hempel&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelrahman_A/0/1/0/all/0/1&quot;&gt;Ahmed Abdelrahman&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weimann_J/0/1/0/all/0/1&quot;&gt;Joachim Weimann&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bershadskyy_D/0/1/0/all/0/1&quot;&gt;Dmitri Bershadskyy&lt;/a&gt; (2) ((1) Neuro-Information Technology Group, Otto-von-Guericke University Magdeburg (2) Faculty of Economics and Management, Otto-von-Guericke University Magdeburg)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06630">
<title>Image Transformation Sequence Retrieval with General Reinforcement Learning. (arXiv:2307.06630v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06630</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, the novel Image Transformation Sequence Retrieval (ITSR) task
is presented, in which a model must retrieve the sequence of transformations
between two given images that act as source and target, respectively. Given
certain characteristics of the challenge such as the multiplicity of a correct
sequence or the correlation between consecutive steps of the process, we
propose a solution to ITSR using a general model-based Reinforcement Learning
such as Monte Carlo Tree Search (MCTS), which is combined with a deep neural
network. Our experiments provide a benchmark in both synthetic and real
domains, where the proposed approach is compared with supervised training. The
results report that a model trained with MCTS is able to outperform its
supervised counterpart in both the simplest and the most complex cases. Our
work draws interesting conclusions about the nature of ITSR and its associated
challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mas_Candela_E/0/1/0/all/0/1&quot;&gt;Enrique Mas-Candela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rios_Vila_A/0/1/0/all/0/1&quot;&gt;Antonio R&amp;#xed;os-Vila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvo_Zaragoza_J/0/1/0/all/0/1&quot;&gt;Jorge Calvo-Zaragoza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06647">
<title>DeepIPCv2: LiDAR-powered Robust Environmental Perception and Navigational Control for Autonomous Vehicle. (arXiv:2307.06647v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06647</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DeepIPCv2, an autonomous driving model that perceives the
environment using a LiDAR sensor for more robust drivability, especially when
driving under poor illumination conditions. DeepIPCv2 takes a set of LiDAR
point clouds for its main perception input. As point clouds are not affected by
illumination changes, they can provide a clear observation of the surroundings
no matter what the condition is. This results in a better scene understanding
and stable features provided by the perception module to support the controller
module in estimating navigational control properly. To evaluate its
performance, we conduct several tests by deploying the model to predict a set
of driving records and perform real automated driving under three different
conditions. We also conduct ablation and comparative studies with some recent
models to justify its performance. Based on the experimental results, DeepIPCv2
shows a robust performance by achieving the best drivability in all conditions.
Codes are available at https://github.com/oskarnatan/DeepIPCv2
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natan_O/0/1/0/all/0/1&quot;&gt;Oskar Natan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miura_J/0/1/0/all/0/1&quot;&gt;Jun Miura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06659">
<title>A Comprehensive Analysis of Blockchain Applications for Securing Computer Vision Systems. (arXiv:2307.06659v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.06659</link>
<description rdf:parseType="Literal">&lt;p&gt;Blockchain (BC) and Computer Vision (CV) are the two emerging fields with the
potential to transform various sectors.The ability of BC can help in offering
decentralized and secure data storage, while CV allows machines to learn and
understand visual data. This integration of the two technologies holds massive
promise for developing innovative applications that can provide solutions to
the challenges in various sectors such as supply chain management, healthcare,
smart cities, and defense. This review explores a comprehensive analysis of the
integration of BC and CV by examining their combination and potential
applications. It also provides a detailed analysis of the fundamental concepts
of both technologies, highlighting their strengths and limitations. This paper
also explores current research efforts that make use of the benefits offered by
this combination. The effort includes how BC can be used as an added layer of
security in CV systems and also ensure data integrity, enabling decentralized
image and video analytics using BC. The challenges and open issues associated
with this integration are also identified, and appropriate potential future
directions are also proposed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+M_R/0/1/0/all/0/1&quot;&gt;Ramalingam M&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selvi_C/0/1/0/all/0/1&quot;&gt;Chemmalar Selvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Victor_N/0/1/0/all/0/1&quot;&gt;Nancy Victor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chengoden_R/0/1/0/all/0/1&quot;&gt;Rajeswari Chengoden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Sweta Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddikunta_P/0/1/0/all/0/1&quot;&gt;Praveen Kumar Reddy Maddikunta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Duehee Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piran_M/0/1/0/all/0/1&quot;&gt;Md. Jalil Piran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khare_N/0/1/0/all/0/1&quot;&gt;Neelu Khare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yendri_G/0/1/0/all/0/1&quot;&gt;Gokul Yendri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadekallu_T/0/1/0/all/0/1&quot;&gt;Thippa Reddy Gadekallu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06666">
<title>Transformer-based end-to-end classification of variable-length volumetric data. (arXiv:2307.06666v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06666</link>
<description rdf:parseType="Literal">&lt;p&gt;The automatic classification of 3D medical data is memory-intensive. Also,
variations in the number of slices between samples is common. Naive solutions
such as subsampling can solve these problems, but at the cost of potentially
eliminating relevant diagnosis information. Transformers have shown promising
performance for sequential data analysis. However, their application for
long-sequences is data, computationally, and memory demanding. In this paper,
we propose an end-to-end Transformer-based framework that allows to classify
volumetric data of variable length in an efficient fashion. Particularly, by
randomizing the input slice-wise resolution during training, we enhance the
capacity of the learnable positional embedding assigned to each volume slice.
Consequently, the accumulated positional information in each positional
embedding can be generalized to the neighbouring slices, even for high
resolution volumes at the test time. By doing so, the model will be more robust
to variable volume length and amenable to different computational budgets. We
evaluated the proposed approach in retinal OCT volume classification and
achieved 21.96% average improvement in balanced accuracy on a 9-class
diagnostic task, compared to state-of-the-art video transformers. Our findings
show that varying the slice-wise resolution of the input during training
results in more informative volume representation as compared to training with
fixed number of slices per volume. Our code is available at:
https://github.com/marziehoghbaie/VLFAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oghbaie_M/0/1/0/all/0/1&quot;&gt;Marzieh Oghbaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_T/0/1/0/all/0/1&quot;&gt;Teresa Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emre_T/0/1/0/all/0/1&quot;&gt;Taha Emre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1&quot;&gt;Ursula Schmidt-Erfurth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1&quot;&gt;Hrvoje Bogunovic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06667">
<title>DGCNet: An Efficient 3D-Densenet based on Dynamic Group Convolution for Hyperspectral Remote Sensing Image Classification. (arXiv:2307.06667v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06667</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks face many problems in the field of hyperspectral image
classification, lack of effective utilization of spatial spectral information,
gradient disappearance and overfitting as the model depth increases. In order
to accelerate the deployment of the model on edge devices with strict latency
requirements and limited computing power, we introduce a lightweight model
based on the improved 3D-Densenet model and designs DGCNet. It improves the
disadvantage of group convolution. Referring to the idea of dynamic network,
dynamic group convolution(DGC) is designed on 3d convolution kernel. DGC
introduces small feature selectors for each grouping to dynamically decide
which part of the input channel to connect based on the activations of all
input channels. Multiple groups can capture different and complementary visual
and semantic features of input images, allowing convolution neural network(CNN)
to learn rich features. 3D convolution extracts high-dimensional and redundant
hyperspectral data, and there is also a lot of redundant information between
convolution kernels. DGC module allows 3D-Densenet to select channel
information with richer semantic features and discard inactive regions. The
3D-CNN passing through the DGC module can be regarded as a pruned network. DGC
not only allows 3D-CNN to complete sufficient feature extraction, but also
takes into account the requirements of speed and calculation amount. The
inference speed and accuracy have been improved, with outstanding performance
on the IN, Pavia and KSC datasets, ahead of the mainstream hyperspectral image
classification methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guandong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06689">
<title>YOLIC: An Efficient Method for Object Localization and Classification on Edge Devices. (arXiv:2307.06689v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06689</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of Tiny AI, we introduce &quot;You Only Look at Interested Cells&quot;
(YOLIC), an efficient method for object localization and classification on edge
devices. Seamlessly blending the strengths of semantic segmentation and object
detection, YOLIC offers superior computational efficiency and precision. By
adopting Cells of Interest for classification instead of individual pixels,
YOLIC encapsulates relevant information, reduces computational load, and
enables rough object shape inference. Importantly, the need for bounding box
regression is obviated, as YOLIC capitalizes on the predetermined cell
configuration that provides information about potential object location, size,
and shape. To tackle the issue of single-label classification limitations, a
multi-label classification approach is applied to each cell, effectively
recognizing overlapping or closely situated objects. This paper presents
extensive experiments on multiple datasets, demonstrating that YOLIC achieves
detection performance comparable to the state-of-the-art YOLO algorithms while
surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU. All resources
related to this study, including datasets, cell designer, image annotation
tool, and source code, have been made publicly available on our project website
at https://kai3316.github.io/yolic.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qiangfu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomioka_Y/0/1/0/all/0/1&quot;&gt;Yoichi Tomioka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06701">
<title>S-HR-VQVAE: Sequential Hierarchical Residual Learning Vector Quantized Variational Autoencoder for Video Prediction. (arXiv:2307.06701v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06701</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the video prediction task by putting forth a novel model that
combines (i) our recently proposed hierarchical residual vector quantized
variational autoencoder (HR-VQVAE), and (ii) a novel spatiotemporal PixelCNN
(ST-PixelCNN). We refer to this approach as a sequential hierarchical residual
learning vector quantized variational autoencoder (S-HR-VQVAE). By leveraging
the intrinsic capabilities of HR-VQVAE at modeling still images with a
parsimonious representation, combined with the ST-PixelCNN&apos;s ability at
handling spatiotemporal information, S-HR-VQVAE can better deal with chief
challenges in video prediction. These include learning spatiotemporal
information, handling high dimensional data, combating blurry prediction, and
implicit modeling of physical characteristics. Extensive experimental results
on the KTH Human Action and Moving-MNIST tasks demonstrate that our model
compares favorably against top video prediction techniques both in quantitative
and qualitative evaluations despite a much smaller model size. Finally, we
boost S-HR-VQVAE by proposing a novel training method to jointly estimate the
HR-VQVAE and ST-PixelCNN parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adiban_M/0/1/0/all/0/1&quot;&gt;Mohammad Adiban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefanov_K/0/1/0/all/0/1&quot;&gt;Kalin Stefanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siniscalchi_S/0/1/0/all/0/1&quot;&gt;Sabato Marco Siniscalchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvi_G/0/1/0/all/0/1&quot;&gt;Giampiero Salvi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06720">
<title>Weakly supervised marine animal detection from remote sensing images using vector-quantized variational autoencoder. (arXiv:2307.06720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06720</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies a reconstruction-based approach for weakly-supervised
animal detection from aerial images in marine environments. Such an approach
leverages an anomaly detection framework that computes metrics directly on the
input space, enhancing interpretability and anomaly localization compared to
feature embedding methods. Building upon the success of Vector-Quantized
Variational Autoencoders in anomaly detection on computer vision datasets, we
adapt them to the marine animal detection domain and address the challenge of
handling noisy data. To evaluate our approach, we compare it with existing
methods in the context of marine animal detection from aerial image data.
Experiments conducted on two dedicated datasets demonstrate the superior
performance of the proposed method over recent studies in the literature. Our
framework offers improved interpretability and localization of anomalies,
providing valuable insights for monitoring marine ecosystems and mitigating the
impact of human activities on marine animals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh-Tan Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangloff_H/0/1/0/all/0/1&quot;&gt;Hugo Gangloff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Lef&amp;#xe8;vre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06724">
<title>Multimodal Object Detection in Remote Sensing. (arXiv:2307.06724v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06724</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection in remote sensing is a crucial computer vision task that has
seen significant advancements with deep learning techniques. However, most
existing works in this area focus on the use of generic object detection and do
not leverage the potential of multimodal data fusion. In this paper, we present
a comparison of methods for multimodal object detection in remote sensing,
survey available multimodal datasets suitable for evaluation, and discuss
future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belmouhcine_A/0/1/0/all/0/1&quot;&gt;Abdelbadie Belmouhcine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnel_J/0/1/0/all/0/1&quot;&gt;Jean-Christophe Burnel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courtrai_L/0/1/0/all/0/1&quot;&gt;Luc Courtrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_M/0/1/0/all/0/1&quot;&gt;Minh-Tan Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefevre_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Lef&amp;#xe8;vre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06737">
<title>Improving 2D Human Pose Estimation across Unseen Camera Views with Synthetic Data. (arXiv:2307.06737v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06737</link>
<description rdf:parseType="Literal">&lt;p&gt;Human Pose Estimation is a thoroughly researched problem; however, most
datasets focus on the side and front-view scenarios. We address the limitation
by proposing a novel approach that tackles the challenges posed by extreme
viewpoints and poses. We introduce a new method for synthetic data generation -
RePoGen, RarE POses GENerator - with comprehensive control over pose and view
to augment the COCO dataset. Experiments on a new dataset of real images show
that adding RePoGen data to the COCO surpasses previous attempts to top-view
pose estimation and significantly improves performance on the bottom-view
dataset. Through an extensive ablation study on both the top and bottom view
data, we elucidate the contributions of methodological choices and demonstrate
improved performance. The code and the datasets are available on the project
website.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purkrabek_M/0/1/0/all/0/1&quot;&gt;Miroslav Purkr&amp;#xe1;bek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1&quot;&gt;Ji&amp;#x159;&amp;#xed; Matas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06751">
<title>Watch Your Pose: Unsupervised Domain Adaption with Pose based Triplet Selection for Gait Recognition. (arXiv:2307.06751v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06751</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait Recognition is a computer vision task aiming to identify people by their
walking patterns. Existing methods show impressive results on individual
datasets but lack the ability to generalize to unseen scenarios. Unsupervised
Domain Adaptation (UDA) tries to adapt a model, pre-trained in a supervised
manner on a source domain, to an unlabelled target domain. UDA for Gait
Recognition is still in its infancy and existing works proposed solutions to
limited scenarios. In this paper, we reveal a fundamental phenomenon in
adaptation of gait recognition models, in which the target domain is biased to
pose-based features rather than identity features, causing a significant
performance drop in the identification task. We suggest Gait Orientation-based
method for Unsupervised Domain Adaptation (GOUDA) to reduce this bias. To this
end, we present a novel Triplet Selection algorithm with a curriculum learning
framework, aiming to adapt the embedding space by pushing away samples of
similar poses and bringing closer samples of different poses. We provide
extensive experiments on four widely-used gait datasets, CASIA-B, OU-MVLP,
GREW, and Gait3D, and on three backbones, GaitSet, GaitPart, and GaitGL,
showing the superiority of our proposed method over prior works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habib_G/0/1/0/all/0/1&quot;&gt;Gavriel Habib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barzilay_N/0/1/0/all/0/1&quot;&gt;Noa Barzilay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimshi_O/0/1/0/all/0/1&quot;&gt;Or Shimshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Ari_R/0/1/0/all/0/1&quot;&gt;Rami Ben-Ari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darshan_N/0/1/0/all/0/1&quot;&gt;Nir Darshan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06771">
<title>Generalizing Supervised Deep Learning MRI Reconstruction to Multiple and Unseen Contrasts using Meta-Learning Hypernetworks. (arXiv:2307.06771v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.06771</link>
<description rdf:parseType="Literal">&lt;p&gt;Meta-learning has recently been an emerging data-efficient learning technique
for various medical imaging operations and has helped advance contemporary deep
learning models. Furthermore, meta-learning enhances the knowledge
generalization of the imaging tasks by learning both shared and discriminative
weights for various configurations of imaging tasks. However, existing
meta-learning models attempt to learn a single set of weight initializations of
a neural network that might be restrictive for multimodal data. This work aims
to develop a multimodal meta-learning model for image reconstruction, which
augments meta-learning with evolutionary capabilities to encompass diverse
acquisition settings of multimodal data. Our proposed model called KM-MAML
(Kernel Modulation-based Multimodal Meta-Learning), has hypernetworks that
evolve to generate mode-specific weights. These weights provide the
mode-specific inductive bias for multiple modes by re-calibrating each kernel
of the base network for image reconstruction via a low-rank kernel modulation
operation. We incorporate gradient-based meta-learning (GBML) in the contextual
space to update the weights of the hypernetworks for different modes. The
hypernetworks and the reconstruction network in the GBML setting provide
discriminative mode-specific features and low-level image features,
respectively. Experiments on multi-contrast MRI reconstruction show that our
model, (i) exhibits superior reconstruction performance over joint training,
other meta-learning methods, and context-specific MRI reconstruction methods,
and (ii) better adaptation capabilities with improvement margins of 0.5 dB in
PSNR and 0.01 in SSIM. Besides, a representation analysis with U-Net shows that
kernel modulation infuses 80% of mode-specific representation changes in the
high-resolution layers. Our source code is available at
https://github.com/sriprabhar/KM-MAML/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ramanarayanan_S/0/1/0/all/0/1&quot;&gt;Sriprabha Ramanarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Palla_A/0/1/0/all/0/1&quot;&gt;Arun Palla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ram_K/0/1/0/all/0/1&quot;&gt;Keerthi Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sivaprakasam_M/0/1/0/all/0/1&quot;&gt;Mohanasankar Sivaprakasam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06784">
<title>Robotic surface exploration with vision and tactile sensing for cracks detection and characterisation. (arXiv:2307.06784v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.06784</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel algorithm for crack localisation and detection
based on visual and tactile analysis via fibre-optics. A finger-shaped sensor
based on fibre-optics is employed for the data acquisition to collect data for
the analysis and the experiments. To detect the possible locations of cracks a
camera is used to scan an environment while running an object detection
algorithm. Once the crack is detected, a fully-connected graph is created from
a skeletonised version of the crack. A minimum spanning tree is then employed
for calculating the shortest path to explore the crack which is then used to
develop the motion planner for the robotic manipulator. The motion planner
divides the crack into multiple nodes which are then explored individually.
Then, the manipulator starts the exploration and performs the tactile data
classification to confirm if there is indeed a crack in that location or just a
false positive from the vision algorithm. If a crack is detected, also the
length, width, orientation and number of branches are calculated. This is
repeated until all the nodes of the crack are explored.
&lt;/p&gt;
&lt;p&gt;In order to validate the complete algorithm, various experiments are
performed: comparison of exploration of cracks through full scan and motion
planning algorithm, implementation of frequency-based features for crack
classification and geometry analysis using a combination of vision and tactile
data. From the results of the experiments, it is shown that the proposed
algorithm is able to detect cracks and improve the results obtained from vision
to correctly classify cracks and their geometry with minimal cost thanks to the
motion planning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palermo_F/0/1/0/all/0/1&quot;&gt;Francesca Palermo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omarali_B/0/1/0/all/0/1&quot;&gt;Bukeikhan Omarali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Althoefer_K/0/1/0/all/0/1&quot;&gt;Kaspar Althoefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farkhatdinov_I/0/1/0/all/0/1&quot;&gt;Ildar Farkhatdinov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06795">
<title>Leveraging Vision-Language Foundation Models for Fine-Grained Downstream Tasks. (arXiv:2307.06795v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06795</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language foundation models such as CLIP have shown impressive
zero-shot performance on many tasks and datasets, especially thanks to their
free-text inputs. However, they struggle to handle some downstream tasks, such
as fine-grained attribute detection and localization. In this paper, we propose
a multitask fine-tuning strategy based on a positive/negative prompt
formulation to further leverage the capacities of the vision-language
foundation models. Using the CLIP architecture as baseline, we show strong
improvements on bird fine-grained attribute detection and localization tasks,
while also increasing the classification performance on the CUB200-2011
dataset. We provide source code for reproducibility purposes: it is available
at https://github.com/FactoDeepLearning/MultitaskVLFM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coquenet_D/0/1/0/all/0/1&quot;&gt;Denis Coquenet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambour_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Rambour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalsasso_E/0/1/0/all/0/1&quot;&gt;Emanuele Dalsasso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thome_N/0/1/0/all/0/1&quot;&gt;Nicolas Thome&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06844">
<title>Garbage in, garbage out: Zero-shot detection of crime using Large Language Models. (arXiv:2307.06844v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06844</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes exploiting the common sense knowledge learned by large
language models to perform zero-shot reasoning about crimes given textual
descriptions of surveillance videos. We show that when video is (manually)
converted to high quality textual descriptions, large language models are
capable of detecting and classifying crimes with state-of-the-art performance
using only zero-shot reasoning. However, existing automated video-to-text
approaches are unable to generate video descriptions of sufficient quality to
support reasoning (garbage video descriptions into the large language model,
garbage out).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simmons_A/0/1/0/all/0/1&quot;&gt;Anj Simmons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasa_R/0/1/0/all/0/1&quot;&gt;Rajesh Vasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06853">
<title>LVLane: Deep Learning for Lane Detection and Classification in Challenging Conditions. (arXiv:2307.06853v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06853</link>
<description rdf:parseType="Literal">&lt;p&gt;Lane detection plays a pivotal role in the field of autonomous vehicles and
advanced driving assistant systems (ADAS). Over the years, numerous algorithms
have emerged, spanning from rudimentary image processing techniques to
sophisticated deep neural networks. The performance of deep learning-based
models is highly dependent on the quality of their training data. Consequently,
these models often experience a decline in performance when confronted with
challenging scenarios such as extreme lighting conditions, partially visible
lane markings, and sparse lane markings like Botts&apos; dots. To address this, we
present an end-to-end lane detection and classification system based on deep
learning methodologies. In our study, we introduce a unique dataset
meticulously curated to encompass scenarios that pose significant challenges
for state-of-the-art (SOTA) models. Through fine-tuning selected models, we aim
to achieve enhanced localization accuracy. Moreover, we propose a CNN-based
classification branch, seamlessly integrated with the detector, facilitating
the identification of distinct lane types. This architecture enables informed
lane-changing decisions and empowers more resilient ADAS capabilities. We also
investigate the effect of using mixed precision training and testing on
different models and batch sizes. Experimental evaluations conducted on the
widely-used TuSimple dataset, Caltech lane dataset, and our LVLane dataset
demonstrate the effectiveness of our model in accurately detecting and
classifying lanes amidst challenging scenarios. Our method achieves
state-of-the-art classification results on the TuSimple dataset. The code of
the work will be published upon the acceptance of the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_Z/0/1/0/all/0/1&quot;&gt;Zillur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_B/0/1/0/all/0/1&quot;&gt;Brendan Tran Morris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06855">
<title>Data Augmentation in Training CNNs: Injecting Noise to Images. (arXiv:2307.06855v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06855</link>
<description rdf:parseType="Literal">&lt;p&gt;Noise injection is a fundamental tool for data augmentation, and yet there is
no widely accepted procedure to incorporate it with learning frameworks. This
study analyzes the effects of adding or applying different noise models of
varying magnitudes to Convolutional Neural Network (CNN) architectures. Noise
models that are distributed with different density functions are given common
magnitude levels via Structural Similarity (SSIM) metric in order to create an
appropriate ground for comparison. The basic results are conforming with the
most of the common notions in machine learning, and also introduce some novel
heuristics and recommendations on noise injection. The new approaches will
provide better understanding on optimal learning procedures for image
classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbiyik_M/0/1/0/all/0/1&quot;&gt;M. Eren Akbiyik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06913">
<title>Uncovering Unique Concept Vectors through Latent Space Decomposition. (arXiv:2307.06913v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.06913</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpreting the inner workings of deep learning models is crucial for
establishing trust and ensuring model safety. Concept-based explanations have
emerged as a superior approach that is more interpretable than feature
attribution estimates such as pixel saliency. However, defining the concepts
for the interpretability analysis biases the explanations by the user&apos;s
expectations on the concepts. To address this, we propose a novel post-hoc
unsupervised method that automatically uncovers the concepts learned by deep
models during training. By decomposing the latent space of a layer in singular
vectors and refining them by unsupervised clustering, we uncover concept
vectors aligned with directions of high variance that are relevant to the model
prediction, and that point to semantically distinct concepts. Our extensive
experiments reveal that the majority of our concepts are readily understandable
to humans, exhibit coherency, and bear relevance to the task at hand. Moreover,
we showcase the practical utility of our method in dataset exploration, where
our concept vectors successfully identify outlier training samples affected by
various confounding factors. This novel exploration technique has remarkable
versatility to data types and model architectures and it will facilitate the
identification of biases and the discovery of sources of error within training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graziani_M/0/1/0/all/0/1&quot;&gt;Mara Graziani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahony_L/0/1/0/all/0/1&quot;&gt;Laura O&amp;#x27; Mahony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;An-Phi Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_H/0/1/0/all/0/1&quot;&gt;Henning M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrearczyk_V/0/1/0/all/0/1&quot;&gt;Vincent Andrearczyk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06925">
<title>Domain-Agnostic Tuning-Encoder for Fast Personalization of Text-To-Image Models. (arXiv:2307.06925v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06925</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image (T2I) personalization allows users to guide the creative image
generation process by combining their own visual concepts in natural language
prompts. Recently, encoder-based techniques have emerged as a new effective
approach for T2I personalization, reducing the need for multiple images and
long training times. However, most existing encoders are limited to a
single-class domain, which hinders their ability to handle diverse concepts. In
this work, we propose a domain-agnostic method that does not require any
specialized dataset or prior information about the personalized concepts. We
introduce a novel contrastive-based regularization technique to maintain high
fidelity to the target concept characteristics while keeping the predicted
embeddings close to editable regions of the latent space, by pushing the
predicted tokens toward their nearest existing CLIP tokens. Our experimental
results demonstrate the effectiveness of our approach and show how the learned
tokens are more semantic than tokens predicted by unregularized models. This
leads to a better representation that achieves state-of-the-art performance
while being more flexible than previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1&quot;&gt;Moab Arar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gal_R/0/1/0/all/0/1&quot;&gt;Rinon Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atzmon_Y/0/1/0/all/0/1&quot;&gt;Yuval Atzmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chechik_G/0/1/0/all/0/1&quot;&gt;Gal Chechik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamir_A/0/1/0/all/0/1&quot;&gt;Ariel Shamir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bermano_A/0/1/0/all/0/1&quot;&gt;Amit H. Bermano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06930">
<title>mBLIP: Efficient Bootstrapping of Multilingual Vision-LLMs. (arXiv:2307.06930v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06930</link>
<description rdf:parseType="Literal">&lt;p&gt;Modular vision-language models (Vision-LLMs) align pretrained image encoders
with (pretrained) large language models (LLMs), representing a computationally
much more efficient alternative to end-to-end training of large vision-language
models from scratch, which is prohibitively expensive for most. Vision-LLMs
instead post-hoc condition LLMs to `understand&apos; the output of an image encoder.
With the abundance of readily available high-quality English image-text data as
well as monolingual English LLMs, the research focus has been on English-only
Vision-LLMs. Multilingual vision-language models are still predominantly
obtained via expensive end-to-end pretraining, resulting in comparatively
smaller models, trained on limited multilingual image data supplemented with
text-only multilingual corpora. In this work, we present mBLIP, the first
multilingual Vision-LLM, which we obtain in a computationally efficient manner
-- on consumer hardware using only a few million training examples -- by
leveraging a pretrained multilingual LLM. To this end, we \textit{re-align} an
image encoder previously tuned to an English LLM to a new, multilingual LLM --
for this, we leverage multilingual data from a mix of vision-and-language
tasks, which we obtain by machine-translating high-quality English data to 95
languages. On the IGLUE benchmark, mBLIP yields results competitive with
state-of-the-art models. Moreover, in image captioning on XM3600, mBLIP
(zero-shot) even outperforms PaLI-X (a model with 55B parameters). Compared to
these very large multilingual vision-language models trained from scratch, we
obtain mBLIP by training orders of magnitude fewer parameters on magnitudes
less data. We release our model and code at
\url{https://github.com/gregor-ge/mBLIP}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geigle_G/0/1/0/all/0/1&quot;&gt;Gregor Geigle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Abhay Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1&quot;&gt;Radu Timofte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glavas_G/0/1/0/all/0/1&quot;&gt;Goran Glava&amp;#x161;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06940">
<title>Animate-A-Story: Storytelling with Retrieval-Augmented Video Generation. (arXiv:2307.06940v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06940</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating videos for visual storytelling can be a tedious and complex
process that typically requires either live-action filming or graphics
animation rendering. To bypass these challenges, our key idea is to utilize the
abundance of existing video clips and synthesize a coherent storytelling video
by customizing their appearances. We achieve this by developing a framework
comprised of two functional modules: (i) Motion Structure Retrieval, which
provides video candidates with desired scene or motion context described by
query texts, and (ii) Structure-Guided Text-to-Video Synthesis, which generates
plot-aligned videos under the guidance of motion structure and text prompts.
For the first module, we leverage an off-the-shelf video retrieval system and
extract video depths as motion structure. For the second module, we propose a
controllable video generation model that offers flexible controls over
structure and characters. The videos are synthesized by following the
structural guidance and appearance instruction. To ensure visual consistency
across clips, we propose an effective concept personalization approach, which
allows the specification of the desired character identities through text
prompts. Extensive experiments demonstrate that our approach exhibits
significant advantages over various existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yingqing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Menghan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cun_X/0/1/0/all/0/1&quot;&gt;Xiaodong Cun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinbo Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_C/0/1/0/all/0/1&quot;&gt;Chao Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06941">
<title>On the Connection between Game-Theoretic Feature Attributions and Counterfactual Explanations. (arXiv:2307.06941v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.06941</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Artificial Intelligence (XAI) has received widespread interest in
recent years, and two of the most popular types of explanations are feature
attributions, and counterfactual explanations. These classes of approaches have
been largely studied independently and the few attempts at reconciling them
have been primarily empirical. This work establishes a clear theoretical
connection between game-theoretic feature attributions, focusing on but not
limited to SHAP, and counterfactuals explanations. After motivating operative
changes to Shapley values based feature attributions and counterfactual
explanations, we prove that, under conditions, they are in fact equivalent. We
then extend the equivalency result to game-theoretic solution concepts beyond
Shapley values. Moreover, through the analysis of the conditions of such
equivalence, we shed light on the limitations of naively using counterfactual
explanations to provide feature importances. Experiments on three datasets
quantitatively show the difference in explanations at every stage of the
connection between the two approaches and corroborate the theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albini_E/0/1/0/all/0/1&quot;&gt;Emanuele Albini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Shubham Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Saumitra Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dervovic_D/0/1/0/all/0/1&quot;&gt;Danial Dervovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magazzeni_D/0/1/0/all/0/1&quot;&gt;Daniele Magazzeni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06942">
<title>InternVid: A Large-scale Video-Text Dataset for Multimodal Understanding and Generation. (arXiv:2307.06942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06942</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces InternVid, a large-scale video-centric multimodal
dataset that enables learning powerful and transferable video-text
representations for multimodal understanding and generation. The InternVid
dataset contains over 7 million videos lasting nearly 760K hours, yielding 234M
video clips accompanied by detailed descriptions of total 4.1B words. Our core
contribution is to develop a scalable approach to autonomously build a
high-quality video-text dataset with large language models (LLM), thereby
showcasing its efficacy in learning video-language representation at scale.
Specifically, we utilize a multi-scale approach to generate video-related
descriptions. Furthermore, we introduce ViCLIP, a video-text representation
learning model based on ViT-L. Learned on InternVid via contrastive learning,
this model demonstrates leading zero-shot action recognition and competitive
video retrieval performance. Beyond basic video understanding tasks like
recognition and retrieval, our dataset and model have broad applications. They
are particularly beneficial for generating interleaved video-text data for
learning a video-centric dialogue system, advancing video-to-text and
text-to-video generation research. These proposed resources provide a tool for
researchers and practitioners interested in multimodal video understanding and
generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yinan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yizhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kunchang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiashuo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yali Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Limin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06947">
<title>Video-FocalNets: Spatio-Temporal Focal Modulation for Video Action Recognition. (arXiv:2307.06947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06947</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent video recognition models utilize Transformer models for long-range
spatio-temporal context modeling. Video transformer designs are based on
self-attention that can model global context at a high computational cost. In
comparison, convolutional designs for videos offer an efficient alternative but
lack long-range dependency modeling. Towards achieving the best of both
designs, this work proposes Video-FocalNet, an effective and efficient
architecture for video recognition that models both local and global contexts.
Video-FocalNet is based on a spatio-temporal focal modulation architecture that
reverses the interaction and aggregation steps of self-attention for better
efficiency. Further, the aggregation step and the interaction step are both
implemented using efficient convolution and element-wise multiplication
operations that are computationally less expensive than their self-attention
counterparts on video representations. We extensively explore the design space
of focal modulation-based spatio-temporal context modeling and demonstrate our
parallel spatial and temporal encoding design to be the optimal choice.
Video-FocalNets perform favorably well against the state-of-the-art
transformer-based models for video recognition on three large-scale datasets
(Kinetics-400, Kinetics-600, and SS-v2) at a lower computational cost. Our
code/models are released at https://github.com/TalalWasim/Video-FocalNets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1&quot;&gt;Syed Talal Wasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1&quot;&gt;Muhammad Uzair Khattak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06948">
<title>Self-regulating Prompts: Foundational Model Adaptation without Forgetting. (arXiv:2307.06948v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06948</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt learning has emerged as an efficient alternative for fine-tuning
foundational models, such as CLIP, for various downstream tasks. Conventionally
trained using the task-specific objective, i.e., cross-entropy loss, prompts
tend to overfit downstream data distributions and find it challenging to
capture task-agnostic general features from the frozen CLIP. This leads to the
loss of the model&apos;s original generalization capability. To address this issue,
our work introduces a self-regularization framework for prompting called
PromptSRC (Prompting with Self-regulating Constraints). PromptSRC guides the
prompts to optimize for both task-specific and task-agnostic general
representations using a three-pronged approach by: (a) regulating {prompted}
representations via mutual agreement maximization with the frozen model, (b)
regulating with self-ensemble of prompts over the training trajectory to encode
their complementary strengths, and (c) regulating with textual diversity to
mitigate sample diversity imbalance with the visual branch. To the best of our
knowledge, this is the first regularization framework for prompt learning that
avoids overfitting by jointly attending to pre-trained model features, the
training trajectory during prompting, and the textual diversity. PromptSRC
explicitly steers the prompts to learn a representation space that maximizes
performance on downstream tasks without compromising CLIP generalization. We
perform extensive experiments on 4 benchmarks where PromptSRC overall performs
favorably well compared to the existing methods. Our code and pre-trained
models are publicly available at: https://github.com/muzairkhattak/PromptSRC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khattak_M/0/1/0/all/0/1&quot;&gt;Muhammad Uzair Khattak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wasim_S/0/1/0/all/0/1&quot;&gt;Syed Talal Wasim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06949">
<title>HyperDreamBooth: HyperNetworks for Fast Personalization of Text-to-Image Models. (arXiv:2307.06949v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.06949</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalization has emerged as a prominent aspect within the field of
generative AI, enabling the synthesis of individuals in diverse contexts and
styles, while retaining high-fidelity to their identities. However, the process
of personalization presents inherent challenges in terms of time and memory
requirements. Fine-tuning each personalized model needs considerable GPU time
investment, and storing a personalized model per subject can be demanding in
terms of storage capacity. To overcome these challenges, we propose
HyperDreamBooth-a hypernetwork capable of efficiently generating a small set of
personalized weights from a single image of a person. By composing these
weights into the diffusion model, coupled with fast finetuning, HyperDreamBooth
can generate a person&apos;s face in various contexts and styles, with high subject
details while also preserving the model&apos;s crucial knowledge of diverse styles
and semantic modifications. Our method achieves personalization on faces in
roughly 20 seconds, 25x faster than DreamBooth and 125x faster than Textual
Inversion, using as few as one reference image, with the same quality and style
diversity as DreamBooth. Also our method yields a model that is 10000x smaller
than a normal DreamBooth model. Project page: https://hyperdreambooth.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_N/0/1/0/all/0/1&quot;&gt;Nataniel Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jampani_V/0/1/0/all/0/1&quot;&gt;Varun Jampani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingbo Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pritch_Y/0/1/0/all/0/1&quot;&gt;Yael Pritch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_N/0/1/0/all/0/1&quot;&gt;Neal Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_M/0/1/0/all/0/1&quot;&gt;Michael Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2005.04490">
<title>Human in Events: A Large-Scale Benchmark for Human-centric Video Analysis in Complex Events. (arXiv:2005.04490v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2005.04490</link>
<description rdf:parseType="Literal">&lt;p&gt;Along with the development of modern smart cities, human-centric video
analysis has been encountering the challenge of analyzing diverse and complex
events in real scenes. A complex event relates to dense crowds, anomalous
individuals, or collective behaviors. However, limited by the scale and
coverage of existing video datasets, few human analysis approaches have
reported their performances on such complex events. To this end, we present a
new large-scale dataset with comprehensive annotations, named Human-in-Events
or HiEve (Human-centric video analysis in complex Events), for the
understanding of human motions, poses, and actions in a variety of realistic
events, especially in crowd &amp;amp; complex events. It contains a record number of
poses (&amp;gt;1M), the largest number of action instances (&amp;gt;56k) under complex
events, as well as one of the largest numbers of trajectories lasting for
longer time (with an average trajectory length of &amp;gt;480 frames). Based on its
diverse annotation, we present two simple baselines for action recognition and
pose estimation, respectively. They leverage cross-label information during
training to enhance the feature learning in corresponding visual tasks.
Experiments show that they could boost the performance of existing action
recognition and pose estimation pipelines. More importantly, they prove the
widely ranged annotations in HiEve can improve various video tasks.
Furthermore, we conduct extensive experiments to benchmark recent video
analysis approaches together with our baseline methods, demonstrating HiEve is
a challenging dataset for human-centric video analysis. We expect that the
dataset will advance the development of cutting-edge techniques in
human-centric analysis and the understanding of complex events. The dataset is
available at &lt;a href=&quot;http://humaninevents.org&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weiyao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huabin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shizhan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1&quot;&gt;Rui Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Ning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guo-Jun Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.13445">
<title>Emergent Neural Network Mechanisms for Generalization to Objects in Novel Orientations. (arXiv:2109.13445v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.13445</link>
<description rdf:parseType="Literal">&lt;p&gt;The capability of Deep Neural Networks (DNNs) to recognize objects in
orientations outside the distribution of the training data is not well
understood. We present evidence that DNNs are capable of generalizing to
objects in novel orientations by disseminating orientation-invariance obtained
from familiar objects seen from many viewpoints. This capability strengthens
when training the DNN with an increasing number of familiar objects, but only
in orientations that involve 2D rotations of familiar orientations. We show
that this dissemination is achieved via neurons tuned to common features
between familiar and unfamiliar objects. These results implicate brain-like
neural mechanisms for generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cooper_A/0/1/0/all/0/1&quot;&gt;Avi Cooper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boix_X/0/1/0/all/0/1&quot;&gt;Xavier Boix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harari_D/0/1/0/all/0/1&quot;&gt;Daniel Harari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madan_S/0/1/0/all/0/1&quot;&gt;Spandan Madan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_H/0/1/0/all/0/1&quot;&gt;Hanspeter Pfister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sasaki_T/0/1/0/all/0/1&quot;&gt;Tomotake Sasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_P/0/1/0/all/0/1&quot;&gt;Pawan Sinha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.09405">
<title>Improving Chest X-Ray Report Generation by Leveraging Warm Starting. (arXiv:2201.09405v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.09405</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically generating a report from a patient&apos;s Chest X-Rays (CXRs) is a
promising solution to reducing clinical workload and improving patient care.
However, current CXR report generators -- which are predominantly
encoder-to-decoder models -- lack the diagnostic accuracy to be deployed in a
clinical setting. To improve CXR report generation, we investigate warm
starting the encoder and decoder with recent open-source computer vision and
natural language processing checkpoints, such as the Vision Transformer (ViT)
and PubMedBERT. To this end, each checkpoint is evaluated on the MIMIC-CXR and
IU X-Ray datasets. Our experimental investigation demonstrates that the
Convolutional vision Transformer (CvT) ImageNet-21K and the Distilled
Generative Pre-trained Transformer 2 (DistilGPT2) checkpoints are best for warm
starting the encoder and decoder, respectively. Compared to the
state-of-the-art ($\mathcal{M}^2$ Transformer Progressive), CvT2DistilGPT2
attained an improvement of 8.3\% for CE F-1, 1.8\% for BLEU-4, 1.6\% for
ROUGE-L, and 1.0\% for METEOR. The reports generated by CvT2DistilGPT2 have a
higher similarity to radiologist reports than previous approaches. This
indicates that leveraging warm starting improves CXR report generation. Code
and checkpoints for CvT2DistilGPT2 are available at
https://github.com/aehrc/cvt2distilgpt2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolson_A/0/1/0/all/0/1&quot;&gt;Aaron Nicolson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dowling_J/0/1/0/all/0/1&quot;&gt;Jason Dowling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koopman_B/0/1/0/all/0/1&quot;&gt;Bevan Koopman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.02819">
<title>Block shuffling learning for Deepfake Detection. (arXiv:2202.02819v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.02819</link>
<description rdf:parseType="Literal">&lt;p&gt;Deepfake detection methods based on convolutional neural networks (CNN) have
demonstrated high accuracy. \textcolor{black}{However, these methods often
suffer from decreased performance when faced with unknown forgery methods and
common transformations such as resizing and blurring, resulting in deviations
between training and testing domains.} This phenomenon, known as overfitting,
poses a significant challenge. To address this issue, we propose a novel block
shuffling regularization method. Firstly, our approach involves dividing the
images into blocks and applying both intra-block and inter-block shuffling
techniques. This process indirectly achieves weight-sharing across different
dimensions. Secondly, we introduce an adversarial loss algorithm to mitigate
the overfitting problem induced by the shuffling noise. Finally, we restore the
spatial layout of the blocks to capture the semantic associations among them.
Extensive experiments validate the effectiveness of our proposed method, which
surpasses existing approaches in forgery face detection. Notably, our method
exhibits excellent generalization capabilities, demonstrating robustness
against cross-dataset evaluations and common image transformations. Especially
our method can be easily integrated with various CNN models. Source code is
available at
\href{https://github.com/NoWindButRain/BlockShuffleLearning}{Github}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sitong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_Z/0/1/0/all/0/1&quot;&gt;Zhichao Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Siqi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Liang Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16301">
<title>PEGG-Net: Pixel-Wise Efficient Grasp Generation in Complex Scenes. (arXiv:2203.16301v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16301</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-based grasp estimation is an essential part of robotic manipulation
tasks in the real world. Existing planar grasp estimation algorithms have been
demonstrated to work well in relatively simple scenes. But when it comes to
complex scenes, such as cluttered scenes with messy backgrounds and moving
objects, the algorithms from previous works are prone to generate inaccurate
and unstable grasping contact points. In this work, we first study the existing
planar grasp estimation algorithms and analyze the related challenges in
complex scenes. Secondly, we design a Pixel-wise Efficient Grasp Generation
Network (PEGG-Net) to tackle the problem of grasping in complex scenes.
PEGG-Net can achieve improved state-of-the-art performance on the Cornell
dataset (98.9%) and second-best performance on the Jacquard dataset (93.8%),
outperforming other existing algorithms without the introduction of complex
structures. Thirdly, PEGG-Net could operate in a closed-loop manner for added
robustness in dynamic environments using position-based visual servoing (PBVS).
Finally, we conduct real-world experiments on static, dynamic, and cluttered
objects in different complex scenes. The results show that our proposed network
achieves a high success rate in grasping irregular objects, household objects,
and workshop tools. To benefit the community, our trained model and
supplementary materials are available at https://github.com/HZWang96/PEGG-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Huan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ang_M/0/1/0/all/0/1&quot;&gt;Marcelo H Ang Jr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07031">
<title>Rebalanced Zero-shot Learning. (arXiv:2210.07031v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07031</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot learning (ZSL) aims to identify unseen classes with zero samples
during training. Broadly speaking, present ZSL methods usually adopt
class-level semantic labels and compare them with instance-level semantic
predictions to infer unseen classes. However, we find that such existing models
mostly produce imbalanced semantic predictions, i.e. these models could perform
precisely for some semantics, but may not for others. To address the drawback,
we aim to introduce an imbalanced learning framework into ZSL. However, we find
that imbalanced ZSL has two unique challenges: (1) Its imbalanced predictions
are highly correlated with the value of semantic labels rather than the number
of samples as typically considered in the traditional imbalanced learning; (2)
Different semantics follow quite different error distributions between classes.
To mitigate these issues, we first formalize ZSL as an imbalanced regression
problem which offers empirical evidences to interpret how semantic labels lead
to imbalanced semantic predictions. We then propose a re-weighted loss termed
Re-balanced Mean-Squared Error (ReMSE), which tracks the mean and variance of
error distributions, thus ensuring rebalanced learning across classes. As a
major contribution, we conduct a series of analyses showing that ReMSE is
theoretically well established. Extensive experiments demonstrate that the
proposed method effectively alleviates the imbalance in semantic prediction and
outperforms many state-of-the-art ZSL methods. Our code is available at
https://github.com/FouriYe/ReZSL-TIP23.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zihan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaobo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Youfa Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaizhu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01671">
<title>Visually Adversarial Attacks and Defenses in the Physical World: A Survey. (arXiv:2211.01671v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01671</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Deep Neural Networks (DNNs) have been widely applied in various
real-world scenarios, they are vulnerable to adversarial examples. The current
adversarial attacks in computer vision can be divided into digital attacks and
physical attacks according to their different attack forms. Compared with
digital attacks, which generate perturbations in the digital pixels, physical
attacks are more practical in the real world. Owing to the serious security
problem caused by physically adversarial examples, many works have been
proposed to evaluate the physically adversarial robustness of DNNs in the past
years. In this paper, we summarize a survey versus the current physically
adversarial attacks and physically adversarial defenses in computer vision. To
establish a taxonomy, we organize the current physical attacks from attack
tasks, attack forms, and attack methods, respectively. Thus, readers can have a
systematic knowledge of this topic from different aspects. For the physical
defenses, we establish the taxonomy from pre-processing, in-processing, and
post-processing for the DNN models to achieve full coverage of the adversarial
defenses. Based on the above survey, we finally discuss the challenges of this
research field and further outlook on the future direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_B/0/1/0/all/0/1&quot;&gt;Bangzheng Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiefan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05156">
<title>Local Implicit Normalizing Flow for Arbitrary-Scale Image Super-Resolution. (arXiv:2303.05156v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05156</link>
<description rdf:parseType="Literal">&lt;p&gt;Flow-based methods have demonstrated promising results in addressing the
ill-posed nature of super-resolution (SR) by learning the distribution of
high-resolution (HR) images with the normalizing flow. However, these methods
can only perform a predefined fixed-scale SR, limiting their potential in
real-world applications. Meanwhile, arbitrary-scale SR has gained more
attention and achieved great progress. Nonetheless, previous arbitrary-scale SR
methods ignore the ill-posed problem and train the model with per-pixel L1
loss, leading to blurry SR outputs. In this work, we propose &quot;Local Implicit
Normalizing Flow&quot; (LINF) as a unified solution to the above problems. LINF
models the distribution of texture details under different scaling factors with
normalizing flow. Thus, LINF can generate photo-realistic HR images with rich
texture details in arbitrary scale factors. We evaluate LINF with extensive
experiments and show that LINF achieves the state-of-the-art perceptual quality
compared with prior arbitrary-scale SR methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jie-En Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsao_L/0/1/0/all/0/1&quot;&gt;Li-Yuan Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_Y/0/1/0/all/0/1&quot;&gt;Yi-Chen Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_R/0/1/0/all/0/1&quot;&gt;Roy Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chia-Che Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08715">
<title>EfficientNet Algorithm for Classification of Different Types of Cancer. (arXiv:2304.08715v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08715</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and efficient classification of different types of cancer is
critical for early detection and effective treatment. In this paper, we present
the results of our experiments using the EfficientNet algorithm for
classification of brain tumor, breast cancer mammography, chest cancer, and
skin cancer. We used publicly available datasets and preprocessed the images to
ensure consistency and comparability. Our experiments show that the
EfficientNet algorithm achieved high accuracy, precision, recall, and F1 scores
on each of the cancer datasets, outperforming other state-of-the-art algorithms
in the literature. We also discuss the strengths and weaknesses of the
EfficientNet algorithm and its potential applications in clinical practice. Our
results suggest that the EfficientNet algorithm is well-suited for
classification of different types of cancer and can be used to improve the
accuracy and efficiency of cancer diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Samir_R/0/1/0/all/0/1&quot;&gt;Romario Sameh Samir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05222">
<title>FishRecGAN: An End to End GAN Based Network for Fisheye Rectification and Calibration. (arXiv:2305.05222v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05222</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an end-to-end deep learning approach to rectify fisheye images and
simultaneously calibrate camera intrinsic and distortion parameters. Our method
consists of two parts: a Quick Image Rectification Module developed with a
Pix2Pix GAN and Wasserstein GAN (W-Pix2PixGAN), and a Calibration Module with a
CNN architecture. Our Quick Rectification Network performs robust rectification
with good resolution, making it suitable for constant calibration in
camera-based surveillance equipment. To achieve high-quality calibration, we
use the straightened output from the Quick Rectification Module as a
guidance-like semantic feature map for the Calibration Module to learn the
geometric relationship between the straightened feature and the distorted
feature. We train and validate our method with a large synthesized dataset
labeled with well-simulated parameters applied to a perspective image dataset.
Our solution has achieved robust performance in high-resolution with a
significant PSNR value of 22.343.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joo_K/0/1/0/all/0/1&quot;&gt;Kyungdon Joo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jean Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10400">
<title>What You See is What You Read? Improving Text-Image Alignment Evaluation. (arXiv:2305.10400v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10400</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically determining whether a text and a corresponding image are
semantically aligned is a significant challenge for vision-language models,
with applications in generative text-to-image and image-to-text tasks. In this
work, we study methods for automatic text-image alignment evaluation. We first
introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets
from both text-to-image and image-to-text generation tasks, with human
judgements for whether a given text-image pair is semantically aligned. We then
describe two automatic methods to determine alignment: the first involving a
pipeline based on question generation and visual question answering models, and
the second employing an end-to-end classification approach by finetuning
multimodal pretrained models. Both methods surpass prior approaches in various
text-image alignment tasks, with significant improvements in challenging cases
that involve complex composition or unnatural images. Finally, we demonstrate
how our approaches can localize specific misalignments between an image and a
given text, and how they can be used to automatically re-rank candidates in
text-to-image generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarom_M/0/1/0/all/0/1&quot;&gt;Michal Yarom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1&quot;&gt;Soravit Changpinyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharoni_R/0/1/0/all/0/1&quot;&gt;Roee Aharoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_J/0/1/0/all/0/1&quot;&gt;Jonathan Herzig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lang_O/0/1/0/all/0/1&quot;&gt;Oran Lang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ofek_E/0/1/0/all/0/1&quot;&gt;Eran Ofek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szpektor_I/0/1/0/all/0/1&quot;&gt;Idan Szpektor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00543">
<title>A Novel Driver Distraction Behavior Detection Method Based on Self-supervised Learning with Masked Image Modeling. (arXiv:2306.00543v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00543</link>
<description rdf:parseType="Literal">&lt;p&gt;Driver distraction causes a significant number of traffic accidents every
year, resulting in economic losses and casualties. Currently, the level of
automation in commercial vehicles is far from completely unmanned, and drivers
still play an important role in operating and controlling the vehicle.
Therefore, driver distraction behavior detection is crucial for road safety. At
present, driver distraction detection primarily relies on traditional
convolutional neural networks (CNN) and supervised learning methods. However,
there are still challenges such as the high cost of labeled datasets, limited
ability to capture high-level semantic information, and weak generalization
performance. In order to solve these problems, this paper proposes a new
self-supervised learning method based on masked image modeling for driver
distraction behavior detection. Firstly, a self-supervised learning framework
for masked image modeling (MIM) is introduced to solve the serious human and
material consumption issues caused by dataset labeling. Secondly, the Swin
Transformer is employed as an encoder. Performance is enhanced by reconfiguring
the Swin Transformer block and adjusting the distribution of the number of
window multi-head self-attention (W-MSA) and shifted window multi-head
self-attention (SW-MSA) detection heads across all stages, which leads to model
more lightening. Finally, various data augmentation strategies are used along
with the best random masking strategy to strengthen the model&apos;s recognition and
generalization ability. Test results on a large-scale driver distraction
behavior dataset show that the self-supervised learning method proposed in this
paper achieves an accuracy of 99.60%, approximating the excellent performance
of advanced supervised learning methods. Our code is publicly available at
github.com/Rocky1salady-killer/SL-DDBD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Taiguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xinghong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08249">
<title>Deblurring Masked Autoencoder is Better Recipe for Ultrasound Image Recognition. (arXiv:2306.08249v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08249</link>
<description rdf:parseType="Literal">&lt;p&gt;Masked autoencoder (MAE) has attracted unprecedented attention and achieves
remarkable performance in many vision tasks. It reconstructs random masked
image patches (known as proxy task) during pretraining and learns meaningful
semantic representations that can be transferred to downstream tasks. However,
MAE has not been thoroughly explored in ultrasound imaging. In this work, we
investigate the potential of MAE for ultrasound image recognition. Motivated by
the unique property of ultrasound imaging in high noise-to-signal ratio, we
propose a novel deblurring MAE approach that incorporates deblurring into the
proxy task during pretraining. The addition of deblurring facilitates the
pretraining to better recover the subtle details presented in the ultrasound
images, thus improving the performance of the downstream classification task.
Our experimental results demonstrate the effectiveness of our deblurring MAE,
achieving state-of-the-art performance in ultrasound image classification.
Overall, our work highlights the potential of MAE for ultrasound image
recognition and presents a novel approach that incorporates deblurring to
further improve its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Q/0/1/0/all/0/1&quot;&gt;Qingbo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_Q/0/1/0/all/0/1&quot;&gt;Qicheng Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11740">
<title>A survey on deep learning approaches for data integration in autonomous driving system. (arXiv:2306.11740v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11740</link>
<description rdf:parseType="Literal">&lt;p&gt;The perception module of self-driving vehicles relies on a multi-sensor
system to understand its environment. Recent advancements in deep learning have
led to the rapid development of approaches that integrate multi-sensory
measurements to enhance perception capabilities. This paper surveys the latest
deep learning integration techniques applied to the perception module in
autonomous driving systems, categorizing integration approaches based on &quot;what,
how, and when to integrate&quot;. A new taxonomy of integration is proposed, based
on three dimensions: multi-view, multi-modality, and multi-frame. The
integration operations and their pros and cons are summarized, providing new
insights into the properties of an &quot;ideal&quot; data integration approach that can
alleviate the limitations of existing methods. After reviewing hundreds of
relevant papers, this survey concludes with a discussion of the key features of
an optimal data integration approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Likang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Caifa Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiya Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yue Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14824">
<title>Kosmos-2: Grounding Multimodal Large Language Models to the World. (arXiv:2306.14824v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14824</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Kosmos-2, a Multimodal Large Language Model (MLLM), enabling new
capabilities of perceiving object descriptions (e.g., bounding boxes) and
grounding text to the visual world. Specifically, we represent refer
expressions as links in Markdown, i.e., ``[text span](bounding boxes)&apos;&apos;, where
object descriptions are sequences of location tokens. Together with multimodal
corpora, we construct large-scale data of grounded image-text pairs (called
GrIT) to train the model. In addition to the existing capabilities of MLLMs
(e.g., perceiving general modalities, following instructions, and performing
in-context learning), Kosmos-2 integrates the grounding capability into
downstream applications. We evaluate Kosmos-2 on a wide range of tasks,
including (i) multimodal grounding, such as referring expression comprehension,
and phrase grounding, (ii) multimodal referring, such as referring expression
generation, (iii) perception-language tasks, and (iv) language understanding
and generation. This work lays out the foundation for the development of
Embodiment AI and sheds light on the big convergence of language, multimodal
perception, action, and world modeling, which is a key step toward artificial
general intelligence. Code and pretrained models are available at
https://aka.ms/kosmos-2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhiliang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Li Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yaru Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaohan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01844">
<title>Advancing Wound Filling Extraction on 3D Faces: Auto-Segmentation and Wound Face Regeneration Approach. (arXiv:2307.01844v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01844</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial wound segmentation plays a crucial role in preoperative planning and
optimizing patient outcomes in various medical applications. In this paper, we
propose an efficient approach for automating 3D facial wound segmentation using
a two-stream graph convolutional network. Our method leverages the Cir3D-FaIR
dataset and addresses the challenge of data imbalance through extensive
experimentation with different loss functions. To achieve accurate
segmentation, we conducted thorough experiments and selected a high-performing
model from the trained models. The selected model demonstrates exceptional
segmentation performance for complex 3D facial wounds. Furthermore, based on
the segmentation model, we propose an improved approach for extracting 3D
facial wound fillers and compare it to the results of the previous study. Our
method achieved a remarkable accuracy of 0.9999986\% on the test suite,
surpassing the performance of the previous method. From this result, we use 3D
printing technology to illustrate the shape of the wound filling. The outcomes
of this study have significant implications for physicians involved in
preoperative planning and intervention design. By automating facial wound
segmentation and improving the accuracy of wound-filling extraction, our
approach can assist in carefully assessing and optimizing interventions,
leading to enhanced patient outcomes. Additionally, it contributes to advancing
facial reconstruction techniques by utilizing machine learning and 3D
bioprinting for printing skin tissue implants. Our source code is available at
\url{https://github.com/SIMOGroup/WoundFilling3D}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Q. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thinh D. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong D. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Nga T.K. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Xuan_H/0/1/0/all/0/1&quot;&gt;H. Nguyen-Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02698">
<title>Applying a Color Palette with Local Control using Diffusion Models. (arXiv:2307.02698v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02698</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate two novel editing procedures in the context of fantasy card
art. Palette transfer applies a specified reference palette to a given card.
For fantasy art, the desired change in palette can be very large, leading to
huge changes in the &quot;look&quot; of the art. We demonstrate that a pipeline of vector
quantization; matching; and &quot;vector dequantization&quot; (using a diffusion model)
produces successful extreme palette transfers. Segment control allows an artist
to move one or more image segments, and to optionally specify the desired color
of the result. The combination of these two types of edit yields valuable
workflows, including: move a segment, then recolor; recolor, then force some
segments to take a prescribed color. We demonstrate our methods on the
challenging Yu-Gi-Oh card art dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1&quot;&gt;Vaibhav Vavilala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;David Forsyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03398">
<title>Beyond Geo-localization: Fine-grained Orientation of Street-view Images by Cross-view Matching with Satellite Imagery with Supplementary Materials. (arXiv:2307.03398v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03398</link>
<description rdf:parseType="Literal">&lt;p&gt;Street-view imagery provides us with novel experiences to explore different
places remotely. Carefully calibrated street-view images (e.g. Google Street
View) can be used for different downstream tasks, e.g. navigation, map features
extraction. As personal high-quality cameras have become much more affordable
and portable, an enormous amount of crowdsourced street-view images are
uploaded to the internet, but commonly with missing or noisy sensor
information. To prepare this hidden treasure for &quot;ready-to-use&quot; status,
determining missing location information and camera orientation angles are two
equally important tasks. Recent methods have achieved high performance on
geo-localization of street-view images by cross-view matching with a pool of
geo-referenced satellite imagery. However, most of the existing works focus
more on geo-localization than estimating the image orientation. In this work,
we re-state the importance of finding fine-grained orientation for street-view
images, formally define the problem and provide a set of evaluation metrics to
assess the quality of the orientation estimation. We propose two methods to
improve the granularity of the orientation estimation, achieving 82.4% and
72.3% accuracy for images with estimated angle errors below 2 degrees for CVUSA
and CVACT datasets, corresponding to 34.9% and 28.2% absolute improvement
compared to previous works. Integrating fine-grained orientation estimation in
training also improves the performance on geo-localization, giving top 1 recall
95.5%/85.5% and 86.8%/80.4% for orientation known/unknown tests on the two
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenmiao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yifang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Georgescu_A/0/1/0/all/0/1&quot;&gt;Andrei Georgescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_A/0/1/0/all/0/1&quot;&gt;An Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruppa_H/0/1/0/all/0/1&quot;&gt;Hannes Kruppa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_S/0/1/0/all/0/1&quot;&gt;See-Kiong Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zimmermann_R/0/1/0/all/0/1&quot;&gt;Roger Zimmermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03847">
<title>Blocks2World: Controlling Realistic Scenes with Editable Primitives. (arXiv:2307.03847v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03847</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Blocks2World, a novel method for 3D scene rendering and editing
that leverages a two-step process: convex decomposition of images and
conditioned synthesis. Our technique begins by extracting 3D parallelepipeds
from various objects in a given scene using convex decomposition, thus
obtaining a primitive representation of the scene. These primitives are then
utilized to generate paired data through simple ray-traced depth maps. The next
stage involves training a conditioned model that learns to generate images from
the 2D-rendered convex primitives. This step establishes a direct mapping
between the 3D model and its 2D representation, effectively learning the
transition from a 3D model to an image. Once the model is fully trained, it
offers remarkable control over the synthesis of novel and edited scenes. This
is achieved by manipulating the primitives at test time, including translating
or adding them, thereby enabling a highly customizable scene rendering process.
Our method provides a fresh perspective on 3D scene rendering and editing,
offering control and flexibility. It opens up new avenues for research and
applications in the field, including authoring and data augmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1&quot;&gt;Vaibhav Vavilala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Seemandhar Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasanth_R/0/1/0/all/0/1&quot;&gt;Rahul Vasanth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1&quot;&gt;Anand Bhattad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;David Forsyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04246">
<title>Convex Decomposition of Indoor Scenes. (arXiv:2307.04246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a method to parse a complex, cluttered indoor scene into
primitives which offer a parsimonious abstraction of scene structure. Our
primitives are simple convexes. Our method uses a learned regression procedure
to parse a scene into a fixed number of convexes from RGBD input, and can
optionally accept segmentations to improve the decomposition. The result is
then polished with a descent method which adjusts the convexes to produce a
very good fit, and greedily removes superfluous primitives. Because the entire
scene is parsed, we can evaluate using traditional depth, normal, and
segmentation error metrics. Our evaluation procedure demonstrates that the
error from our primitive representation is comparable to that of predicting
depth from a single image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1&quot;&gt;Vaibhav Vavilala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;David Forsyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05201">
<title>The Staged Knowledge Distillation in Video Classification: Harmonizing Student Progress by a Complementary Weakly Supervised Framework. (arXiv:2307.05201v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05201</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of label-efficient learning on video data, the distillation
method and the structural design of the teacher-student architecture have a
significant impact on knowledge distillation. However, the relationship between
these factors has been overlooked in previous research. To address this gap, we
propose a new weakly supervised learning framework for knowledge distillation
in video classification that is designed to improve the efficiency and accuracy
of the student model. Our approach leverages the concept of substage-based
learning to distill knowledge based on the combination of student substages and
the correlation of corresponding substages. We also employ the progressive
cascade training method to address the accuracy loss caused by the large
capacity gap between the teacher and the student. Additionally, we propose a
pseudo-label optimization strategy to improve the initial data label. To
optimize the loss functions of different distillation substages during the
training process, we introduce a new loss method based on feature distribution.
We conduct extensive experiments on both real and simulated data sets,
demonstrating that our proposed approach outperforms existing distillation
methods in terms of knowledge distillation for video classification tasks. Our
proposed substage-based distillation approach has the potential to inform
future research on label-efficient learning for video data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zheng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05766">
<title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05766</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. We will make all annotations and
our code for annotation generation, model evaluation, and training publicly
available upon acceptance. Our dataset and code is available at
https://github.com/ChantalMP/Rad-ReStruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1&quot;&gt;Matthias Keicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05845">
<title>PIGEON: Predicting Image Geolocations. (arXiv:2307.05845v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05845</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PIGEON, a multi-task end-to-end system for planet-scale image
geolocalization that achieves state-of-the-art performance on both external
benchmarks and in human evaluation. Our work incorporates semantic geocell
creation with label smoothing, conducts pretraining of a vision transformer on
images with geographic information, and refines location predictions with
ProtoNets across a candidate set of geocells. The contributions of PIGEON are
three-fold: first, we design a semantic geocells creation and splitting
algorithm based on open-source data which can be adapted to any geospatial
dataset. Second, we show the effectiveness of intra-geocell refinement and the
applicability of unsupervised clustering and ProtNets to the task. Finally, we
make our pre-trained CLIP transformer model, StreetCLIP, publicly available for
use in adjacent domains with applications to fighting climate change and urban
and rural scene understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haas_L/0/1/0/all/0/1&quot;&gt;Lukas Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skreta_M/0/1/0/all/0/1&quot;&gt;Michal Skreta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alberti_S/0/1/0/all/0/1&quot;&gt;Silas Alberti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05921">
<title>Reading Radiology Imaging Like The Radiologist. (arXiv:2307.05921v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05921</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated radiology report generation aims to generate radiology reports that
contain rich, fine-grained descriptions of radiology imaging. Compared with
image captioning in the natural image domain, medical images are very similar
to each other, with only minor differences in the occurrence of diseases. Given
the importance of these minor differences in the radiology report, it is
crucial to encourage the model to focus more on the subtle regions of disease
occurrence. Secondly, the problem of visual and textual data biases is serious.
Not only do normal cases make up the majority of the dataset, but sentences
describing areas with pathological changes also constitute only a small part of
the paragraph. Lastly, generating medical image reports involves the challenge
of long text generation, which requires more expertise and empirical training
in medical knowledge. As a result, the difficulty of generating such reports is
increased. To address these challenges, we propose a disease-oriented retrieval
framework that utilizes similar reports as prior knowledge references. We
design a factual consistency captioning generator to generate more accurate and
factually consistent disease descriptions. Our framework can find most similar
reports for a given disease from the CXR database by retrieving a
disease-oriented mask consisting of the position and morphological
characteristics. By referencing the disease-oriented similar report and the
visual features, the factual consistency model can generate a more accurate
radiology report.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06065">
<title>Operational Support Estimator Networks. (arXiv:2307.06065v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06065</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a novel approach called Operational Support
Estimator Networks (OSENs) for the support estimation task. Support Estimation
(SE) is defined as finding the locations of non-zero elements in a sparse
signal. By its very nature, the mapping between the measurement and sparse
signal is a non-linear operation. Traditional support estimators rely on
computationally expensive iterative signal recovery techniques to achieve such
non-linearity. Contrary to the convolution layers, the proposed OSEN approach
consists of operational layers that can learn such complex non-linearities
without the need for deep networks. In this way, the performance of the
non-iterative support estimation is greatly improved. Moreover, the operational
layers comprise so-called generative \textit{super neurons} with non-local
kernels. The kernel location for each neuron/feature map is optimized jointly
for the SE task during the training. We evaluate the OSENs in three different
applications: i. support estimation from Compressive Sensing (CS) measurements,
ii. representation-based classification, and iii. learning-aided CS
reconstruction where the output of OSENs is used as prior knowledge to the CS
algorithm for an enhanced reconstruction. Experimental results show that the
proposed approach achieves computational efficiency and outperforms competing
methods, especially at low measurement rates by a significant margin. The
software implementation is publicly shared at
https://github.com/meteahishali/OSEN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahishali_M/0/1/0/all/0/1&quot;&gt;Mete Ahishali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamac_M/0/1/0/all/0/1&quot;&gt;Mehmet Yamac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiranyaz_S/0/1/0/all/0/1&quot;&gt;Serkan Kiranyaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbouj_M/0/1/0/all/0/1&quot;&gt;Moncef Gabbouj&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>