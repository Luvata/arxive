<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06167" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06175" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06199" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06293" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06394" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09910" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15632" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.08535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01055" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06102" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02877" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.06137">
<title>QuasiNet: a neural network with trainable product layers. (arXiv:2401.06137v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.06137</link>
<description rdf:parseType="Literal">&lt;p&gt;Classical neural networks achieve only limited convergence in hard problems
such as XOR or parity when the number of hidden neurons is small. With the
motivation to improve the success rate of neural networks in these problems, we
propose a new neural network model inspired by existing neural network models
with so called product neurons and a learning rule derived from classical error
backpropagation, which elegantly solves the problem of mutually exclusive
situations. Unlike existing product neurons, which have weights that are preset
and not adaptable, our product layers of neurons also do learn. We tested the
model and compared its success rate to a classical multilayer perceptron in the
aforementioned problems as well as in other hard problems such as the two
spirals. Our results indicate that our model is clearly more successful than
the classical MLP and has the potential to be used in many tasks and
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinovska_K/0/1/0/all/0/1&quot;&gt;Krist&amp;#xed;na Malinovsk&amp;#xe1;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holenda_S/0/1/0/all/0/1&quot;&gt;Slavom&amp;#xed;r Holenda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malinovsky_L/0/1/0/all/0/1&quot;&gt;&amp;#x13d;udov&amp;#xed;t Malinovsk&amp;#xfd;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06143">
<title>Redefining Recon: Bridging Gaps with UAVs, 360 degree Cameras, and Neural Radiance Fields. (arXiv:2401.06143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06143</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of digital situational awareness during disaster situations,
accurate digital representations, like 3D models, play an indispensable role.
To ensure the safety of rescue teams, robotic platforms are often deployed to
generate these models. In this paper, we introduce an innovative approach that
synergizes the capabilities of compact Unmaned Arial Vehicles (UAVs), smaller
than 30 cm, equipped with 360 degree cameras and the advances of Neural
Radiance Fields (NeRFs). A NeRF, a specialized neural network, can deduce a 3D
representation of any scene using 2D images and then synthesize it from various
angles upon request. This method is especially tailored for urban environments
which have experienced significant destruction, where the structural integrity
of buildings is compromised to the point of barring entry-commonly observed
post-earthquakes and after severe fires. We have tested our approach through
recent post-fire scenario, underlining the efficacy of NeRFs even in
challenging outdoor environments characterized by water, snow, varying light
conditions, and reflective surfaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surmann_H/0/1/0/all/0/1&quot;&gt;Hartmut Surmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Digakis_N/0/1/0/all/0/1&quot;&gt;Niklas Digakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kremer_J/0/1/0/all/0/1&quot;&gt;Jan-Nicklas Kremer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meine_J/0/1/0/all/0/1&quot;&gt;Julien Meine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulte_M/0/1/0/all/0/1&quot;&gt;Max Schulte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voigt_N/0/1/0/all/0/1&quot;&gt;Niklas Voigt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06148">
<title>Artificial Intelligence for Digital and Computational Pathology. (arXiv:2401.06148v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06148</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in digitizing tissue slides and the fast-paced progress in
artificial intelligence, including deep learning, have boosted the field of
computational pathology. This field holds tremendous potential to automate
clinical diagnosis, predict patient prognosis and response to therapy, and
discover new morphological biomarkers from tissue images. Some of these
artificial intelligence-based systems are now getting approved to assist
clinical diagnosis; however, technical barriers remain for their widespread
clinical adoption and integration as a research tool. This Review consolidates
recent methodological advances in computational pathology for predicting
clinical end points in whole-slide images and highlights how these developments
enable the automation of clinical practice and the discovery of new biomarkers.
We then provide future perspectives as the field expands into a broader range
of clinical and research tasks with increasingly diverse modalities of clinical
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_A/0/1/0/all/0/1&quot;&gt;Andrew H. Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaume_G/0/1/0/all/0/1&quot;&gt;Guillaume Jaume&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Williamson_D/0/1/0/all/0/1&quot;&gt;Drew F.K. Williamson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Ming Y. Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vaidya_A/0/1/0/all/0/1&quot;&gt;Anurag Vaidya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miller_T/0/1/0/all/0/1&quot;&gt;Tiffany R. Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06150">
<title>D-STGCNT: A Dense Spatio-Temporal Graph Conv-GRU Network based on transformer for assessment of patient physical rehabilitation. (arXiv:2401.06150v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.06150</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper tackles the challenge of automatically assessing physical
rehabilitation exercises for patients who perform the exercises without
clinician supervision. The objective is to provide a quality score to ensure
correct performance and achieve desired results. To achieve this goal, a new
graph-based model, the Dense Spatio-Temporal Graph Conv-GRU Network with
Transformer, is introduced. This model combines a modified version of STGCN and
transformer architectures for efficient handling of spatio-temporal data. The
key idea is to consider skeleton data respecting its non-linear structure as a
graph and detecting joints playing the main role in each rehabilitation
exercise. Dense connections and GRU mechanisms are used to rapidly process
large 3D skeleton inputs and effectively model temporal dynamics. The
transformer encoder&apos;s attention mechanism focuses on relevant parts of the
input sequence, making it useful for evaluating rehabilitation exercises. The
evaluation of our proposed approach on the KIMORE and UI-PRMD datasets
highlighted its potential, surpassing state-of-the-art methods in terms of
accuracy and computational time. This resulted in faster and more accurate
learning and assessment of rehabilitation exercises. Additionally, our model
provides valuable feedback through qualitative illustrations, effectively
highlighting the significance of joints in specific exercises.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mourchid_Y/0/1/0/all/0/1&quot;&gt;Youssef Mourchid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Slama_R/0/1/0/all/0/1&quot;&gt;Rim Slama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06151">
<title>Towards Joint Sequence-Structure Generation of Nucleic Acid and Protein Complexes with SE(3)-Discrete Diffusion. (arXiv:2401.06151v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.06151</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models of macromolecules carry abundant and impactful implications
for industrial and biomedical efforts in protein engineering. However, existing
methods are currently limited to modeling protein structures or sequences,
independently or jointly, without regard to the interactions that commonly
occur between proteins and other macromolecules. In this work, we introduce
MMDiff, a generative model that jointly designs sequences and structures of
nucleic acid and protein complexes, independently or in complex, using joint
SE(3)-discrete diffusion noise. Such a model has important implications for
emerging areas of macromolecular design including structure-based transcription
factor design and design of noncoding RNA sequences. We demonstrate the utility
of MMDiff through a rigorous new design benchmark for macromolecular complex
generation that we introduce in this work. Our results demonstrate that MMDiff
is able to successfully generate micro-RNA and single-stranded DNA molecules
while being modestly capable of joint modeling DNA and RNA molecules in
interaction with multi-chain protein complexes. Source code:
https://github.com/Profluent-Internships/MMDiff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Morehead_A/0/1/0/all/0/1&quot;&gt;Alex Morehead&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ruffolo_J/0/1/0/all/0/1&quot;&gt;Jeffrey Ruffolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bhatnagar_A/0/1/0/all/0/1&quot;&gt;Aadyot Bhatnagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Madani_A/0/1/0/all/0/1&quot;&gt;Ali Madani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06153">
<title>Multi-Modal Optimization with k-Cluster Big Bang-Big Crunch Algorithm. (arXiv:2401.06153v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.06153</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal optimization is often encountered in engineering problems,
especially when different and alternative solutions are sought. Evolutionary
algorithms can efficiently tackle multi-modal optimization thanks to their
features such as the concept of population, exploration/exploitation, and being
suitable for parallel computation.
&lt;/p&gt;
&lt;p&gt;This paper introduces a multi-modal optimization version of the Big Bang-Big
Crunch algorithm based on clustering, namely, k-BBBC. This algorithm guarantees
a complete convergence of the entire population, retrieving on average the 99\%
of local optima for a specific problem. Additionally, we introduce two
post-processing methods to (i) identify the local optima in a set of retrieved
solutions (i.e., a population), and (ii) quantify the number of correctly
retrieved optima against the expected ones (i.e., success rate).
&lt;/p&gt;
&lt;p&gt;Our results show that k-BBBC performs well even with problems having a large
number of optima (tested on 379 optima) and high dimensionality (tested on 32
decision variables). When compared to other multi-modal optimization methods,
it outperforms them in terms of accuracy (in both search and objective space)
and success rate (number of correctly retrieved optima) -- especially when
elitism is applied. Lastly, we validated our proposed post-processing methods
by comparing their success rate to the actual one. Results suggest that these
methods can be used to evaluate the performance of a multi-modal optimization
algorithm by correctly identifying optima and providing an indication of
success -- without the need to know where the optima are located in the search
space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yenin_K/0/1/0/all/0/1&quot;&gt;Kemal Erdem Yenin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayin_R/0/1/0/all/0/1&quot;&gt;Reha Oguz Sayin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arar_K/0/1/0/all/0/1&quot;&gt;Kuzey Arar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atalay_K/0/1/0/all/0/1&quot;&gt;Kadir Kaan Atalay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stroppa_F/0/1/0/all/0/1&quot;&gt;Fabio Stroppa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06157">
<title>UDEEP: Edge-based Computer Vision for In-Situ Underwater Crayfish and Plastic Detection. (arXiv:2401.06157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06157</link>
<description rdf:parseType="Literal">&lt;p&gt;Invasive signal crayfish have a detrimental impact on ecosystems. They spread
the fungal-type crayfish plague disease (Aphanomyces astaci) that is lethal to
the native white clawed crayfish, the only native crayfish species in Britain.
Invasive signal crayfish extensively burrow, causing habitat destruction,
erosion of river banks and adverse changes in water quality, while also
competing with native species for resources and leading to declines in native
populations. Moreover, pollution exacerbates the vulnerability of White-clawed
crayfish, with their populations declining by over 90% in certain English
counties, making them highly susceptible to extinction. To safeguard aquatic
ecosystems, it is imperative to address the challenges posed by invasive
species and discarded plastics in the United Kingdom&apos;s river ecosystem&apos;s. The
UDEEP platform can play a crucial role in environmental monitoring by
performing on-the-fly classification of Signal crayfish and plastic debris
while leveraging the efficacy of AI, IoT devices and the power of edge
computing (i.e., NJN). By providing accurate data on the presence, spread and
abundance of these species, the UDEEP platform can contribute to monitoring
efforts and aid in mitigating the spread of invasive species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monari_D/0/1/0/all/0/1&quot;&gt;Dennis Monari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larkin_J/0/1/0/all/0/1&quot;&gt;Jack Larkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_P/0/1/0/all/0/1&quot;&gt;Pedro Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bird_J/0/1/0/all/0/1&quot;&gt;Jordan J. Bird&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ihianle_I/0/1/0/all/0/1&quot;&gt;Isibor Kennedy Ihianle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yahaya_S/0/1/0/all/0/1&quot;&gt;Salisu Wada Yahaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tash_F/0/1/0/all/0/1&quot;&gt;Farhad Fassihi Tash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Mahmudul Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotfi_A/0/1/0/all/0/1&quot;&gt;Ahmad Lotfi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06160">
<title>Future-proofing Education: A Prototype for Simulating Oral Examinations Using Large Language Models. (arXiv:2401.06160v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.06160</link>
<description rdf:parseType="Literal">&lt;p&gt;This study explores the impact of Large Language Models (LLMs) in higher
education, focusing on an automated oral examination simulation using a
prototype. The design considerations of the prototype are described, and the
system is evaluated with a select group of educators and students. Technical
and pedagogical observations are discussed. The prototype proved to be
effective in simulating oral exams, providing personalized feedback, and
streamlining educators&apos; workloads. The promising results of the prototype show
the potential for LLMs in democratizing education, inclusion of diverse student
populations, and improvement of teaching quality and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nitze_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Nitze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06161">
<title>Trustworthy human-centric based Automated Decision-Making Systems. (arXiv:2401.06161v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.06161</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated Decision-Making Systems (ADS) have become pervasive across various
fields, activities, and occupations, to enhance performance. However, this
widespread adoption introduces potential risks, including the misuse of ADS.
Such misuse may manifest when ADS is employed in situations where it is
unnecessary or when essential requirements, conditions, and terms are
overlooked, leading to unintended consequences. This research paper presents a
thorough examination of the implications, distinctions, and ethical
considerations associated with digitalization, digital transformation, and the
utilization of ADS in contemporary society and future contexts. Emphasis is
placed on the imperative need for regulation, transparency, and ethical conduct
in the deployment of ADS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabrera_M/0/1/0/all/0/1&quot;&gt;Marcelino Cabrera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_C/0/1/0/all/0/1&quot;&gt;Carlos Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Novoa_Hernandez_P/0/1/0/all/0/1&quot;&gt;Pavel Novoa-Hern&amp;#xe1;ndez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelta_D/0/1/0/all/0/1&quot;&gt;David A. Pelta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verdegay_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Luis Verdegay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06162">
<title>A debiasing technique for place-based algorithmic patrol management. (arXiv:2401.06162v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.06162</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been a revolution in data-driven policing. With
that has come scrutiny on how bias in historical data affects algorithmic
decision making. In this exploratory work, we introduce a debiasing technique
for place-based algorithmic patrol management systems. We show that the
technique efficiently eliminates racially biased features while retaining high
accuracy in the models. Finally, we provide a lengthy list of potential future
research in the realm of fairness and data-driven policing which this work
uncovered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Einarsson_A/0/1/0/all/0/1&quot;&gt;Alexander Einarsson&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oestmo_S/0/1/0/all/0/1&quot;&gt;Simen Oestmo&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wollman_L/0/1/0/all/0/1&quot;&gt;Lester Wollman&lt;/a&gt; (2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purves_D/0/1/0/all/0/1&quot;&gt;Duncan Purves&lt;/a&gt; (3), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenkins_R/0/1/0/all/0/1&quot;&gt;Ryan Jenkins&lt;/a&gt; (4) ((1) Northwestern University (2) SoundThinking Inc. (3) University of Florida (4) California Polytechnic State University)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06166">
<title>Adjustable Molecular Representation for Unified Pre-training Strategy. (arXiv:2401.06166v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.06166</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new large-scale molecular model, named AdaMR, which stands for
Adjustable Molecular Representation for Unified Pre-training Strategy. Unlike
recent large-scale molecular models that use a single molecular encoding, AdaMR
employs a granularity-adjustable molecular encoder, learning molecular
representations at both the atomic and substructure levels. For the
pre-training process, we designed a task for molecular canonicalization, which
involves transforming ltiple generic molecular representations into canonical
representations. By adjusting the granularity of molecular encoding, the
trained model can improve the effects on multiple downstream tasks, such as
model attribute prediction and molecule generation. Substructure-level
molecular representation retains information of specific atom groups or
arrangements that determine chemical properties and have similar functions,
which is beneficial for tasks like property prediction. Meanwhile, atomic-level
representation, combined with generative molecular canonicalization
pre-training tasks, enhances the validity, novelty, and uniqueness in
generative tasks. These features of AdaMR demonstrate its strong performance in
numerous downstream tasks. We use different molecular properties prediction
tasks on six different datasets on MoleculeNet and two generative tasks on
ZINC250K dataset to evaluate our proposed molecular encoding and pre-training
methods, and obtain state-of-the-art (SOTA) results on five of these tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zeliang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruyi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gu_Z/0/1/0/all/0/1&quot;&gt;Zhongze Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06167">
<title>Enhancing Multimodal Understanding with CLIP-Based Image-to-Text Transformation. (arXiv:2401.06167v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06167</link>
<description rdf:parseType="Literal">&lt;p&gt;The process of transforming input images into corresponding textual
explanations stands as a crucial and complex endeavor within the domains of
computer vision and natural language processing. In this paper, we propose an
innovative ensemble approach that harnesses the capabilities of Contrastive
Language-Image Pretraining models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1&quot;&gt;Chang Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qunwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiaxin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Liqiang Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06168">
<title>A Survey on Game Theory Optimal Poker. (arXiv:2401.06168v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2401.06168</link>
<description rdf:parseType="Literal">&lt;p&gt;Poker is in the family of imperfect information games unlike other games such
as chess, connect four, etc which are perfect information game instead. While
many perfect information games have been solved, no non-trivial imperfect
information game has been solved to date. This makes poker a great test bed for
Artificial Intelligence research. In this paper we firstly compare Game theory
optimal poker to Exploitative poker. Secondly, we discuss the intricacies of
abstraction techniques, betting models, and specific strategies employed by
successful poker bots like Tartanian[1] and Pluribus[6]. Thirdly, we also
explore 2-player vs multi-player games and the limitations that come when
playing with more players. Finally, this paper discusses the role of machine
learning and theoretical approaches in developing winning strategies and
suggests future directions for this rapidly evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonawane_P/0/1/0/all/0/1&quot;&gt;Prathamesh Sonawane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chheda_A/0/1/0/all/0/1&quot;&gt;Arav Chheda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06171">
<title>Harnessing Artificial Intelligence for Sustainable Agricultural Development in Africa: Opportunities, Challenges, and Impact. (arXiv:2401.06171v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.06171</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the transformative potential of artificial intelligence
(AI) in the context of sustainable agricultural development across diverse
regions in Africa. Delving into opportunities, challenges, and impact, the
study navigates through the dynamic landscape of AI applications in
agriculture. Opportunities such as precision farming, crop monitoring, and
climate-resilient practices are examined, alongside challenges related to
technological infrastructure, data accessibility, and skill gaps. The article
analyzes the impact of AI on smallholder farmers, supply chains, and inclusive
growth. Ethical considerations and policy implications are also discussed,
offering insights into responsible AI integration. By providing a nuanced
understanding, this paper contributes to the ongoing discourse on leveraging AI
for fostering sustainability in African agriculture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gikunda_K/0/1/0/all/0/1&quot;&gt;Kinyua Gikunda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06175">
<title>MTAD: Tools and Benchmarks for Multivariate Time Series Anomaly Detection. (arXiv:2401.06175v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.06175</link>
<description rdf:parseType="Literal">&lt;p&gt;Key Performance Indicators (KPIs) are essential time-series metrics for
ensuring the reliability and stability of many software systems. They
faithfully record runtime states to facilitate the understanding of anomalous
system behaviors and provide informative clues for engineers to pinpoint the
root causes. The unprecedented scale and complexity of modern software systems,
however, make the volume of KPIs explode. Consequently, many traditional
methods of KPI anomaly detection become impractical, which serves as a catalyst
for the fast development of machine learning-based solutions in both academia
and industry. However, there is currently a lack of rigorous comparison among
these KPI anomaly detection methods, and re-implementation demands a
non-trivial effort. Moreover, we observe that different works adopt independent
evaluation processes with different metrics. Some of them may not fully reveal
the capability of a model and some are creating an illusion of progress. To
better understand the characteristics of different KPI anomaly detectors and
address the evaluation issue, in this paper, we provide a comprehensive review
and evaluation of twelve state-of-the-art methods, and propose a novel metric
called salience. Particularly, the selected methods include five traditional
machine learning-based methods and seven deep learning-based methods. These
methods are evaluated with five multivariate KPI datasets that are publicly
available. A unified toolkit with easy-to-use interfaces is also released. We
report the benchmark results in terms of accuracy, salience, efficiency, and
delay, which are of practical importance for industrial deployment. We believe
our work can contribute as a basis for future academic research and industrial
application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Wenwei Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuangbin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yichen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yuxin Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_M/0/1/0/all/0/1&quot;&gt;Michael R. Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06176">
<title>GOODAT: Towards Test-time Graph Out-of-Distribution Detection. (arXiv:2401.06176v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06176</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have found widespread application in modeling
graph data across diverse domains. While GNNs excel in scenarios where the
testing data shares the distribution of their training counterparts (in
distribution, ID), they often exhibit incorrect predictions when confronted
with samples from an unfamiliar distribution (out-of-distribution, OOD). To
identify and reject OOD samples with GNNs, recent studies have explored graph
OOD detection, often focusing on training a specific model or modifying the
data on top of a well-trained GNN. Despite their effectiveness, these methods
come with heavy training resources and costs, as they need to optimize the
GNN-based models on training data. Moreover, their reliance on modifying the
original GNNs and accessing training data further restricts their universality.
To this end, this paper introduces a method to detect Graph Out-of-Distribution
At Test-time (namely GOODAT), a data-centric, unsupervised, and plug-and-play
solution that operates independently of training data and modifications of GNN
architecture. With a lightweight graph masker, GOODAT can learn informative
subgraphs from test samples, enabling the capture of distinct graph patterns
between OOD and ID samples. To optimize the graph masker, we meticulously
design three unsupervised objective functions based on the graph information
bottleneck principle, motivating the masker to capture compact yet informative
subgraphs for OOD detection. Comprehensive evaluations confirm that our GOODAT
method outperforms state-of-the-art benchmarks across a variety of real-world
datasets. The code is available at Github: https://github.com/Ee1s/GOODAT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Luzhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Dongxiao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;He Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;Di Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06178">
<title>AI Art is Theft: Labour, Extraction, and Exploitation, Or, On the Dangers of Stochastic Pollocks. (arXiv:2401.06178v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.06178</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the launch of applications such as DALL-E, Midjourney, and Stable
Diffusion, generative artificial intelligence has been controversial as a tool
for creating artwork. While some have presented longtermist worries about these
technologies as harbingers of fully automated futures to come, more pressing is
the impact of generative AI on creative labour in the present. Already,
business leaders have begun replacing human artistic labour with AI-generated
images. In response, the artistic community has launched a protest movement,
which argues that AI image generation is a kind of theft. This paper analyzes,
substantiates, and critiques these arguments, concluding that AI image
generators involve an unethical kind of labour theft. If correct, many other AI
applications also rely upon theft.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goetze_T/0/1/0/all/0/1&quot;&gt;Trystan S. Goetze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06183">
<title>End to end Hindi to English speech conversion using Bark, mBART and a finetuned XLSR Wav2Vec2. (arXiv:2401.06183v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.06183</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech has long been a barrier to effective communication and connection,
persisting as a challenge in our increasingly interconnected world. This
research paper introduces a transformative solution to this persistent obstacle
an end-to-end speech conversion framework tailored for Hindi-to-English
translation, culminating in the synthesis of English audio. By integrating
cutting-edge technologies such as XLSR Wav2Vec2 for automatic speech
recognition (ASR), mBART for neural machine translation (NMT), and a
Text-to-Speech (TTS) synthesis component, this framework offers a unified and
seamless approach to cross-lingual communication. We delve into the intricate
details of each component, elucidating their individual contributions and
exploring the synergies that enable a fluid transition from spoken Hindi to
synthesized English audio.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tathe_A/0/1/0/all/0/1&quot;&gt;Aniket Tathe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamble_A/0/1/0/all/0/1&quot;&gt;Anand Kamble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumbharkar_S/0/1/0/all/0/1&quot;&gt;Suyash Kumbharkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhandare_A/0/1/0/all/0/1&quot;&gt;Atharva Bhandare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitra_A/0/1/0/all/0/1&quot;&gt;Anirban C. Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06194">
<title>CrisisKAN: Knowledge-infused and Explainable Multimodal Attention Network for Crisis Event Classification. (arXiv:2401.06194v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06194</link>
<description rdf:parseType="Literal">&lt;p&gt;Pervasive use of social media has become the emerging source for real-time
information (like images, text, or both) to identify various events. Despite
the rapid growth of image and text-based event classification, the
state-of-the-art (SOTA) models find it challenging to bridge the semantic gap
between features of image and text modalities due to inconsistent encoding.
Also, the black-box nature of models fails to explain the model&apos;s outcomes for
building trust in high-stakes situations such as disasters, pandemic.
Additionally, the word limit imposed on social media posts can potentially
introduce bias towards specific events. To address these issues, we proposed
CrisisKAN, a novel Knowledge-infused and Explainable Multimodal Attention
Network that entails images and texts in conjunction with external knowledge
from Wikipedia to classify crisis events. To enrich the context-specific
understanding of textual information, we integrated Wikipedia knowledge using
proposed wiki extraction algorithm. Along with this, a guided cross-attention
module is implemented to fill the semantic gap in integrating visual and
textual data. In order to ensure reliability, we employ a model-specific
approach called Gradient-weighted Class Activation Mapping (Grad-CAM) that
provides a robust explanation of the predictions of the proposed model. The
comprehensive experiments conducted on the CrisisMMD dataset yield in-depth
analysis across various crisis-specific tasks and settings. As a result,
CrisisKAN outperforms existing SOTA methodologies and provides a novel view in
the domain of explainable multimodal event classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shubham Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_N/0/1/0/all/0/1&quot;&gt;Nandini Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kundu_S/0/1/0/all/0/1&quot;&gt;Suman Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_D/0/1/0/all/0/1&quot;&gt;Debasis Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06195">
<title>NeuSpin: Design of a Reliable Edge Neuromorphic System Based on Spintronics for Green AI. (arXiv:2401.06195v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2401.06195</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet of Things (IoT) and smart wearable devices for personalized
healthcare will require storing and computing ever-increasing amounts of data.
The key requirements for these devices are ultra-low-power, high-processing
capabilities, autonomy at low cost, as well as reliability and accuracy to
enable Green AI at the edge. Artificial Intelligence (AI) models, especially
Bayesian Neural Networks (BayNNs) are resource-intensive and face challenges
with traditional computing architectures due to the memory wall problem.
Computing-in-Memory (CIM) with emerging resistive memories offers a solution by
combining memory blocks and computing units for higher efficiency and lower
power consumption. However, implementing BayNNs on CIM hardware, particularly
with spintronic technologies, presents technical challenges due to variability
and manufacturing defects. The NeuSPIN project aims to address these challenges
through full-stack hardware and software co-design, developing novel
algorithmic and circuit design approaches to enhance the performance,
energy-efficiency and robustness of BayNNs on sprintronic-based CIM platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Soyed Tuhin Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danouchi_K/0/1/0/all/0/1&quot;&gt;Kamal Danouchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenat_G/0/1/0/all/0/1&quot;&gt;Guillaume Prenat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anghel_L/0/1/0/all/0/1&quot;&gt;Lorena Anghel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tahoori_M/0/1/0/all/0/1&quot;&gt;Mehdi B. Tahoori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06199">
<title>xTrimoPGLM: Unified 100B-Scale Pre-trained Transformer for Deciphering the Language of Protein. (arXiv:2401.06199v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2401.06199</link>
<description rdf:parseType="Literal">&lt;p&gt;Protein language models have shown remarkable success in learning biological
information from protein sequences. However, most existing models are limited
by either autoencoding or autoregressive pre-training objectives, which makes
them struggle to handle protein understanding and generation tasks
concurrently. We propose a unified protein language model, xTrimoPGLM, to
address these two types of tasks simultaneously through an innovative
pre-training framework. Our key technical contribution is an exploration of the
compatibility and the potential for joint optimization of the two types of
objectives, which has led to a strategy for training xTrimoPGLM at an
unprecedented scale of 100 billion parameters and 1 trillion training tokens.
Our extensive experiments reveal that 1) xTrimoPGLM significantly outperforms
other advanced baselines in 18 protein understanding benchmarks across four
categories. The model also facilitates an atomic-resolution view of protein
structures, leading to an advanced 3D structural prediction model that
surpasses existing language model-based tools. 2) xTrimoPGLM not only can
generate de novo protein sequences following the principles of natural ones,
but also can perform programmable generation after supervised fine-tuning (SFT)
on curated sequences. These results highlight the substantial capability and
versatility of xTrimoPGLM in understanding and generating protein sequences,
contributing to the evolving landscape of foundation models in protein science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xingyi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yangli-ao Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gong_J/0/1/0/all/0/1&quot;&gt;Jing Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bei_Z/0/1/0/all/0/1&quot;&gt;Zhilei Bei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xin Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chiming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Aohan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Le Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06204">
<title>An Exploratory Assessment of LLM&apos;s Potential Toward Flight Trajectory Reconstruction Analysis. (arXiv:2401.06204v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06204</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) hold transformative potential in aviation,
particularly in reconstructing flight trajectories. This paper investigates
this potential, grounded in the notion that LLMs excel at processing sequential
data and deciphering complex data structures. Utilizing the LLaMA 2 model, a
pre-trained open-source LLM, the study focuses on reconstructing flight
trajectories using Automatic Dependent Surveillance-Broadcast (ADS-B) data with
irregularities inherent in real-world scenarios. The findings demonstrate the
model&apos;s proficiency in filtering noise and estimating both linear and curved
flight trajectories. However, the analysis also reveals challenges in managing
longer data sequences, which may be attributed to the token length limitations
of LLM models. The study&apos;s insights underscore the promise of LLMs in flight
trajectory reconstruction and open new avenues for their broader application
across the aviation and transportation sectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qilei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mott_J/0/1/0/all/0/1&quot;&gt;John H. Mott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06210">
<title>Learning Unsupervised Semantic Document Representation for Fine-grained Aspect-based Sentiment Analysis. (arXiv:2401.06210v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06210</link>
<description rdf:parseType="Literal">&lt;p&gt;Document representation is the core of many NLP tasks on machine
understanding. A general representation learned in an unsupervised manner
reserves generality and can be used for various applications. In practice,
sentiment analysis (SA) has been a challenging task that is regarded to be
deeply semantic-related and is often used to assess general representations.
Existing methods on unsupervised document representation learning can be
separated into two families: sequential ones, which explicitly take the
ordering of words into consideration, and non-sequential ones, which do not
explicitly do so. However, both of them suffer from their own weaknesses. In
this paper, we propose a model that overcomes difficulties encountered by both
families of methods. Experiments show that our model outperforms
state-of-the-art methods on popular SA datasets and a fine-grained aspect-based
SA by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hao-Ming Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pu-Jen Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06230">
<title>WISE: full-Waveform variational Inference via Subsurface Extensions. (arXiv:2401.06230v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2401.06230</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a probabilistic technique for full-waveform inversion, employing
variational inference and conditional normalizing flows to quantify uncertainty
in migration-velocity models and its impact on imaging. Our approach integrates
generative artificial intelligence with physics-informed common-image gathers,
reducing reliance on accurate initial velocity models. Considered case studies
demonstrate its efficacy producing realizations of migration-velocity models
conditioned by the data. These models are used to quantify amplitude and
positioning effects during subsequent imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Orozco_R/0/1/0/all/0/1&quot;&gt;Rafael Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Louboutin_M/0/1/0/all/0/1&quot;&gt;Mathias Louboutin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Herrmann_F/0/1/0/all/0/1&quot;&gt;Felix J. Herrmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06256">
<title>A Universal Knowledge Model and Cognitive Architecture for Prototyping AGI. (arXiv:2401.06256v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.06256</link>
<description rdf:parseType="Literal">&lt;p&gt;The article identified 42 cognitive architectures for creating general
artificial intelligence (AGI) and proposed a set of interrelated functional
blocks that an agent approaching AGI in its capabilities should possess. Since
the required set of blocks is not found in any of the existing architectures,
the article proposes a new cognitive architecture for intelligent systems
approaching AGI in their capabilities. As one of the key solutions within the
framework of the architecture, a universal method of knowledge representation
is proposed, which allows combining various non-formalized, partially and fully
formalized methods of knowledge representation in a single knowledge base, such
as texts in natural languages, images, audio and video recordings, graphs,
algorithms, databases, neural networks, knowledge graphs, ontologies, frames,
essence-property-relation models, production systems, predicate calculus
models, conceptual models, and others. To combine and structure various
fragments of knowledge, archigraph models are used, constructed as a
development of annotated metagraphs. As components, the cognitive architecture
being developed includes machine consciousness, machine subconsciousness,
blocks of interaction with the external environment, a goal management block,
an emotional control system, a block of social interaction, a block of
reflection, an ethics block and a worldview block, a learning block, a
monitoring block, blocks of statement and solving problems, self-organization
and meta learning block.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukhobokov_A/0/1/0/all/0/1&quot;&gt;Artem Sukhobokov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belousov_E/0/1/0/all/0/1&quot;&gt;Evgeny Belousov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gromozdov_D/0/1/0/all/0/1&quot;&gt;Danila Gromozdov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenger_A/0/1/0/all/0/1&quot;&gt;Anna Zenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popov_I/0/1/0/all/0/1&quot;&gt;Ilya Popov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06293">
<title>MultiSlot ReRanker: A Generic Model-based Re-Ranking Framework in Recommendation Systems. (arXiv:2401.06293v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.06293</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a generic model-based re-ranking framework,
MultiSlot ReRanker, which simultaneously optimizes relevance, diversity, and
freshness. Specifically, our Sequential Greedy Algorithm (SGA) is efficient
enough (linear time complexity) for large-scale production recommendation
engines. It achieved a lift of $+6\%$ to $ +10\%$ offline Area Under the
receiver operating characteristic Curve (AUC) which is mainly due to explicitly
modeling mutual influences among items of a list, and leveraging the second
pass ranking scores of multiple objectives. In addition, we have generalized
the offline replay theory to multi-slot re-ranking scenarios, with trade-offs
among multiple objectives. The offline replay results can be further improved
by Pareto Optimality. Moreover, we&apos;ve built a multi-slot re-ranking simulator
based on OpenAI Gym integrated with the Ray framework. It can be easily
configured for different assumptions to quickly benchmark both reinforcement
learning and supervised learning algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Q/0/1/0/all/0/1&quot;&gt;Qiang Charles Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muralidharan_A/0/1/0/all/0/1&quot;&gt;Ajith Muralidharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwana_B/0/1/0/all/0/1&quot;&gt;Birjodh Tiwana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Johnson Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borisyuk_F/0/1/0/all/0/1&quot;&gt;Fedor Borisyuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Aman Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodard_D/0/1/0/all/0/1&quot;&gt;Dawn Woodard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06300">
<title>Advantage of Quantum Neural Networks as Quantum Information Decoders. (arXiv:2401.06300v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2401.06300</link>
<description rdf:parseType="Literal">&lt;p&gt;A promising strategy to protect quantum information from noise-induced errors
is to encode it into the low-energy states of a topological quantum memory
device. However, readout errors from such memory under realistic settings is
less understood. We study the problem of decoding quantum information encoded
in the groundspaces of topological stabilizer Hamiltonians in the presence of
generic perturbations, such as quenched disorder. We first prove that the
standard stabilizer-based error correction and decoding schemes work adequately
well in such perturbed quantum codes by showing that the decoding error
diminishes exponentially in the distance of the underlying unperturbed code. We
then prove that Quantum Neural Network (QNN) decoders provide an almost
quadratic improvement on the readout error. Thus, we demonstrate provable
advantage of using QNNs for decoding realistic quantum error-correcting codes,
and our result enables the exploration of a wider range of non-stabilizer codes
in the near-term laboratory settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhong_W/0/1/0/all/0/1&quot;&gt;Weishun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shtanko_O/0/1/0/all/0/1&quot;&gt;Oles Shtanko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Movassagh_R/0/1/0/all/0/1&quot;&gt;Ramis Movassagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06308">
<title>A Semantic-Aware Multiple Access Scheme for Distributed, Dynamic 6G-Based Applications. (arXiv:2401.06308v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.06308</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of the semantic-aware paradigm presents opportunities for
innovative services, especially in the context of 6G-based applications.
Although significant progress has been made in semantic extraction techniques,
the incorporation of semantic information into resource allocation
decision-making is still in its early stages, lacking consideration of the
requirements and characteristics of future systems. In response, this paper
introduces a novel formulation for the problem of multiple access to the
wireless spectrum. It aims to optimize the utilization-fairness trade-off,
using the $\alpha$-fairness metric, while accounting for user data correlation
by introducing the concepts of self- and assisted throughputs. Initially, the
problem is analyzed to identify its optimal solution. Subsequently, a
Semantic-Aware Multi-Agent Double and Dueling Deep Q-Learning (SAMA-D3QL)
technique is proposed. This method is grounded in Model-free Multi-Agent Deep
Reinforcement Learning (MADRL), enabling the user equipment to autonomously
make decisions regarding wireless spectrum access based solely on their local
individual observations. The efficiency of the proposed technique is evaluated
through two scenarios: single-channel and multi-channel. The findings
illustrate that, across a spectrum of $\alpha$ values, association matrices,
and channels, SAMA-D3QL consistently outperforms alternative approaches. This
establishes it as a promising candidate for facilitating the realization of
future federated, dynamically evolving applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazandarani_H/0/1/0/all/0/1&quot;&gt;Hamidreza Mazandarani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shokrnezhad_M/0/1/0/all/0/1&quot;&gt;Masoud Shokrnezhad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taleb_T/0/1/0/all/0/1&quot;&gt;Tarik Taleb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06318">
<title>Striking a Balance in Fairness for Dynamic Systems Through Reinforcement Learning. (arXiv:2401.06318v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06318</link>
<description rdf:parseType="Literal">&lt;p&gt;While significant advancements have been made in the field of fair machine
learning, the majority of studies focus on scenarios where the decision model
operates on a static population. In this paper, we study fairness in dynamic
systems where sequential decisions are made. Each decision may shift the
underlying distribution of features or user behavior. We model the dynamic
system through a Markov Decision Process (MDP). By acknowledging that
traditional fairness notions and long-term fairness are distinct requirements
that may not necessarily align with one another, we propose an algorithmic
framework to integrate various fairness considerations with reinforcement
learning using both pre-processing and in-processing approaches. Three case
studies show that our method can strike a balance between traditional fairness
notions, long-term fairness, and utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yaowei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lear_J/0/1/0/all/0/1&quot;&gt;Jacob Lear&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06340">
<title>A Temporal-Spectral Fusion Transformer with Subject-specific Adapter for Enhancing RSVP-BCI Decoding. (arXiv:2401.06340v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.06340</link>
<description rdf:parseType="Literal">&lt;p&gt;The Rapid Serial Visual Presentation (RSVP)-based Brain-Computer Interface
(BCI) is an efficient technology for target retrieval using
electroencephalography (EEG) signals. The performance improvement of
traditional decoding methods relies on a substantial amount of training data
from new test subjects, which increases preparation time for BCI systems.
Several studies introduce data from existing subjects to reduce the dependence
of performance improvement on data from new subjects, but their optimization
strategy based on adversarial learning with extensive data increases training
time during the preparation procedure. Moreover, most previous methods only
focus on the single-view information of EEG signals, but ignore the information
from other views which may further improve performance. To enhance decoding
performance while reducing preparation time, we propose a Temporal-Spectral
fusion transformer with Subject-specific Adapter (TSformer-SA). Specifically, a
cross-view interaction module is proposed to facilitate information transfer
and extract common representations across two-view features extracted from EEG
temporal signals and spectrogram images. Then, an attention-based fusion module
fuses the features of two views to obtain comprehensive discriminative features
for classification. Furthermore, a multi-view consistency loss is proposed to
maximize the feature similarity between two views of the same EEG signal.
Finally, we propose a subject-specific adapter to rapidly transfer the
knowledge of the model trained on data from existing subjects to decode data
from new subjects. Experimental results show that TSformer-SA significantly
outperforms comparison methods and achieves outstanding performance with
limited training data from new subjects. This facilitates efficient decoding
and rapid deployment of BCI systems in practical use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xujin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_W/0/1/0/all/0/1&quot;&gt;Wei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Shuang Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Huiguang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06370">
<title>Graph Relation Distillation for Efficient Biomedical Instance Segmentation. (arXiv:2401.06370v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06370</link>
<description rdf:parseType="Literal">&lt;p&gt;Instance-aware embeddings predicted by deep neural networks have
revolutionized biomedical instance segmentation, but its resource requirements
are substantial. Knowledge distillation offers a solution by transferring
distilled knowledge from heavy teacher networks to lightweight yet
high-performance student networks. However, existing knowledge distillation
methods struggle to extract knowledge for distinguishing instances and overlook
global relation information. To address these challenges, we propose a graph
relation distillation approach for efficient biomedical instance segmentation,
which considers three essential types of knowledge: instance-level features,
instance relations, and pixel-level boundaries. We introduce two graph
distillation schemes deployed at both the intra-image level and the inter-image
level: instance graph distillation (IGD) and affinity graph distillation (AGD).
IGD constructs a graph representing instance features and relations,
transferring these two types of knowledge by enforcing instance graph
consistency. AGD constructs an affinity graph representing pixel relations to
capture structured knowledge of instance boundaries, transferring
boundary-related knowledge by ensuring pixel affinity consistency. Experimental
results on a number of biomedical datasets validate the effectiveness of our
approach, enabling student models with less than $ 1\%$ parameters and less
than $10\%$ inference time while achieving promising performance compared to
teacher models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yueyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06373">
<title>How Johnny Can Persuade LLMs to Jailbreak Them: Rethinking Persuasion to Challenge AI Safety by Humanizing LLMs. (arXiv:2401.06373v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06373</link>
<description rdf:parseType="Literal">&lt;p&gt;Most traditional AI safety research has approached AI models as machines and
centered on algorithm-focused attacks developed by security experts. As large
language models (LLMs) become increasingly common and competent, non-expert
users can also impose risks during daily interactions. This paper introduces a
new perspective to jailbreak LLMs as human-like communicators, to explore this
overlooked intersection between everyday language interaction and AI safety.
Specifically, we study how to persuade LLMs to jailbreak them. First, we
propose a persuasion taxonomy derived from decades of social science research.
Then, we apply the taxonomy to automatically generate interpretable persuasive
adversarial prompts (PAP) to jailbreak LLMs. Results show that persuasion
significantly increases the jailbreak performance across all risk categories:
PAP consistently achieves an attack success rate of over $92\%$ on Llama 2-7b
Chat, GPT-3.5, and GPT-4 in $10$ trials, surpassing recent algorithm-focused
attacks. On the defense side, we explore various mechanisms against PAP and,
found a significant gap in existing defenses, and advocate for more fundamental
mitigation for highly interactive LLMs
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hongpeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Diyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Ruoxi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weiyan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06375">
<title>Cognitive BPM as an Equalizer: Improving Access and Efficiency for Employees with (and without) Cognitive Disabilities. (arXiv:2401.06375v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.06375</link>
<description rdf:parseType="Literal">&lt;p&gt;We examine ProcessGPT, an AI model designed to automate, augment, and improve
business processes, to study the challenges of managing business processes
within the cognitive limitations of the human workforce, particularly
individuals with cognitive disabilities. ProcessGPT provides a blueprint for
designing efficient business processes that take into account human cognitive
limitations. By viewing this through the lens of cognitive disabilities, we
show that ProcessGPT improves process usability for individuals with and
without cognitive disabilities. We also demonstrate that organizations
implementing ProcessGPT-like capabilities will realize increased productivity,
morale, and inclusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banks_G/0/1/0/all/0/1&quot;&gt;Gordon Banks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bierhuizen_G/0/1/0/all/0/1&quot;&gt;Gates Bierhuizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCrum_K/0/1/0/all/0/1&quot;&gt;Katherine McCrum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wengert_E/0/1/0/all/0/1&quot;&gt;Ellen Wengert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06379">
<title>Vehicle: Bridging the Embedding Gap in the Verification of Neuro-Symbolic Programs. (arXiv:2401.06379v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.06379</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuro-symbolic programs -- programs containing both machine learning
components and traditional symbolic code -- are becoming increasingly
widespread. However, we believe that there is still a lack of a general
methodology for verifying these programs whose correctness depends on the
behaviour of the machine learning components. In this paper, we identify the
``embedding gap&apos;&apos; -- the lack of techniques for linking semantically-meaningful
``problem-space&apos;&apos; properties to equivalent ``embedding-space&apos;&apos; properties -- as
one of the key issues, and describe Vehicle, a tool designed to facilitate the
end-to-end verification of neural-symbolic programs in a modular fashion.
Vehicle provides a convenient language for specifying ``problem-space&apos;&apos;
properties of neural networks and declaring their relationship to the
``embedding-space&quot;, and a powerful compiler that automates interpretation of
these properties in the language of a chosen machine-learning training
environment, neural network verifier, and interactive theorem prover. We
demonstrate Vehicle&apos;s utility by using it to formally verify the safety of a
simple autonomous car equipped with a neural network controller.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daggitt_M/0/1/0/all/0/1&quot;&gt;Matthew L. Daggitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kokke_W/0/1/0/all/0/1&quot;&gt;Wen Kokke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkey_R/0/1/0/all/0/1&quot;&gt;Robert Atkey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slusarz_N/0/1/0/all/0/1&quot;&gt;Natalia Slusarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnaboldi_L/0/1/0/all/0/1&quot;&gt;Luca Arnaboldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komendantskaya_E/0/1/0/all/0/1&quot;&gt;Ekaterina Komendantskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06382">
<title>What should I say? -- Interacting with AI and Natural Language Interfaces. (arXiv:2401.06382v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.06382</link>
<description rdf:parseType="Literal">&lt;p&gt;As Artificial Intelligence (AI) technology becomes more and more prevalent,
it becomes increasingly important to explore how we as humans interact with AI.
The Human-AI Interaction (HAI) sub-field has emerged from the Human-Computer
Interaction (HCI) field and aims to examine this very notion. Many interaction
patterns have been implemented without fully understanding the changes in
required cognition as well as the cognitive science implications of using these
alternative interfaces that aim to be more human-like in nature. Prior research
suggests that theory of mind representations are crucial to successful and
effortless communication, however very little is understood when it comes to
how theory of mind representations are established when interacting with AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adkins_M/0/1/0/all/0/1&quot;&gt;Mark Adkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06394">
<title>Adaptive Data Augmentation for Aspect Sentiment Quad Prediction. (arXiv:2401.06394v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06394</link>
<description rdf:parseType="Literal">&lt;p&gt;Aspect sentiment quad prediction (ASQP) aims to predict the quad sentiment
elements for a given sentence, which is a critical task in the field of
aspect-based sentiment analysis. However, the data imbalance issue has not
received sufficient attention in ASQP task. In this paper, we divide the issue
into two-folds, quad-pattern imbalance and aspect-category imbalance, and
propose an Adaptive Data Augmentation (ADA) framework to tackle the imbalance
issue. Specifically, a data augmentation process with a condition function
adaptively enhances the tail quad patterns and aspect categories, alleviating
the data imbalance in ASQP. Following previous studies, we also further explore
the generative framework for extracting complete quads by introducing the
category prior knowledge and syntax-guided decoding target. Experimental
results demonstrate that data augmentation for imbalance in ASQP task can
improve the performance, and the proposed ADA method is superior to naive data
oversampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinghua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shiyao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuebin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tingwen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06401">
<title>DevEval: Evaluating Code Generation in Practical Software Projects. (arXiv:2401.06401v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.06401</link>
<description rdf:parseType="Literal">&lt;p&gt;How to evaluate Large Language Models (LLMs) in code generation is an open
question. Many benchmarks have been proposed but are inconsistent with
practical software projects, e.g., unreal program distributions, insufficient
dependencies, and small-scale project contexts. Thus, the capabilities of LLMs
in practical projects are still unclear. In this paper, we propose a new
benchmark named DevEval, aligned with Developers&apos; experiences in practical
projects. DevEval is collected through a rigorous pipeline, containing 2,690
samples from 119 practical projects and covering 10 domains. Compared to
previous benchmarks, DevEval aligns to practical projects in multiple
dimensions, e.g., real program distributions, sufficient dependencies, and
enough-scale project contexts. We assess five popular LLMs on DevEval (e.g.,
gpt-4, gpt-3.5-turbo, CodeLLaMa, and StarCoder) and reveal their actual
abilities in code generation. For instance, the highest Pass@1 of gpt-3.5-turbo
only is 42 in our experiments. We also discuss the challenges and future
directions of code generation in practical projects. We open-source DevEval and
hope it can facilitate the development of code generation in practical
projects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunfei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongmin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huanyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kaibo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lecheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lanshen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jiazheng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuanming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yihong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuqi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengfei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06406">
<title>Knowledge-Informed Machine Learning for Cancer Diagnosis and Prognosis: A review. (arXiv:2401.06406v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06406</link>
<description rdf:parseType="Literal">&lt;p&gt;Cancer remains one of the most challenging diseases to treat in the medical
field. Machine learning has enabled in-depth analysis of rich multi-omics
profiles and medical imaging for cancer diagnosis and prognosis. Despite these
advancements, machine learning models face challenges stemming from limited
labeled sample sizes, the intricate interplay of high-dimensionality data
types, the inherent heterogeneity observed among patients and within tumors,
and concerns about interpretability and consistency with existing biomedical
knowledge. One approach to surmount these challenges is to integrate biomedical
knowledge into data-driven models, which has proven potential to improve the
accuracy, robustness, and interpretability of model results. Here, we review
the state-of-the-art machine learning studies that adopted the fusion of
biomedical knowledge and data, termed knowledge-informed machine learning, for
cancer diagnosis and prognosis. Emphasizing the properties inherent in four
primary data types including clinical, imaging, molecular, and treatment data,
we highlight modeling considerations relevant to these contexts. We provide an
overview of diverse forms of knowledge representation and current strategies of
knowledge integration into machine learning pipelines with concrete examples.
We conclude the review article by discussing future directions to advance
cancer research through knowledge-informed machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1&quot;&gt;Lingchao Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hairong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Leland S. Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_N/0/1/0/all/0/1&quot;&gt;Nhan L Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canoll_P/0/1/0/all/0/1&quot;&gt;Peter D Canoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swanson_K/0/1/0/all/0/1&quot;&gt;Kristin R Swanson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06416">
<title>Mission: Impossible Language Models. (arXiv:2401.06416v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06416</link>
<description rdf:parseType="Literal">&lt;p&gt;Chomsky and others have very directly claimed that large language models
(LLMs) are equally capable of learning languages that are possible and
impossible for humans to learn. However, there is very little published
experimental evidence to support such a claim. Here, we develop a set of
synthetic impossible languages of differing complexity, each designed by
systematically altering English data with unnatural word orders and grammar
rules. These languages lie on an impossibility continuum: at one end are
languages that are inherently impossible, such as random and irreversible
shuffles of English words, and on the other, languages that may not be
intuitively impossible but are often considered so in linguistics, particularly
those with rules based on counting word positions. We report on a wide range of
evaluations to assess the capacity of GPT-2 small models to learn these
uncontroversially impossible languages, and crucially, we perform these
assessments at various stages throughout training to compare the learning
process for each language. Our core finding is that GPT-2 struggles to learn
impossible languages when compared to English as a control, challenging the
core claim. More importantly, we hope our approach opens up a productive line
of inquiry in which different LLM architectures are tested on a variety of
impossible languages in an effort to learn more about how LLMs can be used as
tools for these cognitive and typological investigations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kallini_J/0/1/0/all/0/1&quot;&gt;Julie Kallini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadimitriou_I/0/1/0/all/0/1&quot;&gt;Isabel Papadimitriou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Futrell_R/0/1/0/all/0/1&quot;&gt;Richard Futrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1&quot;&gt;Kyle Mahowald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06421">
<title>Uncertainty quantification for probabilistic machine learning in earth observation using conformal prediction. (arXiv:2401.06421v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06421</link>
<description rdf:parseType="Literal">&lt;p&gt;Unreliable predictions can occur when using artificial intelligence (AI)
systems with negative consequences for downstream applications, particularly
when employed for decision-making. Conformal prediction provides a
model-agnostic framework for uncertainty quantification that can be applied to
any dataset, irrespective of its distribution, post hoc. In contrast to other
pixel-level uncertainty quantification methods, conformal prediction operates
without requiring access to the underlying model and training dataset,
concurrently offering statistically valid and informative prediction regions,
all while maintaining computational efficiency. In response to the increased
need to report uncertainty alongside point predictions, we bring attention to
the promise of conformal prediction within the domain of Earth Observation (EO)
applications. To accomplish this, we assess the current state of uncertainty
quantification in the EO domain and found that only 20% of the reviewed Google
Earth Engine (GEE) datasets incorporated a degree of uncertainty information,
with unreliable methods prevalent. Next, we introduce modules that seamlessly
integrate into existing GEE predictive modelling workflows and demonstrate the
application of these tools for datasets spanning local to global scales,
including the Dynamic World and Global Ecosystem Dynamics Investigation (GEDI)
datasets. These case studies encompass regression and classification tasks,
featuring both traditional and deep learning-based workflows. Subsequently, we
discuss the opportunities arising from the use of conformal prediction in EO.
We anticipate that the increased availability of easy-to-use implementations of
conformal predictors, such as those provided here, will drive wider adoption of
rigorous uncertainty quantification in EO, thereby enhancing the reliability of
uses such as operational monitoring and decision making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Geethen Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moncrieff_G/0/1/0/all/0/1&quot;&gt;Glenn Moncrieff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venter_Z/0/1/0/all/0/1&quot;&gt;Zander Venter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cawse_Nicholson_K/0/1/0/all/0/1&quot;&gt;Kerry Cawse-Nicholson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slingsby_J/0/1/0/all/0/1&quot;&gt;Jasper Slingsby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_T/0/1/0/all/0/1&quot;&gt;Tamara B Robinson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06426">
<title>UPDP: A Unified Progressive Depth Pruner for CNN and Vision Transformer. (arXiv:2401.06426v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06426</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional channel-wise pruning methods by reducing network channels
struggle to effectively prune efficient CNN models with depth-wise
convolutional layers and certain efficient modules, such as popular inverted
residual blocks. Prior depth pruning methods by reducing network depths are not
suitable for pruning some efficient models due to the existence of some
normalization layers. Moreover, finetuning subnet by directly removing
activation layers would corrupt the original model weights, hindering the
pruned model from achieving high performance. To address these issues, we
propose a novel depth pruning method for efficient models. Our approach
proposes a novel block pruning strategy and progressive training method for the
subnet. Additionally, we extend our pruning method to vision transformer
models. Experimental results demonstrate that our method consistently
outperforms existing depth pruning methods across various pruning
configurations. We obtained three pruned ConvNeXtV1 models with our method
applying on ConvNeXtV1, which surpass most SOTA efficient models with
comparable inference performance. Our method also achieves state-of-the-art
pruning performance on the vision transformer model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Dehua Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanxian Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mingjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jinzhang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Fan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirasao_A/0/1/0/all/0/1&quot;&gt;Ashish Sirasao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06431">
<title>From Automation to Augmentation: Large Language Models Elevating Essay Scoring Landscape. (arXiv:2401.06431v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06431</link>
<description rdf:parseType="Literal">&lt;p&gt;Receiving immediate and personalized feedback is crucial for second-language
learners, and Automated Essay Scoring (AES) systems are a vital resource when
human instructors are unavailable. This study investigates the effectiveness of
Large Language Models (LLMs), specifically GPT-4 and fine-tuned GPT-3.5, as
tools for AES. Our comprehensive set of experiments, conducted on both public
and private datasets, highlights the remarkable advantages of LLM-based AES
systems. They include superior accuracy, consistency, generalizability, and
interpretability, with fine-tuned GPT-3.5 surpassing traditional grading
models. Additionally, we undertake LLM-assisted human evaluation experiments
involving both novice and expert graders. One pivotal discovery is that LLMs
not only automate the grading process but also enhance the performance of human
graders. Novice graders when provided with feedback generated by LLMs, achieve
a level of accuracy on par with experts, while experts become more efficient
and maintain greater consistency in their assessments. These results underscore
the potential of LLMs in educational technology, paving the way for effective
collaboration between humans and AI, ultimately leading to transformative
learning experiences through AI-generated feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Changrong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wenxing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Sean Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kunpeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qi Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06436">
<title>Improving Graph Convolutional Networks with Transformer Layer in social-based items recommendation. (arXiv:2401.06436v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06436</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we have proposed an approach for improving the GCN for
predicting ratings in social networks. Our model is expanded from the standard
model with several layers of transformer architecture. The main focus of the
paper is on the encoder architecture for node embedding in the network. Using
the embedding layer from the graph-based convolution layer, the attention
mechanism could rearrange the feature space to get a more efficient embedding
for the downstream task. The experiments showed that our proposed architecture
achieves better performance than GCN on the traditional link prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Thi Linh Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Tuan Dung Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ta_V/0/1/0/all/0/1&quot;&gt;Viet Cuong Ta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06437">
<title>3D-PreMise: Can Large Language Models Generate 3D Shapes with Sharp Features and Parametric Control?. (arXiv:2401.06437v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2401.06437</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in implicit 3D representations and generative models have
markedly propelled the field of 3D object generation forward. However, it
remains a significant challenge to accurately model geometries with defined
sharp features under parametric controls, which is crucial in fields like
industrial design and manufacturing. To bridge this gap, we introduce a
framework that employs Large Language Models (LLMs) to generate text-driven 3D
shapes, manipulating 3D software via program synthesis. We present 3D-PreMise,
a dataset specifically tailored for 3D parametric modeling of industrial
shapes, designed to explore state-of-the-art LLMs within our proposed pipeline.
Our work reveals effective generation strategies and delves into the
self-correction capabilities of LLMs using a visual interface. Our work
highlights both the potential and limitations of LLMs in 3D parametric modeling
for industrial applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zeqing Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_H/0/1/0/all/0/1&quot;&gt;Haoxuan Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1&quot;&gt;Qiang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junbo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06461">
<title>Between Lines of Code: Unraveling the Distinct Patterns of Machine and Human Programmers. (arXiv:2401.06461v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.06461</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models have catalyzed an unprecedented wave in code
generation. While achieving significant advances, they blur the distinctions
between machine-and human-authored source code, causing integrity and
authenticity issues of software artifacts. Previous methods such as DetectGPT
have proven effective in discerning machine-generated texts, but they do not
identify and harness the unique patterns of machine-generated code. Thus, its
applicability falters when applied to code. In this paper, we carefully study
the specific patterns that characterize machine and human-authored code.
Through a rigorous analysis of code attributes such as length, lexical
diversity, and naturalness, we expose unique pat-terns inherent to each source.
We particularly notice that the structural segmentation of code is a critical
factor in identifying its provenance. Based on our findings, we propose a novel
machine-generated code detection method called DetectCodeGPT, which improves
DetectGPT by capturing the distinct structural patterns of code. Diverging from
conventional techniques that depend on external LLMs for perturbations,
DetectCodeGPT perturbs the code corpus by strategically inserting spaces and
newlines, ensuring both efficacy and efficiency. Experiment results show that
our approach significantly outperforms state-of-the-art techniques in detecting
machine-generated code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuling Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_C/0/1/0/all/0/1&quot;&gt;Chengcheng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06465">
<title>Sanity Checks Revisited: An Exploration to Repair the Model Parameter Randomisation Test. (arXiv:2401.06465v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.06465</link>
<description rdf:parseType="Literal">&lt;p&gt;The Model Parameter Randomisation Test (MPRT) is widely acknowledged in the
eXplainable Artificial Intelligence (XAI) community for its well-motivated
evaluative principle: that the explanation function should be sensitive to
changes in the parameters of the model function. However, recent works have
identified several methodological caveats for the empirical interpretation of
MPRT. To address these caveats, we introduce two adaptations to the original
MPRT -- Smooth MPRT and Efficient MPRT, where the former minimises the impact
that noise has on the evaluation results through sampling and the latter
circumvents the need for biased similarity measurements by re-interpreting the
test through the explanation&apos;s rise in complexity, after full parameter
randomisation. Our experimental results demonstrate that these proposed
variants lead to improved metric reliability, thus enabling a more trustworthy
application of XAI methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1&quot;&gt;Anna Hedstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_L/0/1/0/all/0/1&quot;&gt;Leander Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1&quot;&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1&quot;&gt;Marina MC H&amp;#xf6;hne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06466">
<title>PersianMind: A Cross-Lingual Persian-English Large Language Model. (arXiv:2401.06466v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06466</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models demonstrate remarkable proficiency in various
linguistic tasks and have extensive knowledge across various domains. Although
they perform best in English, their ability in other languages is notable too.
In contrast, open-source models, such as LLaMa, are primarily trained on
English datasets, resulting in poor performance in non-English languages. In
this paper, we introduce PersianMind, an open-source bilingual large language
model which demonstrates comparable performance to closed-source GPT-3.5-turbo
in the Persian language. By expanding LLaMa2&apos;s vocabulary with 10,000 Persian
tokens and training it on a dataset comprising nearly 2 billion Persian tokens,
we show that our approach preserves the model&apos;s English knowledge and employs
transfer learning to excel at transferring task knowledge from one language to
another.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostami_P/0/1/0/all/0/1&quot;&gt;Pedram Rostami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salemi_A/0/1/0/all/0/1&quot;&gt;Ali Salemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dousti_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Dousti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06471">
<title>A Brain-inspired Computational Model for Human-like Concept Learning. (arXiv:2401.06471v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.06471</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept learning is a fundamental aspect of human cognition and plays a
critical role in mental processes such as categorization, reasoning, memory,
and decision-making. Researchers across various disciplines have shown
consistent interest in the process of concept acquisition in individuals. To
elucidate the mechanisms involved in human concept learning, this study
examines the findings from computational neuroscience and cognitive psychology.
These findings indicate that the brain&apos;s representation of concepts relies on
two essential components: multisensory representation and text-derived
representation. These two types of representations are coordinated by a
semantic control system, ultimately leading to the acquisition of concepts.
Drawing inspiration from this mechanism, the study develops a human-like
computational model for concept learning based on spiking neural networks. By
effectively addressing the challenges posed by diverse sources and imbalanced
dimensionality of the two forms of concept representations, the study
successfully attains human-like concept representations. Tests involving
similar concepts demonstrate that our model, which mimics the way humans learn
concepts, yields representations that closely align with human cognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06477">
<title>Kun: Answer Polishment for Chinese Self-Alignment with Instruction Back-Translation. (arXiv:2401.06477v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06477</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce Kun, a novel approach for creating high-quality
instruction-tuning datasets for large language models (LLMs) without relying on
manual annotations. Adapting a self-training algorithm based on instruction
back-translation and answer polishment, Kun leverages unlabelled data from
diverse sources such as Wudao, Wanjuan, and SkyPile to generate a substantial
dataset of over a million Chinese instructional data points. This approach
significantly deviates from traditional methods by using a self-curation
process to refine and select the most effective instruction-output pairs. Our
experiments with the 6B-parameter Yi model across various benchmarks
demonstrate Kun&apos;s robustness and scalability. Our method&apos;s core contributions
lie in its algorithmic advancement, which enhances data retention and clarity,
and its innovative data generation approach that substantially reduces the
reliance on costly and time-consuming manual annotations. This methodology
presents a scalable and efficient solution for improving the
instruction-following capabilities of LLMs, with significant implications for
their application across diverse fields. The code and dataset can be found at
https://github.com/Zheng0428/COIG-Kun
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianyu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shuyue Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xingwei Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiawei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weixu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinrun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenghua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06493">
<title>Expected Shapley-Like Scores of Boolean Functions: Complexity and Applications to Probabilistic Databases. (arXiv:2401.06493v1 [cs.DB])</title>
<link>http://arxiv.org/abs/2401.06493</link>
<description rdf:parseType="Literal">&lt;p&gt;Shapley values, originating in game theory and increasingly prominent in
explainable AI, have been proposed to assess the contribution of facts in query
answering over databases, along with other similar power indices such as
Banzhaf values. In this work we adapt these Shapley-like scores to
probabilistic settings, the objective being to compute their expected value. We
show that the computations of expected Shapley values and of the expected
values of Boolean functions are interreducible in polynomial time, thus
obtaining the same tractability landscape. We investigate the specific
tractable case where Boolean functions are represented as deterministic
decomposable circuits, designing a polynomial-time algorithm for this setting.
We present applications to probabilistic databases through database provenance,
and an effective implementation of this algorithm within the ProvSQL system,
which experimentally validates its feasibility over a standard benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmakar_P/0/1/0/all/0/1&quot;&gt;Pratik Karmakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monet_M/0/1/0/all/0/1&quot;&gt;Mika&amp;#xeb;l Monet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senellart_P/0/1/0/all/0/1&quot;&gt;Pierre Senellart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bressan_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Bressan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06503">
<title>Improving the Detection of Small Oriented Objects in Aerial Images. (arXiv:2401.06503v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06503</link>
<description rdf:parseType="Literal">&lt;p&gt;Small oriented objects that represent tiny pixel-area in large-scale aerial
images are difficult to detect due to their size and orientation. Existing
oriented aerial detectors have shown promising results but are mainly focused
on orientation modeling with less regard to the size of the objects. In this
work, we proposed a method to accurately detect small oriented objects in
aerial images by enhancing the classification and regression tasks of the
oriented object detection model. We designed the Attention-Points Network
consisting of two losses: Guided-Attention Loss (GALoss) and Box-Points Loss
(BPLoss). GALoss uses an instance segmentation mask as ground-truth to learn
the attention features needed to improve the detection of small objects. These
attention features are then used to predict box points for BPLoss, which
determines the points&apos; position relative to the target oriented bounding box.
Experimental results show the effectiveness of our Attention-Points Network on
a standard oriented aerial dataset with small object instances (DOTA-v1.5) and
on a maritime-related dataset (HRSC2016). The code is publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doloriel_C/0/1/0/all/0/1&quot;&gt;Chandler Timm C. Doloriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cajote_R/0/1/0/all/0/1&quot;&gt;Rhandley D. Cajote&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06506">
<title>Frequency Masking for Universal Deepfake Detection. (arXiv:2401.06506v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06506</link>
<description rdf:parseType="Literal">&lt;p&gt;We study universal deepfake detection. Our goal is to detect synthetic images
from a range of generative AI approaches, particularly from emerging ones which
are unseen during training of the deepfake detector. Universal deepfake
detection requires outstanding generalization capability. Motivated by recently
proposed masked image modeling which has demonstrated excellent generalization
in self-supervised pre-training, we make the first attempt to explore masked
image modeling for universal deepfake detection. We study spatial and frequency
domain masking in training deepfake detectors. Based on empirical analysis, we
propose a novel deepfake detector via frequency masking. Our focus on frequency
domain is different from the majority, which primarily target spatial domain
detection. Our comparative analyses reveal substantial performance gains over
existing methods. Code and models are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doloriel_C/0/1/0/all/0/1&quot;&gt;Chandler Timm Doloriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06513">
<title>ML-On-Rails: Safeguarding Machine Learning Models in Software Systems A Case Study. (arXiv:2401.06513v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.06513</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML), especially with the emergence of large language models
(LLMs), has significantly transformed various industries. However, the
transition from ML model prototyping to production use within software systems
presents several challenges. These challenges primarily revolve around ensuring
safety, security, and transparency, subsequently influencing the overall
robustness and trustworthiness of ML models. In this paper, we introduce
ML-On-Rails, a protocol designed to safeguard ML models, establish a
well-defined endpoint interface for different ML tasks, and clear communication
between ML providers and ML consumers (software engineers). ML-On-Rails
enhances the robustness of ML models via incorporating detection capabilities
to identify unique challenges specific to production ML. We evaluated the
ML-On-Rails protocol through a real-world case study of the MoveReminder
application. Through this evaluation, we emphasize the importance of
safeguarding ML models in production.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelkader_H/0/1/0/all/0/1&quot;&gt;Hala Abdelkader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelrazek_M/0/1/0/all/0/1&quot;&gt;Mohamed Abdelrazek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnett_S/0/1/0/all/0/1&quot;&gt;Scott Barnett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Jean-Guy Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_P/0/1/0/all/0/1&quot;&gt;Priya Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasa_R/0/1/0/all/0/1&quot;&gt;Rajesh Vasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06528">
<title>PCB-Vision: A Multiscene RGB-Hyperspectral Benchmark Dataset of Printed Circuit Boards. (arXiv:2401.06528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06528</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the critical theme of recycling electronic waste (E-waste), this
contribution is dedicated to developing advanced automated data processing
pipelines as a basis for decision-making and process control. Aligning with the
broader goals of the circular economy and the United Nations (UN) Sustainable
Development Goals (SDG), our work leverages non-invasive analysis methods
utilizing RGB and hyperspectral imaging data to provide both quantitative and
qualitative insights into the E-waste stream composition for optimizing
recycling efficiency. In this paper, we introduce &apos;PCB-Vision&apos;; a pioneering
RGB-hyperspectral printed circuit board (PCB) benchmark dataset, comprising 53
RGB images of high spatial resolution paired with their corresponding high
spectral resolution hyperspectral data cubes in the visible and near-infrared
(VNIR) range. Grounded in open science principles, our dataset provides a
comprehensive resource for researchers through high-quality ground truths,
focusing on three primary PCB components: integrated circuits (IC), capacitors,
and connectors. We provide extensive statistical investigations on the proposed
dataset together with the performance of several state-of-the-art (SOTA)
models, including U-Net, Attention U-Net, Residual U-Net, LinkNet, and
DeepLabv3+. By openly sharing this multi-scene benchmark dataset along with the
baseline codes, we hope to foster transparent, traceable, and comparable
developments of advanced data processing across various scientific communities,
including, but not limited to, computer vision and remote sensing. Emphasizing
our commitment to supporting a collaborative and inclusive scientific
community, all materials, including code, data, ground truth, and masks, will
be accessible at https://github.com/hifexplo/PCBVision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbash_E/0/1/0/all/0/1&quot;&gt;Elias Arbash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuchs_M/0/1/0/all/0/1&quot;&gt;Margret Fuchs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1&quot;&gt;Behnood Rasti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_S/0/1/0/all/0/1&quot;&gt;Sandra Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1&quot;&gt;Pedram Ghamisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gloaguen_R/0/1/0/all/0/1&quot;&gt;Richard Gloaguen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06538">
<title>Intelligent Data-Driven Architectural Features Orchestration for Network Slicing. (arXiv:2401.06538v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.06538</link>
<description rdf:parseType="Literal">&lt;p&gt;Network slicing is a crucial enabler and a trend for the Next Generation
Mobile Network (NGMN) and various other new systems like the Internet of
Vehicles (IoV) and Industrial IoT (IIoT). Orchestration and machine learning
are key elements with a crucial role in the network-slicing processes since the
NS process needs to orchestrate resources and functionalities, and machine
learning can potentially optimize the orchestration process. However, existing
network-slicing architectures lack the ability to define intelligent approaches
to orchestrate features and resources in the slicing process. This paper
discusses machine learning-based orchestration of features and capabilities in
network slicing architectures. Initially, the slice resource orchestration and
allocation in the slicing planning, configuration, commissioning, and operation
phases are analyzed. In sequence, we highlight the need for optimized
architectural feature orchestration and recommend using ML-embed agents,
federated learning intrinsic mechanisms for knowledge acquisition, and a
data-driven approach embedded in the network slicing architecture. We further
develop an architectural features orchestration case embedded in the SFI2
network slicing architecture. An attack prevention security mechanism is
developed for the SFI2 architecture using distributed embedded and cooperating
ML agents. The case presented illustrates the architectural feature&apos;s
orchestration process and benefits, highlighting its importance for the network
slicing process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreira_R/0/1/0/all/0/1&quot;&gt;Rodrigo Moreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_F/0/1/0/all/0/1&quot;&gt;Flavio de Oliveira Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_T/0/1/0/all/0/1&quot;&gt;Tereza Cristina Melo de Brito Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_J/0/1/0/all/0/1&quot;&gt;Joberto S. B. Martins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06541">
<title>Medical Dialogue Generation via Intuitive-then-Analytical Differential Diagnosis. (arXiv:2401.06541v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06541</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical dialogue systems have attracted growing research attention as they
have the potential to provide rapid diagnoses, treatment plans, and health
consultations. In medical dialogues, a proper diagnosis is crucial as it
establishes the foundation for future consultations. Clinicians typically
employ both intuitive and analytic reasoning to formulate a differential
diagnosis. This reasoning process hypothesizes and verifies a variety of
possible diseases and strives to generate a comprehensive and rigorous
diagnosis. However, recent studies on medical dialogue generation have
overlooked the significance of modeling a differential diagnosis, which hinders
the practical application of these systems. To address the above issue, we
propose a medical dialogue generation framework with the
Intuitive-then-Analytic Differential Diagnosis (IADDx). Our method starts with
a differential diagnosis via retrieval-based intuitive association and
subsequently refines it through a graph-enhanced analytic procedure. The
resulting differential diagnosis is then used to retrieve medical knowledge and
guide response generation. Experimental results on two datasets validate the
efficacy of our method. Besides, we demonstrate how our framework assists both
clinicians and patients in understanding the diagnostic process, for instance,
by producing intermediate results and graph-based diagnosis paths.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaishuai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_W/0/1/0/all/0/1&quot;&gt;Wenjun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjie Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06550">
<title>Multimodal Learning for detecting urban functional zones using remote sensing image and multi-semantic information. (arXiv:2401.06550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06550</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban area-of-interest (AOI) refers to an integrated urban functional zone
with defined boundaries. The rapid development of urban commerce has resulted
in an increased demand for more precise requirements in defining AOIs. However,
existing research primarily concentrates on broad AOI mining for urban planning
or regional economic analysis, failing to cater to the precise requirements of
mobile Internet online-to-offline businesses. These businesses necessitate
accuracy down to a specific community, school, or hospital. In this paper, we
propose an end-to-end multimodal deep learning algorithm for detecting AOI
fence polygon using remote sensing images and multi-semantics reference
information. We then evaluate its timeliness through a cascaded module that
incorporates dynamic human mobility and logistics address information.
Specifically, we begin by selecting a point-of-interest (POI) of specific
category, and use it to recall corresponding remote sensing images, nearby
POIs, road nodes, human mobility, and logistics addresses to build a multimodal
detection model based on transformer encoder-decoder architecture, titled
AOITR. In the model, in addition to the remote sensing images, multi-semantic
information including core POI and road nodes is embedded and reorganized as
the query content part for the transformer decoder to generate the AOI polygon.
Meanwhile, relatively dynamic distribution features of human mobility, nearby
POIs, and logistics addresses are used for AOI reliability evaluation through a
cascaded feedforward network. The experimental results demonstrate that our
algorithm significantly outperforms two existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chuanji Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaotuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qiqi Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06557">
<title>Treatment-Aware Hyperbolic Representation Learning for Causal Effect Estimation with Social Networks. (arXiv:2401.06557v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06557</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the individual treatment effect (ITE) from observational data is a
crucial research topic that holds significant value across multiple domains.
How to identify hidden confounders poses a key challenge in ITE estimation.
Recent studies have incorporated the structural information of social networks
to tackle this challenge, achieving notable advancements. However, these
methods utilize graph neural networks to learn the representation of hidden
confounders in Euclidean space, disregarding two critical issues: (1) the
social networks often exhibit a scalefree structure, while Euclidean embeddings
suffer from high distortion when used to embed such graphs, and (2) each
ego-centric network within a social network manifests a treatment-related
characteristic, implying significant patterns of hidden confounders. To address
these issues, we propose a novel method called Treatment-Aware Hyperbolic
Representation Learning (TAHyper). Firstly, TAHyper employs the hyperbolic
space to encode the social networks, thereby effectively reducing the
distortion of confounder representation caused by Euclidean embeddings.
Secondly, we design a treatment-aware relationship identification module that
enhances the representation of hidden confounders by identifying whether an
individual and her neighbors receive the same treatment. Extensive experiments
on two benchmark datasets are conducted to demonstrate the superiority of our
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xing Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yang Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1&quot;&gt;Bowei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiuqiang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chen Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06559">
<title>A General Benchmark Framework is Dynamic Graph Neural Network Need. (arXiv:2401.06559v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06559</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic graph learning is crucial for modeling real-world systems with
evolving relationships and temporal dynamics. However, the lack of a unified
benchmark framework in current research has led to inaccurate evaluations of
dynamic graph models. This paper highlights the significance of dynamic graph
learning and its applications in various domains. It emphasizes the need for a
standardized benchmark framework that captures temporal dynamics, evolving
graph structures, and downstream task requirements. Establishing a unified
benchmark will help researchers understand the strengths and limitations of
existing models, foster innovation, and advance dynamic graph learning. In
conclusion, this paper identifies the lack of a standardized benchmark
framework as a current limitation in dynamic graph learning research . Such a
framework will facilitate accurate model evaluation, drive advancements in
dynamic graph learning techniques, and enable the development of more effective
models for real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yusen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06568">
<title>Lost in the Source Language: How Large Language Models Evaluate the Quality of Machine Translation. (arXiv:2401.06568v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06568</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have achieved remarkable results in the machine
translation evaluation task, yet there remains a gap in knowledge regarding how
they utilize the provided data to conduct evaluations. This study aims to
explore how LLMs leverage source and reference information in evaluating
translations, with the ultimate goal of better understanding the working
mechanism of LLMs. To this end, we design the controlled experiments across
various input modes and model types, and employ both coarse-grained and
fine-grained prompts to discern the utility of source versus reference
information. Surprisingly, we find that reference information significantly
enhances the evaluation accuracy, while source information sometimes is
counterproductive, indicating a lack of cross-lingual capability when using
LLMs to evaluate translations. We further conduct a meta-evaluation for
translation error detection of LLMs, observing a similar phenomenon. These
findings also suggest a potential research direction for LLMs that fully
exploits the cross-lingual capability of LLMs to achieve better performance in
machine translation evaluation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xiang Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yichao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shujian Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06583">
<title>Mapping Transformer Leveraged Embeddings for Cross-Lingual Document Representation. (arXiv:2401.06583v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06583</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommendation systems, for documents, have become tools to find relevant
content on the Web. However, these systems have limitations when it comes to
recommending documents in languages different from the query language, which
means they might overlook resources in non-native languages. This research
focuses on representing documents across languages by using Transformer
Leveraged Document Representations (TLDRs) that are mapped to a cross-lingual
domain. Four multilingual pre-trained transformer models (mBERT, mT5 XLM
RoBERTa, ErnieM) were evaluated using three mapping methods across 20 language
pairs representing combinations of five selected languages of the European
Union. Metrics like Mate Retrieval Rate and Reciprocal Rank were used to
measure the effectiveness of mapped TLDRs compared to non-mapped ones. The
results highlight the power of cross-lingual representations achieved through
pre-trained transformers and mapping approaches suggesting a promising
direction for expanding beyond language connections, between two specific
languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tashu_T/0/1/0/all/0/1&quot;&gt;Tsegaye Misikir Tashu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kontos_E/0/1/0/all/0/1&quot;&gt;Eduard-Raul Kontos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabatelli_M/0/1/0/all/0/1&quot;&gt;Matthia Sabatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valdenegro_Toro_M/0/1/0/all/0/1&quot;&gt;Matias Valdenegro-Toro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06588">
<title>Dynamic Behaviour of Connectionist Speech Recognition with Strong Latency Constraints. (arXiv:2401.06588v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.06588</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes the use of connectionist techniques in phonetic speech
recognition with strong latency constraints. The constraints are imposed by the
task of deriving the lip movements of a synthetic face in real time from the
speech signal, by feeding the phonetic string into an articulatory synthesiser.
Particular attention has been paid to analysing the interaction between the
time evolution model learnt by the multi-layer perceptrons and the transition
model imposed by the Viterbi decoder, in different latency conditions. Two
experiments were conducted in which the time dependencies in the language model
(LM) were controlled by a parameter. The results show a strong interaction
between the three factors involved, namely the neural network topology, the
length of time dependencies in the LM and the decoder latency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Salvi_G/0/1/0/all/0/1&quot;&gt;Giampiero Salvi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06595">
<title>Every Node is Different: Dynamically Fusing Self-Supervised Tasks for Attributed Graph Clustering. (arXiv:2401.06595v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06595</link>
<description rdf:parseType="Literal">&lt;p&gt;Attributed graph clustering is an unsupervised task that partitions nodes
into different groups. Self-supervised learning (SSL) shows great potential in
handling this task, and some recent studies simultaneously learn multiple SSL
tasks to further boost performance. Currently, different SSL tasks are assigned
the same set of weights for all graph nodes. However, we observe that some
graph nodes whose neighbors are in different groups require significantly
different emphases on SSL tasks. In this paper, we propose to dynamically learn
the weights of SSL tasks for different nodes and fuse the embeddings learned
from different SSL tasks to boost performance. We design an innovative graph
clustering approach, namely Dynamically Fusing Self-Supervised Learning
(DyFSS). Specifically, DyFSS fuses features extracted from diverse SSL tasks
using distinct weights derived from a gating network. To effectively learn the
gating network, we design a dual-level self-supervised strategy that
incorporates pseudo labels and the graph structure. Extensive experiments on
five datasets show that DyFSS outperforms the state-of-the-art multi-task SSL
methods by up to 8.66% on the accuracy metric. The code of DyFSS is available
at: https://github.com/q086/DyFSS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Pengfei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06633">
<title>Ada-Retrieval: An Adaptive Multi-Round Retrieval Paradigm for Sequential Recommendations. (arXiv:2401.06633v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.06633</link>
<description rdf:parseType="Literal">&lt;p&gt;Retrieval models aim at selecting a small set of item candidates which match
the preference of a given user. They play a vital role in large-scale
recommender systems since subsequent models such as rankers highly depend on
the quality of item candidates. However, most existing retrieval models employ
a single-round inference paradigm, which may not adequately capture the dynamic
nature of user preferences and stuck in one area in the item space. In this
paper, we propose Ada-Retrieval, an adaptive multi-round retrieval paradigm for
recommender systems that iteratively refines user representations to better
capture potential candidates in the full item space. Ada-Retrieval comprises
two key modules: the item representation adapter and the user representation
adapter, designed to inject context information into items&apos; and users&apos;
representations. The framework maintains a model-agnostic design, allowing
seamless integration with various backbone models such as RNNs or Transformers.
We perform experiments on three widely used public datasets, incorporating five
powerful sequential recommenders as backbone models. Our results demonstrate
that Ada-Retrieval significantly enhances the performance of various base
models, with consistent improvements observed across different datasets. Our
code and data are publicly available at:
https://github.com/ll0ruc/Ada-Retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1&quot;&gt;Jianxun Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06634">
<title>CCFC: Bridging Federated Clustering and Contrastive Learning. (arXiv:2401.06634v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06634</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated clustering, an essential extension of centralized clustering for
federated scenarios, enables multiple data-holding clients to collaboratively
group data while keeping their data locally. In centralized scenarios,
clustering driven by representation learning has made significant advancements
in handling high-dimensional complex data. However, the combination of
federated clustering and representation learning remains underexplored. To
bridge this, we first tailor a cluster-contrastive model for learning
clustering-friendly representations. Then, we harness this model as the
foundation for proposing a new federated clustering method, named
cluster-contrastive federated clustering (CCFC). Benefiting from representation
learning, the clustering performance of CCFC even double those of the best
baseline methods in some cases. Compared to the most related baseline, the
benefit results in substantial NMI score improvements of up to 0.4155 on the
most conspicuous case. Moreover, CCFC also shows superior performance in
handling device failures from a practical viewpoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jie Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhong-Yuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06640">
<title>Experimental Contexts Can Facilitate Robust Semantic Property Inference in Language Models, but Inconsistently. (arXiv:2401.06640v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06640</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent zero-shot evaluations have highlighted important limitations in the
abilities of language models (LMs) to perform meaning extraction. However, it
is now well known that LMs can demonstrate radical improvements in the presence
of experimental contexts such as in-context examples and instructions. How well
does this translate to previously studied meaning-sensitive tasks? We present a
case-study on the extent to which experimental contexts can improve LMs&apos;
robustness in performing property inheritance -- predicting semantic properties
of novel concepts, a task that they have been previously shown to fail on. Upon
carefully controlling the nature of the in-context examples and the
instructions, our work reveals that they can indeed lead to non-trivial
property inheritance behavior in LMs. However, this ability is inconsistent:
with a minimal reformulation of the task, some LMs were found to pick up on
shallow, non-semantic heuristics from their inputs, suggesting that the
computational principles of semantic property inference are yet to be mastered
by LMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_K/0/1/0/all/0/1&quot;&gt;Kanishka Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ettinger_A/0/1/0/all/0/1&quot;&gt;Allyson Ettinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahowald_K/0/1/0/all/0/1&quot;&gt;Kyle Mahowald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06654">
<title>Decoupling Pixel Flipping and Occlusion Strategy for Consistent XAI Benchmarks. (arXiv:2401.06654v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06654</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature removal is a central building block for eXplainable AI (XAI), both
for occlusion-based explanations (Shapley values) as well as their evaluation
(pixel flipping, PF). However, occlusion strategies can vary significantly from
simple mean replacement up to inpainting with state-of-the-art diffusion
models. This ambiguity limits the usefulness of occlusion-based approaches. For
example, PF benchmarks lead to contradicting rankings. This is amplified by
competing PF measures: Features are either removed starting with most
influential first (MIF) or least influential first (LIF). This study proposes
two complementary perspectives to resolve this disagreement problem. Firstly,
we address the common criticism of occlusion-based XAI, that artificial samples
lead to unreliable model evaluations. We propose to measure the reliability by
the R(eference)-Out-of-Model-Scope (OMS) score. The R-OMS score enables a
systematic comparison of occlusion strategies and resolves the disagreement
problem by grouping consistent PF rankings. Secondly, we show that the
insightfulness of MIF and LIF is conversely dependent on the R-OMS score. To
leverage this, we combine the MIF and LIF measures into the symmetric relevance
gain (SRG) measure. This breaks the inherent connection to the underlying
occlusion strategy and leads to consistent rankings. This resolves the
disagreement problem, which we verify for a set of 40 different occlusion
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blucher_S/0/1/0/all/0/1&quot;&gt;Stefan Bl&amp;#xfc;cher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielhaben_J/0/1/0/all/0/1&quot;&gt;Johanna Vielhaben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strodthoff_N/0/1/0/all/0/1&quot;&gt;Nils Strodthoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06676">
<title>LLMRS: Unlocking Potentials of LLM-Based Recommender Systems for Software Purchase. (arXiv:2401.06676v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.06676</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommendation systems are ubiquitous, from Spotify playlist suggestions to
Amazon product suggestions. Nevertheless, depending on the methodology or the
dataset, these systems typically fail to capture user preferences and generate
general recommendations. Recent advancements in Large Language Models (LLM)
offer promising results for analyzing user queries. However, employing these
models to capture user preferences and efficiency remains an open question. In
this paper, we propose LLMRS, an LLM-based zero-shot recommender system where
we employ pre-trained LLM to encode user reviews into a review score and
generate user-tailored recommendations. We experimented with LLMRS on a
real-world dataset, the Amazon product reviews, for software purchase use
cases. The results show that LLMRS outperforms the ranking-based baseline model
while successfully capturing meaningful information from product reviews,
thereby providing more reliable recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+John_A/0/1/0/all/0/1&quot;&gt;Angela John&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aidoo_T/0/1/0/all/0/1&quot;&gt;Theophilus Aidoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Behmanush_H/0/1/0/all/0/1&quot;&gt;Hamayoon Behmanush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_I/0/1/0/all/0/1&quot;&gt;Irem B. Gunduz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_H/0/1/0/all/0/1&quot;&gt;Hewan Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Maxx Richard Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maass_W/0/1/0/all/0/1&quot;&gt;Wolfgang Maa&amp;#xdf;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06683">
<title>DQNC2S: DQN-based Cross-stream Crisis event Summarizer. (arXiv:2401.06683v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.06683</link>
<description rdf:parseType="Literal">&lt;p&gt;Summarizing multiple disaster-relevant data streams simultaneously is
particularly challenging as existing Retrieve&amp;amp;Re-ranking strategies suffer from
the inherent redundancy of multi-stream data and limited scalability in a
multi-query setting. This work proposes an online approach to crisis timeline
generation based on weak annotation with Deep Q-Networks. It selects on-the-fly
the relevant pieces of text without requiring neither human annotations nor
content re-ranking. This makes the inference time independent of the number of
input queries. The proposed approach also incorporates a redundancy filter into
the reward function to effectively handle cross-stream content overlaps. The
achieved ROUGE and BERTScore results are superior to those of best-performing
models on the CrisisFACTS 2022 benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambrin_D/0/1/0/all/0/1&quot;&gt;Daniele Rege Cambrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cagliero_L/0/1/0/all/0/1&quot;&gt;Luca Cagliero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garza_P/0/1/0/all/0/1&quot;&gt;Paolo Garza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06686">
<title>Exploring Conversational Agents as an Effective Tool for Measuring Cognitive Biases in Decision-Making. (arXiv:2401.06686v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.06686</link>
<description rdf:parseType="Literal">&lt;p&gt;Heuristics and cognitive biases are an integral part of human
decision-making. Automatically detecting a particular cognitive bias could
enable intelligent tools to provide better decision-support. Detecting the
presence of a cognitive bias currently requires a hand-crafted experiment and
human interpretation. Our research aims to explore conversational agents as an
effective tool to measure various cognitive biases in different domains. Our
proposed conversational agent incorporates a bias measurement mechanism that is
informed by the existing experimental designs and various experimental tasks
identified in the literature. Our initial experiments to measure framing and
loss-aversion biases indicate that the conversational agents can be effectively
used to measure the biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilli_S/0/1/0/all/0/1&quot;&gt;Stephen Pilli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06692">
<title>An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models. (arXiv:2401.06692v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06692</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised finetuning (SFT) on instruction datasets has played a crucial role
in achieving the remarkable zero-shot generalization capabilities observed in
modern large language models (LLMs). However, the annotation efforts required
to produce high quality responses for instructions are becoming prohibitively
expensive, especially as the number of tasks spanned by instruction datasets
continues to increase. Active learning is effective in identifying useful
subsets of samples to annotate from an unlabeled pool, but its high
computational cost remains a barrier to its widespread applicability in the
context of LLMs. To mitigate the annotation cost of SFT and circumvent the
computational bottlenecks of active learning, we propose using experimental
design. Experimental design techniques select the most informative samples to
label, and typically maximize some notion of uncertainty and/or diversity. In
our work, we implement a framework that evaluates several existing and novel
experimental design techniques and find that these methods consistently yield
significant gains in label efficiency with little computational overhead. On
generative tasks, our methods achieve the same generalization performance with
only $50\%$ of annotation cost required by random sampling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav M. Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1&quot;&gt;Sang T. Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeffrey Bilmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon S. Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ash_J/0/1/0/all/0/1&quot;&gt;Jordan T. Ash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert D. Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06699">
<title>A Closed-form Solution for Weight Optimization in Fully-connected Feed-forward Neural Networks. (arXiv:2401.06699v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.06699</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses weight optimization problem for fully-connected
feed-forward neural networks. Unlike existing approaches that are based on
back-propagation (BP) and chain rule gradient-based optimization (which implies
iterative execution, potentially burdensome and time-consuming in some cases),
the proposed approach offers the solution for weight optimization in
closed-form by means of least squares (LS) methodology. In the case where the
input-to-output mapping is injective, the new approach optimizes the weights in
a back-propagating fashion in a single iteration by jointly optimizing a set of
weights in each layer for each neuron. In the case where the input-to-output
mapping is not injective (e.g., in classification problems), the proposed
solution is easily adapted to obtain its final solution in a few iterations. An
important advantage over the existing solutions is that these computations (for
all neurons in a layer) are independent from each other; thus, they can be
carried out in parallel to optimize all weights in a given layer
simultaneously. Furthermore, its running time is deterministic in the sense
that one can obtain the exact number of computations necessary to optimize the
weights in all network layers (per iteration, in the case of non-injective
mapping). Our simulation and empirical results show that the proposed scheme,
BPLS, works well and is competitive with existing ones in terms of accuracy,
but significantly surpasses them in terms of running time. To summarize, the
new method is straightforward to implement, is competitive and computationally
more efficient than the existing ones, and is well-tailored for parallel
implementation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomic_S/0/1/0/all/0/1&quot;&gt;Slavisa Tomic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matos_Carvalho_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Pedro Matos-Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beko_M/0/1/0/all/0/1&quot;&gt;Marko Beko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06709">
<title>Reliability Analysis of Psychological Concept Extraction and Classification in User-penned Text. (arXiv:2401.06709v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06709</link>
<description rdf:parseType="Literal">&lt;p&gt;The social NLP research community witness a recent surge in the computational
advancements of mental health analysis to build responsible AI models for a
complex interplay between language use and self-perception. Such responsible AI
models aid in quantifying the psychological concepts from user-penned texts on
social media. On thinking beyond the low-level (classification) task, we
advance the existing binary classification dataset, towards a higher-level task
of reliability analysis through the lens of explanations, posing it as one of
the safety measures. We annotate the LoST dataset to capture nuanced textual
cues that suggest the presence of low self-esteem in the posts of Reddit users.
We further state that the NLP models developed for determining the presence of
low self-esteem, focus more on three types of textual cues: (i) Trigger: words
that triggers mental disturbance, (ii) LoST indicators: text indicators
emphasizing low self-esteem, and (iii) Consequences: words describing the
consequences of mental disturbance. We implement existing classifiers to
examine the attention mechanism in pre-trained language models (PLMs) for a
domain-specific psychology-grounded task. Our findings suggest the need of
shifting the focus of PLMs from Trigger and Consequences to a more
comprehensive explanation, emphasizing LoST indicators while determining low
self-esteem in Reddit posts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_M/0/1/0/all/0/1&quot;&gt;Muskan Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sathvik_M/0/1/0/all/0/1&quot;&gt;MSVPJ Sathvik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Amrit Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_S/0/1/0/all/0/1&quot;&gt;Shaina Raza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Sunghwan Sohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06715">
<title>Reframing Tax Law Entailment as Analogical Reasoning. (arXiv:2401.06715v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06715</link>
<description rdf:parseType="Literal">&lt;p&gt;Statutory reasoning refers to the application of legislative provisions to a
series of case facts described in natural language. We re-frame statutory
reasoning as an analogy task, where each instance of the analogy task involves
a combination of two instances of statutory reasoning. This increases the
dataset size by two orders of magnitude, and introduces an element of
interpretability. We show that this task is roughly as difficult to Natural
Language Processing models as the original task. Finally, we come back to
statutory reasoning, solving it with a combination of a retrieval mechanism and
analogy models, and showing some progress on prior comparable work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xinrui Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weir_N/0/1/0/all/0/1&quot;&gt;Nathaniel Weir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Durme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holzenberger_N/0/1/0/all/0/1&quot;&gt;Nils Holzenberger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06730">
<title>Relying on the Unreliable: The Impact of Language Models&apos; Reluctance to Express Uncertainty. (arXiv:2401.06730v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06730</link>
<description rdf:parseType="Literal">&lt;p&gt;As natural language becomes the default interface for human-AI interaction,
there is a critical need for LMs to appropriately communicate uncertainties in
downstream applications. In this work, we investigate how LMs incorporate
confidence about their responses via natural language and how downstream users
behave in response to LM-articulated uncertainties. We examine publicly
deployed models and find that LMs are unable to express uncertainties when
answering questions even when they produce incorrect responses. LMs can be
explicitly prompted to express confidences, but tend to be overconfident,
resulting in high error rates (on average 47%) among confident responses. We
test the risks of LM overconfidence by running human experiments and show that
users rely heavily on LM generations, whether or not they are marked by
certainty. Lastly, we investigate the preference-annotated datasets used in
RLHF alignment and find that humans have a bias against texts with uncertainty.
Our work highlights a new set of safety harms facing human-LM interactions and
proposes design recommendations and mitigating strategies moving forward.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaitlyn Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jena D. Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sap_M/0/1/0/all/0/1&quot;&gt;Maarten Sap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06742">
<title>Using Natural Language Inference to Improve Persona Extraction from Dialogue in a New Domain. (arXiv:2401.06742v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06742</link>
<description rdf:parseType="Literal">&lt;p&gt;While valuable datasets such as PersonaChat provide a foundation for training
persona-grounded dialogue agents, they lack diversity in conversational and
narrative settings, primarily existing in the &quot;real&quot; world. To develop dialogue
agents with unique personas, models are trained to converse given a specific
persona, but hand-crafting these persona can be time-consuming, thus methods
exist to automatically extract persona information from existing
character-specific dialogue. However, these persona-extraction models are also
trained on datasets derived from PersonaChat and struggle to provide
high-quality persona information from conversational settings that do not take
place in the real world, such as the fantasy-focused dataset, LIGHT. Creating
new data to train models on a specific setting is human-intensive, thus
prohibitively expensive. To address both these issues, we introduce a natural
language inference method for post-hoc adapting a trained persona extraction
model to a new setting. We draw inspiration from the literature of dialog
natural language inference (NLI), and devise NLI-reranking methods to extract
structured persona information from dialogue. Compared to existing persona
extraction models, our method returns higher-quality extracted persona and
requires less human annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeLucia_A/0/1/0/all/0/1&quot;&gt;Alexandra DeLucia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maeda_Y/0/1/0/all/0/1&quot;&gt;Yoshinori Maeda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoda_M/0/1/0/all/0/1&quot;&gt;Makoto Yoda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_K/0/1/0/all/0/1&quot;&gt;Keiichi Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wakaki_H/0/1/0/all/0/1&quot;&gt;Hiromi Wakaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06751">
<title>The Unreasonable Effectiveness of Easy Training Data for Hard Tasks. (arXiv:2401.06751v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.06751</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we train models to perform well on hard test data when hard training
data is by definition difficult to label correctly? This question has been
termed the scalable oversight problem and has drawn increasing attention as
language models have continually improved. In this paper, we present the
surprising conclusion that current language models often generalize relatively
well from easy to hard data, even performing as well as &quot;oracle&quot; models trained
on hard data. We demonstrate this kind of easy-to-hard generalization using
simple training methods like in-context learning, linear classifier heads, and
QLoRA for seven different measures of datapoint hardness, including six
empirically diverse human hardness measures (like grade level) and one
model-based measure (loss-based). Furthermore, we show that even if one cares
most about model performance on hard data, it can be better to collect and
train on easy data rather than hard data, since hard data is generally noisier
and costlier to collect. Our experiments use open models up to 70b in size and
four publicly available question-answering datasets with questions ranging in
difficulty from 3rd grade science questions to college level STEM questions and
general-knowledge trivia. We conclude that easy-to-hard generalization in LMs
is surprisingly strong for the tasks studied, suggesting the scalable oversight
problem may be easier than previously thought. Our code is available at
https://github.com/allenai/easy-to-hard-generalization
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hase_P/0/1/0/all/0/1&quot;&gt;Peter Hase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_P/0/1/0/all/0/1&quot;&gt;Peter Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiegreffe_S/0/1/0/all/0/1&quot;&gt;Sarah Wiegreffe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06757">
<title>Synthetic Data Generation Framework, Dataset, and Efficient Deep Model for Pedestrian Intention Prediction. (arXiv:2401.06757v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.06757</link>
<description rdf:parseType="Literal">&lt;p&gt;Pedestrian intention prediction is crucial for autonomous driving. In
particular, knowing if pedestrians are going to cross in front of the
ego-vehicle is core to performing safe and comfortable maneuvers. Creating
accurate and fast models that predict such intentions from sequential images is
challenging. A factor contributing to this is the lack of datasets with diverse
crossing and non-crossing (C/NC) scenarios. We address this scarceness by
introducing a framework, named ARCANE, which allows programmatically generating
synthetic datasets consisting of C/NC video clip samples. As an example, we use
ARCANE to generate a large and diverse dataset named PedSynth. We will show how
PedSynth complements widely used real-world datasets such as JAAD and PIE, so
enabling more accurate models for C/NC prediction. Considering the onboard
deployment of C/NC prediction models, we also propose a deep model named
PedGNN, which is fast and has a very low memory footprint. PedGNN is based on a
GNN-GRU architecture that takes a sequence of pedestrian skeletons as input to
predict crossing intentions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Naveed Riaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wielgosz_M/0/1/0/all/0/1&quot;&gt;Maciej Wielgosz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romera_A/0/1/0/all/0/1&quot;&gt;Abel Garcia Romera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1&quot;&gt;Antonio M. Lopez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01719">
<title>Learning Temporal Resolution in Spectrogram for Audio Classification. (arXiv:2210.01719v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01719</link>
<description rdf:parseType="Literal">&lt;p&gt;The audio spectrogram is a time-frequency representation that has been widely
used for audio classification. One of the key attributes of the audio
spectrogram is the temporal resolution, which depends on the hop size used in
the Short-Time Fourier Transform (STFT). Previous works generally assume the
hop size should be a constant value (e.g., 10 ms). However, a fixed temporal
resolution is not always optimal for different types of sound. The temporal
resolution affects not only classification accuracy but also computational
cost. This paper proposes a novel method, DiffRes, that enables differentiable
temporal resolution modeling for audio classification. Given a spectrogram
calculated with a fixed hop size, DiffRes merges non-essential time frames
while preserving important frames. DiffRes acts as a &quot;drop-in&quot; module between
an audio spectrogram and a classifier and can be jointly optimized with the
classification task. We evaluate DiffRes on five audio classification tasks,
using mel-spectrograms as the acoustic features, followed by off-the-shelf
classifier backbones. Compared with previous methods using the fixed temporal
resolution, the DiffRes-based method can achieve the equivalent or better
classification accuracy with at least 25% computational cost reduction. We
further show that DiffRes can improve classification accuracy by increasing the
temporal resolution of input acoustic features, without adding to the
computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haohe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xubo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Qiuqiang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenwu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1&quot;&gt;Mark D. Plumbley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03050">
<title>State-of-the-art generalisation research in NLP: A taxonomy and review. (arXiv:2210.03050v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03050</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to generalise well is one of the primary desiderata of natural
language processing (NLP). Yet, what &apos;good generalisation&apos; entails and how it
should be evaluated is not well understood, nor are there any evaluation
standards for generalisation. In this paper, we lay the groundwork to address
both of these issues. We present a taxonomy for characterising and
understanding generalisation research in NLP. Our taxonomy is based on an
extensive literature review of generalisation research, and contains five axes
along which studies can differ: their main motivation, the type of
generalisation they investigate, the type of data shift they consider, the
source of this data shift, and the locus of the shift within the modelling
pipeline. We use our taxonomy to classify over 400 papers that test
generalisation, for a total of more than 600 individual experiments.
Considering the results of this review, we present an in-depth analysis that
maps out the current state of generalisation research in NLP, and we make
recommendations for which areas might deserve attention in the future. Along
with this paper, we release a webpage where the results of our review can be
dynamically explored, and which we intend to update as new NLP generalisation
studies are published. With this work, we aim to take steps towards making
state-of-the-art generalisation testing the new status quo in NLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1&quot;&gt;Dieuwke Hupkes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giulianelli_M/0/1/0/all/0/1&quot;&gt;Mario Giulianelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dankers_V/0/1/0/all/0/1&quot;&gt;Verna Dankers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1&quot;&gt;Mikel Artetxe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elazar_Y/0/1/0/all/0/1&quot;&gt;Yanai Elazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1&quot;&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christodoulopoulos_C/0/1/0/all/0/1&quot;&gt;Christos Christodoulopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lasri_K/0/1/0/all/0/1&quot;&gt;Karim Lasri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saphra_N/0/1/0/all/0/1&quot;&gt;Naomi Saphra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinclair_A/0/1/0/all/0/1&quot;&gt;Arabella Sinclair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_D/0/1/0/all/0/1&quot;&gt;Dennis Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schottmann_F/0/1/0/all/0/1&quot;&gt;Florian Schottmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batsuren_K/0/1/0/all/0/1&quot;&gt;Khuyagbaatar Batsuren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Kaiser Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_K/0/1/0/all/0/1&quot;&gt;Koustuv Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalatbari_L/0/1/0/all/0/1&quot;&gt;Leila Khalatbari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryskina_M/0/1/0/all/0/1&quot;&gt;Maria Ryskina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frieske_R/0/1/0/all/0/1&quot;&gt;Rita Frieske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhijing Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07946">
<title>Active Inference and Reinforcement Learning: A unified inference on continuous state and action spaces under partially observability. (arXiv:2212.07946v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07946</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) has garnered significant attention for developing
decision-making agents that aim to maximize rewards, specified by an external
supervisor, within fully observable environments. However, many real-world
problems involve partial observations, formulated as partially observable
Markov decision processes (POMDPs). Previous studies have tackled RL in POMDPs
by either incorporating the memory of past actions and observations or by
inferring the true state of the environment from observed data. However,
aggregating observed data over time becomes impractical in continuous spaces.
Moreover, inference-based RL approaches often require many samples to perform
well, as they focus solely on reward maximization and neglect uncertainty in
the inferred state. Active inference (AIF) is a framework formulated in POMDPs
and directs agents to select actions by minimizing a function called expected
free energy (EFE). This supplies reward-maximizing (exploitative) behaviour, as
in RL, with information-seeking (exploratory) behaviour. Despite this
exploratory behaviour of AIF, its usage is limited to discrete spaces due to
the computational challenges associated with EFE. In this paper, we propose a
unified principle that establishes a theoretical connection between AIF and RL,
enabling seamless integration of these two approaches and overcoming their
aforementioned limitations in continuous space POMDP settings. We substantiate
our findings with theoretical analysis, providing novel perspectives for
utilizing AIF in the design of artificial agents. Experimental results
demonstrate the superior learning capabilities of our method in solving
continuous space partially observable tasks. Notably, our approach harnesses
information-seeking exploration, enabling it to effectively solve reward-free
problems and rendering explicit task reward design by an external supervisor
optional.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malekzadeh_P/0/1/0/all/0/1&quot;&gt;Parvin Malekzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos N. Plataniotis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15954">
<title>TraffNet: Learning Causality of Traffic Generation for What-if Prediction. (arXiv:2303.15954v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15954</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time what-if traffic prediction is crucial for decision making in
intelligent traffic management and control. Although current deep learning
methods demonstrate significant advantages in traffic prediction, they are
powerless in what-if traffic prediction due to their nature of
correlation-based. Here, we present a simple deep learning framework called
TraffNet that learns the mechanisms of traffic generation for what-if
prediction from vehicle trajectory data. First, we use a heterogeneous graph to
represent the road network, allowing the model to incorporate causal features
of traffic flows, such as Origin-Destination (OD) demands and routes. Next, we
propose a method for learning segment representations, which involves modeling
the process of assigning OD demands onto the road network. The learned segment
representations effectively encapsulate the intricate causes of traffic
generation, facilitating downstream what-if traffic prediction. Finally, we
conduct experiments on synthetic datasets to evaluate the effectiveness of
TraffNet. The code and datasets of TraffNet is available at
https://github.com/mayunyi-1999/TraffNet_code.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Ming Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_Q/0/1/0/all/0/1&quot;&gt;Qiang Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruimin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunyi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Geqi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangfu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Haibo Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05614">
<title>Bilingual analogical proportions via hedges. (arXiv:2305.05614v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05614</link>
<description rdf:parseType="Literal">&lt;p&gt;Analogical proportions are expressions of the form ``$a$ is to $b$ what $c$
is to $d$&apos;&apos; at the core of analogical reasoning which itself is at the core of
human and artificial intelligence. The author has recently introduced {\em from
first principles} an abstract algebro-logical framework of analogical
proportions within the general setting of universal algebra and first-order
logic. In that framework, the source and target algebras have the {\em same}
underlying language. The purpose of this paper is to generalize his unilingual
framework to a bilingual one where the underlying languages may differ. This is
achieved by using hedges in justifications of proportions. The outcome is a
major generalization vastly extending the applicability of the underlying
framework. In a broader sense, this paper is a further step towards a
mathematical theory of analogical reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1&quot;&gt;Christian Anti&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16671">
<title>A Unified Approach for Maximizing Continuous DR-submodular Functions. (arXiv:2305.16671v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16671</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a unified approach for maximizing continuous
DR-submodular functions that encompasses a range of settings and oracle access
types. Our approach includes a Frank-Wolfe type offline algorithm for both
monotone and non-monotone functions, with different restrictions on the general
convex set. We consider settings where the oracle provides access to either the
gradient of the function or only the function value, and where the oracle
access is either deterministic or stochastic. We determine the number of
required oracle accesses in all cases. Our approach gives new/improved results
for nine out of the sixteen considered cases, avoids computationally expensive
projections in two cases, with the proposed framework matching performance of
state-of-the-art approaches in the remaining five cases. Notably, our approach
for the stochastic function value-based oracle enables the first regret bounds
with bandit feedback for stochastic DR-submodular functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedramfar_M/0/1/0/all/0/1&quot;&gt;Mohammad Pedramfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinn_C/0/1/0/all/0/1&quot;&gt;Christopher John Quinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06251">
<title>Design Principles for Model Generalization and Scalable AI Integration in Radio Access Networks. (arXiv:2306.06251v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06251</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) has emerged as a powerful tool for addressing
complex and dynamic tasks in radio communication systems. Research in this
area, however, focused on AI solutions for specific, limited conditions,
hindering models from learning and adapting to generic situations, such as
those met across radio communication systems.
&lt;/p&gt;
&lt;p&gt;This paper emphasizes the pivotal role of achieving model generalization in
enhancing performance and enabling scalable AI integration within radio
communications. We outline design principles for model generalization in three
key domains: environment for robustness, intents for adaptability to system
objectives, and control tasks for reducing AI-driven control loops.
Implementing these principles can decrease the number of models deployed and
increase adaptability in diverse radio communication environments. To address
the challenges of model generalization in communication systems, we propose a
learning architecture that leverages centralization of training and data
management functionalities, combined with distributed data generation. We
illustrate these concepts by designing a generalized link adaptation algorithm,
demonstrating the benefits of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soldati_P/0/1/0/all/0/1&quot;&gt;Pablo Soldati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghadimi_E/0/1/0/all/0/1&quot;&gt;Euhanna Ghadimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirel_B/0/1/0/all/0/1&quot;&gt;Burak Demirel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaigalas_R/0/1/0/all/0/1&quot;&gt;Raimundas Gaigalas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sintorn_M/0/1/0/all/0/1&quot;&gt;Mathias Sintorn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09910">
<title>LabelBench: A Comprehensive Framework for Benchmarking Adaptive Label-Efficient Learning. (arXiv:2306.09910v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09910</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeled data are critical to modern machine learning applications, but
obtaining labels can be expensive. To mitigate this cost, machine learning
methods, such as transfer learning, semi-supervised learning and active
learning, aim to be label-efficient: achieving high predictive performance from
relatively few labeled examples. While obtaining the best label-efficiency in
practice often requires combinations of these techniques, existing benchmark
and evaluation frameworks do not capture a concerted combination of all such
techniques. This paper addresses this deficiency by introducing LabelBench, a
new computationally-efficient framework for joint evaluation of multiple
label-efficient learning techniques. As an application of LabelBench, we
introduce a novel benchmark of state-of-the-art active learning methods in
combination with semi-supervised learning for fine-tuning pretrained vision
transformers. Our benchmark demonstrates better label-efficiencies than
previously reported in active learning. LabelBench&apos;s modular codebase is
open-sourced for the broader community to contribute label-efficient learning
methods and benchmarks. The repository can be found at:
https://github.com/EfficientTraining/LabelBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jifan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canal_G/0/1/0/all/0/1&quot;&gt;Gregory Canal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav M. Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeffrey Bilmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon Shaolei Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamieson_K/0/1/0/all/0/1&quot;&gt;Kevin Jamieson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowak_R/0/1/0/all/0/1&quot;&gt;Robert D Nowak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11086">
<title>Enhancing variational quantum state diagonalization using reinforcement learning techniques. (arXiv:2306.11086v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11086</link>
<description rdf:parseType="Literal">&lt;p&gt;The variational quantum algorithms are crucial for the application of NISQ
computers. Such algorithms require short quantum circuits, which are more
amenable to implementation on near-term hardware, and many such methods have
been developed. One of particular interest is the so-called variational quantum
state diagonalization method, which constitutes an important algorithmic
subroutine and can be used directly to work with data encoded in quantum
states. In particular, it can be applied to discern the features of quantum
states, such as entanglement properties of a system, or in quantum machine
learning algorithms. In this work, we tackle the problem of designing a very
shallow quantum circuit, required in the quantum state diagonalization task, by
utilizing reinforcement learning (RL). We use a novel encoding method for the
RL-state, a dense reward function, and an $\epsilon$-greedy policy to achieve
this. We demonstrate that the circuits proposed by the reinforcement learning
methods are shallower than the standard variational quantum state
diagonalization algorithm and thus can be used in situations where hardware
capabilities limit the depth of quantum circuits. The methods we propose in the
paper can be readily adapted to address a wide range of variational quantum
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Kundu_A/0/1/0/all/0/1&quot;&gt;Akash Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Bedelek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Bede&amp;#x142;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Ostaszewski_M/0/1/0/all/0/1&quot;&gt;Mateusz Ostaszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Danaci_O/0/1/0/all/0/1&quot;&gt;Onur Danaci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Patel_Y/0/1/0/all/0/1&quot;&gt;Yash J. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Dunjko_V/0/1/0/all/0/1&quot;&gt;Vedran Dunjko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Miszczak_J/0/1/0/all/0/1&quot;&gt;Jaros&amp;#x142;aw A. Miszczak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15632">
<title>Asynchronous Algorithmic Alignment with Cocycles. (arXiv:2306.15632v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15632</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art neural algorithmic reasoners make use of message passing in
graph neural networks (GNNs). But typical GNNs blur the distinction between the
definition and invocation of the message function, forcing a node to send
messages to its neighbours at every layer, synchronously. When applying GNNs to
learn to execute dynamic programming algorithms, however, on most steps only a
handful of the nodes would have meaningful updates to send. One, hence, runs
the risk of inefficiencies by sending too much irrelevant data across the
graph. But more importantly, many intermediate GNN steps have to learn the
identity functions, which is a non-trivial learning problem. In this work, we
explicitly separate the concepts of node state update and message function
invocation. With this separation, we obtain a mathematical formulation that
allows us to reason about asynchronous computation in both algorithms and
neural networks. Our analysis yields several practical implementations of
synchronous scalable GNN layers that are provably invariant under various forms
of asynchrony.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudzik_A/0/1/0/all/0/1&quot;&gt;Andrew Dudzik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glehn_T/0/1/0/all/0/1&quot;&gt;Tamara von Glehn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17010">
<title>milliFlow: Scene Flow Estimation on mmWave Radar Point Cloud for Human Motion Sensing. (arXiv:2306.17010v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17010</link>
<description rdf:parseType="Literal">&lt;p&gt;Approaching the era of ubiquitous computing, human motion sensing plays a
crucial role in smart systems for decision making, user interaction, and
personalized services. Extensive research has been conducted on human tracking,
pose estimation, gesture recognition, and activity recognition, which are
predominantly based on cameras in traditional methods. However, the intrusive
nature of cameras limits their use in smart home applications. To address this,
mmWave radars have gained popularity due to their privacy-friendly features. In
this work, we propose milliFlow, a novel deep learning method for scene flow
estimation as a complementary motion information for mmWave point cloud,
serving as an intermediate level of features and directly benefiting downstream
human motion sensing tasks. Experimental results demonstrate the superior
performance of our method with an average 3D endpoint error of 4.6cm,
significantly surpassing the competing approaches. Furthermore, by
incorporating scene flow information, we achieve remarkable improvements in
human activity recognition, human parsing, and human body part tracking. To
foster further research in this area, we will provide our codebase and dataset
for open access upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_F/0/1/0/all/0/1&quot;&gt;Fangqiang Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peijun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Xiaoxuan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06857">
<title>Lightweight reranking for language model generations. (arXiv:2307.06857v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06857</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) can exhibit considerable variation in the
quality of their sampled outputs. Reranking and selecting the best generation
from the sampled set is a popular way of obtaining strong gains in generation
quality. In this paper, we present a novel approach for reranking LLM
generations. Unlike other techniques that might involve additional inferences
or training a specialized reranker, our approach relies on easy to compute
pairwise statistics between the generations that have minimal compute overhead.
We show that our approach can be formalized as an extension of self-consistency
and analyze its performance in that framework, theoretically as well as via
simulations. We show strong improvements for selecting the best k generations
for code generation tasks as well as robust improvements for the best
generation for the tasks of autoformalization, summarization, and translation.
While our approach only assumes black-box access to LLMs, we show that
additional access to token probabilities can improve performance even further.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Siddhartha Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaofei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1&quot;&gt;Anoop Deoras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1&quot;&gt;Bing Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.08535">
<title>Visual Speech Recognition for Languages with Limited Labeled Data using Automatic Labels from Whisper. (arXiv:2309.08535v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.08535</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a powerful Visual Speech Recognition (VSR) method for
multiple languages, especially for low-resource languages that have a limited
number of labeled data. Different from previous methods that tried to improve
the VSR performance for the target language by using knowledge learned from
other languages, we explore whether we can increase the amount of training data
itself for the different languages without human intervention. To this end, we
employ a Whisper model which can conduct both language identification and
audio-based speech recognition. It serves to filter data of the desired
languages and transcribe labels from the unannotated, multilingual audio-visual
data pool. By comparing the performances of VSR models trained on automatic
labels and the human-annotated labels, we show that we can achieve similar VSR
performance to that of human-annotated labels even without utilizing human
annotations. Through the automated labeling process, we label large-scale
unlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours
of data for four low VSR resource languages, French, Italian, Spanish, and
Portuguese. With the automatic labels, we achieve new state-of-the-art
performance on mTEDx in four languages, significantly surpassing the previous
methods. The automatic labels are available online:
https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeo_J/0/1/0/all/0/1&quot;&gt;Jeong Hun Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_Y/0/1/0/all/0/1&quot;&gt;Yong Man Ro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13745">
<title>Overview of Computer Vision Techniques in Robotized Wire Harness Assembly: Current State and Future Opportunities. (arXiv:2309.13745v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13745</link>
<description rdf:parseType="Literal">&lt;p&gt;Wire harnesses are essential hardware for electronic systems in modern
automotive vehicles. With a shift in the automotive industry towards
electrification and autonomous driving, more and more automotive electronics
are responsible for energy transmission and safety-critical functions such as
maneuvering, driver assistance, and safety system. This paradigm shift places
more demand on automotive wire harnesses from the safety perspective and
stresses the greater importance of high-quality wire harness assembly in
vehicles. However, most of the current operations of wire harness assembly are
still performed manually by skilled workers, and some of the manual processes
are problematic in terms of quality control and ergonomics. There is also a
persistent demand in the industry to increase competitiveness and gain market
share. Hence, assuring assembly quality while improving ergonomics and
optimizing labor costs is desired. Robotized assembly, accomplished by robots
or in human-robot collaboration, is a key enabler for fulfilling the
increasingly demanding quality and safety as it enables more replicable,
transparent, and comprehensible processes than completely manual operations.
However, robotized assembly of wire harnesses is challenging in practical
environments due to the flexibility of the deformable objects, though many
preliminary automation solutions have been proposed under simplified industrial
configurations. Previous research efforts have proposed the use of computer
vision technology to facilitate robotized automation of wire harness assembly,
enabling the robots to better perceive and manipulate the flexible wire
harness. This article presents an overview of computer vision technology
proposed for robotized wire harness assembly and derives research gaps that
require further study to facilitate a more practical robotized assembly of wire
harnesses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salunkhe_O/0/1/0/all/0/1&quot;&gt;Omkar Salunkhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quadrini_W/0/1/0/all/0/1&quot;&gt;Walter Quadrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamkull_D/0/1/0/all/0/1&quot;&gt;Dan L&amp;#xe4;mkull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ore_F/0/1/0/all/0/1&quot;&gt;Fredrik Ore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Johansson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stahre_J/0/1/0/all/0/1&quot;&gt;Johan Stahre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02772">
<title>Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02772</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose a new paradigm for training spiking neural
networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are
energy-efficient but difficult to train. Consequently, many researchers have
proposed various methods to solve this problem, among which online training
through time (OTTT) is a method that allows inferring at each time step while
suppressing the memory cost. However, to compute efficiently on GPUs, OTTT
requires operations with spike trains and weighted summation of spike trains
during forwarding. In addition, OTTT has shown a relationship with the Spike
Representation, an alternative training method, though theoretical agreement
with Spike Representation has yet to be proven. Our proposed method can solve
these problems; namely, SAF can halve the number of operations during the
forward process, and it can be theoretically proven that SAF is consistent with
the Spike Representation and OTTT, respectively. Furthermore, we confirmed the
above contents through experiments and showed that it is possible to reduce
memory and training time while maintaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saiin_R/0/1/0/all/0/1&quot;&gt;Ryuji Saiin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirakawa_T/0/1/0/all/0/1&quot;&gt;Tomoya Shirakawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshihara_S/0/1/0/all/0/1&quot;&gt;Sota Yoshihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawada_Y/0/1/0/all/0/1&quot;&gt;Yoshihide Sawada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusumoto_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Kusumoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14403">
<title>O3D: Offline Data-driven Discovery and Distillation for Sequential Decision-Making with Large Language Models. (arXiv:2310.14403v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14403</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have exhibited promising
performance in solving sequential decision-making problems. By imitating
few-shot examples provided in the prompts (i.e., in-context learning), an LLM
agent can interact with an external environment and complete given tasks
without additional training. However, such few-shot examples are often
insufficient to generate high-quality solutions for complex and long-horizon
tasks, while the limited context length cannot consume larger-scale
demonstrations. To this end, we propose an offline learning framework that
utilizes offline data at scale (e.g, logs of human interactions) to facilitate
the in-context learning performance of LLM agents. We formally define
LLM-powered policies with both text-based approaches and code-based approaches.
We then introduce an Offline Data-driven Discovery and Distillation (O3D)
framework to improve LLM-powered policies without finetuning. O3D automatically
discovers reusable skills and distills generalizable knowledge across multiple
tasks based on offline interaction data, advancing the capability of solving
downstream tasks. Empirical results under two interactive decision-making
benchmarks (ALFWorld and WebShop) demonstrate that O3D can notably enhance the
decision-making capabilities of LLMs through the offline discovery and
distillation process, and consistently outperform baselines across various LLMs
with both text-based-policy and code-based-policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yuchen Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yanchao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengda Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhushani_U/0/1/0/all/0/1&quot;&gt;Udari Madhushani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vann_J/0/1/0/all/0/1&quot;&gt;Jared Vann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1&quot;&gt;Deepeka Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_S/0/1/0/all/0/1&quot;&gt;Sumitra Ganesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01811">
<title>DiffDub: Person-generic Visual Dubbing Using Inpainting Renderer with Diffusion Auto-encoder. (arXiv:2311.01811v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01811</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating high-quality and person-generic visual dubbing remains a
challenge. Recent innovation has seen the advent of a two-stage paradigm,
decoupling the rendering and lip synchronization process facilitated by
intermediate representation as a conduit. Still, previous methodologies rely on
rough landmarks or are confined to a single speaker, thus limiting their
performance. In this paper, we propose DiffDub: Diffusion-based dubbing. We
first craft the Diffusion auto-encoder by an inpainting renderer incorporating
a mask to delineate editable zones and unaltered regions. This allows for
seamless filling of the lower-face region while preserving the remaining parts.
Throughout our experiments, we encountered several challenges. Primarily, the
semantic encoder lacks robustness, constricting its ability to capture
high-level features. Besides, the modeling ignored facial positioning, causing
mouth or nose jitters across frames. To tackle these issues, we employ
versatile strategies, including data augmentation and supplementary eye
guidance. Moreover, we encapsulated a conformer-based reference encoder and
motion generator fortified by a cross-attention mechanism. This enables our
model to learn person-specific textures with varying references and reduces
reliance on paired audio-visual data. Our rigorous experiments comprehensively
highlight that our ground-breaking approach outpaces existing methods with
considerable margins and delivers seamless, intelligible videos in
person-generic and multilingual scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chenpeng Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Shuai Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feilong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_K/0/1/0/all/0/1&quot;&gt;Kai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03865">
<title>When Fairness Meets Privacy: Exploring Privacy Threats in Fair Binary Classifiers through Membership Inference Attacks. (arXiv:2311.03865v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03865</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous studies have developed fairness methods for biased models that
exhibit discriminatory behaviors towards specific subgroups. While these models
have shown promise in achieving fair predictions, recent research has
identified their potential vulnerability to score-based membership inference
attacks (MIAs). In these attacks, adversaries can infer whether a particular
data sample was used during training by analyzing the model&apos;s prediction
scores. However, our investigations reveal that these score-based MIAs are
ineffective when targeting fairness-enhanced models in binary classifications.
The attack models trained to launch the MIAs degrade into simplistic threshold
models, resulting in lower attack performance. Meanwhile, we observe that
fairness methods often lead to prediction performance degradation for the
majority subgroups of the training data. This raises the barrier to successful
attacks and widens the prediction gaps between member and non-member data.
Building upon these insights, we propose an efficient MIA method against
fairness-enhanced models based on fairness discrepancy results (FD-MIA). It
leverages the difference in the predictions from both the original and
fairness-enhanced models and exploits the observed prediction gaps as attack
clues. We also explore potential strategies for mitigating privacy leakages.
Extensive experiments validate our findings and demonstrate the efficacy of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Huan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guangsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tianqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wanlei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07946">
<title>The Impact of Adversarial Node Placement in Decentralized Federated Learning Networks. (arXiv:2311.07946v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07946</link>
<description rdf:parseType="Literal">&lt;p&gt;As Federated Learning (FL) grows in popularity, new decentralized frameworks
are becoming widespread. These frameworks leverage the benefits of
decentralized environments to enable fast and energy-efficient inter-device
communication. However, this growing popularity also intensifies the need for
robust security measures. While existing research has explored various aspects
of FL security, the role of adversarial node placement in decentralized
networks remains largely unexplored. This paper addresses this gap by analyzing
the performance of decentralized FL for various adversarial placement
strategies when adversaries can jointly coordinate their placement within a
network. We establish two baseline strategies for placing adversarial node:
random placement and network centrality-based placement. Building on this
foundation, we propose a novel attack algorithm that prioritizes adversarial
spread over adversarial centrality by maximizing the average network distance
between adversaries. We show that the new attack algorithm significantly
impacts key performance metrics such as testing accuracy, outperforming the
baseline frameworks by between 9% and 66.5% for the considered setups. Our
findings provide valuable insights into the vulnerabilities of decentralized FL
systems, setting the stage for future research aimed at developing more secure
and robust decentralized FL frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piaseczny_A/0/1/0/all/0/1&quot;&gt;Adam Piaseczny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruzomberka_E/0/1/0/all/0/1&quot;&gt;Eric Ruzomberka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parasnis_R/0/1/0/all/0/1&quot;&gt;Rohit Parasnis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brinton_C/0/1/0/all/0/1&quot;&gt;Christopher G. Brinton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17431">
<title>Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v8 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17431</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Tao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hanlin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaojin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01655">
<title>Quantum Polar Metric Learning: Efficient Classically Learned Quantum Embeddings. (arXiv:2312.01655v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01655</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep metric learning has recently shown extremely promising results in the
classical data domain, creating well-separated feature spaces. This idea was
also adapted to quantum computers via Quantum Metric Learning(QMeL). QMeL
consists of a 2 step process with a classical model to compress the data to fit
into the limited number of qubits, then train a Parameterized Quantum
Circuit(PQC) to create better separation in Hilbert Space. However, on Noisy
Intermediate Scale Quantum (NISQ) devices. QMeL solutions result in high
circuit width and depth, both of which limit scalability. We propose Quantum
Polar Metric Learning (QPMeL) that uses a classical model to learn the
parameters of the polar form of a qubit. We then utilize a shallow PQC with
$R_y$ and $R_z$ gates to create the state and a trainable layer of
$ZZ(\theta)$-gates to learn entanglement. The circuit also computes fidelity
via a SWAP Test for our proposed Fidelity Triplet Loss function, used to train
both classical and quantum components. When compared to QMeL approaches, QPMeL
achieves 3X better multi-class separation, while using only 1/2 the number of
gates and depth. We also demonstrate that QPMeL outperforms classical networks
with similar configurations, presenting a promising avenue for future research
on fully classical models with quantum loss functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Sharma_V/0/1/0/all/0/1&quot;&gt;Vinayak Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Aviral Shrivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14890">
<title>NPHardEval: Dynamic Benchmark on Reasoning Ability of Large Language Models via Complexity Classes. (arXiv:2312.14890v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14890</link>
<description rdf:parseType="Literal">&lt;p&gt;Complex reasoning ability is one of the most important features of current
LLMs, which has also been leveraged to play an integral role in complex
decision-making tasks. Therefore, the investigation into the reasoning
capabilities of Large Language Models (LLMs) is critical: numerous benchmarks
have been established to assess the reasoning abilities of LLMs. However,
current benchmarks are inadequate in offering a rigorous evaluation of the full
extent of reasoning abilities that LLMs are capable of achieving. They are also
prone to the risk of overfitting, as these benchmarks, being publicly
accessible and static, allow models to potentially tailor their responses to
specific benchmark metrics, thereby inflating their performance. Addressing
these limitations, our research introduces a new benchmark, named NPHardEval.
This benchmark is designed to evaluate the reasoning abilities of LLMs across a
broad spectrum of 900 algorithmic questions, extending up to the NP-Hard
complexity class. These questions are meticulously chosen to represent a wide
range of complexity class below the NP-hard complexity class, offering a
rigorous measure of the reasoning ability of LLMs. Through this study, we shed
light on the current state of reasoning in LLMs, providing an objective and
rigorous perspective through the comparison of LLMs&apos; performance across complex
classes. Moreover, this benchmark is designed with a dynamic update mechanism,
where the datapoints are refreshed on a monthly basis. Such regular updates
play a crucial role in mitigating the risk of LLMs overfitting to the
benchmark, promoting a more accurate and reliable assessment of their reasoning
capabilities. The benchmark dataset and code of NPHardEval are available at
https://github.com/casmlab/NPHardEval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lizhou Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingyao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Haoyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16015">
<title>A Comprehensive Survey of Evaluation Techniques for Recommendation Systems. (arXiv:2312.16015v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16015</link>
<description rdf:parseType="Literal">&lt;p&gt;The effectiveness of recommendation systems is pivotal to user engagement and
satisfaction in online platforms. As these recommendation systems increasingly
influence user choices, their evaluation transcends mere technical performance
and becomes central to business success. This paper addresses the multifaceted
nature of recommendations system evaluation by introducing a comprehensive
suite of metrics, each tailored to capture a distinct aspect of system
performance. We discuss
&lt;/p&gt;
&lt;p&gt;* Similarity Metrics: to quantify the precision of content-based filtering
mechanisms and assess the accuracy of collaborative filtering techniques.
&lt;/p&gt;
&lt;p&gt;* Candidate Generation Metrics: to evaluate how effectively the system
identifies a broad yet relevant range of items.
&lt;/p&gt;
&lt;p&gt;* Predictive Metrics: to assess the accuracy of forecasted user preferences.
&lt;/p&gt;
&lt;p&gt;* Ranking Metrics: to evaluate the effectiveness of the order in which
recommendations are presented.
&lt;/p&gt;
&lt;p&gt;* Business Metrics: to align the performance of the recommendation system
with economic objectives.
&lt;/p&gt;
&lt;p&gt;Our approach emphasizes the contextual application of these metrics and their
interdependencies. In this paper, we identify the strengths and limitations of
current evaluation practices and highlight the nuanced trade-offs that emerge
when optimizing recommendation systems across different metrics. The paper
concludes by proposing a framework for selecting and interpreting these metrics
to not only improve system performance but also to advance business goals. This
work is to aid researchers and practitioners in critically assessing
recommendation systems and fosters the development of more nuanced, effective,
and economically viable personalization strategies. Our code is available at
GitHub -
https://github.com/aryan-jadon/Evaluation-Metrics-for-Recommendation-Systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jadon_A/0/1/0/all/0/1&quot;&gt;Aryan Jadon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patil_A/0/1/0/all/0/1&quot;&gt;Avinash Patil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01055">
<title>LLaMA Beyond English: An Empirical Study on Language Capability Transfer. (arXiv:2401.01055v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01055</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, substantial advancements have been witnessed in large
language models (LLMs), exemplified by ChatGPT, showcasing remarkable
proficiency across a range of complex tasks. However, many mainstream LLMs
(e.g. LLaMA) are pretrained on English-dominant corpus, which limits their
performance in other non-English languages. In this paper, we focus on how to
effectively transfer the capabilities of language generation and following
instructions to a non-English language. To answer this question, we conduct an
extensive empirical investigation based on LLaMA, accumulating over 1440 GPU
hours. We analyze the impact of key factors such as vocabulary extension,
further pretraining, and instruction tuning on transfer. To accurately assess
the model&apos;s level of knowledge, we employ four widely used standardized testing
benchmarks: C-Eval, MMLU, AGI-Eval, and GAOKAO-Bench. Furthermore, a
comprehensive evaluation of the model&apos;s response quality is conducted,
considering aspects such as accuracy, fluency, informativeness, logical
coherence, and harmlessness, based on LLM-Eval, a benchmarks consisting
instruction tasks from 17 diverse categories. Our evaluation results
demonstrate that comparable performance to state-of-the-art transfer models can
be achieved with less than 1% of the pretraining data, both in terms of
knowledge alignment and response quality. Furthermore, the experimental
outcomes across the thirteen low-resource languages also exhibit similar
trends. We anticipate that the conclusions revealed by the experiments will aid
the community in developing non-English LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Luhui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01623">
<title>Can AI Be as Creative as Humans?. (arXiv:2401.01623v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01623</link>
<description rdf:parseType="Literal">&lt;p&gt;Creativity serves as a cornerstone for societal progress and innovation. With
the rise of advanced generative AI models capable of tasks once reserved for
human creativity, the study of AI&apos;s creative potential becomes imperative for
its responsible development and application. In this paper, we provide a
theoretical answer to the question of whether AI can be creative. We prove in
theory that AI can be as creative as humans under the condition that AI can fit
the existing data generated by human creators. Therefore, the debate on AI&apos;s
creativity is reduced into the question of its ability of fitting a massive
amount of data. To arrive at this conclusion, this paper first addresses the
complexities in defining creativity by introducing a new concept called
Relative Creativity. Instead of trying to define creativity universally, we
shift the focus to whether AI can match the creative abilities of a
hypothetical human. This perspective draws inspiration from the Turing Test,
expanding upon it to address the challenges and subjectivities inherent in
assessing creativity. This methodological shift leads to a statistically
quantifiable assessment of AI&apos;s creativity, which we term Statistical
Creativity. This concept allows for comparisons of AI&apos;s creative abilities with
those of specific human groups, and facilitates the theoretical findings of
AI&apos;s creative potential. Building on this foundation, we discuss the
application of statistical creativity in prompt-conditioned autoregressive
models, providing a practical means for evaluating creative abilities of
contemporary AI models, such as Large Language Models (LLMs). In addition to
defining and analyzing creativity, we introduce an actionable training
guideline, effectively bridging the gap between theoretical quantification of
creativity and practical model training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1&quot;&gt;Michael Mozer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anirudh Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamb_A/0/1/0/all/0/1&quot;&gt;Alex Lamb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linjun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie J Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_M/0/1/0/all/0/1&quot;&gt;Michael Qizhe Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_H/0/1/0/all/0/1&quot;&gt;Hannah Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04003">
<title>Simultaneous Task Allocation and Planning for Multi-Robots under Hierarchical Temporal Logic Specifications. (arXiv:2401.04003v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04003</link>
<description rdf:parseType="Literal">&lt;p&gt;Past research into robotic planning with temporal logic specifications,
notably Linear Temporal Logic (LTL), was largely based on singular formulas for
individual or groups of robots. But with increasing task complexity, LTL
formulas unavoidably grow lengthy, complicating interpretation and
specification generation, and straining the computational capacities of the
planners. By leveraging the intrinsic structure of tasks, we introduced a
hierarchical structure to LTL specifications with requirements on syntax and
semantics, and proved that they are more expressive than their flat
counterparts. Second, we employ a search-based approach to synthesize plans for
a multi-robot system, accomplishing simultaneous task allocation and planning.
The search space is approximated by loosely interconnected sub-spaces, with
each sub-space corresponding to one LTL specification. The search is
predominantly confined to a single sub-space, transitioning to another
sub-space under certain conditions, determined by the decomposition of
automatons. Moreover, multiple heuristics are formulated to expedite the search
significantly. A theoretical analysis concerning completeness and optimality is
conducted under mild assumptions. When compared with existing methods on
service tasks, our method outperforms in terms of execution times with
comparable solution quality. Finally, scalability is evaluated by testing a
group of 30 robots and achieving reasonable runtimes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xusheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04531">
<title>MERA: A Comprehensive LLM Evaluation in Russian. (arXiv:2401.04531v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04531</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, one of the most notable advancements in AI research
has been in foundation models (FMs), headlined by the rise of language models
(LMs). As the models&apos; size increases, LMs demonstrate enhancements in
measurable aspects and the development of new qualitative features. However,
despite researchers&apos; attention and the rapid growth in LM application, the
capabilities, limitations, and associated risks still need to be better
understood. To address these issues, we introduce an open Multimodal Evaluation
of Russian-language Architectures (MERA), a new instruction benchmark for
evaluating foundation models oriented towards the Russian language. The
benchmark encompasses 21 evaluation tasks for generative models in 11 skill
domains and is designed as a black-box test to ensure the exclusion of data
leakage. The paper introduces a methodology to evaluate FMs and LMs in zero-
and few-shot fixed instruction settings that can be extended to other
modalities. We propose an evaluation methodology, an open-source code base for
the MERA assessment, and a leaderboard with a submission system. We evaluate
open LMs as baselines and find that they are still far behind the human level.
We publicly release MERA to guide forthcoming research, anticipate
groundbreaking model features, standardize the evaluation procedure, and
address potential societal drawbacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1&quot;&gt;Alena Fenogenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chervyakov_A/0/1/0/all/0/1&quot;&gt;Artem Chervyakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martynov_N/0/1/0/all/0/1&quot;&gt;Nikita Martynov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozlova_A/0/1/0/all/0/1&quot;&gt;Anastasia Kozlova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhonova_M/0/1/0/all/0/1&quot;&gt;Maria Tikhonova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akhmetgareeva_A/0/1/0/all/0/1&quot;&gt;Albina Akhmetgareeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emelyanov_A/0/1/0/all/0/1&quot;&gt;Anton Emelyanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shevelev_D/0/1/0/all/0/1&quot;&gt;Denis Shevelev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebedev_P/0/1/0/all/0/1&quot;&gt;Pavel Lebedev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinev_L/0/1/0/all/0/1&quot;&gt;Leonid Sinev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isaeva_U/0/1/0/all/0/1&quot;&gt;Ulyana Isaeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolomeytseva_K/0/1/0/all/0/1&quot;&gt;Katerina Kolomeytseva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskovskiy_D/0/1/0/all/0/1&quot;&gt;Daniil Moskovskiy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goncharova_E/0/1/0/all/0/1&quot;&gt;Elizaveta Goncharova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savushkin_N/0/1/0/all/0/1&quot;&gt;Nikita Savushkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikhailova_P/0/1/0/all/0/1&quot;&gt;Polina Mikhailova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitrov_D/0/1/0/all/0/1&quot;&gt;Denis Dimitrov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panchenko_A/0/1/0/all/0/1&quot;&gt;Alexander Panchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markov_S/0/1/0/all/0/1&quot;&gt;Sergei Markov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04679">
<title>RoSA: Accurate Parameter-Efficient Fine-Tuning via Robust Adaptation. (arXiv:2401.04679v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04679</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate parameter-efficient fine-tuning (PEFT) methods that can
provide good accuracy under limited computational and memory budgets in the
context of large language models (LLMs). We present a new PEFT method called
Robust Adaptation (RoSA) inspired by robust principal component analysis (PCA)
that jointly trains $\textit{low-rank}$ and $\textit{highly-sparse}$ components
on top of a set of fixed pretrained weights to efficiently approximate the
performance of a full-fine-tuning (FFT) solution. Across a series of
challenging generative tasks such as grade-school math and SQL query
generation, which require fine-tuning for good performance, we show that RoSA
outperforms both LoRA and pure sparse fine-tuning, at the same parameter
budget. We provide system support for RoSA to complement the training
algorithm, specifically in the form of sparse GPU kernels which enable memory-
and computationally-efficient training. Our code will be made available at
https://github.com/IST-DASLab/RoSA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikdan_M/0/1/0/all/0/1&quot;&gt;Mahdi Nikdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabesh_S/0/1/0/all/0/1&quot;&gt;Soroush Tabesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alistarh_D/0/1/0/all/0/1&quot;&gt;Dan Alistarh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05437">
<title>Representation Learning for Wearable-Based Applications in the Case of Missing Data. (arXiv:2401.05437v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05437</link>
<description rdf:parseType="Literal">&lt;p&gt;Wearable devices continuously collect sensor data and use it to infer an
individual&apos;s behavior, such as sleep, physical activity, and emotions. Despite
the significant interest and advancements in this field, modeling multimodal
sensor data in real-world environments is still challenging due to low data
quality and limited data annotations. In this work, we investigate
representation learning for imputing missing wearable data and compare it with
state-of-the-art statistical approaches. We investigate the performance of the
transformer model on 10 physiological and behavioral signals with different
masking ratios. Our results show that transformers outperform baselines for
missing data imputation of signals that change more frequently, but not for
monotonic signals. We further investigate the impact of imputation strategies
and masking rations on downstream classification tasks. Our study provides
insights for the design and development of masking-based self-supervised
learning tasks and advocates the adoption of hybrid-based imputation strategies
to address the challenge of missing data in wearable devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jungo_J/0/1/0/all/0/1&quot;&gt;Janosch Jungo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yutong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gashi_S/0/1/0/all/0/1&quot;&gt;Shkurta Gashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Holz_C/0/1/0/all/0/1&quot;&gt;Christian Holz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05442">
<title>Functional Graphical Models: Structure Enables Offline Data-Driven Optimization. (arXiv:2401.05442v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05442</link>
<description rdf:parseType="Literal">&lt;p&gt;While machine learning models are typically trained to solve prediction
problems, we might often want to use them for optimization problems. For
example, given a dataset of proteins and their corresponding fluorescence
levels, we might want to optimize for a new protein with the highest possible
fluorescence. This kind of data-driven optimization (DDO) presents a range of
challenges beyond those in standard prediction problems, since we need models
that successfully predict the performance of new designs that are better than
the best designs seen in the training set. It is not clear theoretically when
existing approaches can even perform better than the naive approach that simply
selects the best design in the dataset. In this paper, we study how structure
can enable sample-efficient data-driven optimization. To formalize the notion
of structure, we introduce functional graphical models (FGMs) and show
theoretically how they can provide for principled data-driven optimization by
decomposing the original high-dimensional optimization problem into smaller
sub-problems. This allows us to derive much more practical regret bounds for
DDO, and the result implies that DDO with FGMs can achieve nearly optimal
designs in situations where naive approaches fail due to insufficient coverage
of the offline data. We further present a data-driven optimization algorithm
that inferes the FGM structure itself, either over the original input variables
or a latent variable representation of the inputs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuba_J/0/1/0/all/0/1&quot;&gt;Jakub Grudzien Kuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uehara_M/0/1/0/all/0/1&quot;&gt;Masatoshi Uehara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05566">
<title>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05566</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1&quot;&gt;Evan Hubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1&quot;&gt;Carson Denison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1&quot;&gt;Jesse Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1&quot;&gt;Mike Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Meg Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1&quot;&gt;Monte MacDiarmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1&quot;&gt;Tamera Lanham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1&quot;&gt;Daniel M. Ziegler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1&quot;&gt;Tim Maxwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Newton Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1&quot;&gt;Adam Jermyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1&quot;&gt;Amanda Askell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1&quot;&gt;Ansh Radhakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1&quot;&gt;Cem Anil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1&quot;&gt;Deep Ganguli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1&quot;&gt;Jack Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1&quot;&gt;Kamal Ndousse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1&quot;&gt;Kshitij Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1&quot;&gt;Michael Sellitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Mrinank Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1&quot;&gt;Nova DasSarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1&quot;&gt;Shauna Kravec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuntao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1&quot;&gt;Zachary Witten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1&quot;&gt;Marina Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1&quot;&gt;Jan Brauner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1&quot;&gt;Holden Karnofsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1&quot;&gt;Paul Christiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1&quot;&gt;Logan Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1&quot;&gt;Jared Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1&quot;&gt;Ryan Greenblatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1&quot;&gt;Buck Shlegeris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1&quot;&gt;Nicholas Schiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05949">
<title>Universal Vulnerabilities in Large Language Models: In-context Learning Backdoor Attacks. (arXiv:2401.05949v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05949</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning, a paradigm bridging the gap between pre-training and
fine-tuning, has demonstrated high efficacy in several NLP tasks, especially in
few-shot settings. Unlike traditional fine-tuning methods, in-context learning
adapts pre-trained models to unseen tasks without updating any parameters.
Despite being widely applied, in-context learning is vulnerable to malicious
attacks. In this work, we raise security concerns regarding this paradigm. Our
studies demonstrate that an attacker can manipulate the behavior of large
language models by poisoning the demonstration context, without the need for
fine-tuning the model. Specifically, we have designed a new backdoor attack
method, named ICLAttack, to target large language models based on in-context
learning. Our method encompasses two types of attacks: poisoning demonstration
examples and poisoning prompts, which can make models behave in accordance with
predefined intentions. ICLAttack does not require additional fine-tuning to
implant a backdoor, thus preserving the model&apos;s generality. Furthermore, the
poisoned examples are correctly labeled, enhancing the natural stealth of our
attack method. Extensive experimental results across several language models,
ranging in size from 1.3B to 40B parameters, demonstrate the effectiveness of
our attack method, exemplified by a high average attack success rate of 95.0%
across the three datasets on OPT models. Our findings highlight the
vulnerabilities of language models, and we hope this work will raise awareness
of the possible security threats associated with in-context learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shuai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_M/0/1/0/all/0/1&quot;&gt;Meihuizi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuan_L/0/1/0/all/0/1&quot;&gt;Luu Anh Tuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jinming Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06013">
<title>Surgical-DINO: Adapter Learning of Foundation Models for Depth Estimation in Endoscopic Surgery. (arXiv:2401.06013v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06013</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Depth estimation in robotic surgery is vital in 3D reconstruction,
surgical navigation and augmented reality visualization. Although the
foundation model exhibits outstanding performance in many vision tasks,
including depth estimation (e.g., DINOv2), recent works observed its
limitations in medical and surgical domain-specific applications. This work
presents a low-ranked adaptation (LoRA) of the foundation model for surgical
depth estimation. Methods: We design a foundation model-based depth estimation
method, referred to as Surgical-DINO, a low-rank adaptation of the DINOv2 for
depth estimation in endoscopic surgery. We build LoRA layers and integrate them
into DINO to adapt with surgery-specific domain knowledge instead of
conventional fine-tuning. During training, we freeze the DINO image encoder,
which shows excellent visual representation capacity, and only optimize the
LoRA layers and depth decoder to integrate features from the surgical scene.
Results: Our model is extensively validated on a MICCAI challenge dataset of
SCARED, which is collected from da Vinci Xi endoscope surgery. We empirically
show that Surgical-DINO significantly outperforms all the state-of-the-art
models in endoscopic depth estimation tasks. The analysis with ablation studies
has shown evidence of the remarkable effect of our LoRA layers and adaptation.
Conclusion: Surgical-DINO shed some light on the successful adaptation of the
foundation models into the surgical domain for depth estimation. There is clear
evidence in the results that zero-shot prediction on pre-trained weights in
computer vision datasets or naive fine-tuning is not sufficient to use the
foundation model in the surgical domain directly. Code is available at
https://github.com/BeileiCui/SurgicalDINO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_B/0/1/0/all/0/1&quot;&gt;Beilei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Mobarakol Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Long Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_H/0/1/0/all/0/1&quot;&gt;Hongliang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06080">
<title>Secrets of RLHF in Large Language Models Part II: Reward Modeling. (arXiv:2401.06080v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06080</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) has become a crucial
technology for aligning language models with human values and intentions,
enabling models to produce more helpful and harmless responses. Reward models
are trained as proxies for human preferences to drive reinforcement learning
optimization. While reward models are often considered central to achieving
high performance, they face the following challenges in practical applications:
(1) Incorrect and ambiguous preference pairs in the dataset may hinder the
reward model from accurately capturing human intent. (2) Reward models trained
on data from a specific distribution often struggle to generalize to examples
outside that distribution and are not suitable for iterative RLHF training.
&lt;/p&gt;
&lt;p&gt;In this report, we attempt to address these two issues. (1) From a data
perspective, we propose a method to measure the strength of preferences within
the data, based on a voting mechanism of multiple reward models. Experimental
results confirm that data with varying preference strengths have different
impacts on reward model performance. We introduce a series of novel methods to
mitigate the influence of incorrect and ambiguous preferences in the dataset
and fully leverage high-quality preference data. (2) From an algorithmic
standpoint, we introduce contrastive learning to enhance the ability of reward
models to distinguish between chosen and rejected responses, thereby improving
model generalization. Furthermore, we employ meta-learning to enable the reward
model to maintain the ability to differentiate subtle differences in
out-of-distribution samples, and this approach can be utilized for iterative
RLHF optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Binghai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_R/0/1/0/all/0/1&quot;&gt;Rui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_S/0/1/0/all/0/1&quot;&gt;Shihan Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Caishuang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;Senjie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Enyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chenyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Songyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiaoran Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zhiheng Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_T/0/1/0/all/0/1&quot;&gt;Tao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lixing Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_T/0/1/0/all/0/1&quot;&gt;Tao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanjing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06102">
<title>Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models. (arXiv:2401.06102v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06102</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspecting the information encoded in hidden representations of large
language models (LLMs) can explain models&apos; behavior and verify their alignment
with human values. Given the capabilities of LLMs in generating
human-understandable text, we propose leveraging the model itself to explain
its internal representations in natural language. We introduce a framework
called Patchscopes and show how it can be used to answer a wide range of
questions about an LLM&apos;s computation. We show that prior interpretability
methods based on projecting representations into the vocabulary space and
intervening on the LLM computation can be viewed as instances of this
framework. Moreover, several of their shortcomings such as failure in
inspecting early layers or lack of expressivity can be mitigated by
Patchscopes. Beyond unifying prior inspection techniques, Patchscopes also
opens up new possibilities such as using a more capable model to explain the
representations of a smaller model, and unlocks new applications such as
self-correction in multi-hop reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghandeharioun_A/0/1/0/all/0/1&quot;&gt;Asma Ghandeharioun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caciularu_A/0/1/0/all/0/1&quot;&gt;Avi Caciularu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pearce_A/0/1/0/all/0/1&quot;&gt;Adam Pearce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dixon_L/0/1/0/all/0/1&quot;&gt;Lucas Dixon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geva_M/0/1/0/all/0/1&quot;&gt;Mor Geva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02877">
<title>Inner-IoU: More Effective Intersection over Union Loss with Auxiliary Bounding Box. (arXiv:2311.02877v4 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.02877</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of detectors, Bounding Box Regression (BBR) loss
function has constantly updated and optimized. However, the existing IoU-based
BBR still focus on accelerating convergence by adding new loss terms, ignoring
the limitations of IoU loss term itself. Although theoretically IoU loss can
effectively describe the state of bounding box regression,in practical
applications, it cannot adjust itself according to different detectors and
detection tasks, and does not have strong generalization. Based on the above,
we first analyzed the BBR model and concluded that distinguishing different
regression samples and using different scales of auxiliary bounding boxes to
calculate losses can effectively accelerate the bounding box regression
process. For high IoU samples, using smaller auxiliary bounding boxes to
calculate losses can accelerate convergence, while larger auxiliary bounding
boxes are suitable for low IoU samples. Then, we propose Inner-IoU loss, which
calculates IoU loss through auxiliary bounding boxes. For different datasets
and detectors, we introduce a scaling factor ratio to control the scale size of
the auxiliary bounding boxes for calculating losses. Finally, integrate
Inner-IoU into the existing IoU-based loss functions for simulation and
comparative experiments. The experiment result demonstrate a further
enhancement in detection performance with the utilization of the method
proposed in this paper, verifying the effectiveness and generalization ability
of Inner-IoU loss. Code is available at
https://github.com/malagoutou/Inner-IoU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuaijie Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>