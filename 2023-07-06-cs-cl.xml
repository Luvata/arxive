<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-05T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01214" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.12926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10329" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10503" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12493" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14796" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00165" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.01201">
<title>Schema-learning and rebinding as mechanisms of in-context learning and emergence. (arXiv:2307.01201v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01201</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) is one of the most powerful and most unexpected
capabilities to emerge in recent transformer-based large language models
(LLMs). Yet the mechanisms that underlie it are poorly understood. In this
paper, we demonstrate that comparable ICL capabilities can be acquired by an
alternative sequence prediction learning method using clone-structured causal
graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike
transformer-based LLMs, they are {\em interpretable}, which considerably
simplifies the task of explaining how ICL works. Specifically, we show that it
uses a combination of (a) learning template (schema) circuits for pattern
completion, (b) retrieving relevant templates in a context-sensitive manner,
and (c) rebinding of novel tokens to appropriate slots in the templates. We go
on to marshall evidence for the hypothesis that similar mechanisms underlie ICL
in LLMs. For example, we find that, with CSCGs as with LLMs, different
capabilities emerge at different levels of overparameterization, suggesting
that overparameterization helps in learning more complex template (schema)
circuits. By showing how ICL can be achieved with small models and datasets, we
open up a path to novel architectures, and take a vital step towards a more
general understanding of the mechanics behind this important capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swaminathan_S/0/1/0/all/0/1&quot;&gt;Sivaramakrishnan Swaminathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dedieu_A/0/1/0/all/0/1&quot;&gt;Antoine Dedieu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raju_R/0/1/0/all/0/1&quot;&gt;Rajkumar Vasudeva Raju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanahan_M/0/1/0/all/0/1&quot;&gt;Murray Shanahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazaro_Gredilla_M/0/1/0/all/0/1&quot;&gt;Miguel Lazaro-Gredilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+George_D/0/1/0/all/0/1&quot;&gt;Dileep George&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01202">
<title>Predictive Patentomics: Forecasting Innovation Success and Valuation with ChatGPT. (arXiv:2307.01202v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01202</link>
<description rdf:parseType="Literal">&lt;p&gt;Analysis of innovation has been fundamentally limited by conventional
approaches to broad, structural variables. This paper pushes the boundaries,
taking an LLM approach to patent analysis with the groundbreaking ChatGPT
technology. OpenAI&apos;s state-of-the-art textual embedding accesses complex
information about the quality and impact of each invention to power deep
learning predictive models. The nuanced embedding drives a 24% incremental
improvement in R-squared predicting patent value and clearly isolates the worst
and best applications. These models enable a revision of the contemporary
Kogan, Papanikolaou, Seru, and Stoffman (2017) valuation of patents by a median
deviation of 1.5 times, accounting for potential institutional predictions.
Furthermore, the market fails to incorporate timely information about
applications; a long-short portfolio based on predicted acceptance rates
achieves significant abnormal returns of 3.3% annually. The models provide an
opportunity to revolutionize startup and small-firm corporate policy vis-a-vis
patenting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Stephen Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01209">
<title>Multi-Dialectal Representation Learning of Sinitic Phonology. (arXiv:2307.01209v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01209</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning techniques have shown their competence for representing and
reasoning in symbolic systems such as language and phonology. In Sinitic
Historical Phonology, notable tasks that could benefit from machine learning
include the comparison of dialects and reconstruction of proto-languages
systems. Motivated by this, this paper provides an approach for obtaining
multi-dialectal representations of Sinitic syllables, by constructing a
knowledge graph from structured phonological data, then applying the BoxE
technique from knowledge base learning. We applied unsupervised clustering
techniques to the obtained representations to observe that the representations
capture phonemic contrast from the input dialects. Furthermore, we trained
classifiers to perform inference of unobserved Middle Chinese labels, showing
the representations&apos; potential for indicating archaic, proto-language features.
The representations can be used for performing completion of fragmented Sinitic
phonological knowledge bases, estimating divergences between different
characters, or aiding the exploration and reconstruction of archaic features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Zhibai Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01211">
<title>An automated method for the ontological representation of security directives. (arXiv:2307.01211v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.01211</link>
<description rdf:parseType="Literal">&lt;p&gt;Large documents written in juridical language are difficult to interpret,
with long sentences leading to intricate and intertwined relations between the
nouns. The present paper frames this problem in the context of recent European
security directives. The complexity of their language is here thwarted by
automating the extraction of the relevant information, namely of the parts of
speech from each clause, through a specific tailoring of Natural Language
Processing (NLP) techniques. These contribute, in combination with ontology
development principles, to the design of our automated method for the
representation of security directives as ontologies. The method is showcased on
a practical problem, namely to derive an ontology representing the NIS 2
directive, which is the peak of cybersecurity prescripts at the European level.
Although the NLP techniques adopted showed some limitations and had to be
complemented by manual analysis, the overall results provide valid support for
directive compliance in general and for ontology development in particular.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bella_G/0/1/0/all/0/1&quot;&gt;Giampaolo Bella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castiglione_G/0/1/0/all/0/1&quot;&gt;Gianpietro Castiglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santamaria_D/0/1/0/all/0/1&quot;&gt;Daniele Francesco Santamaria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01214">
<title>Automatic Counterfactual Augmentation for Robust Text Classification Based on Word-Group Search. (arXiv:2307.01214v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01214</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite large-scale pre-trained language models have achieved striking
results for text classificaion, recent work has raised concerns about the
challenge of shortcut learning. In general, a keyword is regarded as a shortcut
if it creates a superficial association with the label, resulting in a false
prediction. Conversely, shortcut learning can be mitigated if the model relies
on robust causal features that help produce sound predictions. To this end,
many studies have explored post-hoc interpretable methods to mine shortcuts and
causal features for robustness and generalization. However, most existing
methods focus only on single word in a sentence and lack consideration of
word-group, leading to wrong causal features. To solve this problem, we propose
a new Word-Group mining approach, which captures the causal effect of any
keyword combination and orders the combinations that most affect the
prediction. Our approach bases on effective post-hoc analysis and beam search,
which ensures the mining effect and reduces the complexity. Then, we build a
counterfactual augmentation method based on the multiple word-groups, and use
an adaptive voting mechanism to learn the influence of different augmentated
samples on the prediction results, so as to force the model to pay attention to
effective causal features. We demonstrate the effectiveness of the proposed
method by several tasks on 8 affective review datasets and 4 toxic language
datasets, including cross-domain text classificaion, text attack and gender
fairness test.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_R/0/1/0/all/0/1&quot;&gt;Rui Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1&quot;&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01216">
<title>Discovering Patterns of Definitions and Methods from Scientific Documents. (arXiv:2307.01216v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01216</link>
<description rdf:parseType="Literal">&lt;p&gt;The difficulties of automatic extraction of definitions and methods from
scientific documents lie in two aspects: (1) the complexity and diversity of
natural language texts, which requests an analysis method to support the
discovery of pattern; and, (2) a complete definition or method represented by a
scientific paper is usually distributed within text, therefore an effective
approach should not only extract single sentence definitions and methods but
also integrate the sentences to obtain a complete definition or method. This
paper proposes an analysis method for discovering patterns of definition and
method and uses the method to discover patterns of definition and method.
Completeness of the patterns at the semantic level is guaranteed by a complete
set of semantic relations that identify definitions and methods respectively.
The completeness of the patterns at the syntactic and lexical levels is
guaranteed by syntactic and lexical constraints. Experiments on the self-built
dataset and two public definition datasets show that the discovered patterns
are effective. The patterns can be used to extract definitions and methods from
scientific documents and can be tailored or extended to suit other
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yutian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuge_H/0/1/0/all/0/1&quot;&gt;Hai Zhuge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01225">
<title>Interpretability and Transparency-Driven Detection and Transformation of Textual Adversarial Examples (IT-DT). (arXiv:2307.01225v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01225</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based text classifiers like BERT, Roberta, T5, and GPT-3 have
shown impressive performance in NLP. However, their vulnerability to
adversarial examples poses a security risk. Existing defense methods lack
interpretability, making it hard to understand adversarial classifications and
identify model vulnerabilities. To address this, we propose the
Interpretability and Transparency-Driven Detection and Transformation (IT-DT)
framework. It focuses on interpretability and transparency in detecting and
transforming textual adversarial examples. IT-DT utilizes techniques like
attention maps, integrated gradients, and model feedback for interpretability
during detection. This helps identify salient features and perturbed words
contributing to adversarial classifications. In the transformation phase, IT-DT
uses pre-trained embeddings and model feedback to generate optimal replacements
for perturbed words. By finding suitable substitutions, we aim to convert
adversarial examples into non-adversarial counterparts that align with the
model&apos;s intended behavior while preserving the text&apos;s meaning. Transparency is
emphasized through human expert involvement. Experts review and provide
feedback on detection and transformation results, enhancing decision-making,
especially in complex scenarios. The framework generates insights and threat
intelligence empowering analysts to identify vulnerabilities and improve model
robustness. Comprehensive experiments demonstrate the effectiveness of IT-DT in
detecting and transforming adversarial examples. The approach enhances
interpretability, provides transparency, and enables accurate identification
and successful transformation of adversarial inputs. By combining technical
analysis and human expertise, IT-DT significantly improves the resilience and
trustworthiness of transformer-based text classifiers against adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_B/0/1/0/all/0/1&quot;&gt;Bushra Sabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1&quot;&gt;M. Ali Babar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuadbba_S/0/1/0/all/0/1&quot;&gt;Sharif Abuadbba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01226">
<title>vONTSS: vMF based semi-supervised neural topic modeling with optimal transport. (arXiv:2307.01226v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.01226</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Neural Topic Models (NTM), inspired by variational autoencoders,
have attracted a lot of research interest; however, these methods have limited
applications in the real world due to the challenge of incorporating human
knowledge. This work presents a semi-supervised neural topic modeling method,
vONTSS, which uses von Mises-Fisher (vMF) based variational autoencoders and
optimal transport. When a few keywords per topic are provided, vONTSS in the
semi-supervised setting generates potential topics and optimizes topic-keyword
quality and topic classification. Experiments show that vONTSS outperforms
existing semi-supervised topic modeling methods in classification accuracy and
diversity. vONTSS also supports unsupervised topic modeling. Quantitative and
qualitative experiments show that vONTSS in the unsupervised setting
outperforms recent NTMs on multiple aspects: vONTSS discovers highly clustered
and coherent topics on benchmark datasets. It is also much faster than the
state-of-the-art weakly supervised text classification method while achieving
similar classification performance. We further prove the equivalence of optimal
transport loss and cross-entropy loss at the global minimum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengamedu_S/0/1/0/all/0/1&quot;&gt;Srinivasan H. Sengamedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1&quot;&gt;Francis Iannacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinjin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01230">
<title>Large Language and Text-to-3D Models for Engineering Design Optimization. (arXiv:2307.01230v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01230</link>
<description rdf:parseType="Literal">&lt;p&gt;The current advances in generative AI for learning large neural network
models with the capability to produce essays, images, music and even 3D assets
from text prompts create opportunities for a manifold of disciplines. In the
present paper, we study the potential of deep text-to-3D models in the
engineering domain, with focus on the chances and challenges when integrating
and interacting with 3D assets in computational simulation-based design
optimization. In contrast to traditional design optimization of 3D geometries
that often searches for the optimum designs using numerical representations,
such as B-Spline surface or deformation parameters in vehicle aerodynamic
optimization, natural language challenges the optimization framework by
requiring a different interpretation of variation operators while at the same
time may ease and motivate the human user interaction. Here, we propose and
realize a fully automated evolutionary design optimization framework using
Shap-E, a recently published text-to-3D asset network by OpenAI, in the context
of aerodynamic vehicle optimization. For representing text prompts in the
evolutionary optimization, we evaluate (a) a bag-of-words approach based on
prompt templates and Wordnet samples, and (b) a tokenisation approach based on
prompt templates and the byte pair encoding method from GPT4. Our main findings
from the optimizations indicate that, first, it is important to ensure that the
designs generated from prompts are within the object class of application, i.e.
diverse and novel designs need to be realistic, and, second, that more research
is required to develop methods where the strength of text prompt variations and
the resulting variations of the 3D designs share causal relations to some
degree to improve the optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rios_T/0/1/0/all/0/1&quot;&gt;Thiago Rios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menzel_S/0/1/0/all/0/1&quot;&gt;Stefan Menzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sendhoff_B/0/1/0/all/0/1&quot;&gt;Bernhard Sendhoff&lt;/a&gt; (Honda Research Institute Europe)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01299">
<title>The Evolution of Substance Use Coverage in the Philadelphia Inquirer. (arXiv:2307.01299v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01299</link>
<description rdf:parseType="Literal">&lt;p&gt;The media&apos;s representation of illicit substance use can lead to harmful
stereotypes and stigmatization for individuals struggling with addiction,
ultimately influencing public perception, policy, and public health outcomes.
To explore how the discourse and coverage of illicit drug use changed over
time, this study analyzes 157,476 articles published in the Philadelphia
Inquirer over a decade. Specifically, the study focuses on articles that
mentioned at least one commonly abused substance, resulting in a sample of
3,903 articles. Our analysis shows that cannabis and narcotics are the most
frequently discussed classes of drugs. Hallucinogenic drugs are portrayed more
positively than other categories, whereas narcotics are portrayed the most
negatively. Our research aims to highlight the need for accurate and inclusive
portrayals of substance use and addiction in the media.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouzoubaa_L/0/1/0/all/0/1&quot;&gt;Layla Bouzoubaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehsani_R/0/1/0/all/0/1&quot;&gt;Ramtin Ehsani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatterjee_P/0/1/0/all/0/1&quot;&gt;Preetha Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezapour_R/0/1/0/all/0/1&quot;&gt;Rezvaneh Rezapour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01310">
<title>Exploring Spoken Named Entity Recognition: A Cross-Lingual Perspective. (arXiv:2307.01310v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01310</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Named Entity Recognition (NER) have significantly
improved the identification of entities in textual data. However, spoken NER, a
specialized field of spoken document retrieval, lags behind due to its limited
research and scarce datasets. Moreover, cross-lingual transfer learning in
spoken NER has remained unexplored. This paper utilizes transfer learning
across Dutch, English, and German using pipeline and End-to-End (E2E) schemes.
We employ Wav2Vec2-XLS-R models on custom pseudo-annotated datasets and
investigate several architectures for the adaptability of cross-lingual
systems. Our results demonstrate that End-to-End spoken NER outperforms
pipeline-based alternatives over our limited annotations. Notably, transfer
learning from German to Dutch surpasses the Dutch E2E system by 7% and the
Dutch pipeline system by 4%. This study not only underscores the feasibility of
transfer learning in spoken NER but also sets promising outcomes for future
evaluations, hinting at the need for comprehensive data collection to augment
the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benaicha_M/0/1/0/all/0/1&quot;&gt;Moncef Benaicha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1&quot;&gt;David Thulke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turan_M/0/1/0/all/0/1&quot;&gt;M. A. Tu&amp;#x11f;tekin Turan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01323">
<title>Semantic enrichment towards efficient speech representations. (arXiv:2307.01323v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01323</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, self-supervised learned speech representations have
emerged as fruitful replacements for conventional surface representations when
solving Spoken Language Understanding (SLU) tasks. Simultaneously, multilingual
models trained on massive textual data were introduced to encode language
agnostic semantics. Recently, the SAMU-XLSR approach introduced a way to make
profit from such textual models to enrich multilingual speech representations
with language agnostic semantics. By aiming for better semantic extraction on a
challenging Spoken Language Understanding task and in consideration with
computation costs, this study investigates a specific in-domain semantic
enrichment of the SAMU-XLSR model by specializing it on a small amount of
transcribed data from the downstream task. In addition, we show the benefits of
the use of same-domain French and Italian benchmarks for low-resource language
portability and explore cross-domain capacities of the enriched SAMU-XLSR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laperriere_G/0/1/0/all/0/1&quot;&gt;Ga&amp;#xeb;lle Laperri&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Ha Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghannay_S/0/1/0/all/0/1&quot;&gt;Sahar Ghannay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabaian_B/0/1/0/all/0/1&quot;&gt;Bassam Jabaian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Esteve_Y/0/1/0/all/0/1&quot;&gt;Yannick Est&amp;#xe8;ve&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01370">
<title>Multilingual Language Models are not Multicultural: A Case Study in Emotion. (arXiv:2307.01370v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01370</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotions are experienced and expressed differently across the world. In order
to use Large Language Models (LMs) for multilingual tasks that require
emotional sensitivity, LMs must reflect this cultural variation in emotion. In
this study, we investigate whether the widely-used multilingual LMs in 2023
reflect differences in emotional expressions across cultures and languages. We
find that embeddings obtained from LMs (e.g., XLM-RoBERTa) are Anglocentric,
and generative LMs (e.g., ChatGPT) reflect Western norms, even when responding
to prompts in other languages. Our results show that multilingual LMs do not
successfully learn the culturally appropriate nuances of emotion and we
highlight possible research directions towards correcting this.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1&quot;&gt;Shreya Havaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rai_S/0/1/0/all/0/1&quot;&gt;Sunny Rai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_B/0/1/0/all/0/1&quot;&gt;Bhumika Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guntuku_L/0/1/0/all/0/1&quot;&gt;Langchen Liu Sharath Chandra Guntuku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ungar_L/0/1/0/all/0/1&quot;&gt;Lyle Ungar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01377">
<title>Shiftable Context: Addressing Training-Inference Context Mismatch in Simultaneous Speech Translation. (arXiv:2307.01377v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01377</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models using segment-based processing have been an effective
architecture for simultaneous speech translation. However, such models create a
context mismatch between training and inference environments, hindering
potential translation accuracy. We solve this issue by proposing Shiftable
Context, a simple yet effective scheme to ensure that consistent segment and
context sizes are maintained throughout training and inference, even with the
presence of partially filled segments due to the streaming nature of
simultaneous translation. Shiftable Context is also broadly applicable to
segment-based transformers for streaming tasks. Our experiments on the
English-German, English-French, and English-Spanish language pairs from the
MUST-C dataset demonstrate that when applied to the Augmented Memory
Transformer, a state-of-the-art model for simultaneous speech translation, the
proposed scheme achieves an average increase of 2.09, 1.83, and 1.95 BLEU
scores across each wait-k value for the three language pairs, respectively,
with a minimal impact on computation-aware Average Lagging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1&quot;&gt;Matthew Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penney_D/0/1/0/all/0/1&quot;&gt;Drew Penney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lizhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01379">
<title>Shifting Attention to Relevance: Towards the Uncertainty Estimation of Large Language Models. (arXiv:2307.01379v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01379</link>
<description rdf:parseType="Literal">&lt;p&gt;Although Large Language Models (LLMs) have shown great potential in Natural
Language Generation, it is still challenging to characterize the uncertainty of
model generations, i.e., when users could trust model outputs. Our research is
derived from the heuristic facts that tokens are created unequally in
reflecting the meaning of generations by auto-regressive LLMs, i.e., some
tokens are more relevant (or representative) than others, yet all the tokens
are equally valued when estimating uncertainty. It is because of the linguistic
redundancy where mostly a few keywords are sufficient to convey the meaning of
a long sentence. We name these inequalities as generative inequalities and
investigate how they affect uncertainty estimation. Our results reveal that
considerable tokens and sentences containing limited semantics are weighted
equally or even heavily when estimating uncertainty. To tackle these biases
posed by generative inequalities, we propose to jointly Shifting Attention to
more Relevant (SAR) components from both the token level and the sentence level
while estimating uncertainty. We conduct experiments over popular
&quot;off-the-shelf&quot; LLMs (e.g., OPT, LLaMA) with model sizes up to 30B and powerful
commercial LLMs (e.g., Davinci from OpenAI), across various free-form
question-answering tasks. Experimental results and detailed demographic
analysis indicate the superior performance of SAR. Code is available at
https://github.com/jinhaoduan/shifting-attention-to-relevance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinhao Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavalny_A/0/1/0/all/0/1&quot;&gt;Alex Zavalny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Renjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1&quot;&gt;Bhavya Kailkhura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kaidi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01381">
<title>Implicit Memory Transformer for Computationally Efficient Simultaneous Speech Translation. (arXiv:2307.01381v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01381</link>
<description rdf:parseType="Literal">&lt;p&gt;Simultaneous speech translation is an essential communication task difficult
for humans whereby a translation is generated concurrently with oncoming speech
inputs. For such a streaming task, transformers using block processing to break
an input sequence into segments have achieved state-of-the-art performance at a
reduced cost. Current methods to allow information to propagate across
segments, including left context and memory banks, have faltered as they are
both insufficient representations and unnecessarily expensive to compute. In
this paper, we propose an Implicit Memory Transformer that implicitly retains
memory through a new left context method, removing the need to explicitly
represent memory with memory banks. We generate the left context from the
attention output of the previous segment and include it in the keys and values
of the current segment&apos;s attention calculation. Experiments on the MuST-C
dataset show that the Implicit Memory Transformer provides a substantial
speedup on the encoder forward pass with nearly identical translation quality
when compared with the state-of-the-art approach that employs both left context
and memory banks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1&quot;&gt;Matthew Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lizhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01387">
<title>ALBERTI, a Multilingual Domain Specific Language Model for Poetry Analysis. (arXiv:2307.01387v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01387</link>
<description rdf:parseType="Literal">&lt;p&gt;The computational analysis of poetry is limited by the scarcity of tools to
automatically analyze and scan poems. In a multilingual settings, the problem
is exacerbated as scansion and rhyme systems only exist for individual
languages, making comparative studies very challenging and time consuming. In
this work, we present \textsc{Alberti}, the first multilingual pre-trained
large language model for poetry. Through domain-specific pre-training (DSP), we
further trained multilingual BERT on a corpus of over 12 million verses from 12
languages. We evaluated its performance on two structural poetry tasks: Spanish
stanza type classification, and metrical pattern prediction for Spanish,
English and German. In both cases, \textsc{Alberti} outperforms multilingual
BERT and other transformers-based models of similar sizes, and even achieves
state-of-the-art results for German when compared to rule-based systems,
demonstrating the feasibility and effectiveness of DSP in the poetry domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1&quot;&gt;Javier de la Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pozo_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;lvaro P&amp;#xe9;rez Pozo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ros_S/0/1/0/all/0/1&quot;&gt;Salvador Ros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_Blanco_E/0/1/0/all/0/1&quot;&gt;Elena Gonz&amp;#xe1;lez-Blanco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01401">
<title>Multi-Task Learning Improves Performance In Deep Argument Mining Models. (arXiv:2307.01401v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01401</link>
<description rdf:parseType="Literal">&lt;p&gt;The successful analysis of argumentative techniques from user-generated text
is central to many downstream tasks such as political and market analysis.
Recent argument mining tools use state-of-the-art deep learning methods to
extract and annotate argumentative techniques from various online text corpora,
however each task is treated as separate and different bespoke models are
fine-tuned for each dataset. We show that different argument mining tasks share
common semantic and logical structure by implementing a multi-task approach to
argument mining that achieves better performance than state-of-the-art methods
for the same problems. Our model builds a shared representation of the input
text that is common to all tasks and exploits similarities between tasks in
order to further boost performance via parameter-sharing. Our results are
important for argument mining as they show that different tasks share
substantial similarities and suggest a holistic approach to the extraction of
argumentative techniques from text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farzam_A/0/1/0/all/0/1&quot;&gt;Amirhossein Farzam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekhar_S/0/1/0/all/0/1&quot;&gt;Shashank Shekhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehlhaff_I/0/1/0/all/0/1&quot;&gt;Isaac Mehlhaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morucci_M/0/1/0/all/0/1&quot;&gt;Marco Morucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01420">
<title>Modeling Tag Prediction based on Question Tagging Behavior Analysis of CommunityQA Platform Users. (arXiv:2307.01420v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01420</link>
<description rdf:parseType="Literal">&lt;p&gt;In community question-answering platforms, tags play essential roles in
effective information organization and retrieval, better question routing,
faster response to questions, and assessment of topic popularity. Hence,
automatic assistance for predicting and suggesting tags for posts is of high
utility to users of such platforms. To develop better tag prediction across
diverse communities and domains, we performed a thorough analysis of users&apos;
tagging behavior in 17 StackExchange communities. We found various common
inherent properties of this behavior in those diverse domains. We used the
findings to develop a flexible neural tag prediction architecture, which
predicts both popular tags and more granular tags for each question. Our
extensive experiments and obtained performance show the effectiveness of our
model
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_K/0/1/0/all/0/1&quot;&gt;Kuntal Kumar Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gamon_M/0/1/0/all/0/1&quot;&gt;Michael Gamon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasekaran_N/0/1/0/all/0/1&quot;&gt;Nirupama Chandrasekaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucerzan_S/0/1/0/all/0/1&quot;&gt;Silviu Cucerzan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01446">
<title>On Conditional and Compositional Language Model Differentiable Prompting. (arXiv:2307.01446v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01446</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompts have been shown to be an effective method to adapt a frozen
Pretrained Language Model (PLM) to perform well on downstream tasks. Prompts
can be represented by a human-engineered word sequence or by a learned
continuous embedding. In this work, we investigate conditional and
compositional differentiable prompting. We propose a new model, Prompt
Production System (PRopS), which learns to transform task instructions or input
metadata, into continuous prompts that elicit task-specific outputs from the
PLM. Our model uses a modular network structure based on our neural formulation
of Production Systems, which allows the model to learn discrete rules -- neural
functions that learn to specialize in transforming particular prompt input
patterns, making it suitable for compositional transfer learning and few-shot
learning. We present extensive empirical and theoretical analysis and show that
PRopS consistently surpasses other PLM adaptation techniques, and often
improves upon fully fine-tuned models, on compositional generalization tasks,
controllable summarization and multilingual translation, while needing fewer
trainable parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilault_J/0/1/0/all/0/1&quot;&gt;Jonathan Pilault&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Can Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dreyer_M/0/1/0/all/0/1&quot;&gt;Markus Dreyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01448">
<title>ReactIE: Enhancing Chemical Reaction Extraction with Weak Supervision. (arXiv:2307.01448v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01448</link>
<description rdf:parseType="Literal">&lt;p&gt;Structured chemical reaction information plays a vital role for chemists
engaged in laboratory work and advanced endeavors such as computer-aided drug
design. Despite the importance of extracting structured reactions from
scientific literature, data annotation for this purpose is cost-prohibitive due
to the significant labor required from domain experts. Consequently, the
scarcity of sufficient training data poses an obstacle to the progress of
related models in this domain. In this paper, we propose ReactIE, which
combines two weakly supervised approaches for pre-training. Our method utilizes
frequent patterns within the text as linguistic cues to identify specific
characteristics of chemical reactions. Additionally, we adopt synthetic data
from patent records as distant supervision to incorporate domain knowledge into
the model. Experiments demonstrate that ReactIE achieves substantial
improvements and outperforms all existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1&quot;&gt;Ming Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1&quot;&gt;Siru Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1&quot;&gt;Vivian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_Y/0/1/0/all/0/1&quot;&gt;Yizhu Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01453">
<title>Diverse Retrieval-Augmented In-Context Learning for Dialogue State Tracking. (arXiv:2307.01453v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01453</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been significant interest in zero and few-shot learning for
dialogue state tracking (DST) due to the high cost of collecting and annotating
task-oriented dialogues. Recent work has demonstrated that in-context learning
requires very little data and zero parameter updates, and even outperforms
trained methods in the few-shot setting (Hu et al. 2022). We propose RefPyDST,
which advances the state of the art with three advancements to in-context
learning for DST. First, we formulate DST as a Python programming task,
explicitly modeling language coreference as variable reference in Python.
Second, since in-context learning depends highly on the context examples, we
propose a method to retrieve a diverse set of relevant examples to improve
performance. Finally, we introduce a novel re-weighting method during decoding
that takes into account probabilities of competing surface forms, and produces
a more accurate dialogue state prediction. We evaluate our approach using
MultiWOZ and achieve state-of-the-art multi-domain joint-goal accuracy in zero
and few-shot settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_B/0/1/0/all/0/1&quot;&gt;Brendan King&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1&quot;&gt;Jeffrey Flanigan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01458">
<title>CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care. (arXiv:2307.01458v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01458</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advances in NLP, have led to a new trend of applying LLMs to
real-world scenarios. While the latest LLMs are astonishingly fluent when
interacting with humans, they suffer from the misinformation problem by
unintentionally generating factually false statements. This can lead to harmful
consequences, especially when produced within sensitive contexts, such as
healthcare. Yet few previous works have focused on evaluating misinformation in
the long-form generation of LLMs, especially for knowledge-intensive topics.
Moreover, although LLMs have been shown to perform well in different languages,
misinformation evaluation has been mostly conducted in English. To this end, we
present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a
sensitive topic, specifically the maternity and infant care domain; and 2) a
language other than English, namely Chinese. Most importantly, we provide an
innovative paradigm for building long-form generation evaluation benchmarks
that can be transferred to other knowledge-intensive domains and low-resourced
languages. Our proposed benchmark fills the gap between the extensive usage of
LLMs and the lack of datasets for assessing the misinformation generated by
these models. It contains 1,612 expert-checked questions, accompanied with
human-selected references. Using our benchmark, we conduct extensive
experiments and found that current Chinese LLMs are far from perfect in the
topic of maternity and infant care. In an effort to minimize the reliance on
human resources for performance evaluation, we offer a judgment model for
automatically assessing the long-form output of LLMs using the benchmark
questions. Moreover, we compare potential solutions for long-form generation
evaluation and provide insights for building more robust and efficient
automated metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liangzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wangyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1&quot;&gt;Mingbai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_N/0/1/0/all/0/1&quot;&gt;Noa Garcia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01488">
<title>SCAT: Robust Self-supervised Contrastive Learning via Adversarial Training for Text Classification. (arXiv:2307.01488v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01488</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their promising performance across various natural language
processing (NLP) tasks, current NLP systems are vulnerable to textual
adversarial attacks. To defend against these attacks, most existing methods
apply adversarial training by incorporating adversarial examples. However,
these methods have to rely on ground-truth labels to generate adversarial
examples, rendering it impractical for large-scale model pre-training which is
commonly used nowadays for NLP and many other tasks. In this paper, we propose
a novel learning framework called SCAT (Self-supervised Contrastive Learning
via Adversarial Training), which can learn robust representations without
requiring labeled data. Specifically, SCAT modifies random augmentations of the
data in a fully labelfree manner to generate adversarial examples. Adversarial
training is achieved by minimizing the contrastive loss between the
augmentations and their adversarial counterparts. We evaluate SCAT on two text
classification datasets using two state-of-the-art attack schemes proposed
recently. Our results show that SCAT can not only train robust language models
from scratch, but it can also significantly improve the robustness of existing
pre-trained language models. Moreover, to demonstrate its flexibility, we show
that SCAT can also be combined with supervised adversarial training to further
enhance model robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junjie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_D/0/1/0/all/0/1&quot;&gt;Dit-Yan Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01503">
<title>On Evaluating and Mitigating Gender Biases in Multilingual Settings. (arXiv:2307.01503v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01503</link>
<description rdf:parseType="Literal">&lt;p&gt;While understanding and removing gender biases in language models has been a
long-standing problem in Natural Language Processing, prior research work has
primarily been limited to English. In this work, we investigate some of the
challenges with evaluating and mitigating biases in multilingual settings which
stem from a lack of existing benchmarks and resources for bias evaluation
beyond English especially for non-western context. In this paper, we first
create a benchmark for evaluating gender biases in pre-trained masked language
models by extending DisCo to different Indian languages using human
annotations. We extend various debiasing methods to work beyond English and
evaluate their effectiveness for SOTA massively multilingual models on our
proposed metric. Overall, our work highlights the challenges that arise while
studying social biases in multilingual settings and provides resources as well
as mitigation techniques to take a step toward scaling to more languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vashishtha_A/0/1/0/all/0/1&quot;&gt;Aniket Vashishtha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1&quot;&gt;Kabir Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitaram_S/0/1/0/all/0/1&quot;&gt;Sunayana Sitaram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01540">
<title>Learning to Prompt in the Classroom to Understand AI Limits: A pilot study. (arXiv:2307.01540v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.01540</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence&apos;s progress holds great promise in assisting society
in addressing pressing societal issues. In particular Large Language Models
(LLM) and the derived chatbots, like ChatGPT, have highly improved the natural
language processing capabilities of AI systems allowing them to process an
unprecedented amount of unstructured data. The consequent hype has also
backfired, raising negative sentiment even after novel AI methods&apos; surprising
contributions. One of the causes, but also an important issue per se, is the
rising and misleading feeling of being able to access and process any form of
knowledge to solve problems in any domain with no effort or previous expertise
in AI or problem domain, disregarding current LLMs limits, such as
hallucinations and reasoning limits. Acknowledging AI fallibility is crucial to
address the impact of dogmatic overconfidence in possibly erroneous suggestions
generated by LLMs. At the same time, it can reduce fear and other negative
attitudes toward AI. AI literacy interventions are necessary that allow the
public to understand such LLM limits and learn how to use them in a more
effective manner, i.e. learning to &quot;prompt&quot;. With this aim, a pilot educational
intervention was performed in a high school with 30 students. It involved (i)
presenting high-level concepts about intelligence, AI, and LLM, (ii) an initial
naive practice with ChatGPT in a non-trivial task, and finally (iii) applying
currently-accepted prompting strategies. Encouraging preliminary results have
been collected such as students reporting a) high appreciation of the activity,
b) improved quality of the interaction with the LLM during the educational
activity, c) decreased negative sentiments toward AI, d) increased
understanding of limitations and specifically We aim to study factors that
impact AI acceptance and to refine and repeat this activity in more controlled
settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theophilou_E/0/1/0/all/0/1&quot;&gt;Emily Theophilou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyuturk_C/0/1/0/all/0/1&quot;&gt;Cansu Koyuturk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavari_M/0/1/0/all/0/1&quot;&gt;Mona Yavari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursic_S/0/1/0/all/0/1&quot;&gt;Sathya Bursic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donabauer_G/0/1/0/all/0/1&quot;&gt;Gregor Donabauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telari_A/0/1/0/all/0/1&quot;&gt;Alessia Telari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Testa_A/0/1/0/all/0/1&quot;&gt;Alessia Testa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boiano_R/0/1/0/all/0/1&quot;&gt;Raffaele Boiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Leo_D/0/1/0/all/0/1&quot;&gt;Davinia Hernandez-Leo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruskov_M/0/1/0/all/0/1&quot;&gt;Martin Ruskov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taibi_D/0/1/0/all/0/1&quot;&gt;Davide Taibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbiadini_A/0/1/0/all/0/1&quot;&gt;Alessandro Gabbiadini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ognibene_D/0/1/0/all/0/1&quot;&gt;Dimitri Ognibene&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01542">
<title>Mitigating the Learning Bias towards Repetition by Self-Contrastive Training for Open-Ended Generation. (arXiv:2307.01542v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01542</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the huge progress in myriad generation tasks, pretrained language
models (LMs) such as GPT2 still tend to generate repetitive texts with
maximization-based decoding algorithms for open-ended generation. We attribute
their overestimation of token-level repetition probabilities to the learning
bias: LMs capture simple repetitive patterns faster with the MLE loss. We
propose self-contrastive training to penalize the output of a premature
checkpoint of the same model when it incorrectly predicts repetition, which is
shown to mitigate repetition effectively while maintaining fluency on two
datasets. Furthermore, we find that LMs use longer-range dependencies to
predict repetitive tokens than non-repetitive ones, which may be the cause of
sentence-level repetition loops.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_J/0/1/0/all/0/1&quot;&gt;Jian Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Minlie Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01595">
<title>Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases. (arXiv:2307.01595v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01595</link>
<description rdf:parseType="Literal">&lt;p&gt;As the representation capability of Pre-trained Language Models (PLMs)
improve, there is growing concern that they will inherit social biases from
unprocessed corpora. Most previous debiasing techniques used Counterfactual
Data Augmentation (CDA) to balance the training corpus. However, CDA slightly
modifies the original corpus, limiting the representation distance between
different demographic groups to a narrow range. As a result, the debiasing
model easily fits the differences between counterfactual pairs, which affects
its debiasing performance with limited text resources. In this paper, we
propose an adversarial training-inspired two-stage debiasing model using
Contrastive learning with Continuous Prompt Augmentation (named CCPA) to
mitigate social biases in PLMs&apos; encoding. In the first stage, we propose a data
augmentation method based on continuous prompt tuning to push farther the
representation distance between sample pairs along different demographic
groups. In the second stage, we utilize contrastive learning to pull closer the
representation distance between the augmented sample pairs and then fine-tune
PLMs&apos; parameters to get debiased encoding. Our approach guides the model to
achieve stronger debiasing performance by adding difficulty to the training
process. Extensive experiments show that CCPA outperforms baselines in terms of
debiasing performance. Meanwhile, experimental results on the GLUE benchmark
show that CCPA retains the language modeling capability of PLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingji Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01609">
<title>A Language Model for Grammatical Error Correction in L2 Russian. (arXiv:2307.01609v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01609</link>
<description rdf:parseType="Literal">&lt;p&gt;Grammatical error correction is one of the fundamental tasks in Natural
Language Processing. For the Russian language, most of the spellcheckers
available correct typos and other simple errors with high accuracy, but often
fail when faced with non-native (L2) writing, since the latter contains errors
that are not typical for native speakers. In this paper, we propose a pipeline
involving a language model intended for correcting errors in L2 Russian
writing. The language model proposed is trained on untagged texts of the
Newspaper subcorpus of the Russian National Corpus, and the quality of the
model is validated against the RULEC-GEC corpus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Remnev_N/0/1/0/all/0/1&quot;&gt;Nikita Remnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obiedkov_S/0/1/0/all/0/1&quot;&gt;Sergei Obiedkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhilina_E/0/1/0/all/0/1&quot;&gt;Ekaterina Rakhilina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smirnov_I/0/1/0/all/0/1&quot;&gt;Ivan Smirnov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vyrenkova_A/0/1/0/all/0/1&quot;&gt;Anastasia Vyrenkova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01640">
<title>Chain of Thought Prompting Elicits Knowledge Augmentation. (arXiv:2307.01640v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01640</link>
<description rdf:parseType="Literal">&lt;p&gt;The knowledge-augmented deep learning paradigm refers to a paradigm in which
domain knowledge is identified and integrated into deep models. Conventional
methods typically employ task-specific approaches to gather external knowledge
from various sources. In contrast, large language models are extensively
pre-trained and can serve as a comprehensive source of external knowledge. In
this paper, we propose CoT-KA, a Chain-of-Thought-based method that augments
knowledge for deep learning. CoT-KA avoids the need for additional knowledge
retrieval or knowledge reasoning models, as required in conventional
augmentation methods. Our results demonstrate that CoT-KA outperforms both pure
CoT-based methods and the non-augmented method across the majority of eleven
publicly available benchmarks for various reasoning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dingjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinmei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01644">
<title>Insert-expansions for Tool-enabled Conversational Agents. (arXiv:2307.01644v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.01644</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper delves into an advanced implementation of
Chain-of-Thought-Prompting in Large Language Models, focusing on the use of
tools (or &quot;plug-ins&quot;) within the explicit reasoning paths generated by this
prompting method. We find that tool-enabled conversational agents often become
sidetracked, as additional context from tools like search engines or
calculators diverts from original user intents. To address this, we explore a
concept wherein the user becomes the tool, providing necessary details and
refining their requests. Through Conversation Analysis, we characterize this
interaction as insert-expansion - an intermediary conversation designed to
facilitate the preferred response. We explore possibilities arising from this
&apos;user-as-a-tool&apos; approach in two empirical studies using direct comparison, and
find benefits in the recommendation domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldi_A/0/1/0/all/0/1&quot;&gt;Andreas G&amp;#xf6;ldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rietsche_R/0/1/0/all/0/1&quot;&gt;Roman Rietsche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01664">
<title>Unified Conversational Models with System-Initiated Transitions between Chit-Chat and Task-Oriented Dialogues. (arXiv:2307.01664v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01664</link>
<description rdf:parseType="Literal">&lt;p&gt;Spoken dialogue systems (SDSs) have been separately developed under two
different categories, task-oriented and chit-chat. The former focuses on
achieving functional goals and the latter aims at creating engaging social
conversations without special goals. Creating a unified conversational model
that can engage in both chit-chat and task-oriented dialogue is a promising
research topic in recent years. However, the potential ``initiative&apos;&apos; that
occurs when there is a change between dialogue modes in one dialogue has rarely
been explored. In this work, we investigate two kinds of dialogue scenarios,
one starts from chit-chat implicitly involving task-related topics and finally
switching to task-oriented requests; the other starts from task-oriented
interaction and eventually changes to casual chat after all requested
information is provided. We contribute two efficient prompt models which can
proactively generate a transition sentence to trigger system-initiated
transitions in a unified dialogue model. One is a discrete prompt model trained
with two discrete tokens, the other one is a continuous prompt model using
continuous prompt embeddings automatically generated by a classifier. We
furthermore show that the continuous prompt model can also be used to guide the
proactive transitions between particular domains in a multi-domain
task-oriented setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ultes_S/0/1/0/all/0/1&quot;&gt;Stefan Ultes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minker_W/0/1/0/all/0/1&quot;&gt;Wolfgang Minker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_W/0/1/0/all/0/1&quot;&gt;Wolfgang Maier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01672">
<title>Boosting Norwegian Automatic Speech Recognition. (arXiv:2307.01672v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01672</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present several baselines for automatic speech recognition
(ASR) models for the two official written languages in Norway: Bokm{\aa}l and
Nynorsk. We compare the performance of models of varying sizes and pre-training
approaches on multiple Norwegian speech datasets. Additionally, we measure the
performance of these models against previous state-of-the-art ASR models, as
well as on out-of-domain datasets. We improve the state of the art on the
Norwegian Parliamentary Speech Corpus (NPSC) from a word error rate (WER) of
17.10\% to 7.60\%, with models achieving 5.81\% for Bokm{\aa}l and 11.54\% for
Nynorsk. We also discuss the challenges and potential solutions for further
improving ASR models for Norwegian.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosa_J/0/1/0/all/0/1&quot;&gt;Javier de la Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braaten_R/0/1/0/all/0/1&quot;&gt;Rolv-Arild Braaten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kummervold_P/0/1/0/all/0/1&quot;&gt;Per Egil Kummervold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetjen_F/0/1/0/all/0/1&quot;&gt;Freddy Wetjen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brygfjeld_S/0/1/0/all/0/1&quot;&gt;Svein Arne Brygfjeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01673">
<title>Disentanglement in a GAN for Unconditional Speech Synthesis. (arXiv:2307.01673v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.01673</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we develop a model that can synthesize realistic speech directly from a
latent space, without explicit conditioning? Despite several efforts over the
last decade, previous adversarial and diffusion-based approaches still struggle
to achieve this, even on small-vocabulary datasets. To address this, we propose
AudioStyleGAN (ASGAN) -- a generative adversarial network for unconditional
speech synthesis tailored to learn a disentangled latent space. Building upon
the StyleGAN family of image synthesis models, ASGAN maps sampled noise to a
disentangled latent vector which is then mapped to a sequence of audio features
so that signal aliasing is suppressed at every layer. To successfully train
ASGAN, we introduce a number of new techniques, including a modification to
adaptive discriminator augmentation which probabilistically skips discriminator
updates. We apply it on the small-vocabulary Google Speech Commands digits
dataset, where it achieves state-of-the-art results in unconditional speech
synthesis. It is also substantially faster than existing top-performing
diffusion models. We confirm that ASGAN&apos;s latent space is disentangled: we
demonstrate how simple linear operations in the space can be used to perform
several tasks unseen during training. Specifically, we perform evaluations in
voice conversion, speech enhancement, speaker verification, and keyword
classification. Our work indicates that GANs are still highly competitive in
the unconditional speech synthesis landscape, and that disentangled latent
spaces can be used to aid generalization to unseen tasks. Code, models,
samples: https://github.com/RF5/simple-asgan/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baas_M/0/1/0/all/0/1&quot;&gt;Matthew Baas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kamper_H/0/1/0/all/0/1&quot;&gt;Herman Kamper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01680">
<title>Robust Hate Speech Detection in Social Media: A Cross-Dataset Empirical Evaluation. (arXiv:2307.01680v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01680</link>
<description rdf:parseType="Literal">&lt;p&gt;The automatic detection of hate speech online is an active research area in
NLP. Most of the studies to date are based on social media datasets that
contribute to the creation of hate speech detection models trained on them.
However, data creation processes contain their own biases, and models
inherently learn from these dataset-specific biases. In this paper, we perform
a large-scale cross-dataset comparison where we fine-tune language models on
different hate speech detection datasets. This analysis shows how some datasets
are more generalisable than others when used as training data. Crucially, our
experiments show how combining hate speech detection datasets can contribute to
the development of robust hate speech detection models. This robustness holds
even when controlling by data size and compared with the best individual
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antypas_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Antypas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camacho_Collados_J/0/1/0/all/0/1&quot;&gt;Jose Camacho-Collados&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01693">
<title>Racial Bias Trends in the Text of US Legal Opinions. (arXiv:2307.01693v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01693</link>
<description rdf:parseType="Literal">&lt;p&gt;Although there is widespread recognition of racial bias in US law, it is
unclear how such bias appears in the language of law, namely judicial opinions,
and whether it varies across time period or region. Building upon approaches
for measuring implicit racial bias in large-scale corpora, we approximate GloVe
word embeddings for over 6 million US federal and state court cases from 1860
to 2009. We find strong evidence of racial bias across nearly all regions and
time periods, as traditionally Black names are more closely associated with
pre-classified &quot;unpleasant&quot; terms whereas traditionally White names are more
closely associated with pre-classified &quot;pleasant&quot; terms. We also test whether
legal opinions before 1950 exhibit more implicit racial bias than those after
1950, as well as whether opinions from Southern states exhibit less change in
racial bias than those from Northeastern states. We do not find evidence of
elevated bias in legal opinions before 1950, or evidence that legal opinions
from Northeastern states show greater change in racial bias over time compared
to Southern states. These results motivate further research into
institutionalized racial bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jinturkar_R/0/1/0/all/0/1&quot;&gt;Rohan Jinturkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01709">
<title>Dipping PLMs Sauce: Bridging Structure and Text for Effective Knowledge Graph Completion via Conditional Soft Prompting. (arXiv:2307.01709v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01709</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Graph Completion (KGC) often requires both KG structural and
textual information to be effective. Pre-trained Language Models (PLMs) have
been used to learn the textual information, usually under the fine-tune
paradigm for the KGC task. However, the fine-tuned PLMs often overwhelmingly
focus on the textual information and overlook structural knowledge. To tackle
this issue, this paper proposes CSProm-KG (Conditional Soft Prompts for KGC)
which maintains a balance between structural information and textual knowledge.
CSProm-KG only tunes the parameters of Conditional Soft Prompts that are
generated by the entities and relations representations. We verify the
effectiveness of CSProm-KG on three popular static KGC benchmarks WN18RR,
FB15K-237 and Wikidata5M, and two temporal KGC benchmarks ICEWS14 and
ICEWS05-15. CSProm-KG outperforms competitive baseline models and sets new
state-of-the-art on these benchmarks. We conduct further analysis to show (i)
the effectiveness of our proposed components, (ii) the efficiency of CSProm-KG,
and (iii) the flexibility of CSProm-KG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1&quot;&gt;Aixin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kwok-Yan Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01715">
<title>Align With Purpose: Optimize Desired Properties in CTC Models with a General Plug-and-Play Framework. (arXiv:2307.01715v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01715</link>
<description rdf:parseType="Literal">&lt;p&gt;Connectionist Temporal Classification (CTC) is a widely used criterion for
training supervised sequence-to-sequence (seq2seq) models. It enables learning
the relations between input and output sequences, termed alignments, by
marginalizing over perfect alignments (that yield the ground truth), at the
expense of imperfect alignments. This binary differentiation of perfect and
imperfect alignments falls short of capturing other essential alignment
properties that hold significance in other real-world applications. Here we
propose $\textit{Align With Purpose}$, a $\textbf{general Plug-and-Play
framework}$ for enhancing a desired property in models trained with the CTC
criterion. We do that by complementing the CTC with an additional loss term
that prioritizes alignments according to a desired property. Our method does
not require any intervention in the CTC loss function, enables easy
optimization of a variety of properties, and allows differentiation between
both perfect and imperfect alignments. We apply our framework in the domain of
Automatic Speech Recognition (ASR) and show its generality in terms of property
selection, architectural choice, and scale of training dataset (up to 280,000
hours). To demonstrate the effectiveness of our framework, we apply it to two
unrelated properties: emission time and word error rate (WER). For the former,
we report an improvement of up to 570ms in latency optimization with a minor
reduction in WER, and for the latter, we report a relative improvement of 4.5%
WER over the baseline models. To the best of our knowledge, these applications
have never been demonstrated to work on a scale of data as large as ours.
Notably, our method can be implemented using only a few lines of code, and can
be extended to other alignment-free loss functions and to domains other than
ASR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Segev_E/0/1/0/all/0/1&quot;&gt;Eliya Segev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alroy_M/0/1/0/all/0/1&quot;&gt;Maya Alroy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsir_R/0/1/0/all/0/1&quot;&gt;Ronen Katsir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wies_N/0/1/0/all/0/1&quot;&gt;Noam Wies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenhav_A/0/1/0/all/0/1&quot;&gt;Ayana Shenhav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Oren_Y/0/1/0/all/0/1&quot;&gt;Yael Ben-Oren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zar_D/0/1/0/all/0/1&quot;&gt;David Zar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tadmor_O/0/1/0/all/0/1&quot;&gt;Oren Tadmor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitterman_J/0/1/0/all/0/1&quot;&gt;Jacob Bitterman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shashua_A/0/1/0/all/0/1&quot;&gt;Amnon Shashua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenwein_T/0/1/0/all/0/1&quot;&gt;Tal Rosenwein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01764">
<title>Knowledge-Aware Audio-Grounded Generative Slot Filling for Limited Annotated Data. (arXiv:2307.01764v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01764</link>
<description rdf:parseType="Literal">&lt;p&gt;Manually annotating fine-grained slot-value labels for task-oriented dialogue
(ToD) systems is an expensive and time-consuming endeavour. This motivates
research into slot-filling methods that operate with limited amounts of
labelled data. Moreover, the majority of current work on ToD is based solely on
text as the input modality, neglecting the additional challenges of imperfect
automatic speech recognition (ASR) when working with spoken language. In this
work, we propose a Knowledge-Aware Audio-Grounded generative slot-filling
framework, termed KA2G, that focuses on few-shot and zero-shot slot filling for
ToD with speech input. KA2G achieves robust and data-efficient slot filling for
speech-based ToD by 1) framing it as a text generation task, 2) grounding text
generation additionally in the audio modality, and 3) conditioning on available
external knowledge (e.g. a predefined list of possible slot values). We show
that combining both modalities within the KA2G framework improves the
robustness against ASR errors. Further, the knowledge-aware slot-value
generator in KA2G, implemented via a pointer generator mechanism, particularly
benefits few-shot and zero-shot learning. Experiments, conducted on the
standard speech-based single-turn SLURP dataset and a multi-turn dataset
extracted from a commercial ToD system, display strong and consistent gains
over prior work, especially in few-shot and zero-shot setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guangzhi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vulic_I/0/1/0/all/0/1&quot;&gt;Ivan Vuli&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budzianowski_P/0/1/0/all/0/1&quot;&gt;Pawe&amp;#x142; Budzianowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodland_P/0/1/0/all/0/1&quot;&gt;Philip C. Woodland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01784">
<title>The Inner Sentiments of a Thought. (arXiv:2307.01784v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.01784</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based large-scale language models (LLMs) are able to generate
highly realistic text. They are duly able to express, and at least implicitly
represent, a wide range of sentiments and color, from the obvious, such as
valence and arousal to the subtle, such as determination and admiration. We
provide a first exploration of these representations and how they can be used
for understanding the inner sentimental workings of single sentences. We train
predictors of the quantiles of the distributions of final sentiments of
sentences from the hidden representations of an LLM applied to prefixes of
increasing lengths. After showing that predictors of distributions of valence,
determination, admiration, anxiety and annoyance are well calibrated, we
provide examples of using these predictors for analyzing sentences,
illustrating, for instance, how even ordinary conjunctions (e.g., &quot;but&quot;) can
dramatically alter the emotional trajectory of an utterance. We then show how
to exploit the distributional predictions to generate sentences with sentiments
in the tails of distributions. We discuss the implications of our results for
the inner workings of thoughts, for instance for psychiatric dysfunction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gagne_C/0/1/0/all/0/1&quot;&gt;Chris Gagne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayan_P/0/1/0/all/0/1&quot;&gt;Peter Dayan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.12926">
<title>Compositionality as Lexical Symmetry. (arXiv:2201.12926v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2201.12926</link>
<description rdf:parseType="Literal">&lt;p&gt;In tasks like semantic parsing, instruction following, and question
answering, standard deep networks fail to generalize compositionally from small
datasets. Many existing approaches overcome this limitation with model
architectures that enforce a compositional process of sentence interpretation.
In this paper, we present a domain-general and model-agnostic formulation of
compositionality as a constraint on symmetries of data distributions rather
than models. Informally, we prove that whenever a task can be solved by a
compositional model, there is a corresponding data augmentation scheme -- a
procedure for transforming examples into other well formed examples -- that
imparts compositional inductive bias on any model trained to solve the same
task. We describe a procedure called LEXSYM that discovers these
transformations automatically, then applies them to training data for ordinary
neural sequence models. Unlike existing compositional data augmentation
procedures, LEXSYM can be deployed agnostically across text, structured data,
and even images. It matches or surpasses state-of-the-art, task-specific models
on COGS semantic parsing, SCAN and ALCHEMY instruction following, and
CLEVR-COGENT visual question answering datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akyurek_E/0/1/0/all/0/1&quot;&gt;Ekin Aky&amp;#xfc;rek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05098">
<title>IsoVec: Controlling the Relative Isomorphism of Word Embedding Spaces. (arXiv:2210.05098v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05098</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to extract high-quality translation dictionaries from monolingual
word embedding spaces depends critically on the geometric similarity of the
spaces -- their degree of &quot;isomorphism.&quot; We address the root-cause of faulty
cross-lingual mapping: that word embedding training resulted in the underlying
spaces being non-isomorphic. We incorporate global measures of isomorphism
directly into the Skip-gram loss function, successfully increasing the relative
isomorphism of trained word embedding spaces and improving their ability to be
mapped to a shared cross-lingual space. The result is improved bilingual
lexicon induction in general data conditions, under domain mismatch, and with
training algorithm dissimilarities. We release IsoVec at
https://github.com/kellymarchisio/isovec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1&quot;&gt;Kelly Marchisio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_N/0/1/0/all/0/1&quot;&gt;Neha Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duh_K/0/1/0/all/0/1&quot;&gt;Kevin Duh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koehn_P/0/1/0/all/0/1&quot;&gt;Philipp Koehn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10209">
<title>Exclusive Supermask Subnetwork Training for Continual Learning. (arXiv:2210.10209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10209</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning (CL) methods focus on accumulating knowledge over time
while avoiding catastrophic forgetting. Recently, Wortsman et al. (2020)
proposed a CL method, SupSup, which uses a randomly initialized, fixed base
network (model) and finds a supermask for each new task that selectively keeps
or removes each weight to produce a subnetwork. They prevent forgetting as the
network weights are not being updated. Although there is no forgetting, the
performance of SupSup is sub-optimal because fixed weights restrict its
representational power. Furthermore, there is no accumulation or transfer of
knowledge inside the model when new tasks are learned. Hence, we propose
ExSSNeT (Exclusive Supermask SubNEtwork Training), that performs exclusive and
non-overlapping subnetwork weight training. This avoids conflicting updates to
the shared weights by subsequent tasks to improve performance while still
preventing forgetting. Furthermore, we propose a novel KNN-based Knowledge
Transfer (KKT) module that utilizes previously acquired knowledge to learn new
tasks better and faster. We demonstrate that ExSSNeT outperforms strong
previous methods on both NLP and Vision domains while preventing forgetting.
Moreover, ExSSNeT is particularly advantageous for sparse masks that activate
2-10% of the model parameters, resulting in an average improvement of 8.3% over
SupSup. Furthermore, ExSSNeT scales to a large number of tasks (100). Our code
is available at https://github.com/prateeky2806/exessnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Prateek Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10329">
<title>Language Detoxification with Attribute-Discriminative Latent Space. (arXiv:2210.10329v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10329</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based Language Models (LMs) have achieved impressive results on
natural language understanding tasks, but they can also generate toxic text
such as insults, threats, and profanity, limiting their real-world
applications. To overcome this issue, a few text generation approaches aim to
detoxify toxic texts using additional LMs or perturbations. However, previous
methods require excessive memory, computations, and time which are serious
bottlenecks in their real-world application. To address such limitations, we
propose an effective yet efficient method for language detoxification using an
attribute-discriminative latent space. Specifically, we project the latent
space of an original Transformer LM onto a discriminative latent space that
well-separates texts by their attributes using a projection block and an
attribute discriminator. This allows the LM to control the text generation to
be non-toxic with minimal memory and computation overhead. We validate our
model, Attribute-Discriminative Language Model (ADLM) on detoxified language
and dialogue generation tasks, on which our method significantly outperforms
baselines both in performance and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1&quot;&gt;Jin Myung Kwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Minseon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15536">
<title>Sentiment analysis and opinion mining on E-commerce site. (arXiv:2211.15536v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15536</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentiment analysis or opinion mining help to illustrate the phrase NLP
(Natural Language Processing). Sentiment analysis has been the most significant
topic in recent years. The goal of this study is to solve the sentiment
polarity classification challenges in sentiment analysis. A broad technique for
categorizing sentiment opposition is presented, along with comprehensive
process explanations. With the results of the analysis, both sentence-level
classification and review-level categorization are conducted. Finally, we
discuss our plans for future sentiment analysis research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anny_F/0/1/0/all/0/1&quot;&gt;Fatema Tuz Zohra Anny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_O/0/1/0/all/0/1&quot;&gt;Oahidul Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01936">
<title>Democratizing Neural Machine Translation with OPUS-MT. (arXiv:2212.01936v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01936</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the OPUS ecosystem with a focus on the development of
open machine translation models and tools, and their integration into end-user
applications, development platforms and professional workflows. We discuss our
on-going mission of increasing language coverage and translation quality, and
also describe on-going work on the development of modular translation models
and speed-optimized compact solutions for real-time translation on regular
desktops and small devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiedemann_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rg Tiedemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aulamo_M/0/1/0/all/0/1&quot;&gt;Mikko Aulamo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakshandaeva_D/0/1/0/all/0/1&quot;&gt;Daria Bakshandaeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boggia_M/0/1/0/all/0/1&quot;&gt;Michele Boggia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gronroos_S/0/1/0/all/0/1&quot;&gt;Stig-Arne Gr&amp;#xf6;nroos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nieminen_T/0/1/0/all/0/1&quot;&gt;Tommi Nieminen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raganato_A/0/1/0/all/0/1&quot;&gt;Alessandro Raganato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherrer_Y/0/1/0/all/0/1&quot;&gt;Yves Scherrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazquez_R/0/1/0/all/0/1&quot;&gt;Raul Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virpioja_S/0/1/0/all/0/1&quot;&gt;Sami Virpioja&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05961">
<title>RPN: A Word Vector Level Data Augmentation Algorithm in Deep Learning for Language Understanding. (arXiv:2212.05961v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05961</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation is a widely used technique in machine learning to improve
model performance. However, existing data augmentation techniques in natural
language understanding (NLU) may not fully capture the complexity of natural
language variations, and they can be challenging to apply to large datasets.
This paper proposes the Random Position Noise (RPN) algorithm, a novel data
augmentation technique that operates at the word vector level. RPN modifies the
word embeddings of the original text by introducing noise based on the existing
values of selected word vectors, allowing for more fine-grained modifications
and better capturing natural language variations. Unlike traditional data
augmentation methods, RPN does not require gradients in the computational graph
during virtual sample updates, making it simpler to apply to large datasets.
Experimental results demonstrate that RPN consistently outperforms existing
data augmentation techniques across various NLU tasks, including sentiment
analysis, natural language inference, and paraphrase detection. Moreover, RPN
performs well in low-resource settings and is applicable to any model featuring
a word embeddings layer. The proposed RPN algorithm is a promising approach for
enhancing NLU performance and addressing the challenges associated with
traditional data augmentation techniques in large-scale NLU tasks. Our
experimental results demonstrated that the RPN algorithm achieved
state-of-the-art performance in all seven NLU tasks, thereby highlighting its
effectiveness and potential for real-world NLU applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xuecong Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_H/0/1/0/all/0/1&quot;&gt;Huiwen Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhuanzhe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10503">
<title>Mini-Model Adaptation: Efficiently Extending Pretrained Models to New Languages via Aligned Shallow Training. (arXiv:2212.10503v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10503</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior work shows that it is possible to expand pretrained Masked Language
Models (MLMs) to new languages by learning a new set of embeddings, while
keeping the transformer body frozen. Despite learning a small subset of
parameters, this approach is not compute-efficient, as training the new
embeddings requires a full forward and backward pass over the entire model. We
propose mini-model adaptation, a compute-efficient alternative that builds a
shallow mini-model from a fraction of a large model&apos;s parameters. New
language-specific embeddings can then be efficiently trained over the
mini-model and plugged into the aligned large model for rapid cross-lingual
transfer. We explore two approaches to learn mini-models: MiniJoint, which
jointly pretrains the primary model and the mini-model using a single
transformer with a secondary MLM head at a middle layer; and MiniPost, where we
start from a regular pretrained model, build a mini-model by extracting and
freezing a few layers, and learn a small number of parameters on top.
Experiments on XNLI, MLQA and PAWS-X show that mini-model adaptation matches
the performance of the standard approach using 2.3x less compute on average.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1&quot;&gt;Kelly Marchisio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_P/0/1/0/all/0/1&quot;&gt;Patrick Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1&quot;&gt;Mikel Artetxe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10549">
<title>Cross-modal Attention Congruence Regularization for Vision-Language Relation Alignment. (arXiv:2212.10549v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10549</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent progress towards scaling up multimodal vision-language models,
these models are still known to struggle on compositional generalization
benchmarks such as Winoground. We find that a critical component lacking from
current vision-language models is relation-level alignment: the ability to
match directional semantic relations in text (e.g., &quot;mug in grass&quot;) with
spatial relationships in the image (e.g., the position of the mug relative to
the grass). To tackle this problem, we show that relation alignment can be
enforced by encouraging the directed language attention from &apos;mug&apos; to &apos;grass&apos;
(capturing the semantic relation &apos;in&apos;) to match the directed visual attention
from the mug to the grass. Tokens and their corresponding objects are softly
identified using the cross-modal attention. We prove that this notion of soft
relation alignment is equivalent to enforcing congruence between vision and
language attention matrices under a &apos;change of basis&apos; provided by the
cross-modal attention matrix. Intuitively, our approach projects visual
attention into the language attention space to calculate its divergence from
the actual language attention, and vice versa. We apply our Cross-modal
Attention Congruence Regularization (CACR) loss to UNITER and improve on the
state-of-the-art approach to Winoground.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_R/0/1/0/all/0/1&quot;&gt;Rohan Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10075">
<title>Gender Neutralization for an Inclusive Machine Translation: from Theoretical Foundations to Open Challenges. (arXiv:2301.10075v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10075</link>
<description rdf:parseType="Literal">&lt;p&gt;Gender inclusivity in language technologies has become a prominent research
topic. In this study, we explore gender-neutral translation (GNT) as a form of
gender inclusivity and a goal to be achieved by machine translation (MT)
models, which have been found to perpetuate gender bias and discrimination.
Specifically, we focus on translation from English into Italian, a language
pair representative of salient gender-related linguistic transfer problems. To
define GNT, we review a selection of relevant institutional guidelines for
gender-inclusive language, discuss its scenarios of use, and examine the
technical challenges of performing GNT in MT, concluding with a discussion of
potential solutions to encourage advancements toward greater inclusivity in MT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piergentili_A/0/1/0/all/0/1&quot;&gt;Andrea Piergentili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fucci_D/0/1/0/all/0/1&quot;&gt;Dennis Fucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savoldi_B/0/1/0/all/0/1&quot;&gt;Beatrice Savoldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bentivogli_L/0/1/0/all/0/1&quot;&gt;Luisa Bentivogli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negri_M/0/1/0/all/0/1&quot;&gt;Matteo Negri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04391">
<title>The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04391</link>
<description rdf:parseType="Literal">&lt;p&gt;In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The experimental results
and human evaluation results verify our idea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tong Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08468">
<title>LEVER: Learning to Verify Language-to-Code Generation with Execution. (arXiv:2302.08468v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08468</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models trained on code (code LLMs) has led to
significant progress in language-to-code generation. State-of-the-art
approaches in this area combine LLM decoding with sample pruning and reranking
using test cases or heuristics based on the execution results. However, it is
challenging to obtain test cases for many real-world language-to-code
applications, and heuristics cannot well capture the semantic features of the
execution results, such as data type and value range, which often indicates the
correctness of the program. In this work, we propose LEVER, a simple approach
to improve language-to-code generation by learning to verify the generated
programs with their execution results. Specifically, we train verifiers to
determine whether a program sampled from the LLMs is correct or not based on
the natural language input, the program itself and its execution results. The
sampled programs are reranked by combining the verification score with the LLM
generation probability, and marginalizing over programs with the same execution
results. On four datasets across the domains of table QA, math QA and basic
Python programming, LEVER consistently improves over the base code LLMs(4.6% to
10.9% with code-davinci-002) and achieves new state-of-the-art results on all
of them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1&quot;&gt;Ansong Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_S/0/1/0/all/0/1&quot;&gt;Srini Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radev_D/0/1/0/all/0/1&quot;&gt;Dragomir Radev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoyanov_V/0/1/0/all/0/1&quot;&gt;Ves Stoyanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yih_W/0/1/0/all/0/1&quot;&gt;Wen-tau Yih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sida I. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xi Victoria Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13988">
<title>Machine Psychology: Investigating Emergent Capabilities and Behavior in Large Language Models Using Psychological Methods. (arXiv:2303.13988v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13988</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are currently at the forefront of intertwining
AI systems with human communication and everyday life. Due to rapid
technological advances and their extreme versatility, LLMs nowadays have
millions of users and are at the cusp of being the main go-to technology for
information retrieval, content generation, problem-solving, etc. Therefore, it
is of great importance to thoroughly assess and scrutinize their capabilities.
Due to increasingly complex and novel behavioral patterns in current LLMs, this
can be done by treating them as participants in psychology experiments that
were originally designed to test humans. For this purpose, the paper introduces
a new field of research called &quot;machine psychology&quot;. The paper outlines how
different subfields of psychology can inform behavioral tests for LLMs. It
defines methodological standards for machine psychology research, especially by
focusing on policies for prompt designs. Additionally, it describes how
behavioral patterns discovered in LLMs are to be interpreted. In sum, machine
psychology aims to discover emergent abilities in LLMs that cannot be detected
by most traditional natural language processing benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hagendorff_T/0/1/0/all/0/1&quot;&gt;Thilo Hagendorff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03682">
<title>BenCoref: A Multi-Domain Dataset of Nominal Phrases and Pronominal Reference Annotations. (arXiv:2304.03682v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03682</link>
<description rdf:parseType="Literal">&lt;p&gt;Coreference Resolution is a well studied problem in NLP. While widely studied
for English and other resource-rich languages, research on coreference
resolution in Bengali largely remains unexplored due to the absence of relevant
datasets. Bengali, being a low-resource language, exhibits greater
morphological richness compared to English. In this article, we introduce a new
dataset, BenCoref, comprising coreference annotations for Bengali texts
gathered from four distinct domains. This relatively small dataset contains
5200 mention annotations forming 502 mention clusters within 48,569 tokens. We
describe the process of creating this dataset and report performance of
multiple models trained using BenCoref. We expect that our work provides some
valuable insights on the variations in coreference phenomena across several
domains in Bengali and encourages the development of additional resources for
Bengali. Furthermore, we found poor crosslingual performance at zero-shot
setting from English, highlighting the need for more language-specific
resources for this task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohan_S/0/1/0/all/0/1&quot;&gt;Shadman Rohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_M/0/1/0/all/0/1&quot;&gt;Mojammel Hossain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_M/0/1/0/all/0/1&quot;&gt;Mohammad Mamun Or Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1&quot;&gt;Nabeel Mohammed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02301">
<title>Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes. (arXiv:2305.02301v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02301</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying large language models (LLMs) is challenging because they are memory
inefficient and compute-intensive for practical applications. In reaction,
researchers train smaller task-specific models by either finetuning with human
labels or distilling using LLM-generated labels. However, finetuning and
distillation require large amounts of training data to achieve comparable
performance to LLMs. We introduce Distilling step-by-step, a new mechanism that
(a) trains smaller models that outperform LLMs, and (b) achieves so by
leveraging less training data needed by finetuning or distillation. Our method
extracts LLM rationales as additional supervision for training small models
within a multi-task framework. We present three findings across 4 NLP
benchmarks: First, compared to both finetuning and distillation, our mechanism
achieves better performance with much fewer labeled/unlabeled training
examples. Second, compared to few-shot prompted LLMs, we achieve better
performance using substantially smaller model sizes. Third, we reduce both the
model size and the amount of data required to outperform LLMs; our finetuned
770M T5 model outperforms the few-shot prompted 540B PaLM model using only 80%
of available data on a benchmark, whereas standard finetuning the same T5 model
struggles to match even by using 100% of the dataset. We release the code at:
https://github.com/google-research/distilling-step-by-step .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cheng-Yu Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chun-Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chih-Kuan Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakhost_H/0/1/0/all/0/1&quot;&gt;Hootan Nakhost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_Y/0/1/0/all/0/1&quot;&gt;Yasuhisa Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ratner_A/0/1/0/all/0/1&quot;&gt;Alexander Ratner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chen-Yu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfister_T/0/1/0/all/0/1&quot;&gt;Tomas Pfister&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02531">
<title>A Cross-Linguistic Analysis of Intertemporal Preferences in GPT-3.5. (arXiv:2305.02531v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02531</link>
<description rdf:parseType="Literal">&lt;p&gt;Language has a strong influence on our perceptions of time and rewards. This
raises the question of whether large language models, when asked the same
question in different languages, show different preferences for rewards over
time and if their choices are similar to those of humans. In this study, we
analyze the responses of GPT-3.5 (hereafter referred to as GPT) to prompts in
multiple languages, exploring preferences between smaller, sooner rewards and
larger, later rewards. Our results show that GPT displays greater patience when
prompted in languages with weak future tense references (FTR), such as German
and Mandarin, compared to languages with strong FTR, like English and French.
These findings are consistent with the existing literature and suggest a
correlation between GPT&apos;s choices and the preferences of speakers of these
languages. However, further analysis reveals that the preference for earlier or
later rewards does not systematically change with reward gaps, indicating a
lexicographic preference for earlier payments. While GPT may capture intriguing
variations across languages, our findings indicate that the choices made by
these models do not correspond to those of human decision-makers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goli_A/0/1/0/all/0/1&quot;&gt;Ali Goli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Amandeep Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07611">
<title>Multimodal Sentiment Analysis: A Survey. (arXiv:2305.07611v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07611</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal sentiment analysis has become an important research area in the
field of artificial intelligence. With the latest advances in deep learning,
this technology has reached new heights. It has great potential for both
application and research, making it a popular research topic. This review
provides an overview of the definition, background, and development of
multimodal sentiment analysis. It also covers recent datasets and advanced
models, emphasizing the challenges and future prospects of this technology.
Finally, it looks ahead to future research directions. It should be noted that
this review provides constructive suggestions for promising research directions
and building better performing multimodal sentiment analysis models, which can
help researchers in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_S/0/1/0/all/0/1&quot;&gt;Songning Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xifeng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoxuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zhaoxia Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11991">
<title>Evaluation of medium-large Language Models at zero-shot closed book generative question answering. (arXiv:2305.11991v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11991</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have garnered significant attention, but the
definition of &quot;large&quot; lacks clarity. This paper focuses on medium-sized
language models (MLMs), defined as having at least six billion parameters but
less than 100 billion. The study evaluates MLMs regarding zero-shot generative
question answering, which requires models to provide elaborate answers without
external document retrieval. The paper introduces an own test dataset and
presents results from human evaluation. Results show that combining the best
answers from different MLMs yielded an overall correct answer rate of 82.7%
which is better than the 60.9% of ChatGPT. The best MLM achieved 71.8% and has
33B parameters, which highlights the importance of using appropriate training
data for fine-tuning rather than solely relying on the number of parameters.
More fine-grained feedback should be used to further improve the quality of
answers. The open source community is quickly closing the gap to the best
commercial models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peinl_R/0/1/0/all/0/1&quot;&gt;Ren&amp;#xe9; Peinl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirth_J/0/1/0/all/0/1&quot;&gt;Johannes Wirth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12493">
<title>Contextualized End-to-End Speech Recognition with Contextual Phrase Prediction Network. (arXiv:2305.12493v4 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12493</link>
<description rdf:parseType="Literal">&lt;p&gt;Contextual information plays a crucial role in speech recognition
technologies and incorporating it into the end-to-end speech recognition models
has drawn immense interest recently. However, previous deep bias methods lacked
explicit supervision for bias tasks. In this study, we introduce a contextual
phrase prediction network for an attention-based deep bias method. This network
predicts context phrases in utterances using contextual embeddings and
calculates bias loss to assist in the training of the contextualized model. Our
method achieved a significant word error rate (WER) reduction across various
end-to-end speech recognition models. Experiments on the LibriSpeech corpus
show that our proposed model obtains a 12.1% relative WER improvement over the
baseline model, and the WER of the context phrases decreases relatively by
40.5%. Moreover, by applying a context phrase filtering strategy, we also
effectively eliminate the WER degradation when using a larger biasing list.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaixun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhanheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Pengcheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mu_B/0/1/0/all/0/1&quot;&gt;Bingshen Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lei Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14705">
<title>Mixture-of-Experts Meets Instruction Tuning:A Winning Combination for Large Language Models. (arXiv:2305.14705v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14705</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse Mixture-of-Experts (MoE) is a neural architecture design that can be
utilized to add learnable parameters to Large Language Models (LLMs) without
increasing inference cost. Instruction tuning is a technique for training LLMs
to follow instructions. We advocate combining these two approaches, as we find
that MoE models benefit more from instruction tuning than dense models. In
particular, we conduct empirical studies across three experimental setups: (i)
Direct finetuning on individual downstream tasks devoid of instruction tuning;
(ii) Instructiontuning followed by in-context few-shot or zero-shot
generalization on downstream tasks; and (iii) Instruction tuning supplemented
by further finetuning on individual downstream tasks. In the first scenario,
MoE models overall underperform dense models of identical computational
capacity. This narrative, however, dramatically changes with the introduction
of instruction tuning (second and third scenario), used independently or in
conjunction with task-specific finetuning. Our most powerful model,
FLAN-MOE-32B, surpasses the performance of FLAN-PALM-62B on four benchmark
tasks, while using only a third of the FLOPs. The advancements embodied
byFLAN-MOE inspire a reevaluation of the design principles of large-scale,
high-performance language models in the framework of task-agnostic learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Sheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_L/0/1/0/all/0/1&quot;&gt;Le Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yanqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_N/0/1/0/all/0/1&quot;&gt;Nan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Longpre_S/0/1/0/all/0/1&quot;&gt;Shayne Longpre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jason Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_H/0/1/0/all/0/1&quot;&gt;Hyung Won Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoph_B/0/1/0/all/0/1&quot;&gt;Barret Zoph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedus_W/0/1/0/all/0/1&quot;&gt;William Fedus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Tu Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuexin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wuyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webson_A/0/1/0/all/0/1&quot;&gt;Albert Webson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_V/0/1/0/all/0/1&quot;&gt;Vincent Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongkun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keutzer_K/0/1/0/all/0/1&quot;&gt;Kurt Keutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Denny Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16243">
<title>Surface-Based Retrieval Reduces Perplexity of Retrieval-Augmented Language Models. (arXiv:2305.16243v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16243</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmenting language models with a retrieval mechanism has been shown to
significantly improve their performance while keeping the number of parameters
low. Retrieval-augmented models commonly rely on a semantic retrieval mechanism
based on the similarity between dense representations of the query chunk and
potential neighbors. In this paper, we study the state-of-the-art Retro model
and observe that its performance gain is better explained by surface-level
similarities, such as token overlap. Inspired by this, we replace the semantic
retrieval in Retro with a surface-level method based on BM25, obtaining a
significant reduction in perplexity. As full BM25 retrieval can be
computationally costly for large datasets, we also apply it in a re-ranking
scenario, gaining part of the perplexity reduction with minimal computational
overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doostmohammadi_E/0/1/0/all/0/1&quot;&gt;Ehsan Doostmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norlund_T/0/1/0/all/0/1&quot;&gt;Tobias Norlund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhlmann_M/0/1/0/all/0/1&quot;&gt;Marco Kuhlmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johansson_R/0/1/0/all/0/1&quot;&gt;Richard Johansson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18169">
<title>LM-CPPF: Paraphrasing-Guided Data Augmentation for Contrastive Prompt-Based Few-Shot Fine-Tuning. (arXiv:2305.18169v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18169</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been significant progress in developing
pre-trained language models for NLP. However, these models often struggle when
fine-tuned on small datasets. To address this issue, researchers have proposed
various adaptation approaches. Prompt-based tuning is arguably the most common
way, especially for larger models. Previous research shows that adding
contrastive learning to prompt-based fine-tuning is effective as it helps the
model generate embeddings that are more distinguishable between classes, and it
can also be more sample-efficient as the model learns from positive and
negative examples simultaneously. One of the most important components of
contrastive learning is data augmentation, but unlike computer vision,
effective data augmentation for NLP is still challenging. This paper proposes
LM-CPPF, Contrastive Paraphrasing-guided Prompt-based Fine-tuning of Language
Models, which leverages prompt-based few-shot paraphrasing using generative
language models, especially large language models such as GPT-3 and OPT-175B,
for data augmentation. Our experiments on multiple text classification
benchmarks show that this augmentation method outperforms other methods, such
as easy data augmentation, back translation, and multiple templates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abaskohi_A/0/1/0/all/0/1&quot;&gt;Amirhossein Abaskohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rothe_S/0/1/0/all/0/1&quot;&gt;Sascha Rothe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaghoobzadeh_Y/0/1/0/all/0/1&quot;&gt;Yadollah Yaghoobzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03361">
<title>WHAT, WHEN, and HOW to Ground: Designing User Persona-Aware Conversational Agents for Engaging Dialogue. (arXiv:2306.03361v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03361</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a method for building a personalized open-domain dialogue
system to address the WWH (WHAT, WHEN, and HOW) problem for natural response
generation in a commercial setting, where personalized dialogue responses are
heavily interleaved with casual response turns. The proposed approach involves
weighted dataset blending, negative persona information augmentation methods,
and the design of personalized conversation datasets to address the challenges
of WWH in personalized, open-domain dialogue systems. Our work effectively
balances dialogue fluency and tendency to ground, while also introducing a
response-type label to improve the controllability and explainability of the
grounded responses. The combination of these methods leads to more fluent
conversations, as evidenced by subjective human evaluations as well as
objective evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1&quot;&gt;Deuksin Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sunwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Ki Hyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seojin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taeyoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_E/0/1/0/all/0/1&quot;&gt;Eric Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07797">
<title>Monolingual and Cross-Lingual Knowledge Transfer for Topic Classification. (arXiv:2306.07797v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07797</link>
<description rdf:parseType="Literal">&lt;p&gt;This article investigates the knowledge transfer from the RuQTopics dataset.
This Russian topical dataset combines a large sample number (361,560
single-label, 170,930 multi-label) with extensive class coverage (76 classes).
We have prepared this dataset from the &quot;Yandex Que&quot; raw data. By evaluating the
RuQTopics - trained models on the six matching classes of the Russian MASSIVE
subset, we have proved that the RuQTopics dataset is suitable for real-world
conversational tasks, as the Russian-only models trained on this dataset
consistently yield an accuracy around 85\% on this subset. We also have figured
out that for the multilingual BERT, trained on the RuQTopics and evaluated on
the same six classes of MASSIVE (for all MASSIVE languages), the language-wise
accuracy closely correlates (Spearman correlation 0.773 with p-value 2.997e-11)
with the approximate size of the pretraining BERT&apos;s data for the corresponding
language. At the same time, the correlation of the language-wise accuracy with
the linguistical distance from Russian is not statistically significant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karpov_D/0/1/0/all/0/1&quot;&gt;Dmitry Karpov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burtsev_M/0/1/0/all/0/1&quot;&gt;Mikhail Burtsev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12245">
<title>Bidirectional End-to-End Learning of Retriever-Reader Paradigm for Entity Linking. (arXiv:2306.12245v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12245</link>
<description rdf:parseType="Literal">&lt;p&gt;Entity Linking (EL) is a fundamental task for Information Extraction and
Knowledge Graphs. The general form of EL (i.e., end-to-end EL) aims to first
find mentions in the given input document and then link the mentions to
corresponding entities in a specific knowledge base. Recently, the paradigm of
retriever-reader promotes the progress of end-to-end EL, benefiting from the
advantages of dense entity retrieval and machine reading comprehension.
However, the existing study only trains the retriever and the reader separately
in a pipeline manner, which ignores the benefit that the interaction between
the retriever and the reader can bring to the task. To advance the
retriever-reader paradigm to perform more perfectly on end-to-end EL, we
propose BEER$^2$, a Bidirectional End-to-End training framework for Retriever
and Reader. Through our designed bidirectional end-to-end training, BEER$^2$
guides the retriever and the reader to learn from each other, make progress
together, and ultimately improve EL performance. Extensive experiments on
benchmarks of multiple domains demonstrate the effectiveness of our proposed
BEER$^2$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xingyu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hai-Tao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Ying Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15736">
<title>DMNER: Biomedical Entity Recognition by Detection and Matching. (arXiv:2306.15736v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15736</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical named entity recognition (BNER) serves as the foundation for
numerous biomedical text mining tasks. Unlike general NER, BNER require a
comprehensive grasp of the domain, and incorporating external knowledge beyond
training data poses a significant challenge. In this study, we propose a novel
BNER framework called DMNER. By leveraging existing entity representation
models SAPBERT, we tackle BNER as a two-step process: entity boundary detection
and biomedical entity matching. DMNER exhibits applicability across multiple
NER scenarios: 1) In supervised NER, we observe that DMNER effectively
rectifies the output of baseline NER models, thereby further enhancing
performance. 2) In distantly supervised NER, combining MRC and AutoNER as span
boundary detectors enables DMNER to achieve satisfactory results. 3) For
training NER by merging multiple datasets, we adopt a framework similar to
DS-NER but additionally leverage ChatGPT to obtain high-quality phrases in the
training. Through extensive experiments conducted on 10 benchmark datasets, we
demonstrate the versatility and effectiveness of DMNER.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Junyi Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Rongze Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1&quot;&gt;Weiqi Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Shanfeng Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17059">
<title>The mapKurator System: A Complete Pipeline for Extracting and Linking Text from Historical Maps. (arXiv:2306.17059v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17059</link>
<description rdf:parseType="Literal">&lt;p&gt;Scanned historical maps in libraries and archives are valuable repositories
of geographic data that often do not exist elsewhere. Despite the potential of
machine learning tools like the Google Vision APIs for automatically
transcribing text from these maps into machine-readable formats, they do not
work well with large-sized images (e.g., high-resolution scanned documents),
cannot infer the relation between the recognized text and other datasets, and
are challenging to integrate with post-processing tools. This paper introduces
the mapKurator system, an end-to-end system integrating machine learning models
with a comprehensive data processing pipeline. mapKurator empowers automated
extraction, post-processing, and linkage of text labels from large numbers of
large-dimension historical map scans. The output data, comprising bounding
polygons and recognized text, is in the standard GeoJSON format, making it
easily modifiable within Geographic Information Systems (GIS). The proposed
system allows users to quickly generate valuable data from large numbers of
historical maps for in-depth analysis of the map content and, in turn,
encourages map findability, accessibility, interoperability, and reusability
(FAIR principles). We deployed the mapKurator system and enabled the processing
of over 60,000 maps and over 100 million text/place names in the David Rumsey
Historical Map collection. We also demonstrated a seamless integration of
mapKurator with a collaborative web platform to enable accessing automated
approaches for extracting and linking text labels from historical map scans and
collective work to improve the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jina Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zekun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yijun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namgung_M/0/1/0/all/0/1&quot;&gt;Min Namgung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_L/0/1/0/all/0/1&quot;&gt;Leeje Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Y/0/1/0/all/0/1&quot;&gt;Yao-Yi Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17256">
<title>Towards Personalized Cold-Start Recommendation with Prompts. (arXiv:2306.17256v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17256</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems play a crucial role in helping users discover information
that aligns with their interests based on their past behaviors. However,
developing personalized recommendation systems becomes challenging when
historical records of user-item interactions are unavailable, leading to what
is known as the system cold-start recommendation problem. This issue is
particularly prominent in start-up businesses or platforms with insufficient
user engagement history. Previous studies focus on user or item cold-start
scenarios, where systems could make recommendations for new users or items but
are still trained with historical user-item interactions in the same domain,
which cannot solve our problem. To bridge the gap, our research introduces an
innovative and effective approach, capitalizing on the capabilities of
pre-trained language models. We transform the recommendation process into
sentiment analysis of natural languages containing information of user profiles
and item attributes, where the sentiment polarity is predicted with prompt
learning. By harnessing the extensive knowledge housed within language models,
the prediction can be made without historical user-item interaction records. A
benchmark is also introduced to evaluate the proposed method under the
cold-start setting, and the results demonstrate the effectiveness of our
method. To the best of our knowledge, this is the first study to tackle the
system cold-start recommendation problem. The benchmark and implementation of
the method are available at https://github.com/JacksonWuxs/PromptRec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuansheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Huachi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wenlin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01163">
<title>Improving Language Plasticity via Pretraining with Active Forgetting. (arXiv:2307.01163v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01163</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretrained language models (PLMs) are today the primary model for natural
language processing. Despite their impressive downstream performance, it can be
difficult to apply PLMs to new languages, a barrier to making their
capabilities universally accessible. While prior work has shown it possible to
address this issue by learning a new embedding layer for the new language,
doing so is both data and compute inefficient. We propose to use an active
forgetting mechanism during pretraining, as a simple way of creating PLMs that
can quickly adapt to new languages. Concretely, by resetting the embedding
layer every K updates during pretraining, we encourage the PLM to improve its
ability of learning new embeddings within a limited number of updates, similar
to a meta-learning effect. Experiments with RoBERTa show that models pretrained
with our forgetting mechanism not only demonstrate faster convergence during
language adaptation but also outperform standard ones in a low-data regime,
particularly for languages that are distant from English.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yihong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchisio_K/0/1/0/all/0/1&quot;&gt;Kelly Marchisio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raileanu_R/0/1/0/all/0/1&quot;&gt;Roberta Raileanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adelani_D/0/1/0/all/0/1&quot;&gt;David Ifeoluwa Adelani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stenetorp_P/0/1/0/all/0/1&quot;&gt;Pontus Stenetorp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_S/0/1/0/all/0/1&quot;&gt;Sebastian Riedel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artetxe_M/0/1/0/all/0/1&quot;&gt;Mikel Artetxe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14796">
<title>Are the Best Multilingual Document Embeddings simply Based on Sentence Embeddings?. (arXiv:2304.14796v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2304.14796</link>
<description rdf:parseType="Literal">&lt;p&gt;Dense vector representations for textual data are crucial in modern NLP. Word
embeddings and sentence embeddings estimated from raw texts are key in
achieving state-of-the-art results in various tasks requiring semantic
understanding. However, obtaining embeddings at the document level is
challenging due to computational requirements and lack of appropriate data.
Instead, most approaches fall back on computing document embeddings based on
sentence representations. Although there exist architectures and models to
encode documents fully, they are in general limited to English and few other
high-resourced languages. In this work, we provide a systematic comparison of
methods to produce document-level representations from sentences based on
LASER, LaBSE, and Sentence BERT pre-trained multilingual models. We compare
input token number truncation, sentence averaging as well as some simple
windowing and in some cases new augmented and learnable approaches, on 3 multi-
and cross-lingual tasks in 8 languages belonging to 3 different language
families. Our task-based extrinsic evaluations show that, independently of the
language, a clever combination of sentence embeddings is usually better than
encoding the full document as a single unit, even when this is possible. We
demonstrate that while a simple sentence average results in a strong baseline
for classification tasks, more complex combinations are necessary for semantic
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sannigrahi_S/0/1/0/all/0/1&quot;&gt;Sonal Sannigrahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1&quot;&gt;Josef van Genabith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espana_Bonet_C/0/1/0/all/0/1&quot;&gt;Cristina Espana-Bonet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00165">
<title>Counterfactual Collaborative Reasoning. (arXiv:2307.00165v1 [cs.IR] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.00165</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal reasoning and logical reasoning are two important types of reasoning
abilities for human intelligence. However, their relationship has not been
extensively explored under machine intelligence context. In this paper, we
explore how the two reasoning abilities can be jointly modeled to enhance both
accuracy and explainability of machine learning models. More specifically, by
integrating two important types of reasoning ability -- counterfactual
reasoning and (neural) logical reasoning -- we propose Counterfactual
Collaborative Reasoning (CCR), which conducts counterfactual logic reasoning to
improve the performance. In particular, we use recommender system as an example
to show how CCR alleviate data scarcity, improve accuracy and enhance
transparency. Technically, we leverage counterfactual reasoning to generate
&quot;difficult&quot; counterfactual training examples for data augmentation, which --
together with the original training examples -- can enhance the model
performance. Since the augmented data is model irrelevant, they can be used to
enhance any model, enabling the wide applicability of the technique. Besides,
most of the existing data augmentation methods focus on &quot;implicit data
augmentation&quot; over users&apos; implicit feedback, while our framework conducts
&quot;explicit data augmentation&quot; over users explicit feedback based on
counterfactual logic reasoning. Experiments on three real-world datasets show
that CCR achieves better performance than non-augmented models and implicitly
augmented models, and also improves model transparency by generating
counterfactual explanations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_J/0/1/0/all/0/1&quot;&gt;Jianchao Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zelong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_M/0/1/0/all/0/1&quot;&gt;Max Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Juntao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>