<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-20T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12458" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12554" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12619" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12634" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.13218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.00350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.00196" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03719" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07879" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10080" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1012.5754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1210.1161" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.12442">
<title>Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC) -- an end-to-end model for characterizing severity and diagnosis. (arXiv:2312.12442v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12442</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated classification of cancer pathology reports can extract information
from unstructured reports and categorize each report into structured diagnosis
and severity categories. Thus, such system can reduce the burden for populating
tumor registries, help registration for clinical trial as well as developing
large dataset for deep learning model development using true pathologic ground
truth. However, the content of breast pathology reports can be difficult for
categorize due to the high linguistic variability in content and wide variety
of potential diagnoses &amp;gt;50. Existing NLP models are primarily focused on
developing classifier for primary breast cancer types (e.g. IDC, DCIS, ILC) and
tumor characteristics, and ignore the rare diagnosis of cancer subtypes. We
then developed a hierarchical hybrid transformer-based pipeline (59 labels) -
Hierarchical Classification System for Breast Cancer Specimen Report (HCSBC),
which utilizes the potential of the transformer context-preserving NLP
technique and compared our model to several state of the art ML and DL models.
We trained the model on the EUH data and evaluated our model&apos;s performance on
two external datasets - MGH and Mayo Clinic. We publicly release the code and a
live application under Huggingface spaces repository
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_T/0/1/0/all/0/1&quot;&gt;Thiago Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamath_H/0/1/0/all/0/1&quot;&gt;Harish Kamath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAdams_C/0/1/0/all/0/1&quot;&gt;Christopher R. McAdams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newell_M/0/1/0/all/0/1&quot;&gt;Mary S. Newell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mosunjac_M/0/1/0/all/0/1&quot;&gt;Marina Mosunjac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oprea_Ilies_G/0/1/0/all/0/1&quot;&gt;Gabriela Oprea-Ilies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_G/0/1/0/all/0/1&quot;&gt;Geoffrey Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_C/0/1/0/all/0/1&quot;&gt;Constance Lehman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1&quot;&gt;Judy Gichoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_I/0/1/0/all/0/1&quot;&gt;Imon Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1&quot;&gt;Hari Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12444">
<title>What Makes Pre-Trained Visual Representations Successful for Robust Manipulation?. (arXiv:2312.12444v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12444</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the success of transfer learning in computer vision, roboticists
have investigated visual pre-training as a means to improve the learning
efficiency and generalization ability of policies learned from pixels. To that
end, past work has favored large object interaction datasets, such as
first-person videos of humans completing diverse tasks, in pursuit of
manipulation-relevant features. Although this approach improves the efficiency
of policy learning, it remains unclear how reliable these representations are
in the presence of distribution shifts that arise commonly in robotic
applications. Surprisingly, we find that visual representations designed for
manipulation and control tasks do not necessarily generalize under subtle
changes in lighting and scene texture or the introduction of distractor
objects. To understand what properties do lead to robust representations, we
compare the performance of 15 pre-trained vision models under different visual
appearances. We find that emergent segmentation ability is a strong predictor
of out-of-distribution generalization among ViT models. The rank order induced
by this metric is more predictive than metrics that have previously guided
generalization research within computer vision and machine learning, such as
downstream ImageNet accuracy, in-domain accuracy, or shape-bias as evaluated by
cue-conflict performance. We test this finding extensively on a suite of
distribution shifts in ten tasks across two simulated manipulation
environments. On the ALOHA setup, segmentation score predicts real-world
performance after offline training with 50 demonstrations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burns_K/0/1/0/all/0/1&quot;&gt;Kaylee Burns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witzel_Z/0/1/0/all/0/1&quot;&gt;Zach Witzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamid_J/0/1/0/all/0/1&quot;&gt;Jubayer Ibn Hamid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianhe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12450">
<title>Can It Edit? Evaluating the Ability of Large Language Models to Follow Code Editing Instructions. (arXiv:2312.12450v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.12450</link>
<description rdf:parseType="Literal">&lt;p&gt;A significant amount of research is focused on developing and evaluating
large language models for a variety of code synthesis tasks. These include
synthesizing code from natural language instructions, synthesizing tests from
code, and synthesizing explanations of code. In contrast, the behavior of
instructional code editing with LLMs is understudied. These are tasks in which
the model is instructed to update a block of code provided in a prompt. The
editing instruction may ask for a feature to added or removed, describe a bug
and ask for a fix, ask for a different kind of solution, or many other common
code editing tasks.
&lt;/p&gt;
&lt;p&gt;We introduce a carefully crafted benchmark of code editing tasks and use it
evaluate several cutting edge LLMs. Our evaluation exposes a significant gap
between the capabilities of state-of-the-art open and closed models. For
example, even GPT-3.5-Turbo is 8.8% better than the best open model at editing
code.
&lt;/p&gt;
&lt;p&gt;We also introduce a new, carefully curated, permissively licensed training
set of code edits coupled with natural language instructions. Using this
training set, we show that we can fine-tune open Code LLMs to significantly
improve their code editing capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cassano_F/0/1/0/all/0/1&quot;&gt;Federico Cassano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Luisa Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethi_A/0/1/0/all/0/1&quot;&gt;Akul Sethi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shinn_N/0/1/0/all/0/1&quot;&gt;Noah Shinn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brennan_Jones_A/0/1/0/all/0/1&quot;&gt;Abby Brennan-Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozhkov_A/0/1/0/all/0/1&quot;&gt;Anton Lozhkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_C/0/1/0/all/0/1&quot;&gt;Carolyn Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guha_A/0/1/0/all/0/1&quot;&gt;Arjun Guha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12455">
<title>FengWu-4DVar: Coupling the Data-driven Weather Forecasting Model with 4D Variational Assimilation. (arXiv:2312.12455v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2312.12455</link>
<description rdf:parseType="Literal">&lt;p&gt;Weather forecasting is a crucial yet highly challenging task. With the
maturity of Artificial Intelligence (AI), the emergence of data-driven weather
forecasting models has opened up a new paradigm for the development of weather
forecasting systems. Despite the significant successes that have been achieved
(e.g., surpassing advanced traditional physical models for global medium-range
forecasting), existing data-driven weather forecasting models still rely on the
analysis fields generated by the traditional assimilation and forecasting
system, which hampers the significance of data-driven weather forecasting
models regarding both computational cost and forecasting accuracy. In this
work, we explore the possibility of coupling the data-driven weather
forecasting model with data assimilation by integrating the global AI weather
forecasting model, FengWu, with one of the most popular assimilation
algorithms, Four-Dimensional Variational (4DVar) assimilation, and develop an
AI-based cyclic weather forecasting system, FengWu-4DVar. FengWu-4DVar can
incorporate observational data into the data-driven weather forecasting model
and consider the temporal evolution of atmospheric dynamics to obtain accurate
analysis fields for making predictions in a cycling manner without the help of
physical models. Owning to the auto-differentiation ability of deep learning
models, FengWu-4DVar eliminates the need of developing the cumbersome adjoint
model, which is usually required in the traditional implementation of the 4DVar
algorithm. Experiments on the simulated observational dataset demonstrate that
FengWu-4DVar is capable of generating reasonable analysis fields for making
accurate and efficient iterative predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Xue_W/0/1/0/all/0/1&quot;&gt;Wei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12457">
<title>Let AI Entertain You: Increasing User Engagement with Generative AI and Rejection Sampling. (arXiv:2312.12457v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.12457</link>
<description rdf:parseType="Literal">&lt;p&gt;While generative AI excels in content generation, it does not always increase
user engagement. This can be attributed to two main factors. First, generative
AI generates content without incorporating explicit or implicit feedback about
user interactions. Even if the generated content seems to be more informative
or well-written, it does not necessarily lead to an increase in user
activities, such as clicks. Second, there is a concern with the quality of the
content generative AI produces, which often lacks the distinctiveness and
authenticity that human-created content possesses. These two factors can lead
to content that fails to meet specific needs and preferences of users,
ultimately reducing its potential to be engaging.
&lt;/p&gt;
&lt;p&gt;This paper presents a generic framework of how to improve user engagement
with generative AI by leveraging user feedback. Our solutions employ rejection
sampling, a technique used in reinforcement learning, to boost engagement
metrics. We leveraged the framework in the context of email notification
subject lines generation for an online social network, and achieved significant
engagement metric lift including +1% Session and +0.4% Weekly Active Users. We
believe our work offers a universal framework that enhances user engagement
with generative AI, particularly when standard generative AI reaches its limits
in terms of enhancing content to be more captivating. To the best of our
knowledge, this represents an early milestone in the industry&apos;s successful use
of generative AI to enhance user engagement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Jingying Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jaewon Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_W/0/1/0/all/0/1&quot;&gt;Waleed Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Richard Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Q/0/1/0/all/0/1&quot;&gt;Qi He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12458">
<title>When Parameter-efficient Tuning Meets General-purpose Vision-language Models. (arXiv:2312.12458v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12458</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning has shown promising potential for developing
general-purpose AI capabilities by using large-scale pre-trained models and
boosts growing research to integrate multimodal information for creative
applications. However, existing works still face two main limitations: the high
training costs and heavy computing resource dependence of full model
fine-tuning, and the lack of semantic information in instructions, which
hinders multimodal alignment. Addressing these challenges, this paper proposes
a novel approach to utilize Parameter-Efficient Tuning for generAl-purpose
vision-Language models, namely PETAL. PETAL revolutionizes the training process
by requiring only 0.5% of the total parameters, achieved through a unique mode
approximation technique, which significantly reduces the training costs and
reliance on heavy computing resources. Furthermore, PETAL enhances the semantic
depth of instructions in two innovative ways: 1) by introducing adaptive
instruction mixture-of-experts(MOEs), and 2) by fortifying the score-based
linkage between parameter-efficient tuning and mutual information. Our
extensive experiments across five multimodal downstream benchmarks reveal that
PETAL not only outperforms current state-of-the-art methods in most scenarios
but also surpasses full fine-tuning models in effectiveness. Additionally, our
approach demonstrates remarkable advantages in few-shot settings, backed by
comprehensive visualization analyses. Our source code is available at:
https://github. com/melonking32/PETAL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yihang Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haixin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Jianlong Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinlong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jinan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12462">
<title>Towards an End-to-End Artificial Intelligence Driven Global Weather Forecasting System. (arXiv:2312.12462v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2312.12462</link>
<description rdf:parseType="Literal">&lt;p&gt;The weather forecasting system is important for science and society, and
significant achievements have been made in applying artificial intelligence
(AI) to medium-range weather forecasting. However, existing AI-based weather
forecasting models still rely on analysis or reanalysis products from the
traditional numerical weather prediction (NWP) systems as initial conditions
for making predictions, preventing them from being fully independent systems.
As a crucial component of an end-to-end global weather forecasting system, data
assimilation is vital in generating initial states for forecasting. In this
paper, we present an AI-based data assimilation model, i.e., Adas, for global
weather variables, which learns to generate the analysis from the background
and sparse observations. Different from existing assimilation methods, Adas
employs the gated convolution module to handle sparse observations and the
gated cross-attention module for capturing the interactions between
observations and background efficiently, which are guided by the confidence
matrix to represent the availability and quality of observations. Then, we
combine Adas with the advanced AI-based weather forecasting model (i.e.,
FengWu) and construct the first end-to-end AI-based global weather forecasting
system: FengWu-Adas. Experiments demonstrate that Adas can assimilate the
simulated global observations with the AI-generated background through a
one-year simulation and generate high-quality analysis stably in a cyclic
manner. Based on the generated analysis, FengWu-Adas exhibits skillful
performance and outperforms the Integrated Forecasting System (IFS) in weather
forecasting over seven days.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ling_F/0/1/0/all/0/1&quot;&gt;Fenghua Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12464">
<title>Towards Better Serialization of Tabular Data for Few-shot Classification. (arXiv:2312.12464v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12464</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a study on the integration of Large Language Models (LLMs) in
tabular data classification, emphasizing an efficient framework. Building upon
existing work done in TabLLM (&lt;a href=&quot;/abs/2210.10723&quot;&gt;arXiv:2210.10723&lt;/a&gt;), we introduce three novel
serialization techniques, including the standout LaTeX serialization method.
This method significantly boosts the performance of LLMs in processing
domain-specific datasets, Our method stands out for its memory efficiency and
ability to fully utilize complex data structures. Through extensive
experimentation, including various serialization approaches like feature
combination and importance, we demonstrate our work&apos;s superiority in accuracy
and efficiency over traditional models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaitly_S/0/1/0/all/0/1&quot;&gt;Sukriti Jaitly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_T/0/1/0/all/0/1&quot;&gt;Tanay Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shugani_A/0/1/0/all/0/1&quot;&gt;Ashish Shugani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grewal_R/0/1/0/all/0/1&quot;&gt;Razik Singh Grewal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12467">
<title>Learning Flexible Body Collision Dynamics with Hierarchical Contact Mesh Transformer. (arXiv:2312.12467v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12467</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, many mesh-based graph neural network (GNN) models have been
proposed for modeling complex high-dimensional physical systems. Remarkable
achievements have been made in significantly reducing the solving time compared
to traditional numerical solvers. These methods are typically designed to i)
reduce the computational cost in solving physical dynamics and/or ii) propose
techniques to enhance the solution accuracy in fluid and rigid body dynamics.
However, it remains under-explored whether they are effective in addressing the
challenges of flexible body dynamics, where instantaneous collisions occur
within a very short timeframe. In this paper, we present Hierarchical Contact
Mesh Transformer (HCMT), which uses hierarchical mesh structures and can learn
long-range dependencies (occurred by collisions) among spatially distant
positions of a body -- two close positions in a higher-level mesh corresponds
to two distant positions in a lower-level mesh. HCMT enables long-range
interactions, and the hierarchical mesh structure quickly propagates collision
effects to faraway positions. To this end, it consists of a contact mesh
Transformer and a hierarchical mesh Transformer (CMT and HMT, respectively).
Lastly, we propose a flexible body dynamics dataset, consisting of trajectories
that reflect experimental settings frequently used in the display industry for
product designs. We also compare the performance of several baselines using
well-known benchmark datasets. Our results show that HCMT provides significant
performance improvements over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Youn-Yeol Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jeongwhan Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_W/0/1/0/all/0/1&quot;&gt;Woojin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kookjin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Nayong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kiseok Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_C/0/1/0/all/0/1&quot;&gt;ChangSeung Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_I/0/1/0/all/0/1&quot;&gt;Ilho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;SeokWoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Joon Young Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sooyoung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Noseong Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12469">
<title>Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed. (arXiv:2312.12469v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12469</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural construction models have shown promising performance for Vehicle
Routing Problems (VRPs) by adopting either the Autoregressive (AR) or
Non-Autoregressive (NAR) learning approach. While AR models produce
high-quality solutions, they generally have a high inference latency due to
their sequential generation nature. Conversely, NAR models generate solutions
in parallel with a low inference latency but generally exhibit inferior
performance. In this paper, we propose a generic Guided Non-Autoregressive
Knowledge Distillation (GNARKD) method to obtain high-performance NAR models
having a low inference latency. GNARKD removes the constraint of sequential
generation in AR models while preserving the learned pivotal components in the
network architecture to obtain the corresponding NAR models through knowledge
distillation. We evaluate GNARKD by applying it to three widely adopted AR
models to obtain NAR VRP solvers for both synthesized and real-world instances.
The experimental results demonstrate that GNARKD significantly reduces the
inference time (4-5 times faster) with acceptable performance drop (2-3\%). To
the best of our knowledge, this study is first-of-its-kind to obtain NAR VRP
solvers from AR ones through knowledge distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yubin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingzhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Changliang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12472">
<title>A Performance Evaluation of a Quantized Large Language Model on Various Smartphones. (arXiv:2312.12472v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12472</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the feasibility and performance of on-device large
language model (LLM) inference on various Apple iPhone models. Amidst the rapid
evolution of generative AI, on-device LLMs offer solutions to privacy,
security, and connectivity challenges inherent in cloud-based models.
Leveraging existing literature on running multi-billion parameter LLMs on
resource-limited devices, our study examines the thermal effects and
interaction speeds of a high-performing LLM across different smartphone
generations. We present real-world performance results, providing insights into
on-device inference capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coplu_T/0/1/0/all/0/1&quot;&gt;Tolga &amp;#xc7;&amp;#xf6;pl&amp;#xfc;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loedi_M/0/1/0/all/0/1&quot;&gt;Marc Loedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bendiken_A/0/1/0/all/0/1&quot;&gt;Arto Bendiken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makohin_M/0/1/0/all/0/1&quot;&gt;Mykhailo Makohin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouw_J/0/1/0/all/0/1&quot;&gt;Joshua J. Bouw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobb_S/0/1/0/all/0/1&quot;&gt;Stephen Cobb&lt;/a&gt; (Haltia, Inc.)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12473">
<title>A Study on Social Robot Behavior in Group Conversation. (arXiv:2312.12473v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.12473</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, research in human-robot interaction began to consider a robot&apos;s
influence at the group level. Despite the recent growth in research
investigating the effects of robots within groups of people, our overall
understanding of what happens when robots are placed within groups or teams of
people is still limited. This paper investigates several key problems for soci
robots that manage conversations in a group setting, where the number of
participants is more than two. In a group setting, the conversation dynamics
are a lot more complicated than the conventional one-to-one conversation, thus,
there are more challenges need to be solved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nichols_E/0/1/0/all/0/1&quot;&gt;Eric Nichols&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_R/0/1/0/all/0/1&quot;&gt;Randy Gomez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12475">
<title>Learning to Reweight for Graph Neural Network. (arXiv:2312.12475v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12475</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) show promising results for graph tasks. However,
existing GNNs&apos; generalization ability will degrade when there exist
distribution shifts between testing and training graph data. The cardinal
impetus underlying the severe degeneration is that the GNNs are architected
predicated upon the I.I.D assumptions. In such a setting, GNNs are inclined to
leverage imperceptible statistical correlations subsisting in the training set
to predict, albeit it is a spurious correlation. In this paper, we study the
problem of the generalization ability of GNNs in Out-Of-Distribution (OOD)
settings. To solve this problem, we propose the Learning to Reweight for
Generalizable Graph Neural Network (L2R-GNN) to enhance the generalization
ability for achieving satisfactory performance on unseen testing graphs that
have different distributions with training graphs. We propose a novel nonlinear
graph decorrelation method, which can substantially improve the
out-of-distribution generalization ability and compares favorably to previous
methods in restraining the over-reduced sample size. The variables of the graph
representation are clustered based on the stability of the correlation, and the
graph decorrelation method learns weights to remove correlations between the
variables of different clusters rather than any two variables. Besides, we
interpose an efficacious stochastic algorithm upon bi-level optimization for
the L2R-GNN framework, which facilitates simultaneously learning the optimal
weights and GNN parameters, and avoids the overfitting problem. Experimental
results show that L2R-GNN greatly outperforms baselines on various graph
prediction benchmarks under distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Teng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_K/0/1/0/all/0/1&quot;&gt;Kun Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Z/0/1/0/all/0/1&quot;&gt;Zheqi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinluan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chengqiang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12477">
<title>Survey on Trustworthy Graph Neural Networks: From A Causal Perspective. (arXiv:2312.12477v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12477</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have emerged as powerful representation learning
tools for capturing complex dependencies within diverse graph-structured data.
Despite their success in a wide range of graph mining tasks, GNNs have raised
serious concerns regarding their trustworthiness, including susceptibility to
distribution shift, biases towards certain populations, and lack of
explainability. Recently, integrating causal learning techniques into GNNs has
sparked numerous ground-breaking studies since most of the trustworthiness
issues can be alleviated by capturing the underlying data causality rather than
superficial correlations. In this survey, we provide a comprehensive review of
recent research efforts on causality-inspired GNNs. Specifically, we first
present the key trustworthy risks of existing GNN models through the lens of
causality. Moreover, we introduce a taxonomy of Causality-Inspired GNNs
(CIGNNs) based on the type of causal learning capability they are equipped
with, i.e., causal reasoning and causal representation learning. Besides, we
systematically discuss typical methods within each category and demonstrate how
they mitigate trustworthiness risks. Finally, we summarize useful resources and
discuss several future directions, hoping to shed light on new research
opportunities in this emerging field. The representative papers, along with
open-source data and codes, are available in
https://github.com/usail-hkust/Causality-Inspired-GNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenzhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12483">
<title>SCoTTi: Save Computation at Training Time with an adaptive framework. (arXiv:2312.12483v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12483</link>
<description rdf:parseType="Literal">&lt;p&gt;On-device training is an emerging approach in machine learning where models
are trained on edge devices, aiming to enhance privacy protection and real-time
performance. However, edge devices typically possess restricted computational
power and resources, making it challenging to perform computationally intensive
model training tasks. Consequently, reducing resource consumption during
training has become a pressing concern in this field. To this end, we propose
SCoTTi (Save Computation at Training Time), an adaptive framework that
addresses the aforementioned challenge. It leverages an optimizable threshold
parameter to effectively reduce the number of neuron updates during training
which corresponds to a decrease in memory and computation footprint. Our
proposed approach demonstrates superior performance compared to the
state-of-the-art methods regarding computational resource savings on various
commonly employed benchmarks and popular architectures, including ResNets,
MobileNet, and Swin-T.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Ziyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Van-Tam Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12485">
<title>Learning Deterministic Surrogates for Robust Convex QCQPs. (arXiv:2312.12485v1 [math.OC])</title>
<link>http://arxiv.org/abs/2312.12485</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision-focused learning is a promising development for contextual
optimisation. It enables us to train prediction models that reflect the
contextual sensitivity structure of the problem. However, there have been
limited attempts to extend this paradigm to robust optimisation. We propose a
double implicit layer model for training prediction models with respect to
robust decision loss in uncertain convex quadratically constrained quadratic
programs (QCQP). The first layer solves a deterministic version of the problem,
the second layer evaluates the worst case realisation for an uncertainty set
centred on the observation given the decisions obtained from the first layer.
This enables us to learn model parameterisations that lead to robust decisions
while only solving a simpler deterministic problem at test time. Additionally,
instead of having to solve a robust counterpart we solve two smaller and
potentially easier problems in training. The second layer (worst case problem)
can be seen as a regularisation approach for predict-and-optimise by fitting to
a neighbourhood of problems instead of just a point observation. We motivate
relaxations of the worst-case problem in cases of uncertainty sets that would
otherwise lead to trust region problems, and leverage various relaxations to
deal with uncertain constraints. Both layers are typically strictly convex in
this problem setting and thus have meaningful gradients almost everywhere. We
demonstrate an application of this model on simulated experiments. The method
is an effective regularisation tool for decision-focused learning for uncertain
convex QCQPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Persak_E/0/1/0/all/0/1&quot;&gt;Egon Per&amp;#x161;ak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Anjos_M/0/1/0/all/0/1&quot;&gt;Miguel F. Anjos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12486">
<title>Vision-Based Automatic Groceries Tracking System -- Smart Homes. (arXiv:2312.12486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12486</link>
<description rdf:parseType="Literal">&lt;p&gt;With advanced AI, while every industry is growing at rocket speed, the smart
home industry has not reached the next generation. There is still a huge leap
of innovation that needs to happen before we call a home a Smart home. A Smart
home should predict residents&apos; needs and fulfill them in a timely manner. One
of the important tasks of maintaining a home is timely grocery tracking and
supply maintenance. Grocery tracking models are very famous in the retail
industry but they are nonexistent in the common household. Groceries detection
in household refrigerators or storage closets is very complicated compared to
retail shelving data. In this paper, home grocery tracking problem is resolved
by combining retail shelving data and fruits dataset with real-time 360 view
data points collected from home groceries storage. By integrating this
vision-based object detection system along with supply chain and user food
interest prediction systems, complete automation of groceries ordering can be
achieved.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mereddy_D/0/1/0/all/0/1&quot;&gt;Divya Mereddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12487">
<title>Adaptive Guidance: Training-free Acceleration of Conditional Diffusion Models. (arXiv:2312.12487v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12487</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a comprehensive study on the role of Classifier-Free
Guidance (CFG) in text-conditioned diffusion models from the perspective of
inference efficiency. In particular, we relax the default choice of applying
CFG in all diffusion steps and instead search for efficient guidance policies.
We formulate the discovery of such policies in the differentiable Neural
Architecture Search framework. Our findings suggest that the denoising steps
proposed by CFG become increasingly aligned with simple conditional steps,
which renders the extra neural network evaluation of CFG redundant, especially
in the second half of the denoising process. Building upon this insight, we
propose &quot;Adaptive Guidance&quot; (AG), an efficient variant of CFG, that adaptively
omits network evaluations when the denoising process displays convergence. Our
experiments demonstrate that AG preserves CFG&apos;s image quality while reducing
computation by 25%. Thus, AG constitutes a plug-and-play alternative to
Guidance Distillation, achieving 50% of the speed-ups of the latter while being
training-free and retaining the capacity to handle negative prompts. Finally,
we uncover further redundancies of CFG in the first half of the diffusion
process, showing that entire neural function evaluations can be replaced by
simple affine transformations of past score estimates. This method, termed
LinearAG, offers even cheaper inference at the cost of deviating from the
baseline model. Our findings provide insights into the efficiency of the
conditional denoising process that contribute to more practical and swift
deployment of text-conditioned diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_A/0/1/0/all/0/1&quot;&gt;Angela Castillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jonas Kohler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1&quot;&gt;Juan C. P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_J/0/1/0/all/0/1&quot;&gt;Juan Pablo P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pumarola_A/0/1/0/all/0/1&quot;&gt;Albert Pumarola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbelaez_P/0/1/0/all/0/1&quot;&gt;Pablo Arbel&amp;#xe1;ez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thabet_A/0/1/0/all/0/1&quot;&gt;Ali Thabet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12489">
<title>H-ensemble: An Information Theoretic Approach to Reliable Few-Shot Multi-Source-Free Transfer. (arXiv:2312.12489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12489</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-source transfer learning is an effective solution to data scarcity by
utilizing multiple source tasks for the learning of the target task. However,
access to source data and model details is limited in the era of commercial
models, giving rise to the setting of multi-source-free (MSF) transfer learning
that aims to leverage source domain knowledge without such access. As a newly
defined problem paradigm, MSF transfer learning remains largely underexplored
and not clearly formulated. In this work, we adopt an information theoretic
perspective on it and propose a framework named H-ensemble, which dynamically
learns the optimal linear combination, or ensemble, of source models for the
target task, using a generalization of maximal correlation regression. The
ensemble weights are optimized by maximizing an information theoretic metric
for transferability. Compared to previous works, H-ensemble is characterized
by: 1) its adaptability to a novel and realistic MSF setting for few-shot
target tasks, 2) theoretical reliability, 3) a lightweight structure easy to
interpret and adapt. Our method is empirically validated by ablation studies,
along with extensive comparative analysis with other task ensemble and transfer
learning methods. We show that the H-ensemble can successfully learn the
optimal task ensemble, as well as outperform prior arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yanru Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12490">
<title>InstructVideo: Instructing Video Diffusion Models with Human Feedback. (arXiv:2312.12490v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12490</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as the de facto paradigm for video generation.
However, their reliance on web-scale data of varied quality often yields
results that are visually unappealing and misaligned with the textual prompts.
To tackle this problem, we propose InstructVideo to instruct text-to-video
diffusion models with human feedback by reward fine-tuning. InstructVideo has
two key ingredients: 1) To ameliorate the cost of reward fine-tuning induced by
generating through the full DDIM sampling chain, we recast reward fine-tuning
as editing. By leveraging the diffusion process to corrupt a sampled video,
InstructVideo requires only partial inference of the DDIM sampling chain,
reducing fine-tuning cost while improving fine-tuning efficiency. 2) To
mitigate the absence of a dedicated video reward model for human preferences,
we repurpose established image reward models, e.g., HPSv2. To this end, we
propose Segmental Video Reward, a mechanism to provide reward signals based on
segmental sparse sampling, and Temporally Attenuated Reward, a method that
mitigates temporal modeling degradation during fine-tuning. Extensive
experiments, both qualitative and quantitative, validate the practicality and
efficacy of using image reward models in InstructVideo, significantly enhancing
the visual quality of generated videos without compromising generalization
capabilities. Code and models will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hangjie Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yujie Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Tao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yining Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanie_S/0/1/0/all/0/1&quot;&gt;Samuel Albanie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12554">
<title>Rectangle Search: An Anytime Beam Search (Extended Version). (arXiv:2312.12554v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.12554</link>
<description rdf:parseType="Literal">&lt;p&gt;Anytime heuristic search algorithms try to find a (potentially suboptimal)
solution as quickly as possible and then work to find better and better
solutions until an optimal solution is obtained or time is exhausted. The most
widely-known anytime search algorithms are based on best-first search. In this
paper, we propose a new algorithm, rectangle search, that is instead based on
beam search, a variant of breadth-first search. It repeatedly explores
alternatives at all depth levels and is thus best-suited to problems featuring
deep local minima. Experiments using a variety of popular search benchmarks
suggest that rectangle search is competitive with fixed-width beam search and
often performs better than the previous best anytime search algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lemons_S/0/1/0/all/0/1&quot;&gt;Sofia Lemons&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruml_W/0/1/0/all/0/1&quot;&gt;Wheeler Ruml&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holte_R/0/1/0/all/0/1&quot;&gt;Robert C. Holte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_C/0/1/0/all/0/1&quot;&gt;Carlos Linares L&amp;#xf3;pez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12560">
<title>Comprehensive Validation on Reweighting Samples for Bias Mitigation via AIF360. (arXiv:2312.12560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12560</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness AI aims to detect and alleviate bias across the entire AI
development life cycle, encompassing data curation, modeling, evaluation, and
deployment-a pivotal aspect of ethical AI implementation. Addressing data bias,
particularly concerning sensitive attributes like gender and race, reweighting
samples proves efficient for fairness AI. This paper contributes a systematic
examination of reweighting samples for traditional machine learning (ML)
models, employing five models for binary classification on the Adult Income and
COMPUS datasets with various protected attributes. The study evaluates
prediction results using five fairness metrics, uncovering the nuanced and
model-specific nature of reweighting sample effectiveness in achieving fairness
in traditional ML models, as well as revealing the complexity of bias dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blow_C/0/1/0/all/0/1&quot;&gt;Christina Hastings Blow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_L/0/1/0/all/0/1&quot;&gt;Lijun Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gibson_C/0/1/0/all/0/1&quot;&gt;Camille Gibson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obiomon_P/0/1/0/all/0/1&quot;&gt;Pamela Obiomon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xishuang Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12568">
<title>Scaling Opponent Shaping to High Dimensional Games. (arXiv:2312.12568v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.12568</link>
<description rdf:parseType="Literal">&lt;p&gt;In multi-agent settings with mixed incentives, methods developed for zero-sum
games have been shown to lead to detrimental outcomes. To address this issue,
opponent shaping (OS) methods explicitly learn to influence the learning
dynamics of co-players and empirically lead to improved individual and
collective outcomes. However, OS methods have only been evaluated in
low-dimensional environments due to the challenges associated with estimating
higher-order derivatives or scaling model-free meta-learning. Alternative
methods that scale to more complex settings either converge to undesirable
solutions or rely on unrealistic assumptions about the environment or
co-players. In this paper, we successfully scale an OS-based approach to
general-sum games with temporally-extended actions and long-time horizons for
the first time. After analysing the representations of the meta-state and
history used by previous algorithms, we propose a simplified version called
Shaper. We show empirically that Shaper leads to improved individual and
collective outcomes in a range of challenging settings from literature. We
further formalize a technique previously implicit in the literature, and
analyse its contribution to opponent shaping. We show empirically that this
technique is helpful for the functioning of prior methods in certain
environments. Lastly, we show that previous environments, such as the CoinGame,
are inadequate for analysing temporally-extended general-sum interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Akbir Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willi_T/0/1/0/all/0/1&quot;&gt;Timon Willi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwan_N/0/1/0/all/0/1&quot;&gt;Newton Kwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tacchetti_A/0/1/0/all/0/1&quot;&gt;Andrea Tacchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Chris Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grefenstette_E/0/1/0/all/0/1&quot;&gt;Edward Grefenstette&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocktaschel_T/0/1/0/all/0/1&quot;&gt;Tim Rockt&amp;#xe4;schel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Foerster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12585">
<title>BadRL: Sparse Targeted Backdoor Attack Against Reinforcement Learning. (arXiv:2312.12585v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12585</link>
<description rdf:parseType="Literal">&lt;p&gt;Backdoor attacks in reinforcement learning (RL) have previously employed
intense attack strategies to ensure attack success. However, these methods
suffer from high attack costs and increased detectability. In this work, we
propose a novel approach, BadRL, which focuses on conducting highly sparse
backdoor poisoning efforts during training and testing while maintaining
successful attacks. Our algorithm, BadRL, strategically chooses state
observations with high attack values to inject triggers during training and
testing, thereby reducing the chances of detection. In contrast to the previous
methods that utilize sample-agnostic trigger patterns, BadRL dynamically
generates distinct trigger patterns based on targeted state observations,
thereby enhancing its effectiveness. Theoretical analysis shows that the
targeted backdoor attack is always viable and remains stealthy under specific
assumptions. Empirical results on various classic RL tasks illustrate that
BadRL can substantially degrade the performance of a victim agent with minimal
poisoning efforts 0.003% of total training steps) during training and
infrequent attacks during testing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jing Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yufei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jianbin Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12598">
<title>A Case Study on Test Case Construction with Large Language Models: Unveiling Practical Insights and Challenges. (arXiv:2312.12598v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.12598</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a detailed case study examining the application of Large
Language Models (LLMs) in the construction of test cases within the context of
software engineering. LLMs, characterized by their advanced natural language
processing capabilities, are increasingly garnering attention as tools to
automate and enhance various aspects of the software development lifecycle.
Leveraging a case study methodology, we systematically explore the integration
of LLMs in the test case construction process, aiming to shed light on their
practical efficacy, challenges encountered, and implications for software
quality assurance.
&lt;/p&gt;
&lt;p&gt;The study encompasses the selection of a representative software application,
the formulation of test case construction methodologies employing LLMs, and the
subsequent evaluation of outcomes. Through a blend of qualitative and
quantitative analyses, we assess the impact of LLMs on test case
comprehensiveness, accuracy, and efficiency. Additionally, we delve into
challenges such as model interpretability, ethical considerations, and
adaptation to diverse software contexts.
&lt;/p&gt;
&lt;p&gt;The findings from this case study contribute nuanced insights into the
practical utility of LLMs in the domain of test case construction, elucidating
their potential benefits and limitations. By addressing real-world scenarios
and complexities, this research aims to inform software practitioners and
researchers alike about the tangible implications of incorporating LLMs into
the software testing landscape, fostering a more comprehensive understanding of
their role in optimizing the software development process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junior_R/0/1/0/all/0/1&quot;&gt;Roberto Francisco de Lima Junior&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Presta_L/0/1/0/all/0/1&quot;&gt;Luiz Fernando Paes de Barros Presta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borborema_L/0/1/0/all/0/1&quot;&gt;Lucca Santos Borborema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_V/0/1/0/all/0/1&quot;&gt;Vanderson Nogueira da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahia_M/0/1/0/all/0/1&quot;&gt;Marcio Leal de Melo Dahia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_A/0/1/0/all/0/1&quot;&gt;Anderson Carlos Sousa e Santos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12599">
<title>Unsupervised Segmentation of Colonoscopy Images. (arXiv:2312.12599v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.12599</link>
<description rdf:parseType="Literal">&lt;p&gt;Colonoscopy plays a crucial role in the diagnosis and prognosis of various
gastrointestinal diseases. Due to the challenges of collecting large-scale
high-quality ground truth annotations for colonoscopy images, and more
generally medical images, we explore using self-supervised features from vision
transformers in three challenging tasks for colonoscopy images. Our results
indicate that image-level features learned from DINO models achieve image
classification performance comparable to fully supervised models, and
patch-level features contain rich semantic information for object detection.
Furthermore, we demonstrate that self-supervised features combined with
unsupervised segmentation can be used to discover multiple clinically relevant
structures in a fully unsupervised manner, demonstrating the tremendous
potential of applying these methods in medical image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Heming Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Luscher_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xf4;me L&amp;#xfc;scher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Becker_B/0/1/0/all/0/1&quot;&gt;Benjamin Gutierrez Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arus_Pous_J/0/1/0/all/0/1&quot;&gt;Josep Ar&amp;#xfa;s-Pous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Biancalani_T/0/1/0/all/0/1&quot;&gt;Tommaso Biancalani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bigorgne_A/0/1/0/all/0/1&quot;&gt;Amelie Bigorgne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Richmond_D/0/1/0/all/0/1&quot;&gt;David Richmond&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12619">
<title>Hierarchical Vision Transformers for Context-Aware Prostate Cancer Grading in Whole Slide Images. (arXiv:2312.12619v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12619</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) have ushered in a new era in computer vision,
showcasing unparalleled performance in many challenging tasks. However, their
practical deployment in computational pathology has largely been constrained by
the sheer size of whole slide images (WSIs), which result in lengthy input
sequences. Transformers faced a similar limitation when applied to long
documents, and Hierarchical Transformers were introduced to circumvent it.
Given the analogous challenge with WSIs and their inherent hierarchical
structure, Hierarchical Vision Transformers (H-ViTs) emerge as a promising
solution in computational pathology. This work delves into the capabilities of
H-ViTs, evaluating their efficiency for prostate cancer grading in WSIs. Our
results show that they achieve competitive performance against existing
state-of-the-art solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grisi_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Grisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Litjens_G/0/1/0/all/0/1&quot;&gt;Geert Litjens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laak_J/0/1/0/all/0/1&quot;&gt;Jeroen van der Laak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12634">
<title>MotionScript: Natural Language Descriptions for Expressive 3D Human Motions. (arXiv:2312.12634v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12634</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes MotionScript, a motion-to-text conversion algorithm and
natural language representation for human body motions. MotionScript aims to
describe movements in greater detail and with more accuracy than previous
natural language approaches. Many motion datasets describe relatively objective
and simple actions with little variation on the way they are expressed (e.g.
sitting, walking, dribbling a ball). But for expressive actions that contain a
diversity of movements in the class (e.g. being sad, dancing), or for actions
outside the domain of standard motion capture datasets (e.g. stylistic walking,
sign-language), more specific and granular natural language descriptions are
needed. Our proposed MotionScript descriptions differ from existing natural
language representations in that it provides direct descriptions in natural
language instead of simple action labels or high-level human captions. To the
best of our knowledge, this is the first attempt at translating 3D motions to
natural language descriptions without requiring training data. Our experiments
show that when MotionScript representations are used in a text-to-motion neural
task, body movements are more accurately reconstructed, and large language
models can be used to generate unseen complex motions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yazdian_P/0/1/0/all/0/1&quot;&gt;Payam Jome Yazdian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_E/0/1/0/all/0/1&quot;&gt;Eric Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Li Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_A/0/1/0/all/0/1&quot;&gt;Angelica Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12655">
<title>Can Transformers Learn Sequential Function Classes In Context?. (arXiv:2312.12655v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12655</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) has revolutionized the capabilities of transformer
models in NLP. In our project, we extend the understanding of the mechanisms
underpinning ICL by exploring whether transformers can learn from sequential,
non-textual function class data distributions. We introduce a novel sliding
window sequential function class and employ toy-sized transformers with a GPT-2
architecture to conduct our experiments. Our analysis indicates that these
models can indeed leverage ICL when trained on non-textual sequential function
classes. Additionally, our experiments with randomized y-label sequences
highlights that transformers retain some ICL capabilities even when the label
associations are obfuscated. We provide evidence that transformers can reason
with and understand sequentiality encoded within function classes, as reflected
by the effective learning of our proposed tasks. Our results also show that the
performance deteriorated with increasing randomness in the labels, though not
to the extent one might expect, implying a potential robustness of learned
sequentiality against label noise. Future research may want to look into how
previous explanations of transformers, such as induction heads and task
vectors, relate to sequentiality in ICL in these toy examples. Our
investigation lays the groundwork for further research into how transformers
process and perceive sequential data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_R/0/1/0/all/0/1&quot;&gt;Ryan Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_E/0/1/0/all/0/1&quot;&gt;Emma Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_E/0/1/0/all/0/1&quot;&gt;Evan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vir_R/0/1/0/all/0/1&quot;&gt;Reya Vir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_E/0/1/0/all/0/1&quot;&gt;Ethan Hsiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12657">
<title>The Convex Landscape of Neural Networks: Characterizing Global Optima and Stationary Points via Lasso Models. (arXiv:2312.12657v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12657</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the non-convex nature of training Deep Neural Network (DNN) models,
their effectiveness relies on the use of non-convex optimization heuristics.
Traditional methods for training DNNs often require costly empirical methods to
produce successful models and do not have a clear theoretical foundation. In
this study, we examine the use of convex optimization theory and sparse
recovery models to refine the training process of neural networks and provide a
better interpretation of their optimal weights. We focus on training two-layer
neural networks with piecewise linear activations and demonstrate that they can
be formulated as a finite-dimensional convex program. These programs include a
regularization term that promotes sparsity, which constitutes a variant of
group Lasso. We first utilize semi-infinite programming theory to prove strong
duality for finite width neural networks and then we express these
architectures equivalently as high dimensional convex sparse recovery models.
Remarkably, the worst-case complexity to solve the convex program is polynomial
in the number of samples and number of neurons when the rank of the data matrix
is bounded, which is the case in convolutional networks. To extend our method
to training data of arbitrary rank, we develop a novel polynomial-time
approximation scheme based on zonotope subsampling that comes with a guaranteed
approximation ratio. We also show that all the stationary of the nonconvex
training objective can be characterized as the global optimum of a subsampled
convex program. Our convex models can be trained using standard convex solvers
without resorting to heuristics or extensive hyper-parameter tuning unlike
non-convex methods. Through extensive numerical experiments, we show that
convex models can outperform traditional non-convex methods and are not
sensitive to optimizer hyperparameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ergen_T/0/1/0/all/0/1&quot;&gt;Tolga Ergen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilanci_M/0/1/0/all/0/1&quot;&gt;Mert Pilanci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12667">
<title>Discovering Malicious Signatures in Software from Structural Interactions. (arXiv:2312.12667v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.12667</link>
<description rdf:parseType="Literal">&lt;p&gt;Malware represents a significant security concern in today&apos;s digital
landscape, as it can destroy or disable operating systems, steal sensitive user
information, and occupy valuable disk space. However, current malware detection
methods, such as static-based and dynamic-based approaches, struggle to
identify newly developed (``zero-day&quot;) malware and are limited by customized
virtual machine (VM) environments. To overcome these limitations, we propose a
novel malware detection approach that leverages deep learning, mathematical
techniques, and network science. Our approach focuses on static and dynamic
analysis and utilizes the Low-Level Virtual Machine (LLVM) to profile
applications within a complex network. The generated network topologies are
input into the GraphSAGE architecture to efficiently distinguish between benign
and malicious software applications, with the operation names denoted as node
features. Importantly, the GraphSAGE models analyze the network&apos;s topological
geometry to make predictions, enabling them to detect state-of-the-art malware
and prevent potential damage during execution in a VM. To evaluate our
approach, we conduct a study on a dataset comprising source code from 24,376
applications, specifically written in C/C++, sourced directly from
widely-recognized malware and various types of benign software. The results
show a high detection performance with an Area Under the Receiver Operating
Characteristic Curve (AUROC) of 99.85%. Our approach marks a substantial
improvement in malware detection, providing a notably more accurate and
efficient solution when compared to current state-of-the-art malware detection
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chenzhong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hantang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Mingxi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiongye Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xin Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1&quot;&gt;Paul Bogdan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12668">
<title>Convolutional Channel-wise Competitive Learning for the Forward-Forward Algorithm. (arXiv:2312.12668v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12668</link>
<description rdf:parseType="Literal">&lt;p&gt;The Forward-Forward (FF) Algorithm has been recently proposed to alleviate
the issues of backpropagation (BP) commonly used to train deep neural networks.
However, its current formulation exhibits limitations such as the generation of
negative data, slower convergence, and inadequate performance on complex tasks.
In this paper, we take the main ideas of FF and improve them by leveraging
channel-wise competitive learning in the context of convolutional neural
networks for image classification tasks. A layer-wise loss function is
introduced that promotes competitive learning and eliminates the need for
negative data construction. To enhance both the learning of compositional
features and feature space partitioning, a channel-wise feature separator and
extractor block is proposed that complements the competitive learning process.
Our method outperforms recent FF-based models on image classification tasks,
achieving testing errors of 0.58%, 7.69%, 21.89%, and 48.77% on MNIST,
Fashion-MNIST, CIFAR-10 and CIFAR-100 respectively. Our approach bridges the
performance gap between FF learning and BP methods, indicating the potential of
our proposed approach to learn useful representations in a layer-wise modular
fashion, enabling more efficient and flexible learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papachristodoulou_A/0/1/0/all/0/1&quot;&gt;Andreas Papachristodoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrkou_C/0/1/0/all/0/1&quot;&gt;Christos Kyrkou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timotheou_S/0/1/0/all/0/1&quot;&gt;Stelios Timotheou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theocharides_T/0/1/0/all/0/1&quot;&gt;Theocharis Theocharides&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12670">
<title>On the Role of Server Momentum in Federated Learning. (arXiv:2312.12670v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12670</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Averaging (FedAvg) is known to experience convergence issues when
encountering significant clients system heterogeneity and data heterogeneity.
Server momentum has been proposed as an effective mitigation. However, existing
server momentum works are restrictive in the momentum formulation, do not
properly schedule hyperparameters and focus only on system homogeneous
settings, which leaves the role of server momentum still an under-explored
problem. In this paper, we propose a general framework for server momentum,
that (a) covers a large class of momentum schemes that are unexplored in
federated learning (FL), (b) enables a popular stagewise hyperparameter
scheduler, (c) allows heterogeneous and asynchronous local computing. We
provide rigorous convergence analysis for the proposed framework. To our best
knowledge, this is the first work that thoroughly analyzes the performances of
server momentum with a hyperparameter scheduler and system heterogeneity.
Extensive experiments validate the effectiveness of our proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianhui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xidong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aidong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12676">
<title>Combinatorial Gaussian Process Bandits in Bayesian Settings: Theory and Application for Energy-Efficient Navigation. (arXiv:2312.12676v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12676</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a combinatorial Gaussian process semi-bandit problem with
time-varying arm availability. Each round, an agent is provided a set of
available base arms and must select a subset of them to maximize the long-term
cumulative reward. Assuming the expected rewards are sampled from a Gaussian
process (GP) over the arm space, the agent can efficiently learn. We study the
Bayesian setting and provide novel Bayesian regret bounds for three GP-based
algorithms: GP-UCB, Bayes-GP-UCB and GP-TS. Our bounds extend previous results
for GP-UCB and GP-TS to a combinatorial setting with varying arm availability
and to the best of our knowledge, we provide the first Bayesian regret bound
for Bayes-GP-UCB. Time-varying arm availability encompasses other widely
considered bandit problems such as contextual bandits. We formulate the online
energy-efficient navigation problem as a combinatorial and contextual bandit
and provide a comprehensive experimental study on synthetic and real-world road
networks with detailed simulations. The contextual GP model obtains lower
regret and is less dependent on the informativeness of the prior compared to
the non-contextual Bayesian inference model. In addition, Thompson sampling
obtains lower regret than Bayes-UCB for both the contextual and non-contextual
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandberg_J/0/1/0/all/0/1&quot;&gt;Jack Sandberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+%7B%5CAA%7Dkerblom_N/0/1/0/all/0/1&quot;&gt;Niklas &amp;#xc5;kerblom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chehreghani_M/0/1/0/all/0/1&quot;&gt;Morteza Haghir Chehreghani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12678">
<title>Causal Discovery for fMRI data: Challenges, Solutions, and a Case Study. (arXiv:2312.12678v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2312.12678</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing studies that apply causal discovery requires navigating many
researcher degrees of freedom. This complexity is exacerbated when the study
involves fMRI data. In this paper we (i) describe nine challenges that occur
when applying causal discovery to fMRI data, (ii) discuss the space of
decisions that need to be made, (iii) review how a recent case study made those
decisions, (iv) and identify existing gaps that could potentially be solved by
the development of new methods. Overall, causal discovery is a promising
approach for analyzing fMRI data, and multiple successful applications have
indicated that it is superior to traditional fMRI functional connectivity
methods, but current causal discovery methods for fMRI leave room for
improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rawls_E/0/1/0/all/0/1&quot;&gt;Eric Rawls&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Andrews_B/0/1/0/all/0/1&quot;&gt;Bryan Andrews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lim_K/0/1/0/all/0/1&quot;&gt;Kelvin Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kummerfeld_E/0/1/0/all/0/1&quot;&gt;Erich Kummerfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12679">
<title>Towards Efficient Verification of Quantized Neural Networks. (arXiv:2312.12679v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12679</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantization replaces floating point arithmetic with integer arithmetic in
deep neural network models, providing more efficient on-device inference with
less power and memory. In this work, we propose a framework for formally
verifying properties of quantized neural networks. Our baseline technique is
based on integer linear programming which guarantees both soundness and
completeness. We then show how efficiency can be improved by utilizing
gradient-based heuristic search methods and also bound-propagation techniques.
We evaluate our approach on perception networks quantized with PyTorch. Our
results show that we can verify quantized networks with better scalability and
efficiency than the previous state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Pei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoze Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuting Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daukantas_I/0/1/0/all/0/1&quot;&gt;Ieva Daukantas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yedi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_C/0/1/0/all/0/1&quot;&gt;Clark Barrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12681">
<title>Imitation of Life: A Search Engine for Biologically Inspired Design. (arXiv:2312.12681v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12681</link>
<description rdf:parseType="Literal">&lt;p&gt;Biologically Inspired Design (BID), or Biomimicry, is a problem-solving
methodology that applies analogies from nature to solve engineering challenges.
For example, Speedo engineers designed swimsuits based on shark skin. Finding
relevant biological solutions for real-world problems poses significant
challenges, both due to the limited biological knowledge engineers and
designers typically possess and to the limited BID resources. Existing BID
datasets are hand-curated and small, and scaling them up requires costly human
annotations.
&lt;/p&gt;
&lt;p&gt;In this paper, we introduce BARcode (Biological Analogy Retriever), a search
engine for automatically mining bio-inspirations from the web at scale. Using
advances in natural language understanding and data programming, BARcode
identifies potential inspirations for engineering challenges. Our experiments
demonstrate that BARcode can retrieve inspirations that are valuable to
engineers and designers tackling real-world problems, as well as recover famous
historical BID examples. We release data and code; we view BARcode as a step
towards addressing the challenges that have historically hindered the practical
application of BID to engineering innovation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emuna_H/0/1/0/all/0/1&quot;&gt;Hen Emuna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borenstein_N/0/1/0/all/0/1&quot;&gt;Nadav Borenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xin Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_H/0/1/0/all/0/1&quot;&gt;Hyeonsu Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_J/0/1/0/all/0/1&quot;&gt;Joel Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kittur_A/0/1/0/all/0/1&quot;&gt;Aniket Kittur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1&quot;&gt;Dafna Shahaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12682">
<title>Mini-GPTs: Efficient Large Language Models through Contextual Pruning. (arXiv:2312.12682v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12682</link>
<description rdf:parseType="Literal">&lt;p&gt;In AI research, the optimization of Large Language Models (LLMs) remains a
significant challenge, crucial for advancing the field&apos;s practical applications
and sustainability. Building upon the foundational work of Professor Song Han&apos;s
lab at MIT, this paper introduces a novel approach in developing Mini-GPTs via
contextual pruning. Our methodology strategically prunes the computational
architecture of traditional LLMs, like Phi-1.5, focusing on retaining core
functionalities while drastically reducing model sizes. We employ the technique
across diverse and complex datasets, including US law, Medical Q&amp;amp;A, Skyrim
dialogue, English-Taiwanese translation, and Economics articles. The results
underscore the efficiency and effectiveness of contextual pruning, not merely
as a theoretical concept but as a practical tool in developing domain-specific,
resource-efficient LLMs. Contextual pruning is a promising method for building
domain-specific LLMs, and this research is a building block towards future
development with more hardware compute, refined fine-tuning, and quantization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valicenti_T/0/1/0/all/0/1&quot;&gt;Tim Valicenti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidal_J/0/1/0/all/0/1&quot;&gt;Justice Vidal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patnaik_R/0/1/0/all/0/1&quot;&gt;Ritik Patnaik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12705">
<title>Optimizing Distributed Training on Frontier for Large Language Models. (arXiv:2312.12705v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2312.12705</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLM) are showing tremendous success as foundation
models, and many downstream applications benefit from fine-tuning. Prior works
on loss scaling have demonstrated that the larger LLMs perform better than
their smaller counterparts. However, training LLMs with billions of parameters
requires considerable computational resources; to train a one trillion
GPT-style model on 20 trillion tokens, we need to perform 120 million exaflops.
Frontier is the world&apos;s first and fastest exascale supercomputer for open
science and is equipped with 75264 MI250X GPUs. This work explores efficient
distributed strategies such as tensor parallelism, pipeline parallelism, and
sharded data parallelism to train a trillion-parameter model on the Frontier
exascale supercomputer. We analyze these distributed training techniques and
associated parameters individually to decide which techniques to use and what
associated parameters to select for a particular technique. We perform
hyperparameter tuning on these techniques to understand their complex
interplay. Combined with these two tuning efforts, we have found optimal
strategies to train three models of size 22B, 175B, and 1T parameters with
$38.38\%$ , $36.14\%$ , and $31.96\%$ achieved throughput. For training the
175B parameter model and 1T model, we have achieved $100\%$ weak scaling
efficiency and $89\%$ and $87\%$ strong scaling efficiency, respectively. Our
work presents a set of strategies for distributed training of LLMs through
experimental findings and hyperparameter tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1&quot;&gt;Sajal Dash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyngaas_I/0/1/0/all/0/1&quot;&gt;Isaac Lyngaas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Junqi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Egele_R/0/1/0/all/0/1&quot;&gt;Romain Egele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_G/0/1/0/all/0/1&quot;&gt;Guojing Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feiyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaprakash_P/0/1/0/all/0/1&quot;&gt;Prasanna Balaprakash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12713">
<title>Response Enhanced Semi-Supervised Dialogue Query Generation. (arXiv:2312.12713v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12713</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging vast and continually updated knowledge from the Internet has been
considered an important ability for a dialogue system. Therefore, the dialogue
query generation task is proposed for generating search queries from dialogue
histories, which will be submitted to a search engine for retrieving relevant
websites on the Internet. In this regard, previous efforts were devoted to
collecting conversations with annotated queries and training a query producer
(QP) via standard supervised learning. However, these studies still face the
challenges of data scarcity and domain adaptation. To address these issues, in
this paper, we propose a semi-supervised learning framework -- SemiDQG, to
improve model performance with unlabeled conversations. Based on the
observation that the search query is typically related to the topic of dialogue
response, we train a response-augmented query producer (RA) to provide rich and
effective training signals for QP. We first apply a similarity-based query
selection strategy to select high-quality RA-generated pseudo queries, which
are used to construct pseudo instances for training QP and RA. Then, we adopt
the REINFORCE algorithm to further enhance QP, with RA-provided rewards as
fine-grained training signals. Experimental results and in-depth analysis of
three benchmarks show the effectiveness of our framework in cross-domain and
low-resource scenarios. Particularly, SemiDQG significantly surpasses ChatGPT
and competitive baselines. Our code is available at
\url{https://github.com/DeepLearnXMU/SemiDQG}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Ante Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Linfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linfeng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jinsong Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12724">
<title>Progressive Poisoned Data Isolation for Training-time Backdoor Defense. (arXiv:2312.12724v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.12724</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNN) are susceptible to backdoor attacks where
malicious attackers manipulate the model&apos;s predictions via data poisoning. It
is hence imperative to develop a strategy for training a clean model using a
potentially poisoned dataset. Previous training-time defense mechanisms
typically employ an one-time isolation process, often leading to suboptimal
isolation outcomes. In this study, we present a novel and efficacious defense
method, termed Progressive Isolation of Poisoned Data (PIPD), that
progressively isolates poisoned data to enhance the isolation accuracy and
mitigate the risk of benign samples being misclassified as poisoned ones. Once
the poisoned portion of the dataset has been identified, we introduce a
selective training process to train a clean model. Through the implementation
of these techniques, we ensure that the trained model manifests a significantly
diminished attack success rate against the poisoned data. Extensive experiments
on multiple benchmark datasets and DNN models, assessed against nine
state-of-the-art backdoor attacks, demonstrate the superior performance of our
PIPD method for backdoor defense. For instance, our PIPD achieves an average
True Positive Rate (TPR) of 99.95% and an average False Positive Rate (FPR) of
0.06% for diverse attacks over CIFAR-10 dataset, markedly surpassing the
performance of state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haiwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12728">
<title>Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy. (arXiv:2312.12728v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.12728</link>
<description rdf:parseType="Literal">&lt;p&gt;As Large Language Models (LLMs) have made significant advancements across
various tasks, such as question answering, translation, text summarization, and
dialogue systems, the need for accuracy in information becomes crucial,
especially for serious financial products serving billions of users like
Alipay. To address this, Alipay has developed a Retrieval-Augmented Generation
(RAG) system that grounds LLMs on the most accurate and up-to-date information.
However, for a real-world product serving millions of users, the inference
speed of LLMs becomes a critical factor compared to a mere experimental model.
&lt;/p&gt;
&lt;p&gt;Hence, this paper presents a generic framework for accelerating the inference
process, resulting in a substantial increase in speed and cost reduction for
our RAG system, with lossless generation accuracy. In the traditional inference
process, each token is generated sequentially by the LLM, leading to a time
consumption proportional to the number of generated tokens. To enhance this
process, our framework, named \textit{lookahead}, introduces a
\textit{multi-branch} strategy. Instead of generating a single token at a time,
we propose a \textit{Trie-based Retrieval} (TR) process that enables the
generation of multiple branches simultaneously, each of which is a sequence of
tokens. Subsequently, for each branch, a \textit{Verification and Accept} (VA)
process is performed to identify the longest correct sub-sequence as the final
output. Our strategy offers two distinct advantages: (1) it guarantees absolute
correctness of the output, avoiding any approximation algorithms, and (2) the
worst-case performance of our approach is equivalent to the conventional
process. We conduct extensive experiments to demonstrate the significant
improvements achieved by applying our inference acceleration framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhitian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_C/0/1/0/all/0/1&quot;&gt;Chenyi Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12731">
<title>Robustly Improving Bandit Algorithms with Confounded and Selection Biased Offline Data: A Causal Approach. (arXiv:2312.12731v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12731</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies bandit problems where an agent has access to offline data
that might be utilized to potentially improve the estimation of each arm&apos;s
reward distribution. A major obstacle in this setting is the existence of
compound biases from the observational data. Ignoring these biases and blindly
fitting a model with the biased data could even negatively affect the online
learning phase. In this work, we formulate this problem from a causal
perspective. First, we categorize the biases into confounding bias and
selection bias based on the causal structure they imply. Next, we extract the
causal bound for each arm that is robust towards compound biases from biased
observational data. The derived bounds contain the ground truth mean reward and
can effectively guide the bandit agent to learn a nearly-optimal decision
policy. We also conduct regret analysis in both contextual and non-contextual
bandit settings and show that prior causal bounds could help consistently
reduce the asymptotic regret.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12747">
<title>ALMANACS: A Simulatability Benchmark for Language Model Explainability. (arXiv:2312.12747v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12747</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we measure the efficacy of language model explainability methods?
While many explainability methods have been developed, they are typically
evaluated on bespoke tasks, preventing an apples-to-apples comparison. To help
fill this gap, we present ALMANACS, a language model explainability benchmark.
ALMANACS scores explainability methods on simulatability, i.e., how well the
explanations improve behavior prediction on new inputs. The ALMANACS scenarios
span twelve safety-relevant topics such as ethical reasoning and advanced AI
behaviors; they have idiosyncratic premises to invoke model-specific behavior;
and they have a train-test distributional shift to encourage faithful
explanations. By using another language model to predict behavior based on the
explanations, ALMANACS is a fully automated benchmark. We use ALMANACS to
evaluate counterfactuals, rationalizations, attention, and Integrated Gradients
explanations. Our results are sobering: when averaged across all topics, no
explanation method outperforms the explanation-free control. We conclude that
despite modest successes in prior work, developing an explanation method that
aids simulatability in ALMANACS remains an open challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mills_E/0/1/0/all/0/1&quot;&gt;Edmund Mills&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shiye Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1&quot;&gt;Stuart Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emmons_S/0/1/0/all/0/1&quot;&gt;Scott Emmons&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12750">
<title>Parallel Ranking of Ads and Creatives in Real-Time Advertising Systems. (arXiv:2312.12750v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.12750</link>
<description rdf:parseType="Literal">&lt;p&gt;&quot;Creativity is the heart and soul of advertising services&quot;. Effective
creatives can create a win-win scenario: advertisers can reach target users and
achieve marketing objectives more effectively, users can more quickly find
products of interest, and platforms can generate more advertising revenue. With
the advent of AI-Generated Content, advertisers now can produce vast amounts of
creative content at a minimal cost. The current challenge lies in how
advertising systems can select the most pertinent creative in real-time for
each user personally. Existing methods typically perform serial ranking of ads
or creatives, limiting the creative module in terms of both effectiveness and
efficiency. In this paper, we propose for the first time a novel architecture
for online parallel estimation of ads and creatives ranking, as well as the
corresponding offline joint optimization model. The online architecture enables
sophisticated personalized creative modeling while reducing overall latency.
The offline joint model for CTR estimation allows mutual awareness and
collaborative optimization between ads and creatives. Additionally, we optimize
the offline evaluation metrics for the implicit feedback sorting task involved
in ad creative ranking. We conduct extensive experiments to compare ours with
two state-of-the-art approaches. The results demonstrate the effectiveness of
our approach in both offline evaluations and real-world advertising platforms
online in terms of response time, CTR, and CPM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhiguang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chun Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sang_L/0/1/0/all/0/1&quot;&gt;Liufang Sang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jie He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Changping Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhangang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jingping Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12751">
<title>Human-Centred Learning Analytics and AI in Education: a Systematic Literature Review. (arXiv:2312.12751v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.12751</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid expansion of Learning Analytics (LA) and Artificial Intelligence in
Education (AIED) offers new scalable, data-intensive systems but also raises
concerns about data privacy and agency. Excluding stakeholders -- like students
and teachers -- from the design process can potentially lead to mistrust and
inadequately aligned tools. Despite a shift towards human-centred design in
recent LA and AIED research, there remain gaps in our understanding of the
importance of human control, safety, reliability, and trustworthiness in the
design and implementation of these systems. We conducted a systematic
literature review to explore these concerns and gaps. We analysed 108 papers to
provide insights about i) the current state of human-centred LA/AIED research;
ii) the extent to which educational stakeholders have contributed to the design
process of human-centred LA/AIED systems; iii) the current balance between
human control and computer automation of such systems; and iv) the extent to
which safety, reliability and trustworthiness have been considered in the
literature. Results indicate some consideration of human control in LA/AIED
system design, but limited end-user involvement in actual design. Based on
these findings, we recommend: 1) carefully balancing stakeholders&apos; involvement
in designing and deploying LA/AIED systems throughout all design phases, 2)
actively involving target end-users, especially students, to delineate the
balance between human control and automation, and 3) exploring safety,
reliability, and trustworthiness as principles in future human-centred LA/AIED
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alfredo_R/0/1/0/all/0/1&quot;&gt;Riordan Alfredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echeverria_V/0/1/0/all/0/1&quot;&gt;Vanessa Echeverria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yueqiao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Lixiang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swiecki_Z/0/1/0/all/0/1&quot;&gt;Zachari Swiecki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1&quot;&gt;Dragan Ga&amp;#x161;evi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Maldonado_R/0/1/0/all/0/1&quot;&gt;Roberto Martinez-Maldonado&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12772">
<title>Realistic Rainy Weather Simulation for LiDARs in CARLA Simulator. (arXiv:2312.12772v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.12772</link>
<description rdf:parseType="Literal">&lt;p&gt;Employing data augmentation methods to enhance perception performance in
adverse weather has attracted considerable attention recently. Most of the
LiDAR augmentation methods post-process the existing dataset by physics-based
models or machine-learning methods. However, due to the limited environmental
annotations and the fixed vehicle trajectories in the existing dataset, it is
challenging to edit the scene and expand the diversity of traffic flow and
scenario. To this end, we propose a simulator-based physical modeling approach
to augment LiDAR data in rainy weather in order to improve the perception
performance of LiDAR in this scenario. We complete the modeling task of the
rainy weather in the CARLA simulator and establish a pipeline for LiDAR data
collection. In particular, we pay special attention to the spray and splash
rolled up by the wheels of surrounding vehicles in rain and complete the
simulation of this special scenario through the Spray Emitter method we
developed. In addition, we examine the influence of different weather
conditions on the intensity of the LiDAR echo, develop a prediction network for
the intensity of the LiDAR echo, and complete the simulation of 4-feat LiDAR
point cloud data. In the experiment, we observe that the model augmented by the
synthetic data improves the object detection task&apos;s performance in the rainy
sequence of the Waymo Open Dataset. Both the code and the dataset will be made
publicly available at https://github.com/PJLab-ADG/PCSim#rainypcsim.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Donglin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenfeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wentao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_G/0/1/0/all/0/1&quot;&gt;Guohang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xinyu Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12778">
<title>Collaborative business intelligence virtual assistant. (arXiv:2312.12778v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.12778</link>
<description rdf:parseType="Literal">&lt;p&gt;The present-day business landscape necessitates novel methodologies that
integrate intelligent technologies and tools capable of swiftly providing
precise and dependable information for decision-making purposes. Contemporary
society is characterized by vast amounts of accumulated data across various
domains, which hold considerable potential for informing and guiding
decision-making processes. However, these data are typically collected and
stored by disparate and unrelated software systems, stored in diverse formats,
and offer varying levels of accessibility and security. To address the
challenges associated with processing such large volumes of data, organizations
often rely on data analysts. Nonetheless, a significant hurdle in harnessing
the benefits of accumulated data lies in the lack of direct communication
between technical specialists, decision-makers, and business process analysts.
To overcome this issue, the application of collaborative business intelligence
(CBI) emerges as a viable solution. This research focuses on the applications
of data mining and aims to model CBI processes within distributed virtual teams
through the interaction of users and a CBI Virtual Assistant. The proposed
virtual assistant for CBI endeavors to enhance data exploration accessibility
for a wider range of users and streamline the time and effort required for data
analysis. The key contributions of this study encompass: 1) a reference model
representing collaborative BI, inspired by linguistic theory; 2) an approach
that enables the transformation of user queries into executable commands,
thereby facilitating their utilization within data exploration software; and 3)
the primary workflow of a conversational agent designed for data analytics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cherednichenko_O/0/1/0/all/0/1&quot;&gt;Olga Cherednichenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muhammad_F/0/1/0/all/0/1&quot;&gt;Fahad Muhammad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12781">
<title>DynaLay: An Introspective Approach to Dynamic Layer Selection for Deep Networks. (arXiv:2312.12781v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12781</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have become increasingly computationally intensive,
requiring extensive computational resources and time for both training and
inference. A significant contributing factor to this challenge is the uniform
computational effort expended on each input example, regardless of its
complexity. We introduce \textbf{DynaLay}, an alternative architecture that
features a decision-making agent to adaptively select the most suitable layers
for processing each input, thereby endowing the model with a remarkable level
of introspection. DynaLay reevaluates more complex inputs during inference,
adjusting the computational effort to optimize both performance and efficiency.
The core of the system is a main model equipped with Fixed-Point Iterative
(FPI) layers, capable of accurately approximating complex functions, paired
with an agent that chooses these layers or a direct action based on the
introspection of the models inner state. The model invests more time in
processing harder examples, while minimal computation is required for easier
ones. This introspective approach is a step toward developing deep learning
models that &quot;think&quot; and &quot;ponder&quot;, rather than &quot;ballistically&apos;&apos; produce answers.
Our experiments demonstrate that DynaLay achieves accuracy comparable to
conventional deep models while significantly reducing computational demands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathur_M/0/1/0/all/0/1&quot;&gt;Mrinal Mathur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1&quot;&gt;Sergey Plis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12783">
<title>Stable Distillation: Regularizing Continued Pre-training for Low-Resource Automatic Speech Recognition. (arXiv:2312.12783v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.12783</link>
<description rdf:parseType="Literal">&lt;p&gt;Continued self-supervised (SSL) pre-training for adapting existing SSL models
to the target domain has shown to be extremely effective for low-resource
Automatic Speech Recognition (ASR). This paper proposes Stable Distillation, a
simple and novel approach for SSL-based continued pre-training that boosts ASR
performance in the target domain where both labeled and unlabeled data are
limited. Stable Distillation employs self-distillation as regularization for
continued pre-training, alleviating the over-fitting issue, a common problem
continued pre-training faces when the source and target domains differ.
Specifically, first, we perform vanilla continued pre-training on an initial
SSL pre-trained model on the target domain ASR dataset and call it the teacher.
Next, we take the same initial pre-trained model as a student to perform
continued pre-training while enforcing its hidden representations to be close
to that of the teacher (via MSE loss). This student is then used for downstream
ASR fine-tuning on the target dataset. In practice, Stable Distillation
outperforms all our baselines by 0.8 - 7 WER when evaluated in various
experimental settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1&quot;&gt;Ashish Seth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sreyan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1&quot;&gt;S. Umesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12791">
<title>Model-Based Control with Sparse Neural Dynamics. (arXiv:2312.12791v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.12791</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning predictive models from observations using deep neural networks
(DNNs) is a promising new approach to many real-world planning and control
problems. However, common DNNs are too unstructured for effective planning, and
current control methods typically rely on extensive sampling or local gradient
descent. In this paper, we propose a new framework for integrated model
learning and predictive control that is amenable to efficient optimization
algorithms. Specifically, we start with a ReLU neural model of the system
dynamics and, with minimal losses in prediction accuracy, we gradually sparsify
it by removing redundant neurons. This discrete sparsification process is
approximated as a continuous problem, enabling an end-to-end optimization of
both the model architecture and the weight parameters. The sparsified model is
subsequently used by a mixed-integer predictive controller, which represents
the neuron activations as binary variables and employs efficient
branch-and-bound algorithms. Our framework is applicable to a wide variety of
DNNs, from simple multilayer perceptrons to complex graph neural dynamics. It
can efficiently handle tasks involving complicated contact dynamics, such as
object pushing, compositional object sorting, and manipulation of deformable
objects. Numerical and hardware experiments show that, despite the aggressive
sparsification, our framework can deliver better closed-loop performance than
existing state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Genggeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jeff He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcucci_T/0/1/0/all/0/1&quot;&gt;Tobia Marcucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunzhu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12806">
<title>MedBench: A Large-Scale Chinese Benchmark for Evaluating Medical Large Language Models. (arXiv:2312.12806v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12806</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of various medical large language models (LLMs) in the medical
domain has highlighted the need for unified evaluation standards, as manual
evaluation of LLMs proves to be time-consuming and labor-intensive. To address
this issue, we introduce MedBench, a comprehensive benchmark for the Chinese
medical domain, comprising 40,041 questions sourced from authentic examination
exercises and medical reports of diverse branches of medicine. In particular,
this benchmark is composed of four key components: the Chinese Medical
Licensing Examination, the Resident Standardization Training Examination, the
Doctor In-Charge Qualification Examination, and real-world clinic cases
encompassing examinations, diagnoses, and treatments. MedBench replicates the
educational progression and clinical practice experiences of doctors in
Mainland China, thereby establishing itself as a credible benchmark for
assessing the mastery of knowledge and reasoning abilities in medical language
learning models. We perform extensive experiments and conduct an in-depth
analysis from diverse perspectives, which culminate in the following findings:
(1) Chinese medical LLMs underperform on this benchmark, highlighting the need
for significant advances in clinical knowledge and diagnostic precision. (2)
Several general-domain LLMs surprisingly possess considerable medical
knowledge. These findings elucidate both the capabilities and limitations of
LLMs within the context of MedBench, with the ultimate goal of aiding the
medical research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linlin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melo_G/0/1/0/all/0/1&quot;&gt;Gerard de Melo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12807">
<title>All but One: Surgical Concept Erasing with Model Preservation in Text-to-Image Diffusion Models. (arXiv:2312.12807v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12807</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-Image models such as Stable Diffusion have shown impressive image
generation synthesis, thanks to the utilization of large-scale datasets.
However, these datasets may contain sexually explicit, copyrighted, or
undesirable content, which allows the model to directly generate them. Given
that retraining these large models on individual concept deletion requests is
infeasible, fine-tuning algorithms have been developed to tackle concept
erasing in diffusion models. While these algorithms yield good concept erasure,
they all present one of the following issues: 1) the corrupted feature space
yields synthesis of disintegrated objects, 2) the initially synthesized content
undergoes a divergence in both spatial structure and semantics in the generated
images, and 3) sub-optimal training updates heighten the model&apos;s susceptibility
to utility harm. These issues severely degrade the original utility of
generative models. In this work, we present a new approach that solves all of
these challenges. We take inspiration from the concept of classifier guidance
and propose a surgical update on the classifier guidance term while
constraining the drift of the unconditional score term. Furthermore, our
algorithm empowers the user to select an alternative to the erasing concept,
allowing for more controllability. Our experimental results show that our
algorithm not only erases the target concept effectively but also preserves the
model&apos;s generation capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Seunghoo Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juhun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Simon S. Woo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12815">
<title>OCTOPUS: Open-vocabulary Content Tracking and Object Placement Using Semantic Understanding in Mixed Reality. (arXiv:2312.12815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12815</link>
<description rdf:parseType="Literal">&lt;p&gt;One key challenge in augmented reality is the placement of virtual content in
natural locations. Existing automated techniques are only able to work with a
closed-vocabulary, fixed set of objects. In this paper, we introduce a new
open-vocabulary method for object placement. Our eight-stage pipeline leverages
recent advances in segmentation models, vision-language models, and LLMs to
place any virtual object in any AR camera frame or scene. In a preliminary user
study, we show that our method performs at least as well as human experts 57%
of the time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoffe_L/0/1/0/all/0/1&quot;&gt;Luke Yoffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Aditya Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollerer_T/0/1/0/all/0/1&quot;&gt;Tobias H&amp;#xf6;llerer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12828">
<title>TagCLIP: A Local-to-Global Framework to Enhance Open-Vocabulary Multi-Label Classification of CLIP Without Training. (arXiv:2312.12828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12828</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) has demonstrated impressive
capabilities in open-vocabulary classification. The class token in the image
encoder is trained to capture the global features to distinguish different text
descriptions supervised by contrastive loss, making it highly effective for
single-label classification. However, it shows poor performance on multi-label
datasets because the global feature tends to be dominated by the most prominent
class and the contrastive nature of softmax operation aggravates it. In this
study, we observe that the multi-label classification results heavily rely on
discriminative local features but are overlooked by CLIP. As a result, we
dissect the preservation of patch-wise spatial information in CLIP and proposed
a local-to-global framework to obtain image tags. It comprises three steps: (1)
patch-level classification to obtain coarse scores; (2) dual-masking attention
refinement (DMAR) module to refine the coarse scores; (3) class-wise
reidentification (CWR) module to remedy predictions from a global perspective.
This framework is solely based on frozen CLIP and significantly enhances its
multi-label classification performance on various benchmarks without
dataset-specific training. Besides, to comprehensively assess the quality and
practicality of generated tags, we extend their application to the downstream
task, i.e., weakly supervised semantic segmentation (WSSS) with generated tags
as image-level pseudo labels. Experiments demonstrate that this
classify-then-segment paradigm dramatically outperforms other annotation-free
segmentation methods and validates the effectiveness of generated tags. Our
code is available at https://github.com/linyq2117/TagCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hengjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_D/0/1/0/all/0/1&quot;&gt;Dongqin Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Binbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haifeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12832">
<title>Turning Dust into Gold: Distilling Complex Reasoning Capabilities from LLMs by Leveraging Negative Data. (arXiv:2312.12832v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12832</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have performed well on various reasoning tasks,
but their inaccessibility and numerous parameters hinder wide application in
practice. One promising way is distilling the reasoning ability from LLMs to
small models by the generated chain-of-thought reasoning paths. In some cases,
however, LLMs may produce incorrect reasoning chains, especially when facing
complex mathematical problems. Previous studies only transfer knowledge from
positive samples and drop the synthesized data with wrong answers. In this
work, we illustrate the merit of negative data and propose a model
specialization framework to distill LLMs with negative samples besides positive
ones. The framework consists of three progressive steps, covering from training
to inference stages, to absorb knowledge from negative data. We conduct
extensive experiments across arithmetic reasoning tasks to demonstrate the role
of negative data in distillation from LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_P/0/1/0/all/0/1&quot;&gt;Peiwen Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shaoxiong Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1&quot;&gt;Boyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinglin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heda Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kan Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12844">
<title>Causal Discovery under Identifiable Heteroscedastic Noise Model. (arXiv:2312.12844v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12844</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing the underlying structural causal relations represented by Directed
Acyclic Graphs (DAGs) has been a fundamental task in various AI disciplines.
Causal DAG learning via the continuous optimization framework has recently
achieved promising performance in terms of both accuracy and efficiency.
However, most methods make strong assumptions of homoscedastic noise, i.e.,
exogenous noises have equal variances across variables, observations, or even
both. The noises in real data usually violate both assumptions due to the
biases introduced by different data collection processes. To address the issue
of heteroscedastic noise, we introduce relaxed and implementable sufficient
conditions, proving the identifiability of a general class of SEM subject to
these conditions. Based on the identifiable general SEM, we propose a novel
formulation for DAG learning that accounts for the variation in noise variance
across variables and observations. We then propose an effective two-phase
iterative DAG learning algorithm to address the increasing optimization
difficulties and to learn a causal DAG from data with heteroscedastic variable
noise under varying variance. We show significant empirical gains of the
proposed approaches over state-of-the-art methods on both synthetic data and
real data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_N/0/1/0/all/0/1&quot;&gt;Naiyu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_T/0/1/0/all/0/1&quot;&gt;Tian Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yue Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1&quot;&gt;Qiang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12852">
<title>Language Resources for Dutch Large Language Modelling. (arXiv:2312.12852v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.12852</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the rapid expansion of types of large language models, there remains
a notable gap in models specifically designed for the Dutch language. This gap
is not only a shortage in terms of pretrained Dutch models but also in terms of
data, and benchmarks and leaderboards. This work provides a small step to
improve the situation. First, we introduce two fine-tuned variants of the Llama
2 13B model. We first fine-tuned Llama 2 using Dutch-specific web-crawled data
and subsequently refined this model further on multiple synthetic instruction
and chat datasets. These datasets as well as the model weights are made
available. In addition, we provide a leaderboard to keep track of the
performance of (Dutch) models on a number of generation tasks, and we include
results of a number of state-of-the-art models, including our own. Finally we
provide a critical conclusion on what we believe is needed to push forward
Dutch language models and the whole eco-system around the models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanroy_B/0/1/0/all/0/1&quot;&gt;Bram Vanroy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12856">
<title>SkyScript: A Large and Semantically Diverse Vision-Language Dataset for Remote Sensing. (arXiv:2312.12856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12856</link>
<description rdf:parseType="Literal">&lt;p&gt;Remote sensing imagery, despite its broad applications in helping achieve
Sustainable Development Goals and tackle climate change, has not yet benefited
from the recent advancements of versatile, task-agnostic vision language models
(VLMs). A key reason is that the large-scale, semantically diverse image-text
dataset required for developing VLMs is still absent for remote sensing images.
Unlike natural images, remote sensing images and their associated text
descriptions cannot be efficiently collected from the public Internet at scale.
In this work, we bridge this gap by using geo-coordinates to automatically
connect open, unlabeled remote sensing images with rich semantics covered in
OpenStreetMap, and thus construct SkyScript, a comprehensive vision-language
dataset for remote sensing images, comprising 2.6 million image-text pairs
covering 29K distinct semantic tags. With continual pre-training on this
dataset, we obtain a VLM that surpasses baseline models with a 6.2% average
accuracy gain in zero-shot scene classification across seven benchmark
datasets. It also demonstrates the ability of zero-shot transfer for
fine-grained object attribute classification and cross-modal retrieval. We hope
this dataset can support the advancement of VLMs for various multi-modal tasks
in remote sensing, such as open-vocabulary classification, retrieval,
captioning, and text-to-image synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhecheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabha_R/0/1/0/all/0/1&quot;&gt;Rajanie Prabha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tianyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajagopal_R/0/1/0/all/0/1&quot;&gt;Ram Rajagopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12865">
<title>RadEdit: stress-testing biomedical vision models via diffusion image editing. (arXiv:2312.12865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12865</link>
<description rdf:parseType="Literal">&lt;p&gt;Biomedical imaging datasets are often small and biased, meaning that
real-world performance of predictive models can be substantially lower than
expected from internal testing. This work proposes using generative image
editing to simulate dataset shifts and diagnose failure modes of biomedical
vision models; this can be used in advance of deployment to assess readiness,
potentially reducing cost and patient harm. Existing editing methods can
produce undesirable changes, with spurious correlations learned due to the
co-occurrence of disease and treatment interventions, limiting practical
applicability. To address this, we train a text-to-image diffusion model on
multiple chest X-ray datasets and introduce a new editing method RadEdit that
uses multiple masks, if present, to constrain changes and ensure consistency in
the edited images. We consider three types of dataset shifts: acquisition
shift, manifestation shift, and population shift, and demonstrate that our
approach can diagnose failures and quantify model robustness without additional
data collection, complementing more qualitative tools for explainable AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1&quot;&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bond_Taylor_S/0/1/0/all/0/1&quot;&gt;Sam Bond-Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_P/0/1/0/all/0/1&quot;&gt;Pedro P. Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breugel_B/0/1/0/all/0/1&quot;&gt;Boris van Breugel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1&quot;&gt;Daniel C. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Harshita Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatelli_V/0/1/0/all/0/1&quot;&gt;Valentina Salvatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1&quot;&gt;Maria T. A. Wetscherek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Richardson_H/0/1/0/all/0/1&quot;&gt;Hannah Richardson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nori_A/0/1/0/all/0/1&quot;&gt;Aditya Nori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1&quot;&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1&quot;&gt;Ozan Oktay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilse_M/0/1/0/all/0/1&quot;&gt;Maximilian Ilse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12868">
<title>Towards Machines that Trust: AI Agents Learn to Trust in the Trust Game. (arXiv:2312.12868v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.12868</link>
<description rdf:parseType="Literal">&lt;p&gt;Widely considered a cornerstone of human morality, trust shapes many aspects
of human social interactions. In this work, we present a theoretical analysis
of the $\textit{trust game}$, the canonical task for studying trust in
behavioral and brain sciences, along with simulation results supporting our
analysis. Specifically, leveraging reinforcement learning (RL) to train our AI
agents, we systematically investigate learning trust under various
parameterizations of this task. Our theoretical analysis, corroborated by the
simulations results presented, provides a mathematical basis for the emergence
of trust in the trust game.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nobandegani_A/0/1/0/all/0/1&quot;&gt;Ardavan S. Nobandegani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shultz_T/0/1/0/all/0/1&quot;&gt;Thomas R. Shultz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12869">
<title>Parameterized Projected Bellman Operator. (arXiv:2312.12869v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12869</link>
<description rdf:parseType="Literal">&lt;p&gt;Approximate value iteration~(AVI) is a family of algorithms for reinforcement
learning~(RL) that aims to obtain an approximation of the optimal value
function. Generally, AVI algorithms implement an iterated procedure where each
step consists of (i) an application of the Bellman operator and (ii) a
projection step into a considered function space. Notoriously, the Bellman
operator leverages transition samples, which strongly determine its behavior,
as uninformative samples can result in negligible updates or long detours,
whose detrimental effects are further exacerbated by the computationally
intensive projection step. To address these issues, we propose a novel
alternative approach based on learning an approximate version of the Bellman
operator rather than estimating it through samples as in AVI approaches. This
way, we are able to (i) generalize across transition samples and (ii) avoid the
computationally intensive projection step. For this reason, we call our novel
operator projected Bellman operator (PBO). We formulate an optimization problem
to learn PBO for generic sequential decision-making problems, and we
theoretically analyze its properties in two representative classes of RL
problems. Furthermore, we theoretically study our approach under the lens of
AVI and devise algorithmic implementations to learn PBO in offline and online
settings by leveraging neural network parameterizations. Finally, we
empirically showcase the benefits of PBO w.r.t. the regular Bellman operator on
several RL problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;o Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metelli_A/0/1/0/all/0/1&quot;&gt;Alberto Maria Metelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belousov_B/0/1/0/all/0/1&quot;&gt;Boris Belousov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Restelli_M/0/1/0/all/0/1&quot;&gt;Marcello Restelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DEramo_C/0/1/0/all/0/1&quot;&gt;Carlo D&amp;#x27;Eramo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12872">
<title>Integration and Performance Analysis of Artificial Intelligence and Computer Vision Based on Deep Learning Algorithms. (arXiv:2312.12872v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12872</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on the analysis of the application effectiveness of the
integration of deep learning and computer vision technologies. Deep learning
achieves a historic breakthrough by constructing hierarchical neural networks,
enabling end-to-end feature learning and semantic understanding of images. The
successful experiences in the field of computer vision provide strong support
for training deep learning algorithms. The tight integration of these two
fields has given rise to a new generation of advanced computer vision systems,
significantly surpassing traditional methods in tasks such as machine vision
image classification and object detection. In this paper, typical image
classification cases are combined to analyze the superior performance of deep
neural network models while also pointing out their limitations in
generalization and interpretability, proposing directions for future
improvements. Overall, the efficient integration and development trend of deep
learning with massive visual data will continue to drive technological
breakthroughs and application expansion in the field of computer vision, making
it possible to build truly intelligent machine vision systems. This deepening
fusion paradigm will powerfully promote unprecedented tasks and functions in
computer vision, providing stronger development momentum for related
disciplines and industries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Liqiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_C/0/1/0/all/0/1&quot;&gt;Chang Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qunwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12882">
<title>BSL: Understanding and Improving Softmax Loss for Recommendation. (arXiv:2312.12882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12882</link>
<description rdf:parseType="Literal">&lt;p&gt;Loss functions steer the optimization direction of recommendation models and
are critical to model performance, but have received relatively little
attention in recent recommendation research. Among various losses, we find
Softmax loss (SL) stands out for not only achieving remarkable accuracy but
also better robustness and fairness. Nevertheless, the current literature lacks
a comprehensive explanation for the efficacy of SL. Toward addressing this
research gap, we conduct theoretical analyses on SL and uncover three insights:
1) Optimizing SL is equivalent to performing Distributionally Robust
Optimization (DRO) on the negative data, thereby learning against perturbations
on the negative distribution and yielding robustness to noisy negatives. 2)
Comparing with other loss functions, SL implicitly penalizes the prediction
variance, resulting in a smaller gap between predicted values and and thus
producing fairer results. Building on these insights, we further propose a
novel loss function Bilateral SoftMax Loss (BSL) that extends the advantage of
SL to both positive and negative sides. BSL augments SL by applying the same
Log-Expectation-Exp structure to positive examples as is used for negatives,
making the model robust to the noisy positives as well. Remarkably, BSL is
simple and easy-to-implement -- requiring just one additional line of code
compared to SL. Experiments on four real-world datasets and three
representative backbones demonstrate the effectiveness of our proposal. The
code is available at https://github.com/junkangwu/BSL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Junkang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiancan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wentao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jizhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12891">
<title>MinePlanner: A Benchmark for Long-Horizon Planning in Large Minecraft Worlds. (arXiv:2312.12891v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.12891</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new benchmark for planning tasks based on the Minecraft game.
Our benchmark contains 45 tasks overall, but also provides support for creating
both propositional and numeric instances of new Minecraft tasks automatically.
We benchmark numeric and propositional planning systems on these tasks, with
results demonstrating that state-of-the-art planners are currently incapable of
dealing with many of the challenges advanced by our new benchmark, such as
scaling to instances with thousands of objects. Based on these results, we
identify areas of improvement for future planners. Our framework is made
available at https://github.com/IretonLiu/mine-pddl/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hill_W/0/1/0/all/0/1&quot;&gt;William Hill&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_I/0/1/0/all/0/1&quot;&gt;Ireton Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_A/0/1/0/all/0/1&quot;&gt;Anita De Mello Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harvey_D/0/1/0/all/0/1&quot;&gt;Damion Harvey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konidaris_G/0/1/0/all/0/1&quot;&gt;George Konidaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+James_S/0/1/0/all/0/1&quot;&gt;Steven James&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12904">
<title>PGN: A perturbation generation network against deep reinforcement learning. (arXiv:2312.12904v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.12904</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning has advanced greatly and applied in many areas.
In this paper, we explore the vulnerability of deep reinforcement learning by
proposing a novel generative model for creating effective adversarial examples
to attack the agent. Our proposed model can achieve both targeted attacks and
untargeted attacks. Considering the specificity of deep reinforcement learning,
we propose the action consistency ratio as a measure of stealthiness, and a new
measurement index of effectiveness and stealthiness. Experiment results show
that our method can ensure the effectiveness and stealthiness of attack
compared with other algorithms. Moreover, our methods are considerably faster
and thus can achieve rapid and efficient verification of the vulnerability of
deep reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangjuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feifan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Quan Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12917">
<title>Sign Language Production with Latent Motion Transformer. (arXiv:2312.12917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.12917</link>
<description rdf:parseType="Literal">&lt;p&gt;Sign Language Production (SLP) is the tough task of turning sign language
into sign videos. The main goal of SLP is to create these videos using a sign
gloss. In this research, we&apos;ve developed a new method to make high-quality sign
videos without using human poses as a middle step. Our model works in two main
parts: first, it learns from a generator and the video&apos;s hidden features, and
next, it uses another model to understand the order of these hidden features.
To make this method even better for sign videos, we make several significant
improvements. (i) In the first stage, we take an improved 3D VQ-GAN to learn
downsampled latent representations. (ii) In the second stage, we introduce
sequence-to-sequence attention to better leverage conditional information.
(iii) The separated two-stage training discards the realistic visual semantic
of the latent codes in the second stage. To endow the latent sequences semantic
information, we extend the token-level autoregressive latent codes learning
with perceptual loss and reconstruction loss for the prior model with visual
perception. Compared with previous state-of-the-art approaches, our model
performs consistently better on two word-level sign language datasets, i.e.,
WLASL and NMFs-CSL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Taiyi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qipeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12936">
<title>Concept-based Explainable Artificial Intelligence: A Survey. (arXiv:2312.12936v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.12936</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of explainable artificial intelligence emerged in response to the
growing need for more transparent and reliable models. However, using raw
features to provide explanations has been disputed in several works lately,
advocating for more user-understandable explanations. To address this issue, a
wide range of papers proposing Concept-based eXplainable Artificial
Intelligence (C-XAI) methods have arisen in recent years. Nevertheless, a
unified categorization and precise field definition are still missing. This
paper fills the gap by offering a thorough review of C-XAI approaches. We
define and identify different concepts and explanation types. We provide a
taxonomy identifying nine categories and propose guidelines for selecting a
suitable category based on the development context. Additionally, we report
common evaluation strategies including metrics, human evaluations and dataset
employed, aiming to assist the development of future methods. We believe this
survey will serve researchers, practitioners, and domain experts in
comprehending and advancing this innovative field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poeta_E/0/1/0/all/0/1&quot;&gt;Eleonora Poeta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciravegna_G/0/1/0/all/0/1&quot;&gt;Gabriele Ciravegna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pastor_E/0/1/0/all/0/1&quot;&gt;Eliana Pastor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerquitelli_T/0/1/0/all/0/1&quot;&gt;Tania Cerquitelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baralis_E/0/1/0/all/0/1&quot;&gt;Elena Baralis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13000">
<title>Accelerator-driven Data Arrangement to Minimize Transformers Run-time on Multi-core Architectures. (arXiv:2312.13000v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2312.13000</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing complexity of transformer models in artificial intelligence
expands their computational costs, memory usage, and energy consumption.
Hardware acceleration tackles the ensuing challenges by designing processors
and accelerators tailored for transformer models, supporting their computation
hotspots with high efficiency. However, memory bandwidth can hinder
improvements in hardware accelerators. Against this backdrop, in this paper we
propose a novel memory arrangement strategy, governed by the hardware
accelerator&apos;s kernel size, which effectively minimizes off-chip data access.
This arrangement is particularly beneficial for end-to-end transformer model
inference, where most of the computation is based on general matrix
multiplication (GEMM) operations. Additionally, we address the overhead of
non-GEMM operations in transformer models within the scope of this memory data
arrangement. Our study explores the implementation and effectiveness of the
proposed accelerator-driven data arrangement approach in both single- and
multi-core systems. Our evaluation demonstrates that our approach can achieve
up to a 2.8x speed increase when executing inferences employing
state-of-the-art transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amirshahi_A/0/1/0/all/0/1&quot;&gt;Alireza Amirshahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ansaloni_G/0/1/0/all/0/1&quot;&gt;Giovanni Ansaloni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atienza_D/0/1/0/all/0/1&quot;&gt;David Atienza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13008">
<title>No More Shortcuts: Realizing the Potential of Temporal Self-Supervision. (arXiv:2312.13008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13008</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised approaches for video have shown impressive results in video
understanding tasks. However, unlike early works that leverage temporal
self-supervision, current state-of-the-art methods primarily rely on tasks from
the image domain (e.g., contrastive learning) that do not explicitly promote
the learning of temporal features. We identify two factors that limit existing
temporal self-supervision: 1) tasks are too simple, resulting in saturated
training performance, and 2) we uncover shortcuts based on local appearance
statistics that hinder the learning of high-level features. To address these
issues, we propose 1) a more challenging reformulation of temporal
self-supervision as frame-level (rather than clip-level) recognition tasks and
2) an effective augmentation strategy to mitigate shortcuts. Our model extends
a representation of single video frames, pre-trained through contrastive
learning, with a transformer that we train through temporal self-supervision.
We demonstrate experimentally that our more challenging frame-level task
formulations and the removal of shortcuts drastically improve the quality of
features learned through temporal self-supervision. The generalization
capability of our self-supervised video method is evidenced by its
state-of-the-art performance in a wide range of high-level semantic tasks,
including video retrieval, action classification, and video attribute
recognition (such as object and scene identification), as well as low-level
temporal correspondence tasks like video object segmentation and pose tracking.
Additionally, we show that the video representations learned through our method
exhibit increased robustness to the input perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_I/0/1/0/all/0/1&quot;&gt;Ishan Rajendrakumar Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenni_S/0/1/0/all/0/1&quot;&gt;Simon Jenni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13026">
<title>FusDom: Combining In-Domain and Out-of-Domain Knowledge for Continuous Self-Supervised Learning. (arXiv:2312.13026v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.13026</link>
<description rdf:parseType="Literal">&lt;p&gt;Continued pre-training (CP) offers multiple advantages, like target domain
adaptation and the potential to exploit the continuous stream of unlabeled data
available online. However, continued pre-training on out-of-domain
distributions often leads to catastrophic forgetting of previously acquired
knowledge, leading to sub-optimal ASR performance. This paper presents FusDom,
a simple and novel methodology for SSL-based continued pre-training. FusDom
learns speech representations that are robust and adaptive yet not forgetful of
concepts seen in the past. Instead of solving the SSL pre-text task on the
output representations of a single model, FusDom leverages two identical
pre-trained SSL models, a teacher and a student, with a modified pre-training
head to solve the CP SSL pre-text task. This head employs a cross-attention
mechanism between the representations of both models while only the student
receives gradient updates and the teacher does not. Finally, the student is
fine-tuned for ASR. In practice, FusDom outperforms all our baselines across
settings significantly, with WER improvements in the range of 0.2 WER - 7.3 WER
in the target domain while retaining the performance in the earlier domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seth_A/0/1/0/all/0/1&quot;&gt;Ashish Seth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Sreyan Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Umesh_S/0/1/0/all/0/1&quot;&gt;S. Umesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13032">
<title>NodeMixup: Tackling Under-Reaching for Graph Neural Networks. (arXiv:2312.13032v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13032</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have become mainstream methods for solving the
semi-supervised node classification problem. However, due to the uneven
location distribution of labeled nodes in the graph, labeled nodes are only
accessible to a small portion of unlabeled nodes, leading to the
\emph{under-reaching} issue. In this study, we firstly reveal under-reaching by
conducting an empirical investigation on various well-known graphs. Then, we
demonstrate that under-reaching results in unsatisfactory distribution
alignment between labeled and unlabeled nodes through systematic experimental
analysis, significantly degrading GNNs&apos; performance. To tackle under-reaching
for GNNs, we propose an architecture-agnostic method dubbed NodeMixup. The
fundamental idea is to (1) increase the reachability of labeled nodes by
labeled-unlabeled pairs mixup, (2) leverage graph structures via fusing the
neighbor connections of intra-class node pairs to improve performance gains of
mixup, and (3) use neighbor label distribution similarity incorporating node
degrees to determine sampling weights for node mixup. Extensive experiments
demonstrate the efficacy of NodeMixup in assisting GNNs in handling
under-reaching. The source code is available at
\url{https://github.com/WeigangLu/NodeMixup}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weigang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Long Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13033">
<title>Explainable artificial intelligence approaches for brain-computer interfaces: a review and design space. (arXiv:2312.13033v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.13033</link>
<description rdf:parseType="Literal">&lt;p&gt;This review paper provides an integrated perspective of Explainable
Artificial Intelligence techniques applied to Brain-Computer Interfaces. BCIs
use predictive models to interpret brain signals for various high-stake
applications. However, achieving explainability in these complex models is
challenging as it compromises accuracy. The field of XAI has emerged to address
the need for explainability across various stakeholders, but there is a lack of
an integrated perspective in XAI for BCI (XAI4BCI) literature. It is necessary
to differentiate key concepts like explainability, interpretability, and
understanding in this context and formulate a comprehensive framework. To
understand the need of XAI for BCI, we pose six key research questions for a
systematic review and meta-analysis, encompassing its purposes, applications,
usability, and technical feasibility. We employ the PRISMA methodology --
preferred reporting items for systematic reviews and meta-analyses to review
(n=1246) and analyze (n=84) studies published in 2015 and onwards for key
insights. The results highlight that current research primarily focuses on
interpretability for developers and researchers, aiming to justify outcomes and
enhance model performance. We discuss the unique approaches, advantages, and
limitations of XAI4BCI from the literature. We draw insights from philosophy,
psychology, and social sciences. We propose a design space for XAI4BCI,
considering the evolving need to visualize and investigate predictive model
outcomes customised for various stakeholders in the BCI development and
deployment lifecycle. This paper is the first to focus solely on reviewing
XAI4BCI research articles. This systematic review and meta-analysis findings
with the proposed design space prompt important discussions on establishing
standards for BCI explanations, highlighting current limitations, and guiding
the future of XAI in BCI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpura_P/0/1/0/all/0/1&quot;&gt;Param Rajpura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecotti_H/0/1/0/all/0/1&quot;&gt;Hubert Cecotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meena_Y/0/1/0/all/0/1&quot;&gt;Yogesh Kumar Meena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13068">
<title>Continuous-time Graph Representation with Sequential Survival Process. (arXiv:2312.13068v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13068</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past two decades, there has been a tremendous increase in the growth
of representation learning methods for graphs, with numerous applications
across various fields, including bioinformatics, chemistry, and the social
sciences. However, current dynamic network approaches focus on discrete-time
networks or treat links in continuous-time networks as instantaneous events.
Therefore, these approaches have limitations in capturing the persistence or
absence of links that continuously emerge and disappear over time for
particular durations. To address this, we propose a novel stochastic process
relying on survival functions to model the durations of links and their
absences over time. This forms a generic new likelihood specification
explicitly accounting for intermittent edge-persistent networks, namely GraSSP:
Graph Representation with Sequential Survival Process. We apply the developed
framework to a recent continuous time dynamic latent distance model
characterizing network dynamics in terms of a sequence of piecewise linear
movements of nodes in latent space. We quantitatively assess the developed
framework in various downstream tasks, such as link prediction and network
completion, demonstrating that the developed modeling framework accounting for
link persistence and absence well tracks the intrinsic trajectories of nodes in
a latent space and captures the underlying characteristics of evolving network
structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celikkanat_A/0/1/0/all/0/1&quot;&gt;Abdulkadir Celikkanat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Nakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morup_M/0/1/0/all/0/1&quot;&gt;Morten M&amp;#xf8;rup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13116">
<title>VSR-Net: Vessel-like Structure Rehabilitation Network with Graph Clustering. (arXiv:2312.13116v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.13116</link>
<description rdf:parseType="Literal">&lt;p&gt;The morphologies of vessel-like structures, such as blood vessels and nerve
fibres, play significant roles in disease diagnosis, e.g., Parkinson&apos;s disease.
Deep network-based refinement segmentation methods have recently achieved
promising vessel-like structure segmentation results. There are still two
challenges: (1) existing methods have limitations in rehabilitating subsection
ruptures in segmented vessel-like structures; (2) they are often overconfident
in predicted segmentation results. To tackle these two challenges, this paper
attempts to leverage the potential of spatial interconnection relationships
among subsection ruptures from the structure rehabilitation perspective. Based
on this, we propose a novel Vessel-like Structure Rehabilitation Network
(VSR-Net) to rehabilitate subsection ruptures and improve the model calibration
based on coarse vessel-like structure segmentation results. VSR-Net first
constructs subsection rupture clusters with Curvilinear Clustering Module
(CCM). Then, the well-designed Curvilinear Merging Module (CMM) is applied to
rehabilitate the subsection ruptures to obtain the refined vessel-like
structures. Extensive experiments on five 2D/3D medical image datasets show
that VSR-Net significantly outperforms state-of-the-art (SOTA) refinement
segmentation methods with lower calibration error. Additionally, we provide
quantitative analysis to explain the morphological difference between the
rehabilitation results of VSR-Net and ground truth (GT), which is smaller than
SOTA methods and GT, demonstrating that our method better rehabilitates
vessel-like structures by restoring subsection ruptures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Haili Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13131">
<title>Scaling Compute Is Not All You Need for Adversarial Robustness. (arXiv:2312.13131v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13131</link>
<description rdf:parseType="Literal">&lt;p&gt;The last six years have witnessed significant progress in adversarially
robust deep learning. As evidenced by the CIFAR-10 dataset category in
RobustBench benchmark, the accuracy under $\ell_\infty$ adversarial
perturbations improved from 44\% in \citet{Madry2018Towards} to 71\% in
\citet{peng2023robust}. Although impressive, existing state-of-the-art is still
far from satisfactory. It is further observed that best-performing models are
often very large models adversarially trained by industrial labs with
significant computational budgets. In this paper, we aim to understand: ``how
much longer can computing power drive adversarial robustness advances?&quot; To
answer this question, we derive \emph{scaling laws for adversarial robustness}
which can be extrapolated in the future to provide an estimate of how much cost
we would need to pay to reach a desired level of robustness. We show that
increasing the FLOPs needed for adversarial training does not bring as much
advantage as it does for standard training in terms of performance
improvements. Moreover, we find that some of the top-performing techniques are
difficult to exactly reproduce, suggesting that they are not robust enough for
minor changes in the training setup. Our analysis also uncovers potentially
worthwhile directions to pursue in future research. Finally, we make our
benchmarking framework (built on top of \texttt{timm}~\citep{rw2019timm})
publicly available to facilitate future analysis in efficient robust deep
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debenedetti_E/0/1/0/all/0/1&quot;&gt;Edoardo Debenedetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zishen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1&quot;&gt;Maksym Andriushchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sehwag_V/0/1/0/all/0/1&quot;&gt;Vikash Sehwag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_K/0/1/0/all/0/1&quot;&gt;Kshitij Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kailkhura_B/0/1/0/all/0/1&quot;&gt;Bhavya Kailkhura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13156">
<title>AccidentGPT: Accident analysis and prevention from V2X Environmental Perception with Multi-modal Large Model. (arXiv:2312.13156v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2312.13156</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic accidents, being a significant contributor to both human casualties
and property damage, have long been a focal point of research for many scholars
in the field of traffic safety. However, previous studies, whether focusing on
static environmental assessments or dynamic driving analyses, as well as
pre-accident predictions or post-accident rule analyses, have typically been
conducted in isolation. There has been a lack of an effective framework for
developing a comprehensive understanding and application of traffic safety. To
address this gap, this paper introduces AccidentGPT, a comprehensive accident
analysis and prevention multi-modal large model. AccidentGPT establishes a
multi-modal information interaction framework grounded in multi-sensor
perception, thereby enabling a holistic approach to accident analysis and
prevention in the field of traffic safety. Specifically, our capabilities can
be categorized as follows: for autonomous driving vehicles, we provide
comprehensive environmental perception and understanding to control the vehicle
and avoid collisions. For human-driven vehicles, we offer proactive long-range
safety warnings and blind-spot alerts while also providing safety driving
recommendations and behavioral norms through human-machine dialogue and
interaction. Additionally, for traffic police and management agencies, our
framework supports intelligent and real-time analysis of traffic safety,
encompassing pedestrian, vehicles, roads, and the environment through
collaborative perception from multiple vehicles and road testing devices. The
system is also capable of providing a thorough analysis of accident causes and
liability after vehicle collisions. Our framework stands as the first large
model to integrate comprehensive scene understanding into traffic safety
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lening Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Han Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yilong Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haiyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuesong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinhai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13193">
<title>HCDIR: End-to-end Hate Context Detection, and Intensity Reduction model for online comments. (arXiv:2312.13193v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.13193</link>
<description rdf:parseType="Literal">&lt;p&gt;Warning: This paper contains examples of the language that some people may
find offensive.
&lt;/p&gt;
&lt;p&gt;Detecting and reducing hateful, abusive, offensive comments is a critical and
challenging task on social media. Moreover, few studies aim to mitigate the
intensity of hate speech. While studies have shown that context-level semantics
are crucial for detecting hateful comments, most of this research focuses on
English due to the ample datasets available. In contrast, low-resource
languages, like Indian languages, remain under-researched because of limited
datasets. Contrary to hate speech detection, hate intensity reduction remains
unexplored in high-resource and low-resource languages. In this paper, we
propose a novel end-to-end model, HCDIR, for Hate Context Detection, and Hate
Intensity Reduction in social media posts. First, we fine-tuned several
pre-trained language models to detect hateful comments to ascertain the
best-performing hateful comments detection model. Then, we identified the
contextual hateful words. Identification of such hateful words is justified
through the state-of-the-art explainable learning model, i.e., Integrated
Gradient (IG). Lastly, the Masked Language Modeling (MLM) model has been
employed to capture domain-specific nuances to reduce hate intensity. We masked
the 50\% hateful words of the comments identified as hateful and predicted the
alternative words for these masked terms to generate convincing sentences. An
optimal replacement for the original hate comments from the feasible sentences
is preferred. Extensive experiments have been conducted on several recent
datasets using automatic metric-based evaluation (BERTScore) and thorough human
evaluation. To enhance the faithfulness in human evaluation, we arranged a
group of three human annotators with varied expertise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_N/0/1/0/all/0/1&quot;&gt;Neeraj Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_K/0/1/0/all/0/1&quot;&gt;Koyel Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_J/0/1/0/all/0/1&quot;&gt;Joy Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1&quot;&gt;Utpal Garain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senapati_A/0/1/0/all/0/1&quot;&gt;Apurbalal Senapati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13212">
<title>A 3D super-resolution of wind fields via physics-informed pixel-wise self-attention generative adversarial network. (arXiv:2312.13212v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2312.13212</link>
<description rdf:parseType="Literal">&lt;p&gt;To mitigate global warming, greenhouse gas sources need to be resolved at a
high spatial resolution and monitored in time to ensure the reduction and
ultimately elimination of the pollution source. However, the complexity of
computation in resolving high-resolution wind fields left the simulations
impractical to test different time lengths and model configurations. This study
presents a preliminary development of a physics-informed super-resolution (SR)
generative adversarial network (GAN) that super-resolves the three-dimensional
(3D) low-resolution wind fields by upscaling x9 times. We develop a pixel-wise
self-attention (PWA) module that learns 3D weather dynamics via a
self-attention computation followed by a 2D convolution. We also employ a loss
term that regularizes the self-attention map during pretraining, capturing the
vertical convection process from input wind data. The new PWA SR-GAN shows the
high-fidelity super-resolved 3D wind data, learns a wind structure at the
high-frequency domain, and reduces the computational cost of a high-resolution
wind simulation by x89.7 times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kurihana_T/0/1/0/all/0/1&quot;&gt;Takuya Kurihana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yeo_K/0/1/0/all/0/1&quot;&gt;Kyongmin Yeo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Szwarcman_D/0/1/0/all/0/1&quot;&gt;Daniela Szwarcman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Elmegreen_B/0/1/0/all/0/1&quot;&gt;Bruce Elmegreen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mukkavilli_K/0/1/0/all/0/1&quot;&gt;Karthik Mukkavilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Schmude_J/0/1/0/all/0/1&quot;&gt;Johannes Schmude&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Klein_L/0/1/0/all/0/1&quot;&gt;Levente Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.13218">
<title>FiFAR: A Fraud Detection Dataset for Learning to Defer. (arXiv:2312.13218v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.13218</link>
<description rdf:parseType="Literal">&lt;p&gt;Public dataset limitations have significantly hindered the development and
benchmarking of learning to defer (L2D) algorithms, which aim to optimally
combine human and AI capabilities in hybrid decision-making systems. In such
systems, human availability and domain-specific concerns introduce
difficulties, while obtaining human predictions for training and evaluation is
costly. Financial fraud detection is a high-stakes setting where algorithms and
human experts often work in tandem; however, there are no publicly available
datasets for L2D concerning this important application of human-AI teaming. To
fill this gap in L2D research, we introduce the Financial Fraud Alert Review
Dataset (FiFAR), a synthetic bank account fraud detection dataset, containing
the predictions of a team of 50 highly complex and varied synthetic fraud
analysts, with varied bias and feature dependence. We also provide a realistic
definition of human work capacity constraints, an aspect of L2D systems that is
often overlooked, allowing for extensive testing of assignment systems under
real-world conditions. We use our dataset to develop a capacity-aware L2D
method and rejection learning approach under realistic data availability
conditions, and benchmark these baselines under an array of 300 distinct
testing scenarios. We believe that this dataset will serve as a pivotal
instrument in facilitating a systematic, rigorous, reproducible, and
transparent evaluation and comparison of L2D methods, thereby fostering the
development of more synergistic human-AI collaboration in decision-making
systems. The public dataset and detailed synthetic expert information are
available at: https://github.com/feedzai/fifar-dataset
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alves_J/0/1/0/all/0/1&quot;&gt;Jean V. Alves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leitao_D/0/1/0/all/0/1&quot;&gt;Diogo Leit&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jesus_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;rgio Jesus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sampaio_M/0/1/0/all/0/1&quot;&gt;Marco O. P. Sampaio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleiro_P/0/1/0/all/0/1&quot;&gt;Pedro Saleiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Figueiredo_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe1;rio A. T. Figueiredo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bizarro_P/0/1/0/all/0/1&quot;&gt;Pedro Bizarro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.00350">
<title>The Interpretability of LSTM Models for Predicting Oil Company Stocks: Impact of Correlated Features. (arXiv:2201.00350v5 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2201.00350</link>
<description rdf:parseType="Literal">&lt;p&gt;Oil companies are among the largest companies in the world whose economic
indicators in the global stock market have a great impact on the world
economy\cite{ec00} and market due to their relation to gold\cite{ec01}, crude
oil\cite{ec02}, and the dollar\cite{ec03}. This study investigates the impact
of correlated features on the interpretability of Long Short-Term
Memory(LSTM)\cite{ec04} models for predicting oil company stocks. To achieve
this, we designed a Standard Long Short-Term Memory (LSTM) network and trained
it using various correlated datasets. Our approach aims to improve the accuracy
of stock price prediction by considering the multiple factors affecting the
market, such as crude oil prices, gold prices, and the US dollar. The results
demonstrate that adding a feature correlated with oil stocks does not improve
the interpretability of LSTM models. These findings suggest that while LSTM
models may be effective in predicting stock prices, their interpretability may
be limited. Caution should be exercised when relying solely on LSTM models for
stock price prediction as their lack of interpretability may make it difficult
to fully understand the underlying factors driving stock price movements. We
have employed complexity analysis to support our argument, considering that
financial markets encompass a form of physical complex system\cite{ec05}. One
of the fundamental challenges faced in utilizing LSTM models for financial
markets lies in interpreting the unexpected feedback dynamics within them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Firouzjaee_J/0/1/0/all/0/1&quot;&gt;Javad T. Firouzjaee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Khaliliyan_P/0/1/0/all/0/1&quot;&gt;Pouriya Khaliliyan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04587">
<title>Multipoint-BAX: A New Approach for Efficiently Tuning Particle Accelerator Emittance via Virtual Objectives. (arXiv:2209.04587v5 [physics.acc-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04587</link>
<description rdf:parseType="Literal">&lt;p&gt;Although beam emittance is critical for the performance of high-brightness
accelerators, optimization is often time limited as emittance calculations,
commonly done via quadrupole scans, are typically slow. Such calculations are a
type of $\textit{multipoint query}$, i.e. each query requires multiple
secondary measurements. Traditional black-box optimizers such as Bayesian
optimization are slow and inefficient when dealing with such objectives as they
must acquire the full series of measurements, but return only the emittance,
with each query. We propose a new information-theoretic algorithm,
Multipoint-BAX, for black-box optimization on multipoint queries, which queries
and models individual beam-size measurements using techniques from Bayesian
Algorithm Execution (BAX). Our method avoids the slow multipoint query on the
accelerator by acquiring points through a $\textit{virtual objective}$, i.e.
calculating the emittance objective from a fast learned model rather than
directly from the accelerator. We use Multipoint-BAX to minimize emittance at
the Linac Coherent Light Source (LCLS) and the Facility for Advanced
Accelerator Experimental Tests II (FACET-II). In simulation, our method is
20$\times$ faster and more robust to noise compared to existing methods. In
live tests, it matched the hand-tuned emittance at FACET-II and achieved a 24%
lower emittance than hand-tuning at LCLS. Our method represents a conceptual
shift for optimizing multipoint queries, and we anticipate that it can be
readily adapted to similar problems in particle accelerators and other
scientific instruments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Miskovich_S/0/1/0/all/0/1&quot;&gt;Sara A. Miskovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Neiswanger_W/0/1/0/all/0/1&quot;&gt;Willie Neiswanger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Colocho_W/0/1/0/all/0/1&quot;&gt;William Colocho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Emma_C/0/1/0/all/0/1&quot;&gt;Claudio Emma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Garrahan_J/0/1/0/all/0/1&quot;&gt;Jacqueline Garrahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Maxwell_T/0/1/0/all/0/1&quot;&gt;Timothy Maxwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Mayes_C/0/1/0/all/0/1&quot;&gt;Christopher Mayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Edelen_A/0/1/0/all/0/1&quot;&gt;Auralee Edelen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ratner_D/0/1/0/all/0/1&quot;&gt;Daniel Ratner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15657">
<title>Detecting fake accounts through Generative Adversarial Network in online social media. (arXiv:2210.15657v4 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15657</link>
<description rdf:parseType="Literal">&lt;p&gt;Online social media is integral to human life, facilitating messaging,
information sharing, and confidential communication while preserving privacy.
Platforms like Twitter, Instagram, and Facebook exemplify this phenomenon.
However, users face challenges due to network anomalies, often stemming from
malicious activities such as identity theft for financial gain or harm. This
paper proposes a novel method using user similarity measures and the Generative
Adversarial Network (GAN) algorithm to identify fake user accounts in the
Twitter dataset. Despite the problem&apos;s complexity, the method achieves an AUC
rate of 80\% in classifying and detecting fake accounts. Notably, the study
builds on previous research, highlighting advancements and insights into the
evolving landscape of anomaly detection in online social networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1&quot;&gt;Jinus Bordbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Mohammadrezaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1&quot;&gt;Saman Ardalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiri_M/0/1/0/all/0/1&quot;&gt;Mohammad Ebrahim Shiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01071">
<title>Fake detection in imbalance dataset by Semi-supervised learning with GAN. (arXiv:2212.01071v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01071</link>
<description rdf:parseType="Literal">&lt;p&gt;As social media continues to grow rapidly, the prevalence of harassment on
these platforms has also increased. This has piqued the interest of researchers
in the field of fake detection. Social media data, often forms complex graphs
with numerous nodes, posing several challenges. These challenges and
limitations include dealing with a significant amount of irrelevant features in
matrices and addressing issues such as high data dispersion and an imbalanced
class distribution within the dataset. To overcome these challenges and
limitations, researchers have employed auto-encoders and a combination of
semi-supervised learning with a GAN algorithm, referred to as SGAN. Our
proposed method utilizes auto-encoders for feature extraction and incorporates
SGAN. By leveraging an unlabeled dataset, the unsupervised layer of SGAN
compensates for the limited availability of labeled data, making efficient use
of the limited number of labeled instances. Multiple evaluation metrics were
employed, including the Confusion Matrix and the ROC curve. The dataset was
divided into training and testing sets, with 100 labeled samples for training
and 1,000 samples for testing. The novelty of our research lies in applying
SGAN to address the issue of imbalanced datasets in fake account detection. By
optimizing the use of a smaller number of labeled instances and reducing the
need for extensive computational power, our method offers a more efficient
solution. Additionally, our study contributes to the field by achieving an 81%
accuracy in detecting fake accounts using only 100 labeled samples. This
demonstrates the potential of SGAN as a powerful tool for handling minority
classes and addressing big data challenges in fake account detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bordbar_J/0/1/0/all/0/1&quot;&gt;Jinus Bordbar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardalan_S/0/1/0/all/0/1&quot;&gt;Saman Ardalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadrezaie_M/0/1/0/all/0/1&quot;&gt;Mohammadreza Mohammadrezaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghasemi_Z/0/1/0/all/0/1&quot;&gt;Zahra Ghasemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13779">
<title>FLAME: A small language model for spreadsheet formulas. (arXiv:2301.13779v2 [cs.PL] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13779</link>
<description rdf:parseType="Literal">&lt;p&gt;Spreadsheets are a vital tool for end-user data management. Using large
language models for formula authoring assistance in these environments can be
difficult, as these models are expensive to train and challenging to deploy due
to their size (up to billions of parameters). We present FLAME, a
transformer-based model trained exclusively on Excel formulas that leverages
domain insights to achieve competitive performance while being substantially
smaller (60M parameters) and training on two orders of magnitude less data. We
curate a training dataset using sketch deduplication, introduce an
Excel-specific formula tokenizer, and use domain-specific versions of masked
span prediction and noisy auto-encoding as pre-training objectives. We evaluate
FLAME on formula repair, formula completion, and similarity-based formula
retrieval. FLAME can outperform much larger models, such as the Davinci (175B)
and Cushman (12B) variants of Codex and CodeT5 (220M), in 10 of 14 evaluation
settings for the repair and completion tasks. For formula retrieval, FLAME
outperforms CodeT5, CodeBERT, and GraphCodeBERT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_H/0/1/0/all/0/1&quot;&gt;Harshit Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebenezer_A/0/1/0/all/0/1&quot;&gt;Abishai Ebenezer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambronero_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Cambronero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulwani_S/0/1/0/all/0/1&quot;&gt;Sumit Gulwani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1&quot;&gt;Aditya Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_V/0/1/0/all/0/1&quot;&gt;Vu Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radicek_I/0/1/0/all/0/1&quot;&gt;Ivan Radi&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbruggen_G/0/1/0/all/0/1&quot;&gt;Gust Verbruggen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02515">
<title>Deep Learning for Time Series Classification and Extrinsic Regression: A Current Survey. (arXiv:2302.02515v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02515</link>
<description rdf:parseType="Literal">&lt;p&gt;Time Series Classification and Extrinsic Regression are important and
challenging machine learning tasks. Deep learning has revolutionized natural
language processing and computer vision and holds great promise in other fields
such as time series analysis where the relevant features must often be
abstracted from the raw data but are not known a priori. This paper surveys the
current state of the art in the fast-moving field of deep learning for time
series classification and extrinsic regression. We review different network
architectures and training methods used for these tasks and discuss the
challenges and opportunities when applying deep learning to time series data.
We also summarize two critical applications of time series classification and
extrinsic regression, human activity recognition and satellite earth
observation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foumani_N/0/1/0/all/0/1&quot;&gt;Navid Mohammadi Foumani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_L/0/1/0/all/0/1&quot;&gt;Lynn Miller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chang Wei Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forestier_G/0/1/0/all/0/1&quot;&gt;Germain Forestier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehi_M/0/1/0/all/0/1&quot;&gt;Mahsa Salehi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.00196">
<title>Transformed Low-Rank Parameterization Can Help Robust Generalization for Tensor Neural Networks. (arXiv:2303.00196v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.00196</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving efficient and robust multi-channel data learning is a challenging
task in data science. By exploiting low-rankness in the transformed domain,
i.e., transformed low-rankness, tensor Singular Value Decomposition (t-SVD) has
achieved extensive success in multi-channel data representation and has
recently been extended to function representation such as Neural Networks with
t-product layers (t-NNs). However, it still remains unclear how t-SVD
theoretically affects the learning behavior of t-NNs. This paper is the first
to answer this question by deriving the upper bounds of the generalization
error of both standard and adversarially trained t-NNs. It reveals that the
t-NNs compressed by exact transformed low-rank parameterization can achieve a
sharper adversarial generalization bound. In practice, although t-NNs rarely
have exactly transformed low-rank weights, our analysis further shows that by
adversarial training with gradient flow (GF), the over-parameterized t-NNs with
ReLU activations are trained with implicit regularization towards transformed
low-rank parameterization under certain conditions. We also establish
adversarial generalization bounds for t-NNs with approximately transformed
low-rank weights. Our analysis indicates that the transformed low-rank
parameterization can promisingly enhance robust generalization for t-NNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_A/0/1/0/all/0/1&quot;&gt;Andong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_M/0/1/0/all/0/1&quot;&gt;Mingyuan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guoxu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qibin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12484">
<title>Label-Efficient Deep Learning in Medical Image Analysis: Challenges and Future Directions. (arXiv:2303.12484v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12484</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has seen rapid growth in recent years and achieved
state-of-the-art performance in a wide range of applications. However, training
models typically requires expensive and time-consuming collection of large
quantities of labeled data. This is particularly true within the scope of
medical imaging analysis (MIA), where data are limited and labels are expensive
to be acquired. Thus, label-efficient deep learning methods are developed to
make comprehensive use of the labeled data as well as the abundance of
unlabeled and weak-labeled data. In this survey, we extensively investigated
over 300 recent papers to provide a comprehensive overview of recent progress
on label-efficient learning strategies in MIA. We first present the background
of label-efficient learning and categorize the approaches into different
schemes. Next, we examine the current state-of-the-art methods in detail
through each scheme. Specifically, we provide an in-depth investigation,
covering not only canonical semi-supervised, self-supervised, and
multi-instance learning schemes, but also recently emerged active and
annotation-efficient learning strategies. Moreover, as a comprehensive
contribution to the field, this survey not only elucidates the commonalities
and unique features of the surveyed methods but also presents a detailed
analysis of the current challenges in the field and suggests potential avenues
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhengrui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Luyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01246">
<title>Safety Analysis in the Era of Large Language Models: A Case Study of STPA using ChatGPT. (arXiv:2304.01246v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01246</link>
<description rdf:parseType="Literal">&lt;p&gt;Can safety analysis make use of Large Language Models (LLMs)? A case study
explores Systems Theoretic Process Analysis (STPA) applied to Automatic
Emergency Brake (AEB) and Electricity Demand Side Management (DSM) systems
using ChatGPT. We investigate how collaboration schemes, input semantic
complexity, and prompt guidelines influence STPA results. Comparative results
show that using ChatGPT without human intervention may be inadequate due to
reliability related issues, but with careful design, it may outperform human
experts. No statistically significant differences are found when varying the
input semantic complexity or using common prompt guidelines, which suggests the
necessity for developing domain-specific prompt engineering. We also highlight
future challenges, including concerns about LLM trustworthiness and the
necessity for standardisation and regulation in this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khastgir_S/0/1/0/all/0/1&quot;&gt;Siddartha Khastgir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03898">
<title>The Short Text Matching Model Enhanced with Knowledge via Contrastive Learning. (arXiv:2304.03898v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03898</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, short Text Matching tasks have been widely applied in the
fields ofadvertising search and recommendation. The difficulty lies in the lack
of semantic information and word ambiguity caused by the short length of the
text. Previous works have introduced complement sentences or knowledge bases to
provide additional feature information. However, these methods have not fully
interacted between the original sentence and the complement sentence, and have
not considered the noise issue that may arise from the introduction of external
knowledge bases. Therefore, this paper proposes a short Text Matching model
that combines contrastive learning and external knowledge. The model uses a
generative model to generate corresponding complement sentences and uses the
contrastive learning method to guide the model to obtain more semantically
meaningful encoding of the original sentence. In addition, to avoid noise, we
use keywords as the main semantics of the original sentence to retrieve
corresponding knowledge words in the knowledge base, and construct a knowledge
graph. The graph encoding model is used to integrate the knowledge base
information into the model. Our designed model achieves state-of-the-art
performance on two publicly available Chinese Text Matching datasets,
demonstrating the effectiveness of our model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruiqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Q/0/1/0/all/0/1&quot;&gt;Qiqiang Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Mengmeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1&quot;&gt;Hanjie Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaohua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangzheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yanlong Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00054">
<title>LAVA: Data Valuation without Pre-Specified Learning Algorithms. (arXiv:2305.00054v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00054</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditionally, data valuation (DV) is posed as a problem of equitably
splitting the validation performance of a learning algorithm among the training
data. As a result, the calculated data values depend on many design choices of
the underlying learning algorithm. However, this dependence is undesirable for
many DV use cases, such as setting priorities over different data sources in a
data acquisition process and informing pricing mechanisms in a data
marketplace. In these scenarios, data needs to be valued before the actual
analysis and the choice of the learning algorithm is still undetermined then.
Another side-effect of the dependence is that to assess the value of individual
points, one needs to re-run the learning algorithm with and without a point,
which incurs a large computation burden. This work leapfrogs over the current
limits of data valuation methods by introducing a new framework that can value
training data in a way that is oblivious to the downstream learning algorithm.
Our main results are as follows. (1) We develop a proxy for the validation
performance associated with a training set based on a non-conventional
class-wise Wasserstein distance between training and validation sets. We show
that the distance characterizes the upper bound of the validation performance
for any given model under certain Lipschitz conditions. (2) We develop a novel
method to value individual data based on the sensitivity analysis of the
class-wise Wasserstein distance. Importantly, these values can be directly
obtained for free from the output of off-the-shelf optimization solvers when
computing the distance. (3) We evaluate our new data valuation framework over
various use cases related to detecting low-quality data and show that,
surprisingly, the learning-agnostic feature of our framework enables a
significant improvement over SOTA performance while being orders of magnitude
faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Just_H/0/1/0/all/0/1&quot;&gt;Hoang Anh Just&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_F/0/1/0/all/0/1&quot;&gt;Feiyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiachen T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_M/0/1/0/all/0/1&quot;&gt;Myeongseob Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_M/0/1/0/all/0/1&quot;&gt;Ming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_R/0/1/0/all/0/1&quot;&gt;Ruoxi Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05418">
<title>Measuring Rule-based LTLf Process Specifications: A Probabilistic Data-driven Approach. (arXiv:2305.05418v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05418</link>
<description rdf:parseType="Literal">&lt;p&gt;Declarative process specifications define the behavior of processes by means
of rules based on Linear Temporal Logic on Finite Traces (LTLf). In a mining
context, these specifications are inferred from, and checked on, multi-sets of
runs recorded by information systems (namely, event logs). To this end, being
able to gauge the degree to which process data comply with a specification is
key. However, existing mining and verification techniques analyze the rules in
isolation, thereby disregarding their interplay. In this paper, we introduce a
framework to devise probabilistic measures for declarative process
specifications. Thereupon, we propose a technique that measures the degree of
satisfaction of specifications over event logs. To assess our approach, we
conduct an evaluation with real-world data, evidencing its applicability in
discovery, checking, and drift detection contexts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cecconi_A/0/1/0/all/0/1&quot;&gt;Alessio Cecconi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbaro_L/0/1/0/all/0/1&quot;&gt;Luca Barbaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ciccio_C/0/1/0/all/0/1&quot;&gt;Claudio Di Ciccio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senderovich_A/0/1/0/all/0/1&quot;&gt;Arik Senderovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11662">
<title>Separating form and meaning: Using self-consistency to quantify task understanding across multiple senses. (arXiv:2305.11662v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11662</link>
<description rdf:parseType="Literal">&lt;p&gt;At the staggering pace with which the capabilities of large language models
(LLMs) are increasing, creating future-proof evaluation sets to assess their
understanding becomes more and more challenging. In this paper, we propose a
novel paradigm for evaluating LLMs which leverages the idea that correct world
understanding should be consistent across different (Fregean) senses of the
same meaning. Accordingly, we measure understanding not in terms of correctness
but by evaluating consistency across multiple senses that are generated by the
model itself. We showcase our approach by instantiating a test where the
different senses are different languages, hence using multilingual
self-consistency as a litmus test for the model&apos;s understanding and
simultaneously addressing the important topic of multilinguality. Taking one of
the latest versions of ChatGPT as our object of study, we evaluate multilingual
consistency for two different tasks across three different languages. We show
that its multilingual consistency is still lacking, and that its task and world
understanding are thus not language-independent. As our approach does not
require any static evaluation corpora in languages other than English, it can
easily and cheaply be extended to different languages and tasks and could
become an integral part of future benchmarking efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohmer_X/0/1/0/all/0/1&quot;&gt;Xenia Ohmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1&quot;&gt;Elia Bruni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1&quot;&gt;Dieuwke Hupkes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15296">
<title>MultiFusion: Fusing Pre-Trained Models for Multi-Lingual, Multi-Modal Image Generation. (arXiv:2305.15296v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15296</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent popularity of text-to-image diffusion models (DM) can largely be
attributed to the intuitive interface they provide to users. The intended
generation can be expressed in natural language, with the model producing
faithful interpretations of text prompts. However, expressing complex or
nuanced ideas in text alone can be difficult. To ease image generation, we
propose MultiFusion that allows one to express complex and nuanced concepts
with arbitrarily interleaved inputs of multiple modalities and languages.
MutliFusion leverages pre-trained models and aligns them for integration into a
cohesive system, thereby avoiding the need for extensive training from scratch.
Our experimental results demonstrate the efficient transfer of capabilities
from individual modules to the downstream model. Specifically, the fusion of
all independent components allows the image generation module to utilize
multilingual, interleaved multimodal inputs despite being trained solely on
monomodal data in a single language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1&quot;&gt;Marco Bellagente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1&quot;&gt;Hannah Teufel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deiseroth_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Deiseroth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eichenberg_C/0/1/0/all/0/1&quot;&gt;Constantin Eichenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldock_R/0/1/0/all/0/1&quot;&gt;Robert Baldock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_S/0/1/0/all/0/1&quot;&gt;Souradeep Nanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1&quot;&gt;Koen Oostermeijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_Salinas_A/0/1/0/all/0/1&quot;&gt;Andres Felipe Cruz-Salinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinbach_S/0/1/0/all/0/1&quot;&gt;Samuel Weinbach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15685">
<title>RewriteLM: An Instruction-Tuned Large Language Model for Text Rewriting. (arXiv:2305.15685v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15685</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated impressive capabilities in
creative tasks such as storytelling and E-mail generation. However, as LLMs are
primarily trained on final text results rather than intermediate revisions, it
might be challenging for them to perform text rewriting tasks. Most studies in
the rewriting tasks focus on a particular transformation type within the
boundaries of single sentences. In this work, we develop new strategies for
instruction tuning and reinforcement learning to better align LLMs for
cross-sentence rewriting tasks using diverse wording and structures expressed
through natural languages including 1) generating rewriting instruction data
from Wiki edits and public corpus through instruction generation and
chain-of-thought prompting; 2) collecting comparison data for reward model
training through a new ranking function. To facilitate this research, we
introduce OpenRewriteEval, a novel benchmark covers a wide variety of rewriting
types expressed through natural language instructions. Our results show
significant improvements over a variety of baselines. The public repository is
available on GitHub under Google Research
(https://github.com/google-research/google-research/tree/master/rewritelm).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_L/0/1/0/all/0/1&quot;&gt;Lei Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_L/0/1/0/all/0/1&quot;&gt;Liangchen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoskere_J/0/1/0/all/0/1&quot;&gt;Jayakumar Hoskere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yinxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_S/0/1/0/all/0/1&quot;&gt;Simon Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jindong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_L/0/1/0/all/0/1&quot;&gt;Lei Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17330">
<title>MADiff: Offline Multi-agent Learning with Diffusion Models. (arXiv:2305.17330v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17330</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion model (DM), as a powerful generative model, recently achieved huge
success in various scenarios including offline reinforcement learning, where
the policy learns to conduct planning by generating trajectory in the online
evaluation. However, despite the effectiveness shown for single-agent learning,
it remains unclear how DMs can operate in multi-agent problems, where agents
can hardly complete teamwork without good coordination by independently
modeling each agent&apos;s trajectories. In this paper, we propose MADiff, a novel
generative multi-agent learning framework to tackle this problem. MADiff is
realized with an attention-based diffusion model to model the complex
coordination among behaviors of multiple diffusion agents. To the best of our
knowledge, MADiff is the first diffusion-based multi-agent offline RL
framework, which behaves as both a decentralized policy and a centralized
controller. During decentralized executions, MADiff simultaneously performs
teammate modeling, and the centralized controller can also be applied in
multi-agent trajectory predictions. Our experiments show the superior
performance of MADiff compared to baseline algorithms in a wide range of
multi-agent learning tasks, which emphasizes the effectiveness of MADiff in
modeling complex multi-agent interactions. Our code is available at
https://github.com/zbzhu99/madiff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhengbang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1&quot;&gt;Liyuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minkai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01266">
<title>Self Contrastive Learning for Session-based Recommendation. (arXiv:2306.01266v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01266</link>
<description rdf:parseType="Literal">&lt;p&gt;Session-based recommendation, which aims to predict the next item of users&apos;
interest as per an existing sequence interaction of items, has attracted
growing applications of Contrastive Learning (CL) with improved user and item
representations. However, these contrastive objectives: (1) serve a similar
role as the cross-entropy loss while ignoring the item representation space
optimisation; and (2) commonly require complicated modelling, including complex
positive/negative sample constructions and extra data augmentation. In this
work, we introduce Self-Contrastive Learning (SCL), which simplifies the
application of CL and enhances the performance of state-of-the-art CL-based
recommendation techniques. Specifically, SCL is formulated as an objective
function that directly promotes a uniform distribution among item
representations and efficiently replaces all the existing contrastive objective
components of state-of-the-art models. Unlike previous works, SCL eliminates
the need for any positive/negative sample construction or data augmentation,
leading to enhanced interpretability of the item representation space and
facilitating its extensibility to existing recommender systems. Through
experiments on three benchmark datasets, we demonstrate that SCL consistently
improves the performance of state-of-the-art models with statistical
significance. Notably, our experiments show that SCL improves the performance
of two best-performing models by 8.2% and 9.5% in P@10 (Precision) and 9.9% and
11.2% in MRR@10 (Mean Reciprocal Rank) on average across different benchmarks.
Additionally, our analysis elucidates the improvement in terms of alignment and
uniformity of representations, as well as the effectiveness of SCL with a low
computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhengxiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1&quot;&gt;Aldo Lipani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11698">
<title>DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models. (arXiv:2306.11698v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11698</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Pre-trained Transformer (GPT) models have exhibited exciting
progress in their capabilities, capturing the interest of practitioners and the
public alike. Yet, while the literature on the trustworthiness of GPT models
remains limited, practitioners have proposed employing capable GPT models for
sensitive applications such as healthcare and finance -- where mistakes can be
costly. To this end, this work proposes a comprehensive trustworthiness
evaluation for large language models with a focus on GPT-4 and GPT-3.5,
considering diverse perspectives -- including toxicity, stereotype bias,
adversarial robustness, out-of-distribution robustness, robustness on
adversarial demonstrations, privacy, machine ethics, and fairness. Based on our
evaluations, we discover previously unpublished vulnerabilities to
trustworthiness threats. For instance, we find that GPT models can be easily
misled to generate toxic and biased outputs and leak private information in
both training data and conversation history. We also find that although GPT-4
is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more
vulnerable given jailbreaking system or user prompts, potentially because GPT-4
follows (misleading) instructions more precisely. Our work illustrates a
comprehensive trustworthiness evaluation of GPT models and sheds light on the
trustworthiness gaps. Our benchmark is publicly available at
https://decodingtrust.github.io/; our dataset can be previewed at
https://huggingface.co/datasets/AI-Secure/DecodingTrust; a concise version of
this work is at https://openreview.net/pdf?id=kaHpo8OZw2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_H/0/1/0/all/0/1&quot;&gt;Hengzhi Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Chulin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Mintong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chejian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zidi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_R/0/1/0/all/0/1&quot;&gt;Ritik Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1&quot;&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_S/0/1/0/all/0/1&quot;&gt;Sang T. Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_S/0/1/0/all/0/1&quot;&gt;Simran Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1&quot;&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zinan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dawn Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05209">
<title>Contextual Pre-Planning on Reward Machine Abstractions for Enhanced Transfer in Deep Reinforcement Learning. (arXiv:2307.05209v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05209</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies show that deep reinforcement learning (DRL) agents tend to
overfit to the task on which they were trained and fail to adapt to minor
environment changes. To expedite learning when transferring to unseen tasks, we
propose a novel approach to representing the current task using reward machines
(RMs), state machine abstractions that induce subtasks based on the current
task&apos;s rewards and dynamics. Our method provides agents with symbolic
representations of optimal transitions from their current abstract state and
rewards them for achieving these transitions. These representations are shared
across tasks, allowing agents to exploit knowledge of previously encountered
symbols and transitions, thus enhancing transfer. Empirical results show that
our representations improve sample efficiency and few-shot transfer in a
variety of domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azran_G/0/1/0/all/0/1&quot;&gt;Guy Azran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danesh_M/0/1/0/all/0/1&quot;&gt;Mohamad H. Danesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keren_S/0/1/0/all/0/1&quot;&gt;Sarah Keren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08742">
<title>PMET: Precise Model Editing in a Transformer. (arXiv:2308.08742v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08742</link>
<description rdf:parseType="Literal">&lt;p&gt;Model editing techniques modify a minor proportion of knowledge in Large
Language Models (LLMs) at a relatively low cost, which have demonstrated
notable success. Existing methods assume Transformer Layer (TL) hidden states
are values of key-value memories of the Feed-Forward Network (FFN). They
usually optimize the TL hidden states to memorize target knowledge and use it
to update the weights of the FFN in LLMs. However, the information flow of TL
hidden states comes from three parts: Multi-Head Self-Attention (MHSA), FFN,
and residual connections. Existing methods neglect the fact that the TL hidden
states contains information not specifically required for FFN. Consequently,
the performance of model editing decreases. To achieve more precise model
editing, we analyze hidden states of MHSA and FFN, finding that MHSA encodes
certain general knowledge extraction patterns. This implies that MHSA weights
do not require updating when new knowledge is introduced. Based on above
findings, we introduce PMET, which simultaneously optimizes Transformer
Component (TC, namely MHSA and FFN) hidden states, while only using the
optimized TC hidden states of FFN to precisely update FFN weights. Our
experiments demonstrate that PMET exhibits state-of-the-art performance on both
the COUNTERFACT and zsRE datasets. Our ablation experiments substantiate the
effectiveness of our enhancements, further reinforcing the finding that the
MHSA encodes certain general knowledge extraction patterns and indicating its
storage of a small amount of factual knowledge. Our code is available at
https://github.com/xpq-tech/PMET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shasha Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shezheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jie Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09128">
<title>ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing. (arXiv:2309.09128v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09128</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating outputs of large language models (LLMs) is challenging, requiring
making -- and making sense of -- many responses. Yet tools that go beyond basic
prompting tend to require knowledge of programming APIs, focus on narrow
domains, or are closed-source. We present ChainForge, an open-source visual
toolkit for prompt engineering and on-demand hypothesis testing of text
generation LLMs. ChainForge provides a graphical interface for comparison of
responses across models and prompt variations. Our system was designed to
support three tasks: model selection, prompt template design, and hypothesis
testing (e.g., auditing). We released ChainForge early in its development and
iterated on its design with academics and online users. Through in-lab and
interview studies, we find that a range of people could use ChainForge to
investigate hypotheses that matter to them, including in real-world settings.
We identify three modes of prompt engineering and LLM hypothesis testing:
opportunistic exploration, limited evaluation, and iterative refinement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arawjo_I/0/1/0/all/0/1&quot;&gt;Ian Arawjo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swoopes_C/0/1/0/all/0/1&quot;&gt;Chelse Swoopes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaithilingam_P/0/1/0/all/0/1&quot;&gt;Priyan Vaithilingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1&quot;&gt;Martin Wattenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glassman_E/0/1/0/all/0/1&quot;&gt;Elena Glassman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13059">
<title>Beyond Traditional Teaching: The Potential of Large Language Models and Chatbots in Graduate Engineering Education. (arXiv:2309.13059v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13059</link>
<description rdf:parseType="Literal">&lt;p&gt;In the rapidly evolving landscape of education, digital technologies have
repeatedly disrupted traditional pedagogical methods. This paper explores the
latest of these disruptions: the potential integration of large language models
(LLMs) and chatbots into graduate engineering education. We begin by tracing
historical and technological disruptions to provide context and then introduce
key terms such as machine learning and deep learning and the underlying
mechanisms of recent advancements, namely attention/transformer models and
graphics processing units. The heart of our investigation lies in the
application of an LLM-based chatbot in a graduate fluid mechanics course. We
developed a question bank from the course material and assessed the chatbot&apos;s
ability to provide accurate, insightful responses. The results are encouraging,
demonstrating not only the bot&apos;s ability to effectively answer complex
questions but also the potential advantages of chatbot usage in the classroom,
such as the promotion of self-paced learning, the provision of instantaneous
feedback, and the reduction of instructors&apos; workload. The study also examines
the transformative effect of intelligent prompting on enhancing the chatbot&apos;s
performance. Furthermore, we demonstrate how powerful plugins like Wolfram
Alpha for mathematical problem-solving and code interpretation can
significantly extend the chatbot&apos;s capabilities, transforming it into a
comprehensive educational tool. While acknowledging the challenges and ethical
implications surrounding the use of such AI models in education, we advocate
for a balanced approach. The use of LLMs and chatbots in graduate education can
be greatly beneficial but requires ongoing evaluation and adaptation to ensure
ethical and efficient use.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abedi_M/0/1/0/all/0/1&quot;&gt;Mahyar Abedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshybani_I/0/1/0/all/0/1&quot;&gt;Ibrahem Alshybani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahadat_M/0/1/0/all/0/1&quot;&gt;Muhammad Rubayat Bin Shahadat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murillo_M/0/1/0/all/0/1&quot;&gt;Michael S. Murillo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15312">
<title>MAPTree: Beating &quot;Optimal&quot; Decision Trees with Bayesian Decision Trees. (arXiv:2309.15312v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15312</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision trees remain one of the most popular machine learning models today,
largely due to their out-of-the-box performance and interpretability. In this
work, we present a Bayesian approach to decision tree induction via maximum a
posteriori inference of a posterior distribution over trees. We first
demonstrate a connection between maximum a posteriori inference of decision
trees and AND/OR search. Using this connection, we propose an AND/OR search
algorithm, dubbed MAPTree, which is able to recover the maximum a posteriori
tree. Lastly, we demonstrate the empirical performance of the maximum a
posteriori tree both on synthetic data and in real world settings. On 16 real
world datasets, MAPTree either outperforms baselines or demonstrates comparable
performance but with much smaller trees. On a synthetic dataset, MAPTree also
demonstrates greater robustness to noise and better generalization than
existing approaches. Finally, MAPTree recovers the maxiumum a posteriori tree
faster than existing sampling approaches and, in contrast with those
algorithms, is able to provide a certificate of optimality. The code for our
experiments is available at https://github.com/ThrunGroup/maptree.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_C/0/1/0/all/0/1&quot;&gt;Colin Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1&quot;&gt;Mo Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrun_S/0/1/0/all/0/1&quot;&gt;Sebastian Thrun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17255">
<title>Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17255</link>
<description rdf:parseType="Literal">&lt;p&gt;The term life sciences refers to the disciplines that study living organisms
and life processes, and include chemistry, biology, medicine, and a range of
other related disciplines. Research efforts in life sciences are heavily
data-driven, as they produce and consume vast amounts of scientific data, much
of which is intrinsically relational and graph-structured.
&lt;/p&gt;
&lt;p&gt;The volume of data and the complexity of scientific concepts and relations
referred to therein promote the application of advanced knowledge-driven
technologies for managing and interpreting data, with the ultimate aim to
advance scientific discovery.
&lt;/p&gt;
&lt;p&gt;In this survey and position paper, we discuss recent developments and
advances in the use of graph-based technologies in life sciences and set out a
vision for how these technologies will impact these fields into the future. We
focus on three broad topics: the construction and management of Knowledge
Graphs (KGs), the use of KGs and associated technologies in the discovery of
new knowledge, and the use of KGs in artificial intelligence applications to
support explanations (explainable AI). We select a few exemplary use cases for
each topic, discuss the challenges and open research questions within these
topics, and conclude with a perspective and outlook that summarizes the
overarching challenges and their potential solutions as a guide for future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hastings_J/0/1/0/all/0/1&quot;&gt;Janna Hastings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1&quot;&gt;Ernesto Jim&amp;#xe9;nez-Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_V/0/1/0/all/0/1&quot;&gt;Vanessa L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1&quot;&gt;Pierre Monnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pesquita_C/0/1/0/all/0/1&quot;&gt;Catia Pesquita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skoda_P/0/1/0/all/0/1&quot;&gt;Petr &amp;#x160;koda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamma_V/0/1/0/all/0/1&quot;&gt;Valentina Tamma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04469">
<title>Taming Binarized Neural Networks and Mixed-Integer Programs. (arXiv:2310.04469v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04469</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been a great deal of recent interest in binarized neural networks,
especially because of their explainability. At the same time, automatic
differentiation algorithms such as backpropagation fail for binarized neural
networks, which limits their applicability. By reformulating the problem of
training binarized neural networks as a subadditive dual of a mixed-integer
program, we show that binarized neural networks admit a tame representation.
This, in turn, makes it possible to use the framework of Bolte et al. for
implicit differentiation, which offers the possibility for practical
implementation of backpropagation in the context of binarized neural networks.
&lt;/p&gt;
&lt;p&gt;This approach could also be used for a broader class of mixed-integer
programs, beyond the training of binarized neural networks, as encountered in
symbolic approaches to AI and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aspman_J/0/1/0/all/0/1&quot;&gt;Johannes Aspman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korpas_G/0/1/0/all/0/1&quot;&gt;Georgios Korpas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marecek_J/0/1/0/all/0/1&quot;&gt;Jakub Marecek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09688">
<title>Recursively-Constrained Partially Observable Markov Decision Processes. (arXiv:2310.09688v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09688</link>
<description rdf:parseType="Literal">&lt;p&gt;In many problems, it is desirable to optimize an objective function while
imposing constraints on some other objectives. A Constrained Partially
Observable Markov Decision Process (C-POMDP) allows modeling of such problems
under transition uncertainty and partial observability. Typically, the
constraints in C-POMDPs enforce a threshold on expected cumulative costs
starting from an initial state distribution. In this work, we first show that
optimal C-POMDP policies may violate Bellman&apos;s principle of optimality and thus
may exhibit unintuitive behaviors, which can be undesirable for some (e.g.,
safety critical) applications. Additionally, online re-planning with C-POMDPs
is often ineffective due to the inconsistency resulting from the violation of
Bellman&apos;s principle of optimality. To address these drawbacks, we introduce a
new formulation: the Recursively-Constrained POMDP (RC-POMDP), that imposes
additional history-dependent cost constraints on the C-POMDP. We show that,
unlike C-POMDPs, RC-POMDPs always have deterministic optimal policies, and that
optimal policies obey Bellman&apos;s principle of optimality. We also present a
point-based dynamic programming algorithm that synthesizes admissible
near-optimal policies for RC-POMDPs. Evaluations on a set of benchmark problems
demonstrate the efficacy of our algorithm and show that policies for RC-POMDPs
produce more desirable behaviors than policies for C-POMDPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_Q/0/1/0/all/0/1&quot;&gt;Qi Heng Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_T/0/1/0/all/0/1&quot;&gt;Tyler Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraske_B/0/1/0/all/0/1&quot;&gt;Benjamin Kraske&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laouar_Z/0/1/0/all/0/1&quot;&gt;Zakariya Laouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feather_M/0/1/0/all/0/1&quot;&gt;Martin S. Feather&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_F/0/1/0/all/0/1&quot;&gt;Federico Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lahijanian_M/0/1/0/all/0/1&quot;&gt;Morteza Lahijanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunberg_Z/0/1/0/all/0/1&quot;&gt;Zachary N. Sunberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08206">
<title>Human-Centric Autonomous Systems With LLMs for User Command Reasoning. (arXiv:2311.08206v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08206</link>
<description rdf:parseType="Literal">&lt;p&gt;The evolution of autonomous driving has made remarkable advancements in
recent years, evolving into a tangible reality. However, a human-centric
large-scale adoption hinges on meeting a variety of multifaceted requirements.
To ensure that the autonomous system meets the user&apos;s intent, it is essential
to accurately discern and interpret user commands, especially in complex or
emergency situations. To this end, we propose to leverage the reasoning
capabilities of Large Language Models (LLMs) to infer system requirements from
in-cabin users&apos; commands. Through a series of experiments that include
different LLM models and prompt designs, we explore the few-shot multivariate
binary classification accuracy of system requirements from natural language
textual commands. We confirm the general ability of LLMs to understand and
reason about prompts but underline that their effectiveness is conditioned on
the quality of both the LLM model and the design of appropriate sequential
prompts. Code and models are public with the link
\url{https://github.com/KTH-RPL/DriveCmd_LLM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Ci Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marta_D/0/1/0/all/0/1&quot;&gt;Daniel Sim&amp;#xf5;es Marta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batool_N/0/1/0/all/0/1&quot;&gt;Nazre Batool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Folkesson_J/0/1/0/all/0/1&quot;&gt;John Folkesson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12420">
<title>How Far Have We Gone in Vulnerability Detection Using Large Language Models. (arXiv:2311.12420v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12420</link>
<description rdf:parseType="Literal">&lt;p&gt;As software becomes increasingly complex and prone to vulnerabilities,
automated vulnerability detection is critically important, yet challenging.
Given the significant successes of large language models (LLMs) in various
tasks, there is growing anticipation of their efficacy in vulnerability
detection. However, a quantitative understanding of their potential in
vulnerability detection is still missing. To bridge this gap, we introduce a
comprehensive vulnerability benchmark VulBench. This benchmark aggregates
high-quality data from a wide range of CTF (Capture-the-Flag) challenges and
real-world applications, with annotations for each vulnerable function
detailing the vulnerability type and its root cause. Through our experiments
encompassing 16 LLMs and 6 state-of-the-art (SOTA) deep learning-based models
and static analyzers, we find that several LLMs outperform traditional deep
learning approaches in vulnerability detection, revealing an untapped potential
in LLMs. This work contributes to the understanding and utilization of LLMs for
enhanced software security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zeyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16716">
<title>GraphPro: Graph Pre-training and Prompt Learning for Recommendation. (arXiv:2311.16716v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16716</link>
<description rdf:parseType="Literal">&lt;p&gt;GNN-based recommenders have excelled in modeling intricate user-item
interactions through multi-hop message passing. However, existing methods often
overlook the dynamic nature of evolving user-item interactions, which impedes
the adaption to changing user preferences and distribution shifts in newly
arriving data. Thus, their scalability and performances in real-world dynamic
environments are limited. In this study, we propose GraphPro, a framework that
incorporates parameter-efficient and dynamic graph pre-training with prompt
learning. This novel combination empowers GNNs to effectively capture both
long-term user preferences and short-term behavior dynamics, enabling the
delivery of accurate and timely recommendations. Our GraphPro framework
addresses the challenge of evolving user preferences by seamlessly integrating
a temporal prompt mechanism and a graph-structural prompt learning mechanism
into the pre-trained GNN model. The temporal prompt mechanism encodes time
information on user-item interaction, allowing the model to naturally capture
temporal context, while the graph-structural prompt learning mechanism enables
the transfer of pre-trained knowledge to adapt to behavior dynamics without the
need for continuous incremental training. We further bring in a dynamic
evaluation setting for recommendation to mimic real-world dynamic scenarios and
bridge the offline-online gap to a better level. Our extensive experiments
including a large-scale industrial deployment showcases the lightweight plug-in
scalability of our GraphPro when integrated with various state-of-the-art
recommenders, emphasizing the advantages of GraphPro in terms of effectiveness,
robustness and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Lianghao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Da Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kangyi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03719">
<title>Assessing AI Chatbots Performance in Comprehensive Standardized Test Preparation; A Case Study with GRE. (arXiv:2312.03719v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03719</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper presents a comprehensive evaluation of the performance of
three artificial 10 intelligence chatbots: Bing, ChatGPT, and GPT-4, in
addressing standardized test questions. Graduate record examination, known as
GRE, serves as a case study in this paper, encompassing both quantitative
reasoning and verbal skills. A total of 137 quantitative reasoning questions,
featuring diverse styles and 157 verbal questions categorized into varying
levels of difficulty (easy, medium, and hard) were administered to assess the
chatbots&apos; capabilities. This paper provides a detailed examination of the
results and their implications for the utilization of artificial intelligence
in standardized test preparation by presenting the performance of each chatbot
across various skills and styles tested in the exam. Additionally, this paper
explores the proficiency of artificial intelligence in addressing image-based
questions and illustrates the uncertainty level of each chatbot. The results
reveal varying degrees of success across the chatbots, demonstrating the
influence of model sophistication and training data. GPT-4 emerged as the most
proficient, especially in complex language understanding tasks, highlighting
the evolution of artificial intelligence in language comprehension and its
ability to pass the exam with a high score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_Haifa_M/0/1/0/all/0/1&quot;&gt;Mohammad Abu-Haifa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etawi_B/0/1/0/all/0/1&quot;&gt;Bara&amp;#x27;a Etawi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkhatatbeh_H/0/1/0/all/0/1&quot;&gt;Huthaifa Alkhatatbeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ababneh_A/0/1/0/all/0/1&quot;&gt;Ayman Ababneh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05614">
<title>Transformer as Linear Expansion of Learngene. (arXiv:2312.05614v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.05614</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose expanding the shared Transformer module to produce and initialize
Transformers of varying depths, enabling adaptation to diverse resource
constraints. Drawing an analogy to genetic expansibility, we term such module
as learngene. To identify the expansion mechanism, we delve into the
relationship between the layer&apos;s position and its corresponding weight value,
and find that linear function appropriately approximates this relationship.
Building on this insight, we present Transformer as Linear Expansion of
learnGene (TLEG), a novel approach for flexibly producing and initializing
Transformers of diverse depths. Specifically, to learn learngene, we firstly
construct an auxiliary Transformer linearly expanded from learngene, after
which we train it through employing soft distillation. Subsequently, we can
produce and initialize Transformers of varying depths via linearly expanding
the well-trained learngene, thereby supporting diverse downstream scenarios.
Extensive experiments on ImageNet-1K demonstrate that TLEG achieves comparable
or better performance in contrast to many individual models trained from
scratch, while reducing around 2x training cost. When transferring to several
downstream classification datasets, TLEG surpasses existing initialization
methods by a large margin (e.g., +6.87% on iNat 2019 and +7.66% on CIFAR-100).
Under the situation where we need to produce models of varying depths adapting
for different resource constraints, TLEG achieves comparable results while
reducing around 19x parameters stored to initialize these models and around 5x
pre-training costs, in contrast to the pre-training and fine-tuning approach.
When transferring a fixed set of parameters to initialize different models,
TLEG presents better flexibility and competitive performance while reducing
around 2.9x parameters stored to initialize, compared to the pre-training
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shiyu Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Miaosen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haokun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07392">
<title>ReRoGCRL: Representation-based Robustness in Goal-Conditioned Reinforcement Learning. (arXiv:2312.07392v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07392</link>
<description rdf:parseType="Literal">&lt;p&gt;While Goal-Conditioned Reinforcement Learning (GCRL) has gained attention,
its algorithmic robustness against adversarial perturbations remains
unexplored. The attacks and robust representation training methods that are
designed for traditional RL become less effective when applied to GCRL. To
address this challenge, we first propose the Semi-Contrastive Representation
attack, a novel approach inspired by the adversarial contrastive attack. Unlike
existing attacks in RL, it only necessitates information from the policy
function and can be seamlessly implemented during deployment. Then, to mitigate
the vulnerability of existing GCRL algorithms, we introduce Adversarial
Representation Tactics, which combines Semi-Contrastive Adversarial
Augmentation with Sensitivity-Aware Regularizer to improve the adversarial
robustness of the underlying RL agent against various types of perturbations.
Extensive experiments validate the superior performance of our attack and
defence methods across multiple state-of-the-art GCRL algorithms. Our tool
ReRoGCRL is available at https://github.com/TrustAI/ReRoGCRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_W/0/1/0/all/0/1&quot;&gt;Wenjie Ruan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07879">
<title>CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation. (arXiv:2312.07879v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07879</link>
<description rdf:parseType="Literal">&lt;p&gt;Current text-to-image editing models often encounter challenges with smoothly
manipulating multiple attributes using a single instruction. Taking inspiration
from the Chain-of-Thought prompting technique utilized in language models, we
present an innovative concept known as Chain-of-Instruct Editing (CoIE), which
enhances the capabilities of these models through step-by-step editing using a
series of instructions. In particular, in the context of face manipulation, we
leverage the contextual learning abilities of a pretrained Large Language Model
(LLM), such as GPT-4, to generate a sequence of instructions from the original
input, utilizing a purpose-designed 1-shot template. To further improve the
precision of each editing step, we conduct fine-tuning on the editing models
using our self-constructed instruction-guided face editing dataset,
Instruct-CelebA. And additionally, we incorporate a super-resolution module to
mitigate the adverse effects of editability and quality degradation.
Experimental results across various challenging cases confirm the significant
boost in multi-attribute facial image manipulation using chain-of-instruct
editing. This is evident in enhanced editing success rates, measured by CLIPSim
and Coverage metrics, improved by 17.86% and 85.45% respectively, and
heightened controllability indicated by Preserve L1 and Quality metrics,
improved by 11.58% and 4.93% respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenduo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo-Wen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08403">
<title>Earthfarseer: Versatile Spatio-Temporal Dynamical Systems Modeling in One Model. (arXiv:2312.08403v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08403</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently modeling spatio-temporal (ST) physical processes and observations
presents a challenging problem for the deep learning community. Many recent
studies have concentrated on meticulously reconciling various advantages,
leading to designed models that are neither simple nor practical. To address
this issue, this paper presents a systematic study on existing shortcomings
faced by off-the-shelf models, including lack of local fidelity, poor
prediction performance over long time-steps,low scalability, and inefficiency.
To systematically address the aforementioned problems, we propose an
EarthFarseer, a concise framework that combines parallel local convolutions and
global Fourier-based transformer architectures, enabling dynamically capture
the local-global spatial interactions and dependencies. EarthFarseer also
incorporates a multi-scale fully convolutional and Fourier architectures to
efficiently and effectively capture the temporal evolution. Our proposal
demonstrates strong adaptability across various tasks and datasets, with fast
convergence and better local fidelity in long time-steps predictions. Extensive
experiments and visualizations over eight human society physical and natural
physical datasets demonstrates the state-of-the-art performance of
EarthFarseer. We release our code at
https://github.com/easylearningscores/EarthFarseer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09085">
<title>The Earth is Flat because...: Investigating LLMs&apos; Belief towards Misinformation via Persuasive Conversation. (arXiv:2312.09085v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09085</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) encapsulate vast amounts of knowledge but still
remain vulnerable to external misinformation. Existing research mainly studied
this susceptibility behavior in a single-turn setting. However, belief can
change during a multi-turn conversation, especially a persuasive one.
Therefore, in this study, we delve into LLMs&apos; susceptibility to persuasive
conversations, particularly on factual questions that they can answer
correctly. We first curate the Farm (i.e., Fact to Misinform) dataset, which
contains factual questions paired with systematically generated persuasive
misinformation. Then, we develop a testing framework to track LLMs&apos; belief
changes in a persuasive dialogue. Through extensive experiments, we find that
LLMs&apos; correct beliefs on factual knowledge can be easily manipulated by various
persuasive strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rongwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Brian S. Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shujian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weiyan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhixuan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09513">
<title>CGS-Mask: Making Time Series Predictions Intuitive for All. (arXiv:2312.09513v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09513</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) has immense potential in time series prediction,
but most explainable tools have limited capabilities in providing a systematic
understanding of important features over time. These tools typically rely on
evaluating a single time point, overlook the time ordering of inputs, and
neglect the time-sensitive nature of time series applications. These factors
make it difficult for users, particularly those without domain knowledge, to
comprehend AI model decisions and obtain meaningful explanations. We propose
CGS-Mask, a post-hoc and model-agnostic cellular genetic strip mask-based
saliency approach to address these challenges. CGS-Mask uses consecutive time
steps as a cohesive entity to evaluate the impact of features on the final
prediction, providing binary and sustained feature importance scores over time.
Our algorithm optimizes the mask population iteratively to obtain the optimal
mask in a reasonable time. We evaluated CGS-Mask on synthetic and real-world
datasets, and it outperformed state-of-the-art methods in elucidating the
importance of features over time. According to our pilot user study via a
questionnaire survey, CGS-Mask is the most effective approach in presenting
easily understandable time series prediction results, enabling users to
comprehend the decision-making process of AI models with ease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1&quot;&gt;Feng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Cheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yufei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zomaya_A/0/1/0/all/0/1&quot;&gt;Albert Y. Zomaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10080">
<title>No prejudice! Fair Federated Graph Neural Networks for Personalized Recommendation. (arXiv:2312.10080v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10080</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring fairness in Recommendation Systems (RSs) across demographic groups
is critical due to the increased integration of RSs in applications such as
personalized healthcare, finance, and e-commerce. Graph-based RSs play a
crucial role in capturing intricate higher-order interactions among entities.
However, integrating these graph models into the Federated Learning (FL)
paradigm with fairness constraints poses formidable challenges as this requires
access to the entire interaction graph and sensitive user information (such as
gender, age, etc.) at the central server. This paper addresses the pervasive
issue of inherent bias within RSs for different demographic groups without
compromising the privacy of sensitive user attributes in FL environment with
the graph-based model. To address the group bias, we propose F2PGNN (Fair
Federated Personalized Graph Neural Network), a novel framework that leverages
the power of Personalized Graph Neural Network (GNN) coupled with fairness
considerations. Additionally, we use differential privacy techniques to fortify
privacy protection. Experimental evaluation on three publicly available
datasets showcases the efficacy of F2PGNN in mitigating group unfairness by 47%
- 99% compared to the state-of-the-art while preserving privacy and maintaining
the utility. The results validate the significance of our framework in
achieving equitable and personalized recommendations using GNN within the FL
landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_N/0/1/0/all/0/1&quot;&gt;Nimesh Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirohi_A/0/1/0/all/0/1&quot;&gt;Anuj Kumar Sirohi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayadeva/0/1/0/all/0/1&quot;&gt;Jayadeva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sandeep Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10589">
<title>NN-Steiner: A Mixed Neural-algorithmic Approach for the Rectilinear Steiner Minimum Tree Problem. (arXiv:2312.10589v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10589</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed rapid advances in the use of neural networks to
solve combinatorial optimization problems. Nevertheless, designing the &quot;right&quot;
neural model that can effectively handle a given optimization problem can be
challenging, and often there is no theoretical understanding or justification
of the resulting neural model. In this paper, we focus on the rectilinear
Steiner minimum tree (RSMT) problem, which is of critical importance in IC
layout design and as a result has attracted numerous heuristic approaches in
the VLSI literature. Our contributions are two-fold. On the methodology front,
we propose NN-Steiner, which is a novel mixed neural-algorithmic framework for
computing RSMTs that leverages the celebrated PTAS algorithmic framework of
Arora to solve this problem (and other geometric optimization problems). Our
NN-Steiner replaces key algorithmic components within Arora&apos;s PTAS by suitable
neural components. In particular, NN-Steiner only needs four neural network
(NN) components that are called repeatedly within an algorithmic framework.
Crucially, each of the four NN components is only of bounded size independent
of input size, and thus easy to train. Furthermore, as the NN component is
learning a generic algorithmic step, once learned, the resulting mixed
neural-algorithmic framework generalizes to much larger instances not seen in
training. Our NN-Steiner, to our best knowledge, is the first neural
architecture of bounded size that has capacity to approximately solve RSMT (and
variants). On the empirical front, we show how NN-Steiner can be implemented
and demonstrate the effectiveness of our resulting approach, especially in
terms of generalization, by comparing with state-of-the-art methods (both
neural and non-neural based).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahng_A/0/1/0/all/0/1&quot;&gt;Andrew B. Kahng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nerem_R/0/1/0/all/0/1&quot;&gt;Robert R. Nerem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yusu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chien-Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11057">
<title>DataElixir: Purifying Poisoned Dataset to Mitigate Backdoor Attacks via Diffusion Models. (arXiv:2312.11057v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11057</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset sanitization is a widely adopted proactive defense against
poisoning-based backdoor attacks, aimed at filtering out and removing poisoned
samples from training datasets. However, existing methods have shown limited
efficacy in countering the ever-evolving trigger functions, and often leading
to considerable degradation of benign accuracy. In this paper, we propose
DataElixir, a novel sanitization approach tailored to purify poisoned datasets.
We leverage diffusion models to eliminate trigger features and restore benign
features, thereby turning the poisoned samples into benign ones. Specifically,
with multiple iterations of the forward and reverse process, we extract
intermediary images and their predicted labels for each sample in the original
dataset. Then, we identify anomalous samples in terms of the presence of label
transition of the intermediary images, detect the target label by quantifying
distribution discrepancy, select their purified images considering pixel and
feature distance, and determine their ground-truth labels by training a benign
model. Experiments conducted on 9 popular attacks demonstrates that DataElixir
effectively mitigates various complex attacks while exerting minimal impact on
benign accuracy, surpassing the performance of baseline defense methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiachen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_P/0/1/0/all/0/1&quot;&gt;Peizhuo Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Y/0/1/0/all/0/1&quot;&gt;Yibing Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1&quot;&gt;Guozhu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hualong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11143">
<title>Learning Domain-Independent Heuristics for Grounded and Lifted Planning. (arXiv:2312.11143v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11143</link>
<description rdf:parseType="Literal">&lt;p&gt;We present three novel graph representations of planning tasks suitable for
learning domain-independent heuristics using Graph Neural Networks (GNNs) to
guide search. In particular, to mitigate the issues caused by large grounded
GNNs we present the first method for learning domain-independent heuristics
with only the lifted representation of a planning task. We also provide a
theoretical analysis of the expressiveness of our models, showing that some are
more powerful than STRIPS-HGN, the only other existing model for learning
domain-independent heuristics. Our experiments show that our heuristics
generalise to much larger problems than those in the training set, vastly
surpassing STRIPS-HGN heuristics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dillon Z. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiebaux_S/0/1/0/all/0/1&quot;&gt;Sylvie Thi&amp;#xe9;baux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trevizan_F/0/1/0/all/0/1&quot;&gt;Felipe Trevizan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11193">
<title>&quot;Paraphrasing The Original Text&quot; Makes High Accuracy Long-Context QA. (arXiv:2312.11193v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11193</link>
<description rdf:parseType="Literal">&lt;p&gt;Although LLMs continue to iterate and improve, most open-source models still
have a context window of no more than 4k, limiting their ability to handle
long-context problems. Most existing open-source models for long-context chat
still lack satisfactory accuracy. To address this issue, I approach it from the
perspective of training data and theoretically prove that training the
capability to handle long contexts requires &quot;effective&quot; rather than &quot;long&quot;
data. Based on this, I propose using the &quot;original text paraphrase&quot; task, and
successfully extend the context window of the existing model to 32k by a
low-cost and effective method, achieving extremely high accuracy in
multi-document-QA and surpassing all existing open-source models of the same
scale. The model and training data have been open-sourced on
HuggingFace(https://huggingface.co/yuyijiong/Qwen-14b-chat-yarn-32k) and
WiseModel(https://wisemodel.cn/models/yuyijiong/Qwen-14b-chat-yarn-32k).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yijiong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11562">
<title>A Survey of Reasoning with Foundation Models: Concepts, Methodologies, and Outlook. (arXiv:2312.11562v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11562</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning, a crucial ability for complex problem-solving, plays a pivotal
role in various real-world settings such as negotiation, medical diagnosis, and
criminal investigation. It serves as a fundamental methodology in the field of
Artificial General Intelligence (AGI). With the ongoing development of
foundation models, there is a growing interest in exploring their abilities in
reasoning tasks. In this paper, we introduce seminal foundation models proposed
or adaptable for reasoning, highlighting the latest advancements in various
reasoning tasks, methods, and benchmarks. We then delve into the potential
future directions behind the emergence of reasoning abilities within foundation
models. We also discuss the relevance of multimodal learning, autonomous
agents, and super alignment in the context of reasoning. By discussing these
future research directions, we hope to inspire researchers in their exploration
of this field, stimulate further advancements in reasoning with foundation
models, and contribute to the development of AGI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiankai Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_R/0/1/0/all/0/1&quot;&gt;Ruihang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jianing Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Mingyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_M/0/1/0/all/0/1&quot;&gt;Mengzhe Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junsong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhangyue Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junxian He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wu Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingdong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yike Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11675">
<title>PRP Rebooted: Advancing the State of the Art in FOND Planning. (arXiv:2312.11675v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11675</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully Observable Non-Deterministic (FOND) planning is a variant of classical
symbolic planning in which actions are nondeterministic, with an action&apos;s
outcome known only upon execution. It is a popular planning paradigm with
applications ranging from robot planning to dialogue-agent design and reactive
synthesis. Over the last 20 years, a number of approaches to FOND planning have
emerged. In this work, we establish a new state of the art, following in the
footsteps of some of the most powerful FOND planners to date. Our planner, PR2,
decisively outperforms the four leading FOND planners, at times by a large
margin, in 17 of 18 domains that represent a comprehensive benchmark suite.
Ablation studies demonstrate the impact of various techniques we introduce,
with the largest improvement coming from our novel FOND-aware heuristic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muise_C/0/1/0/all/0/1&quot;&gt;Christian Muise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1&quot;&gt;Sheila A. McIlraith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beck_J/0/1/0/all/0/1&quot;&gt;J. Christopher Beck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11681">
<title>Designing LLM Chains by Adapting Techniques from Crowdsourcing Workflows. (arXiv:2312.11681v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11681</link>
<description rdf:parseType="Literal">&lt;p&gt;LLM chains enable complex tasks by decomposing work into a sequence of
sub-tasks. Crowdsourcing workflows similarly decompose complex tasks into
smaller tasks for human crowdworkers. Chains address LLM errors analogously to
the way crowdsourcing workflows address human error. To characterize
opportunities for LLM chaining, we survey 107 papers across the crowdsourcing
and chaining literature to construct a design space for chain development. The
design space connects an LLM designer&apos;s objectives to strategies they can use
to achieve those objectives, and tactics to implement each strategy. To explore
how techniques from crowdsourcing may apply to chaining, we adapt crowdsourcing
workflows to implement LLM chains across three case studies: creating a
taxonomy, shortening text, and writing a short story. From the design space and
our case studies, we identify which techniques transfer from crowdsourcing to
LLM chaining and raise implications for future research and development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grunde_McLaughlin_M/0/1/0/all/0/1&quot;&gt;Madeleine Grunde-McLaughlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1&quot;&gt;Michelle S. Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weld_D/0/1/0/all/0/1&quot;&gt;Daniel S. Weld&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heer_J/0/1/0/all/0/1&quot;&gt;Jeffrey Heer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11831">
<title>Locally-Minimal Probabilistic Explanations. (arXiv:2312.11831v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11831</link>
<description rdf:parseType="Literal">&lt;p&gt;Formal abductive explanations offer crucial guarantees of rigor and so are of
interest in high-stakes uses of machine learning (ML). One drawback of
abductive explanations is explanation size, justified by the cognitive limits
of human decision-makers. Probabilistic abductive explanations (PAXps) address
this limitation, but their theoretical and practical complexity makes their
exact computation most often unrealistic. This paper proposes novel efficient
algorithms for the computation of locally-minimal PXAps, which offer
high-quality approximations of PXAps in practice. The experimental results
demonstrate the practical efficiency of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1&quot;&gt;Yacine Izza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1&quot;&gt;Kuldeep S. Meel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1&quot;&gt;Joao Marques-Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12010">
<title>Outlier detection using flexible categorisation and interrogative agendas. (arXiv:2312.12010v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12010</link>
<description rdf:parseType="Literal">&lt;p&gt;Categorization is one of the basic tasks in machine learning and data
analysis. Building on formal concept analysis (FCA), the starting point of the
present work is that different ways to categorize a given set of objects exist,
which depend on the choice of the sets of features used to classify them, and
different such sets of features may yield better or worse categorizations,
relative to the task at hand. In their turn, the (a priori) choice of a
particular set of features over another might be subjective and express a
certain epistemic stance (e.g. interests, relevance, preferences) of an agent
or a group of agents, namely, their interrogative agenda. In the present paper,
we represent interrogative agendas as sets of features, and explore and compare
different ways to categorize objects w.r.t. different sets of features
(agendas). We first develop a simple unsupervised FCA-based algorithm for
outlier detection which uses categorizations arising from different agendas. We
then present a supervised meta-learning algorithm to learn suitable (fuzzy)
agendas for categorization as sets of features with different weights or
masses. We combine this meta-learning algorithm with the unsupervised outlier
detection algorithm to obtain a supervised outlier detection algorithm. We show
that these algorithms perform at par with commonly used algorithms for outlier
detection on commonly used datasets in outlier detection. These algorithms
provide both local and global explanations of their results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boersma_M/0/1/0/all/0/1&quot;&gt;Marcel Boersma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manoorkar_K/0/1/0/all/0/1&quot;&gt;Krishna Manoorkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmigiano_A/0/1/0/all/0/1&quot;&gt;Alessandra Palmigiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panettiere_M/0/1/0/all/0/1&quot;&gt;Mattia Panettiere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzimoulis_A/0/1/0/all/0/1&quot;&gt;Apostolos Tzimoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijnberg_N/0/1/0/all/0/1&quot;&gt;Nachoem Wijnberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12036">
<title>LHManip: A Dataset for Long-Horizon Language-Grounded Manipulation Tasks in Cluttered Tabletop Environments. (arXiv:2312.12036v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12036</link>
<description rdf:parseType="Literal">&lt;p&gt;Instructing a robot to complete an everyday task within our homes has been a
long-standing challenge for robotics. While recent progress in
language-conditioned imitation learning and offline reinforcement learning has
demonstrated impressive performance across a wide range of tasks, they are
typically limited to short-horizon tasks -- not reflective of those a home
robot would be expected to complete. While existing architectures have the
potential to learn these desired behaviours, the lack of the necessary
long-horizon, multi-step datasets for real robotic systems poses a significant
challenge. To this end, we present the Long-Horizon Manipulation (LHManip)
dataset comprising 200 episodes, demonstrating 20 different manipulation tasks
via real robot teleoperation. The tasks entail multiple sub-tasks, including
grasping, pushing, stacking and throwing objects in highly cluttered
environments. Each task is paired with a natural language instruction and
multi-camera viewpoints for point-cloud or NeRF reconstruction. In total, the
dataset comprises 176,278 observation-action pairs which form part of the Open
X-Embodiment dataset. The full LHManip dataset is made publicly available at
https://github.com/fedeceola/LHManip.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ceola_F/0/1/0/all/0/1&quot;&gt;Federico Ceola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natale_L/0/1/0/all/0/1&quot;&gt;Lorenzo Natale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunderhauf_N/0/1/0/all/0/1&quot;&gt;Niko S&amp;#xfc;nderhauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rana_K/0/1/0/all/0/1&quot;&gt;Krishan Rana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12037">
<title>Founder-GPT: Self-play to evaluate the Founder-Idea fit. (arXiv:2312.12037v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12037</link>
<description rdf:parseType="Literal">&lt;p&gt;This research introduces an innovative evaluation method for the
&quot;founder-idea&quot; fit in early-stage startups, utilizing advanced large language
model techniques to assess founders&apos; profiles against their startup ideas to
enhance decision-making. Embeddings, self-play, tree-of-thought, and
critique-based refinement techniques show early promising results that each
idea&apos;s success patterns are unique and they should be evaluated based on the
context of the founder&apos;s background.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_S/0/1/0/all/0/1&quot;&gt;Sichao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ihlamur_Y/0/1/0/all/0/1&quot;&gt;Yigit Ihlamur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12430">
<title>Efficient Title Reranker for Fast and Improved Knowledge-Intense NLP. (arXiv:2312.12430v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12430</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Efficient Title Reranker via Broadcasting Query Encoder, a novel
title reranking technique to achieve efficient title reranking 20x-40x faster
than vanilla passage reranker. However, one of the challenges with the training
of Efficient Title Reranker is the instability. Analyzing the issue, we found
some very difficult ground truths might act as noisy labels causing accuracy to
drop as well as some extreme values in model probability output causing nan. To
address these issues, we introduce the Sigmoid Trick, a novel technique that
reduces the gradient update of both cases resulting in better retrieval
efficacy. Experiments showed the effectiveness of ETR and sigmoid trick as we
achieved four state-of-the-art positions on the kilt knowledge benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Ziyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_H/0/1/0/all/0/1&quot;&gt;Heyi Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_D/0/1/0/all/0/1&quot;&gt;Daqian Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jize Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12436">
<title>A Challenger to GPT-4V? Early Explorations of Gemini in Visual Expertise. (arXiv:2312.12436v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12436</link>
<description rdf:parseType="Literal">&lt;p&gt;The surge of interest towards Multi-modal Large Language Models (MLLMs),
e.g., GPT-4V(ision) from OpenAI, has marked a significant trend in both
academia and industry. They endow Large Language Models (LLMs) with powerful
capabilities in visual understanding, enabling them to tackle diverse
multi-modal tasks. Very recently, Google released Gemini, its newest and most
capable MLLM built from the ground up for multi-modality. In light of the
superior reasoning capabilities, can Gemini challenge GPT-4V&apos;s leading position
in multi-modal learning? In this paper, we present a preliminary exploration of
Gemini Pro&apos;s visual understanding proficiency, which comprehensively covers
four domains: fundamental perception, advanced cognition, challenging vision
tasks, and various expert capacities. We compare Gemini Pro with the
state-of-the-art GPT-4V to evaluate its upper limits, along with the latest
open-sourced MLLM, Sphinx, which reveals the gap between manual efforts and
black-box systems. The qualitative samples indicate that, while GPT-4V and
Gemini showcase different answering styles and preferences, they can exhibit
comparable visual reasoning capabilities, and Sphinx still trails behind them
concerning domain generalizability. Specifically, GPT-4V tends to elaborate
detailed explanations and intermediate steps, and Gemini prefers to output a
direct and concise answer. The quantitative evaluation on the popular MME
benchmark also demonstrates the potential of Gemini to be a strong challenger
to GPT-4V. Our early investigation of Gemini also observes some common issues
of MLLMs, indicating that there still remains a considerable distance towards
artificial general intelligence. Our project for tracking the progress of MLLM
is released at
https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chaoyou Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yubo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Longtian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1&quot;&gt;Gaoxiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yunhang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengdan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Peixian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sirui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shaohui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Deqiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Di Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xing Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1012.5754">
<title>Software Effort Estimation with Ridge Regression and Evolutionary Attribute Selection. (arXiv:1012.5754v1 [cs.SE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1012.5754</link>
<description rdf:parseType="Literal">&lt;p&gt;Software cost estimation is one of the prerequisite managerial activities
carried out at the software development initiation stages and also repeated
throughout the whole software life-cycle so that amendments to the total cost
are made. In software cost estimation typically, a selection of project
attributes is employed to produce effort estimations of the expected human
resources to deliver a software product. However, choosing the appropriate
project cost drivers in each case requires a lot of experience and knowledge on
behalf of the project manager which can only be obtained through years of
software engineering practice. A number of studies indicate that popular
methods applied in the literature for software cost estimation, such as linear
regression, are not robust enough and do not yield accurate predictions.
Recently the dual variables Ridge Regression (RR) technique has been used for
effort estimation yielding promising results. In this work we show that results
may be further improved if an AI method is used to automatically select
appropriate project cost drivers (inputs) for the technique. We propose a
hybrid approach combining RR with a Genetic Algorithm, the latter evolving the
subset of attributes for approximating effort more accurately. The proposed
hybrid cost model has been applied on a widely known high-dimensional dataset
of software project samples and the results obtained show that accuracy may be
increased if redundant attributes are eliminated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papatheocharous_E/0/1/0/all/0/1&quot;&gt;Efi Papatheocharous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_H/0/1/0/all/0/1&quot;&gt;Harris Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreou_A/0/1/0/all/0/1&quot;&gt;Andreas S. Andreou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1210.1161">
<title>Feature Subset Selection for Software Cost Modelling and Estimation. (arXiv:1210.1161v1 [cs.SE] CROSS LISTED)</title>
<link>http://arxiv.org/abs/1210.1161</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection has been recently used in the area of software engineering
for improving the accuracy and robustness of software cost models. The idea
behind selecting the most informative subset of features from a pool of
available cost drivers stems from the hypothesis that reducing the
dimensionality of datasets will significantly minimise the complexity and time
required to reach to an estimation using a particular modelling technique. This
work investigates the appropriateness of attributes, obtained from empirical
project databases and aims to reduce the cost drivers used while preserving
performance. Finding suitable subset selections that may cater improved
predictions may be considered as a pre-processing step of a particular
technique employed for cost estimation (filter or wrapper) or an internal
(embedded) step to minimise the fitting error. This paper compares nine
relatively popular feature selection methods and uses the empirical values of
selected attributes recorded in the ISBSG and Desharnais datasets to estimate
software development effort.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papatheocharous_E/0/1/0/all/0/1&quot;&gt;Efi Papatheocharous&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_H/0/1/0/all/0/1&quot;&gt;Harris Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreou_A/0/1/0/all/0/1&quot;&gt;Andreas S. Andreou&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>