<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04515" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04528" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04622" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04725" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04826" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05095" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2002.03729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.01615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.01937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.05119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.14797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.11378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.07898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.12860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03217" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2308.04466">
<title>Backdoor Federated Learning by Poisoning Backdoor-Critical Layers. (arXiv:2308.04466v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2308.04466</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) has been widely deployed to enable machine learning
training on sensitive data across distributed devices. However, the
decentralized learning paradigm and heterogeneity of FL further extend the
attack surface for backdoor attacks. Existing FL attack and defense
methodologies typically focus on the whole model. None of them recognizes the
existence of backdoor-critical (BC) layers-a small subset of layers that
dominate the model vulnerabilities. Attacking the BC layers achieves equivalent
effects as attacking the whole model but at a far smaller chance of being
detected by state-of-the-art (SOTA) defenses. This paper proposes a general
in-situ approach that identifies and verifies BC layers from the perspective of
attackers. Based on the identified BC layers, we carefully craft a new backdoor
attack methodology that adaptively seeks a fundamental balance between
attacking effects and stealthiness under various defense strategies. Extensive
experiments show that our BC layer-aware backdoor attacks can successfully
backdoor FL under seven SOTA defenses with only 10% malicious clients and
outperform the latest backdoor attack methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_H/0/1/0/all/0/1&quot;&gt;Haomin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_M/0/1/0/all/0/1&quot;&gt;Mingxian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yang Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xu Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04468">
<title>3D Scene Diffusion Guidance using Scene Graphs. (arXiv:2308.04468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04468</link>
<description rdf:parseType="Literal">&lt;p&gt;Guided synthesis of high-quality 3D scenes is a challenging task. Diffusion
models have shown promise in generating diverse data, including 3D scenes.
However, current methods rely directly on text embeddings for controlling the
generation, limiting the incorporation of complex spatial relationships between
objects. We propose a novel approach for 3D scene diffusion guidance using
scene graphs. To leverage the relative spatial information the scene graphs
provide, we make use of relational graph convolutional blocks within our
denoising network. We show that our approach significantly improves the
alignment between scene description and generated scene.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naanaa_M/0/1/0/all/0/1&quot;&gt;Mohammad Naanaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_K/0/1/0/all/0/1&quot;&gt;Katharina Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yinyu Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04515">
<title>Toward unlabeled multi-view 3D pedestrian detection by generalizable AI: techniques and performance analysis. (arXiv:2308.04515v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04515</link>
<description rdf:parseType="Literal">&lt;p&gt;We unveil how generalizable AI can be used to improve multi-view 3D
pedestrian detection in unlabeled target scenes. One way to increase
generalization to new scenes is to automatically label target data, which can
then be used for training a detector model. In this context, we investigate two
approaches for automatically labeling target data: pseudo-labeling using a
supervised detector and automatic labeling using an untrained detector (that
can be applied out of the box without any training). We adopt a training
framework for optimizing detector models using automatic labeling procedures.
This framework encompasses different training sets/modes and multi-round
automatic labeling strategies. We conduct our analyses on the
publicly-available WILDTRACK and MultiviewX datasets. We show that, by using
the automatic labeling approach based on an untrained detector, we can obtain
superior results than directly using the untrained detector or a detector
trained with an existing labeled source dataset. It achieved a MODA about 4%
and 1% better than the best existing unlabeled method when using WILDTRACK and
MultiviewX as target datasets, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Paulo Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_D/0/1/0/all/0/1&quot;&gt;Diego Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchiyama_H/0/1/0/all/0/1&quot;&gt;Hideaki Uchiyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teichrieb_V/0/1/0/all/0/1&quot;&gt;Veronica Teichrieb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04526">
<title>Large-Scale Multi-Hypotheses Cell Tracking Using Ultrametric Contours Maps. (arXiv:2308.04526v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04526</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we describe a method for large-scale 3D cell-tracking through a
segmentation selection approach. The proposed method is effective at tracking
cells across large microscopy datasets on two fronts: (i) It can solve problems
containing millions of segmentation instances in terabyte-scale 3D+t datasets;
(ii) It achieves competitive results with or without deep learning, which
requires 3D annotated data, that is scarce in the fluorescence microscopy
field. The proposed method computes cell tracks and segments using a hierarchy
of segmentation hypotheses and selects disjoint segments by maximizing the
overlap between adjacent frames. We show that this method achieves
state-of-the-art results in 3D images from the cell tracking challenge and has
a faster integer linear programming formulation. Moreover, our framework is
flexible and supports segmentations from off-the-shelf cell segmentation models
and can combine them into an ensemble that improves tracking. The code is
available https://github.com/royerlab/ultrack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bragantini_J/0/1/0/all/0/1&quot;&gt;Jord&amp;#xe3;o Bragantini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_M/0/1/0/all/0/1&quot;&gt;Merlin Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Royer_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Royer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04528">
<title>Unsupervised Camouflaged Object Segmentation as Domain Adaptation. (arXiv:2308.04528v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04528</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning for unsupervised image segmentation remains challenging due to
the absence of human labels. The common idea is to train a segmentation head,
with the supervision of pixel-wise pseudo-labels generated based on the
representation of self-supervised backbones. By doing so, the model performance
depends much on the distance between the distributions of target datasets and
the pre-training dataset (e.g., ImageNet). In this work, we investigate a new
task, namely unsupervised camouflaged object segmentation (UCOS), where the
target objects own a common rarely-seen attribute, i.e., camouflage.
Unsurprisingly, we find that the state-of-the-art unsupervised models struggle
in adapting UCOS, due to the domain gap between the properties of generic and
camouflaged objects. To this end, we formulate the UCOS as a source-free
unsupervised domain adaptation task (UCOS-DA), where both source labels and
target labels are absent during the whole model training process. Specifically,
we define a source model consisting of self-supervised vision transformers
pre-trained on ImageNet. On the other hand, the target domain includes a simple
linear layer (i.e., our target model) and unlabeled camouflaged objects. We
then design a pipeline for foreground-background-contrastive self-adversarial
domain adaptation, to achieve robust UCOS. As a result, our baseline model
achieves superior segmentation performance when compared with competing
unsupervised models on the UCOS benchmark, with the training set which&apos;s scale
is only one tenth of the supervised COS counterpart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chengyi Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04529">
<title>Generating Modern Persian Carpet Map by Style-transfer. (arXiv:2308.04529v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04529</link>
<description rdf:parseType="Literal">&lt;p&gt;Today, the great performance of Deep Neural Networks(DNN) has been proven in
various fields. One of its most attractive applications is to produce artistic
designs. A carpet that is known as a piece of art is one of the most important
items in a house, which has many enthusiasts all over the world. The first
stage of producing a carpet is to prepare its map, which is a difficult,
time-consuming, and expensive task. In this research work, our purpose is to
use DNN for generating a Modern Persian Carpet Map. To reach this aim, three
different DNN style transfer methods are proposed and compared against each
other. In the proposed methods, the Style-Swap method is utilized to create the
initial carpet map, and in the following, to generate more diverse designs,
methods Clip-Styler, Gatys, and Style-Swap are used separately. In addition,
some methods are examined and introduced for coloring the produced carpet maps.
The designed maps are evaluated via the results of filled questionnaires where
the outcomes of user evaluations confirm the popularity of generated carpet
maps. Eventually, for the first time, intelligent methods are used in producing
carpet maps, and it reduces human intervention. The proposed methods can
successfully produce diverse carpet designs, and at a higher speed than
traditional ways.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmatian_D/0/1/0/all/0/1&quot;&gt;Dorsa Rahmatian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moshavash_M/0/1/0/all/0/1&quot;&gt;Monireh Moshavash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eftekhari_M/0/1/0/all/0/1&quot;&gt;Mahdi Eftekhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoseinkhani_K/0/1/0/all/0/1&quot;&gt;Kamran Hoseinkhani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04535">
<title>Estimation of Human Condition at Disaster Site Using Aerial Drone Images. (arXiv:2308.04535v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04535</link>
<description rdf:parseType="Literal">&lt;p&gt;Drones are being used to assess the situation in various disasters. In this
study, we investigate a method to automatically estimate the damage status of
people based on their actions in aerial drone images in order to understand
disaster sites faster and save labor. We constructed a new dataset of aerial
images of human actions in a hypothetical disaster that occurred in an urban
area, and classified the human damage status using 3D ResNet. The results
showed that the status with characteristic human actions could be classified
with a recall rate of more than 80%, while other statuses with similar human
actions could only be classified with a recall rate of about 50%. In addition,
a cloud-based VR presentation application suggested the effectiveness of using
drones to understand the disaster site and estimate the human condition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arai_T/0/1/0/all/0/1&quot;&gt;Tomoki Arai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iwata_K/0/1/0/all/0/1&quot;&gt;Kenji Iwata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hara_K/0/1/0/all/0/1&quot;&gt;Kensho Hara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satoh_Y/0/1/0/all/0/1&quot;&gt;Yutaka Satoh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04536">
<title>Facial Prior Based First Order Motion Model for Micro-expression Generation. (arXiv:2308.04536v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04536</link>
<description rdf:parseType="Literal">&lt;p&gt;Spotting facial micro-expression from videos finds various potential
applications in fields including clinical diagnosis and interrogation,
meanwhile this task is still difficult due to the limited scale of training
data. To solve this problem, this paper tries to formulate a new task called
micro-expression generation and then presents a strong baseline which combines
the first order motion model with facial prior knowledge. Given a target face,
we intend to drive the face to generate micro-expression videos according to
the motion patterns of source videos. Specifically, our new model involves
three modules. First, we extract facial prior features from a region focusing
module. Second, we estimate facial motion using key points and local affine
transformations with a motion prediction module. Third, expression generation
module is used to drive the target face to generate videos. We train our model
on public CASME II, SAMM and SMIC datasets and then use the model to generate
new micro-expression videos for evaluation. Our model achieves the first place
in the Facial Micro-Expression Challenge 2021 (MEGC2021), where our superior
performance is verified by three experts with Facial Action Coding System
certification. Source code is provided in
https://github.com/Necolizer/Facial-Prior-Based-FOMM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Youjun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xinhua Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04542">
<title>YUDO: YOLO for Uniform Directed Object Detection. (arXiv:2308.04542v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04542</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an efficient way of detecting directed objects by
predicting their center coordinates and direction angle. Since the objects are
of uniform size, the proposed model works without predicting the object&apos;s width
and height. The dataset used for this problem is presented in Honeybee
Segmentation and Tracking Datasets project. One of the contributions of this
work is an examination of the ability of the standard real-time object
detection architecture like YoloV7 to be customized for position and direction
detection. A very efficient, tiny version of the architecture is used in this
approach. Moreover, only one of three detection heads without anchors is
sufficient for this task. We also introduce the extended Skew Intersection over
Union (SkewIoU) calculation for rotated boxes - directed IoU (DirIoU), which
includes an absolute angle difference. DirIoU is used both in the matching
procedure of target and predicted bounding boxes for mAP calculation, and in
the NMS filtering procedure. The code and models are available at
https://github.com/djordjened92/yudo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nedeljkovic_%7B/0/1/0/all/0/1&quot;&gt;&amp;#x110;or&amp;#x111;e Nedeljkovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04549">
<title>Prune Spatio-temporal Tokens by Semantic-aware Temporal Accumulation. (arXiv:2308.04549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04549</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have become the primary backbone of the computer vision
community due to their impressive performance. However, the unfriendly
computation cost impedes their potential in the video recognition domain. To
optimize the speed-accuracy trade-off, we propose Semantic-aware Temporal
Accumulation score (STA) to prune spatio-temporal tokens integrally. STA score
considers two critical factors: temporal redundancy and semantic importance.
The former depicts a specific region based on whether it is a new occurrence or
a seen entity by aggregating token-to-token similarity in consecutive frames
while the latter evaluates each token based on its contribution to the overall
prediction. As a result, tokens with higher scores of STA carry more temporal
redundancy as well as lower semantics thus being pruned. Based on the STA
score, we are able to progressively prune the tokens without introducing any
additional parameters or requiring further re-training. We directly apply the
STA module to off-the-shelf ViT and VideoSwin backbones, and the empirical
results on Kinetics-400 and Something-Something V2 achieve over 30% computation
reduction with a negligible ~0.2% accuracy drop. The code is released at
https://github.com/Mark12Ding/STA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shuangrui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peisen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_R/0/1/0/all/0/1&quot;&gt;Rui Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hongkai Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Q/0/1/0/all/0/1&quot;&gt;Qi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04551">
<title>Improving Medical Image Classification in Noisy Labels Using Only Self-supervised Pretraining. (arXiv:2308.04551v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04551</link>
<description rdf:parseType="Literal">&lt;p&gt;Noisy labels hurt deep learning-based supervised image classification
performance as the models may overfit the noise and learn corrupted feature
extractors. For natural image classification training with noisy labeled data,
model initialization with contrastive self-supervised pretrained weights has
shown to reduce feature corruption and improve classification performance.
However, no works have explored: i) how other self-supervised approaches, such
as pretext task-based pretraining, impact the learning with noisy label, and
ii) any self-supervised pretraining methods alone for medical images in noisy
label settings. Medical images often feature smaller datasets and subtle inter
class variations, requiring human expertise to ensure correct classification.
Thus, it is not clear if the methods improving learning with noisy labels in
natural image datasets such as CIFAR would also help with medical images. In
this work, we explore contrastive and pretext task-based self-supervised
pretraining to initialize the weights of a deep learning classification model
for two medical datasets with self-induced noisy labels -- NCT-CRC-HE-100K
tissue histological images and COVID-QU-Ex chest X-ray images. Our results show
that models initialized with pretrained weights obtained from self-supervised
learning can effectively learn better features and improve robustness against
noisy labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanal_B/0/1/0/all/0/1&quot;&gt;Bidur Khanal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattarai_B/0/1/0/all/0/1&quot;&gt;Binod Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanal_B/0/1/0/all/0/1&quot;&gt;Bishesh Khanal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Linte_C/0/1/0/all/0/1&quot;&gt;Cristian A. Linte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04553">
<title>From Fake to Real (FFR): A two-stage training pipeline for mitigating spurious correlations with synthetic data. (arXiv:2308.04553v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04553</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual recognition models are prone to learning spurious correlations induced
by an imbalanced training set where certain groups (\eg Females) are
under-represented in certain classes (\eg Programmers). Generative models offer
a promising direction in mitigating this bias by generating synthetic data for
the minority samples and thus balancing the training set. However, prior work
that uses these approaches overlooks that visual recognition models could often
learn to differentiate between real and synthetic images and thus fail to
unlearn the bias in the original dataset. In our work, we propose a novel
two-stage pipeline to mitigate this issue where 1) we pre-train a model on a
balanced synthetic dataset and then 2) fine-tune on the real data. Using this
pipeline, we avoid training on both real and synthetic data, thus avoiding the
bias between real and synthetic data. Moreover, we learn robust features
against the bias in the first step that mitigate the bias in the second step.
Moreover, our pipeline naturally integrates with bias mitigation methods; they
can be simply applied to the fine-tuning step. As our experiments prove, our
pipeline can further improve the performance of bias mitigation methods
obtaining state-of-the-art performance on three large-scale datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qraitem_M/0/1/0/all/0/1&quot;&gt;Maan Qraitem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saenko_K/0/1/0/all/0/1&quot;&gt;Kate Saenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plummer_B/0/1/0/all/0/1&quot;&gt;Bryan A. Plummer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04556">
<title>FocalFormer3D : Focusing on Hard Instance for 3D Object Detection. (arXiv:2308.04556v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04556</link>
<description rdf:parseType="Literal">&lt;p&gt;False negatives (FN) in 3D object detection, {\em e.g.}, missing predictions
of pedestrians, vehicles, or other obstacles, can lead to potentially dangerous
situations in autonomous driving. While being fatal, this issue is understudied
in many current 3D detection methods. In this work, we propose Hard Instance
Probing (HIP), a general pipeline that identifies \textit{FN} in a multi-stage
manner and guides the models to focus on excavating difficult instances. For 3D
object detection, we instantiate this method as FocalFormer3D, a simple yet
effective detector that excels at excavating difficult objects and improving
prediction recall. FocalFormer3D features a multi-stage query generation to
discover hard objects and a box-level transformer decoder to efficiently
distinguish objects from massive object candidates. Experimental results on the
nuScenes and Waymo datasets validate the superior performance of FocalFormer3D.
The advantage leads to strong performance on both detection and tracking, in
both LiDAR and multi-modal settings. Notably, FocalFormer3D achieves a 70.5 mAP
and 73.9 NDS on nuScenes detection benchmark, while the nuScenes tracking
benchmark shows 72.1 AMOTA, both ranking 1st place on the nuScenes LiDAR
leaderboard. Our code is available at
\url{https://github.com/NVlabs/FocalFormer3D}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yilun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_S/0/1/0/all/0/1&quot;&gt;Shiyi Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Animashree Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose Alvarez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04571">
<title>Optimizing Algorithms From Pairwise User Preferences. (arXiv:2308.04571v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.04571</link>
<description rdf:parseType="Literal">&lt;p&gt;Typical black-box optimization approaches in robotics focus on learning from
metric scores. However, that is not always possible, as not all developers have
ground truth available. Learning appropriate robot behavior in human-centric
contexts often requires querying users, who typically cannot provide precise
metric scores. Existing approaches leverage human feedback in an attempt to
model an implicit reward function; however, this reward may be difficult or
impossible to effectively capture. In this work, we introduce SortCMA to
optimize algorithm parameter configurations in high dimensions based on
pairwise user preferences. SortCMA efficiently and robustly leverages user
input to find parameter sets without directly modeling a reward. We apply this
method to tuning a commercial depth sensor without ground truth, and to robot
social navigation, which involves highly complex preferences over robot
behavior. We show that our method succeeds in optimizing for the user&apos;s goals
and perform a user study to evaluate social navigation results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keselman_L/0/1/0/all/0/1&quot;&gt;Leonid Keselman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shih_K/0/1/0/all/0/1&quot;&gt;Katherine Shih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1&quot;&gt;Martial Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinfeld_A/0/1/0/all/0/1&quot;&gt;Aaron Steinfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04583">
<title>LATR: 3D Lane Detection from Monocular Images with Transformer. (arXiv:2308.04583v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04583</link>
<description rdf:parseType="Literal">&lt;p&gt;3D lane detection from monocular images is a fundamental yet challenging task
in autonomous driving. Recent advances primarily rely on structural 3D
surrogates (e.g., bird&apos;s eye view) that are built from front-view image
features and camera parameters. However, the depth ambiguity in monocular
images inevitably causes misalignment between the constructed surrogate feature
map and the original image, posing a great challenge for accurate lane
detection. To address the above issue, we present a novel LATR model, an
end-to-end 3D lane detector that uses 3D-aware front-view features without
transformed view representation. Specifically, LATR detects 3D lanes via
cross-attention based on query and key-value pairs, constructed using our
lane-aware query generator and dynamic 3D ground positional embedding. On the
one hand, each query is generated based on 2D lane-aware features and adopts a
hybrid embedding to enhance the lane information. On the other hand, 3D space
information is injected as positional embedding from an iteratively-updated 3D
ground plane. LATR outperforms previous state-of-the-art methods on both
synthetic Apollo and realistic OpenLane by large margins (e.g., 11.4 gains in
terms of F1 score on OpenLane). Code will be released at
https://github.com/JMoonr/LATR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yueru Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chaoda Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kun_T/0/1/0/all/0/1&quot;&gt;Tang Kun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04589">
<title>Temporal DINO: A Self-supervised Video Strategy to Enhance Action Prediction. (arXiv:2308.04589v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04589</link>
<description rdf:parseType="Literal">&lt;p&gt;The emerging field of action prediction plays a vital role in various
computer vision applications such as autonomous driving, activity analysis and
human-computer interaction. Despite significant advancements, accurately
predicting future actions remains a challenging problem due to high
dimensionality, complex dynamics and uncertainties inherent in video data.
Traditional supervised approaches require large amounts of labelled data, which
is expensive and time-consuming to obtain. This paper introduces a novel
self-supervised video strategy for enhancing action prediction inspired by DINO
(self-distillation with no labels). The Temporal-DINO approach employs two
models; a &apos;student&apos; processing past frames; and a &apos;teacher&apos; processing both
past and future frames, enabling a broader temporal context. During training,
the teacher guides the student to learn future context by only observing past
frames. The strategy is evaluated on ROAD dataset for the action prediction
downstream task using 3D-ResNet, Transformer, and LSTM architectures. The
experimental results showcase significant improvements in prediction
performance across these architectures, with our method achieving an average
enhancement of 9.9% Precision Points (PP), highlighting its effectiveness in
enhancing the backbones&apos; capabilities of capturing long-term dependencies.
Furthermore, our approach demonstrates efficiency regarding the pretraining
dataset size and the number of epochs required. This method overcomes
limitations present in other approaches, including considering various backbone
architectures, addressing multiple prediction horizons, reducing reliance on
hand-crafted augmentations, and streamlining the pretraining process into a
single stage. These findings highlight the potential of our approach in diverse
video-based tasks such as activity recognition, motion planning, and scene
understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teeti_I/0/1/0/all/0/1&quot;&gt;Izzeddin Teeti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhargav_R/0/1/0/all/0/1&quot;&gt;Rongali Sai Bhargav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_V/0/1/0/all/0/1&quot;&gt;Vivek Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_A/0/1/0/all/0/1&quot;&gt;Andrew Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_B/0/1/0/all/0/1&quot;&gt;Biplab Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04598">
<title>1st Place Solution for CVPR2023 BURST Long Tail and Open World Challenges. (arXiv:2308.04598v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04598</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, Video Instance Segmentation (VIS) aims at segmenting and
categorizing objects in videos from a closed set of training categories that
contain only a few dozen of categories, lacking the ability to handle diverse
objects in real-world videos. As TAO and BURST datasets release, we have the
opportunity to research VIS in long-tailed and open-world scenarios.
Traditional VIS methods are evaluated on benchmarks limited to a small number
of common classes, But practical applications require trackers that go beyond
these common classes, detecting and tracking rare and even never-before-seen
objects. Inspired by the latest MOT paper for the long tail task (Tracking
Every Thing in the Wild, Siyuan Li et), for the BURST long tail challenge, we
train our model on a combination of LVISv0.5 and the COCO dataset using repeat
factor sampling. First, train the detector with segmentation and CEM on
LVISv0.5 + COCO dataset. And then, train the instance appearance similarity
head on the TAO dataset. at last, our method (LeTracker) gets 14.9 HOTAall in
the BURST test set, ranking 1st in the benchmark. for the open-world
challenges, we only use 64 classes (Intersection classes of BURST Train subset
and COCO dataset, without LVIS dataset) annotations data training, and testing
on BURST test set data and get 61.4 OWTAall, ranking 1st in the benchmark. Our
code will be released to facilitate future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaer Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04605">
<title>PSRFlow: Probabilistic Super Resolution with Flow-Based Models for Scientific Data. (arXiv:2308.04605v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04605</link>
<description rdf:parseType="Literal">&lt;p&gt;Although many deep-learning-based super-resolution approaches have been
proposed in recent years, because no ground truth is available in the inference
stage, few can quantify the errors and uncertainties of the super-resolved
results. For scientific visualization applications, however, conveying
uncertainties of the results to scientists is crucial to avoid generating
misleading or incorrect information. In this paper, we propose PSRFlow, a novel
normalizing flow-based generative model for scientific data super-resolution
that incorporates uncertainty quantification into the super-resolution process.
PSRFlow learns the conditional distribution of the high-resolution data based
on the low-resolution counterpart. By sampling from a Gaussian latent space
that captures the missing information in the high-resolution data, one can
generate different plausible super-resolution outputs. The efficient sampling
in the Gaussian latent space allows our model to perform uncertainty
quantification for the super-resolved results. During model training, we
augment the training data with samples across various scales to make the model
adaptable to data of different scales, achieving flexible super-resolution for
a given input. Our results demonstrate superior performance and robust
uncertainty quantification compared with existing methods such as interpolation
and GAN-based super-resolution networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jingyi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Han-Wei Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04622">
<title>Rendering Humans from Object-Occluded Monocular Videos. (arXiv:2308.04622v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04622</link>
<description rdf:parseType="Literal">&lt;p&gt;3D understanding and rendering of moving humans from monocular videos is a
challenging task. Despite recent progress, the task remains difficult in
real-world scenarios, where obstacles may block the camera view and cause
partial occlusions in the captured videos. Existing methods cannot handle such
defects due to two reasons. First, the standard rendering strategy relies on
point-point mapping, which could lead to dramatic disparities between the
visible and occluded areas of the body. Second, the naive direct regression
approach does not consider any feasibility criteria (ie, prior information) for
rendering under occlusions. To tackle the above drawbacks, we present OccNeRF,
a neural rendering method that achieves better rendering of humans in severely
occluded scenes. As direct solutions to the two drawbacks, we propose
surface-based rendering by integrating geometry and visibility priors. We
validate our method on both simulated and real-world occlusions and demonstrate
our method&apos;s superiority.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tiange Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1&quot;&gt;Adam Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adeli_E/0/1/0/all/0/1&quot;&gt;Ehsan Adeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04638">
<title>GeoAdapt: Self-Supervised Test-Time Adaption in LiDAR Place Recognition Using Geometric Priors. (arXiv:2308.04638v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04638</link>
<description rdf:parseType="Literal">&lt;p&gt;LiDAR place recognition approaches based on deep learning suffer a
significant degradation in performance when there is a shift between the
distribution of the training and testing datasets, with re-training often
required to achieve top performance. However, obtaining accurate ground truth
on new environments can be prohibitively expensive, especially in complex or
GPS-deprived environments. To address this issue we propose GeoAdapt, which
introduces a novel auxiliary classification head to generate pseudo-labels for
re-training on unseen environments in a self-supervised manner. GeoAdapt uses
geometric consistency as a prior to improve the robustness of our generated
pseudo-labels against domain shift, improving the performance and reliability
of our Test-Time Adaptation approach. Comprehensive experiments show that
GeoAdapt significantly boosts place recognition performance across moderate to
severe domain shifts, and is competitive with fully supervised test-time
adaptation approaches. Our code will be available at
https://github.com/csiro-robotics/GeoAdapt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knights_J/0/1/0/all/0/1&quot;&gt;Joshua Knights&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausler_S/0/1/0/all/0/1&quot;&gt;Stephen Hausler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridharan_S/0/1/0/all/0/1&quot;&gt;Sridha Sridharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghadam_P/0/1/0/all/0/1&quot;&gt;Peyman Moghadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04643">
<title>Long-Distance Gesture Recognition using Dynamic Neural Networks. (arXiv:2308.04643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04643</link>
<description rdf:parseType="Literal">&lt;p&gt;Gestures form an important medium of communication between humans and
machines. An overwhelming majority of existing gesture recognition methods are
tailored to a scenario where humans and machines are located very close to each
other. This short-distance assumption does not hold true for several types of
interactions, for example gesture-based interactions with a floor cleaning
robot or with a drone. Methods made for short-distance recognition are unable
to perform well on long-distance recognition due to gestures occupying only a
small portion of the input data. Their performance is especially worse in
resource constrained settings where they are not able to effectively focus
their limited compute on the gesturing subject. We propose a novel, accurate
and efficient method for the recognition of gestures from longer distances. It
uses a dynamic neural network to select features from gesture-containing
spatial regions of the input sensor data for further processing. This helps the
network focus on features important for gesture recognition while discarding
background features early on, thus making it more compute efficient compared to
other techniques. We demonstrate the performance of our method on the LD-ConGR
long-distance dataset where it outperforms previous state-of-the-art methods on
recognition accuracy and compute efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1&quot;&gt;Shubhang Bhatnagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopal_S/0/1/0/all/0/1&quot;&gt;Sharath Gopal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_N/0/1/0/all/0/1&quot;&gt;Narendra Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_L/0/1/0/all/0/1&quot;&gt;Liu Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04653">
<title>Assessing the performance of deep learning-based models for prostate cancer segmentation using uncertainty scores. (arXiv:2308.04653v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04653</link>
<description rdf:parseType="Literal">&lt;p&gt;This study focuses on comparing deep learning methods for the segmentation
and quantification of uncertainty in prostate segmentation from MRI images. The
aim is to improve the workflow of prostate cancer detection and diagnosis.
Seven different U-Net-based architectures, augmented with Monte-Carlo dropout,
are evaluated for automatic segmentation of the central zone, peripheral zone,
transition zone, and tumor, with uncertainty estimation. The top-performing
model in this study is the Attention R2U-Net, achieving a mean Intersection
over Union (IoU) of 76.3% and Dice Similarity Coefficient (DSC) of 85% for
segmenting all zones. Additionally, Attention R2U-Net exhibits the lowest
uncertainty values, particularly in the boundaries of the transition zone and
tumor, when compared to the other models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Quihui_Rubio_P/0/1/0/all/0/1&quot;&gt;Pablo Cesar Quihui-Rubio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Flores_Araiza_D/0/1/0/all/0/1&quot;&gt;Daniel Flores-Araiza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ochoa_Ruiz_G/0/1/0/all/0/1&quot;&gt;Gilberto Ochoa-Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gonzalez_Mendoza_M/0/1/0/all/0/1&quot;&gt;Miguel Gonzalez-Mendoza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mata_C/0/1/0/all/0/1&quot;&gt;Christian Mata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04657">
<title>Which Tokens to Use? Investigating Token Reduction in Vision Transformers. (arXiv:2308.04657v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04657</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the introduction of the Vision Transformer (ViT), researchers have
sought to make ViTs more efficient by removing redundant information in the
processed tokens. While different methods have been explored to achieve this
goal, we still lack understanding of the resulting reduction patterns and how
those patterns differ across token reduction methods and datasets. To close
this gap, we set out to understand the reduction patterns of 10 different token
reduction methods using four image classification datasets. By systematically
comparing these methods on the different classification tasks, we find that the
Top-K pruning method is a surprisingly strong baseline. Through in-depth
analysis of the different methods, we determine that: the reduction patterns
are generally not consistent when varying the capacity of the backbone model,
the reduction patterns of pruning-based methods significantly differ from fixed
radial patterns, and the reduction patterns of pruning-based methods are
correlated across classification datasets. Finally we report that the
similarity of reduction patterns is a moderate-to-strong proxy for model
performance. Project page at https://vap.aau.dk/tokens.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haurum_J/0/1/0/all/0/1&quot;&gt;Joakim Bruslund Haurum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escalera_S/0/1/0/all/0/1&quot;&gt;Sergio Escalera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_G/0/1/0/all/0/1&quot;&gt;Graham W. Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moeslund_T/0/1/0/all/0/1&quot;&gt;Thomas B. Moeslund&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04663">
<title>Classification of lung cancer subtypes on CT images with synthetic pathological priors. (arXiv:2308.04663v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04663</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate diagnosis on pathological subtypes for lung cancer is of
significant importance for the follow-up treatments and prognosis managements.
In this paper, we propose self-generating hybrid feature network (SGHF-Net) for
accurately classifying lung cancer subtypes on computed tomography (CT) images.
Inspired by studies stating that cross-scale associations exist in the image
patterns between the same case&apos;s CT images and its pathological images, we
innovatively developed a pathological feature synthetic module (PFSM), which
quantitatively maps cross-modality associations through deep neural networks,
to derive the &quot;gold standard&quot; information contained in the corresponding
pathological images from CT images. Additionally, we designed a radiological
feature extraction module (RFEM) to directly acquire CT image information and
integrated it with the pathological priors under an effective feature fusion
framework, enabling the entire classification model to generate more indicative
and specific pathologically related features and eventually output more
accurate predictions. The superiority of the proposed model lies in its ability
to self-generate hybrid features that contain multi-modality image information
based on a single-modality input. To evaluate the effectiveness, adaptability,
and generalization ability of our model, we performed extensive experiments on
a large-scale multi-center dataset (i.e., 829 cases from three hospitals) to
compare our model and a series of state-of-the-art (SOTA) classification
models. The experimental results demonstrated the superiority of our model for
lung cancer subtypes classification with significant accuracy improvements in
terms of accuracy (ACC), area under the curve (AUC), and F1 score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yuan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gege Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Geng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Metaxas_D/0/1/0/all/0/1&quot;&gt;Dimitris N. Metaxas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04669">
<title>A General Implicit Framework for Fast NeRF Composition and Rendering. (arXiv:2308.04669v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04669</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a variety of Neural radiance fields methods have garnered
remarkable success in high render speed. However, current accelerating methods
is specialized and not compatible for various implicit method, which prevent a
real-time composition over different kinds of NeRF works. Since NeRF relies on
sampling along rays, it&apos;s possible to provide a guidance generally. We propose
a general implicit pipeline to rapidly compose NeRF objects. This new method
enables the casting of dynamic shadows within or between objects using
analytical light sources while allowing multiple NeRF objects to be seamlessly
placed and rendered together with any arbitrary rigid transformations. Mainly,
our work introduces a new surface representation known as Neural Depth Fields
(NeDF) that quickly determines the spatial relationship between objects by
allowing direct intersection computation between rays and implicit surfaces. It
leverages an intersection neural network to query NeRF for acceleration instead
of depending on an explicit spatial structure.Our proposed method is the first
to enable both the progressive and interactive composition of NeRF objects.
Additionally, it also serves as a previewing plugin for a range of existing
NeRF works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunlu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_C/0/1/0/all/0/1&quot;&gt;Changqing Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04672">
<title>Resource Constrained Model Compression via Minimax Optimization for Spiking Neural Networks. (arXiv:2308.04672v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04672</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain-inspired Spiking Neural Networks (SNNs) have the characteristics of
event-driven and high energy-efficient, which are different from traditional
Artificial Neural Networks (ANNs) when deployed on edge devices such as
neuromorphic chips. Most previous work focuses on SNNs training strategies to
improve model performance and brings larger and deeper network architectures.
It is difficult to deploy these complex networks on resource-limited edge
devices directly. To meet such demand, people compress SNNs very cautiously to
balance the performance and the computation efficiency. Existing compression
methods either iteratively pruned SNNs using weights norm magnitude or
formulated the problem as a sparse learning optimization. We propose an
improved end-to-end Minimax optimization method for this sparse learning
problem to better balance the model performance and the computation efficiency.
We also demonstrate that jointly applying compression and finetuning on SNNs is
better than sequentially, especially for extreme compression ratios. The
compressed SNN models achieved state-of-the-art (SOTA) performance on various
benchmark datasets and architectures. Our code is available at
https://github.com/chenjallen/Resource-Constrained-Compression-on-SNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Huan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jianchao Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Chengru Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Di Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04674">
<title>Addressing Racial Bias in Facial Emotion Recognition. (arXiv:2308.04674v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04674</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in deep learning models trained with high-dimensional inputs and
subjective labels remains a complex and understudied area. Facial emotion
recognition, a domain where datasets are often racially imbalanced, can lead to
models that yield disparate outcomes across racial groups. This study focuses
on analyzing racial bias by sub-sampling training sets with varied racial
distributions and assessing test performance across these simulations. Our
findings indicate that smaller datasets with posed faces improve on both
fairness and performance metrics as the simulations approach racial balance.
Notably, the F1-score increases by $27.2\%$ points, and demographic parity
increases by $15.7\%$ points on average across the simulations. However, in
larger datasets with greater facial variation, fairness metrics generally
remain constant, suggesting that racial balance by itself is insufficient to
achieve parity in test performance across different racial groups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_A/0/1/0/all/0/1&quot;&gt;Alex Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xingshuo Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Washington_P/0/1/0/all/0/1&quot;&gt;Peter Washington&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04682">
<title>Score Priors Guided Deep Variational Inference for Unsupervised Real-World Single Image Denoising. (arXiv:2308.04682v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04682</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world single image denoising is crucial and practical in computer
vision. Bayesian inversions combined with score priors now have proven
effective for single image denoising but are limited to white Gaussian noise.
Moreover, applying existing score-based methods for real-world denoising
requires not only the explicit train of score priors on the target domain but
also the careful design of sampling procedures for posterior inference, which
is complicated and impractical. To address these limitations, we propose a
score priors-guided deep variational inference, namely ScoreDVI, for practical
real-world denoising. By considering the deep variational image posterior with
a Gaussian form, score priors are extracted based on easily accessible minimum
MSE Non-$i.i.d$ Gaussian denoisers and variational samples, which in turn
facilitate optimizing the variational image posterior. Such a procedure
adaptively applies cheap score priors to denoising. Additionally, we exploit a
Non-$i.i.d$ Gaussian mixture model and variational noise posterior to model the
real-world noise. This scheme also enables the pixel-wise fusion of multiple
image priors and variational image posteriors. Besides, we develop a
noise-aware prior assignment strategy that dynamically adjusts the weight of
image priors in the optimization. Our method outperforms other single
image-based real-world denoising methods and achieves comparable performance to
dataset-based unsupervised methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shan Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04687">
<title>Rapid Training Data Creation by Synthesizing Medical Images for Classification and Localization. (arXiv:2308.04687v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04687</link>
<description rdf:parseType="Literal">&lt;p&gt;While the use of artificial intelligence (AI) for medical image analysis is
gaining wide acceptance, the expertise, time and cost required to generate
annotated data in the medical field are significantly high, due to limited
availability of both data and expert annotation. Strongly supervised object
localization models require data that is exhaustively annotated, meaning all
objects of interest in an image are identified. This is difficult to achieve
and verify for medical images. We present a method for the transformation of
real data to train any Deep Neural Network to solve the above problems. We show
the efficacy of this approach on both a weakly supervised localization model
and a strongly supervised localization model. For the weakly supervised model,
we show that the localization accuracy increases significantly using the
generated data. For the strongly supervised model, this approach overcomes the
need for exhaustive annotation on real images. In the latter model, we show
that the accuracy, when trained with generated images, closely parallels the
accuracy when trained with exhaustively annotated real images. The results are
demonstrated on images of human urine samples obtained using microscopy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kushwaha_A/0/1/0/all/0/1&quot;&gt;Abhishek Kushwaha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Sarthak Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhanushali_A/0/1/0/all/0/1&quot;&gt;Anish Bhanushali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dastidar_T/0/1/0/all/0/1&quot;&gt;Tathagato Rai Dastidar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04699">
<title>GIFD: A Generative Gradient Inversion Method with Feature Domain Optimization. (arXiv:2308.04699v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04699</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has recently emerged as a promising distributed
machine learning framework to preserve clients&apos; privacy, by allowing multiple
clients to upload the gradients calculated from their local data to a central
server. Recent studies find that the exchanged gradients also take the risk of
privacy leakage, e.g., an attacker can invert the shared gradients and recover
sensitive data against an FL system by leveraging pre-trained generative
adversarial networks (GAN) as prior knowledge. However, performing gradient
inversion attacks in the latent space of the GAN model limits their expression
ability and generalizability. To tackle these challenges, we propose
\textbf{G}radient \textbf{I}nversion over \textbf{F}eature \textbf{D}omains
(GIFD), which disassembles the GAN model and searches the feature domains of
the intermediate layers. Instead of optimizing only over the initial latent
code, we progressively change the optimized layer, from the initial latent
space to intermediate layers closer to the output images. In addition, we
design a regularizer to avoid unreal image generation by adding a small ${l_1}$
ball constraint to the searching range. We also extend GIFD to the
out-of-distribution (OOD) setting, which weakens the assumption that the
training sets of GANs and FL tasks obey the same data distribution. Extensive
experiments demonstrate that our method can achieve pixel-level reconstruction
and is superior to the existing methods. Notably, GIFD also shows great
generalizability under different defense strategy settings and batch sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_H/0/1/0/all/0/1&quot;&gt;Hao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04702">
<title>Continual Road-Scene Semantic Segmentation via Feature-Aligned Symmetric Multi-Modal Network. (arXiv:2308.04702v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04702</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art multimodal semantic segmentation approaches combining LiDAR
and color data are usually designed on top of asymmetric information-sharing
schemes and assume that both modalities are always available. Regrettably, this
strong assumption may not hold in real-world scenarios, where sensors are prone
to failure or can face adverse conditions (night-time, rain, fog, etc.) that
make the acquired information unreliable. Moreover, these architectures tend to
fail in continual learning scenarios. In this work, we re-frame the task of
multimodal semantic segmentation by enforcing a tightly-coupled feature
representation and a symmetric information-sharing scheme, which allows our
approach to work even when one of the input modalities is missing. This makes
our model reliable even in safety-critical settings, as is the case of
autonomous driving. We evaluate our approach on the SemanticKITTI dataset,
comparing it with our closest competitor. We also introduce an ad-hoc continual
learning scheme and show results in a class-incremental continual learning
scenario that prove the effectiveness of the approach also in this setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbato_F/0/1/0/all/0/1&quot;&gt;Francesco Barbato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camuffo_E/0/1/0/all/0/1&quot;&gt;Elena Camuffo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milani_S/0/1/0/all/0/1&quot;&gt;Simone Milani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanuttigh_P/0/1/0/all/0/1&quot;&gt;Pietro Zanuttigh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04725">
<title>Self-supervised Learning of Rotation-invariant 3D Point Set Features using Transformer and its Self-distillation. (arXiv:2308.04725v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04725</link>
<description rdf:parseType="Literal">&lt;p&gt;Invariance against rotations of 3D objects is an important property in
analyzing 3D point set data. Conventional 3D point set DNNs having rotation
invariance typically obtain accurate 3D shape features via supervised learning
by using labeled 3D point sets as training samples. However, due to the rapid
increase in 3D point set data and the high cost of labeling, a framework to
learn rotation-invariant 3D shape features from numerous unlabeled 3D point
sets is required. This paper proposes a novel self-supervised learning
framework for acquiring accurate and rotation-invariant 3D point set features
at object-level. Our proposed lightweight DNN architecture decomposes an input
3D point set into multiple global-scale regions, called tokens, that preserve
the spatial layout of partial shapes composing the 3D object. We employ a
self-attention mechanism to refine the tokens and aggregate them into an
expressive rotation-invariant feature per 3D point set. Our DNN is effectively
trained by using pseudo-labels generated by a self-distillation framework. To
facilitate the learning of accurate features, we propose to combine multi-crop
and cut-mix data augmentation techniques to diversify 3D point sets for
training. Through a comprehensive evaluation, we empirically demonstrate that,
(1) existing rotation-invariant DNN architectures designed for supervised
learning do not necessarily learn accurate 3D shape features under a
self-supervised learning scenario, and (2) our proposed algorithm learns
rotation-invariant 3D point set features that are more accurate than those
learned by existing algorithms. Code will be available at
https://github.com/takahikof/RIPT_SDMM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furuya_T/0/1/0/all/0/1&quot;&gt;Takahiko Furuya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhoujie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohbuchi_R/0/1/0/all/0/1&quot;&gt;Ryutarou Ohbuchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Kuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04733">
<title>TextPainter: Multimodal Text Image Generation withVisual-harmony and Text-comprehension for Poster Design. (arXiv:2308.04733v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04733</link>
<description rdf:parseType="Literal">&lt;p&gt;Text design is one of the most critical procedures in poster design, as it
relies heavily on the creativity and expertise of humans to design text images
considering the visual harmony and text-semantic. This study introduces
TextPainter, a novel multimodal approach that leverages contextual visual
information and corresponding text semantics to generate text images.
Specifically, TextPainter takes the global-local background image as a hint of
style and guides the text image generation with visual harmony. Furthermore, we
leverage the language model and introduce a text comprehension module to
achieve both sentence-level and word-level style variations. Besides, we
construct the PosterT80K dataset, consisting of about 80K posters annotated
with sentence-level bounding boxes and text contents. We hope this dataset will
pave the way for further research on multimodal text image generation.
Extensive quantitative and qualitative experiments demonstrate that TextPainter
can generatevisually-and-semantically-harmonious text images for posters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yifan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jinpeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Min Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuanbin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Hongtao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tiezheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuning Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04753">
<title>SAfER: Layer-Level Sensitivity Assessment for Efficient and Robust Neural Network Inference. (arXiv:2308.04753v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04753</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) demonstrate outstanding performance across most
computer vision tasks. Some critical applications, such as autonomous driving
or medical imaging, also require investigation into their behavior and the
reasons behind the decisions they make. In this vein, DNN attribution consists
in studying the relationship between the predictions of a DNN and its inputs.
Attribution methods have been adapted to highlight the most relevant weights or
neurons in a DNN, allowing to more efficiently select which weights or neurons
can be pruned. However, a limitation of these approaches is that weights are
typically compared within each layer separately, while some layers might appear
as more critical than others. In this work, we propose to investigate DNN layer
importance, i.e. to estimate the sensitivity of the accuracy w.r.t.
perturbations applied at the layer level. To do so, we propose a novel dataset
to evaluate our method as well as future works. We benchmark a number of
criteria and draw conclusions regarding how to assess DNN layer importance and,
consequently, how to budgetize layers for increased DNN efficiency (with
applications for DNN pruning and quantization), as well as robustness to
hardware failure (e.g. bit swaps).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yvinec_E/0/1/0/all/0/1&quot;&gt;Edouard Yvinec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dapogny_A/0/1/0/all/0/1&quot;&gt;Arnaud Dapogny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailly_K/0/1/0/all/0/1&quot;&gt;Kevin Bailly&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04758">
<title>Bird&apos;s-Eye-View Scene Graph for Vision-Language Navigation. (arXiv:2308.04758v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04758</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language navigation (VLN), which entails an agent to navigate 3D
environments following human instructions, has shown great advances. However,
current agents are built upon panoramic observations, which hinders their
ability to perceive 3D scene geometry and easily leads to ambiguous selection
of panoramic view. To address these limitations, we present a BEV Scene Graph
(BSG), which leverages multi-step BEV representations to encode scene layouts
and geometric cues of indoor environment under the supervision of 3D detection.
During navigation, BSG builds a local BEV representation at each step and
maintains a BEV-based global scene map, which stores and organizes all the
online collected local BEV representations according to their topological
relations. Based on BSG, the agent predicts a local BEV grid-level decision
score and a global graph-level decision score, combined with a sub-view
selection score on panoramic views, for more accurate action prediction. Our
approach significantly outperforms state-of-the-art methods on REVERIE, R2R,
and R4R, showing the potential of BEV perception in VLN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Rui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04765">
<title>FaceSkin: A Privacy Preserving Facial skin patch Dataset for multi Attributes classification. (arXiv:2308.04765v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04765</link>
<description rdf:parseType="Literal">&lt;p&gt;Human facial skin images contain abundant textural information that can serve
as valuable features for attribute classification, such as age, race, and
gender. Additionally, facial skin images offer the advantages of easy
collection and minimal privacy concerns. However, the availability of
well-labeled human skin datasets with a sufficient number of images is limited.
To address this issue, we introduce a dataset called FaceSkin, which
encompasses a diverse range of ages and races. Furthermore, to broaden the
application scenarios, we incorporate synthetic skin-patches obtained from 2D
and 3D attack images, including printed paper, replays, and 3D masks. We
evaluate the FaceSkin dataset across distinct categories and present
experimental results demonstrating its effectiveness in attribute
classification, as well as its potential for various downstream tasks, such as
Face anti-spoofing and Age estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiushi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1&quot;&gt;Shisha Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04767">
<title>Induction Network: Audio-Visual Modality Gap-Bridging for Self-Supervised Sound Source Localization. (arXiv:2308.04767v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04767</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised sound source localization is usually challenged by the
modality inconsistency. In recent studies, contrastive learning based
strategies have shown promising to establish such a consistent correspondence
between audio and sound sources in visual scenarios. Unfortunately, the
insufficient attention to the heterogeneity influence in the different modality
features still limits this scheme to be further improved, which also becomes
the motivation of our work. In this study, an Induction Network is proposed to
bridge the modality gap more effectively. By decoupling the gradients of visual
and audio modalities, the discriminative visual representations of sound
sources can be learned with the designed Induction Vector in a bootstrap
manner, which also enables the audio modality to be aligned with the visual
modality consistently. In addition to a visual weighted contrastive loss, an
adaptive threshold selection strategy is introduced to enhance the robustness
of the Induction Network. Substantial experiments conducted on SoundNet-Flickr
and VGG-Sound Source datasets have demonstrated a superior performance compared
to other state-of-the-art works in different challenging scenarios. The code is
available at https://github.com/Tahy1/AVIN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1&quot;&gt;Yufei Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_T/0/1/0/all/0/1&quot;&gt;Tao You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04770">
<title>Objects do not disappear: Video object detection by single-frame object location anticipation. (arXiv:2308.04770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04770</link>
<description rdf:parseType="Literal">&lt;p&gt;Objects in videos are typically characterized by continuous smooth motion. We
exploit continuous smooth motion in three ways. 1) Improved accuracy by using
object motion as an additional source of supervision, which we obtain by
anticipating object locations from a static keyframe. 2) Improved efficiency by
only doing the expensive feature computations on a small subset of all frames.
Because neighboring video frames are often redundant, we only compute features
for a single static keyframe and predict object locations in subsequent frames.
3) Reduced annotation cost, where we only annotate the keyframe and use smooth
pseudo-motion between keyframes. We demonstrate computational efficiency,
annotation efficiency, and improved mean average precision compared to the
state-of-the-art on four datasets: ImageNet VID, EPIC KITCHENS-55,
YouTube-BoundingBoxes, and Waymo Open dataset. Our source code is available at
https://github.com/L-KID/Videoobject-detection-by-location-anticipation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejadasl_F/0/1/0/all/0/1&quot;&gt;Fatemeh Karimi Nejadasl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gemert_J/0/1/0/all/0/1&quot;&gt;Jan C. van Gemert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Booij_O/0/1/0/all/0/1&quot;&gt;Olaf Booij&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pintea_S/0/1/0/all/0/1&quot;&gt;Silvia L. Pintea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04771">
<title>SUnAA: Sparse Unmixing using Archetypal Analysis. (arXiv:2308.04771v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04771</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new sparse unmixing technique using archetypal
analysis (SUnAA). First, we design a new model based on archetypal analysis. We
assume that the endmembers of interest are a convex combination of endmembers
provided by a spectral library and that the number of endmembers of interest is
known. Then, we propose a minimization problem. Unlike most conventional sparse
unmixing methods, here the minimization problem is non-convex. We minimize the
optimization objective iteratively using an active set algorithm. Our method is
robust to the initialization and only requires the number of endmembers of
interest. SUnAA is evaluated using two simulated datasets for which results
confirm its better performance over other conventional and advanced techniques
in terms of signal-to-reconstruction error. SUnAA is also applied to Cuprite
dataset and the results are compared visually with the available geological map
provided for this dataset. The qualitative assessment demonstrates the
successful estimation of the minerals abundances and significantly improves the
detection of dominant minerals compared to the conventional regression-based
sparse unmixing methods. The Python implementation of SUnAA can be found at:
https://github.com/BehnoodRasti/SUnAA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1&quot;&gt;Behnood Rasti&lt;/a&gt; (HZDR), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zouaoui_A/0/1/0/all/0/1&quot;&gt;Alexandre Zouaoui&lt;/a&gt; (Thoth), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1&quot;&gt;Julien Mairal&lt;/a&gt; (Thoth), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1&quot;&gt;Jocelyn Chanussot&lt;/a&gt; (Thoth)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04774">
<title>E3-UAV: An Edge-based Energy-Efficient Object Detection System for Unmanned Aerial Vehicles. (arXiv:2308.04774v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2308.04774</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the advances in deep learning techniques, the application of
Unmanned Aerial Vehicle (UAV)-based object detection has proliferated across a
range of fields, including vehicle counting, fire detection, and city
monitoring. While most existing research studies only a subset of the
challenges inherent to UAV-based object detection, there are few studies that
balance various aspects to design a practical system for energy consumption
reduction. In response, we present the E3-UAV, an edge-based energy-efficient
object detection system for UAVs. The system is designed to dynamically support
various UAV devices, edge devices, and detection algorithms, with the aim of
minimizing energy consumption by deciding the most energy-efficient flight
parameters (including flight altitude, flight speed, detection algorithm, and
sampling rate) required to fulfill the detection requirements of the task. We
first present an effective evaluation metric for actual tasks and construct a
transparent energy consumption model based on hundreds of actual flight data to
formalize the relationship between energy consumption and flight parameters.
Then we present a lightweight energy-efficient priority decision algorithm
based on a large quantity of actual flight data to assist the system in
deciding flight parameters. Finally, we evaluate the performance of the system,
and our experimental results demonstrate that it can significantly decrease
energy consumption in real-world scenarios. Additionally, we provide four
insights that can assist researchers and engineers in their efforts to study
UAV-based object detection further.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suo_J/0/1/0/all/0/1&quot;&gt;Jiashun Suo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingzhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Weisong Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04779">
<title>Multi-View Fusion and Distillation for Subgrade Distresses Detection based on 3D-GPR. (arXiv:2308.04779v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04779</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of 3D ground-penetrating radar (3D-GPR) for subgrade distress
detection has gained widespread popularity. To enhance the efficiency and
accuracy of detection, pioneering studies have attempted to adopt automatic
detection techniques, particularly deep learning. However, existing works
typically rely on traditional 1D A-scan, 2D B-scan or 3D C-scan data of the
GPR, resulting in either insufficient spatial information or high computational
complexity. To address these challenges, we introduce a novel methodology for
the subgrade distress detection task by leveraging the multi-view information
from 3D-GPR data. Moreover, we construct a real multi-view image dataset
derived from the original 3D-GPR data for the detection task, which provides
richer spatial information compared to A-scan and B-scan data, while reducing
computational complexity compared to C-scan data. Subsequently, we develop a
novel \textbf{M}ulti-\textbf{V}iew \textbf{V}usion and \textbf{D}istillation
framework, \textbf{GPR-MVFD}, specifically designed to optimally utilize the
multi-view GPR dataset. This framework ingeniously incorporates multi-view
distillation and attention-based fusion to facilitate significant feature
extraction for subgrade distresses. In addition, a self-adaptive learning
mechanism is adopted to stabilize the model training and prevent performance
degeneration in each branch. Extensive experiments conducted on this new GPR
benchmark demonstrate the effectiveness and efficiency of our proposed
framework. Our framework outperforms not only the existing GPR baselines, but
also the state-of-the-art methods in the fields of multi-view learning,
multi-modal learning, and knowledge distillation. We will release the
constructed multi-view GPR dataset with expert-annotated labels and the source
codes of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chunpeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_K/0/1/0/all/0/1&quot;&gt;Kangjie Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haishuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiajun Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04782">
<title>PointMBF: A Multi-scale Bidirectional Fusion Network for Unsupervised RGB-D Point Cloud Registration. (arXiv:2308.04782v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04782</link>
<description rdf:parseType="Literal">&lt;p&gt;Point cloud registration is a task to estimate the rigid transformation
between two unaligned scans, which plays an important role in many computer
vision applications. Previous learning-based works commonly focus on supervised
registration, which have limitations in practice. Recently, with the advance of
inexpensive RGB-D sensors, several learning-based works utilize RGB-D data to
achieve unsupervised registration. However, most of existing unsupervised
methods follow a cascaded design or fuse RGB-D data in a unidirectional manner,
which do not fully exploit the complementary information in the RGB-D data. To
leverage the complementary information more effectively, we propose a network
implementing multi-scale bidirectional fusion between RGB images and point
clouds generated from depth images. By bidirectionally fusing visual and
geometric features in multi-scales, more distinctive deep features for
correspondence estimation can be obtained, making our registration more
accurate. Extensive experiments on ScanNet and 3DMatch demonstrate that our
method achieves new state-of-the-art performance. Code will be released at
https://github.com/phdymz/PointMBF
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingzhi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_K/0/1/0/all/0/1&quot;&gt;Kexue Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yucong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Manning Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04789">
<title>Multi-Scale Memory Comparison for Zero-/Few-Shot Anomaly Detection. (arXiv:2308.04789v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04789</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection has gained considerable attention due to its broad range of
applications, particularly in industrial defect detection. To address the
challenges of data collection, researchers have introduced zero-/few-shot
anomaly detection techniques that require minimal normal images for each
category. However, complex industrial scenarios often involve multiple objects,
presenting a significant challenge. In light of this, we propose a
straightforward yet powerful multi-scale memory comparison framework for
zero-/few-shot anomaly detection. Our approach employs a global memory bank to
capture features across the entire image, while an individual memory bank
focuses on simplified scenes containing a single object. The efficacy of our
method is validated by its remarkable achievement of 4th place in the zero-shot
track and 2nd place in the few-shot track of the Visual Anomaly and Novelty
Detection (VAND) competition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoqin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_A/0/1/0/all/0/1&quot;&gt;Aofan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04798">
<title>Enhancing Mobile Privacy and Security: A Face Skin Patch-Based Anti-Spoofing Approach. (arXiv:2308.04798v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04798</link>
<description rdf:parseType="Literal">&lt;p&gt;As Facial Recognition System(FRS) is widely applied in areas such as access
control and mobile payments due to its convenience and high accuracy. The
security of facial recognition is also highly regarded. The Face anti-spoofing
system(FAS) for face recognition is an important component used to enhance the
security of face recognition systems. Traditional FAS used images containing
identity information to detect spoofing traces, however there is a risk of
privacy leakage during the transmission and storage of these images. Besides,
the encryption and decryption of these privacy-sensitive data takes too long
compared to inference time by FAS model. To address the above issues, we
propose a face anti-spoofing algorithm based on facial skin patches leveraging
pure facial skin patch images as input, which contain no privacy information,
no encryption or decryption is needed for these images. We conduct experiments
on several public datasets, the results prove that our algorithm has
demonstrated superiority in both accuracy and speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qiushi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04802">
<title>Generalized Unbiased Scene Graph Generation. (arXiv:2308.04802v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04802</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing Unbiased Scene Graph Generation (USGG) methods only focus on
addressing the predicate-level imbalance that high-frequency classes dominate
predictions of rare ones, while overlooking the concept-level imbalance.
Actually, even if predicates themselves are balanced, there is still a
significant concept-imbalance within them due to the long-tailed distribution
of contexts (i.e., subject-object combinations). This concept-level imbalance
poses a more pervasive and challenging issue compared to the predicate-level
imbalance since subject-object pairs are inherently complex in combinations.
Hence, we introduce a novel research problem: Generalized Unbiased Scene Graph
Generation (G-USGG), which takes into account both predicate-level and
concept-level imbalance. To the end, we propose the Multi-Concept Learning
(MCL) framework, which ensures a balanced learning process across rare/
uncommon/ common concepts. MCL first quantifies the concept-level imbalance
across predicates in terms of different amounts of concepts, representing as
multiple concept-prototypes within the same class. It then effectively learns
concept-prototypes by applying the Concept Regularization (CR) technique.
Furthermore, to achieve balanced learning over different concepts, we introduce
the Balanced Prototypical Memory (BPM), which guides SGG models to generate
balanced representations for concept-prototypes. Extensive experiments
demonstrate the remarkable efficacy of our model-agnostic strategy in enhancing
the performance of benchmark models on both VG-SGG and OI-SGG datasets, leading
to new state-of-the-art achievements in two key aspects: predicate-level
unbiased relation recognition and concept-level compositional generability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xinyu Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Junlin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_P/0/1/0/all/0/1&quot;&gt;Pengpeng Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yulu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jie Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04808">
<title>Joint-Relation Transformer for Multi-Person Motion Prediction. (arXiv:2308.04808v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04808</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-person motion prediction is a challenging problem due to the dependency
of motion on both individual past movements and interactions with other people.
Transformer-based methods have shown promising results on this task, but they
miss the explicit relation representation between joints, such as skeleton
structure and pairwise distance, which is crucial for accurate interaction
modeling. In this paper, we propose the Joint-Relation Transformer, which
utilizes relation information to enhance interaction modeling and improve
future motion prediction. Our relation information contains the relative
distance and the intra-/inter-person physical constraints. To fuse relation and
joint information, we design a novel joint-relation fusion layer with
relation-aware attention to update both features. Additionally, we supervise
the relation information by forecasting future distance. Experiments show that
our method achieves a 13.4% improvement of 900ms VIM on 3DPW-SoMoF/RC and
17.8%/12.0% improvement of 3s MPJPE on CMU-Mpcap/MuPoTS-3D dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingyao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Weibo Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_J/0/1/0/all/0/1&quot;&gt;Jingze Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenxin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weidi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04821">
<title>HyperCoil-Recon: A Hypernetwork-based Adaptive Coil Configuration Task Switching Network for MRI Reconstruction. (arXiv:2308.04821v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04821</link>
<description rdf:parseType="Literal">&lt;p&gt;Parallel imaging, a fast MRI technique, involves dynamic adjustments based on
the configuration i.e. number, positioning, and sensitivity of the coils with
respect to the anatomy under study. Conventional deep learning-based image
reconstruction models have to be trained or fine-tuned for each configuration,
posing a barrier to clinical translation, given the lack of computational
resources and machine learning expertise for clinicians to train models at
deployment. Joint training on diverse datasets learns a single weight set that
might underfit to deviated configurations. We propose, HyperCoil-Recon, a
hypernetwork-based coil configuration task-switching network for multi-coil MRI
reconstruction that encodes varying configurations of the numbers of coils in a
multi-tasking perspective, posing each configuration as a task. The
hypernetworks infer and embed task-specific weights into the reconstruction
network, 1) effectively utilizing the contextual knowledge of common and
varying image features among the various fields-of-view of the coils, and 2)
enabling generality to unseen configurations at test time. Experiments reveal
that our approach 1) adapts on the fly to various unseen configurations up to
32 coils when trained on lower numbers (i.e. 7 to 11) of randomly varying
coils, and to 120 deviated unseen configurations when trained on 18
configurations in a single model, 2) matches the performance of coil
configuration-specific models, and 3) outperforms configuration-invariant
models with improvement margins of around 1 dB / 0.03 and 0.3 dB / 0.02 in PSNR
/ SSIM for knee and brain data. Our code is available at
https://github.com/sriprabhar/HyperCoil-Recon
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ramanarayanan_S/0/1/0/all/0/1&quot;&gt;Sriprabha Ramanarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fahim_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Fahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+S%2E_R/0/1/0/all/0/1&quot;&gt;Rahul G.S.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jethi_A/0/1/0/all/0/1&quot;&gt;Amrit Kumar Jethi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ram_K/0/1/0/all/0/1&quot;&gt;Keerthi Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sivaprakasam_M/0/1/0/all/0/1&quot;&gt;Mohanasankar Sivaprakasam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04826">
<title>WaveNeRF: Wavelet-based Generalizable Neural Radiance Fields. (arXiv:2308.04826v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04826</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has shown impressive performance in novel view
synthesis via implicit scene representation. However, it usually suffers from
poor scalability as requiring densely sampled images for each new scene.
Several studies have attempted to mitigate this problem by integrating
Multi-View Stereo (MVS) technique into NeRF while they still entail a
cumbersome fine-tuning process for new scenes. Notably, the rendering quality
will drop severely without this fine-tuning process and the errors mainly
appear around the high-frequency features. In the light of this observation, we
design WaveNeRF, which integrates wavelet frequency decomposition into MVS and
NeRF to achieve generalizable yet high-quality synthesis without any per-scene
optimization. To preserve high-frequency information when generating 3D feature
volumes, WaveNeRF builds Multi-View Stereo in the Wavelet domain by integrating
the discrete wavelet transform into the classical cascade MVS, which
disentangles high-frequency information explicitly. With that, disentangled
frequency features can be injected into classic NeRF via a novel hybrid neural
renderer to yield faithful high-frequency details, and an intuitive
frequency-guided sampling strategy can be designed to suppress artifacts around
high-frequency regions. Extensive experiments over three widely studied
benchmarks show that WaveNeRF achieves superior generalizable radiance field
modeling when only given three images as input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Muyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1&quot;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yingchen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoqin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Ling Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04828">
<title>Seeing in Flowing: Adapting CLIP for Action Recognition with Motion Prompts Learning. (arXiv:2308.04828v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04828</link>
<description rdf:parseType="Literal">&lt;p&gt;The Contrastive Language-Image Pre-training (CLIP) has recently shown
remarkable generalization on &quot;zero-shot&quot; training and has applied to many
downstream tasks. We explore the adaptation of CLIP to achieve a more efficient
and generalized action recognition method. We propose that the key lies in
explicitly modeling the motion cues flowing in video frames. To that end, we
design a two-stream motion modeling block to capture motion and spatial
information at the same time. And then, the obtained motion cues are utilized
to drive a dynamic prompts learner to generate motion-aware prompts, which
contain much semantic information concerning human actions. In addition, we
propose a multimodal communication block to achieve a collaborative learning
and further improve the performance. We conduct extensive experiments on
HMDB-51, UCF-101, and Kinetics-400 datasets. Our method outperforms most
existing state-of-the-art methods by a significant margin on &quot;few-shot&quot; and
&quot;zero-shot&quot; training. We also achieve competitive performance on &quot;closed-set&quot;
training with extremely few trainable parameters and additional computational
costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Junlong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shouhong Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04829">
<title>MixReorg: Cross-Modal Mixed Patch Reorganization is a Good Mask Learner for Open-World Semantic Segmentation. (arXiv:2308.04829v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04829</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, semantic segmentation models trained with image-level text
supervision have shown promising results in challenging open-world scenarios.
However, these models still face difficulties in learning fine-grained semantic
alignment at the pixel level and predicting accurate object masks. To address
this issue, we propose MixReorg, a novel and straightforward pre-training
paradigm for semantic segmentation that enhances a model&apos;s ability to
reorganize patches mixed across images, exploring both local visual relevance
and global semantic coherence. Our approach involves generating fine-grained
patch-text pairs data by mixing image patches while preserving the
correspondence between patches and text. The model is then trained to minimize
the segmentation loss of the mixed images and the two contrastive losses of the
original and restored features. With MixReorg as a mask learner, conventional
text-supervised semantic segmentation models can achieve highly generalizable
pixel-semantic alignment ability, which is crucial for open-world segmentation.
After training with large-scale image-text data, MixReorg models can be applied
directly to segment visual objects of arbitrary categories, without the need
for further fine-tuning. Our proposed framework demonstrates strong performance
on popular zero-shot semantic segmentation benchmarks, outperforming GroupViT
by significant margins of 5.0%, 6.2%, 2.5%, and 3.4% mIoU on PASCAL VOC2012,
PASCAL Context, MS COCO, and ADE20K, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_K/0/1/0/all/0/1&quot;&gt;Kaixin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_P/0/1/0/all/0/1&quot;&gt;Pengzhen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangrun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04830">
<title>VAST: Vivify Your Talking Avatar via Zero-Shot Expressive Facial Style Transfer. (arXiv:2308.04830v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04830</link>
<description rdf:parseType="Literal">&lt;p&gt;Current talking face generation methods mainly focus on speech-lip
synchronization. However, insufficient investigation on the facial talking
style leads to a lifeless and monotonous avatar. Most previous works fail to
imitate expressive styles from arbitrary video prompts and ensure the
authenticity of the generated video. This paper proposes an unsupervised
variational style transfer model (VAST) to vivify the neutral photo-realistic
avatars. Our model consists of three key components: a style encoder that
extracts facial style representations from the given video prompts; a hybrid
facial expression decoder to model accurate speech-related movements; a
variational style enhancer that enhances the style space to be highly
expressive and meaningful. With our essential designs on facial style learning,
our model is able to flexibly capture the expressive facial style from
arbitrary video prompts and transfer it onto a personalized image renderer in a
zero-shot manner. Experimental results demonstrate the proposed approach
contributes to a more vivid talking avatar with higher authenticity and richer
expressiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Runnan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_W/0/1/0/all/0/1&quot;&gt;Weihong Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_J/0/1/0/all/0/1&quot;&gt;Jun Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_X/0/1/0/all/0/1&quot;&gt;Xu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Sheng Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04832">
<title>TSSR: A Truncated and Signed Square Root Activation Function for Neural Networks. (arXiv:2308.04832v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04832</link>
<description rdf:parseType="Literal">&lt;p&gt;Activation functions are essential components of neural networks. In this
paper, we introduce a new activation function called the Truncated and Signed
Square Root (TSSR) function. This function is distinctive because it is odd,
nonlinear, monotone and differentiable. Its gradient is continuous and always
positive. Thanks to these properties, it has the potential to improve the
numerical stability of neural networks. Several experiments confirm that the
proposed TSSR has better performance than other stat-of-the-art activation
functions. The proposed function has significant implications for the
development of neural network models and can be applied to a wide range of
applications in fields such as computer vision, natural language processing,
and speech recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yuanhao Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04834">
<title>View while Moving: Efficient Video Recognition in Long-untrimmed Videos. (arXiv:2308.04834v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04834</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent adaptive methods for efficient video recognition mostly follow the
two-stage paradigm of &quot;preview-then-recognition&quot; and have achieved great
success on multiple video benchmarks. However, this two-stage paradigm involves
two visits of raw frames from coarse-grained to fine-grained during inference
(cannot be parallelized), and the captured spatiotemporal features cannot be
reused in the second stage (due to varying granularity), being not friendly to
efficiency and computation optimization. To this end, inspired by human
cognition, we propose a novel recognition paradigm of &quot;View while Moving&quot; for
efficient long-untrimmed video recognition. In contrast to the two-stage
paradigm, our paradigm only needs to access the raw frame once. The two phases
of coarse-grained sampling and fine-grained recognition are combined into
unified spatiotemporal modeling, showing great performance. Moreover, we
investigate the properties of semantic units in video and propose a
hierarchical mechanism to efficiently capture and reason about the unit-level
and video-level temporal semantics in long-untrimmed videos respectively.
Extensive experiments on both long-untrimmed and short-trimmed videos
demonstrate that our approach outperforms state-of-the-art methods in terms of
accuracy as well as efficiency, yielding new efficiency and accuracy trade-offs
for video spatiotemporal modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mengyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lanshan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Que_X/0/1/0/all/0/1&quot;&gt;Xirong Que&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wendong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04868">
<title>InstantAvatar: Efficient 3D Head Reconstruction via Surface Rendering. (arXiv:2308.04868v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04868</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in full-head reconstruction have been obtained by optimizing
a neural field through differentiable surface or volume rendering to represent
a single scene. While these techniques achieve an unprecedented accuracy, they
take several minutes, or even hours, due to the expensive optimization process
required. In this work, we introduce InstantAvatar, a method that recovers
full-head avatars from few images (down to just one) in a few seconds on
commodity hardware. In order to speed up the reconstruction process, we propose
a system that combines, for the first time, a voxel-grid neural field
representation with a surface renderer. Notably, a naive combination of these
two techniques leads to unstable optimizations that do not converge to valid
solutions. In order to overcome this limitation, we present a novel statistical
model that learns a prior distribution over 3D head signed distance functions
using a voxel-grid based architecture. The use of this prior model, in
combination with other design choices, results into a system that achieves 3D
head reconstructions with comparable accuracy as the state-of-the-art with a
100x speed-up.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Canela_A/0/1/0/all/0/1&quot;&gt;Antonio Canela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caselles_P/0/1/0/all/0/1&quot;&gt;Pol Caselles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_I/0/1/0/all/0/1&quot;&gt;Ibrar Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garces_G/0/1/0/all/0/1&quot;&gt;Gil Triginer Garces&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramon_E/0/1/0/all/0/1&quot;&gt;Eduard Ramon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_J/0/1/0/all/0/1&quot;&gt;Jaime Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_Riera_J/0/1/0/all/0/1&quot;&gt;Jordi S&amp;#xe1;nchez-Riera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04872">
<title>Tracking Players in a Badminton Court by Two Cameras. (arXiv:2308.04872v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04872</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposes a simple method for multi-object tracking (MOT) of
players in a badminton court. We leverage two off-the-shelf cameras, one on the
top of the court and the other on the side of the court. The one on the top is
to track players&apos; trajectories, while the one on the side is to analyze the
pixel features of players. By computing the correlations between adjacent
frames and engaging the information of the two cameras, MOT of badminton
players is obtained. This two-camera approach addresses the challenge of player
occlusion and overlapping in a badminton court, providing player trajectory
tracking and multi-angle analysis. The presented system offers insights into
the positions and movements of badminton players, thus serving as a coaching or
self-training tool for badminton players to improve their gaming strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_Y/0/1/0/all/0/1&quot;&gt;Young-Ching Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shen-Ru Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo-Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong-Qi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Cheng-Kuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1&quot;&gt;Yu-Chee Tseng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04880">
<title>Learning multi-domain feature relation for visible and Long-wave Infrared image patch matching. (arXiv:2308.04880v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04880</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learning-based algorithms have achieved promising performance on
cross-spectral image patch matching, which, however, is still far from
satisfactory for practical application. On the one hand, a lack of large-scale
dataset with diverse scenes haunts its further improvement for learning-based
algorithms, whose performances and generalization rely heavily on the dataset
size and diversity. On the other hand, more emphasis has been put on feature
relation in the spatial domain whereas the scale dependency between features
has often been ignored, leading to performance degeneration especially when
encountering significant appearance variations for cross-spectral patches. To
address these issues, we publish, to be best of our knowledge, the largest
visible and Long-wave Infrared (LWIR) image patch matching dataset, termed
VL-CMIM, which contains 1300 pairs of strictly aligned visible and LWIR images
and over 2 million patch pairs covering diverse scenes such as asteroid, field,
country, build, street and water.In addition, a multi-domain feature relation
learning network (MD-FRN) is proposed. Input by the features extracted from a
four-branch network, both feature relations in spatial and scale domains are
learned via a spatial correlation module (SCM) and multi-scale adaptive
aggregation module (MSAG), respectively. To further aggregate the multi-domain
relations, a deep domain interactive mechanism (DIM) is applied, where the
learnt spatial-relation and scale-relation features are exchanged and further
input into MSCRM and SCM. This mechanism allows our model to learn interactive
cross-domain feature relations, leading to improved robustness to significant
appearance changes due to different modality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiuwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zhaoshuai Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04883">
<title>Deep Generative Networks for Heterogeneous Augmentation of Cranial Defects. (arXiv:2308.04883v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04883</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of personalized cranial implants is a challenging and tremendous
task that has become a hot topic in terms of process automation with the use of
deep learning techniques. The main challenge is associated with the high
diversity of possible cranial defects. The lack of appropriate data sources
negatively influences the data-driven nature of deep learning algorithms.
Hence, one of the possible solutions to overcome this problem is to rely on
synthetic data. In this work, we propose three volumetric variations of deep
generative models to augment the dataset by generating synthetic skulls, i.e.
Wasserstein Generative Adversarial Network with Gradient Penalty (WGAN-GP),
WGAN-GP hybrid with Variational Autoencoder pretraining (VAE/WGAN-GP) and
Introspective Variational Autoencoder (IntroVAE). We show that it is possible
to generate dozens of thousands of defective skulls with compatible defects
that achieve a trade-off between defect heterogeneity and the realistic shape
of the skull. We evaluate obtained synthetic data quantitatively by defect
segmentation with the use of V-Net and qualitatively by their latent space
exploration. We show that the synthetically generated skulls highly improve the
segmentation process compared to using only the original unaugmented data. The
generated skulls may improve the automatic design of personalized cranial
implants for real medical cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kwarciak_K/0/1/0/all/0/1&quot;&gt;Kamil Kwarciak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wodzinski_M/0/1/0/all/0/1&quot;&gt;Marek Wodzinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04886">
<title>Unsupervised Out-of-Distribution Dialect Detection with Mahalanobis Distance. (arXiv:2308.04886v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2308.04886</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialect classification is used in a variety of applications, such as machine
translation and speech recognition, to improve the overall performance of the
system. In a real-world scenario, a deployed dialect classification model can
encounter anomalous inputs that differ from the training data distribution,
also called out-of-distribution (OOD) samples. Those OOD samples can lead to
unexpected outputs, as dialects of those samples are unseen during model
training. Out-of-distribution detection is a new research area that has
received little attention in the context of dialect classification. Towards
this, we proposed a simple yet effective unsupervised Mahalanobis distance
feature-based method to detect out-of-distribution samples. We utilize the
latent embeddings from all intermediate layers of a wav2vec 2.0
transformer-based dialect classifier model for multi-task learning. Our
proposed approach outperforms other state-of-the-art OOD detection methods
significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Sourya Dipta Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vadi_Y/0/1/0/all/0/1&quot;&gt;Yash Vadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unnam_A/0/1/0/all/0/1&quot;&gt;Abhishek Unnam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_K/0/1/0/all/0/1&quot;&gt;Kuldeep Yadav&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04892">
<title>Transmission and Color-guided Network for Underwater Image Enhancement. (arXiv:2308.04892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04892</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, with the continuous development of the marine industry,
underwater image enhancement has attracted plenty of attention. Unfortunately,
the propagation of light in water will be absorbed by water bodies and
scattered by suspended particles, resulting in color deviation and low
contrast. To solve these two problems, we propose an Adaptive Transmission and
Dynamic Color guided network (named ATDCnet) for underwater image enhancement.
In particular, to exploit the knowledge of physics, we design an Adaptive
Transmission-directed Module (ATM) to better guide the network. To deal with
the color deviation problem, we design a Dynamic Color-guided Module (DCM) to
post-process the enhanced image color. Further, we design an
Encoder-Decoder-based Compensation (EDC) structure with attention and a
multi-stage feature fusion mechanism to perform color restoration and contrast
enhancement simultaneously. Extensive experiments demonstrate the
state-of-the-art performance of the ATDCnet on multiple benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1&quot;&gt;Pan Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jing Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_H/0/1/0/all/0/1&quot;&gt;Haotian Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1&quot;&gt;Cong Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04899">
<title>Histogram-guided Video Colorization Structure with Spatial-Temporal Connection. (arXiv:2308.04899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04899</link>
<description rdf:parseType="Literal">&lt;p&gt;Video colorization, aiming at obtaining colorful and plausible results from
grayish frames, has aroused a lot of interest recently. Nevertheless, how to
maintain temporal consistency while keeping the quality of colorized results
remains challenging. To tackle the above problems, we present a
Histogram-guided Video Colorization with Spatial-Temporal connection structure
(named ST-HVC). To fully exploit the chroma and motion information, the joint
flow and histogram module is tailored to integrate the histogram and flow
features. To manage the blurred and artifact, we design a combination scheme
attending to temporal detail and flow feature combination. We further recombine
the histogram, flow and sharpness features via a U-shape network. Extensive
comparisons are conducted with several state-of-the-art image and video-based
methods, demonstrating that the developed method achieves excellent performance
both quantitatively and qualitatively in two video datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_P/0/1/0/all/0/1&quot;&gt;Pan Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hanning Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1&quot;&gt;Cong Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04904">
<title>StableVQA: A Deep No-Reference Quality Assessment Model for Video Stability. (arXiv:2308.04904v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04904</link>
<description rdf:parseType="Literal">&lt;p&gt;Video shakiness is an unpleasant distortion of User Generated Content (UGC)
videos, which is usually caused by the unstable hold of cameras. In recent
years, many video stabilization algorithms have been proposed, yet no specific
and accurate metric enables comprehensively evaluating the stability of videos.
Indeed, most existing quality assessment models evaluate video quality as a
whole without specifically taking the subjective experience of video stability
into consideration. Therefore, these models cannot measure the video stability
explicitly and precisely when severe shakes are present. In addition, there is
no large-scale video database in public that includes various degrees of shaky
videos with the corresponding subjective scores available, which hinders the
development of Video Quality Assessment for Stability (VQA-S). To this end, we
build a new database named StableDB that contains 1,952 diversely-shaky UGC
videos, where each video has a Mean Opinion Score (MOS) on the degree of video
stability rated by 34 subjects. Moreover, we elaborately design a novel VQA-S
model named StableVQA, which consists of three feature extractors to acquire
the optical flow, semantic, and blur features respectively, and a regression
layer to predict the final stability score. Extensive experiments demonstrate
that the StableVQA achieves a higher correlation with subjective opinions than
the existing VQA-S models and generic VQA models. The database and codes are
available at https://github.com/QMME/StableVQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_T/0/1/0/all/0/1&quot;&gt;Tengchuan Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04911">
<title>SLPT: Selective Labeling Meets Prompt Tuning on Label-Limited Lesion Segmentation. (arXiv:2308.04911v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04911</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image analysis using deep learning is often challenged by limited
labeled data and high annotation costs. Fine-tuning the entire network in
label-limited scenarios can lead to overfitting and suboptimal performance.
Recently, prompt tuning has emerged as a more promising technique that
introduces a few additional tunable parameters as prompts to a task-agnostic
pre-trained model, and updates only these parameters using supervision from
limited labeled data while keeping the pre-trained model unchanged. However,
previous work has overlooked the importance of selective labeling in downstream
tasks, which aims to select the most valuable downstream samples for annotation
to achieve the best performance with minimum annotation cost. To address this,
we propose a framework that combines selective labeling with prompt tuning
(SLPT) to boost performance in limited labels. Specifically, we introduce a
feature-aware prompt updater to guide prompt tuning and a TandEm Selective
LAbeling (TESLA) strategy. TESLA includes unsupervised diversity selection and
supervised selection using prompt-based uncertainty. In addition, we propose a
diversified visual prompt tuning strategy to provide multi-prompt-based
discrepant predictions for TESLA. We evaluate our method on liver tumor
segmentation and achieve state-of-the-art performance, outperforming
traditional fine-tuning with only 6% of tunable parameters, also achieving 94%
of full-data performance by labeling only 5% of the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_F/0/1/0/all/0/1&quot;&gt;Fan Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xinyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xiaoli Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Le Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Max Q.-H. Meng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04912">
<title>Cross-view Semantic Alignment for Livestreaming Product Recognition. (arXiv:2308.04912v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04912</link>
<description rdf:parseType="Literal">&lt;p&gt;Live commerce is the act of selling products online through live streaming.
The customer&apos;s diverse demands for online products introduce more challenges to
Livestreaming Product Recognition. Previous works have primarily focused on
fashion clothing data or utilize single-modal input, which does not reflect the
real-world scenario where multimodal data from various categories are present.
In this paper, we present LPR4M, a large-scale multimodal dataset that covers
34 categories, comprises 3 modalities (image, video, and text), and is 50?
larger than the largest publicly available dataset. LPR4M contains diverse
videos and noise modality pairs while exhibiting a long-tailed distribution,
resembling real-world problems. Moreover, a cRoss-vIew semantiC alignmEnt
(RICE) model is proposed to learn discriminative instance features from the
image and video views of the products. This is achieved through instance-level
contrastive learning and cross-view patch-level feature propagation. A novel
Patch Feature Reconstruction loss is proposed to penalize the semantic
misalignment between cross-view patches. Extensive experiments demonstrate the
effectiveness of RICE and provide insights into the importance of dataset
diversity and expressivity. The dataset and code are available at
https://github.com/adxcreative/RICE
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenjie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yanhua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xudong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04923">
<title>Deep Learning-Based Prediction of Fractional Flow Reserve along the Coronary Artery. (arXiv:2308.04923v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04923</link>
<description rdf:parseType="Literal">&lt;p&gt;Functionally significant coronary artery disease (CAD) is caused by plaque
buildup in the coronary arteries, potentially leading to narrowing of the
arterial lumen, i.e. coronary stenosis, that significantly obstructs blood flow
to the myocardium. The current reference for establishing the presence of a
functionally significant stenosis is invasive fractional flow reserve (FFR)
measurement. To avoid invasive measurements, non-invasive prediction of FFR
from coronary CT angiography (CCTA) has emerged. For this, machine learning
approaches, characterized by fast inference, are increasingly developed.
However, these methods predict a single FFR value per artery i.e. they don&apos;t
provide information about the stenosis location or treatment strategy. We
propose a deep learning-based method to predict the FFR along the artery from
CCTA scans. This study includes CCTA images of 110 patients who underwent
invasive FFR pullback measurement in 112 arteries. First, a multi planar
reconstruction (MPR) of the artery is fed to a variational autoencoder to
characterize the artery, i.e. through the lumen area and unsupervised artery
encodings. Thereafter, a convolutional neural network (CNN) predicts the FFR
along the artery. The CNN is supervised by multiple loss functions, notably a
loss function inspired by the Earth Mover&apos;s Distance (EMD) to predict the
correct location of FFR drops and a histogram-based loss to explicitly
supervise the slope of the FFR curve. To train and evaluate our model,
eight-fold cross-validation was performed. The resulting FFR curves show good
agreement with the reference allowing the distinction between diffuse and focal
CAD distributions in most cases. Quantitative evaluation yielded a mean
absolute difference in the area under the FFR pullback curve (AUPC) of 1.7. The
method may pave the way towards fast, accurate, automatic prediction of FFR
along the artery from CCTA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hampe_N/0/1/0/all/0/1&quot;&gt;Nils Hampe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Velzen_S/0/1/0/all/0/1&quot;&gt;Sanne G. M. van Velzen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aben_J/0/1/0/all/0/1&quot;&gt;Jean-Paul Aben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Collet_C/0/1/0/all/0/1&quot;&gt;Carlos Collet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isgum_I/0/1/0/all/0/1&quot;&gt;Ivana I&amp;#x161;gum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04928">
<title>GeodesicPSIM: Predicting the Quality of Static Mesh with Texture Map via Geodesic Patch Similarity. (arXiv:2308.04928v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04928</link>
<description rdf:parseType="Literal">&lt;p&gt;Static meshes with texture maps have attracted considerable attention in both
industrial manufacturing and academic research, leading to an urgent
requirement for effective and robust objective quality evaluation. However,
current model-based static mesh quality metrics have obvious limitations: most
of them only consider geometry information, while color information is ignored,
and they have strict constraints for the meshes&apos; geometrical topology. Other
metrics, such as image-based and point-based metrics, are easily influenced by
the prepossessing algorithms, e.g., projection and sampling, hampering their
ability to perform at their best. In this paper, we propose Geodesic Patch
Similarity (GeodesicPSIM), a novel model-based metric to accurately predict
human perception quality for static meshes. After selecting a group keypoints,
1-hop geodesic patches are constructed based on both the reference and
distorted meshes cleaned by an effective mesh cleaning algorithm. A two-step
patch cropping algorithm and a patch texture mapping module refine the size of
1-hop geodesic patches and build the relationship between the mesh geometry and
color information, resulting in the generation of 1-hop textured geodesic
patches. Three types of features are extracted to quantify the distortion:
patch color smoothness, patch discrete mean curvature, and patch pixel color
average and variance. To the best of our knowledge, GeodesicPSIM is the first
model-based metric especially designed for static meshes with texture maps.
GeodesicPSIM provides state-of-the-art performance in comparison with
image-based, point-based, and video-based metrics on a newly created and
challenging database. We also prove the robustness of GeodesicPSIM by
introducing different settings of hyperparameters. Ablation studies also
exhibit the effectiveness of three proposed features and the patch cropping
algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Joel Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04934">
<title>JEDI: Joint Expert Distillation in a Semi-Supervised Multi-Dataset Student-Teacher Scenario for Video Action Recognition. (arXiv:2308.04934v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose JEDI, a multi-dataset semi-supervised learning method, which
efficiently combines knowledge from multiple experts, learned on different
datasets, to train and improve the performance of individual, per dataset,
student models. Our approach achieves this by addressing two important problems
in current machine learning research: generalization across datasets and
limitations of supervised training due to scarcity of labeled data. We start
with an arbitrary number of experts, pretrained on their own specific dataset,
which form the initial set of student models. The teachers are immediately
derived by concatenating the feature representations from the penultimate
layers of the students. We then train all models in a student-teacher
semi-supervised learning scenario until convergence. In our efficient approach,
student-teacher training is carried out jointly and end-to-end, showing that
both students and teachers improve their generalization capacity during
training. We validate our approach on four video action recognition datasets.
By simultaneously considering all datasets within a unified semi-supervised
setting, we demonstrate significant improvements over the initial experts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bicsi_L/0/1/0/all/0/1&quot;&gt;Lucian Bicsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alexe_B/0/1/0/all/0/1&quot;&gt;Bogdan Alexe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ionescu_R/0/1/0/all/0/1&quot;&gt;Radu Tudor Ionescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leordeanu_M/0/1/0/all/0/1&quot;&gt;Marius Leordeanu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04944">
<title>Gaussian Image Anomaly Detection with Greedy Eigencomponent Selection. (arXiv:2308.04944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04944</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) in images, identifying significant deviations from
normality, is a critical issue in computer vision. This paper introduces a
novel approach to dimensionality reduction for AD using pre-trained
convolutional neural network (CNN) that incorporate EfficientNet models. We
investigate the importance of component selection and propose two types of tree
search approaches, both employing a greedy strategy, for optimal eigencomponent
selection. Our study conducts three main experiments to evaluate the
effectiveness of our approach. The first experiment explores the influence of
test set performance on component choice, the second experiment examines the
performance when we train on one anomaly type and evaluate on all other types,
and the third experiment investigates the impact of using a minimum number of
images for training and selecting them based on anomaly types. Our approach
aims to find the optimal subset of components that deliver the highest
performance score, instead of focusing solely on the proportion of variance
explained by each component and also understand the components behaviour in
different settings. Our results indicate that the proposed method surpasses
both Principal Component Analysis (PCA) and Negated Principal Component
Analysis (NPCA) in terms of detection accuracy, even when using fewer
components. Thus, our approach provides a promising alternative to conventional
dimensionality reduction techniques in AD, and holds potential to enhance the
efficiency and effectiveness of AD systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gula_T/0/1/0/all/0/1&quot;&gt;Tetiana Gula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o P C Bertoldo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04946">
<title>SelectNAdapt: Support Set Selection for Few-Shot Domain Adaptation. (arXiv:2308.04946v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04946</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalisation of deep neural networks becomes vulnerable when distribution
shifts are encountered between train (source) and test (target) domain data.
Few-shot domain adaptation mitigates this issue by adapting deep neural
networks pre-trained on the source domain to the target domain using a randomly
selected and annotated support set from the target domain. This paper argues
that randomly selecting the support set can be further improved for effectively
adapting the pre-trained source models to the target domain. Alternatively, we
propose SelectNAdapt, an algorithm to curate the selection of the target domain
samples, which are then annotated and included in the support set. In
particular, for the K-shot adaptation problem, we first leverage
self-supervision to learn features of the target domain data. Then, we propose
a per-class clustering scheme of the learned target domain features and select
K representative target samples using a distance-based scoring function.
Finally, we bring our selection setup towards a practical ground by relying on
pseudo-labels for clustering semantically similar target domain samples. Our
experiments show promising results on three few-shot domain adaptation
benchmarks for image recognition compared to related approaches and the
standard random selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawoud_Y/0/1/0/all/0/1&quot;&gt;Youssef Dawoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belagiannis_V/0/1/0/all/0/1&quot;&gt;Vasileios Belagiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04949">
<title>Branches Mutual Promotion for End-to-End Weakly Supervised Semantic Segmentation. (arXiv:2308.04949v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04949</link>
<description rdf:parseType="Literal">&lt;p&gt;End-to-end weakly supervised semantic segmentation aims at optimizing a
segmentation model in a single-stage training process based on only image
annotations. Existing methods adopt an online-trained classification branch to
provide pseudo annotations for supervising the segmentation branch. However,
this strategy makes the classification branch dominate the whole concurrent
training process, hindering these two branches from assisting each other. In
our work, we treat these two branches equally by viewing them as diverse ways
to generate the segmentation map, and add interactions on both their
supervision and operation to achieve mutual promotion. For this purpose, a
bidirectional supervision mechanism is elaborated to force the consistency
between the outputs of these two branches. Thus, the segmentation branch can
also give feedback to the classification branch to enhance the quality of
localization seeds. Moreover, our method also designs interaction operations
between these two branches to exchange their knowledge to assist each other.
Experiments indicate our work outperforms existing end-to-end weakly supervised
segmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hangzhou He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shuang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qiushi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yanye Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04952">
<title>Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation. (arXiv:2308.04952v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04952</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Few-shot Semantic Segmentation (GFSS) extends Few-shot Semantic
Segmentation (FSS) to simultaneously segment unseen classes and seen classes
during evaluation. Previous works leverage additional branch or prototypical
aggregation to eliminate the constrained setting of FSS. However,
representation division and embedding prejudice, which heavily results in poor
performance of GFSS, have not been synthetical considered. We address the
aforementioned problems by jointing the prototypical kernel learning and
open-set foreground perception. Specifically, a group of learnable kernels is
proposed to perform segmentation with each kernel in charge of a stuff class.
Then, we explore to merge the prototypical learning to the update of base-class
kernels, which is consistent with the prototype knowledge aggregation of
few-shot novel classes. In addition, a foreground contextual perception module
cooperating with conditional bias based inference is adopted to perform
class-agnostic as well as open-set foreground detection, thus to mitigate the
embedding prejudice and prevent novel targets from being misclassified as
background. Moreover, we also adjust our method to the Class Incremental
Few-shot Semantic Segmentation (CIFSS) which takes the knowledge of novel
classes in a incremental stream. Extensive experiments on PASCAL-5i and
COCO-20i datasets demonstrate that our method performs better than previous
state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Feigege Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Y/0/1/0/all/0/1&quot;&gt;Ye Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yutao Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04956">
<title>ACE-HetEM for ab initio Heterogenous Cryo-EM 3D Reconstruction. (arXiv:2308.04956v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.04956</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the extremely low signal-to-noise ratio (SNR) and unknown poses
(projection angles and image translation) in cryo-EM experiments,
reconstructing 3D structures from 2D images is very challenging. On top of
these challenges, heterogeneous cryo-EM reconstruction also has an additional
requirement: conformation classification. An emerging solution to this problem
is called amortized inference, implemented using the autoencoder architecture
or its variants. Instead of searching for the correct
image-to-pose/conformation mapping for every image in the dataset as in
non-amortized methods, amortized inference only needs to train an encoder that
maps images to appropriate latent spaces representing poses or conformations.
Unfortunately, standard amortized-inference-based methods with entangled latent
spaces have difficulty learning the distribution of conformations and poses
from cryo-EM images. In this paper, we propose an unsupervised deep learning
architecture called &quot;ACE-HetEM&quot; based on amortized inference. To explicitly
enforce the disentanglement of conformation classifications and pose
estimations, we designed two alternating training tasks in our method:
image-to-image task and pose-to-pose task. Results on simulated datasets show
that ACE-HetEM has comparable accuracy in pose estimation and produces even
better reconstruction resolution than non-amortized methods. Furthermore, we
show that ACE-HetEM is also applicable to real experimental datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_Z/0/1/0/all/0/1&quot;&gt;Zeqing Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04987">
<title>Self-supervised Landmark Learning with Deformation Reconstruction and Cross-subject Consistency Objectives. (arXiv:2308.04987v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04987</link>
<description rdf:parseType="Literal">&lt;p&gt;A Point Distribution Model (PDM) is the basis of a Statistical Shape Model
(SSM) that relies on a set of landmark points to represent a shape and
characterize the shape variation. In this work, we present a self-supervised
approach to extract landmark points from a given registration model for the
PDMs. Based on the assumption that the landmarks are the points that have the
most influence on registration, existing works learn a point-based registration
model with a small number of points to estimate the landmark points that
influence the deformation the most. However, such approaches assume that the
deformation can be captured by point-based registration and quality landmarks
can be learned solely with the deformation capturing objective. We argue that
data with complicated deformations can not easily be modeled with point-based
registration when only a limited number of points is used to extract
influential landmark points. Further, landmark consistency is not assured in
existing approaches In contrast, we propose to extract landmarks based on a
given registration model, which is tailored for the target data, so we can
obtain more accurate correspondences. Secondly, to establish the anatomical
consistency of the predicted landmarks, we introduce a landmark discovery loss
to explicitly encourage the model to predict the landmarks that are
anatomically consistent across subjects. We conduct experiments on an
osteoarthritis progression prediction task and show our method outperforms
existing image-based and point-based approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_C/0/1/0/all/0/1&quot;&gt;Chun-Hung Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1&quot;&gt;Marc Niethammer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04990">
<title>Foreground Object Search by Distilling Composite Image Feature. (arXiv:2308.04990v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04990</link>
<description rdf:parseType="Literal">&lt;p&gt;Foreground object search (FOS) aims to find compatible foreground objects for
a given background image, producing realistic composite image. We observe that
competitive retrieval performance could be achieved by using a discriminator to
predict the compatibility of composite image, but this approach has
unaffordable time cost. To this end, we propose a novel FOS method via
distilling composite feature (DiscoFOS). Specifically, the abovementioned
discriminator serves as teacher network. The student network employs two
encoders to extract foreground feature and background feature. Their
interaction output is enforced to match the composite image feature from the
teacher network. Additionally, previous works did not release their datasets,
so we contribute two datasets for FOS task: S-FOSD dataset with synthetic
composite images and R-FOSD dataset with real composite images. Extensive
experiments on our two datasets demonstrate the superiority of the proposed
method over previous approaches. The dataset and code are available at
https://github.com/bcmi/Foreground-Object-Search-Dataset-FOSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_J/0/1/0/all/0/1&quot;&gt;Jiacheng Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Li Niu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04995">
<title>IDiff-Face: Synthetic-based Face Recognition through Fizzy Identity-Conditioned Diffusion Models. (arXiv:2308.04995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.04995</link>
<description rdf:parseType="Literal">&lt;p&gt;The availability of large-scale authentic face databases has been crucial to
the significant advances made in face recognition research over the past
decade. However, legal and ethical concerns led to the recent retraction of
many of these databases by their creators, raising questions about the
continuity of future face recognition research without one of its key
resources. Synthetic datasets have emerged as a promising alternative to
privacy-sensitive authentic data for face recognition development. However,
recent synthetic datasets that are used to train face recognition models suffer
either from limitations in intra-class diversity or cross-class (identity)
discrimination, leading to less optimal accuracies, far away from the
accuracies achieved by models trained on authentic data. This paper targets
this issue by proposing IDiff-Face, a novel approach based on conditional
latent diffusion models for synthetic identity generation with realistic
identity variations for face recognition training. Through extensive
evaluations, our proposed synthetic-based face recognition approach pushed the
limits of state-of-the-art performances, achieving, for example, 98.00%
accuracy on the Labeled Faces in the Wild (LFW) benchmark, far ahead from the
recent synthetic-based face recognition solutions with 95.40% and bridging the
gap to authentic-based face recognition with 99.82% accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1&quot;&gt;Fadi Boutros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grebe_J/0/1/0/all/0/1&quot;&gt;Jonas Henry Grebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1&quot;&gt;Arjan Kuijper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dame_N/0/1/0/all/0/1&quot;&gt;Naser Dame&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05005">
<title>Deep Learning Model Transfer in Forest Mapping using Multi-source Satellite SAR and Optical Images. (arXiv:2308.05005v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2308.05005</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) models are gaining popularity in forest variable
prediction using Earth Observation images. However, in practical forest
inventories, reference datasets are often represented by plot- or stand-level
measurements, while high-quality representative wall-to-wall reference data for
end-to-end training of DL models are rarely available. Transfer learning
facilitates expansion of the use of deep learning models into areas with
sub-optimal training data by allowing pretraining of the model in areas where
high-quality teaching data are available. In this study, we perform a &quot;model
transfer&quot; (or domain adaptation) of a pretrained DL model into a target area
using plot-level measurements and compare performance versus other machine
learning models. We use an earlier developed UNet based model (SeUNet) to
demonstrate the approach on two distinct taiga sites with varying forest
structure and composition. Multisource Earth Observation (EO) data are
represented by a combination of Copernicus Sentinel-1 C-band SAR and Sentinel-2
multispectral images, JAXA ALOS-2 PALSAR-2 SAR mosaic and TanDEM-X bistatic
interferometric radar data. The training study site is located in Finnish
Lapland, while the target site is located in Southern Finland. By leveraging
transfer learning, the prediction of SeUNet achieved root mean squared error
(RMSE) of 2.70 m and R$^2$ of 0.882, considerably more accurate than
traditional benchmark methods. We expect such forest-specific DL model transfer
can be suitable also for other forest variables and other EO data sources that
are sensitive to forest structure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Shaojia Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Antropov_O/0/1/0/all/0/1&quot;&gt;Oleg Antropov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hame_T/0/1/0/all/0/1&quot;&gt;Tuomas H&amp;#xe4;me&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McRoberts_R/0/1/0/all/0/1&quot;&gt;Ronald E. McRoberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Miettinen_J/0/1/0/all/0/1&quot;&gt;Jukka Miettinen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05021">
<title>Do Diffusion Models Suffer Error Propagation? Theoretical Analysis and Consistency Regularization. (arXiv:2308.05021v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.05021</link>
<description rdf:parseType="Literal">&lt;p&gt;While diffusion models have achieved promising performances in data
synthesis, they might suffer error propagation because of their cascade
structure, where the distributional mismatch spreads and magnifies through the
chain of denoising modules. However, a strict analysis is expected since many
sequential models such as Conditional Random Field (CRF) are free from error
propagation. In this paper, we empirically and theoretically verify that
diffusion models are indeed affected by error propagation and we then propose a
regularization to address this problem. Our theoretical analysis reveals that
the question can be reduced to whether every denoising module of the diffusion
model is fault-tolerant. We derive insightful transition equations, indicating
that the module can&apos;t recover from input errors and even propagates additional
errors to the next module. Our analysis directly leads to a consistency
regularization scheme for diffusion models, which explicitly reduces the
distribution gap between forward and backward processes. We further introduce a
bootstrapping algorithm to reduce the computation cost of the regularizer. Our
experimental results on multiple image datasets show that our regularization
effectively handles error propagation and significantly improves the
performance of vanilla diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhaozhi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05022">
<title>Feature Modulation Transformer: Cross-Refinement of Global Representation via High-Frequency Prior for Image Super-Resolution. (arXiv:2308.05022v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05022</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based methods have exhibited remarkable potential in single image
super-resolution (SISR) by effectively extracting long-range dependencies.
However, most of the current research in this area has prioritized the design
of transformer blocks to capture global information, while overlooking the
importance of incorporating high-frequency priors, which we believe could be
beneficial. In our study, we conducted a series of experiments and found that
transformer structures are more adept at capturing low-frequency information,
but have limited capacity in constructing high-frequency representations when
compared to their convolutional counterparts. Our proposed solution, the
cross-refinement adaptive feature modulation transformer (CRAFT), integrates
the strengths of both convolutional and transformer structures. It comprises
three key components: the high-frequency enhancement residual block (HFERB) for
extracting high-frequency information, the shift rectangle window attention
block (SRWAB) for capturing global information, and the hybrid fusion block
(HFB) for refining the global representation. Our experiments on multiple
datasets demonstrate that CRAFT outperforms state-of-the-art methods by up to
0.29dB while using fewer parameters. The source code will be made available at:
https://github.com/AVC2-UESTC/CRAFT-SR.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Le Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Ce Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05026">
<title>An End-to-End Framework of Road User Detection, Tracking, and Prediction from Monocular Images. (arXiv:2308.05026v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05026</link>
<description rdf:parseType="Literal">&lt;p&gt;Perception that involves multi-object detection and tracking, and trajectory
prediction are two major tasks of autonomous driving. However, they are
currently mostly studied separately, which results in most trajectory
prediction modules being developed based on ground truth trajectories without
taking into account that trajectories extracted from the detection and tracking
modules in real-world scenarios are noisy. These noisy trajectories can have a
significant impact on the performance of the trajectory predictor and can lead
to serious prediction errors. In this paper, we build an end-to-end framework
for detection, tracking, and trajectory prediction called ODTP (Online
Detection, Tracking and Prediction). It adopts the state-of-the-art online
multi-object tracking model, QD-3DT, for perception and trains the trajectory
predictor, DCENet++, directly based on the detection results without purely
relying on ground truth trajectories. We evaluate the performance of ODTP on
the widely used nuScenes dataset for autonomous driving. Extensive experiments
show that ODPT achieves high performance end-to-end trajectory prediction.
DCENet++, with the enhanced dynamic maps, predicts more accurate trajectories
than its base model. It is also more robust when compared with other generative
and deterministic trajectory prediction models trained on noisy detection
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+liu_M/0/1/0/all/0/1&quot;&gt;Mengmeng liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05032">
<title>Density Crop-guided Semi-supervised Object Detection in Aerial Images. (arXiv:2308.05032v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05032</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the important bottlenecks in training modern object detectors is the
need for labeled images where bounding box annotations have to be produced for
each object present in the image. This bottleneck is further exacerbated in
aerial images where the annotators have to label small objects often
distributed in clusters on high-resolution images. In recent days, the
mean-teacher approach trained with pseudo-labels and weak-strong augmentation
consistency is gaining popularity for semi-supervised object detection.
However, a direct adaptation of such semi-supervised detectors for aerial
images where small clustered objects are often present, might not lead to
optimal results. In this paper, we propose a density crop-guided
semi-supervised detector that identifies the cluster of small objects during
training and also exploits them to improve performance at inference. During
training, image crops of clusters identified from labeled and unlabeled images
are used to augment the training set, which in turn increases the chance of
detecting small objects and creating good pseudo-labels for small objects on
the unlabeled images. During inference, the detector is not only able to detect
the objects of interest but also regions with a high density of small objects
(density crops) so that detections from the input image and detections from
image crops are combined, resulting in an overall more accurate object
prediction, especially for small objects. Empirical studies on the popular
benchmarks of VisDrone and DOTA datasets show the effectiveness of our density
crop-guided semi-supervised detector with an average improvement of more than
2\% over the basic mean-teacher method in COCO style AP. Our code is available
at: https://github.com/akhilpm/DroneSSOD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meethal_A/0/1/0/all/0/1&quot;&gt;Akhil Meethal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granger_E/0/1/0/all/0/1&quot;&gt;Eric Granger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersoli_M/0/1/0/all/0/1&quot;&gt;Marco Pedersoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05051">
<title>PAT: Position-Aware Transformer for Dense Multi-Label Action Detection. (arXiv:2308.05051v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05051</link>
<description rdf:parseType="Literal">&lt;p&gt;We present PAT, a transformer-based network that learns complex temporal
co-occurrence action dependencies in a video by exploiting multi-scale temporal
features. In existing methods, the self-attention mechanism in transformers
loses the temporal positional information, which is essential for robust action
detection. To address this issue, we (i) embed relative positional encoding in
the self-attention mechanism and (ii) exploit multi-scale temporal
relationships by designing a novel non hierarchical network, in contrast to the
recent transformer-based approaches that use a hierarchical structure. We argue
that joining the self-attention mechanism with multiple sub-sampling processes
in the hierarchical approaches results in increased loss of positional
information. We evaluate the performance of our proposed approach on two
challenging dense multi-label benchmark datasets, and show that PAT improves
the current state-of-the-art result by 1.1% and 0.6% mAP on the Charades and
MultiTHUMOS datasets, respectively, thereby achieving the new state-of-the-art
mAP at 26.5% and 44.6%, respectively. We also perform extensive ablation
studies to examine the impact of the different components of our proposed
network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sardari_F/0/1/0/all/0/1&quot;&gt;Faegheh Sardari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1&quot;&gt;Armin Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jackson_P/0/1/0/all/0/1&quot;&gt;Philip J. B. Jackson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilton_A/0/1/0/all/0/1&quot;&gt;Adrian Hilton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05059">
<title>A Novel Method for improving accuracy in neural network by reinstating traditional back propagation technique. (arXiv:2308.05059v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2308.05059</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has revolutionized industries like computer vision, natural
language processing, and speech recognition. However, back propagation, the
main method for training deep neural networks, faces challenges like
computational overhead and vanishing gradients. In this paper, we propose a
novel instant parameter update methodology that eliminates the need for
computing gradients at each layer. Our approach accelerates learning, avoids
the vanishing gradient problem, and outperforms state-of-the-art methods on
benchmark data sets. This research presents a promising direction for efficient
and effective deep neural network training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+R_G/0/1/0/all/0/1&quot;&gt;Gokulprasath R&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05068">
<title>Geometric Learning-Based Transformer Network for Estimation of Segmentation Errors. (arXiv:2308.05068v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2308.05068</link>
<description rdf:parseType="Literal">&lt;p&gt;Many segmentation networks have been proposed for 3D volumetric segmentation
of tumors and organs at risk. Hospitals and clinical institutions seek to
accelerate and minimize the efforts of specialists in image segmentation.
Still, in case of errors generated by these networks, clinicians would have to
manually edit the generated segmentation maps. Given a 3D volume and its
putative segmentation map, we propose an approach to identify and measure
erroneous regions in the segmentation map. Our method can estimate error at any
point or node in a 3D mesh generated from a possibly erroneous volumetric
segmentation map, serving as a Quality Assurance tool. We propose a graph
neural network-based transformer based on the Nodeformer architecture to
measure and classify the segmentation errors at any point. We have evaluated
our network on a high-resolution micro-CT dataset of the human inner-ear bony
labyrinth structure by simulating erroneous 3D segmentation maps. Our network
incorporates a convolutional encoder to compute node-centric features from the
input micro-CT data, the Nodeformer to learn the latent graph embeddings, and a
Multi-Layer Perceptron (MLP) to compute and classify the node-wise errors. Our
network achieves a mean absolute error of ~0.042 over other Graph Neural
Networks (GNN) and an accuracy of 79.53% over other GNNs in estimating and
classifying the node-wise errors, respectively. We also put forth vertex-normal
prediction as a custom pretext task for pre-training the CNN encoder to improve
the network&apos;s overall performance. Qualitative analysis shows the efficiency of
our network in correctly classifying errors and reducing misclassifications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+C_S/0/1/0/all/0/1&quot;&gt;Sneha Sree C&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fahim_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Fahim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ram_K/0/1/0/all/0/1&quot;&gt;Keerthi Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sivaprakasam_M/0/1/0/all/0/1&quot;&gt;Mohanasankar Sivaprakasam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05070">
<title>Volumetric Fast Fourier Convolution for Detecting Ink on the Carbonized Herculaneum Papyri. (arXiv:2308.05070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05070</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Digital Document Restoration (DDR) have led to
significant breakthroughs in analyzing highly damaged written artifacts. Among
those, there has been an increasing interest in applying Artificial
Intelligence techniques for virtually unwrapping and automatically detecting
ink on the Herculaneum papyri collection. This collection consists of
carbonized scrolls and fragments of documents, which have been digitized via
X-ray tomography to allow the development of ad-hoc deep learning-based DDR
solutions. In this work, we propose a modification of the Fast Fourier
Convolution operator for volumetric data and apply it in a segmentation
architecture for ink detection on the challenging Herculaneum papyri,
demonstrating its suitability via deep experimental analysis. To encourage the
research on this task and the application of the proposed operator to other
tasks involving volumetric data, we will release our implementation
(https://github.com/aimagelab/vffc)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quattrini_F/0/1/0/all/0/1&quot;&gt;Fabio Quattrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pippi_V/0/1/0/all/0/1&quot;&gt;Vittorio Pippi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cascianelli_S/0/1/0/all/0/1&quot;&gt;Silvia Cascianelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05074">
<title>Drones4Good: Supporting Disaster Relief Through Remote Sensing and AI. (arXiv:2308.05074v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2308.05074</link>
<description rdf:parseType="Literal">&lt;p&gt;In order to respond effectively in the aftermath of a disaster, emergency
services and relief organizations rely on timely and accurate information about
the affected areas. Remote sensing has the potential to significantly reduce
the time and effort required to collect such information by enabling a rapid
survey of large areas. To achieve this, the main challenge is the automatic
extraction of relevant information from remotely sensed data. In this work, we
show how the combination of drone-based data with deep learning methods enables
automated and large-scale situation assessment. In addition, we demonstrate the
integration of onboard image processing techniques for the deployment of
autonomous drone-based aid delivery. The results show the feasibility of a
rapid and large-scale image analysis in the field, and that onboard image
processing can increase the safety of drone-based aid deliveries.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merkle_N/0/1/0/all/0/1&quot;&gt;Nina Merkle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahmanyar_R/0/1/0/all/0/1&quot;&gt;Reza Bahmanyar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henry_C/0/1/0/all/0/1&quot;&gt;Corentin Henry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azimi_S/0/1/0/all/0/1&quot;&gt;Seyed Majid Azimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xiangtian Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schopferer_S/0/1/0/all/0/1&quot;&gt;Simon Schopferer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gstaiger_V/0/1/0/all/0/1&quot;&gt;Veronika Gstaiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1&quot;&gt;Stefan Auer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneibel_A/0/1/0/all/0/1&quot;&gt;Anne Schneibel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wieland_M/0/1/0/all/0/1&quot;&gt;Marc Wieland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kraft_T/0/1/0/all/0/1&quot;&gt;Thomas Kraft&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05081">
<title>Constructing Holistic Spatio-Temporal Scene Graph for Video Semantic Role Labeling. (arXiv:2308.05081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05081</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Semantic Role Labeling (VidSRL) aims to detect the salient events from
given videos, by recognizing the predict-argument event structures and the
interrelationships between events. While recent endeavors have put forth
methods for VidSRL, they can be mostly subject to two key drawbacks, including
the lack of fine-grained spatial scene perception and the insufficiently
modeling of video temporality. Towards this end, this work explores a novel
holistic spatio-temporal scene graph (namely HostSG) representation based on
the existing dynamic scene graph structures, which well model both the
fine-grained spatial semantics and temporal dynamics of videos for VidSRL.
Built upon the HostSG, we present a nichetargeting VidSRL framework. A
scene-event mapping mechanism is first designed to bridge the gap between the
underlying scene structure and the high-level event semantic structure,
resulting in an overall hierarchical scene-event (termed ICE) graph structure.
We further perform iterative structure refinement to optimize the ICE graph,
such that the overall structure representation can best coincide with end task
demand. Finally, three subtask predictions of VidSRL are jointly decoded, where
the end-to-end paradigm effectively avoids error propagation. On the benchmark
dataset, our framework boosts significantly over the current best-performing
model. Further analyses are shown for a better understanding of the advances of
our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1&quot;&gt;Hao Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yixin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bobo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Meishan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jianguo Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05092">
<title>A degree of image identification at sub-human scales could be possible with more advanced clusters. (arXiv:2308.05092v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05092</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of the research is to determine if currently available
self-supervised learning techniques can accomplish human level comprehension of
visual images using the same degree and amount of sensory input that people
acquire from. Initial research on this topic solely considered data volume
scaling. Here, we scale both the volume of data and the quality of the image.
This scaling experiment is a self-supervised learning method that may be done
without any outside financing. We find that scaling up data volume and picture
resolution at the same time enables human-level item detection performance at
sub-human sizes.We run a scaling experiment with vision transformers trained on
up to 200000 images up to 256 ppi.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+J_P/0/1/0/all/0/1&quot;&gt;Prateek Y J&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05095">
<title>LayoutLLM-T2I: Eliciting Layout Guidance from LLM for Text-to-Image Generation. (arXiv:2308.05095v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05095</link>
<description rdf:parseType="Literal">&lt;p&gt;In the text-to-image generation field, recent remarkable progress in Stable
Diffusion makes it possible to generate rich kinds of novel photorealistic
images. However, current models still face misalignment issues (e.g.,
problematic spatial relation understanding and numeration failure) in complex
natural scenes, which impedes the high-faithfulness text-to-image generation.
Although recent efforts have been made to improve controllability by giving
fine-grained guidance (e.g., sketch and scribbles), this issue has not been
fundamentally tackled since users have to provide such guidance information
manually. In this work, we strive to synthesize high-fidelity images that are
semantically aligned with a given textual prompt without any guidance. Toward
this end, we propose a coarse-to-fine paradigm to achieve layout planning and
image generation. Concretely, we first generate the coarse-grained layout
conditioned on a given textual prompt via in-context learning based on Large
Language Models. Afterward, we propose a fine-grained object-interaction
diffusion method to synthesize high-faithfulness images conditioned on the
prompt and the automatically generated layout. Extensive experiments
demonstrate that our proposed method outperforms the state-of-the-art models in
terms of layout and image generation. Our code and settings are available at
\url{https://layoutllm-t2i.github.io}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1&quot;&gt;Leigang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shengqiong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1&quot;&gt;Hao Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05104">
<title>Scene-Generalizable Interactive Segmentation of Radiance Fields. (arXiv:2308.05104v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2308.05104</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods for interactive segmentation in radiance fields entail
scene-specific optimization and thus cannot generalize across different scenes,
which greatly limits their applicability. In this work we make the first
attempt at Scene-Generalizable Interactive Segmentation in Radiance Fields
(SGISRF) and propose a novel SGISRF method, which can perform 3D object
segmentation for novel (unseen) scenes represented by radiance fields, guided
by only a few interactive user clicks in a given set of multi-view 2D images.
In particular, the proposed SGISRF focuses on addressing three crucial
challenges with three specially designed techniques. First, we devise the
Cross-Dimension Guidance Propagation to encode the scarce 2D user clicks into
informative 3D guidance representations. Second, the Uncertainty-Eliminated 3D
Segmentation module is designed to achieve efficient yet effective 3D
segmentation. Third, Concealment-Revealed Supervised Learning scheme is
proposed to reveal and correct the concealed 3D segmentation errors resulted
from the supervision in 2D space with only 2D mask annotations. Extensive
experiments on two real-world challenging benchmarks covering diverse scenes
demonstrate 1) effectiveness and scene-generalizability of the proposed method,
2) favorable performance compared to classical method requiring scene-specific
optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Songlin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_W/0/1/0/all/0/1&quot;&gt;Wenjie Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xin Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1&quot;&gt;Tanghui Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2002.03729">
<title>A lightweight target detection algorithm based on Mobilenet Convolution. (arXiv:2002.03729v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2002.03729</link>
<description rdf:parseType="Literal">&lt;p&gt;Target detection algorithm based on deep learning needs high computer GPU
configuration, even need to use high performance deep learning workstation,
this not only makes the cost increase, also greatly limits the realizability of
the ground, this paper introduces a kind of lightweight algorithm for target
detection under the condition of the balance accuracy and computational
efficiency, MobileNet as Backbone performs parameter The processing speed is
30fps on the RTX2060 card for images with the CNN separator layer. The
processing speed is 30fps on the RTX2060 card for images with a resolution of
320*320.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuchuk_N/0/1/0/all/0/1&quot;&gt;Nina Kuchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengquan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.01615">
<title>Lawin Transformer: Improving Semantic Segmentation Transformer with Multi-Scale Representations via Large Window Attention. (arXiv:2201.01615v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.01615</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-scale representations are crucial for semantic segmentation. The
community has witnessed the flourish of semantic segmentation convolutional
neural networks (CNN) exploiting multi-scale contextual information. Motivated
by that the vision transformer (ViT) is powerful in image classification, some
semantic segmentation ViTs are recently proposed, most of them attaining
impressive results but at a cost of computational economy. In this paper, we
succeed in introducing multi-scale representations into semantic segmentation
ViT via window attention mechanism and further improves the performance and
efficiency. To this end, we introduce large window attention which allows the
local window to query a larger area of context window at only a little
computation overhead. By regulating the ratio of the context area to the query
area, we enable the $\textit{large window attention}$ to capture the contextual
information at multiple scales. Moreover, the framework of spatial pyramid
pooling is adopted to collaborate with $\textit{the large window attention}$,
which presents a novel decoder named $\textbf{la}$rge $\textbf{win}$dow
attention spatial pyramid pooling (LawinASPP) for semantic segmentation ViT.
Our resulting ViT, Lawin Transformer, is composed of an efficient hierachical
vision transformer (HVT) as encoder and a LawinASPP as decoder. The empirical
results demonstrate that Lawin Transformer offers an improved efficiency
compared to the existing method. Lawin Transformer further sets new
state-of-the-art performance on Cityscapes (84.4% mIoU), ADE20K (56.2% mIoU)
and COCO-Stuff datasets. The code will be released at
https://github.com/yan-hao-tian/lawin
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Haotian Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Ming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.01937">
<title>BoMD: Bag of Multi-label Descriptors for Noisy Chest X-ray Classification. (arXiv:2203.01937v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.01937</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning methods have shown outstanding classification accuracy in
medical imaging problems, which is largely attributed to the availability of
large-scale datasets manually annotated with clean labels. However, given the
high cost of such manual annotation, new medical imaging classification
problems may need to rely on machine-generated noisy labels extracted from
radiology reports. Indeed, many Chest X-ray (CXR) classifiers have already been
modelled from datasets with noisy labels, but their training procedure is in
general not robust to noisy-label samples, leading to sub-optimal models.
Furthermore, CXR datasets are mostly multi-label, so current noisy-label
learning methods designed for multi-class problems cannot be easily adapted. In
this paper, we propose a new method designed for the noisy multi-label CXR
learning, which detects and smoothly re-labels samples from the dataset, which
is then used to train common multi-label classifiers. The proposed method
optimises a bag of multi-label descriptors (BoMD) to promote their similarity
with the semantic descriptors produced by BERT models from the multi-label
image annotation. Our experiments on diverse noisy multi-label training sets
and clean testing sets show that our model has state-of-the-art accuracy and
robustness in many CXR multi-label classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fengbei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yu Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.05119">
<title>MetAug: Contrastive Learning via Meta Feature Augmentation. (arXiv:2203.05119v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.05119</link>
<description rdf:parseType="Literal">&lt;p&gt;What matters for contrastive learning? We argue that contrastive learning
heavily relies on informative features, or &quot;hard&quot; (positive or negative)
features. Early works include more informative features by applying complex
data augmentations and large batch size or memory bank, and recent works design
elaborate sampling approaches to explore informative features. The key
challenge toward exploring such features is that the source multi-view data is
generated by applying random data augmentations, making it infeasible to always
add useful information in the augmented data. Consequently, the informativeness
of features learned from such augmented data is limited. In response, we
propose to directly augment the features in latent space, thereby learning
discriminative representations without a large amount of input data. We perform
a meta learning technique to build the augmentation generator that updates its
network parameters by considering the performance of the encoder. However,
insufficient input data may lead the encoder to learn collapsed features and
therefore malfunction the augmentation generator. A new margin-injected
regularization is further added in the objective function to avoid the encoder
learning a degenerate mapping. To contrast all features in one gradient
back-propagation step, we adopt the proposed optimization-driven unified
contrastive loss instead of the conventional contrastive loss. Empirically, our
method achieves state-of-the-art results on several benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.14797">
<title>3D-Aware Video Generation. (arXiv:2206.14797v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.14797</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models have emerged as an essential building block for many image
synthesis and editing tasks. Recent advances in this field have also enabled
high-quality 3D or video content to be generated that exhibits either
multi-view or temporal consistency. With our work, we explore 4D generative
adversarial networks (GANs) that learn unconditional generation of 3D-aware
videos. By combining neural implicit representations with time-aware
discriminator, we develop a GAN framework that synthesizes 3D video supervised
only with monocular videos. We show that our method learns a rich embedding of
decomposable 3D structures and motions that enables new visual effects of
spatio-temporal renderings while producing imagery with quality comparable to
that of existing 3D or video GANs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahmani_S/0/1/0/all/0/1&quot;&gt;Sherwin Bahmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jeong Joon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paschalidou_D/0/1/0/all/0/1&quot;&gt;Despoina Paschalidou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1&quot;&gt;Radu Timofte&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.11378">
<title>Do Perceptually Aligned Gradients Imply Adversarial Robustness?. (arXiv:2207.11378v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.11378</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarially robust classifiers possess a trait that non-robust models do
not -- Perceptually Aligned Gradients (PAG). Their gradients with respect to
the input align well with human perception. Several works have identified PAG
as a byproduct of robust training, but none have considered it as a standalone
phenomenon nor studied its own implications. In this work, we focus on this
trait and test whether \emph{Perceptually Aligned Gradients imply Robustness}.
To this end, we develop a novel objective to directly promote PAG in training
classifiers and examine whether models with such gradients are more robust to
adversarial attacks. Extensive experiments on multiple datasets and
architectures validate that models with aligned gradients exhibit significant
robustness, exposing the surprising bidirectional connection between PAG and
robustness. Lastly, we show that better gradient alignment leads to increased
robustness and harness this observation to boost the robustness of existing
adversarial training techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganz_R/0/1/0/all/0/1&quot;&gt;Roy Ganz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawar_B/0/1/0/all/0/1&quot;&gt;Bahjat Kawar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elad_M/0/1/0/all/0/1&quot;&gt;Michael Elad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00128">
<title>Archangel: A Hybrid UAV-based Human Detection Benchmark with Position and Pose Metadata. (arXiv:2209.00128v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00128</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning to detect objects, such as humans, in imagery captured by an
unmanned aerial vehicle (UAV) usually suffers from tremendous variations caused
by the UAV&apos;s position towards the objects. In addition, existing UAV-based
benchmark datasets do not provide adequate dataset metadata, which is essential
for precise model diagnosis and learning features invariant to those
variations. In this paper, we introduce Archangel, the first UAV-based object
detection dataset composed of real and synthetic subsets captured with similar
imagining conditions and UAV position and object pose metadata. A series of
experiments are carefully designed with a state-of-the-art object detector to
demonstrate the benefits of leveraging the metadata during model evaluation.
Moreover, several crucial insights involving both real and synthetic data
during model optimization are presented. In the end, we discuss the advantages,
limitations, and future directions regarding Archangel to highlight its
distinct value for the broader machine learning community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yi-Ting Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yaesop Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Heesung Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conover_D/0/1/0/all/0/1&quot;&gt;Damon M. Conover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_S/0/1/0/all/0/1&quot;&gt;Shuvra S. Bhattacharyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vale_N/0/1/0/all/0/1&quot;&gt;Nikolas Vale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gray_J/0/1/0/all/0/1&quot;&gt;Joshua D. Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leong_G/0/1/0/all/0/1&quot;&gt;G. Jeremy Leong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evensen_K/0/1/0/all/0/1&quot;&gt;Kenneth Evensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skirlo_F/0/1/0/all/0/1&quot;&gt;Frank Skirlo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05996">
<title>M$^2$-3DLaneNet: Exploring Multi-Modal 3D Lane Detection. (arXiv:2209.05996v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05996</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating accurate lane lines in 3D space remains challenging due to their
sparse and slim nature. Previous works mainly focused on using images for 3D
lane detection, leading to inherent projection error and loss of geometry
information. To address these issues, we explore the potential of leveraging
LiDAR for 3D lane detection, either as a standalone method or in combination
with existing monocular approaches. In this paper, we propose M$^2$-3DLaneNet
to integrate complementary information from multiple sensors. Specifically,
M$^2$-3DLaneNet lifts 2D features into 3D space by incorporating geometry
information from LiDAR data through depth completion. Subsequently, the lifted
2D features are further enhanced with LiDAR features through cross-modality BEV
fusion. Extensive experiments on the large-scale OpenLane dataset demonstrate
the effectiveness of M$^2$-3DLaneNet, regardless of the range (75m or 100m).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yueru Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chaoda Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_S/0/1/0/all/0/1&quot;&gt;Shuqi Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kun_T/0/1/0/all/0/1&quot;&gt;Tang Kun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Shuguang Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07811">
<title>Modeling Multiple Views via Implicitly Preserving Global Consistency and Local Complementarity. (arXiv:2209.07811v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07811</link>
<description rdf:parseType="Literal">&lt;p&gt;While self-supervised learning techniques are often used to mining implicit
knowledge from unlabeled data via modeling multiple views, it is unclear how to
perform effective representation learning in a complex and inconsistent
context. To this end, we propose a methodology, specifically consistency and
complementarity network (CoCoNet), which avails of strict global inter-view
consistency and local cross-view complementarity preserving regularization to
comprehensively learn representations from multiple views. On the global stage,
we reckon that the crucial knowledge is implicitly shared among views, and
enhancing the encoder to capture such knowledge from data can improve the
discriminability of the learned representations. Hence, preserving the global
consistency of multiple views ensures the acquisition of common knowledge.
CoCoNet aligns the probabilistic distribution of views by utilizing an
efficient discrepancy metric measurement based on the generalized sliced
Wasserstein distance. Lastly on the local stage, we propose a heuristic
complementarity-factor, which joints cross-view discriminative knowledge, and
it guides the encoders to learn not only view-wise discriminability but also
cross-view complementary information. Theoretically, we provide the
information-theoretical-based analyses of our proposed CoCoNet. Empirically, to
investigate the improvement gains of our approach, we conduct adequate
experimental validations, which demonstrate that CoCoNet outperforms the
state-of-the-art self-supervised methods by a significant margin proves that
such implicit consistency and complementarity preserving regularization can
enhance the discriminability of latent representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razzak_F/0/1/0/all/0/1&quot;&gt;Farid Razzak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11359">
<title>CUTS: A Fully Unsupervised Framework for Medical Image Segmentation. (arXiv:2209.11359v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11359</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work we introduce CUTS (Contrastive and Unsupervised Training for
Segmentation), a fully unsupervised deep learning framework for medical image
segmentation to better utilize the vast majority of imaging data that is not
labeled or annotated. We utilize self-supervision from pixels and their local
neighborhoods in the images themselves. Our unsupervised approach optimizes a
training objective that leverages concepts from contrastive learning and
autoencoding. Our framework segments medical images with a novel two-stage
approach without relying on any labeled data at any stage. The first stage
involves the creation of a &quot;pixel-centered patch&quot; that embeds every pixel along
with its surrounding patch, using a vector representation in a high-dimensional
latent embedding space. The second stage utilizes diffusion condensation, a
multi-scale topological data analysis approach, to dynamically coarse-grain
these embedding vectors at all levels of granularity. The final outcome is a
series of coarse-to-fine segmentations that highlight image structures at
various scales. In this work, we show successful multi-scale segmentation on
natural images, retinal fundus images, and brain MRI images. Our framework
delineates structures and patterns at different scales which, in the cases of
medical images, may carry distinct information relevant to clinical
interpretation. Quantitatively, our framework demonstrates improvements ranging
from 10% to 200% on dice coefficient and Hausdorff distance compared to
existing unsupervised methods across three medical image datasets. As we tackle
the problem of segmenting medical images at multiple meaningful granularities
without relying on any label, we hope to demonstrate the possibility to
circumvent tedious and repetitive manual annotations in future practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amodio_M/0/1/0/all/0/1&quot;&gt;Matthew Amodio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Liangbo L. Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avesta_A/0/1/0/all/0/1&quot;&gt;Arman Avesta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1&quot;&gt;Sanjay Aneja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jay C. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priore_L/0/1/0/all/0/1&quot;&gt;Lucian V. Del Priore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1&quot;&gt;Smita Krishnaswamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12126">
<title>One-Shot Neural Fields for 3D Object Understanding. (arXiv:2210.12126v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12126</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a unified and compact scene representation for robotics, where
each object in the scene is depicted by a latent code capturing geometry and
appearance. This representation can be decoded for various tasks such as novel
view rendering, 3D reconstruction (e.g. recovering depth, point clouds, or
voxel maps), collision checking, and stable grasp prediction. We build our
representation from a single RGB input image at test time by leveraging recent
advances in Neural Radiance Fields (NeRF) that learn category-level priors on
large multiview datasets, then fine-tune on novel objects from one or few
views. We expand the NeRF model for additional grasp outputs and explore ways
to leverage this representation for robotics. At test-time, we build the
representation from a single RGB input image observing the scene from only one
viewpoint. We find that the recovered representation allows rendering from
novel views, including of occluded object parts, and also for predicting
successful stable grasps. Grasp poses can be directly decoded from our latent
representation with an implicit grasp decoder. We experimented in both
simulation and real world and demonstrated the capability for robust robotic
grasping using such compact representation. Website:
https://nerfgrasp.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blukis_V/0/1/0/all/0/1&quot;&gt;Valts Blukis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_T/0/1/0/all/0/1&quot;&gt;Taeyeop Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tremblay_J/0/1/0/all/0/1&quot;&gt;Jonathan Tremblay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bowen Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kweon_I/0/1/0/all/0/1&quot;&gt;In So Kweon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kuk-Jin Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fox_D/0/1/0/all/0/1&quot;&gt;Dieter Fox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birchfield_S/0/1/0/all/0/1&quot;&gt;Stan Birchfield&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02408">
<title>Rickrolling the Artist: Injecting Backdoors into Text Encoders for Text-to-Image Synthesis. (arXiv:2211.02408v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02408</link>
<description rdf:parseType="Literal">&lt;p&gt;While text-to-image synthesis currently enjoys great popularity among
researchers and the general public, the security of these models has been
neglected so far. Many text-guided image generation models rely on pre-trained
text encoders from external sources, and their users trust that the retrieved
models will behave as promised. Unfortunately, this might not be the case. We
introduce backdoor attacks against text-guided generative models and
demonstrate that their text encoders pose a major tampering risk. Our attacks
only slightly alter an encoder so that no suspicious model behavior is apparent
for image generations with clean prompts. By then inserting a single character
trigger into the prompt, e.g., a non-Latin character or emoji, the adversary
can trigger the model to either generate images with pre-defined attributes or
images following a hidden, potentially malicious description. We empirically
demonstrate the high effectiveness of our attacks on Stable Diffusion and
highlight that the injection process of a single backdoor takes less than two
minutes. Besides phrasing our approach solely as an attack, it can also force
an encoder to forget phrases related to certain concepts, such as nudity or
violence, and help to make image generation safer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06108">
<title>RaLiBEV: Radar and LiDAR BEV Fusion Learning for Anchor Box Free Object Detection System. (arXiv:2211.06108v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06108</link>
<description rdf:parseType="Literal">&lt;p&gt;In autonomous driving systems, LiDAR and radar play important roles in the
perception of the surrounding environment. LiDAR provides accurate 3D spatial
sensing information but cannot work in adverse weather like fog. On the other
hand, the radar signal can be diffracted when encountering raindrops or mist
particles thanks to its wavelength, but it suffers from large noise. Recent
state-of-the-art works reveal that fusion of radar and LiDAR can lead to robust
detection in adverse weather. The existing works adopt convolutional neural
network architecture to extract features from each sensor data stream, then
align and aggregate the two branch features to predict object detection
results. However, these methods have low accuracy of bounding box estimations
due to a simple design of label assignment and fusion strategies. In this
paper, we propose a bird&apos;s-eye view fusion learning-based anchor box-free
object detection system, which fuses the feature derived from the radar
range-azimuth heatmap and the LiDAR point cloud to estimate the possible
objects. Different label assignment strategies have been designed to facilitate
the consistency between the classification of foreground or background anchor
points and the corresponding bounding box regressions. In addition, the
performance of the proposed object detector is further enhanced by employing a
novel interactive transformer module. The superior performance of the methods
proposed in this paper has been demonstrated using the recently published
Oxford Radar RobotCar dataset. Our system&apos;s average precision significantly
outperforms the best state-of-the-art method by 13.1% and 19.0% at IoU of 0.8
under &apos;Clear+Foggy&apos; training conditions for &apos;Clear&apos; and &apos;Foggy&apos; testing,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yanlong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Q/0/1/0/all/0/1&quot;&gt;Qing-Long Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Gang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07273">
<title>MLIC: Multi-Reference Entropy Model for Learned Image Compression. (arXiv:2211.07273v5 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07273</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learned image compression has achieved remarkable performance. The
entropy model, which estimates the distribution of the latent representation,
plays a crucial role in boosting rate-distortion performance. However, most
entropy models only capture correlations in one dimension, while the latent
representation contain channel-wise, local spatial, and global spatial
correlations. To tackle this issue, we propose the Multi-Reference Entropy
Model (MEM) and the advanced version, MEM$^+$. These models capture the
different types of correlations present in latent representation. Specifically,
We first divide the latent representation into slices. When decoding the
current slice, we use previously decoded slices as context and employ the
attention map of the previously decoded slice to predict global correlations in
the current slice. To capture local contexts, we introduce two enhanced
checkerboard context capturing techniques that avoids performance degradation.
Based on MEM and MEM$^+$, we propose image compression models MLIC and
MLIC$^+$. Extensive experimental evaluations demonstrate that our MLIC and
MLIC$^+$ models achieve state-of-the-art performance, reducing BD-rate by
$8.05\%$ and $11.39\%$ on the Kodak dataset compared to VTM-17.0 when measured
in PSNR. Our code will be available at https://github.com/JiangWeibeta/MLIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yongqi Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ning_P/0/1/0/all/0/1&quot;&gt;Peirong Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ronggang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.07898">
<title>Learning-Augmented Model-Based Planning for Visual Exploration. (arXiv:2211.07898v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2211.07898</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of time-limited robotic exploration in previously
unseen environments where exploration is limited by a predefined amount of
time. We propose a novel exploration approach using learning-augmented
model-based planning. We generate a set of subgoals associated with frontiers
on the current map and derive a Bellman Equation for exploration with these
subgoals. Visual sensing and advances in semantic mapping of indoor scenes are
exploited for training a deep convolutional neural network to estimate
properties associated with each frontier: the expected unobserved area beyond
the frontier and the expected timesteps (discretized actions) required to
explore it. The proposed model-based planner is guaranteed to explore the whole
scene if time permits. We thoroughly evaluate our approach on a large-scale
pseudo-realistic indoor dataset (Matterport3D) with the Habitat simulator. We
compare our approach with classical and more recent RL-based exploration
methods. Our approach surpasses the greedy strategies by 2.1% and the RL-based
exploration methods by 8.4% in terms of coverage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yimeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debnath_A/0/1/0/all/0/1&quot;&gt;Arnab Debnath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stein_G/0/1/0/all/0/1&quot;&gt;Gregory Stein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1&quot;&gt;Jana Kosecka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.12860">
<title>DETRs with Collaborative Hybrid Assignments Training. (arXiv:2211.12860v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.12860</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide the observation that too few queries assigned as
positive samples in DETR with one-to-one set matching leads to sparse
supervision on the encoder&apos;s output which considerably hurt the discriminative
feature learning of the encoder and vice visa for attention learning in the
decoder. To alleviate this, we present a novel collaborative hybrid assignments
training scheme, namely $\mathcal{C}$o-DETR, to learn more efficient and
effective DETR-based detectors from versatile label assignment manners. This
new training scheme can easily enhance the encoder&apos;s learning ability in
end-to-end detectors by training the multiple parallel auxiliary heads
supervised by one-to-many label assignments such as ATSS and Faster RCNN. In
addition, we conduct extra customized positive queries by extracting the
positive coordinates from these auxiliary heads to improve the training
efficiency of positive samples in the decoder. In inference, these auxiliary
heads are discarded and thus our method introduces no additional parameters and
computational cost to the original detector while requiring no hand-crafted
non-maximum suppression (NMS). We conduct extensive experiments to evaluate the
effectiveness of the proposed approach on DETR variants, including DAB-DETR,
Deformable-DETR, and DINO-Deformable-DETR. The state-of-the-art
DINO-Deformable-DETR with Swin-L can be improved from 58.5% to 59.5% AP on COCO
val. Surprisingly, incorporated with ViT-L backbone, we achieve 66.0% AP on
COCO test-dev and 67.9% AP on LVIS val, outperforming previous methods by clear
margins with much fewer model sizes. Codes are available at
\url{https://github.com/Sense-X/Co-DETR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1&quot;&gt;Zhuofan Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guanglu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01331">
<title>Surface Normal Clustering for Implicit Representation of Manhattan Scenes. (arXiv:2212.01331v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01331</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel view synthesis and 3D modeling using implicit neural field
representation are shown to be very effective for calibrated multi-view
cameras. Such representations are known to benefit from additional geometric
and semantic supervision. Most existing methods that exploit additional
supervision require dense pixel-wise labels or localized scene priors. These
methods cannot benefit from high-level vague scene priors provided in terms of
scenes&apos; descriptions. In this work, we aim to leverage the geometric prior of
Manhattan scenes to improve the implicit neural radiance field representations.
More precisely, we assume that only the knowledge of the indoor scene (under
investigation) being Manhattan is known -- with no additional information
whatsoever -- with an unknown Manhattan coordinate frame. Such high-level prior
is used to self-supervise the surface normals derived explicitly in the
implicit neural fields. Our modeling allows us to cluster the derived normals
and exploit their orthogonality constraints for self-supervision. Our
exhaustive experiments on datasets of diverse indoor scenes demonstrate the
significant benefit of the proposed method over the established baselines. The
source code will be available at
https://github.com/nikola3794/normal-clustering-nerf.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1&quot;&gt;Nikola Popovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1&quot;&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05116">
<title>Leveraging Contextual Data Augmentation for Generalizable Melanoma Detection. (arXiv:2212.05116v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05116</link>
<description rdf:parseType="Literal">&lt;p&gt;While skin cancer detection has been a valuable deep learning application for
years, its evaluation has often neglected the context in which testing images
are assessed. Traditional melanoma classifiers assume that their testing
environments are comparable to the structured images they are trained on. This
paper challenges this notion and argues that mole size, a critical attribute in
professional dermatology, can be misleading in automated melanoma detection.
While malignant melanomas tend to be larger than benign melanomas, relying
solely on size can be unreliable and even harmful when contextual scaling of
images is not possible. To address this issue, this implementation proposes a
custom model that performs various data augmentation procedures to prevent
overfitting to incorrect parameters and simulate real-world usage of melanoma
detection applications. Multiple custom models employing different forms of
data augmentation are implemented to highlight the most significant features of
mole classifiers. These implementations emphasize the importance of considering
user unpredictability when deploying such applications. The caution required
when manually modifying data is acknowledged, as it can result in data loss and
biased conclusions. Additionally, the significance of data augmentation in both
the dermatology and deep learning communities is considered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+DiSanto_N/0/1/0/all/0/1&quot;&gt;Nick DiSanto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Harding_G/0/1/0/all/0/1&quot;&gt;Gavin Harding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Martinez_E/0/1/0/all/0/1&quot;&gt;Ethan Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sanders_B/0/1/0/all/0/1&quot;&gt;Benjamin Sanders&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10445">
<title>Model Ratatouille: Recycling Diverse Models for Out-of-Distribution Generalization. (arXiv:2212.10445v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10445</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models are redefining how AI systems are built. Practitioners now
follow a standard procedure to build their machine learning solutions: from a
pre-trained foundation model, they fine-tune the weights on the target task of
interest. So, the Internet is swarmed by a handful of foundation models
fine-tuned on many diverse tasks: these individual fine-tunings exist in
isolation without benefiting from each other. In our opinion, this is a missed
opportunity, as these specialized models contain rich and diverse features. In
this paper, we thus propose model ratatouille, a new strategy to recycle the
multiple fine-tunings of the same foundation model on diverse auxiliary tasks.
Specifically, we repurpose these auxiliary weights as initializations for
multiple parallel fine-tunings on the target task; then, we average all
fine-tuned weights to obtain the final model. This recycling strategy aims at
maximizing the diversity in weights by leveraging the diversity in auxiliary
tasks. Empirically, it improves the state of the art on the reference DomainBed
benchmark for out-of-distribution generalization. Looking forward, this work
contributes to the emerging paradigm of updatable machine learning where, akin
to open-source software development, the community collaborates to reliably
update machine learning models. Our code is released:
https://github.com/facebookresearch/ModelRatatouille.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rame_A/0/1/0/all/0/1&quot;&gt;Alexandre Ram&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahuja_K/0/1/0/all/0/1&quot;&gt;Kartik Ahuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cord_M/0/1/0/all/0/1&quot;&gt;Matthieu Cord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottou_L/0/1/0/all/0/1&quot;&gt;L&amp;#xe9;on Bottou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_Paz_D/0/1/0/all/0/1&quot;&gt;David Lopez-Paz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06436">
<title>Geometric Constraints Enable Self-Supervised Sinogram Inpainting in Sparse-View Tomography. (arXiv:2302.06436v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06436</link>
<description rdf:parseType="Literal">&lt;p&gt;The diagnostic quality of computed tomography (CT) scans is usually
restricted by the induced patient dose, scan speed, and image quality.
Sparse-angle tomographic scans reduce radiation exposure and accelerate data
acquisition, but suffer from image artifacts and noise. Existing image
processing algorithms can restore CT reconstruction quality but often require
large training data sets or can not be used for truncated objects. This work
presents a self-supervised projection inpainting method that allows optimizing
missing projective views via gradient-based optimization. By reconstructing
independent stacks of projection data, a self-supervised loss is calculated in
the CT image domain and used to directly optimize projection image intensities
to match the missing tomographic views constrained by the projection geometry.
Our experiments on real X-ray microscope (XRM) tomographic mouse tibia bone
scans show that our method improves reconstructions by 3.1-7.4%/7.7-17.6% in
terms of PSNR/SSIM with respect to the interpolation baseline. Our approach is
applicable as a flexible self-supervised projection inpainting tool for
tomographic applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_F/0/1/0/all/0/1&quot;&gt;Fabian Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thies_M/0/1/0/all/0/1&quot;&gt;Mareike Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maul_N/0/1/0/all/0/1&quot;&gt;Noah Maul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pfaff_L/0/1/0/all/0/1&quot;&gt;Laura Pfaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aust_O/0/1/0/all/0/1&quot;&gt;Oliver Aust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechmann_S/0/1/0/all/0/1&quot;&gt;Sabrina Pechmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syben_C/0/1/0/all/0/1&quot;&gt;Christopher Syben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maier_A/0/1/0/all/0/1&quot;&gt;Andreas Maier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03281">
<title>Visual Place Recognition: A Tutorial. (arXiv:2303.03281v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03281</link>
<description rdf:parseType="Literal">&lt;p&gt;Localization is an essential capability for mobile robots. A rapidly growing
field of research in this area is Visual Place Recognition (VPR), which is the
ability to recognize previously seen places in the world based solely on
images. This present work is the first tutorial paper on visual place
recognition. It unifies the terminology of VPR and complements prior research
in two important directions: 1) It provides a systematic introduction for
newcomers to the field, covering topics such as the formulation of the VPR
problem, a general-purpose algorithmic pipeline, an evaluation methodology for
VPR approaches, and the major challenges for VPR and how they may be addressed.
2) As a contribution for researchers acquainted with the VPR problem, it
examines the intricacies of different VPR problem types regarding input, data
processing, and output. The tutorial also discusses the subtleties behind the
evaluation of VPR algorithms, e.g., the evaluation of a VPR system that has to
find all matching database images per query, as opposed to just a single match.
Practical code examples in Python illustrate to prospective practitioners and
researchers how VPR is implemented and evaluated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schubert_S/0/1/0/all/0/1&quot;&gt;Stefan Schubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neubert_P/0/1/0/all/0/1&quot;&gt;Peer Neubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sourav Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milford_M/0/1/0/all/0/1&quot;&gt;Michael Milford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1&quot;&gt;Tobias Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05689">
<title>Inducing Neural Collapse to a Fixed Hierarchy-Aware Frame for Reducing Mistake Severity. (arXiv:2303.05689v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05689</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a recently discovered and intriguing phenomenon called Neural
Collapse: at the terminal phase of training a deep neural network for
classification, the within-class penultimate feature means and the associated
classifier vectors of all flat classes collapse to the vertices of a simplex
Equiangular Tight Frame (ETF). Recent work has tried to exploit this phenomenon
by fixing the related classifier weights to a pre-computed ETF to induce neural
collapse and maximize the separation of the learned features when training with
imbalanced data. In this work, we propose to fix the linear classifier of a
deep neural network to a Hierarchy-Aware Frame (HAFrame), instead of an ETF,
and use a cosine similarity-based auxiliary loss to learn hierarchy-aware
penultimate features that collapse to the HAFrame. We demonstrate that our
approach reduces the mistake severity of the model&apos;s predictions while
maintaining its top-1 accuracy on several datasets of varying scales with
hierarchies of heights ranging from 3 to 12. Code:
https://github.com/ltong1130ztr/HAFrame
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1&quot;&gt;Tong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jim Davis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06457">
<title>Active Visual Exploration Based on Attention-Map Entropy. (arXiv:2303.06457v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06457</link>
<description rdf:parseType="Literal">&lt;p&gt;Active visual exploration addresses the issue of limited sensor capabilities
in real-world scenarios, where successive observations are actively chosen
based on the environment. To tackle this problem, we introduce a new technique
called Attention-Map Entropy (AME). It leverages the internal uncertainty of
the transformer-based model to determine the most informative observations. In
contrast to existing solutions, it does not require additional loss components,
which simplifies the training. Through experiments, which also mimic
retina-like sensors, we show that such simplified training significantly
improves the performance of reconstruction, segmentation and classification on
publicly available datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pardyl_A/0/1/0/all/0/1&quot;&gt;Adam Pardyl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rypesc_G/0/1/0/all/0/1&quot;&gt;Grzegorz Rype&amp;#x15b;&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurzejamski_G/0/1/0/all/0/1&quot;&gt;Grzegorz Kurzejamski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1&quot;&gt;Bartosz Zieli&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09472">
<title>DiffIR: Efficient Diffusion Model for Image Restoration. (arXiv:2303.09472v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09472</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion model (DM) has achieved SOTA performance by modeling the image
synthesis process into a sequential application of a denoising network.
However, different from image synthesis, image restoration (IR) has a strong
constraint to generate results in accordance with ground-truth. Thus, for IR,
traditional DMs running massive iterations on a large model to estimate whole
images or feature maps is inefficient. To address this issue, we propose an
efficient DM for IR (DiffIR), which consists of a compact IR prior extraction
network (CPEN), dynamic IR transformer (DIRformer), and denoising network.
Specifically, DiffIR has two training stages: pretraining and training DM. In
pretraining, we input ground-truth images into CPEN$_{S1}$ to capture a compact
IR prior representation (IPR) to guide DIRformer. In the second stage, we train
the DM to directly estimate the same IRP as pretrained CPEN$_{S1}$ only using
LQ images. We observe that since the IPR is only a compact vector, DiffIR can
use fewer iterations than traditional DM to obtain accurate estimations and
generate more stable and realistic results. Since the iterations are few, our
DiffIR can adopt a joint optimization of CPEN$_{S2}$, DIRformer, and denoising
network, which can further reduce the estimation error influence. We conduct
extensive experiments on several IR tasks and achieve SOTA performance while
consuming less computational costs. Code is available at
\url{https://github.com/Zj-BinXia/DiffIR}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_B/0/1/0/all/0/1&quot;&gt;Bin Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yitong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinglong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yapeng Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10437">
<title>Grounding 3D Object Affordance from 2D Interactions in Images. (arXiv:2303.10437v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10437</link>
<description rdf:parseType="Literal">&lt;p&gt;Grounding 3D object affordance seeks to locate objects&apos; &apos;&apos;action
possibilities&apos;&apos; regions in the 3D space, which serves as a link between
perception and operation for embodied agents. Existing studies primarily focus
on connecting visual affordances with geometry structures, e.g. relying on
annotations to declare interactive regions of interest on the object and
establishing a mapping between the regions and affordances. However, the
essence of learning object affordance is to understand how to use it, and the
manner that detaches interactions is limited in generalization. Normally,
humans possess the ability to perceive object affordances in the physical world
through demonstration images or videos. Motivated by this, we introduce a novel
task setting: grounding 3D object affordance from 2D interactions in images,
which faces the challenge of anticipating affordance through interactions of
different sources. To address this problem, we devise a novel
Interaction-driven 3D Affordance Grounding Network (IAG), which aligns the
region feature of objects from different sources and models the interactive
contexts for 3D object affordance grounding. Besides, we collect a Point-Image
Affordance Dataset (PIAD) to support the proposed task. Comprehensive
experiments on PIAD demonstrate the reliability of the proposed task and the
superiority of our method. The project is available at
https://github.com/yyvhang/IAGNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1&quot;&gt;Wei Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hongchen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10438">
<title>Spatial-Aware Token for Weakly Supervised Object Localization. (arXiv:2303.10438v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10438</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly supervised object localization (WSOL) is a challenging task aiming to
localize objects with only image-level supervision. Recent works apply visual
transformer to WSOL and achieve significant success by exploiting the
long-range feature dependency in self-attention mechanism. However, existing
transformer-based methods synthesize the classification feature maps as the
localization map, which leads to optimization conflicts between classification
and localization tasks. To address this problem, we propose to learn a
task-specific spatial-aware token (SAT) to condition localization in a weakly
supervised manner. Specifically, a spatial token is first introduced in the
input space to aggregate representations for localization task. Then a spatial
aware attention module is constructed, which allows spatial token to generate
foreground probabilities of different patches by querying and to extract
localization knowledge from the classification task. Besides, for the problem
of sparse and unbalanced pixel-level supervision obtained from the image-level
label, two spatial constraints, including batch area loss and normalization
loss, are designed to compensate and enhance this supervision. Experiments show
that the proposed SAT achieves state-of-the-art performance on both CUB-200 and
ImageNet, with 98.45% and 73.13% GT-known Loc, respectively. Even under the
extreme setting of using only 1 image per class from ImageNet for training, SAT
already exceeds the SOTA method by 2.1% GT-known Loc. Code and models are
available at https://github.com/wpy1999/SAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_P/0/1/0/all/0/1&quot;&gt;Pingyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_W/0/1/0/all/0/1&quot;&gt;Wei Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yang Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiebo Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16839">
<title>MaMMUT: A Simple Architecture for Joint Learning for MultiModal Tasks. (arXiv:2303.16839v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16839</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of language models have moved from encoder-decoder to
decoder-only designs. In addition, we observe that the two most popular
multimodal tasks, the generative and contrastive tasks, are nontrivial to
accommodate in one architecture, and further need adaptations for downstream
tasks. We propose a novel paradigm of training with a decoder-only model for
multimodal tasks, which is surprisingly effective in jointly learning of these
disparate vision-language tasks. This is done with a simple model, called
MaMMUT. It consists of a single vision encoder and a text decoder, and is able
to accommodate contrastive and generative learning by a novel two-pass approach
on the text decoder. We demonstrate that joint learning of these diverse
objectives is simple, effective, and maximizes the weight-sharing of the model
across these tasks. Furthermore, the same architecture enables straightforward
extensions to open-vocabulary object detection and video-language tasks. The
model tackles a diverse range of tasks, while being modest in capacity. Our
model achieves the state of the art on image-text and text-image retrieval,
video question answering and open-vocabulary detection tasks, outperforming
much larger and more extensively trained foundational models. It shows very
competitive results on VQA and Video Captioning, especially considering its
capacity. Ablations confirm the flexibility and advantages of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_W/0/1/0/all/0/1&quot;&gt;Weicheng Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piergiovanni_A/0/1/0/all/0/1&quot;&gt;AJ Piergiovanni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dahun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caine_B/0/1/0/all/0/1&quot;&gt;Ben Caine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogale_A/0/1/0/all/0/1&quot;&gt;Abhijit Ogale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Luowei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Claire Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Angelova_A/0/1/0/all/0/1&quot;&gt;Anelia Angelova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02942">
<title>InterFormer: Real-time Interactive Image Segmentation. (arXiv:2304.02942v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02942</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive image segmentation enables annotators to efficiently perform
pixel-level annotation for segmentation tasks. However, the existing
interactive segmentation pipeline suffers from inefficient computations of
interactive models because of the following two issues. First, annotators&apos;
later click is based on models&apos; feedback of annotators&apos; former click. This
serial interaction is unable to utilize model&apos;s parallelism capabilities.
Second, in each interaction step, the model handles the invariant image along
with the sparse variable clicks, resulting in a process that&apos;s highly
repetitive and redundant. For efficient computations, we propose a method named
InterFormer that follows a new pipeline to address these issues. InterFormer
extracts and preprocesses the computationally time-consuming part i.e. image
processing from the existing process. Specifically, InterFormer employs a large
vision transformer (ViT) on high-performance devices to preprocess images in
parallel, and then uses a lightweight module called interactive multi-head self
attention (I-MSA) for interactive segmentation. Furthermore, the I-MSA module&apos;s
deployment on low-power devices extends the practical application of
interactive segmentation. The I-MSA module utilizes the preprocessed features
to efficiently response to the annotator inputs in real-time. The experiments
on several datasets demonstrate the effectiveness of InterFormer, which
outperforms previous interactive segmentation models in terms of computational
efficiency and segmentation quality, achieve real-time high-quality interactive
segmentation on CPU-only devices. The code is available at
https://github.com/YouHuang67/InterFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;You Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_K/0/1/0/all/0/1&quot;&gt;Ke Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Liujuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guannan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04963">
<title>PlantDet: A benchmark for Plant Detection in the Three-Rivers-Source Region. (arXiv:2304.04963v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04963</link>
<description rdf:parseType="Literal">&lt;p&gt;The Three-River-Source region is a highly significant natural reserve in
China that harbors a plethora of botanical resources. To meet the practical
requirements of botanical research and intelligent plant management, we
construct a dataset for Plant detection in the Three-River-Source region
(PTRS). It comprises 21 types, 6965 high-resolution images of 2160*3840 pixels,
captured by diverse sensors and platforms, and featuring objects of varying
shapes and sizes. The PTRS presents us with challenges such as dense occlusion,
varying leaf resolutions, and high feature similarity among plants, prompting
us to develop a novel object detection network named PlantDet. This network
employs a window-based efficient self-attention module (ST block) to generate
robust feature representation at multiple scales, improving the detection
efficiency for small and densely-occluded objects. Our experimental results
validate the efficacy of our proposed plant detection benchmark, with a
precision of 88.1%, a mean average precision (mAP) of 77.6%, and a higher
recall compared to the baseline. Additionally, our method effectively overcomes
the issue of missing small objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huanhuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu-an Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhaba_J/0/1/0/all/0/1&quot;&gt;Jiangcai Zhaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guomei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yongga_L/0/1/0/all/0/1&quot;&gt;Lamao Yongga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05731">
<title>SketchANIMAR: Sketch-based 3D Animal Fine-Grained Retrieval. (arXiv:2304.05731v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05731</link>
<description rdf:parseType="Literal">&lt;p&gt;The retrieval of 3D objects has gained significant importance in recent years
due to its broad range of applications in computer vision, computer graphics,
virtual reality, and augmented reality. However, the retrieval of 3D objects
presents significant challenges due to the intricate nature of 3D models, which
can vary in shape, size, and texture, and have numerous polygons and vertices.
To this end, we introduce a novel SHREC challenge track that focuses on
retrieving relevant 3D animal models from a dataset using sketch queries and
expedites accessing 3D models through available sketches. Furthermore, a new
dataset named ANIMAR was constructed in this study, comprising a collection of
711 unique 3D animal models and 140 corresponding sketch queries. Our contest
requires participants to retrieve 3D models based on complex and detailed
sketches. We receive satisfactory results from eight teams and 204 runs.
Although further improvement is necessary, the proposed task has the potential
to incentivize additional research in the domain of 3D object retrieval,
potentially yielding benefits for a wide range of applications. We also provide
insights into potential areas of future research, such as improving techniques
for feature extraction and matching and creating more diverse datasets to
evaluate retrieval performance. https://aichallenge.hcmus.edu.vn/sketchanimar
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tam V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Minh-Quan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Trong-Thuan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1&quot;&gt;Viet-Tham Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1&quot;&gt;Trong-Le Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_K/0/1/0/all/0/1&quot;&gt;Khanh-Duy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Mai-Khiem Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_Xuan_N/0/1/0/all/0/1&quot;&gt;Nhat Hoang-Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Ho_T/0/1/0/all/0/1&quot;&gt;Thang-Long Nguyen-Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Vinh-Tiep Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Pham_N/0/1/0/all/0/1&quot;&gt;Nhat-Quynh Le-Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Huu-Phuc Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Trong-Vu Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quang-Binh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Mau_T/0/1/0/all/0/1&quot;&gt;Trong-Hieu Nguyen-Mau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_T/0/1/0/all/0/1&quot;&gt;Tuan-Luc Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thanh-Danh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Ha_N/0/1/0/all/0/1&quot;&gt;Ngoc-Linh Nguyen-Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_Thuy_T/0/1/0/all/0/1&quot;&gt;Tuong-Vy Truong-Thuy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phong_T/0/1/0/all/0/1&quot;&gt;Truong Hoai Phong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diep_T/0/1/0/all/0/1&quot;&gt;Tuong-Nghiem Diep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1&quot;&gt;Khanh-Duy Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1&quot;&gt;Xuan-Hieu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thien-Phuc Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1&quot;&gt;Kim-Phat Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1&quot;&gt;Nhu-Vinh Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh-Quang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1&quot;&gt;Hoai-Danh Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_M/0/1/0/all/0/1&quot;&gt;Minh-Hoa Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hai-Dang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1&quot;&gt;Akihiro Sugimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Triet Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06053">
<title>TextANIMAR: Text-based 3D Animal Fine-Grained Retrieval. (arXiv:2304.06053v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06053</link>
<description rdf:parseType="Literal">&lt;p&gt;3D object retrieval is an important yet challenging task that has drawn more
and more attention in recent years. While existing approaches have made strides
in addressing this issue, they are often limited to restricted settings such as
image and sketch queries, which are often unfriendly interactions for common
users. In order to overcome these limitations, this paper presents a novel
SHREC challenge track focusing on text-based fine-grained retrieval of 3D
animal models. Unlike previous SHREC challenge tracks, the proposed task is
considerably more challenging, requiring participants to develop innovative
approaches to tackle the problem of text-based retrieval. Despite the increased
difficulty, we believe this task can potentially drive useful applications in
practice and facilitate more intuitive interactions with 3D objects. Five
groups participated in our competition, submitting a total of 114 runs. While
the results obtained in our competition are satisfactory, we note that the
challenges presented by this task are far from fully solved. As such, we
provide insights into potential areas for future research and improvements. We
believe we can help push the boundaries of 3D object retrieval and facilitate
more user-friendly interactions via vision-language technologies.
https://aichallenge.hcmus.edu.vn/textanimar
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Trung-Nghia Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tam V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Minh-Quan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Trong-Thuan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_V/0/1/0/all/0/1&quot;&gt;Viet-Tham Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1&quot;&gt;Trong-Le Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_K/0/1/0/all/0/1&quot;&gt;Khanh-Duy Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Mai-Khiem Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_Xuan_N/0/1/0/all/0/1&quot;&gt;Nhat Hoang-Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Ho_T/0/1/0/all/0/1&quot;&gt;Thang-Long Nguyen-Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Vinh-Tiep Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diep_T/0/1/0/all/0/1&quot;&gt;Tuong-Nghiem Diep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_K/0/1/0/all/0/1&quot;&gt;Khanh-Duy Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1&quot;&gt;Xuan-Hieu Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thien-Phuc Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1&quot;&gt;Kim-Phat Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_N/0/1/0/all/0/1&quot;&gt;Nhu-Vinh Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh-Quang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1&quot;&gt;E-Ro Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Nhat_M/0/1/0/all/0/1&quot;&gt;Minh-Khoi Nguyen-Nhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+To_T/0/1/0/all/0/1&quot;&gt;Tuan-An To&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_Le_T/0/1/0/all/0/1&quot;&gt;Trung-Truc Huynh-Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Nham-Tan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luong_H/0/1/0/all/0/1&quot;&gt;Hoang-Chau Luong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phong_T/0/1/0/all/0/1&quot;&gt;Truong Hoai Phong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Pham_N/0/1/0/all/0/1&quot;&gt;Nhat-Quynh Le-Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Huu-Phuc Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Trong-Vu Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quang-Binh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hai-Dang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugimoto_A/0/1/0/all/0/1&quot;&gt;Akihiro Sugimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Minh-Triet Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08975">
<title>Neural Architecture Search for Visual Anomaly Segmentation. (arXiv:2304.08975v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08975</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents the first application of neural architecture search to
the complex task of segmenting visual anomalies. Measurement of anomaly
segmentation performance is challenging due to imbalanced anomaly pixels,
varying region areas, and various types of anomalies. First, the
region-weighted Average Precision (rwAP) metric is proposed as an alternative
to existing metrics, which does not need to be limited to a specific maximum
false positive rate. Second, the AutoPatch neural architecture search method is
proposed, which enables efficient segmentation of visual anomalies without any
training. By leveraging a pre-trained supernet, a black-box optimization
algorithm can directly minimize computational complexity and maximize
performance on a small validation set of anomalous examples. Finally,
compelling results are presented on the widely studied MVTec dataset,
demonstrating that AutoPatch outperforms the current state-of-the-art with
lower computational complexity, using only one example per type of anomaly. The
results highlight the potential of automated machine learning to optimize
throughput in industrial quality control. The code for AutoPatch is available
at: https://github.com/tommiekerssies/AutoPatch
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kerssies_T/0/1/0/all/0/1&quot;&gt;Tommie Kerssies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanschoren_J/0/1/0/all/0/1&quot;&gt;Joaquin Vanschoren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03210">
<title>AttentionViz: A Global View of Transformer Attention. (arXiv:2305.03210v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03210</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models are revolutionizing machine learning, but their inner
workings remain mysterious. In this work, we present a new visualization
technique designed to help researchers understand the self-attention mechanism
in transformers that allows these models to learn rich, contextual
relationships between elements of a sequence. The main idea behind our method
is to visualize a joint embedding of the query and key vectors used by
transformer models to compute attention. Unlike previous attention
visualization techniques, our approach enables the analysis of global patterns
across multiple input sequences. We create an interactive visualization tool,
AttentionViz (demo: &lt;a href=&quot;http://attentionviz.com&quot;&gt;this http URL&lt;/a&gt;), based on these joint query-key
embeddings, and use it to study attention mechanisms in both language and
vision transformers. We demonstrate the utility of our approach in improving
model understanding and offering new insights about query-key interactions
through several application scenarios and expert feedback.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Catherine Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yida Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_A/0/1/0/all/0/1&quot;&gt;Aoyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cynthia Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viegas_F/0/1/0/all/0/1&quot;&gt;Fernanda Vi&amp;#xe9;gas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wattenberg_M/0/1/0/all/0/1&quot;&gt;Martin Wattenberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03892">
<title>DocDiff: Document Enhancement via Residual Diffusion Models. (arXiv:2305.03892v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03892</link>
<description rdf:parseType="Literal">&lt;p&gt;Removing degradation from document images not only improves their visual
quality and readability, but also enhances the performance of numerous
automated document analysis and recognition tasks. However, existing
regression-based methods optimized for pixel-level distortion reduction tend to
suffer from significant loss of high-frequency information, leading to
distorted and blurred text edges. To compensate for this major deficiency, we
propose DocDiff, the first diffusion-based framework specifically designed for
diverse challenging document enhancement problems, including document
deblurring, denoising, and removal of watermarks and seals. DocDiff consists of
two modules: the Coarse Predictor (CP), which is responsible for recovering the
primary low-frequency content, and the High-Frequency Residual Refinement (HRR)
module, which adopts the diffusion models to predict the residual
(high-frequency information, including text edges), between the ground-truth
and the CP-predicted image. DocDiff is a compact and computationally efficient
model that benefits from a well-designed network architecture, an optimized
training loss objective, and a deterministic sampling process with short time
steps. Extensive experiments demonstrate that DocDiff achieves state-of-the-art
(SOTA) performance on multiple benchmark datasets, and can significantly
enhance the readability and recognizability of degraded document images.
Furthermore, our proposed HRR module in pre-trained DocDiff is plug-and-play
and ready-to-use, with only 4.17M parameters. It greatly sharpens the text
edges generated by SOTA deblurring methods without additional joint training.
Available codes: https://github.com/Royalvice/DocDiff
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Baolin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yongping Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Lan Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Guibin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Junjie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05144">
<title>Adapt and Align to Improve Zero-Shot Sketch-Based Image Retrieval. (arXiv:2305.05144v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05144</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot sketch-based image retrieval (ZS-SBIR) is challenging due to the
cross-domain nature of sketches and photos, as well as the semantic gap between
seen and unseen image distributions. Previous methods fine-tune pre-trained
models with various side information and learning strategies to learn a compact
feature space that is shared between the sketch and photo domains and bridges
seen and unseen classes. However, these efforts are inadequate in adapting
domains and transferring knowledge from seen to unseen classes. In this paper,
we present an effective ``Adapt and Align&apos;&apos; approach to address the key
challenges. Specifically, we insert simple and lightweight domain adapters to
learn new abstract concepts of the sketch domain and improve cross-domain
representation capabilities. Inspired by recent advances in image-text
foundation models (e.g., CLIP) on zero-shot scenarios, we explicitly align the
learned image embedding with a more semantic text embedding to achieve the
desired knowledge transfer from seen to unseen classes. Extensive experiments
on three benchmark datasets and two popular backbones demonstrate the
superiority of our method in terms of retrieval accuracy and flexibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shiyin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingrui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Nannan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07026">
<title>Decentralization and Acceleration Enables Large-Scale Bundle Adjustment. (arXiv:2305.07026v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07026</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling to arbitrarily large bundle adjustment problems requires data and
compute to be distributed across multiple devices. Centralized methods in prior
works are only able to solve small or medium size problems due to overhead in
computation and communication. In this paper, we present a fully decentralized
method that alleviates computation and communication bottlenecks to solve
arbitrarily large bundle adjustment problems. We achieve this by reformulating
the reprojection error and deriving a novel surrogate function that decouples
optimization variables from different devices. This function makes it possible
to use majorization minimization techniques and reduces bundle adjustment to
independent optimization subproblems that can be solved in parallel. We further
apply Nesterov&apos;s acceleration and adaptive restart to improve convergence while
maintaining its theoretical guarantees. Despite limited peer-to-peer
communication, our method has provable convergence to first-order critical
points under mild conditions. On extensive benchmarks with public datasets, our
method converges much faster than decentralized baselines with similar memory
usage and communication load. Compared to centralized baselines using a single
device, our method, while being decentralized, yields more accurate solutions
with significant speedups of up to 953.7x over Ceres and 174.6x over DeepLM.
Code: https://joeaortiz.github.io/daba.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Taosha Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_J/0/1/0/all/0/1&quot;&gt;Joseph Ortiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_M/0/1/0/all/0/1&quot;&gt;Ming Hsiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monge_M/0/1/0/all/0/1&quot;&gt;Maurizio Monge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murphey_T/0/1/0/all/0/1&quot;&gt;Todd Murphey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1&quot;&gt;Mustafa Mukadam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08992">
<title>The Brain Tumor Segmentation (BraTS) Challenge 2023: Local Synthesis of Healthy Brain Tissue via Inpainting. (arXiv:2305.08992v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08992</link>
<description rdf:parseType="Literal">&lt;p&gt;A myriad of algorithms for the automatic analysis of brain MR images is
available to support clinicians in their decision-making. For brain tumor
patients, the image acquisition time series typically starts with a scan that
is already pathological. This poses problems, as many algorithms are designed
to analyze healthy brains and provide no guarantees for images featuring
lesions. Examples include but are not limited to algorithms for brain anatomy
parcellation, tissue segmentation, and brain extraction. To solve this dilemma,
we introduce the BraTS 2023 inpainting challenge. Here, the participants&apos; task
is to explore inpainting techniques to synthesize healthy brain scans from
lesioned ones. The following manuscript contains the task formulation, dataset,
and submission procedure. Later it will be updated to summarize the findings of
the challenge. The challenge is organized as part of the BraTS 2023 challenge
hosted at the MICCAI 2023 conference in Vancouver, Canada.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meissen_F/0/1/0/all/0/1&quot;&gt;Felix Meissen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Steinbauer_F/0/1/0/all/0/1&quot;&gt;Felix Steinbauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Graf_R/0/1/0/all/0/1&quot;&gt;Robert Graf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oswald_E/0/1/0/all/0/1&quot;&gt;Eva Oswald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rosa_E/0/1/0/all/0/1&quot;&gt;Ezequiel de da Rosa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1&quot;&gt;Ujjwal Baid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hoelzl_F/0/1/0/all/0/1&quot;&gt;Florian Hoelzl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Turgut_O/0/1/0/all/0/1&quot;&gt;Oezguen Turgut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Horvath_I/0/1/0/all/0/1&quot;&gt;Izabela Horvath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Waldmannstetter_D/0/1/0/all/0/1&quot;&gt;Diana Waldmannstetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bukas_C/0/1/0/all/0/1&quot;&gt;Christina Bukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adewole_M/0/1/0/all/0/1&quot;&gt;Maruf Adewole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1&quot;&gt;Syed Muhammad Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janas_A/0/1/0/all/0/1&quot;&gt;Anastasia Janas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kazerooni_A/0/1/0/all/0/1&quot;&gt;Anahita Fathi Kazerooni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+LaBella_D/0/1/0/all/0/1&quot;&gt;Dominic LaBella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moawad_A/0/1/0/all/0/1&quot;&gt;Ahmed W Moawad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1&quot;&gt;Keyvan Farahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eddy_J/0/1/0/all/0/1&quot;&gt;James Eddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bergquist_T/0/1/0/all/0/1&quot;&gt;Timothy Bergquist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1&quot;&gt;Verena Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shinohara_R/0/1/0/all/0/1&quot;&gt;Russell Takeshi Shinohara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dako_F/0/1/0/all/0/1&quot;&gt;Farouk Dako&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiggins_W/0/1/0/all/0/1&quot;&gt;Walter Wiggins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reitman_Z/0/1/0/all/0/1&quot;&gt;Zachary Reitman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Familiar_A/0/1/0/all/0/1&quot;&gt;Ariana Familiar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1&quot;&gt;Gian-Marco Conte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johanson_E/0/1/0/all/0/1&quot;&gt;Elaine Johanson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meier_Z/0/1/0/all/0/1&quot;&gt;Zeke Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Davatzikos_C/0/1/0/all/0/1&quot;&gt;Christos Davatzikos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Freymann_J/0/1/0/all/0/1&quot;&gt;John Freymann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirby_J/0/1/0/all/0/1&quot;&gt;Justin Kirby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bilello_M/0/1/0/all/0/1&quot;&gt;Michel Bilello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fathallah_Shaykh_H/0/1/0/all/0/1&quot;&gt;Hassan M Fathallah-Shaykh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiest_R/0/1/0/all/0/1&quot;&gt;Roland Wiest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirschke_J/0/1/0/all/0/1&quot;&gt;Jan Kirschke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Colen_R/0/1/0/all/0/1&quot;&gt;Rivka R Colen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kotrotsou_A/0/1/0/all/0/1&quot;&gt;Aikaterini Kotrotsou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lamontagne_P/0/1/0/all/0/1&quot;&gt;Pamela Lamontagne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marcus_D/0/1/0/all/0/1&quot;&gt;Daniel Marcus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Milchenko_M/0/1/0/all/0/1&quot;&gt;Mikhail Milchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nazeri_A/0/1/0/all/0/1&quot;&gt;Arash Nazeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Weber_M/0/1/0/all/0/1&quot;&gt;Marc-Andr&amp;#xe9; Weber&lt;/a&gt;, et al. (20 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09417">
<title>Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v3 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09417</link>
<description rdf:parseType="Literal">&lt;p&gt;With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. Please see
https://shivammehta25.github.io/Diff-TTSG/ for video examples, data, and code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Shivam Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alexanderson_S/0/1/0/all/0/1&quot;&gt;Simon Alexanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beskow_J/0/1/0/all/0/1&quot;&gt;Jonas Beskow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szekely_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;va Sz&amp;#xe9;kely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01097">
<title>MVDiffusion: Enabling Holistic Multi-view Image Generation with Correspondence-Aware Diffusion. (arXiv:2307.01097v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces MVDiffusion, a simple yet effective multi-view image
generation method for scenarios where pixel-to-pixel correspondences are
available, such as perspective crops from panorama or multi-view images given
geometry (depth maps and poses). Unlike prior models that rely on iterative
image warping and inpainting, MVDiffusion concurrently generates all images
with a global awareness, encompassing high resolution and rich content,
effectively addressing the error accumulation prevalent in preceding models.
MVDiffusion specifically incorporates a correspondence-aware attention
mechanism, enabling effective cross-view interaction. This mechanism underpins
three pivotal modules: 1) a generation module that produces low-resolution
images while maintaining global correspondence, 2) an interpolation module that
densifies spatial coverage between images, and 3) a super-resolution module
that upscales into high-resolution outputs. In terms of panoramic imagery,
MVDiffusion can generate high-resolution photorealistic images up to
1024$\times$1024 pixels. For geometry-conditioned multi-view image generation,
MVDiffusion demonstrates the first method capable of generating a textured map
of a scene mesh. The project page is at https://mvdiffusion.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Shitao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fuyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furukawa_Y/0/1/0/all/0/1&quot;&gt;Yasutaka Furukawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02347">
<title>Detecting Images Generated by Deep Diffusion Models using their Local Intrinsic Dimensionality. (arXiv:2307.02347v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02347</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models recently have been successfully applied for the visual
synthesis of strikingly realistic appearing images. This raises strong concerns
about their potential for malicious purposes. In this paper, we propose using
the lightweight multi Local Intrinsic Dimensionality (multiLID), which has been
originally developed in context of the detection of adversarial examples, for
the automatic detection of synthetic images and the identification of the
according generator networks. In contrast to many existing detection
approaches, which often only work for GAN-generated images, the proposed method
provides close to perfect detection results in many realistic use cases.
Extensive experiments on known and newly created datasets demonstrate that the
proposed multiLID approach exhibits superiority in diffusion detection and
model identification. Since the empirical evaluations of recent publications on
the detection of generated images are often mainly focused on the
&quot;LSUN-Bedroom&quot; dataset, we further establish a comprehensive benchmark for the
detection of diffusion-generated images, including samples from several
diffusion models with different image sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_P/0/1/0/all/0/1&quot;&gt;Peter Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durall_R/0/1/0/all/0/1&quot;&gt;Ricard Durall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08209">
<title>Ada3D : Exploiting the Spatial Redundancy with Adaptive Inference for Efficient 3D Object Detection. (arXiv:2307.08209v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08209</link>
<description rdf:parseType="Literal">&lt;p&gt;Voxel-based methods have achieved state-of-the-art performance for 3D object
detection in autonomous driving. However, their significant computational and
memory costs pose a challenge for their application to resource-constrained
vehicles. One reason for this high resource consumption is the presence of a
large number of redundant background points in Lidar point clouds, resulting in
spatial redundancy in both 3D voxel and dense BEV map representations. To
address this issue, we propose an adaptive inference framework called Ada3D,
which focuses on exploiting the input-level spatial redundancy. Ada3D
adaptively filters the redundant input, guided by a lightweight importance
predictor and the unique properties of the Lidar point cloud. Additionally, we
utilize the BEV features&apos; intrinsic sparsity by introducing the Sparsity
Preserving Batch Normalization. With Ada3D, we achieve 40% reduction for 3D
voxels and decrease the density of 2D BEV feature maps from 100% to 20% without
sacrificing accuracy. Ada3D reduces the model computational and memory cost by
5x, and achieves 1.52x/1.45x end-to-end GPU latency and 1.5x/4.5x GPU peak
memory optimization for the 3D and 2D backbone respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianchen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_X/0/1/0/all/0/1&quot;&gt;Xuefei Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Ke Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yali Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Lipu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_G/0/1/0/all/0/1&quot;&gt;Guohao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huazhong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10763">
<title>Actor-agnostic Multi-label Action Recognition with Multi-modal Query. (arXiv:2307.10763v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10763</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing action recognition methods are typically actor-specific due to the
intrinsic topological and apparent differences among the actors. This requires
actor-specific pose estimation (e.g., humans vs. animals), leading to
cumbersome model design complexity and high maintenance costs. Moreover, they
often focus on learning the visual modality alone and single-label
classification whilst neglecting other available information sources (e.g.,
class name text) and the concurrent occurrence of multiple actions. To overcome
these limitations, we propose a new approach called &apos;actor-agnostic multi-modal
multi-label action recognition,&apos; which offers a unified solution for various
types of actors, including humans and animals. We further formulate a novel
Multi-modal Semantic Query Network (MSQNet) model in a transformer-based object
detection framework (e.g., DETR), characterized by leveraging visual and
textual modalities to represent the action classes better. The elimination of
actor-specific model designs is a key advantage, as it removes the need for
actor pose estimation altogether. Extensive experiments on five publicly
available benchmarks show that our MSQNet consistently outperforms the prior
arts of actor-specific alternatives on human and animal single- and multi-label
action recognition tasks by up to 50%. Code will be released at
https://github.com/mondalanindya/MSQNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_A/0/1/0/all/0/1&quot;&gt;Anindya Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nag_S/0/1/0/all/0/1&quot;&gt;Sauradip Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prada_J/0/1/0/all/0/1&quot;&gt;Joaquin M Prada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiatian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_A/0/1/0/all/0/1&quot;&gt;Anjan Dutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12217">
<title>LoLep: Single-View View Synthesis with Locally-Learned Planes and Self-Attention Occlusion Inference. (arXiv:2307.12217v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12217</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel method, LoLep, which regresses Locally-Learned planes from
a single RGB image to represent scenes accurately, thus generating better novel
views. Without the depth information, regressing appropriate plane locations is
a challenging problem. To solve this issue, we pre-partition the disparity
space into bins and design a disparity sampler to regress local offsets for
multiple planes in each bin. However, only using such a sampler makes the
network not convergent; we further propose two optimizing strategies that
combine with different disparity distributions of datasets and propose an
occlusion-aware reprojection loss as a simple yet effective geometric
supervision technique. We also introduce a self-attention mechanism to improve
occlusion inference and present a Block-Sampling Self-Attention (BS-SA) module
to address the problem of applying self-attention to large feature maps. We
demonstrate the effectiveness of our approach and generate state-of-the-art
results on different datasets. Compared to MINE, our approach has an LPIPS
reduction of 4.8%-9.0% and an RV reduction of 73.9%-83.5%. We also evaluate the
performance on real-world images and demonstrate the benefits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Ping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manocha_D/0/1/0/all/0/1&quot;&gt;Dinesh Manocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13567">
<title>Mystique: Deconstructing SVG Charts for Layout Reuse. (arXiv:2307.13567v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13567</link>
<description rdf:parseType="Literal">&lt;p&gt;To facilitate the reuse of existing charts, previous research has examined
how to obtain a semantic understanding of a chart by deconstructing its visual
representation into reusable components, such as encodings. However, existing
deconstruction approaches primarily focus on chart styles, handling only basic
layouts. In this paper, we investigate how to deconstruct chart layouts,
focusing on rectangle-based ones, as they cover not only 17 chart types but
also advanced layouts (e.g., small multiples, nested layouts). We develop an
interactive tool, called Mystique, adopting a mixed-initiative approach to
extract the axes and legend, and deconstruct a chart&apos;s layout into four
semantic components: mark groups, spatial relationships, data encodings, and
graphical constraints. Mystique employs a wizard interface that guides chart
authors through a series of steps to specify how the deconstructed components
map to their own data. On 150 rectangle-based SVG charts, Mystique achieves
above 85% accuracy for axis and legend extraction and 96% accuracy for layout
deconstruction. In a chart reproduction study, participants could easily reuse
existing charts on new datasets. We discuss the current limitations of Mystique
and future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Bongshin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yunjeong Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00247">
<title>Unleashing the Power of Self-Supervised Image Denoising: A Comprehensive Review. (arXiv:2308.00247v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00247</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of deep learning has brought a revolutionary transformation to
image denoising techniques. However, the persistent challenge of acquiring
noise-clean pairs for supervised methods in real-world scenarios remains
formidable, necessitating the exploration of more practical self-supervised
image denoising. This paper focuses on self-supervised image denoising methods
that offer effective solutions to address this challenge. Our comprehensive
review thoroughly analyzes the latest advancements in self-supervised image
denoising approaches, categorizing them into three distinct classes: General
methods, Blind Spot Network (BSN)-based methods, and Transformer-based methods.
For each class, we provide a concise theoretical analysis along with their
practical applications. To assess the effectiveness of these methods, we
present both quantitative and qualitative experimental results on various
datasets, utilizing classical algorithms as benchmarks. Additionally, we
critically discuss the current limitations of these methods and propose
promising directions for future research. By offering a detailed overview of
recent developments in self-supervised image denoising, this review serves as
an invaluable resource for researchers and practitioners in the field,
facilitating a deeper understanding of this emerging domain and inspiring
further advancements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fangfang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yuanzhou Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuan Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01097">
<title>Spatio-Temporal Branching for Motion Prediction using Motion Increments. (arXiv:2308.01097v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01097</link>
<description rdf:parseType="Literal">&lt;p&gt;Human motion prediction (HMP) has emerged as a popular research topic due to
its diverse applications, but it remains a challenging task due to the
stochastic and aperiodic nature of future poses. Traditional methods rely on
hand-crafted features and machine learning techniques, which often struggle to
model the complex dynamics of human motion. Recent deep learning-based methods
have achieved success by learning spatio-temporal representations of motion,
but these models often overlook the reliability of motion data. Additionally,
the temporal and spatial dependencies of skeleton nodes are distinct. The
temporal relationship captures motion information over time, while the spatial
relationship describes body structure and the relationships between different
nodes. In this paper, we propose a novel spatio-temporal branching network
using incremental information for HMP, which decouples the learning of
temporal-domain and spatial-domain features, extracts more motion information,
and achieves complementary cross-domain knowledge learning through knowledge
distillation. Our approach effectively reduces noise interference and provides
more expressive information for characterizing motion by separately extracting
temporal and spatial features. We evaluate our approach on standard HMP
benchmarks and outperform state-of-the-art methods in terms of prediction
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiexin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yujie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_Y/0/1/0/all/0/1&quot;&gt;Ying Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_B/0/1/0/all/0/1&quot;&gt;Bing Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01621">
<title>A Novel Convolutional Neural Network Architecture with a Continuous Symmetry. (arXiv:2308.01621v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01621</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a new Convolutional Neural Network (ConvNet)
architecture inspired by a class of partial differential equations (PDEs)
called quasi-linear hyperbolic systems. With comparable performance on the
image classification task, it allows for the modification of the weights via a
continuous group of symmetry. This is a significant shift from traditional
models where the architecture and weights are essentially fixed. We wish to
promote the (internal) symmetry as a new desirable property for a neural
network, and to draw attention to the PDE perspective in analyzing and
interpreting ConvNets in the broader Deep Learning community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hang Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_B/0/1/0/all/0/1&quot;&gt;Bing Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03936">
<title>ALFA -- Leveraging All Levels of Feature Abstraction for Enhancing the Generalization of Histopathology Image Classification Across Unseen Hospitals. (arXiv:2308.03936v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03936</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an exhaustive methodology that leverages all levels of feature
abstraction, targeting an enhancement in the generalizability of image
classification to unobserved hospitals. Our approach incorporates
augmentation-based self-supervision with common distribution shifts in
histopathology scenarios serving as the pretext task. This enables us to derive
invariant features from training images without relying on training labels,
thereby covering different abstraction levels. Moving onto the subsequent
abstraction level, we employ a domain alignment module to facilitate further
extraction of invariant features across varying training hospitals. To
represent the highly specific features of participating hospitals, an encoder
is trained to classify hospital labels, independent of their diagnostic labels.
The features from each of these encoders are subsequently disentangled to
minimize redundancy and segregate the features. This representation, which
spans a broad spectrum of semantic information, enables the development of a
model demonstrating increased robustness to unseen images from disparate
distributions. Experimental results from the PACS dataset (a domain
generalization benchmark), a synthetic dataset created by applying
histopathology-specific jitters to the MHIST dataset (defining different
domains with varied distribution shifts), and a Renal Cell Carcinoma dataset
derived from four image repositories from TCGA, collectively indicate that our
proposed model is adept at managing varying levels of image granularity. Thus,
it shows improved generalizability when faced with new, out-of-distribution
hospital images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sikaroudi_M/0/1/0/all/0/1&quot;&gt;Milad Sikaroudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1&quot;&gt;Maryam Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahnamayan_S/0/1/0/all/0/1&quot;&gt;Shahryar Rahnamayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tizhoosh_H/0/1/0/all/0/1&quot;&gt;H.R. Tizhoosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03999">
<title>Understanding CNN Hidden Neuron Activations Using Structured Background Knowledge and Deductive Reasoning. (arXiv:2308.03999v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03999</link>
<description rdf:parseType="Literal">&lt;p&gt;A major challenge in Explainable AI is in correctly interpreting activations
of hidden neurons: accurate interpretations would provide insights into the
question of what a deep learning system has internally detected as relevant on
the input, demystifying the otherwise black-box character of deep learning
systems. The state of the art indicates that hidden node activations can, in
some cases, be interpretable in a way that makes sense to humans, but
systematic automated methods that would be able to hypothesize and verify
interpretations of hidden neuron activations are underexplored. In this paper,
we provide such a method and demonstrate that it provides meaningful
interpretations. Our approach is based on using large-scale background
knowledge approximately 2 million classes curated from the Wikipedia concept
hierarchy together with a symbolic reasoning approach called Concept Induction
based on description logics, originally developed for applications in the
Semantic Web field. Our results show that we can automatically attach
meaningful labels from the background knowledge to individual neurons in the
dense layer of a Convolutional Neural Network through a hypothesis and
verification process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalal_A/0/1/0/all/0/1&quot;&gt;Abhilekha Dalal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_M/0/1/0/all/0/1&quot;&gt;Md Kamruzzaman Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barua_A/0/1/0/all/0/1&quot;&gt;Adrita Barua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasserman_E/0/1/0/all/0/1&quot;&gt;Eugene Vasserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitzler_P/0/1/0/all/0/1&quot;&gt;Pascal Hitzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04383">
<title>DELFlow: Dense Efficient Learning of Scene Flow for Large-Scale Point Clouds. (arXiv:2308.04383v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04383</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are naturally sparse, while image pixels are dense. The
inconsistency limits feature fusion from both modalities for point-wise scene
flow estimation. Previous methods rarely predict scene flow from the entire
point clouds of the scene with one-time inference due to the memory
inefficiency and heavy overhead from distance calculation and sorting involved
in commonly used farthest point sampling, KNN, and ball query algorithms for
local feature aggregation. To mitigate these issues in scene flow learning, we
regularize raw points to a dense format by storing 3D coordinates in 2D grids.
Unlike the sampling operation commonly used in existing works, the dense 2D
representation 1) preserves most points in the given scene, 2) brings in a
significant boost of efficiency, and 3) eliminates the density gap between
points and pixels, allowing us to perform effective feature fusion. We also
present a novel warping projection technique to alleviate the information loss
problem resulting from the fact that multiple points could be mapped into one
grid during projection when computing cost volume. Sufficient experiments
demonstrate the efficiency and effectiveness of our method, outperforming the
prior-arts on the FlyingThings3D and KITTI dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Chensheng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_X/0/1/0/all/0/1&quot;&gt;Xian Wan Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinrui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_W/0/1/0/all/0/1&quot;&gt;Wei Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hesheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04402">
<title>Person Re-Identification without Identification via Event Anonymization. (arXiv:2308.04402v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04402</link>
<description rdf:parseType="Literal">&lt;p&gt;Wide-scale use of visual surveillance in public spaces puts individual
privacy at stake while increasing resource consumption (energy, bandwidth, and
computation). Neuromorphic vision sensors (event-cameras) have been recently
considered a valid solution to the privacy issue because they do not capture
detailed RGB visual information of the subjects in the scene. However, recent
deep learning architectures have been able to reconstruct images from event
cameras with high fidelity, reintroducing a potential threat to privacy for
event-based vision applications. In this paper, we aim to anonymize
event-streams to protect the identity of human subjects against such image
reconstruction attacks. To achieve this, we propose an end-to-end network
architecture jointly optimized for the twofold objective of preserving privacy
and performing a downstream task such as person ReId. Our network learns to
scramble events, enforcing the degradation of images recovered from the privacy
attacker. In this work, we also bring to the community the first ever
event-based person ReId dataset gathered to evaluate the performance of our
approach. We validate our approach with extensive experiments and report
results on the synthetic event data simulated from the publicly available
SoftBio dataset and our proposed Event-ReId dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1&quot;&gt;Shafiq Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morerio_P/0/1/0/all/0/1&quot;&gt;Pietro Morerio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bue_A/0/1/0/all/0/1&quot;&gt;Alessio Del Bue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03217">
<title>Local Consensus Enhanced Siamese Network with Reciprocal Loss for Two-view Correspondence Learning. (arXiv:2308.03217v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2308.03217</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies of two-view correspondence learning usually establish an
end-to-end network to jointly predict correspondence reliability and relative
pose. We improve such a framework from two aspects. First, we propose a Local
Feature Consensus (LFC) plugin block to augment the features of existing
models. Given a correspondence feature, the block augments its neighboring
features with mutual neighborhood consensus and aggregates them to produce an
enhanced feature. As inliers obey a uniform cross-view transformation and share
more consistent learned features than outliers, feature consensus strengthens
inlier correlation and suppresses outlier distraction, which makes output
features more discriminative for classifying inliers/outliers. Second, existing
approaches supervise network training with the ground truth correspondences and
essential matrix projecting one image to the other for an input image pair,
without considering the information from the reverse mapping. We extend
existing models to a Siamese network with a reciprocal loss that exploits the
supervision of mutual projection, which considerably promotes the matching
performance without introducing additional model parameters. Building upon
MSA-Net, we implement the two proposals and experimentally achieve
state-of-the-art performance on benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Linbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xianyong Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chenjie Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>