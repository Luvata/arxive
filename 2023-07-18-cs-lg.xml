<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-17T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07514" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07738" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07816" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07907" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07941" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1901.10568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1909.07750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1911.02265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1911.10367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2005.11018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2011.11576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.13389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.02797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.04486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.12976" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.01771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.12440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.05767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.01970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.03923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.12086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.14842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.05794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.06767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.10603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.04947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.05379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.10818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.11533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.02641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.16162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.01947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07863" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09921" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.13262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.03323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.03935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.13016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06324" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07264" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.07513">
<title>An empirical study of using radiology reports and images to improve ICU mortality prediction. (arXiv:2307.07513v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07513</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: The predictive Intensive Care Unit (ICU) scoring system plays an
important role in ICU management because it predicts important outcomes,
especially mortality. Many scoring systems have been developed and used in the
ICU. These scoring systems are primarily based on the structured clinical data
in the electronic health record (EHR), which may suffer the loss of important
clinical information in the narratives and images. Methods: In this work, we
build a deep learning based survival prediction model with multi-modality data
to predict ICU mortality. Four sets of features are investigated: (1)
physiological measurements of Simplified Acute Physiology Score (SAPS) II, (2)
common thorax diseases pre-defined by radiologists, (3) BERT-based text
representations, and (4) chest X-ray image features. We use the Medical
Information Mart for Intensive Care IV (MIMIC-IV) dataset to evaluate the
proposed model. Results: Our model achieves the average C-index of 0.7829 (95%
confidence interval, 0.7620-0.8038), which substantially exceeds that of the
baseline with SAPS-II features (0.7470 (0.7263-0.7676)). Ablation studies
further demonstrate the contributions of pre-defined labels (2.00%), text
features (2.44%), and image features (2.82%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingquan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Song Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Ying Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lihui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07514">
<title>Explainability is NOT a Game. (arXiv:2307.07514v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07514</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable artificial intelligence (XAI) aims to help human decision-makers
in understanding complex machine learning (ML) models. One of the hallmarks of
XAI are measures of relative feature importance, which are theoretically
justified through the use of Shapley values. This paper builds on recent work
and offers a simple argument for why Shapley values can provide misleading
measures of relative feature importance, by assigning more importance to
features that are irrelevant for a prediction, and assigning less importance to
features that are relevant for a prediction. The significance of these results
is that they effectively challenge the many proposed uses of measures of
relative feature importance in a fast-growing range of high-stakes application
domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marques_Silva_J/0/1/0/all/0/1&quot;&gt;Joao Marques-Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xuanxiang Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07516">
<title>Voting-based Multimodal Automatic Deception Detection. (arXiv:2307.07516v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07516</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic Deception Detection has been a hot research topic for a long time,
using machine learning and deep learning to automatically detect deception,
brings new light to this old field. In this paper, we proposed a voting-based
method for automatic deception detection from videos using audio, visual and
lexical features. Experiments were done on two datasets, the Real-life trial
dataset by Michigan University and the Miami University deception detection
dataset. Video samples were split into frames of images, audio, and
manuscripts. Our Voting-based Multimodal proposed solution consists of three
models. The first model is CNN for detecting deception from images, the second
model is Support Vector Machine (SVM) on Mel spectrograms for detecting
deception from audio and the third model is Word2Vec on Support Vector Machine
(SVM) for detecting deception from manuscripts. Our proposed solution
outperforms state of the art. Best results achieved on images, audio and text
were 97%, 96%, 92% respectively on Real-Life Trial Dataset, and 97%, 82%, 73%
on video, audio and text respectively on Miami University Deception Detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Touma_L/0/1/0/all/0/1&quot;&gt;Lana Touma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horani_M/0/1/0/all/0/1&quot;&gt;Mohammad Al Horani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tailouni_M/0/1/0/all/0/1&quot;&gt;Manar Tailouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahabiah_A/0/1/0/all/0/1&quot;&gt;Anas Dahabiah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jallad_K/0/1/0/all/0/1&quot;&gt;Khloud Al Jallad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07522">
<title>The Future of Fundamental Science Led by Generative Closed-Loop Artificial Intelligence. (arXiv:2307.07522v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07522</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in machine learning and AI, including Generative AI and LLMs,
are disrupting technological innovation, product development, and society as a
whole. AI&apos;s contribution to technology can come from multiple approaches that
require access to large training data sets and clear performance evaluation
criteria, ranging from pattern recognition and classification to generative
models. Yet, AI has contributed less to fundamental science in part because
large data sets of high-quality data for scientific practice and model
discovery are more difficult to access. Generative AI, in general, and Large
Language Models in particular, may represent an opportunity to augment and
accelerate the scientific discovery of fundamental deep science with
quantitative models. Here we explore and investigate aspects of an AI-driven,
automated, closed-loop approach to scientific discovery, including self-driven
hypothesis generation and open-ended autonomous exploration of the hypothesis
space. Integrating AI-driven automation into the practice of science would
mitigate current problems, including the replication of findings, systematic
production of data, and ultimately democratisation of the scientific process.
Realising these possibilities requires a vision for augmented AI coupled with a
diversity of AI approaches able to deal with fundamental aspects of causality
analysis and model discovery while enabling unbiased search across the space of
putative explanations. These advances hold the promise to unleash AI&apos;s
potential for searching and discovering the fundamental structure of our world
beyond what human scientists have been able to achieve. Such a vision would
push the boundaries of new fundamental science rather than automatize current
workflows and instead open doors for technological innovation to tackle some of
the greatest challenges facing humanity today.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zenil_H/0/1/0/all/0/1&quot;&gt;Hector Zenil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegner_J/0/1/0/all/0/1&quot;&gt;Jesper Tegn&amp;#xe9;r&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrahao_F/0/1/0/all/0/1&quot;&gt;Felipe S. Abrah&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lavin_A/0/1/0/all/0/1&quot;&gt;Alexander Lavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vipin Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frey_J/0/1/0/all/0/1&quot;&gt;Jeremy G. Frey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_A/0/1/0/all/0/1&quot;&gt;Adrian Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soldatova_L/0/1/0/all/0/1&quot;&gt;Larisa Soldatova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bundy_A/0/1/0/all/0/1&quot;&gt;Alan R. Bundy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jennings_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Jennings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_K/0/1/0/all/0/1&quot;&gt;Koichi Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_L/0/1/0/all/0/1&quot;&gt;Lawrence Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dzeroski_S/0/1/0/all/0/1&quot;&gt;Saso Dzeroski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briggs_A/0/1/0/all/0/1&quot;&gt;Andrew Briggs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gregory_F/0/1/0/all/0/1&quot;&gt;Frederick D. Gregory&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_C/0/1/0/all/0/1&quot;&gt;Carla P. Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1&quot;&gt;Christopher K. I. Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rowe_J/0/1/0/all/0/1&quot;&gt;Jon Rowe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evans_J/0/1/0/all/0/1&quot;&gt;James Evans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitano_H/0/1/0/all/0/1&quot;&gt;Hiroaki Kitano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_R/0/1/0/all/0/1&quot;&gt;Ross King&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07527">
<title>Machine Learning for Autonomous Vehicle&apos;s Trajectory Prediction: A comprehensive survey, Challenges, and Future Research Directions. (arXiv:2307.07527v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07527</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Vehicles (AVs) have emerged as a promising solution by replacing
human drivers with advanced computer-aided decision-making systems. However,
for AVs to effectively navigate the road, they must possess the capability to
predict the future behavior of nearby traffic participants, similar to the
predictive driving abilities of human drivers. Building upon existing
literature is crucial to advance the field and develop a comprehensive
understanding of trajectory prediction methods in the context of automated
driving. To address this need, we have undertaken a comprehensive review that
focuses on trajectory prediction methods for AVs, with a particular emphasis on
machine learning techniques including deep learning and reinforcement
learning-based approaches. We have extensively examined over two hundred
studies related to trajectory prediction in the context of AVs. The paper
begins with an introduction to the general problem of predicting vehicle
trajectories and provides an overview of the key concepts and terminology used
throughout. After providing a brief overview of conventional methods, this
review conducts a comprehensive evaluation of several deep learning-based
techniques. Each method is summarized briefly, accompanied by a detailed
analysis of its strengths and weaknesses. The discussion further extends to
reinforcement learning-based methods. This article also examines the various
datasets and evaluation metrics that are commonly used in trajectory prediction
tasks. Encouraging an unbiased and objective discussion, we compare two major
learning processes, considering specific functional features. By identifying
challenges in the existing literature and outlining potential research
directions, this review significantly contributes to the advancement of
knowledge in the domain of AV trajectory prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharilya_V/0/1/0/all/0/1&quot;&gt;Vibha Bharilya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Neetesh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07529">
<title>Learning Multiple Coordinated Agents under Directed Acyclic Graph Constraints. (arXiv:2307.07529v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07529</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel multi-agent reinforcement learning (MARL) method
to learn multiple coordinated agents under directed acyclic graph (DAG)
constraints. Unlike existing MARL approaches, our method explicitly exploits
the DAG structure between agents to achieve more effective learning
performance. Theoretically, we propose a novel surrogate value function based
on a MARL model with synthetic rewards (MARLM-SR) and prove that it serves as a
lower bound of the optimal value function. Computationally, we propose a
practical training algorithm that exploits new notion of leader agent and
reward generator and distributor agent to guide the decomposed follower agents
to better explore the parameter space in environments with DAG constraints.
Empirically, we exploit four DAG environments including a real-world scheduling
for one of Intel&apos;s high volume packaging and test factory to benchmark our
methods and show it outperforms the other non-DAG approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_J/0/1/0/all/0/1&quot;&gt;Jaeyeon Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klabjan_D/0/1/0/all/0/1&quot;&gt;Diego Klabjan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1&quot;&gt;Nital S. Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiuqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ananthanarayanan_B/0/1/0/all/0/1&quot;&gt;Balakrishnan Ananthanarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dauod_H/0/1/0/all/0/1&quot;&gt;Husam Dauod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Juang_T/0/1/0/all/0/1&quot;&gt;Tzung-Han Juang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07539">
<title>Improved Self-Normalized Concentration in Hilbert Spaces: Sublinear Regret for GP-UCB. (arXiv:2307.07539v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07539</link>
<description rdf:parseType="Literal">&lt;p&gt;In the kernelized bandit problem, a learner aims to sequentially compute the
optimum of a function lying in a reproducing kernel Hilbert space given only
noisy evaluations at sequentially chosen points. In particular, the learner
aims to minimize regret, which is a measure of the suboptimality of the choices
made. Arguably the most popular algorithm is the Gaussian Process Upper
Confidence Bound (GP-UCB) algorithm, which involves acting based on a simple
linear estimator of the unknown function. Despite its popularity, existing
analyses of GP-UCB give a suboptimal regret rate, which fails to be sublinear
for many commonly used kernels such as the Mat\&apos;ern kernel. This has led to a
longstanding open question: are existing regret analyses for GP-UCB tight, or
can bounds be improved by using more sophisticated analytical techniques? In
this work, we resolve this open question and show that GP-UCB enjoys nearly
optimal regret. In particular, our results directly imply sublinear regret
rates for the Mat\&apos;ern kernel, improving over the state-of-the-art analyses and
partially resolving a COLT open problem posed by Vakili et al. Our improvements
rely on two key technical results. First, we use modern supermartingale
techniques to construct a novel, self-normalized concentration inequality that
greatly simplifies existing approaches. Second, we address the importance of
regularizing in proportion to the smoothness of the underlying kernel $k$.
Together, these new technical tools enable a simplified, tighter analysis of
the GP-UCB algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whitehouse_J/0/1/0/all/0/1&quot;&gt;Justin Whitehouse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Steven Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramdas_A/0/1/0/all/0/1&quot;&gt;Aaditya Ramdas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07542">
<title>Source-Free Domain Adaptation with Temporal Imputation for Time Series Data. (arXiv:2307.07542v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2307.07542</link>
<description rdf:parseType="Literal">&lt;p&gt;Source-free domain adaptation (SFDA) aims to adapt a pretrained model from a
labeled source domain to an unlabeled target domain without access to the
source domain data, preserving source domain privacy. Despite its prevalence in
visual applications, SFDA is largely unexplored in time series applications.
The existing SFDA methods that are mainly designed for visual applications may
fail to handle the temporal dynamics in time series, leading to impaired
adaptation performance. To address this challenge, this paper presents a simple
yet effective approach for source-free domain adaptation on time series data,
namely MAsk and imPUte (MAPU). First, to capture temporal information of the
source domain, our method performs random masking on the time series signals
while leveraging a novel temporal imputer to recover the original signal from a
masked version in the embedding space. Second, in the adaptation step, the
imputer network is leveraged to guide the target model to produce target
features that are temporally consistent with the source features. To this end,
our MAPU can explicitly account for temporal dependency during the adaptation
while avoiding the imputation in the noisy input space. Our method is the first
to handle temporal consistency in SFDA for time series data and can be
seamlessly equipped with other existing SFDA methods. Extensive experiments
conducted on three real-world time series datasets demonstrate that our MAPU
achieves significant performance gain over existing methods. Our code is
available at \url{https://github.com/mohamedr002/MAPU_SFDA_TS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ragab_M/0/1/0/all/0/1&quot;&gt;Mohamed Ragab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eldele_E/0/1/0/all/0/1&quot;&gt;Emadeldeen Eldele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Foo_C/0/1/0/all/0/1&quot;&gt;Chuan-Sheng Foo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoli Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghua Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07566">
<title>Reconstruction of 3-Axis Seismocardiogram from Right-to-left and Head-to-foot Components Using A Long Short-Term Memory Network. (arXiv:2307.07566v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2307.07566</link>
<description rdf:parseType="Literal">&lt;p&gt;This pilot study aims to develop a deep learning model for predicting
seismocardiogram (SCG) signals in the dorsoventral direction from the SCG
signals in the right-to-left and head-to-foot directions ($\textrm{SCG}_x$ and
$\textrm{SCG}_y$). The dataset used for the training and validation of the
model was obtained from 15 healthy adult subjects. The SCG signals were
recorded using tri-axial accelerometers placed on the chest of each subject.
The signals were then segmented using electrocardiogram R waves, and the
segments were downsampled, normalized, and centered around zero. The resulting
dataset was used to train and validate a long short-term memory (LSTM) network
with two layers and a dropout layer to prevent overfitting. The network took as
input 100-time steps of $\textrm{SCG}_x$ and $\textrm{SCG}_y$, representing one
cardiac cycle, and outputted a vector that mapped to the target variable being
predicted. The results showed that the LSTM model had a mean square error of
0.09 between the predicted and actual SCG segments in the dorsoventral
direction. The study demonstrates the potential of deep learning models for
reconstructing 3-axis SCG signals using the data obtained from dual-axis
accelerometers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mohammad Muntasir Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Taebi_A/0/1/0/all/0/1&quot;&gt;Amirtah&amp;#xe0; Taebi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07568">
<title>Variational Prediction. (arXiv:2307.07568v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07568</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian inference offers benefits over maximum likelihood, but it also comes
with computational costs. Computing the posterior is typically intractable, as
is marginalizing that posterior to form the posterior predictive distribution.
In this paper, we present variational prediction, a technique for directly
learning a variational approximation to the posterior predictive distribution
using a variational bound. This approach can provide good predictive
distributions without test time marginalization costs. We demonstrate
Variational Prediction on an illustrative toy example.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alemi_A/0/1/0/all/0/1&quot;&gt;Alexander A. Alemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poole_B/0/1/0/all/0/1&quot;&gt;Ben Poole&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07572">
<title>Harpa: High-Rate Phase Association with Travel Time Neural Fields. (arXiv:2307.07572v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2307.07572</link>
<description rdf:parseType="Literal">&lt;p&gt;Phase association groups seismic wave arrivals according to their originating
earthquakes. It is a fundamental task in a seismic data processing pipeline,
but challenging to perform for smaller, high-rate seismic events which carry
fundamental information about earthquake dynamics, especially with a commonly
assumed inaccurate wave speed model. As a consequence, most association methods
focus on larger events that occur at a lower rate and are thus easier to
associate, even though microseismicity provides a valuable description of the
elastic medium properties in the subsurface. In this paper, we show that
association is possible at rates much higher than previously reported even when
the wave speed is unknown. We propose Harpa, a high-rate seismic phase
association method which leverages deep neural fields to build generative
models of wave speeds and associated travel times, and first solves a joint
spatio--temporal source localization and wave speed recovery problem, followed
by association. We obviate the need for associated phases by interpreting
arrival time data as probability measures and using an optimal transport loss
to enforce data fidelity. The joint recovery problem is known to admit a unique
solution under certain conditions but due to the non-convexity of the
corresponding loss a simple gradient scheme converges to poor local minima. We
show that this is effectively mitigated by stochastic gradient Langevin
dynamics (SGLD). Numerical experiments show that \harpa~efficiently associates
high-rate seismicity clouds over complex, unknown wave speeds and graciously
handles noisy and missing picks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Cheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hoop_M/0/1/0/all/0/1&quot;&gt;Maarten V. de Hoop&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Dokmanic_I/0/1/0/all/0/1&quot;&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07575">
<title>A Quantitative Approach to Predicting Representational Learning and Performance in Neural Networks. (arXiv:2307.07575v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07575</link>
<description rdf:parseType="Literal">&lt;p&gt;A key property of neural networks (both biological and artificial) is how
they learn to represent and manipulate input information in order to solve a
task. Different types of representations may be suited to different types of
tasks, making identifying and understanding learned representations a critical
part of understanding and designing useful networks. In this paper, we
introduce a new pseudo-kernel based tool for analyzing and predicting learned
representations, based only on the initial conditions of the network and the
training curriculum. We validate the method on a simple test case, before
demonstrating its use on a question about the effects of representational
learning on sequential single versus concurrent multitask performance. We show
that our method can be used to predict the effects of the scale of weight
initialization and training curriculum on representational learning and
downstream concurrent multitasking performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pyle_R/0/1/0/all/0/1&quot;&gt;Ryan Pyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musslick_S/0/1/0/all/0/1&quot;&gt;Sebastian Musslick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_A/0/1/0/all/0/1&quot;&gt;Ankit B. Patel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07595">
<title>Training Discrete Energy-Based Models with Energy Discrepancy. (arXiv:2307.07595v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.07595</link>
<description rdf:parseType="Literal">&lt;p&gt;Training energy-based models (EBMs) on discrete spaces is challenging because
sampling over such spaces can be difficult. We propose to train discrete EBMs
with energy discrepancy (ED), a novel type of contrastive loss functional which
only requires the evaluation of the energy function at data points and their
perturbed counter parts, thus not relying on sampling strategies like Markov
chain Monte Carlo (MCMC). Energy discrepancy offers theoretical guarantees for
a broad class of perturbation processes of which we investigate three types:
perturbations based on Bernoulli noise, based on deterministic transforms, and
based on neighbourhood structures. We demonstrate their relative performance on
lattice Ising models, binary synthetic data, and discrete image data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schroder_T/0/1/0/all/0/1&quot;&gt;Tobias Schr&amp;#xf6;der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ou_Z/0/1/0/all/0/1&quot;&gt;Zijing Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Duncan_A/0/1/0/all/0/1&quot;&gt;Andrew B. Duncan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07604">
<title>Smooth Lower Bounds for Differentially Private Algorithms via Padding-and-Permuting Fingerprinting Codes. (arXiv:2307.07604v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.07604</link>
<description rdf:parseType="Literal">&lt;p&gt;Fingerprinting arguments, first introduced by Bun, Ullman, and Vadhan (STOC
2014), are the most widely used method for establishing lower bounds on the
sample complexity or error of approximately differentially private (DP)
algorithms. Still, there are many problems in differential privacy for which we
don&apos;t know suitable lower bounds, and even for problems that we do, the lower
bounds are not smooth, and usually become vacuous when the error is larger than
some threshold.
&lt;/p&gt;
&lt;p&gt;In this work, we present a simple method to generate hard instances by
applying a padding-and-permuting transformation to a fingerprinting code. We
illustrate the applicability of this method by providing new lower bounds in
various settings:
&lt;/p&gt;
&lt;p&gt;1. A tight lower bound for DP averaging in the low-accuracy regime, which in
particular implies a new lower bound for the private 1-cluster problem
introduced by Nissim, Stemmer, and Vadhan (PODS 2016).
&lt;/p&gt;
&lt;p&gt;2. A lower bound on the additive error of DP algorithms for approximate
k-means clustering, as a function of the multiplicative error, which is tight
for a constant multiplication error.
&lt;/p&gt;
&lt;p&gt;3. A lower bound for estimating the top singular vector of a matrix under DP
in low-accuracy regimes, which is a special case of DP subspace estimation
studied by Singhal and Steinke (NeurIPS 2021).
&lt;/p&gt;
&lt;p&gt;Our main technique is to apply a padding-and-permuting transformation to a
fingerprinting code. However, rather than proving our results using a black-box
access to an existing fingerprinting code (e.g., Tardos&apos; code), we develop a
new fingerprinting lemma that is stronger than those of Dwork et al. (FOCS
2015) and Bun et al. (SODA 2017), and prove our lower bounds directly from the
lemma. Our lemma, in particular, gives a simpler fingerprinting code
construction with optimal rate (up to polylogarithmic factors) that is of
independent interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peter_N/0/1/0/all/0/1&quot;&gt;Naty Peter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsfadia_E/0/1/0/all/0/1&quot;&gt;Eliad Tsfadia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ullman_J/0/1/0/all/0/1&quot;&gt;Jonathan Ullman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07605">
<title>First-order Methods for Affinely Constrained Composite Non-convex Non-smooth Problems: Lower Complexity Bound and Near-optimal Methods. (arXiv:2307.07605v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.07605</link>
<description rdf:parseType="Literal">&lt;p&gt;Many recent studies on first-order methods (FOMs) focus on \emph{composite
non-convex non-smooth} optimization with linear and/or nonlinear function
constraints. Upper (or worst-case) complexity bounds have been established for
these methods. However, little can be claimed about their optimality as no
lower bound is known, except for a few special \emph{smooth non-convex} cases.
In this paper, we make the first attempt to establish lower complexity bounds
of FOMs for solving a class of composite non-convex non-smooth optimization
with linear constraints. Assuming two different first-order oracles, we
establish lower complexity bounds of FOMs to produce a (near)
$\epsilon$-stationary point of a problem (and its reformulation) in the
considered problem class, for any given tolerance $\epsilon&amp;gt;0$. In addition, we
present an inexact proximal gradient (IPG) method by using the more relaxed one
of the two assumed first-order oracles. The oracle complexity of the proposed
IPG, to find a (near) $\epsilon$-stationary point of the considered problem and
its reformulation, matches our established lower bounds up to a logarithmic
factor. Therefore, our lower complexity bounds and the proposed IPG method are
almost non-improvable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lin_Q/0/1/0/all/0/1&quot;&gt;Qihang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yangyang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07614">
<title>Towards Generalizable Detection of Urgency of Discussion Forum Posts. (arXiv:2307.07614v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07614</link>
<description rdf:parseType="Literal">&lt;p&gt;Students who take an online course, such as a MOOC, use the course&apos;s
discussion forum to ask questions or reach out to instructors when encountering
an issue. However, reading and responding to students&apos; questions is difficult
to scale because of the time needed to consider each message. As a result,
critical issues may be left unresolved, and students may lose the motivation to
continue in the course. To help address this problem, we build predictive
models that automatically determine the urgency of each forum post, so that
these posts can be brought to instructors&apos; attention. This paper goes beyond
previous work by predicting not just a binary decision cut-off but a post&apos;s
level of urgency on a 7-point scale. First, we train and cross-validate several
models on an original data set of 3,503 posts from MOOCs at University of
Pennsylvania. Second, to determine the generalizability of our models, we test
their performance on a separate, previously published data set of 29,604 posts
from MOOCs at Stanford University. While the previous work on post urgency used
only one data set, we evaluated the prediction across different data sets and
courses. The best-performing model was a support vector regressor trained on
the Universal Sentence Encoder embeddings of the posts, achieving an RMSE of
1.1 on the training set and 1.4 on the test set. Understanding the urgency of
forum posts enables instructors to focus their time more effectively and, as a
result, better support student learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svabensky_V/0/1/0/all/0/1&quot;&gt;Valdemar &amp;#x160;v&amp;#xe1;bensk&amp;#xfd;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1&quot;&gt;Ryan S. Baker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zambrano_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Zambrano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yishan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Slater_S/0/1/0/all/0/1&quot;&gt;Stefan Slater&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07615">
<title>Efficiently Factorizing Boolean Matrices using Proximal Gradient Descent. (arXiv:2307.07615v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07615</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the interpretability problem of NMF on Boolean data, Boolean
Matrix Factorization (BMF) uses Boolean algebra to decompose the input into
low-rank Boolean factor matrices. These matrices are highly interpretable and
very useful in practice, but they come at the high computational cost of
solving an NP-hard combinatorial optimization problem. To reduce the
computational burden, we propose to relax BMF continuously using a novel
elastic-binary regularizer, from which we derive a proximal gradient algorithm.
Through an extensive set of experiments, we demonstrate that our method works
well in practice: On synthetic data, we show that it converges quickly,
recovers the ground truth precisely, and estimates the simulated rank exactly.
On real-world data, we improve upon the state of the art in recall, loss, and
runtime, and a case study from the medical domain confirms that our results are
easily interpretable and semantically meaningful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalleiger_S/0/1/0/all/0/1&quot;&gt;Sebastian Dalleiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1&quot;&gt;Jilles Vreeken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07620">
<title>Generalizable Embeddings with Cross-batch Metric Learning. (arXiv:2307.07620v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07620</link>
<description rdf:parseType="Literal">&lt;p&gt;Global average pooling (GAP) is a popular component in deep metric learning
(DML) for aggregating features. Its effectiveness is often attributed to
treating each feature vector as a distinct semantic entity and GAP as a
combination of them. Albeit substantiated, such an explanation&apos;s algorithmic
implications to learn generalizable entities to represent unseen classes, a
crucial DML goal, remain unclear. To address this, we formulate GAP as a convex
combination of learnable prototypes. We then show that the prototype learning
can be expressed as a recursive process fitting a linear predictor to a batch
of samples. Building on that perspective, we consider two batches of disjoint
classes at each iteration and regularize the learning by expressing the samples
of a batch with the prototypes that are fitted to the other batch. We validate
our approach on 4 popular DML benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurbuz_Y/0/1/0/all/0/1&quot;&gt;Yeti Z. Gurbuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alatan_A/0/1/0/all/0/1&quot;&gt;A. Aydin Alatan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07631">
<title>Towards Model-Size Agnostic, Compute-Free, Memorization-based Inference of Deep Learning. (arXiv:2307.07631v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07631</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advancement of deep neural networks has significantly improved
various tasks, such as image and speech recognition. However, as the complexity
of these models increases, so does the computational cost and the number of
parameters, making it difficult to deploy them on resource-constrained devices.
This paper proposes a novel memorization-based inference (MBI) that is compute
free and only requires lookups. Specifically, our work capitalizes on the
inference mechanism of the recurrent attention model (RAM), where only a small
window of input domain (glimpse) is processed in a one time step, and the
outputs from multiple glimpses are combined through a hidden vector to
determine the overall classification output of the problem. By leveraging the
low-dimensionality of glimpse, our inference procedure stores key value pairs
comprising of glimpse location, patch vector, etc. in a table. The computations
are obviated during inference by utilizing the table to read out key-value
pairs and performing compute-free inference by memorization. By exploiting
Bayesian optimization and clustering, the necessary lookups are reduced, and
accuracy is improved. We also present in-memory computing circuits to quickly
look up the matching key vector to an input query. Compared to competitive
compute-in-memory (CIM) approaches, MBI improves energy efficiency by almost
2.7 times than multilayer perceptions (MLP)-CIM and by almost 83 times than
ResNet20-CIM for MNIST character recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giacomini_D/0/1/0/all/0/1&quot;&gt;Davide Giacomini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashem_M/0/1/0/all/0/1&quot;&gt;Maeesha Binte Hashem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suarez_J/0/1/0/all/0/1&quot;&gt;Jeremiah Suarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhunia_S/0/1/0/all/0/1&quot;&gt;Swarup Bhunia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_A/0/1/0/all/0/1&quot;&gt;Amit Ranjan Trivedi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07649">
<title>DistTGL: Distributed Memory-Based Temporal Graph Neural Network Training. (arXiv:2307.07649v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07649</link>
<description rdf:parseType="Literal">&lt;p&gt;Memory-based Temporal Graph Neural Networks are powerful tools in dynamic
graph representation learning and have demonstrated superior performance in
many real-world applications. However, their node memory favors smaller batch
sizes to capture more dependencies in graph events and needs to be maintained
synchronously across all trainers. As a result, existing frameworks suffer from
accuracy loss when scaling to multiple GPUs. Evenworse, the tremendous overhead
to synchronize the node memory make it impractical to be deployed to
distributed GPU clusters. In this work, we propose DistTGL -- an efficient and
scalable solution to train memory-based TGNNs on distributed GPU clusters.
DistTGL has three improvements over existing solutions: an enhanced TGNN model,
a novel training algorithm, and an optimized system. In experiments, DistTGL
achieves near-linear convergence speedup, outperforming state-of-the-art
single-machine method by 14.5% in accuracy and 10.17x in training throughput.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongkuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Da Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karypis_G/0/1/0/all/0/1&quot;&gt;George Karypis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasanna_V/0/1/0/all/0/1&quot;&gt;Viktor Prasanna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07650">
<title>SALC: Skeleton-Assisted Learning-Based Clustering for Time-Varying Indoor Localization. (arXiv:2307.07650v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07650</link>
<description rdf:parseType="Literal">&lt;p&gt;Wireless indoor localization has attracted significant amount of attention in
recent years. Using received signal strength (RSS) obtained from WiFi access
points (APs) for establishing fingerprinting database is a widely utilized
method in indoor localization. However, the time-variant problem for indoor
positioning systems is not well-investigated in existing literature. Compared
to conventional static fingerprinting, the dynamicallyreconstructed database
can adapt to a highly-changing environment, which achieves sustainability of
localization accuracy. To deal with the time-varying issue, we propose a
skeleton-assisted learning-based clustering localization (SALC) system,
including RSS-oriented map-assisted clustering (ROMAC), cluster-based online
database establishment (CODE), and cluster-scaled location estimation (CsLE).
The SALC scheme jointly considers similarities from the skeleton-based shortest
path (SSP) and the time-varying RSS measurements across the reference points
(RPs). ROMAC clusters RPs into different feature sets and therefore selects
suitable monitor points (MPs) for enhancing location estimation. Moreover, the
CODE algorithm aims for establishing adaptive fingerprint database to alleviate
the timevarying problem. Finally, CsLE is adopted to acquire the target
position by leveraging the benefits of clustering information and estimated
signal variations in order to rescale the weights fromweighted k-nearest
neighbors (WkNN) method. Both simulation and experimental results demonstrate
that the proposed SALC system can effectively reconstruct the fingerprint
database with an enhanced location estimation accuracy, which outperforms the
other existing schemes in the open literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsiao_A/0/1/0/all/0/1&quot;&gt;An-Hung Hsiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li-Hsiang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chen-Yi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_C/0/1/0/all/0/1&quot;&gt;Chun-Jie Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;Kai-Ten Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07652">
<title>DIGEST: Fast and Communication Efficient Decentralized Learning with Local Updates. (arXiv:2307.07652v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07652</link>
<description rdf:parseType="Literal">&lt;p&gt;Two widely considered decentralized learning algorithms are Gossip and random
walk-based learning. Gossip algorithms (both synchronous and asynchronous
versions) suffer from high communication cost, while random-walk based learning
experiences increased convergence time. In this paper, we design a fast and
communication-efficient asynchronous decentralized learning mechanism DIGEST by
taking advantage of both Gossip and random-walk ideas, and focusing on
stochastic gradient descent (SGD). DIGEST is an asynchronous decentralized
algorithm building on local-SGD algorithms, which are originally designed for
communication efficient centralized learning. We design both single-stream and
multi-stream DIGEST, where the communication overhead may increase when the
number of streams increases, and there is a convergence and communication
overhead trade-off which can be leveraged. We analyze the convergence of
single- and multi-stream DIGEST, and prove that both algorithms approach to the
optimal solution asymptotically for both iid and non-iid data distributions. We
evaluate the performance of single- and multi-stream DIGEST for logistic
regression and a deep neural network ResNet20. The simulation results confirm
that multi-stream DIGEST has nice convergence properties; i.e., its convergence
time is better than or comparable to the baselines in iid setting, and
outperforms the baselines in non-iid setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_P/0/1/0/all/0/1&quot;&gt;Peyman Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seferoglu_H/0/1/0/all/0/1&quot;&gt;Hulya Seferoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07657">
<title>Machine learning for option pricing: an empirical investigation of network architectures. (arXiv:2307.07657v1 [q-fin.CP])</title>
<link>http://arxiv.org/abs/2307.07657</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the supervised learning problem of learning the price of an
option or the implied volatility given appropriate input data (model
parameters) and corresponding output data (option prices or implied
volatilities). The majority of articles in this literature considers a (plain)
feed forward neural network architecture in order to connect the neurons used
for learning the function mapping inputs to outputs. In this article, motivated
by methods in image classification and recent advances in machine learning
methods for PDEs, we investigate empirically whether and how the choice of
network architecture affects the accuracy and training time of a machine
learning algorithm. We find that for option pricing problems, where we focus on
the Black--Scholes and the Heston model, the generalized highway network
architecture outperforms all other variants, when considering the mean squared
error and the training time as criteria. Moreover, for the computation of the
implied volatility, after a necessary transformation, a variant of the DGM
architecture outperforms all other variants, when considering again the mean
squared error and the training time as criteria.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Mieghem_L/0/1/0/all/0/1&quot;&gt;Laurens Van Mieghem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Papapantoleon_A/0/1/0/all/0/1&quot;&gt;Antonis Papapantoleon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Papazoglou_Hennig_J/0/1/0/all/0/1&quot;&gt;Jonas Papazoglou-Hennig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07666">
<title>Efficient Action Robust Reinforcement Learning with Probabilistic Policy Execution Uncertainty. (arXiv:2307.07666v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07666</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust reinforcement learning (RL) aims to find a policy that optimizes the
worst-case performance in the face of uncertainties. In this paper, we focus on
action robust RL with the probabilistic policy execution uncertainty, in which,
instead of always carrying out the action specified by the policy, the agent
will take the action specified by the policy with probability $1-\rho$ and an
alternative adversarial action with probability $\rho$. We establish the
existence of an optimal policy on the action robust MDPs with probabilistic
policy execution uncertainty and provide the action robust Bellman optimality
equation for its solution. Furthermore, we develop Action Robust Reinforcement
Learning with Certificates (ARRLC) algorithm that achieves minimax optimal
regret and sample complexity. Furthermore, we conduct numerical experiments to
validate our approach&apos;s robustness, demonstrating that ARRLC outperforms
non-robust RL algorithms and converges faster than the robust TD algorithm in
the presence of action perturbations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhihan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Lifeng Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07670">
<title>Efficient Adversarial Attacks on Online Multi-agent Reinforcement Learning. (arXiv:2307.07670v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07670</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the broad range of applications of multi-agent reinforcement learning
(MARL), understanding the effects of adversarial attacks against MARL model is
essential for the safe applications of this model. Motivated by this, we
investigate the impact of adversarial attacks on MARL. In the considered setup,
there is an exogenous attacker who is able to modify the rewards before the
agents receive them or manipulate the actions before the environment receives
them. The attacker aims to guide each agent into a target policy or maximize
the cumulative rewards under some specific reward function chosen by the
attacker, while minimizing the amount of manipulation on feedback and action.
We first show the limitations of the action poisoning only attacks and the
reward poisoning only attacks. We then introduce a mixed attack strategy with
both the action poisoning and the reward poisoning. We show that the mixed
attack strategy can efficiently attack MARL agents even if the attacker has no
prior information about the underlying environment and the agents&apos; algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guanlin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_L/0/1/0/all/0/1&quot;&gt;Lifeng Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07674">
<title>An Empirical Study of the Effectiveness of Using a Replay Buffer on Mode Discovery in GFlowNets. (arXiv:2307.07674v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07674</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) algorithms aim to learn an optimal policy by
iteratively sampling actions to learn how to maximize the total expected
return, $R(x)$. GFlowNets are a special class of algorithms designed to
generate diverse candidates, $x$, from a discrete set, by learning a policy
that approximates the proportional sampling of $R(x)$. GFlowNets exhibit
improved mode discovery compared to conventional RL algorithms, which is very
useful for applications such as drug discovery and combinatorial search.
However, since GFlowNets are a relatively recent class of algorithms, many
techniques which are useful in RL have not yet been associated with them. In
this paper, we study the utilization of a replay buffer for GFlowNets. We
explore empirically various replay buffer sampling techniques and assess the
impact on the speed of mode discovery and the quality of the modes discovered.
Our experimental results in the Hypergrid toy domain and a molecule synthesis
environment demonstrate significant improvements in mode discovery when
training with a replay buffer, compared to training only with trajectories
generated on-policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemgal_N/0/1/0/all/0/1&quot;&gt;Nikhil Vemgal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_E/0/1/0/all/0/1&quot;&gt;Elaine Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07675">
<title>On the Robustness of Epoch-Greedy in Multi-Agent Contextual Bandit Mechanisms. (arXiv:2307.07675v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07675</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficient learning in multi-armed bandit mechanisms such as pay-per-click
(PPC) auctions typically involves three challenges: 1) inducing truthful
bidding behavior (incentives), 2) using personalization in the users (context),
and 3) circumventing manipulations in click patterns (corruptions). Each of
these challenges has been studied orthogonally in the literature; incentives
have been addressed by a line of work on truthful multi-armed bandit
mechanisms, context has been extensively tackled by contextual bandit
algorithms, while corruptions have been discussed via a recent line of work on
bandits with adversarial corruptions. Since these challenges co-exist, it is
important to understand the robustness of each of these approaches in
addressing the other challenges, provide algorithms that can handle all
simultaneously, and highlight inherent limitations in this combination. In this
work, we show that the most prominent contextual bandit algorithm,
$\epsilon$-greedy can be extended to handle the challenges introduced by
strategic arms in the contextual multi-arm bandit mechanism setting. We further
show that $\epsilon$-greedy is inherently robust to adversarial data corruption
attacks and achieves performance that degrades linearly with the amount of
corruption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1&quot;&gt;Bhuvesh Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abernethy_J/0/1/0/all/0/1&quot;&gt;Jacob Abernethy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07679">
<title>Sharp Convergence Rates for Matching Pursuit. (arXiv:2307.07679v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.07679</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the fundamental limits of matching pursuit, or the pure greedy
algorithm, for approximating a target function by a sparse linear combination
of elements from a dictionary. When the target function is contained in the
variation space corresponding to the dictionary, many impressive works over the
past few decades have obtained upper and lower bounds on the convergence rate
of matching pursuit, but they do not match. The main contribution of this paper
is to close this gap and obtain a sharp characterization of the performance of
matching pursuit. We accomplish this by improving the existing lower bounds to
match the best upper bound. Specifically, we construct a worst case dictionary
which proves that the existing upper bound cannot be improved. It turns out
that, unlike other greedy algorithm variants, the converge rate is suboptimal
and is determined by the solution to a certain non-linear equation. This
enables us to conclude that any amount of shrinkage improves matching pursuit
in the worst case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Klusowski_J/0/1/0/all/0/1&quot;&gt;Jason M. Klusowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1&quot;&gt;Jonathan W. Siegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07681">
<title>Data-centric Operational Design Domain Characterization for Machine Learning-based Aeronautical Products. (arXiv:2307.07681v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.07681</link>
<description rdf:parseType="Literal">&lt;p&gt;We give a first rigorous characterization of Operational Design Domains
(ODDs) for Machine Learning (ML)-based aeronautical products. Unlike in other
application sectors (such as self-driving road vehicles) where ODD development
is scenario-based, our approach is data-centric: we propose the dimensions
along which the parameters that define an ODD can be explicitly captured,
together with a categorization of the data that ML-based applications can
encounter in operation, whilst identifying their system-level relevance and
impact. Specifically, we discuss how those data categories are useful to
determine: the requirements necessary to drive the design of ML Models (MLMs);
the potential effects on MLMs and higher levels of the system hierarchy; the
learning assurance processes that may be needed, and system architectural
considerations. We illustrate the underlying concepts with an example of an
aircraft flight envelope.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaakai_F/0/1/0/all/0/1&quot;&gt;Fateh Kaakai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adibhatla_S/0/1/0/all/0/1&quot;&gt;Shridhar &amp;quot;Shreeder&amp;quot; Adibhatla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pai_G/0/1/0/all/0/1&quot;&gt;Ganesh Pai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escorihuela_E/0/1/0/all/0/1&quot;&gt;Emmanuelle Escorihuela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07682">
<title>Learning Subjective Time-Series Data via Utopia Label Distribution Approximation. (arXiv:2307.07682v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07682</link>
<description rdf:parseType="Literal">&lt;p&gt;Subjective time-series regression (STR) tasks have gained increasing
attention recently. However, most existing methods overlook the label
distribution bias in STR data, which results in biased models. Emerging studies
on imbalanced regression tasks, such as age estimation and depth estimation,
hypothesize that the prior label distribution of the dataset is uniform.
However, we observe that the label distributions of training and test sets in
STR tasks are likely to be neither uniform nor identical. This distinct feature
calls for new approaches that estimate more reasonable distributions to train a
fair model. In this work, we propose Utopia Label Distribution Approximation
(ULDA) for time-series data, which makes the training label distribution closer
to real-world but unknown (utopia) label distribution. This would enhance the
model&apos;s fairness. Specifically, ULDA first convolves the training label
distribution by a Gaussian kernel. After convolution, the required sample
quantity at each regression label may change. We further devise the Time-slice
Normal Sampling (TNS) to generate new samples when the required sample quantity
is greater than the initial sample quantity, and the Convolutional Weighted
Loss (CWL) to lower the sample weight when the required sample quantity is less
than the initial quantity. These two modules not only assist the model training
on the approximated utopia label distribution, but also maintain the sample
continuity in temporal context space. To the best of our knowledge, ULDA is the
first method to address the label distribution bias in time-series data.
Extensive experiments demonstrate that ULDA lifts the state-of-the-art
performance on two STR tasks and three benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenxin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hexin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xuefeng Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Ying Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07686">
<title>Creating a Dataset Supporting Translation Between OpenMP Fortran and C++ Code. (arXiv:2307.07686v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.07686</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a novel dataset for training machine learning
models translating between OpenMP Fortran and C++ code. To ensure reliability
and applicability, the dataset is initially refined using a meticulous code
similarity test. The effectiveness of our dataset is assessed using both
quantitative (CodeBLEU) and qualitative (human evaluation) methods. We
demonstrate how this dataset can significantly improve the translation
capabilities of large-scale language models, with improvements of \times 5.1
for models with no prior coding knowledge and \times 9.9 for models with some
coding familiarity. Our work highlights the potential of this dataset to
advance the field of code translation for high-performance computing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_B/0/1/0/all/0/1&quot;&gt;Bin Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Le Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_P/0/1/0/all/0/1&quot;&gt;Pei-Hung Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chunhua Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07695">
<title>Reducing operator complexity in Algebraic Multigrid with Machine Learning Approaches. (arXiv:2307.07695v1 [math.NA])</title>
<link>http://arxiv.org/abs/2307.07695</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a data-driven and machine-learning-based approach to compute
non-Galerkin coarse-grid operators in algebraic multigrid (AMG) methods,
addressing the well-known issue of increasing operator complexity. Guided by
the AMG theory on spectrally equivalent coarse-grid operators, we have
developed novel ML algorithms that utilize neural networks (NNs) combined with
smooth test vectors from multigrid eigenvalue problems. The proposed method
demonstrates promise in reducing the complexity of coarse-grid operators while
maintaining overall AMG convergence for solving parametric partial differential
equation (PDE) problems. Numerical experiments on anisotropic rotated Laplacian
and linear elasticity problems are provided to showcase the performance and
compare with existing methods for computing non-Galerkin coarse-grid operators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ru Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Huan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruipeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xi_Y/0/1/0/all/0/1&quot;&gt;Yuanzhe Xi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07700">
<title>NeurASP: Embracing Neural Networks into Answer Set Programming. (arXiv:2307.07700v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07700</link>
<description rdf:parseType="Literal">&lt;p&gt;We present NeurASP, a simple extension of answer set programs by embracing
neural networks. By treating the neural network output as the probability
distribution over atomic facts in answer set programs, NeurASP provides a
simple and effective way to integrate sub-symbolic and symbolic computation. We
demonstrate how NeurASP can make use of a pre-trained neural network in
symbolic computation and how it can improve the neural network&apos;s perception
result by applying symbolic reasoning in answer set programming. Also, NeurASP
can be used to train a neural network better by training with ASP rules so that
a neural network not only learns from implicit correlations from the data but
also from the explicit complex semantic constraints expressed by the rules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishay_A/0/1/0/all/0/1&quot;&gt;Adam Ishay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joohyung Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07703">
<title>Identification of Stochasticity by Matrix-decomposition: Applied on Black Hole Data. (arXiv:2307.07703v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07703</link>
<description rdf:parseType="Literal">&lt;p&gt;Timeseries classification as stochastic (noise-like) or non-stochastic
(structured), helps understand the underlying dynamics, in several domains.
Here we propose a two-legged matrix decomposition-based algorithm utilizing two
complementary techniques for classification. In Singular Value Decomposition
(SVD) based analysis leg, we perform topological analysis (Betti numbers) on
singular vectors containing temporal information, leading to SVD-label.
Parallely, temporal-ordering agnostic Principal Component Analysis (PCA) is
performed, and the proposed PCA-derived features are computed. These features,
extracted from synthetic timeseries of the two labels, are observed to map the
timeseries to a linearly separable feature space. Support Vector Machine (SVM)
is used to produce PCA-label. The proposed methods have been applied to
synthetic data, comprising 41 realisations of white-noise, pink-noise
(stochastic), Logistic-map at growth-rate 4 and Lorentz-system
(non-stochastic), as proof-of-concept. Proposed algorithm is applied on
astronomical data: 12 temporal-classes of timeseries of black hole GRS
1915+105, obtained from RXTE satellite with average length 25000. For a given
timeseries, if SVD-label and PCA-label concur, then the label is retained; else
deemed &quot;Uncertain&quot;. Comparison of obtained results with those in literature are
presented. It&apos;s found that out of 12 temporal classes of GRS 1915+105,
concurrence between SVD-label and PCA-label is obtained on 11 of them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakka_S/0/1/0/all/0/1&quot;&gt;Sai Pradeep Chakka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vengalil_S/0/1/0/all/0/1&quot;&gt;Sunil Kumar Vengalil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1&quot;&gt;Neelam Sinha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07712">
<title>Visual Analytics For Machine Learning: A Data Perspective Survey. (arXiv:2307.07712v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07712</link>
<description rdf:parseType="Literal">&lt;p&gt;The past decade has witnessed a plethora of works that leverage the power of
visualization (VIS) to interpret machine learning (ML) models. The
corresponding research topic, VIS4ML, keeps growing at a fast pace. To better
organize the enormous works and shed light on the developing trend of VIS4ML,
we provide a systematic review of these works through this survey. Since data
quality greatly impacts the performance of ML models, our survey focuses
specifically on summarizing VIS4ML works from the data perspective. First, we
categorize the common data handled by ML models into five types, explain the
unique features of each type, and highlight the corresponding ML models that
are good at learning from them. Second, from the large number of VIS4ML works,
we tease out six tasks that operate on these types of data (i.e., data-centric
tasks) at different stages of the ML pipeline to understand, diagnose, and
refine ML models. Lastly, by studying the distribution of 143 surveyed papers
across the five data types, six data-centric tasks, and their intersections, we
analyze the prospective research directions and envision future research
trends.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shixia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07726">
<title>Towards Optimal Neural Networks: the Role of Sample Splitting in Hyperparameter Selection. (arXiv:2307.07726v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.07726</link>
<description rdf:parseType="Literal">&lt;p&gt;When artificial neural networks have demonstrated exceptional practical
success in a variety of domains, investigations into their theoretical
characteristics, such as their approximation power, statistical properties, and
generalization performance, have made significant strides. In this paper, we
construct a novel theory for understanding the effectiveness of neural networks
by discovering the mystery underlying a common practice during neural network
model construction: sample splitting. Our theory demonstrates that, the optimal
hyperparameters derived from sample splitting can enable a neural network model
that asymptotically minimizes the prediction risk. We conduct extensive
experiments across different application scenarios and network architectures,
and the results manifest our theory&apos;s effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Shijin Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07735">
<title>A Nearly-Linear Time Algorithm for Structured Support Vector Machines. (arXiv:2307.07735v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.07735</link>
<description rdf:parseType="Literal">&lt;p&gt;Quadratic programming is a fundamental problem in the field of convex
optimization. Many practical tasks can be formulated as quadratic programming,
for example, the support vector machine (SVM). Linear SVM is one of the most
popular tools over the last three decades in machine learning before deep
learning method dominating.
&lt;/p&gt;
&lt;p&gt;In general, a quadratic program has input size $\Theta(n^2)$ (where $n$ is
the number of variables), thus takes $\Omega(n^2)$ time to solve. Nevertheless,
quadratic programs coming from SVMs has input size $O(n)$, allowing the
possibility of designing nearly-linear time algorithms. Two important classes
of SVMs are programs admitting low-rank kernel factorizations and low-treewidth
programs. Low-treewidth convex optimization has gained increasing interest in
the past few years (e.g.~linear programming [Dong, Lee and Ye 2021] and
semidefinite programming [Gu and Song 2022]). Therefore, an important open
question is whether there exist nearly-linear time algorithms for quadratic
programs with these nice structures.
&lt;/p&gt;
&lt;p&gt;In this work, we provide the first nearly-linear time algorithm for solving
quadratic programming with low-rank factorization or low-treewidth, and a small
number of linear constraints. Our results imply nearly-linear time algorithms
for low-treewidth or low-rank SVMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lichen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07738">
<title>Negative probabilities in Gene Regulatory Networks. (arXiv:2307.07738v1 [q-bio.MN])</title>
<link>http://arxiv.org/abs/2307.07738</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a natural framework to identify sign-indefinite co-expressions
between genes based on the known expressions and given the sign of their
respective correlations. Specifically, given information concerning the
affinity among genes (i.e., connectivity in the gene regulatory network) and
knowledge whether they promote/inhibit co-expression of the respective protein
production, we seek rates that may explain the observed stationary
distributions at the level of proteins. We propose to encapsulate their
``promotion vs.\ inhibition&apos;&apos; functionality in a sign-indefinite probability
transition matrix--a matrix whose row-sums equal to one, but is otherwise sign
indefinite. The purpose of constructing such a representation for the
interaction network with sign-indefinite contributions in protein regulation,
is to quantify the structure and significance of various links, and to explain
how these may affect the geometry of the network, highlighting the significance
of the regulatory functions of certain genes. We cast the problem of finding
the interaction (sign-indefinite) transition matrix as a solution to a convex
optimization problem from which all the relevant geometric properties may be
easily derived.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dong_A/0/1/0/all/0/1&quot;&gt;Anqi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Georgiou_T/0/1/0/all/0/1&quot;&gt;Tryphon T. Georgiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tannenbaum_A/0/1/0/all/0/1&quot;&gt;Allen Tannenbaum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07744">
<title>On the Utility Gain of Iterative Bayesian Update for Locally Differentially Private Mechanisms. (arXiv:2307.07744v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.07744</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the utility gain of using Iterative Bayesian Update
(IBU) for private discrete distribution estimation using data obfuscated with
Locally Differentially Private (LDP) mechanisms. We compare the performance of
IBU to Matrix Inversion (MI), a standard estimation technique, for seven LDP
mechanisms designed for one-time data collection and for other seven LDP
mechanisms designed for multiple data collections (e.g., RAPPOR). To broaden
the scope of our study, we also varied the utility metric, the number of users
n, the domain size k, and the privacy parameter {\epsilon}, using both
synthetic and real-world data. Our results suggest that IBU can be a useful
post-processing tool for improving the utility of LDP mechanisms in different
scenarios without any additional privacy cost. For instance, our experiments
show that IBU can provide better utility than MI, especially in high privacy
regimes (i.e., when {\epsilon} is small). Our paper provides insights for
practitioners to use IBU in conjunction with existing LDP mechanisms for more
accurate and privacy-preserving data analysis. Finally, we implemented IBU for
all fourteen LDP mechanisms into the state-of-the-art multi-freq-ldpy Python
package (https://pypi.org/project/multi-freq-ldpy/) and open-sourced all our
code used for the experiments as tutorials.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcolezi_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;ber H. Arcolezi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerna_S/0/1/0/all/0/1&quot;&gt;Selene Cerna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palamidessi_C/0/1/0/all/0/1&quot;&gt;Catuscia Palamidessi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07753">
<title>Learning Expressive Priors for Generalization and Uncertainty Estimation in Neural Networks. (arXiv:2307.07753v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07753</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a novel prior learning method for advancing
generalization and uncertainty estimation in deep neural networks. The key idea
is to exploit scalable and structured posteriors of neural networks as
informative priors with generalization guarantees. Our learned priors provide
expressive probabilistic representations at large scale, like Bayesian
counterparts of pre-trained models on ImageNet, and further produce non-vacuous
generalization bounds. We also extend this idea to a continual learning
framework, where the favorable properties of our priors are desirable. Major
enablers are our technical contributions: (1) the sums-of-Kronecker-product
computations, and (2) the derivations and optimizations of tractable objectives
that lead to improved generalization bounds. Empirically, we exhaustively show
the effectiveness of this method for uncertainty estimation and generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnaus_D/0/1/0/all/0/1&quot;&gt;Dominik Schnaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jongseok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cremers_D/0/1/0/all/0/1&quot;&gt;Daniel Cremers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triebel_R/0/1/0/all/0/1&quot;&gt;Rudolph Triebel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07756">
<title>Real-time Traffic Classification for 5G NSA Encrypted Data Flows With Physical Channel Records. (arXiv:2307.07756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07756</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of fifth-generation New-Radio (5G-NR) mobile network
traffic is an emerging topic in the field of telecommunications. It can be
utilized for quality of service (QoS) management and dynamic resource
allocation. However, traditional approaches such as Deep Packet Inspection
(DPI) can not be directly applied to encrypted data flows. Therefore, new
real-time encrypted traffic classification algorithms need to be investigated
to handle dynamic transmission. In this study, we examine the real-time
encrypted 5G Non-Standalone (NSA) application-level traffic classification
using physical channel records. Due to the vastness of their features,
decision-tree-based gradient boosting algorithms are a viable approach for
classification. We generate a noise-limited 5G NSA trace dataset with traffic
from multiple applications. We develop a new pipeline to convert sequences of
physical channel records into numerical vectors. A set of machine learning
models are tested, and we propose our solution based on Light Gradient Boosting
Machine (LGBM) due to its advantages in fast parallel training and low
computational burden in practical scenarios. Our experiments demonstrate that
our algorithm can achieve 95% accuracy on the classification task with a
state-of-the-art response time as quick as 10ms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_X/0/1/0/all/0/1&quot;&gt;Xiao Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martins_P/0/1/0/all/0/1&quot;&gt;Philippe Martins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jialiang Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07770">
<title>randomHAR: Improving Ensemble Deep Learners for Human Activity Recognition with Sensor Selection and Reinforcement Learning. (arXiv:2307.07770v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07770</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has proven to be an effective approach in the field of Human
activity recognition (HAR), outperforming other architectures that require
manual feature engineering. Despite recent advancements, challenges inherent to
HAR data, such as noisy data, intra-class variability and inter-class
similarity, remain. To address these challenges, we propose an ensemble method,
called randomHAR. The general idea behind randomHAR is training a series of
deep learning models with the same architecture on randomly selected sensor
data from the given dataset. Besides, an agent is trained with the
reinforcement learning algorithm to identify the optimal subset of the trained
models that are utilized for runtime prediction. In contrast to existing work,
this approach optimizes the ensemble process rather than the architecture of
the constituent models. To assess the performance of the approach, we compare
it against two HAR algorithms, including the current state of the art, on six
HAR benchmark datasets. The result of the experiment demonstrates that the
proposed approach outperforms the state-of-the-art method, ensembleLSTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiran Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yexu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedel_T/0/1/0/all/0/1&quot;&gt;Till Riedel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Likun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beigl_M/0/1/0/all/0/1&quot;&gt;Michael Beigl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07771">
<title>CatBoost Versus XGBoost and LightGBM: Developing Enhanced Predictive Models for Zero-Inflated Insurance Claim Data. (arXiv:2307.07771v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07771</link>
<description rdf:parseType="Literal">&lt;p&gt;In the property and casualty insurance industry, some challenges are
presented in constructing claim predictive models due to a highly right-skewed
distribution of positive claims with excess zeros. Traditional models, such as
Poisson or negative binomial Generalized Linear Models(GLMs), frequently
struggle with inflated zeros. In response to this, researchers in actuarial
science have employed ``zero-inflated&quot; models that merge a traditional count
model and a binary model to address these datasets more effectively. This paper
uses boosting algorithms to process insurance claim data, including
zero-inflated telematics data, in order to construct claim frequency models. We
evaluated and compared three popular gradient boosting libraries - XGBoost,
LightGBM, and CatBoost - with the aim of identifying the most suitable library
for training insurance claim data and fitting actuarial frequency models.
Through a rigorous analysis of two distinct datasets, we demonstrated that
CatBoost is superior in developing auto claim frequency models based on
predictive performance. We also found that Zero-inflated Poisson boosted tree
models, with variations in their assumptions about the relationship between
inflation probability and distribution mean, outperformed others depending on
data characteristics. Furthermore, by using a specific CatBoost tool, we
explored the effects and interactions of different risk features on the
frequency model when using telematics data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+So_B/0/1/0/all/0/1&quot;&gt;Banghee So&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07785">
<title>The Interpolating Information Criterion for Overparameterized Models. (arXiv:2307.07785v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.07785</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of model selection is considered for the setting of interpolating
estimators, where the number of model parameters exceeds the size of the
dataset. Classical information criteria typically consider the large-data
limit, penalizing model size. However, these criteria are not appropriate in
modern settings where overparameterized models tend to perform well. For any
overparameterized model, we show that there exists a dual underparameterized
model that possesses the same marginal likelihood, thus establishing a form of
Bayesian duality. This enables more classical methods to be used in the
overparameterized setting, revealing the Interpolating Information Criterion, a
measure of model quality that naturally incorporates the choice of prior into
the model selection. Our new information criterion accounts for prior
misspecification, geometric and spectral properties of the model, and is
numerically consistent with known empirical and theoretical behavior in this
regime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hodgkinson_L/0/1/0/all/0/1&quot;&gt;Liam Hodgkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Heide_C/0/1/0/all/0/1&quot;&gt;Chris van der Heide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Salomone_R/0/1/0/all/0/1&quot;&gt;Robert Salomone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Roosta_F/0/1/0/all/0/1&quot;&gt;Fred Roosta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mahoney_M/0/1/0/all/0/1&quot;&gt;Michael W. Mahoney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07810">
<title>Graph Automorphism Group Equivariant Neural Networks. (arXiv:2307.07810v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07810</link>
<description rdf:parseType="Literal">&lt;p&gt;For any graph $G$ having $n$ vertices and its automorphism group
$\textrm{Aut}(G)$, we provide a full characterisation of all of the possible
$\textrm{Aut}(G)$-equivariant neural networks whose layers are some tensor
power of $\mathbb{R}^{n}$. In particular, we find a spanning set of matrices
for the learnable, linear, $\textrm{Aut}(G)$-equivariant layer functions
between such tensor power spaces in the standard basis of $\mathbb{R}^{n}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pearce_Crump_E/0/1/0/all/0/1&quot;&gt;Edward Pearce-Crump&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07816">
<title>Minimal Random Code Learning with Mean-KL Parameterization. (arXiv:2307.07816v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07816</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the qualitative behavior and robustness of two variants of
Minimal Random Code Learning (MIRACLE) used to compress variational Bayesian
neural networks. MIRACLE implements a powerful, conditionally Gaussian
variational approximation for the weight posterior $Q_{\mathbf{w}}$ and uses
relative entropy coding to compress a weight sample from the posterior using a
Gaussian coding distribution $P_{\mathbf{w}}$. To achieve the desired
compression rate, $D_{\mathrm{KL}}[Q_{\mathbf{w}} \Vert P_{\mathbf{w}}]$ must
be constrained, which requires a computationally expensive annealing procedure
under the conventional mean-variance (Mean-Var) parameterization for
$Q_{\mathbf{w}}$. Instead, we parameterize $Q_{\mathbf{w}}$ by its mean and KL
divergence from $P_{\mathbf{w}}$ to constrain the compression cost to the
desired value by construction. We demonstrate that variational training with
Mean-KL parameterization converges twice as fast and maintains predictive
performance after compression. Furthermore, we show that Mean-KL leads to more
meaningful variational distributions with heavier tails and compressed weight
samples which are more robust to pruning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jihao Andreas Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flamich_G/0/1/0/all/0/1&quot;&gt;Gergely Flamich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Lobato_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Miguel Hern&amp;#xe1;ndez-Lobato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07832">
<title>MixupExplainer: Generalizing Explanations for Graph Neural Networks with Data Augmentation. (arXiv:2307.07832v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07832</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have received increasing attention due to their
ability to learn from graph-structured data. However, their predictions are
often not interpretable. Post-hoc instance-level explanation methods have been
proposed to understand GNN predictions. These methods seek to discover
substructures that explain the prediction behavior of a trained GNN. In this
paper, we shed light on the existence of the distribution shifting issue in
existing methods, which affects explanation quality, particularly in
applications on real-life datasets with tight decision boundaries. To address
this issue, we introduce a generalized Graph Information Bottleneck (GIB) form
that includes a label-independent graph variable, which is equivalent to the
vanilla GIB. Driven by the generalized GIB, we propose a graph mixup method,
MixupExplainer, with a theoretical guarantee to resolve the distribution
shifting issue. We conduct extensive experiments on both synthetic and
real-world datasets to validate the effectiveness of our proposed mixup
approach over existing approaches. We also provide a detailed analysis of how
our proposed approach alleviates the distribution shifting issue.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dongsheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07840">
<title>RegExplainer: Generating Explanations for Graph Neural Networks in Regression Task. (arXiv:2307.07840v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07840</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph regression is a fundamental task and has received increasing attention
in a wide range of graph learning tasks. However, the inference process is
often not interpretable. Most existing explanation techniques are limited to
understanding GNN behaviors in classification tasks. In this work, we seek an
explanation to interpret the graph regression models (XAIG-R). We show that
existing methods overlook the distribution shifting and continuously ordered
decision boundary, which hinders them away from being applied in the regression
tasks. To address these challenges, we propose a novel objective based on the
information bottleneck theory and introduce a new mix-up framework, which could
support various GNNs in a model-agnostic manner. We further present a
contrastive learning strategy to tackle the continuously ordered labels in
regression task. To empirically verify the effectiveness of the proposed
method, we introduce three benchmark datasets and a real-life dataset for
evaluation. Extensive experiments show the effectiveness of the proposed method
in interpreting GNN models in regression tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuomin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dongsheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07843">
<title>Transformers are Universal Predictors. (arXiv:2307.07843v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07843</link>
<description rdf:parseType="Literal">&lt;p&gt;We find limits to the Transformer architecture for language modeling and show
it has a universal prediction property in an information-theoretic sense. We
further analyze performance in non-asymptotic data regimes to understand the
role of various components of the Transformer architecture, especially in the
context of data-efficient training. We validate our theoretical analysis with
experiments on both synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_S/0/1/0/all/0/1&quot;&gt;Sourya Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choraria_M/0/1/0/all/0/1&quot;&gt;Moulik Choraria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varshney_L/0/1/0/all/0/1&quot;&gt;Lav R. Varshney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07847">
<title>Neural Video Recovery for Cloud Gaming. (arXiv:2307.07847v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.07847</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud gaming is a multi-billion dollar industry. A client in cloud gaming
sends its movement to the game server on the Internet, which renders and
transmits the resulting video back. In order to provide a good gaming
experience, a latency below 80 ms is required. This means that video rendering,
encoding, transmission, decoding, and display have to finish within that time
frame, which is especially challenging to achieve due to server overload,
network congestion, and losses. In this paper, we propose a new method for
recovering lost or corrupted video frames in cloud gaming. Unlike traditional
video frame recovery, our approach uses game states to significantly enhance
recovery accuracy and utilizes partially decoded frames to recover lost
portions. We develop a holistic system that consists of (i) efficiently
extracting game states, (ii) modifying H.264 video decoder to generate a mask
to indicate which portions of video frames need recovery, and (iii) designing a
novel neural network to recover either complete or partial video frames. Our
approach is extensively evaluated using iPhone 12 and laptop implementations,
and we demonstrate the utility of game states in the game video recovery and
the effectiveness of our overall design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhaoyuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuozhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Diyuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lili Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07849">
<title>Variational Inference with Gaussian Score Matching. (arXiv:2307.07849v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.07849</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference (VI) is a method to approximate the computationally
intractable posterior distributions that arise in Bayesian statistics.
Typically, VI fits a simple parametric distribution to the target posterior by
minimizing an appropriate objective such as the evidence lower bound (ELBO). In
this work, we present a new approach to VI based on the principle of score
matching, that if two distributions are equal then their score functions (i.e.,
gradients of the log density) are equal at every point on their support. With
this, we develop score matching VI, an iterative algorithm that seeks to match
the scores between the variational approximation and the exact posterior. At
each iteration, score matching VI solves an inner optimization, one that
minimally adjusts the current variational estimate to match the scores at a
newly sampled value of the latent variables. We show that when the variational
family is a Gaussian, this inner optimization enjoys a closed form solution,
which we call Gaussian score matching VI (GSM-VI). GSM-VI is also a ``black
box&apos;&apos; variational algorithm in that it only requires a differentiable joint
distribution, and as such it can be applied to a wide class of models. We
compare GSM-VI to black box variational inference (BBVI), which has similar
requirements but instead optimizes the ELBO. We study how GSM-VI behaves as a
function of the problem dimensionality, the condition number of the target
covariance matrix (when the target is Gaussian), and the degree of mismatch
between the approximating and exact posterior distribution. We also study
GSM-VI on a collection of real-world Bayesian inference problems from the
posteriorDB database of datasets and models. In all of our studies we find that
GSM-VI is faster than BBVI, but without sacrificing accuracy. It requires
10-100x fewer gradient evaluations to obtain a comparable quality of
approximation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Modi_C/0/1/0/all/0/1&quot;&gt;Chirag Modi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Margossian_C/0/1/0/all/0/1&quot;&gt;Charles Margossian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuling Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gower_R/0/1/0/all/0/1&quot;&gt;Robert Gower&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blei_D/0/1/0/all/0/1&quot;&gt;David Blei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Saul_L/0/1/0/all/0/1&quot;&gt;Lawrence Saul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07863">
<title>Benchmarking the Effectiveness of Classification Algorithms and SVM Kernels for Dry Beans. (arXiv:2307.07863v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07863</link>
<description rdf:parseType="Literal">&lt;p&gt;Plant breeders and agricultural researchers can increase crop productivity by
identifying desirable features, disease resistance, and nutritional content by
analysing the Dry Bean dataset. This study analyses and compares different
Support Vector Machine (SVM) classification algorithms, namely linear,
polynomial, and radial basis function (RBF), along with other popular
classification algorithms. The analysis is performed on the Dry Bean Dataset,
with PCA (Principal Component Analysis) conducted as a preprocessing step for
dimensionality reduction. The primary evaluation metric used is accuracy, and
the RBF SVM kernel algorithm achieves the highest Accuracy of 93.34%, Precision
of 92.61%, Recall of 92.35% and F1 Score as 91.40%. Along with adept
visualization and empirical analysis, this study offers valuable guidance by
emphasizing the importance of considering different SVM algorithms for complex
and non-linear structured datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_A/0/1/0/all/0/1&quot;&gt;Anant Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_P/0/1/0/all/0/1&quot;&gt;Prajit Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_D/0/1/0/all/0/1&quot;&gt;Divisha Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_H/0/1/0/all/0/1&quot;&gt;Harpreet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diamand_Y/0/1/0/all/0/1&quot;&gt;Yosi Shacham Diamand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07868">
<title>Contrasting the efficiency of stock price prediction models using various types of LSTM models aided with sentiment analysis. (arXiv:2307.07868v1 [q-fin.ST])</title>
<link>http://arxiv.org/abs/2307.07868</link>
<description rdf:parseType="Literal">&lt;p&gt;Our research aims to find the best model that uses companies projections and
sector performances and how the given company fares accordingly to correctly
predict equity share prices for both short and long term goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Sangwan_V/0/1/0/all/0/1&quot;&gt;Varun Sangwan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Singh_V/0/1/0/all/0/1&quot;&gt;Vishesh Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+V_B/0/1/0/all/0/1&quot;&gt;Bibin Christopher V&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07869">
<title>Custom DNN using Reward Modulated Inverted STDP Learning for Temporal Pattern Recognition. (arXiv:2307.07869v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.07869</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal spike recognition plays a crucial role in various domains, including
anomaly detection, keyword spotting and neuroscience. This paper presents a
novel algorithm for efficient temporal spike pattern recognition on sparse
event series data. The algorithm leverages a combination of reward-modulatory
behavior, Hebbian and anti-Hebbian based learning methods to identify patterns
in dynamic datasets with short intervals of training. The algorithm begins with
a preprocessing step, where the input data is rationalized and translated to a
feature-rich yet sparse spike time series data. Next, a linear feed forward
spiking neural network processes this data to identify a trained pattern.
Finally, the next layer performs a weighted check to ensure the correct pattern
has been detected.To evaluate the performance of the proposed algorithm, it was
trained on a complex dataset containing spoken digits with spike information
and its output compared to state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vivekanand_V/0/1/0/all/0/1&quot;&gt;Vijay Shankaran Vivekanand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kubendran_R/0/1/0/all/0/1&quot;&gt;Rajkumar Kubendran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07870">
<title>Large Language Models as Superpositions of Cultural Perspectives. (arXiv:2307.07870v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07870</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are often misleadingly recognized as having a
personality or a set of values. We argue that an LLM can be seen as a
superposition of perspectives with different values and personality traits.
LLMs exhibit context-dependent values and personality traits that change based
on the induced perspective (as opposed to humans, who tend to have more
coherent values and personality traits across contexts). We introduce the
concept of perspective controllability, which refers to a model&apos;s affordance to
adopt various perspectives with differing values and personality traits. In our
experiments, we use questionnaires from psychology (PVQ, VSM, IPIP) to study
how exhibited values and personality traits change based on different
perspectives. Through qualitative experiments, we show that LLMs express
different values when those are (implicitly or explicitly) implied in the
prompt, and that LLMs express different values even when those are not
obviously implied (demonstrating their context-dependent nature). We then
conduct quantitative experiments to study the controllability of different
models (GPT-4, GPT-3.5, OpenAssistant, StableVicuna, StableLM), the
effectiveness of various methods for inducing perspectives, and the smoothness
of the models&apos; drivability. We conclude by examining the broader implications
of our work and outline a variety of associated scientific questions. The
project website is available at
https://sites.google.com/view/llm-superpositions .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1&quot;&gt;Grgur Kova&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawayama_M/0/1/0/all/0/1&quot;&gt;Masataka Sawayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Portelas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Colas_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Colas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1&quot;&gt;Peter Ford Dominey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07871">
<title>The SocialAI School: Insights from Developmental Psychology Towards Artificial Socio-Cultural Agents. (arXiv:2307.07871v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;Developmental psychologists have long-established the importance of
socio-cognitive abilities in human intelligence. These abilities enable us to
enter, participate and benefit from human culture. AI research on social
interactive agents mostly concerns the emergence of culture in a multi-agent
setting (often without a strong grounding in developmental psychology). We
argue that AI research should be informed by psychology and study
socio-cognitive abilities enabling to enter a culture too. We discuss the
theories of Michael Tomasello and Jerome Bruner to introduce some of their
concepts to AI and outline key concepts and socio-cognitive abilities. We
present The SocialAI school - a tool including a customizable parameterized
uite of procedurally generated environments, which simplifies conducting
experiments regarding those concepts. We show examples of such experiments with
RL agents and Large Language Models. The main motivation of this work is to
engage the AI community around the problem of social intelligence informed by
developmental psychology, and to provide a tool to simplify first steps in this
direction. Refer to the project website for code and additional information:
https://sites.google.com/view/socialai-school.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovac_G/0/1/0/all/0/1&quot;&gt;Grgur Kova&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Portelas_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;my Portelas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominey_P/0/1/0/all/0/1&quot;&gt;Peter Ford Dominey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oudeyer_P/0/1/0/all/0/1&quot;&gt;Pierre-Yves Oudeyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07872">
<title>Does Double Descent Occur in Self-Supervised Learning?. (arXiv:2307.07872v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07872</link>
<description rdf:parseType="Literal">&lt;p&gt;Most investigations into double descent have focused on supervised models
while the few works studying self-supervised settings find a surprising lack of
the phenomenon. These results imply that double descent may not exist in
self-supervised models. We show this empirically using a standard and linear
autoencoder, two previously unstudied settings. The test loss is found to have
either a classical U-shape or to monotonically decrease instead of exhibiting a
double-descent curve. We hope that further work on this will help elucidate the
theoretical underpinnings of this phenomenon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lupidi_A/0/1/0/all/0/1&quot;&gt;Alisia Lupidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gideoni_Y/0/1/0/all/0/1&quot;&gt;Yonatan Gideoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayalath_D/0/1/0/all/0/1&quot;&gt;Dulhan Jayalath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07873">
<title>Towards Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07873</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yechao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengshan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Junyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaogeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Wei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07881">
<title>Graph Embedded Intuitionistic Fuzzy RVFL for Class Imbalance Learning. (arXiv:2307.07881v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07881</link>
<description rdf:parseType="Literal">&lt;p&gt;The domain of machine learning is confronted with a crucial research area
known as class imbalance learning, which presents considerable hurdles in the
precise classification of minority classes. This issue can result in biased
models where the majority class takes precedence in the training process,
leading to the underrepresentation of the minority class. The random vector
functional link (RVFL) network is a widely-used and effective learning model
for classification due to its speed and efficiency. However, it suffers from
low accuracy when dealing with imbalanced datasets. To overcome this
limitation, we propose a novel graph embedded intuitionistic fuzzy RVFL for
class imbalance learning (GE-IFRVFL-CIL) model incorporating a weighting
mechanism to handle imbalanced datasets. The proposed GE-IFRVFL-CIL model has a
plethora of benefits, such as $(i)$ it leverages graph embedding to extract
semantically rich information from the dataset, $(ii)$ it uses intuitionistic
fuzzy sets to handle uncertainty and imprecision in the data, $(iii)$ and the
most important, it tackles class imbalance learning. The amalgamation of a
weighting scheme, graph embedding, and intuitionistic fuzzy sets leads to the
superior performance of the proposed model on various benchmark imbalanced
datasets, including UCI and KEEL. Furthermore, we implement the proposed
GE-IFRVFL-CIL on the ADNI dataset and achieved promising results, demonstrating
the model&apos;s effectiveness in real-world applications. The proposed method
provides a promising solution for handling class imbalance in machine learning
and has the potential to be applied to other classification problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganaie_M/0/1/0/all/0/1&quot;&gt;M.A. Ganaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajid_M/0/1/0/all/0/1&quot;&gt;M. Sajid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1&quot;&gt;A.K. Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanveer_M/0/1/0/all/0/1&quot;&gt;M. Tanveer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07882">
<title>Gradient-free training of neural ODEs for system identification and control using ensemble Kalman inversion. (arXiv:2307.07882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07882</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensemble Kalman inversion (EKI) is a sequential Monte Carlo method used to
solve inverse problems within a Bayesian framework. Unlike backpropagation, EKI
is a gradient-free optimization method that only necessitates the evaluation of
artificial neural networks in forward passes. In this study, we examine the
effectiveness of EKI in training neural ordinary differential equations (neural
ODEs) for system identification and control tasks. To apply EKI to optimal
control problems, we formulate inverse problems that incorporate a
Tikhonov-type regularization term. Our numerical results demonstrate that EKI
is an efficient method for training neural ODEs in system identification and
optimal control problems, with runtime and quality of solutions that are
competitive with commonly used gradient-based optimizers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottcher_L/0/1/0/all/0/1&quot;&gt;Lucas B&amp;#xf6;ttcher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07887">
<title>Handwritten and Printed Text Segmentation: A Signature Case Study. (arXiv:2307.07887v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07887</link>
<description rdf:parseType="Literal">&lt;p&gt;While analyzing scanned documents, handwritten text can overlay printed text.
This causes difficulties during the optical character recognition (OCR) and
digitization process of documents, and subsequently, hurts downstream NLP
tasks. Prior research either focuses only on the binary classification of
handwritten text, or performs a three-class segmentation of the document, i.e.,
recognition of handwritten, printed, and background pixels. This results in the
assignment of the handwritten and printed overlapping pixels to only one of the
classes, and thus, they are not accounted for in the other class. Thus, in this
research, we develop novel approaches for addressing the challenges of
handwritten and printed text segmentation with the goal of recovering text in
different classes in whole, especially improving the segmentation performance
on the overlapping parts. As such, to facilitate with this task, we introduce a
new dataset, SignaTR6K, collected from real legal documents, as well as a new
model architecture for handwritten and printed text segmentation task. Our best
configuration outperforms the prior work on two different datasets by 17.9% and
7.3% on IoU scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholamian_S/0/1/0/all/0/1&quot;&gt;Sina Gholamian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vahdat_A/0/1/0/all/0/1&quot;&gt;Ali Vahdat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07892">
<title>Multitemporal SAR images change detection and visualization using RABASAR and simplified GLR. (arXiv:2307.07892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07892</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the state of changed areas requires that precise information be
given about the changes. Thus, detecting different kinds of changes is
important for land surface monitoring. SAR sensors are ideal to fulfil this
task, because of their all-time and all-weather capabilities, with good
accuracy of the acquisition geometry and without effects of atmospheric
constituents for amplitude data. In this study, we propose a simplified
generalized likelihood ratio ($S_{GLR}$) method assuming that corresponding
temporal pixels have the same equivalent number of looks (ENL). Thanks to the
denoised data provided by a ratio-based multitemporal SAR image denoising
method (RABASAR), we successfully applied this similarity test approach to
compute the change areas. A new change magnitude index method and an improved
spectral clustering-based change classification method are also developed. In
addition, we apply the simplified generalized likelihood ratio to detect the
maximum change magnitude time, and the change starting and ending times. Then,
we propose to use an adaptation of the REACTIV method to visualize the
detection results vividly. The effectiveness of the proposed methods is
demonstrated through the processing of simulated and SAR images, and the
comparison with classical techniques. In particular, numerical experiments
proved that the developed method has good performances in detecting farmland
area changes, building area changes, harbour area changes and flooding area
changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiying Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deledalle_C/0/1/0/all/0/1&quot;&gt;Charles-Alban Deledalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denis_L/0/1/0/all/0/1&quot;&gt;Lo&amp;#xef;c Denis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maitre_H/0/1/0/all/0/1&quot;&gt;Henri Ma&amp;#xee;tre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolas_J/0/1/0/all/0/1&quot;&gt;Jean-Marie Nicolas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tupin_F/0/1/0/all/0/1&quot;&gt;Florence Tupin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07893">
<title>Anomaly Detection in Automated Fibre Placement: Learning with Data Limitations. (arXiv:2307.07893v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07893</link>
<description rdf:parseType="Literal">&lt;p&gt;Current defect detection systems for Automated Fibre Placement (AFP) are
mostly based on end-to-end supervised learning methods requiring abundant
labelled defective samples, which are not easily generated in sufficient
numbers. To address this data scarcity problem, we introduce an
autoencoder-based approach compatible with small datasets. Fortunately, the
problem from a foundational point of view can be simplified as a binary
classification between normal and abnormal samples. The proposed approach uses
a depth map of the fibre layup surface, split into small windows aligned to
each composite strip (tow). A subset of these windows that do not contain
anomalies is passed to an autoencoder to reconstruct the input. Because the
autoencoder is trained with normal samples, it produces more accurate
reconstructions for these samples than for abnormal ones. Therefore, the value
of reconstruction error is used as a quantitative metric for whether there are
potential anomalies. These values are combined to produce an anomaly map, which
can localize the manufacturing defects in the depth map. The results show that
although the autoencoder is trained with a very limited number of scans, the
proposed approach can produce sufficient binary classification accuracy and
specify the location of the defects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_A/0/1/0/all/0/1&quot;&gt;Assef Ghamisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charter_T/0/1/0/all/0/1&quot;&gt;Todd Charter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Li Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivard_M/0/1/0/all/0/1&quot;&gt;Maxime Rivard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lund_G/0/1/0/all/0/1&quot;&gt;Gil Lund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najjaran_H/0/1/0/all/0/1&quot;&gt;Homayoun Najjaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07907">
<title>Seeing is not Believing: Robust Reinforcement Learning against Spurious Correlation. (arXiv:2307.07907v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07907</link>
<description rdf:parseType="Literal">&lt;p&gt;Robustness has been extensively studied in reinforcement learning (RL) to
handle various forms of uncertainty such as random perturbations, rare events,
and malicious attacks. In this work, we consider one critical type of
robustness against spurious correlation, where different portions of the state
do not have causality but have correlations induced by unobserved confounders.
These spurious correlations are ubiquitous in real-world tasks, for instance, a
self-driving car usually observes heavy traffic in the daytime and light
traffic at night due to unobservable human activity. A model that learns such
useless or even harmful correlation could catastrophically fail when the
confounder in the test case deviates from the training one. Although motivated,
enabling robustness against spurious correlation poses significant challenges
since the uncertainty set, shaped by the unobserved confounder and sequential
structure of RL, is difficult to characterize and identify. Existing robust
algorithms that assume simple and unstructured uncertainty sets are therefore
inadequate to address this challenge. To solve this issue, we propose Robust
State-Confounded Markov Decision Processes (RSC-MDPs) and theoretically
demonstrate its superiority in breaking spurious correlations compared with
other robust RL counterparts. We also design an empirical algorithm to learn
the robust optimal policy for RSC-MDPs, which outperforms all baselines in
eight realistic self-driving and manipulation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_W/0/1/0/all/0/1&quot;&gt;Wenhao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Laixi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yuejie Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07911">
<title>MESOB: Balancing Equilibria &amp; Social Optimality. (arXiv:2307.07911v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2307.07911</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by bid recommendation in online ad auctions, this paper considers a
general class of multi-level and multi-agent games, with two major
characteristics: one is a large number of anonymous agents, and the other is
the intricate interplay between competition and cooperation. To model such
complex systems, we propose a novel and tractable bi-objective optimization
formulation with mean-field approximation, called MESOB (Mean-field Equilibria
&amp;amp; Social Optimality Balancing), as well as an associated occupation measure
optimization (OMO) method called MESOB-OMO to solve it. MESOB-OMO enables
obtaining approximately Pareto efficient solutions in terms of the dual
objectives of competition and cooperation in MESOB, and in particular allows
for Nash equilibrium selection and social equalization in an asymptotic manner.
We apply MESOB-OMO to bid recommendation in a simulated pay-per-click ad
auction. Experiments demonstrate its efficacy in balancing the interests of
different parties and in handling the competitive nature of bidders, as well as
its advantages over baselines that only consider either the competitive or the
cooperative aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lihong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabi_S/0/1/0/all/0/1&quot;&gt;Sareh Nabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salhab_R/0/1/0/all/0/1&quot;&gt;Rabih Salhab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junzi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07912">
<title>Predicting mechanical properties of Carbon Nanotube (CNT) images Using Multi-Layer Synthetic Finite Element Model Simulations. (arXiv:2307.07912v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07912</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a pipeline for predicting mechanical properties of
vertically-oriented carbon nanotube (CNT) forest images using a deep learning
model for artificial intelligence (AI)-based materials discovery. Our approach
incorporates an innovative data augmentation technique that involves the use of
multi-layer synthetic (MLS) or quasi-2.5D images which are generated by
blending 2D synthetic images. The MLS images more closely resemble 3D synthetic
and real scanning electron microscopy (SEM) images of CNTs but without the
computational cost of performing expensive 3D simulations or experiments.
Mechanical properties such as stiffness and buckling load for the MLS images
are estimated using a physics-based model. The proposed deep learning
architecture, CNTNeXt, builds upon our previous CNTNet neural network, using a
ResNeXt feature representation followed by random forest regression estimator.
Our machine learning approach for predicting CNT physical properties by
utilizing a blended set of synthetic images is expected to outperform single
synthetic image-based learning when it comes to predicting mechanical
properties of real scanning electron microscopy images. This has the potential
to accelerate understanding and control of CNT forest self-assembly for diverse
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safavigerdini_K/0/1/0/all/0/1&quot;&gt;Kaveh Safavigerdini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nouduri_K/0/1/0/all/0/1&quot;&gt;Koundinya Nouduri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surya_R/0/1/0/all/0/1&quot;&gt;Ramakrishna Surya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reinhard_A/0/1/0/all/0/1&quot;&gt;Andrew Reinhard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinlan_Z/0/1/0/all/0/1&quot;&gt;Zach Quinlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunyak_F/0/1/0/all/0/1&quot;&gt;Filiz Bunyak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maschmann_M/0/1/0/all/0/1&quot;&gt;Matthew R. Maschmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniappan_K/0/1/0/all/0/1&quot;&gt;Kannappan Palaniappan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07914">
<title>Exploiting FPGA Capabilities for Accelerated Biomedical Computing. (arXiv:2307.07914v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2307.07914</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents advanced neural network architectures including
Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN), Long
Short-Term Memory Networks (LSTMs), and Deep Belief Networks (DBNs) for
enhanced ECG signal analysis using Field Programmable Gate Arrays (FPGAs). We
utilize the MIT-BIH Arrhythmia Database for training and validation,
introducing Gaussian noise to improve algorithm robustness. The implemented
models feature various layers for distinct processing and classification tasks
and techniques like EarlyStopping callback and Dropout layer are used to
mitigate overfitting. Our work also explores the development of a custom Tensor
Compute Unit (TCU) accelerator for the PYNQ Z1 board, offering comprehensive
steps for FPGA-based machine learning, including setting up the Tensil
toolchain in Docker, selecting architecture, configuring PS-PL, and compiling
and executing models. Performance metrics such as latency and throughput are
calculated for practical insights, demonstrating the potential of FPGAs in
high-performance biomedical computing. The study ultimately offers a guide for
optimizing neural network performance on FPGAs for various applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inadagbo_K/0/1/0/all/0/1&quot;&gt;Kayode Inadagbo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arig_B/0/1/0/all/0/1&quot;&gt;Baran Arig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alici_N/0/1/0/all/0/1&quot;&gt;Nisanur Alici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isik_M/0/1/0/all/0/1&quot;&gt;Murat Isik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07916">
<title>On the Robustness of Split Learning against Adversarial Attacks. (arXiv:2307.07916v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07916</link>
<description rdf:parseType="Literal">&lt;p&gt;Split learning enables collaborative deep learning model training while
preserving data privacy and model security by avoiding direct sharing of raw
data and model details (i.e., sever and clients only hold partial sub-networks
and exchange intermediate computations). However, existing research has mainly
focused on examining its reliability for privacy protection, with little
investigation into model security. Specifically, by exploring full models,
attackers can launch adversarial attacks, and split learning can mitigate this
severe threat by only disclosing part of models to untrusted servers.This paper
aims to evaluate the robustness of split learning against adversarial attacks,
particularly in the most challenging setting where untrusted servers only have
access to the intermediate layers of the model.Existing adversarial attacks
mostly focus on the centralized setting instead of the collaborative setting,
thus, to better evaluate the robustness of split learning, we develop a
tailored attack called SPADV, which comprises two stages: 1) shadow model
training that addresses the issue of lacking part of the model and 2) local
adversarial attack that produces adversarial examples to evaluate.The first
stage only requires a few unlabeled non-IID data, and, in the second stage,
SPADV perturbs the intermediate output of natural samples to craft the
adversarial ones. The overall cost of the proposed attack process is relatively
low, yet the empirical attack effectiveness is significantly high,
demonstrating the surprising vulnerability of split learning to adversarial
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenmeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07932">
<title>A Novel Truncated Norm Regularization Method for Multi-channel Color Image Denoising. (arXiv:2307.07932v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.07932</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the high flexibility and remarkable performance, low-rank
approximation methods has been widely studied for color image denoising.
However, those methods mostly ignore either the cross-channel difference or the
spatial variation of noise, which limits their capacity in real world color
image denoising. To overcome those drawbacks, this paper is proposed to denoise
color images with a double-weighted truncated nuclear norm minus truncated
Frobenius norm minimization (DtNFM) method. Through exploiting the nonlocal
self-similarity of the noisy image, the similar structures are gathered and a
series of similar patch matrices are constructed. For each group, the DtNFM
model is conducted for estimating its denoised version. The denoised image
would be obtained by concatenating all the denoised patch matrices. The
proposed DtNFM model has two merits. First, it models and utilizes both the
cross-channel difference and the spatial variation of noise. This provides
sufficient flexibility for handling the complex distribution of noise in real
world images. Second, the proposed DtNFM model provides a close approximation
to the underlying clean matrix since it can treat different rank components
flexibly. To solve the problem resulted from DtNFM model, an accurate and
effective algorithm is proposed by exploiting the framework of the alternating
direction method of multipliers (ADMM). The generated subproblems are discussed
in detail. And their global optima can be easily obtained in closed-form.
Rigorous mathematical derivation proves that the solution sequences generated
by the algorithm converge to a single critical point. Extensive experiments on
synthetic and real noise datasets demonstrate that the proposed method
outperforms many state-of-the-art color image denoising methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Yiwen Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Haoming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chunming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07941">
<title>Optimal Compression of Unit Norm Vectors in the High Distortion Regime. (arXiv:2307.07941v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2307.07941</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the need for communication-efficient distributed learning, we
investigate the method for compressing a unit norm vector into the minimum
number of bits, while still allowing for some acceptable level of distortion in
recovery. This problem has been explored in the rate-distortion/covering code
literature, but our focus is exclusively on the &quot;high-distortion&quot; regime. We
approach this problem in a worst-case scenario, without any prior information
on the vector, but allowing for the use of randomized compression maps. Our
study considers both biased and unbiased compression methods and determines the
optimal compression rates. It turns out that simple compression schemes are
nearly optimal in this scenario. While the results are a mix of new and known,
they are compiled in this paper for completeness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Heng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Avishek Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumdar_A/0/1/0/all/0/1&quot;&gt;Arya Mazumdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07942">
<title>KECOR: Kernel Coding Rate Maximization for Active 3D Object Detection. (arXiv:2307.07942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07942</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving a reliable LiDAR-based object detector in autonomous driving is
paramount, but its success hinges on obtaining large amounts of precise 3D
annotations. Active learning (AL) seeks to mitigate the annotation burden
through algorithms that use fewer labels and can attain performance comparable
to fully supervised learning. Although AL has shown promise, current approaches
prioritize the selection of unlabeled point clouds with high uncertainty and/or
diversity, leading to the selection of more instances for labeling and reduced
computational efficiency. In this paper, we resort to a novel kernel coding
rate maximization (KECOR) strategy which aims to identify the most informative
point clouds to acquire labels through the lens of information theory. Greedy
search is applied to seek desired point clouds that can maximize the minimal
number of bits required to encode the latent features. To determine the
uniqueness and informativeness of the selected samples from the model
perspective, we construct a proxy network of the 3D detector head and compute
the outer product of Jacobians from all proxy layers to form the empirical
neural tangent kernel (NTK) matrix. To accommodate both one-stage (i.e.,
SECOND) and two-stage detectors (i.e., PVRCNN), we further incorporate the
classification entropy maximization and well trade-off between detection
performance and the total number of bounding boxes selected for annotation.
Extensive experiments conducted on two 3D benchmarks and a 2D detection dataset
evidence the superiority and versatility of the proposed approach. Our results
show that approximately 44% box-level annotation costs and 26% computational
time are reduced compared to the state-of-the-art AL method, without
compromising detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1&quot;&gt;Mahsa Baktashmotlagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07944">
<title>Revisiting Domain-Adaptive 3D Object Detection by Reliable, Diverse and Class-balanced Pseudo-Labeling. (arXiv:2307.07944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (DA) with the aid of pseudo labeling
techniques has emerged as a crucial approach for domain-adaptive 3D object
detection. While effective, existing DA methods suffer from a substantial drop
in performance when applied to a multi-class training setting, due to the
co-existence of low-quality pseudo labels and class imbalance issues. In this
paper, we address this challenge by proposing a novel ReDB framework tailored
for learning to detect all classes at once. Our approach produces Reliable,
Diverse, and class-Balanced pseudo 3D boxes to iteratively guide the
self-training on a distributionally different target domain. To alleviate
disruptions caused by the environmental discrepancy (e.g., beam numbers), the
proposed cross-domain examination (CDE) assesses the correctness of pseudo
labels by copy-pasting target instances into a source environment and measuring
the prediction consistency. To reduce computational overhead and mitigate the
object shift (e.g., scales and point densities), we design an overlapped boxes
counting (OBC) metric that allows to uniformly downsample pseudo-labeled
objects across different geometric characteristics. To confront the issue of
inter-class imbalance, we progressively augment the target point clouds with a
class-balanced set of pseudo-labeled target instances and source objects, which
boosts recognition accuracies on both frequently appearing and rare classes.
Experimental results on three benchmark datasets using both voxel-based (i.e.,
SECOND) and point-based 3D detectors (i.e., PointRCNN) demonstrate that our
proposed ReDB approach outperforms existing 3D domain adaptation methods by a
large margin, improving 23.15% mAP on the nuScenes $\rightarrow$ KITTI task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoxiao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yadan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baktashmotlagh_M/0/1/0/all/0/1&quot;&gt;Mahsa Baktashmotlagh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07950">
<title>Accelerating Distributed ML Training via Selective Synchronization. (arXiv:2307.07950v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.07950</link>
<description rdf:parseType="Literal">&lt;p&gt;In distributed training, deep neural networks (DNNs) are launched over
multiple workers concurrently and aggregate their local updates on each step in
bulk-synchronous parallel (BSP) training. However, BSP does not linearly
scale-out due to high communication cost of aggregation. To mitigate this
overhead, alternatives like Federated Averaging (FedAvg) and Stale-Synchronous
Parallel (SSP) either reduce synchronization frequency or eliminate it
altogether, usually at the cost of lower final accuracy. In this paper, we
present \texttt{SelSync}, a practical, low-overhead method for DNN training
that dynamically chooses to incur or avoid communication at each step either by
calling the aggregation op or applying local updates based on their
significance. We propose various optimizations as part of \texttt{SelSync} to
improve convergence in the context of \textit{semi-synchronous} training. Our
system converges to the same or better accuracy than BSP while reducing
training time by up to 14$\times$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_S/0/1/0/all/0/1&quot;&gt;Sahil Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swany_M/0/1/0/all/0/1&quot;&gt;Martin Swany&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07956">
<title>Automated Polynomial Filter Learning for Graph Neural Networks. (arXiv:2307.07956v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07956</link>
<description rdf:parseType="Literal">&lt;p&gt;Polynomial graph filters have been widely used as guiding principles in the
design of Graph Neural Networks (GNNs). Recently, the adaptive learning of the
polynomial graph filters has demonstrated promising performance for modeling
graph signals on both homophilic and heterophilic graphs, owning to their
flexibility and expressiveness. In this work, we conduct a novel preliminary
study to explore the potential and limitations of polynomial graph filter
learning approaches, revealing a severe overfitting issue. To improve the
effectiveness of polynomial graph filters, we propose Auto-Polynomial, a novel
and general automated polynomial graph filter learning framework that
efficiently learns better filters capable of adapting to various complex graph
signals. Comprehensive experiments and ablation studies demonstrate significant
and consistent performance improvements on both homophilic and heterophilic
graphs across multiple learning settings considering various labeling ratios,
which unleashes the potential of polynomial filter learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wendi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Z/0/1/0/all/0/1&quot;&gt;Zhichao Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaorui Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07963">
<title>Enhancing Energy Efficiency and Reliability in Autonomous Systems Estimation using Neuromorphic Approach. (arXiv:2307.07963v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07963</link>
<description rdf:parseType="Literal">&lt;p&gt;Energy efficiency and reliability have long been crucial factors for ensuring
cost-effective and safe missions in autonomous systems computers. With the
rapid evolution of industries such as space robotics and advanced air mobility,
the demand for these low size, weight, and power (SWaP) computers has grown
significantly. This study focuses on introducing an estimation framework based
on spike coding theories and spiking neural networks (SNN), leveraging the
efficiency and scalability of neuromorphic computers. Therefore, we propose an
SNN-based Kalman filter (KF), a fundamental and widely adopted optimal strategy
for well-defined linear systems. Furthermore, based on the modified sliding
innovation filter (MSIF) we present a robust strategy called SNN-MSIF. Notably,
the weight matrices of the networks are designed according to the system model,
eliminating the need for learning. To evaluate the effectiveness of the
proposed strategies, we compare them to their algorithmic counterparts, namely
the KF and the MSIF, using Monte Carlo simulations. Additionally, we assess the
robustness of SNN-MSIF by comparing it to SNN-KF in the presence of modeling
uncertainties and neuron loss. Our results demonstrate the applicability of the
proposed methods and highlight the superior performance of SNN-MSIF in terms of
accuracy and robustness. Furthermore, the spiking pattern observed from the
networks serves as evidence of the energy efficiency achieved by the proposed
methods, as they exhibited an impressive reduction of approximately 97 percent
in emitted spikes compared to possible spikes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadvand_R/0/1/0/all/0/1&quot;&gt;Reza Ahmadvand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharif_S/0/1/0/all/0/1&quot;&gt;Sarah Safura Sharif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banad_Y/0/1/0/all/0/1&quot;&gt;Yaser Mike Banad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07973">
<title>Heteroscedastic Causal Structure Learning. (arXiv:2307.07973v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07973</link>
<description rdf:parseType="Literal">&lt;p&gt;Heretofore, learning the directed acyclic graphs (DAGs) that encode the
cause-effect relationships embedded in observational data is a computationally
challenging problem. A recent trend of studies has shown that it is possible to
recover the DAGs with polynomial time complexity under the equal variances
assumption. However, this prohibits the heteroscedasticity of the noise, which
allows for more flexible modeling capabilities, but at the same time is
substantially more challenging to handle. In this study, we tackle the
heteroscedastic causal structure learning problem under Gaussian noises. By
exploiting the normality of the causal mechanisms, we can recover a valid
causal ordering, which can uniquely identify the causal DAG using a series of
conditional independence tests. The result is HOST (Heteroscedastic causal
STructure learning), a simple yet effective causal structure learning algorithm
that scales polynomially in both sample size and dimensionality. In addition,
via extensive empirical evaluations on a wide range of both controlled and real
datasets, we show that the proposed HOST method is competitive with
state-of-the-art approaches in both the causal order learning and structure
learning problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duong_B/0/1/0/all/0/1&quot;&gt;Bao Duong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thin Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07975">
<title>Finite element inspired networks: Learning physically-plausible deformable object dynamics from partial observations. (arXiv:2307.07975v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07975</link>
<description rdf:parseType="Literal">&lt;p&gt;The accurate simulation of deformable linear object (DLO) dynamics is
challenging if the task at hand requires a human-interpretable and
data-efficient model that also yields fast predictions. To arrive at such
model, we draw inspiration from the rigid finite element method (R-FEM) and
model a DLO as a serial chain of rigid bodies whose internal state is unrolled
through time by a dynamics network. As this state is not observed directly, the
dynamics network is trained jointly with a physics-informed encoder mapping
observed motion variables to the body chain&apos;s state. To encourage that the
state acquires a physically meaningful representation, we leverage the forward
kinematics (FK) of the underlying R-FEM model as a decoder. We demonstrate in a
robot experiment that this architecture - being termed &quot;Finite element inspired
network&quot; - forms an easy to handle, yet capable DLO dynamics model yielding
physically interpretable predictions from partial observations.
&lt;/p&gt;
&lt;p&gt;The project code is available at: \url{https://tinyurl.com/fei-networks}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mamedov_S/0/1/0/all/0/1&quot;&gt;Shamil Mamedov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geist_A/0/1/0/all/0/1&quot;&gt;A. Ren&amp;#xe9; Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swevers_J/0/1/0/all/0/1&quot;&gt;Jan Swevers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07980">
<title>Byzantine-Robust Distributed Online Learning: Taming Adversarial Participants in An Adversarial Environment. (arXiv:2307.07980v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07980</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies distributed online learning under Byzantine attacks. The
performance of an online learning algorithm is often characterized by
(adversarial) regret, which evaluates the quality of one-step-ahead
decision-making when an environment provides adversarial losses, and a
sublinear bound is preferred. But we prove that, even with a class of
state-of-the-art robust aggregation rules, in an adversarial environment and in
the presence of Byzantine participants, distributed online gradient descent can
only achieve a linear adversarial regret bound, which is tight. This is the
inevitable consequence of Byzantine attacks, even though we can control the
constant of the linear adversarial regret to a reasonable level. Interestingly,
when the environment is not fully adversarial so that the losses of the honest
participants are i.i.d. (independent and identically distributed), we show that
sublinear stochastic regret, in contrast to the aforementioned adversarial
regret, is possible. We develop a Byzantine-robust distributed online momentum
algorithm to attain such a sublinear stochastic regret bound. Extensive
numerical experiments corroborate our theoretical analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xingrong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhaoxian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Q/0/1/0/all/0/1&quot;&gt;Qing Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhi Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07982">
<title>A Survey of Techniques for Optimizing Transformer Inference. (arXiv:2307.07982v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07982</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen a phenomenal rise in performance and applications of
transformer neural networks. The family of transformer networks, including
Bidirectional Encoder Representations from Transformer (BERT), Generative
Pretrained Transformer (GPT) and Vision Transformer (ViT), have shown their
effectiveness across Natural Language Processing (NLP) and Computer Vision (CV)
domains. Transformer-based networks such as ChatGPT have impacted the lives of
common men. However, the quest for high predictive performance has led to an
exponential increase in transformers&apos; memory and compute footprint. Researchers
have proposed techniques to optimize transformer inference at all levels of
abstraction. This paper presents a comprehensive survey of techniques for
optimizing the inference phase of transformer networks. We survey techniques
such as knowledge distillation, pruning, quantization, neural architecture
search and lightweight network design at the algorithmic level. We further
review hardware-level optimization techniques and the design of novel hardware
accelerators for transformers. We summarize the quantitative results on the
number of parameters/FLOPs and accuracy of several models/techniques to
showcase the tradeoff exercised by them. We also outline future directions in
this rapidly evolving field of research. We believe that this survey will
educate both novice and seasoned researchers and also spark a plethora of
research efforts in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitty_Venkata_K/0/1/0/all/0/1&quot;&gt;Krishna Teja Chitty-Venkata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sparsh Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emani_M/0/1/0/all/0/1&quot;&gt;Murali Emani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwanath_V/0/1/0/all/0/1&quot;&gt;Venkatram Vishwanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somani_A/0/1/0/all/0/1&quot;&gt;Arun K. Somani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07997">
<title>MargCTGAN: A &quot;Marginally&apos;&apos; Better CTGAN for the Low Sample Regime. (arXiv:2307.07997v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07997</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential of realistic and useful synthetic data is significant. However,
current evaluation methods for synthetic tabular data generation predominantly
focus on downstream task usefulness, often neglecting the importance of
statistical properties. This oversight becomes particularly prominent in low
sample scenarios, accompanied by a swift deterioration of these statistical
measures. In this paper, we address this issue by conducting an evaluation of
three state-of-the-art synthetic tabular data generators based on their
marginal distribution, column-pair correlation, joint distribution and
downstream task utility performance across high to low sample regimes. The
popular CTGAN model shows strong utility, but underperforms in low sample
settings in terms of utility. To overcome this limitation, we propose MargCTGAN
that adds feature matching of de-correlated marginals, which results in a
consistent improvement in downstream utility as well as statistical properties
of the synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afonja_T/0/1/0/all/0/1&quot;&gt;Tejumade Afonja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dingfan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritz_M/0/1/0/all/0/1&quot;&gt;Mario Fritz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07998">
<title>LUCYD: A Feature-Driven Richardson-Lucy Deconvolution Network. (arXiv:2307.07998v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07998</link>
<description rdf:parseType="Literal">&lt;p&gt;The process of acquiring microscopic images in life sciences often results in
image degradation and corruption, characterised by the presence of noise and
blur, which poses significant challenges in accurately analysing and
interpreting the obtained data. This paper proposes LUCYD, a novel method for
the restoration of volumetric microscopy images that combines the
Richardson-Lucy deconvolution formula and the fusion of deep features obtained
by a fully convolutional network. By integrating the image formation process
into a feature-driven restoration model, the proposed approach aims to enhance
the quality of the restored images whilst reducing computational costs and
maintaining a high degree of interpretability. Our results demonstrate that
LUCYD outperforms the state-of-the-art methods in both synthetic and real
microscopy images, achieving superior performance in terms of image quality and
generalisability. We show that the model can handle various microscopy
modalities and different imaging conditions by evaluating it on two different
microscopy datasets, including volumetric widefield and light-sheet microscopy.
Our experiments indicate that LUCYD can significantly improve resolution,
contrast, and overall quality of microscopy images. Therefore, it can be a
valuable tool for microscopy image restoration and can facilitate further
research in various microscopy applications. We made the source code for the
model accessible under https://github.com/ctom2/lucyd-deconvolution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chobola_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Chobola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_G/0/1/0/all/0/1&quot;&gt;Gesine M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dausmann_V/0/1/0/all/0/1&quot;&gt;Veit Dausmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theileis_A/0/1/0/all/0/1&quot;&gt;Anton Theileis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taucher_J/0/1/0/all/0/1&quot;&gt;Jan Taucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huisken_J/0/1/0/all/0/1&quot;&gt;Jan Huisken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08004">
<title>For One-Shot Decoding: Unsupervised Deep Learning-Based Polar Decoder. (arXiv:2307.08004v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2307.08004</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an unsupervised deep learning-based decoding scheme that enables
one-shot decoding of polar codes. In the proposed scheme, rather than using the
information bit vectors as labels for training the neural network (NN) through
supervised learning as the conventional scheme did, the NN is trained to
function as a bounded distance decoder by leveraging the generator matrix of
polar codes through self-supervised learning. This approach eliminates the
reliance on predefined labels, empowering the potential to train directly on
the actual data within communication systems and thereby enhancing the
applicability. Furthermore, computer simulations demonstrate that (i) the bit
error rate (BER) and block error rate (BLER) performances of the proposed
scheme can approach those of the maximum a posteriori (MAP) decoder for very
short packets and (ii) the proposed NN decoder exhibits much superior
generalization ability compared to the conventional one.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Huiying Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yihao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukuzawa_Y/0/1/0/all/0/1&quot;&gt;Yuma Fukuzawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08013">
<title>Revisiting Implicit Models: Sparsity Trade-offs Capability in Weight-tied Model for Vision Tasks. (arXiv:2307.08013v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08013</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit models such as Deep Equilibrium Models (DEQs) have garnered
significant attention in the community for their ability to train infinite
layer models with elegant solution-finding procedures and constant memory
footprint. However, despite several attempts, these methods are heavily
constrained by model inefficiency and optimization instability. Furthermore,
fair benchmarking across relevant methods for vision tasks is missing. In this
work, we revisit the line of implicit models and trace them back to the
original weight-tied models. Surprisingly, we observe that weight-tied models
are more effective, stable, as well as efficient on vision tasks, compared to
the DEQ variants. Through the lens of these simple-yet-clean weight-tied
models, we further study the fundamental limits in the model capacity of such
models and propose the use of distinct sparse masks to improve the model
capacity. Finally, for practitioners, we offer design guidelines regarding the
depth, width, and sparsity selection for weight-tied models, and demonstrate
the generalizability of our insights to other learning paradigms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Haobo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1&quot;&gt;Soumajit Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08029">
<title>Noise-aware Speech Enhancement using Diffusion Probabilistic Model. (arXiv:2307.08029v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.08029</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent advances of diffusion model, generative speech enhancement (SE)
has attracted a surge of research interest due to its great potential for
unseen testing noises. However, existing efforts mainly focus on inherent
properties of clean speech for inference, underexploiting the varying noise
information in real-world conditions. In this paper, we propose a noise-aware
speech enhancement (NASE) approach that extracts noise-specific information to
guide the reverse process in diffusion model. Specifically, we design a noise
classification (NC) model to produce acoustic embedding as a noise conditioner
for guiding the reverse denoising process. Meanwhile, a multi-task learning
scheme is devised to jointly optimize SE and NC tasks, in order to enhance the
noise specificity of extracted noise conditioner. Our proposed NASE is shown to
be a plug-and-play module that can be generalized to any diffusion SE models.
Experiment evidence on VoiceBank-DEMAND dataset shows that NASE achieves
significant improvement over multiple mainstream diffusion SE models,
especially on unseen testing noises.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuchen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruizhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qiushi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chng_E/0/1/0/all/0/1&quot;&gt;Eng Siong Chng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08033">
<title>Magnetic Field-Based Reward Shaping for Goal-Conditioned Reinforcement Learning. (arXiv:2307.08033v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08033</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal-conditioned reinforcement learning (RL) is an interesting extension of
the traditional RL framework, where the dynamic environment and reward sparsity
can cause conventional learning algorithms to fail. Reward shaping is a
practical approach to improving sample efficiency by embedding human domain
knowledge into the learning process. Existing reward shaping methods for
goal-conditioned RL are typically built on distance metrics with a linear and
isotropic distribution, which may fail to provide sufficient information about
the ever-changing environment with high complexity. This paper proposes a novel
magnetic field-based reward shaping (MFRS) method for goal-conditioned RL tasks
with dynamic target and obstacles. Inspired by the physical properties of
magnets, we consider the target and obstacles as permanent magnets and
establish the reward function according to the intensity values of the magnetic
field generated by these magnets. The nonlinear and anisotropic distribution of
the magnetic field intensity can provide more accessible and conducive
information about the optimization landscape, thus introducing a more
sophisticated magnetic reward compared to the distance-based setting. Further,
we transform our magnetic reward to the form of potential-based reward shaping
by learning a secondary potential function concurrently to ensure the optimal
policy invariance of our method. Experiments results in both simulated and
real-world robotic manipulation tasks demonstrate that MFRS outperforms
relevant existing methods and effectively improves the sample efficiency of RL
algorithms in goal-conditioned tasks with various dynamics of the target and
obstacles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hongyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuanze Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qing Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chunlin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08038">
<title>Bivariate DeepKriging for Large-scale Spatial Interpolation of Wind Fields. (arXiv:2307.08038v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2307.08038</link>
<description rdf:parseType="Literal">&lt;p&gt;High spatial resolution wind data are essential for a wide range of
applications in climate, oceanographic and meteorological studies. Large-scale
spatial interpolation or downscaling of bivariate wind fields having velocity
in two dimensions is a challenging task because wind data tend to be
non-Gaussian with high spatial variability and heterogeneity. In spatial
statistics, cokriging is commonly used for predicting bivariate spatial fields.
However, the cokriging predictor is not optimal except for Gaussian processes.
Additionally, cokriging is computationally prohibitive for large datasets. In
this paper, we propose a method, called bivariate DeepKriging, which is a
spatially dependent deep neural network (DNN) with an embedding layer
constructed by spatial radial basis functions for bivariate spatial data
prediction. We then develop a distribution-free uncertainty quantification
method based on bootstrap and ensemble DNN. Our proposed approach outperforms
the traditional cokriging predictor with commonly used covariance functions,
such as the linear model of co-regionalization and flexible bivariate Mat\&apos;ern
covariance. We demonstrate the computational efficiency and scalability of the
proposed DNN model, with computations that are, on average, 20 times faster
than those of conventional techniques. We apply the bivariate DeepKriging
method to the wind data over the Middle East region at 506,771 locations. The
prediction performance of the proposed method is superior over the cokriging
predictors and dramatically reduces computation time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nag_P/0/1/0/all/0/1&quot;&gt;Pratik Nag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Ying Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Reich_B/0/1/0/all/0/1&quot;&gt;Brian J Reich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08044">
<title>Towards Flexible Time-to-event Modeling: Optimizing Neural Networks via Rank Regression. (arXiv:2307.08044v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.08044</link>
<description rdf:parseType="Literal">&lt;p&gt;Time-to-event analysis, also known as survival analysis, aims to predict the
time of occurrence of an event, given a set of features. One of the major
challenges in this area is dealing with censored data, which can make learning
algorithms more complex. Traditional methods such as Cox&apos;s proportional hazards
model and the accelerated failure time (AFT) model have been popular in this
field, but they often require assumptions such as proportional hazards and
linearity. In particular, the AFT models often require pre-specified parametric
distributional assumptions. To improve predictive performance and alleviate
strict assumptions, there have been many deep learning approaches for
hazard-based models in recent years. However, representation learning for AFT
has not been widely explored in the neural network literature, despite its
simplicity and interpretability in comparison to hazard-focused methods. In
this work, we introduce the Deep AFT Rank-regression model for Time-to-event
prediction (DART). This model uses an objective function based on Gehan&apos;s rank
statistic, which is efficient and reliable for representation learning. On top
of eliminating the requirement to establish a baseline event time distribution,
DART retains the advantages of directly predicting event time in standard AFT
models. The proposed method is a semiparametric approach to AFT modeling that
does not impose any distributional assumptions on the survival time
distribution. This also eliminates the need for additional hyperparameters or
complex model architectures, unlike existing neural network-based AFT models.
Through quantitative analysis on various benchmark datasets, we have shown that
DART has significant potential for modeling high-throughput censored
time-to-event data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyunjun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junhyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1&quot;&gt;Taehwa Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Sangbum Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08045">
<title>Fast Quantum Algorithm for Attention Computation. (arXiv:2307.08045v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2307.08045</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated exceptional performance across
a wide range of tasks. These models, powered by advanced deep learning
techniques, have revolutionized the field of natural language processing (NLP)
and have achieved remarkable results in various language-related tasks.
&lt;/p&gt;
&lt;p&gt;LLMs have excelled in tasks such as machine translation, sentiment analysis,
question answering, text generation, text classification, language modeling,
and more. They have proven to be highly effective in capturing complex
linguistic patterns, understanding context, and generating coherent and
contextually relevant text. The attention scheme plays a crucial role in the
architecture of large language models (LLMs). It is a fundamental component
that enables the model to capture and utilize contextual information during
language processing tasks effectively. Making the attention scheme computation
faster is one of the central questions to speed up the LLMs computation. It is
well-known that quantum machine has certain computational advantages compared
to the classical machine. However, it is currently unknown whether quantum
computing can aid in LLM.
&lt;/p&gt;
&lt;p&gt;In this work, we focus on utilizing Grover&apos;s Search algorithm to compute a
sparse attention computation matrix efficiently. We achieve a polynomial
quantum speed-up over the classical method. Moreover, the attention matrix
outputted by our quantum algorithm exhibits an extra low-rank structure that
will be useful in obtaining a faster training algorithm for LLMs. Additionally,
we present a detailed analysis of the algorithm&apos;s error analysis and time
complexity within the context of computing the attention matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yeqi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zhao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruizhe Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08065">
<title>MaGNAS: A Mapping-Aware Graph Neural Architecture Search Framework for Heterogeneous MPSoC Deployment. (arXiv:2307.08065v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2307.08065</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are becoming increasingly popular for
vision-based applications due to their intrinsic capacity in modeling
structural and contextual relations between various parts of an image frame. On
another front, the rising popularity of deep vision-based applications at the
edge has been facilitated by the recent advancements in heterogeneous
multi-processor Systems on Chips (MPSoCs) that enable inference under
real-time, stringent execution requirements. By extension, GNNs employed for
vision-based applications must adhere to the same execution requirements. Yet
contrary to typical deep neural networks, the irregular flow of graph learning
operations poses a challenge to running GNNs on such heterogeneous MPSoC
platforms. In this paper, we propose a novel unified design-mapping approach
for efficient processing of vision GNN workloads on heterogeneous MPSoC
platforms. Particularly, we develop MaGNAS, a mapping-aware Graph Neural
Architecture Search framework. MaGNAS proposes a GNN architectural design space
coupled with prospective mapping options on a heterogeneous SoC to identify
model architectures that maximize on-device resource efficiency. To achieve
this, MaGNAS employs a two-tier evolutionary search to identify optimal GNNs
and mapping pairings that yield the best performance trade-offs. Through
designing a supernet derived from the recent Vision GNN (ViG) architecture, we
conducted experiments on four (04) state-of-the-art vision datasets using both
(i) a real hardware SoC platform (NVIDIA Xavier AGX) and (ii) a
performance/cost model simulator for DNN accelerators. Our experimental results
demonstrate that MaGNAS is able to provide 1.57x latency speedup and is 3.38x
more energy-efficient for several vision datasets executed on the Xavier MPSoC
vs. the GPU-only deployment while sustaining an average 0.11% accuracy
reduction from the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odema_M/0/1/0/all/0/1&quot;&gt;Mohanad Odema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouzidi_H/0/1/0/all/0/1&quot;&gt;Halima Bouzidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouarnoughi_H/0/1/0/all/0/1&quot;&gt;Hamza Ouarnoughi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niar_S/0/1/0/all/0/1&quot;&gt;Smail Niar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faruque_M/0/1/0/all/0/1&quot;&gt;Mohammad Abdullah Al Faruque&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1901.10568">
<title>Stochastic Gradient MCMC for Nonlinear State Space Models. (arXiv:1901.10568v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/1901.10568</link>
<description rdf:parseType="Literal">&lt;p&gt;State space models (SSMs) provide a flexible framework for modeling complex
time series via a latent stochastic process. Inference for nonlinear,
non-Gaussian SSMs is often tackled with particle methods that do not scale well
to long time series. The challenge is two-fold: not only do computations scale
linearly with time, as in the linear case, but particle filters additionally
suffer from increasing particle degeneracy with longer series. Stochastic
gradient MCMC methods have been developed to scale Bayesian inference for
finite-state hidden Markov models and linear SSMs using buffered stochastic
gradient estimates to account for temporal dependencies. We extend these
stochastic gradient estimators to nonlinear SSMs using particle methods. We
present error bounds that account for both buffering error and particle error
in the case of nonlinear SSMs that are log-concave in the latent process. We
evaluate our proposed particle buffered stochastic gradient using stochastic
gradient MCMC for inference on both long sequential synthetic and
minute-resolution financial returns data, demonstrating the importance of this
class of methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aicher_C/0/1/0/all/0/1&quot;&gt;Christopher Aicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Putcha_S/0/1/0/all/0/1&quot;&gt;Srshti Putcha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nemeth_C/0/1/0/all/0/1&quot;&gt;Christopher Nemeth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fearnhead_P/0/1/0/all/0/1&quot;&gt;Paul Fearnhead&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Fox_E/0/1/0/all/0/1&quot;&gt;Emily B. Fox&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1909.07750">
<title>MDP Playground: An Analysis and Debug Testbed for Reinforcement Learning. (arXiv:1909.07750v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1909.07750</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MDP Playground, a testbed for Reinforcement Learning (RL) agents
with dimensions of hardness that can be controlled independently to challenge
agents in different ways and obtain varying degrees of hardness in toy and
complex RL environments. We consider and allow control over a wide variety of
dimensions, including delayed rewards, sequence lengths, reward density,
stochasticity, image representations, irrelevant features, time unit, action
range and more. We define a parameterised collection of fast-to-run toy
environments in OpenAI Gym by varying these dimensions and propose to use these
to understand agents better. We then show how to design experiments using MDP
Playground to gain insights on the toy environments. We also provide wrappers
that can inject many of these dimensions into any Gym environment. We
experiment with these wrappers on Atari and Mujoco to allow for understanding
the effects of these dimensions on environments that are more complex than the
toy environments. We also compare the effect of the dimensions on the toy and
complex environments. Finally, we show how to use MDP Playground to debug
agents, to study the interaction of multiple dimensions and describe further
use-cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajan_R/0/1/0/all/0/1&quot;&gt;Raghu Rajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_J/0/1/0/all/0/1&quot;&gt;Jessica Lizeth Borja Diaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guttikonda_S/0/1/0/all/0/1&quot;&gt;Suresh Guttikonda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferreira_F/0/1/0/all/0/1&quot;&gt;Fabio Ferreira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biedenkapp_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Biedenkapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartz_J/0/1/0/all/0/1&quot;&gt;Jan Ole von Hartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1911.02265">
<title>Predictive modeling of brain tumor: A Deep learning approach. (arXiv:1911.02265v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1911.02265</link>
<description rdf:parseType="Literal">&lt;p&gt;Image processing concepts can visualize the different anatomy structure of
the human body. Recent advancements in the field of deep learning have made it
possible to detect the growth of cancerous tissue just by a patient&apos;s brain
Magnetic Resonance Imaging (MRI) scans. These methods require very high
accuracy and meager false negative rates to be of any practical use. This paper
presents a Convolutional Neural Network (CNN) based transfer learning approach
to classify the brain MRI scans into two classes using three pre-trained
models. The performances of these models are compared with each other.
Experimental results show that the Resnet-50 model achieves the highest
accuracy and least false negative rates as 95% and zero respectively. It is
followed by VGG-16 and Inception-V3 model with an accuracy of 90% and 55%
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saxena_P/0/1/0/all/0/1&quot;&gt;Priyansh Saxena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_A/0/1/0/all/0/1&quot;&gt;Akshat Maheshwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maheshwari_S/0/1/0/all/0/1&quot;&gt;Saumil Maheshwari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1911.10367">
<title>A Sub-sampled Tensor Method for Non-convex Optimization. (arXiv:1911.10367v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/1911.10367</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a stochastic optimization method that uses a fourth-order
regularized model to find local minima of smooth and potentially non-convex
objective functions with a finite-sum structure. This algorithm uses
sub-sampled derivatives instead of exact quantities. The proposed approach is
shown to find an $(\epsilon_1,\epsilon_2,\epsilon_3)$-third-order critical
point in at most $\bigO\left(\max\left(\epsilon_1^{-4/3}, \epsilon_2^{-2},
\epsilon_3^{-4}\right)\right)$ iterations, thereby matching the rate of
deterministic approaches. In order to prove this result, we derive a novel
tensor concentration inequality for sums of tensors of any order that makes
explicit use of the finite-sum structure of the objective function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lucchi_A/0/1/0/all/0/1&quot;&gt;Aurelien Lucchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kohler_J/0/1/0/all/0/1&quot;&gt;Jonas Kohler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2005.11018">
<title>Semi-Supervised Learning: the Case When Unlabeled Data is Equally Useful. (arXiv:2005.11018v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2005.11018</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning algorithms attempt to take advantage of relatively
inexpensive unlabeled data to improve learning performance. In this work, we
consider statistical models where the data distributions can be characterized
by continuous parameters. We show that under certain conditions on the
distribution, unlabeled data is equally useful as labeled date in terms of
learning rate. Specifically, let $n, m$ be the number of labeled and unlabeled
data, respectively. It is shown that the learning rate of semi-supervised
learning scales as $O(1/n)$ if $m\sim n$, and scales as $O(1/n^{1+\gamma})$ if
$m\sim n^{1+\gamma}$ for some $\gamma&amp;gt;0$, whereas the learning rate of
supervised learning scales as $O(1/n)$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingge Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2011.11576">
<title>Conjecturing-Based Discovery of Patterns in Data. (arXiv:2011.11576v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2011.11576</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the use of a conjecturing machine that suggests feature
relationships in the form of bounds involving nonlinear terms for numerical
features and boolean expressions for categorical features. The proposed
Conjecturing framework recovers known nonlinear and boolean relationships among
features from data. In both settings, true underlying relationships are
revealed. We then compare the method to a previously-proposed framework for
symbolic regression on the ability to recover equations that are satisfied
among features in a dataset. The framework is then applied to patient-level
data regarding COVID-19 outcomes to suggest possible risk factors that are
confirmed in the medical literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brooks_J/0/1/0/all/0/1&quot;&gt;J.P. Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edwards_D/0/1/0/all/0/1&quot;&gt;D.J. Edwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larson_C/0/1/0/all/0/1&quot;&gt;C.E. Larson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cleemput_N/0/1/0/all/0/1&quot;&gt;N. Van Cleemput&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.13389">
<title>Generating Novel Scene Compositions from Single Images and Videos. (arXiv:2103.13389v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.13389</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a large dataset for training, generative adversarial networks (GANs)
can achieve remarkable performance for the image synthesis task. However,
training GANs in extremely low data regimes remains a challenge, as overfitting
often occurs, leading to memorization or training divergence. In this work, we
introduce SIV-GAN, an unconditional generative model that can generate new
scene compositions from a single training image or a single video clip. We
propose a two-branch discriminator architecture, with content and layout
branches designed to judge internal content and scene layout realism separately
from each other. This discriminator design enables synthesis of visually
plausible, novel compositions of a scene, with varying content and layout,
while preserving the context of the original sample. Compared to previous
single image GANs, our model generates more diverse, higher quality images,
while not being restricted to a single image setting. We further introduce a
new challenging task of learning from a few frames of a single video. In this
training setup the training images are highly similar to each other, which
makes it difficult for prior GAN models to achieve a synthesis of both high
quality and diversity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sushko_V/0/1/0/all/0/1&quot;&gt;Vadim Sushko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gall_J/0/1/0/all/0/1&quot;&gt;Juergen Gall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoreva_A/0/1/0/all/0/1&quot;&gt;Anna Khoreva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.02797">
<title>Neural Distributed Source Coding. (arXiv:2106.02797v3 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2106.02797</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed source coding (DSC) is the task of encoding an input in the
absence of correlated side information that is only available to the decoder.
Remarkably, Slepian and Wolf showed in 1973 that an encoder without access to
the side information can asymptotically achieve the same compression rate as
when the side information is available to it. While there is vast prior work on
this topic, practical DSC has been limited to synthetic datasets and specific
correlation structures. Here we present a framework for lossy DSC that is
agnostic to the correlation structure and can scale to high dimensions. Rather
than relying on hand-crafted source modeling, our method utilizes a conditional
Vector-Quantized Variational Autoencoder (VQ-VAE) to learn the distributed
encoder and decoder. We evaluate our method on multiple datasets and show that
our method can handle complex correlations and achieves state-of-the-art PSNR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whang_J/0/1/0/all/0/1&quot;&gt;Jay Whang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagle_A/0/1/0/all/0/1&quot;&gt;Alliot Nagle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Anish Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyeji Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimakis_A/0/1/0/all/0/1&quot;&gt;Alexandros G. Dimakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.04486">
<title>Sketch-Based Anomaly Detection in Streaming Graphs. (arXiv:2106.04486v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2106.04486</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a stream of graph edges from a dynamic graph, how can we assign anomaly
scores to edges and subgraphs in an online manner, for the purpose of detecting
unusual behavior, using constant time and memory? For example, in intrusion
detection, existing work seeks to detect either anomalous edges or anomalous
subgraphs, but not both. In this paper, we first extend the count-min sketch
data structure to a higher-order sketch. This higher-order sketch has the
useful property of preserving the dense subgraph structure (dense subgraphs in
the input turn into dense submatrices in the data structure). We then propose 4
online algorithms that utilize this enhanced data structure, which (a) detect
both edge and graph anomalies; (b) process each edge and graph in constant
memory and constant update time per newly arriving edge, and; (c) outperform
state-of-the-art baselines on 4 real-world datasets. Our method is the first
streaming approach that incorporates dense subgraph search to detect graph
anomalies in constant memory and time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1&quot;&gt;Siddharth Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_M/0/1/0/all/0/1&quot;&gt;Mohit Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_N/0/1/0/all/0/1&quot;&gt;Neil Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1&quot;&gt;Bryan Hooi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.12976">
<title>Approximating Pandora&apos;s Box with Correlations. (arXiv:2108.12976v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2108.12976</link>
<description rdf:parseType="Literal">&lt;p&gt;We revisit the classic Pandora&apos;s Box (PB) problem under correlated
distributions on the box values. Recent work of &lt;a href=&quot;/abs/1911.01632&quot;&gt;arXiv:1911.01632&lt;/a&gt; obtained
constant approximate algorithms for a restricted class of policies for the
problem that visit boxes in a fixed order. In this work, we study the
complexity of approximating the optimal policy which may adaptively choose
which box to visit next based on the values seen so far.
&lt;/p&gt;
&lt;p&gt;Our main result establishes an approximation-preserving equivalence of PB to
the well studied Uniform Decision Tree (UDT) problem from stochastic
optimization and a variant of the Min-Sum Set Cover ($\text{MSSC}_f$) problem.
For distributions of support $m$, UDT admits a $\log m$ approximation, and
while a constant factor approximation in polynomial time is a long-standing
open problem, constant factor approximations are achievable in subexponential
time (&lt;a href=&quot;/abs/1906.11385&quot;&gt;arXiv:1906.11385&lt;/a&gt;). Our main result implies that the same properties hold
for PB and $\text{MSSC}_f$.
&lt;/p&gt;
&lt;p&gt;We also study the case where the distribution over values is given more
succinctly as a mixture of $m$ product distributions. This problem is again
related to a noisy variant of the Optimal Decision Tree which is significantly
more challenging. We give a constant-factor approximation that runs in time
$n^{ \tilde O( m^2/\varepsilon^2 ) }$ when the mixture components on every box
are either identical or separated in TV distance by $\varepsilon$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chawla_S/0/1/0/all/0/1&quot;&gt;Shuchi Chawla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gergatsouli_E/0/1/0/all/0/1&quot;&gt;Evangelia Gergatsouli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMahan_J/0/1/0/all/0/1&quot;&gt;Jeremy McMahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzamos_C/0/1/0/all/0/1&quot;&gt;Christos Tzamos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.01771">
<title>Self-Supervised Beat Tracking in Musical Signals with Polyphonic Contrastive Learning. (arXiv:2201.01771v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2201.01771</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotating musical beats is a very long and tedious process. In order to
combat this problem, we present a new self-supervised learning pretext task for
beat tracking and downbeat estimation. This task makes use of Spleeter, an
audio source separation model, to separate a song&apos;s drums from the rest of its
signal. The first set of signals are used as positives, and by extension
negatives, for contrastive learning pre-training. The drum-less signals, on the
other hand, are used as anchors. When pre-training a fully-convolutional and
recurrent model using this pretext task, an onset function is learned. In some
cases, this function is found to be mapped to periodic elements in a song. We
find that pre-trained models outperform randomly initialized models when a beat
tracking training set is extremely small (less than 10 examples). When this is
not the case, pre-training leads to a learning speed-up that causes the model
to overfit to the training set. More generally, this work defines new
perspectives in the realm of musical self-supervised learning. It is notably
one of the first works to use audio source separation as a fundamental
component of self-supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desblancs_D/0/1/0/all/0/1&quot;&gt;Dorian Desblancs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.12440">
<title>Certifying Model Accuracy under Distribution Shifts. (arXiv:2201.12440v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2201.12440</link>
<description rdf:parseType="Literal">&lt;p&gt;Certified robustness in machine learning has primarily focused on adversarial
perturbations of the input with a fixed attack budget for each point in the
data distribution. In this work, we present provable robustness guarantees on
the accuracy of a model under bounded Wasserstein shifts of the data
distribution. We show that a simple procedure that randomizes the input of the
model within a transformation space is provably robust to distributional shifts
under the transformation. Our framework allows the datum-specific perturbation
size to vary across different points in the input distribution and is general
enough to include fixed-sized perturbations as well. Our certificates produce
guaranteed lower bounds on the performance of the model for any (natural or
adversarial) shift of the input distribution within a Wasserstein ball around
the original distribution. We apply our technique to: (i) certify robustness
against natural (non-adversarial) transformations of images such as color
shifts, hue shifts and changes in brightness and saturation, (ii) certify
robustness against adversarial shifts of the input distribution, and (iii) show
provable lower bounds (hardness results) on the performance of models trained
on so-called &quot;unlearnable&quot; datasets that have been poisoned to interfere with
model training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aounon Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_A/0/1/0/all/0/1&quot;&gt;Alexander Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_S/0/1/0/all/0/1&quot;&gt;Soheil Feizi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.05767">
<title>A PDE-Based Analysis of the Symmetric Two-Armed Bernoulli Bandit. (arXiv:2202.05767v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.05767</link>
<description rdf:parseType="Literal">&lt;p&gt;This work addresses a version of the two-armed Bernoulli bandit problem where
the sum of the means of the arms is one (the symmetric two-armed Bernoulli
bandit). In a regime where the gap between these means goes to zero as the
number of prediction periods approaches infinity, i.e., the difficulty of
detecting the gap increases as the sample size increases, we obtain the leading
order terms of the minmax optimal regret and pseudoregret for this problem by
associating each of them with a solution of a linear heat equation. Our results
improve upon the previously known results; specifically, we explicitly compute
these leading order terms in three different scaling regimes for the gap.
Additionally, we obtain new non-asymptotic bounds for any given time horizon.
Although optimal player strategies are not known for more general bandit
problems, there is significant interest in considering how regret accumulates
under specific player strategies, even when they are not known to be optimal.
We expect that the methods of this paper should be useful in settings of that
type.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobzar_V/0/1/0/all/0/1&quot;&gt;Vladimir A. Kobzar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kohn_R/0/1/0/all/0/1&quot;&gt;Robert V. Kohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07028">
<title>Exploring the Distributed Knowledge Congruence in Proxy-data-free Federated Distillation. (arXiv:2204.07028v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07028</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a privacy-preserving machine learning paradigm in
which the server periodically aggregates local model parameters from clients
without assembling their private data.
&lt;/p&gt;
&lt;p&gt;Constrained communication and personalization requirements pose severe
challenges to FL. Federated distillation (FD) is proposed to simultaneously
address the above two problems, which exchanges knowledge between the server
and clients, supporting heterogeneous local models while significantly reducing
communication overhead. However, most existing FD methods require a proxy
dataset, which is often unavailable in reality.
&lt;/p&gt;
&lt;p&gt;A few recent proxy-data-free FD approaches can eliminate the need for
additional public data, but suffer from remarkable discrepancy among local
knowledge due to client-side model heterogeneity, leading to ambiguous
representation on the server and inevitable accuracy degradation.
&lt;/p&gt;
&lt;p&gt;To tackle this issue, we propose a proxy-data-free FD algorithm based on
distributed knowledge congruence (FedDKC). FedDKC leverages well-designed
refinement strategies to narrow local knowledge differences into an acceptable
upper bound, so as to mitigate the negative effects of knowledge incongruence.
&lt;/p&gt;
&lt;p&gt;Specifically, from perspectives of peak probability and Shannon entropy of
local knowledge, we design kernel-based knowledge refinement (KKR) and
searching-based knowledge refinement (SKR) respectively, and theoretically
guarantee that the refined-local knowledge can satisfy an approximately-similar
distribution and be regarded as congruent.
&lt;/p&gt;
&lt;p&gt;Extensive experiments conducted on three common datasets demonstrate that our
proposed FedDKC significantly outperforms the state-of-the-art on various
heterogeneous settings while evidently improving the convergence speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Q/0/1/0/all/0/1&quot;&gt;Quyang Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zeju Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingxiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.01970">
<title>Non-Stationary Bandit Learning via Predictive Sampling. (arXiv:2205.01970v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.01970</link>
<description rdf:parseType="Literal">&lt;p&gt;Thompson sampling has proven effective across a wide range of stationary
bandit environments. However, as we demonstrate in this paper, it can perform
poorly when applied to non-stationary environments. We attribute such failures
to the fact that, when exploring, the algorithm does not differentiate actions
based on how quickly the information acquired loses its usefulness due to
non-stationarity. Building upon this insight, we propose predictive sampling,
an algorithm that deprioritizes acquiring information that quickly loses
usefulness. A theoretical guarantee on the performance of predictive sampling
is established through a Bayesian regret bound. We provide versions of
predictive sampling for which computations tractably scale to complex bandit
environments of practical interest. Through numerical simulations, we
demonstrate that predictive sampling outperforms Thompson sampling in all
non-stationary environments examined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yueyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_X/0/1/0/all/0/1&quot;&gt;Xu Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.03923">
<title>Unsupervised Discovery and Composition of Object Light Fields. (arXiv:2205.03923v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.03923</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural scene representations, both continuous and discrete, have recently
emerged as a powerful new paradigm for 3D scene understanding. Recent efforts
have tackled unsupervised discovery of object-centric neural scene
representations. However, the high cost of ray-marching, exacerbated by the
fact that each object representation has to be ray-marched separately, leads to
insufficiently sampled radiance fields and thus, noisy renderings, poor
framerates, and high memory and time complexity during training and rendering.
Here, we propose to represent objects in an object-centric, compositional scene
representation as light fields. We propose a novel light field compositor
module that enables reconstructing the global light field from a set of
object-centric light fields. Dubbed Compositional Object Light Fields (COLF),
our method enables unsupervised learning of object-centric neural scene
representations, state-of-the-art reconstruction and novel view synthesis
performance on standard datasets, and rendering and training speeds at orders
of magnitude faster than existing 3D approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1&quot;&gt;Cameron Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hong-Xing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakharov_S/0/1/0/all/0/1&quot;&gt;Sergey Zakharov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durand_F/0/1/0/all/0/1&quot;&gt;Fredo Durand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiajun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitzmann_V/0/1/0/all/0/1&quot;&gt;Vincent Sitzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11168">
<title>Logarithmic regret bounds for continuous-time average-reward Markov decision processes. (arXiv:2205.11168v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11168</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider reinforcement learning for continuous-time Markov decision
processes (MDPs) in the infinite-horizon, average-reward setting. In contrast
to discrete-time MDPs, a continuous-time process moves to a state and stays
there for a random holding time after an action is taken. With unknown
transition probabilities and rates of exponential holding times, we derive
instance-dependent regret lower bounds that are logarithmic in the time
horizon. Moreover, we design a learning algorithm and establish a finite-time
regret bound that achieves the logarithmic growth rate. Our analysis builds
upon upper confidence reinforcement learning, a delicate estimation of the mean
holding times, and stochastic comparison of point processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xuefeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xun Yu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.12086">
<title>Information-Directed Selection for Top-Two Algorithms. (arXiv:2205.12086v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2205.12086</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the best-k-arm identification problem for multi-armed bandits,
where the objective is to select the exact set of k arms with the highest mean
rewards by sequentially allocating measurement effort. We characterize the
necessary and sufficient conditions for the optimal allocation using dual
variables. Remarkably these optimality conditions lead to the extension of
top-two algorithm design principle (Russo, 2020), initially proposed for
best-arm identification. Furthermore, our optimality conditions induce a simple
and effective selection rule dubbed information-directed selection (IDS) that
selects one of the top-two candidates based on a measure of information gain.
As a theoretical guarantee, we prove that integrated with IDS, top-two Thompson
sampling is (asymptotically) optimal for Gaussian best-arm identification,
solving a glaring open problem in the pure exploration literature (Russo,
2020). As a by-product, we show that for k &amp;gt; 1, top-two algorithms cannot
achieve optimality even when the algorithm has access to the unknown &quot;optimal&quot;
tuning parameter. Numerical experiments show the superior performance of the
proposed top-two algorithms with IDS and considerable improvement compared with
algorithms without adaptive selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+You_W/0/1/0/all/0/1&quot;&gt;Wei You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Qin_C/0/1/0/all/0/1&quot;&gt;Chao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuoguang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.14842">
<title>Efficient Reward Poisoning Attacks on Online Deep Reinforcement Learning. (arXiv:2205.14842v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.14842</link>
<description rdf:parseType="Literal">&lt;p&gt;We study reward poisoning attacks on online deep reinforcement learning
(DRL), where the attacker is oblivious to the learning algorithm used by the
agent and the dynamics of the environment. We demonstrate the intrinsic
vulnerability of state-of-the-art DRL algorithms by designing a general,
black-box reward poisoning framework called adversarial MDP attacks. We
instantiate our framework to construct two new attacks which only corrupt the
rewards for a small fraction of the total training timesteps and make the agent
learn a low-performing policy. We provide a theoretical analysis of the
efficiency of our attack and perform an extensive empirical evaluation. Our
results show that our attacks efficiently poison agents learning in several
popular classical control and MuJoCo environments with a variety of
state-of-the-art DRL algorithms, such as DQN, PPO, SAC, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinglun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Q/0/1/0/all/0/1&quot;&gt;Qi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gagandeep Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.05794">
<title>SGD and Weight Decay Provably Induce a Low-Rank Bias in Neural Networks. (arXiv:2206.05794v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.05794</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the bias of Stochastic Gradient Descent (SGD) to learn low-rank
weight matrices when training deep ReLU neural networks. Our results show that
training neural networks with mini-batch SGD and weight decay causes a bias
towards rank minimization over the weight matrices. Specifically, we show, both
theoretically and empirically, that this bias is more pronounced when using
smaller batch sizes, higher learning rates, or increased weight decay.
Additionally, we predict and observe empirically that weight decay is necessary
to achieve this bias. In addition, we show that in the presence of intermediate
neural collapse, the learned weights are particularly low-rank. Unlike previous
literature, our analysis does not rely on assumptions about the data,
convergence, or optimality of the weight matrices. Furthermore, it applies to a
wide range of neural network architectures of any width or depth. Finally, we
empirically investigate the connection between this bias and generalization,
finding that it has a marginal effect on generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1&quot;&gt;Tomer Galanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegel_Z/0/1/0/all/0/1&quot;&gt;Zachary S. Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupte_A/0/1/0/all/0/1&quot;&gt;Aparna Gupte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poggio_T/0/1/0/all/0/1&quot;&gt;Tomaso Poggio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02093">
<title>Predicting Out-of-Domain Generalization with Neighborhood Invariance. (arXiv:2207.02093v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02093</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing and deploying machine learning models safely depends on the
ability to characterize and compare their abilities to generalize to new
environments. Although recent work has proposed a variety of methods that can
directly predict or theoretically bound the generalization capacity of a model,
they rely on strong assumptions such as matching train/test distributions and
access to model gradients. In order to characterize generalization when these
assumptions are not satisfied, we propose neighborhood invariance, a measure of
a classifier&apos;s output invariance in a local transformation neighborhood.
Specifically, we sample a set of transformations and given an input test point,
calculate the invariance as the largest fraction of transformed points
classified into the same class. Crucially, our measure is simple to calculate,
does not depend on the test point&apos;s true label, makes no assumptions about the
data distribution or model, and can be applied even in out-of-domain (OOD)
settings where existing methods cannot, requiring only selecting a set of
appropriate data transformations. In experiments on robustness benchmarks in
image classification, sentiment analysis, and natural language inference, we
demonstrate a strong and robust correlation between our neighborhood invariance
measure and actual OOD generalization on over 4,600 models evaluated on over
100 unique train/test domain pairs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_N/0/1/0/all/0/1&quot;&gt;Nathan Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hulkund_N/0/1/0/all/0/1&quot;&gt;Neha Hulkund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.06767">
<title>Semi-supervised cross-lingual speech emotion recognition. (arXiv:2207.06767v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2207.06767</link>
<description rdf:parseType="Literal">&lt;p&gt;Performance in Speech Emotion Recognition (SER) on a single language has
increased greatly in the last few years thanks to the use of deep learning
techniques. However, cross-lingual SER remains a challenge in real-world
applications due to two main factors: the first is the big gap among the source
and the target domain distributions; the second factor is the major
availability of unlabeled utterances in contrast to the labeled ones for the
new language. Taking into account previous aspects, we propose a
Semi-Supervised Learning (SSL) method for cross-lingual emotion recognition
when only few labeled examples in the target domain (i.e. the new language) are
available. Our method is based on a Transformer and it adapts to the new domain
by exploiting a pseudo-labeling strategy on the unlabeled utterances. In
particular, the use of a hard and soft pseudo-labels approach is investigated.
We thoroughly evaluate the performance of the proposed method in a
speaker-independent setup on both the source and the new language and show its
robustness across five languages belonging to different linguistic strains. The
experimental findings indicate that the unweighted accuracy is increased by an
average of 40% compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarla_M/0/1/0/all/0/1&quot;&gt;Mirko Agarla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianco_S/0/1/0/all/0/1&quot;&gt;Simone Bianco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celona_L/0/1/0/all/0/1&quot;&gt;Luigi Celona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Napoletano_P/0/1/0/all/0/1&quot;&gt;Paolo Napoletano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petrovsky_A/0/1/0/all/0/1&quot;&gt;Alexey Petrovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piccoli_F/0/1/0/all/0/1&quot;&gt;Flavio Piccoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schettini_R/0/1/0/all/0/1&quot;&gt;Raimondo Schettini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanin_I/0/1/0/all/0/1&quot;&gt;Ivan Shanin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.10603">
<title>Unsupervised pre-training of graph transformers on patient population graphs. (arXiv:2207.10603v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.10603</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training has shown success in different areas of machine learning, such
as Computer Vision, Natural Language Processing (NLP), and medical imaging.
However, it has not been fully explored for clinical data analysis. An immense
amount of clinical records are recorded, but still, data and labels can be
scarce for data collected in small hospitals or dealing with rare diseases. In
such scenarios, pre-training on a larger set of unlabelled clinical data could
improve performance. In this paper, we propose novel unsupervised pre-training
techniques designed for heterogeneous, multi-modal clinical data for patient
outcome prediction inspired by masked language modeling (MLM), by leveraging
graph deep learning over population graphs. To this end, we further propose a
graph-transformer-based network, designed to handle heterogeneous clinical
data. By combining masking-based pre-training with a transformer-based network,
we translate the success of masking-based pre-training in other domains to
heterogeneous clinical data. We show the benefit of our pre-training method in
a self-supervised and a transfer learning setting, utilizing three medical
datasets TADPOLE, MIMIC-III, and a Sepsis Prediction Dataset. We find that our
proposed pre-training methods help in modeling the data at a patient and
population level and improve performance in different fine-tuning tasks on all
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazi_A/0/1/0/all/0/1&quot;&gt;Anees Kazi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04935">
<title>A Bayesian Bradley-Terry model to compare multiple ML algorithms on multiple data sets. (arXiv:2208.04935v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04935</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a Bayesian model to compare multiple algorithms on
multiple data sets, on any metric. The model is based on the Bradley-Terry
model, that counts the number of times one algorithm performs better than
another on different data sets. Because of its Bayesian foundations, the
Bayesian Bradley Terry model (BBT) has different characteristics than
frequentist approaches to comparing multiple algorithms on multiple data sets,
such as Demsar (2006) tests on mean rank, and Benavoli et al. (2016) multiple
pairwise Wilcoxon tests with p-adjustment procedures. In particular, a Bayesian
approach allows for more nuanced statements regarding the algorithms beyond
claiming that the difference is or it is not statistically significant.
Bayesian approaches also allow to define when two algorithms are equivalent for
practical purposes, or the region of practical equivalence (ROPE). Different
than a Bayesian signed rank comparison procedure proposed by Benavoli et al.
(2017), our approach can define a ROPE for any metric, since it is based on
probability statements, and not on differences of that metric. This paper also
proposes a local ROPE concept, that evaluates whether a positive difference
between a mean measure across some cross validation to the mean of some other
algorithms is should be really seen as the first algorithm being better than
the second, based on effect sizes. This local ROPE proposal is independent of a
Bayesian use, and can be used in frequentist approaches based on ranks. A R
package and a Python program that implements the BBT is available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wainer_J/0/1/0/all/0/1&quot;&gt;Jacques Wainer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09418">
<title>SAFARI: Versatile and Efficient Evaluations for Robustness of Interpretability. (arXiv:2208.09418v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09418</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability of Deep Learning (DL) is a barrier to trustworthy AI.
Despite great efforts made by the Explainable AI (XAI) community, explanations
lack robustness -- indistinguishable input perturbations may lead to different
XAI results. Thus, it is vital to assess how robust DL interpretability is,
given an XAI method. In this paper, we identify several challenges that the
state-of-the-art is unable to cope with collectively: i) existing metrics are
not comprehensive; ii) XAI techniques are highly heterogeneous; iii)
misinterpretations are normally rare events. To tackle these challenges, we
introduce two black-box evaluation methods, concerning the worst-case
interpretation discrepancy and a probabilistic notion of how robust in general,
respectively. Genetic Algorithm (GA) with bespoke fitness function is used to
solve constrained optimisation for efficient worst-case evaluation. Subset
Simulation (SS), dedicated to estimate rare event probabilities, is used for
evaluating overall robustness. Experiments show that the accuracy, sensitivity,
and efficiency of our methods outperform the state-of-the-arts. Finally, we
demonstrate two applications of our methods: ranking robust XAI methods and
selecting training schemes to improve both classification and interpretation
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_G/0/1/0/all/0/1&quot;&gt;Gaojie Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09833">
<title>Label-Noise Learning with Intrinsically Long-Tailed Data. (arXiv:2208.09833v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09833</link>
<description rdf:parseType="Literal">&lt;p&gt;Label noise is one of the key factors that lead to the poor generalization of
deep learning models. Existing label-noise learning methods usually assume that
the ground-truth classes of the training data are balanced. However, the
real-world data is often imbalanced, leading to the inconsistency between
observed and intrinsic class distribution with label noises. In this case, it
is hard to distinguish clean samples from noisy samples on the intrinsic tail
classes with the unknown intrinsic class distribution. In this paper, we
propose a learning framework for label-noise learning with intrinsically
long-tailed data. Specifically, we propose two-stage bi-dimensional sample
selection (TABASCO) to better separate clean samples from noisy samples,
especially for the tail classes. TABASCO consists of two new separation metrics
that complement each other to compensate for the limitation of using a single
metric in sample separation. Extensive experiments on benchmarks we proposed
with real-world noise and intrinsically long-tailed distribution demonstrate
the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bo Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1&quot;&gt;Yiu-ming Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanzi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00399">
<title>Optimal Regularized Online Allocation by Adaptive Re-Solving. (arXiv:2209.00399v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00399</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a dual-based algorithm framework for solving the
regularized online resource allocation problems, which have potentially
non-concave cumulative rewards, hard resource constraints, and a non-separable
regularizer. Under a strategy of adaptively updating the resource constraints,
the proposed framework only requests approximate solutions to the empirical
dual problems up to a certain accuracy and yet delivers an optimal logarithmic
regret under a locally second-order growth condition. Surprisingly, a delicate
analysis of the dual objective function enables us to eliminate the notorious
log-log factor in regret bound. The flexible framework renders renowned and
computationally fast algorithms immediately applicable, e.g., dual stochastic
gradient descent. Additionally, an infrequent re-solving scheme is proposed,
which significantly reduces computational demands without compromising the
optimal regret performance. A worst-case square-root regret lower bound is
established if the resource constraints are not adaptively updated during dual
optimization, which underscores the critical role of adaptive dual variable
update. Comprehensive numerical experiments demonstrate the merits of the
proposed algorithm framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wanteng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Ying Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_D/0/1/0/all/0/1&quot;&gt;Danny H.K. Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_D/0/1/0/all/0/1&quot;&gt;Dong Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00465">
<title>On Grounded Planning for Embodied Tasks with Language Models. (arXiv:2209.00465v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00465</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models (LMs) have demonstrated their capability in possessing
commonsense knowledge of the physical world, a crucial aspect of performing
tasks in everyday life. However, it remains unclear **whether LMs have the
capacity to generate grounded, executable plans for embodied tasks.** This is a
challenging task as LMs lack the ability to perceive the environment through
vision and feedback from the physical environment. In this paper, we address
this important research question and present the first investigation into the
topic. Our novel problem formulation, named **G-PlanET**, inputs a high-level
goal and a data table about objects in a specific environment, and then outputs
a step-by-step actionable plan for a robotic agent to follow. To facilitate the
study, we establish an **evaluation protocol** and design a dedicated metric to
assess the quality of the plans. Our experiments demonstrate that the use of
tables for encoding the environment and an iterative decoding strategy can
significantly enhance the LMs&apos; ability in grounded planning. Our analysis also
reveals interesting and non-trivial findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengsong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_W/0/1/0/all/0/1&quot;&gt;Wenda Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommerer_S/0/1/0/all/0/1&quot;&gt;Sam Sommerer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.04947">
<title>Kernel Learning for Explainable Climate Science. (arXiv:2209.04947v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.04947</link>
<description rdf:parseType="Literal">&lt;p&gt;The Upper Indus Basin, Himalayas provides water for 270 million people and
countless ecosystems. However, precipitation, a key component to hydrological
modelling, is poorly understood in this area. A key challenge surrounding this
uncertainty comes from the complex spatial-temporal distribution of
precipitation across the basin. In this work we propose Gaussian processes with
structured non-stationary kernels to model precipitation patterns in the UIB.
Previous attempts to quantify or model precipitation in the Hindu Kush
Karakoram Himalayan region have often been qualitative or include crude
assumptions and simplifications which cannot be resolved at lower resolutions.
This body of research also provides little to no error propagation. We account
for the spatial variation in precipitation with a non-stationary Gibbs kernel
parameterised with an input dependent lengthscale. This allows the posterior
function samples to adapt to the varying precipitation patterns inherent in the
distinct underlying topography of the Indus region. The input dependent
lengthscale is governed by a latent Gaussian process with a stationary
squared-exponential kernel to allow the function level hyperparameters to vary
smoothly. In ablation experiments we motivate each component of the proposed
kernel by demonstrating its ability to model the spatial covariance, temporal
structure and joint spatio-temporal reconstruction. We benchmark our model with
a stationary Gaussian process and a Deep Gaussian processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lalchand_V/0/1/0/all/0/1&quot;&gt;Vidhi Lalchand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tazi_K/0/1/0/all/0/1&quot;&gt;Kenza Tazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheema_T/0/1/0/all/0/1&quot;&gt;Talay M. Cheema&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosking_S/0/1/0/all/0/1&quot;&gt;Scott Hosking&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.05379">
<title>Action-based Early Autism Diagnosis Using Contrastive Feature Learning. (arXiv:2209.05379v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.05379</link>
<description rdf:parseType="Literal">&lt;p&gt;Autism, also known as Autism Spectrum Disorder (or ASD), is a neurological
disorder. Its main symptoms include difficulty in (verbal and/or non-verbal)
communication, and rigid/repetitive behavior. These symptoms are often
indistinguishable from a normal (control) individual, due to which this
disorder remains undiagnosed in early childhood leading to delayed treatment.
Since the learning curve is steep during the initial age, an early diagnosis of
autism could allow to take adequate interventions at the right time, which
might positively affect the growth of an autistic child. Further, the
traditional methods of autism diagnosis require multiple visits to a
specialized psychiatrist, however this process can be time-consuming. In this
paper, we present a learning based approach to automate autism diagnosis using
simple and small action video clips of subjects. This task is particularly
challenging because the amount of annotated data available is small, and the
variations among samples from the two categories (ASD and control) are
generally indistinguishable. This is also evident from poor performance of a
binary classifier learned using the cross-entropy loss on top of a baseline
encoder. To address this, we adopt contrastive feature learning in both self
supervised and supervised learning frameworks, and show that these can lead to
a significant increase in the prediction accuracy of a binary classifier on
this task. We further validate this by conducting thorough experimental
analyses under different set-ups on two publicly available datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rani_A/0/1/0/all/0/1&quot;&gt;Asha Rani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Pankaj Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_Y/0/1/0/all/0/1&quot;&gt;Yashaswi Verma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.10818">
<title>Memory-Augmented Graph Neural Networks: A Brain-Inspired Review. (arXiv:2209.10818v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.10818</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a comprehensive review of the existing literature on
memory-augmented GNNs. We review these works through the lens of psychology and
neuroscience, which has several established theories on how multiple memory
systems and mechanisms operate in biological brains. We propose a taxonomy of
memory-augmented GNNs and a set of criteria for comparing their memory
mechanisms. We also provide critical discussions on the limitations of these
works. Finally, we discuss the challenges and future directions for this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_G/0/1/0/all/0/1&quot;&gt;Guixiang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_V/0/1/0/all/0/1&quot;&gt;Vy A. Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willke_T/0/1/0/all/0/1&quot;&gt;Theodore Willke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen K. Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.11533">
<title>A Unified Perspective on Natural Gradient Variational Inference with Gaussian Mixture Models. (arXiv:2209.11533v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.11533</link>
<description rdf:parseType="Literal">&lt;p&gt;Variational inference with Gaussian mixture models (GMMs) enables learning of
highly tractable yet multi-modal approximations of intractable target
distributions with up to a few hundred dimensions. The two currently most
effective methods for GMM-based variational inference, VIPS and iBayes-GMM,
both employ independent natural gradient updates for the individual components
and their weights. We show for the first time, that their derived updates are
equivalent, although their practical implementations and theoretical guarantees
differ. We identify several design choices that distinguish both approaches,
namely with respect to sample selection, natural gradient estimation, stepsize
adaptation, and whether trust regions are enforced or the number of components
adapted. We argue that for both approaches, the quality of the learned
approximations can heavily suffer from the respective design choices: By
updating the individual components using samples from the mixture model,
iBayes-GMM often fails to produce meaningful updates to low-weight components,
and by using a zero-order method for estimating the natural gradient, VIPS
scales badly to higher-dimensional problems. Furthermore, we show that
information-geometric trust-regions (used by VIPS) are effective even when
using first-order natural gradient estimates, and often outperform the improved
Bayesian learning rule (iBLR) update used by iBayes-GMM. We systematically
evaluate the effects of design choices and show that a hybrid approach
significantly outperforms both prior works. Along with this work, we publish
our highly modular and efficient implementation for natural gradient
variational inference with Gaussian mixture models, which supports 432
different combinations of design choices, facilitates the reproduction of all
our experiments, and may prove valuable for the practitioner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenz_O/0/1/0/all/0/1&quot;&gt;Oleg Arenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahlinger_P/0/1/0/all/0/1&quot;&gt;Philipp Dahlinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Zihan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volpp_M/0/1/0/all/0/1&quot;&gt;Michael Volpp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04527">
<title>A policy gradient approach for Finite Horizon Constrained Markov Decision Processes. (arXiv:2210.04527v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04527</link>
<description rdf:parseType="Literal">&lt;p&gt;The infinite horizon setting is widely adopted for problems of reinforcement
learning (RL). These invariably result in stationary policies that are optimal.
In many situations, finite horizon control problems are of interest and for
such problems, the optimal policies are time-varying in general. Another
setting that has become popular in recent times is of Constrained Reinforcement
Learning, where the agent maximizes its rewards while it also aims to satisfy
some given constraint criteria. However, this setting has only been studied in
the context of infinite horizon MDPs where stationary policies are optimal. We
present an algorithm for constrained RL in the Finite Horizon Setting where the
horizon terminates after a fixed (finite) time. We use function approximation
in our algorithm which is essential when the state and action spaces are large
or continuous and use the policy gradient method to find the optimal policy.
The optimal policy that we obtain depends on the stage and so is non-stationary
in general. To the best of our knowledge, our paper presents the first policy
gradient algorithm for the finite horizon setting with constraints. We show the
convergence of our algorithm to a constrained optimal policy. We also compare
and analyze the performance of our algorithm through experiments and show that
our algorithm performs better than some other well known algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guin_S/0/1/0/all/0/1&quot;&gt;Soumyajit Guin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_S/0/1/0/all/0/1&quot;&gt;Shalabh Bhatnagar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10886">
<title>Backdoor Attack and Defense in Federated Generative Adversarial Network-based Medical Image Synthesis. (arXiv:2210.10886v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning-based image synthesis techniques have been applied in
healthcare research for generating medical images to support open research and
augment medical datasets. Training generative adversarial neural networks
(GANs) usually require large amounts of training data. Federated learning (FL)
provides a way of training a central model using distributed data while keeping
raw data locally. However, given that the FL server cannot access the raw data,
it is vulnerable to backdoor attacks, an adversarial by poisoning training
data. Most backdoor attack strategies focus on classification models and
centralized domains. It is still an open question if the existing backdoor
attacks can affect GAN training and, if so, how to defend against the attack in
the FL setting. In this work, we investigate the overlooked issue of backdoor
attacks in federated GANs (FedGANs). The success of this attack is subsequently
determined to be the result of some local discriminators overfitting the
poisoned data and corrupting the local GAN equilibrium, which then further
contaminates other clients when averaging the generator&apos;s parameters and yields
high generator loss. Therefore, we proposed FedDetect, an efficient and
effective way of defending against the backdoor attack in the FL setting, which
allows the server to detect the client&apos;s adversarial behavior based on their
losses and block the malicious clients. Our extensive experiments on two
medical datasets with different modalities demonstrate the backdoor attack on
FedGANs can result in synthetic images with low fidelity. After detecting and
suppressing the detected malicious clients using the proposed defense strategy,
we show that FedGANs can synthesize high-quality medical datasets (with labels)
for data augmentation to improve classification models&apos; performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Ruinan Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13601">
<title>Active Learning for Single Neuron Models with Lipschitz Non-Linearities. (arXiv:2210.13601v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13601</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of active learning for single neuron models, also
sometimes called ``ridge functions&apos;&apos;, in the agnostic setting (under
adversarial label noise). Such models have been shown to be broadly effective
in modeling physical phenomena, and for constructing surrogate data-driven
models for partial differential equations.
&lt;/p&gt;
&lt;p&gt;Surprisingly, we show that for a single neuron model with any Lipschitz
non-linearity (such as the ReLU, sigmoid, absolute value, low-degree
polynomial, among others), strong provable approximation guarantees can be
obtained using a well-known active learning strategy for fitting \emph{linear
functions} in the agnostic setting. % -- i.e. for the case when there is no
non-linearity. Namely, we can collect samples via statistical \emph{leverage
score sampling}, which has been shown to be near-optimal in other active
learning scenarios. We support our theoretical results with empirical
simulations showing that our proposed active learning strategy based on
leverage score sampling outperforms (ordinary) uniform sampling when fitting
single neuron models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gajjar_A/0/1/0/all/0/1&quot;&gt;Aarshvi Gajjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegde_C/0/1/0/all/0/1&quot;&gt;Chinmay Hegde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musco_C/0/1/0/all/0/1&quot;&gt;Christopher Musco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13815">
<title>FocusedCleaner: Sanitizing Poisoned Graphs for Robust GNN-based Node Classification. (arXiv:2210.13815v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13815</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) are vulnerable to data poisoning attacks, which
will generate a poisoned graph as the input to the GNN models. We present
FocusedCleaner as a poisoned graph sanitizer to effectively identify the poison
injected by attackers. Specifically, FocusedCleaner provides a sanitation
framework consisting of two modules: bi-level structural learning and victim
node detection. In particular, the structural learning module will reverse the
attack process to steadily sanitize the graph while the detection module
provides ``the focus&quot; -- a narrowed and more accurate search region -- to
structural learning. These two modules will operate in iterations and reinforce
each other to sanitize a poisoned graph step by step. As an important
application, we show that the adversarial robustness of GNNs trained over the
sanitized graph for the node classification task is significantly improved.
Extensive experiments demonstrate that FocusedCleaner outperforms the
state-of-the-art baselines both on poisoned graph sanitation and improving
robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yulin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_L/0/1/0/all/0/1&quot;&gt;Liang Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gaolei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiapu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kai Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15539">
<title>Multi-Target Tracking with Transferable Convolutional Neural Networks. (arXiv:2210.15539v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15539</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-target tracking (MTT) is a classical signal processing task, where the
goal is to estimate the states of an unknown number of moving targets from
noisy sensor measurements. In this paper, we revisit MTT from a deep learning
perspective and propose a convolutional neural network (CNN) architecture to
tackle it. We represent the target states and sensor measurements as images and
recast the problem as an image-to-image prediction task. Then we train a fully
convolutional model at small tracking areas and transfer it to much larger
areas with numerous targets and sensors. This transfer learning approach
enables MTT at a large scale and is also theoretically supported by our novel
analysis that bounds the generalization error. In practice, the proposed
transferable CNN architecture outperforms random finite set filters on the MTT
task with 10 targets and transfers without re-training to a larger MTT task
with 250 targets with a 29% performance improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Owerko_D/0/1/0/all/0/1&quot;&gt;Damian Owerko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanatsoulis_C/0/1/0/all/0/1&quot;&gt;Charilaos I. Kanatsoulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bondarchuk_J/0/1/0/all/0/1&quot;&gt;Jennifer Bondarchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bucci_D/0/1/0/all/0/1&quot;&gt;Donald J. Bucci Jr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ribeiro_A/0/1/0/all/0/1&quot;&gt;Alejandro Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17011">
<title>A picture of the space of typical learnable tasks. (arXiv:2210.17011v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17011</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop information geometric techniques to understand the representations
learned by deep networks when they are trained on different tasks using
supervised, meta-, semi-supervised and contrastive learning. We shed light on
the following phenomena that relate to the structure of the space of tasks: (1)
the manifold of probabilistic models trained on different tasks using different
representation learning methods is effectively low-dimensional; (2) supervised
learning on one task results in a surprising amount of progress even on
seemingly dissimilar tasks; progress on other tasks is larger if the training
task has diverse classes; (3) the structure of the space of tasks indicated by
our analysis is consistent with parts of the Wordnet phylogenetic tree; (4)
episodic meta-learning algorithms and supervised learning traverse different
trajectories during training but they fit similar models eventually; (5)
contrastive and semi-supervised learning methods traverse trajectories similar
to those of supervised learning. We use classification tasks constructed from
the CIFAR-10 and Imagenet datasets to study these phenomena.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1&quot;&gt;Rahul Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jialin Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Griniasty_I/0/1/0/all/0/1&quot;&gt;Itay Griniasty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rubing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teoh_H/0/1/0/all/0/1&quot;&gt;Han Kheng Teoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Transtrum_M/0/1/0/all/0/1&quot;&gt;Mark Transtrum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sethna_J/0/1/0/all/0/1&quot;&gt;James P. Sethna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.02641">
<title>Graph Neural Networks on SPD Manifolds for Motor Imagery Classification: A Perspective from the Time-Frequency Analysis. (arXiv:2211.02641v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2211.02641</link>
<description rdf:parseType="Literal">&lt;p&gt;The motor imagery (MI) classification has been a prominent research topic in
brain-computer interfaces based on electroencephalography (EEG). Over the past
few decades, the performance of MI-EEG classifiers has gradually improved. In
this study, we enhance the geometric deep learning classifier for MI-EEG
classification from the perspective of time-frequency analysis, introducing a
new architecture called Graph-CSPNet. We refer to this category of classifiers
as geometric methods, emphasizing their rich background in differential
geometry induced by signal covariance matrices. Graph-CSPNet utilizes a novel
SPD matrix-valued graph convolutional techniques to capture the EEG features in
the time-frequency domain, providing greater flexibility in signal segmentation
and capturing localized fluctuations. To evaluate the effectiveness of
Graph-CSPNet, we employ five commonly-used publicly available MI-EEG datasets,
achieving near-optimal classification accuracies in nine out of eleven
scenarios. The Python repository can be found at
https://github.com/GeometricBCI/Tensor-CSPNet-and-Graph-CSPNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Ce Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guan_C/0/1/0/all/0/1&quot;&gt;Cuntai Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10773">
<title>A Two-Stage Active Learning Algorithm for $k$-Nearest Neighbors. (arXiv:2211.10773v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10773</link>
<description rdf:parseType="Literal">&lt;p&gt;$k$-nearest neighbor classification is a popular non-parametric method
because of desirable properties like automatic adaption to distributional scale
changes. Unfortunately, it has thus far proved difficult to design active
learning strategies for the training of local voting-based classifiers that
naturally retain these desirable properties, and hence active learning
strategies for $k$-nearest neighbor classification have been conspicuously
missing from the literature. In this work, we introduce a simple and intuitive
active learning algorithm for the training of $k$-nearest neighbor classifiers,
the first in the literature which retains the concept of the $k$-nearest
neighbor vote at prediction time. We provide consistency guarantees for a
modified $k$-nearest neighbors classifier trained on samples acquired via our
scheme, and show that when the conditional probability function
$\mathbb{P}(Y=y|X=x)$ is sufficiently smooth and the Tsybakov noise condition
holds, our actively trained classifiers converge to the Bayes optimal
classifier at a faster asymptotic rate than passively trained $k$-nearest
neighbor classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rittler_N/0/1/0/all/0/1&quot;&gt;Nick Rittler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1&quot;&gt;Kamalika Chaudhuri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14400">
<title>Optimal Approximation Rates for Deep ReLU Neural Networks on Sobolev and Besov Spaces. (arXiv:2211.14400v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14400</link>
<description rdf:parseType="Literal">&lt;p&gt;Let $\Omega = [0,1]^d$ be the unit cube in $\mathbb{R}^d$. We study the
problem of how efficiently, in terms of the number of parameters, deep neural
networks with the ReLU activation function can approximate functions in the
Sobolev spaces $W^s(L_q(\Omega))$ and Besov spaces $B^s_r(L_q(\Omega))$, with
error measured in the $L_p(\Omega)$ norm. This problem is important when
studying the application of neural networks in a variety of fields, including
scientific computing and signal processing, and has previously been completely
solved only when $p=q=\infty$. Our contribution is to provide a complete
solution for all $1\leq p,q\leq \infty$ and $s &amp;gt; 0$, including asymptotically
matching upper and lower bounds. The key technical tool is a novel
bit-extraction technique which gives an optimal encoding of sparse vectors.
This enables us to obtain sharp upper bounds in the non-linear regime where $p
&amp;gt; q$. We also provide a novel method for deriving $L_p$-approximation lower
bounds based upon VC-dimension when $p &amp;lt; \infty$. Our results show that very
deep ReLU networks significantly outperform classical methods of approximation
in terms of the number of parameters, but that this comes at the cost of
parameters which are not encodable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Siegel_J/0/1/0/all/0/1&quot;&gt;Jonathan W. Siegel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16110">
<title>PAC-Bayes Bounds for Bandit Problems: A Survey and Experimental Comparison. (arXiv:2211.16110v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16110</link>
<description rdf:parseType="Literal">&lt;p&gt;PAC-Bayes has recently re-emerged as an effective theory with which one can
derive principled learning algorithms with tight performance guarantees.
However, applications of PAC-Bayes to bandit problems are relatively rare,
which is a great misfortune. Many decision-making problems in healthcare,
finance and natural sciences can be modelled as bandit problems. In many of
these applications, principled algorithms with strong performance guarantees
would be very much appreciated. This survey provides an overview of PAC-Bayes
bounds for bandit problems and an experimental comparison of these bounds. On
the one hand, we found that PAC-Bayes bounds are a useful tool for designing
offline bandit algorithms with performance guarantees. In our experiments, a
PAC-Bayesian offline contextual bandit algorithm was able to learn randomised
neural network polices with competitive expected reward and non-vacuous
performance guarantees. On the other hand, the PAC-Bayesian online bandit
algorithms that we tested had loose cumulative regret bounds. We conclude by
discussing some topics for future work on PAC-Bayesian bandit algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flynn_H/0/1/0/all/0/1&quot;&gt;Hamish Flynn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reeb_D/0/1/0/all/0/1&quot;&gt;David Reeb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kandemir_M/0/1/0/all/0/1&quot;&gt;Melih Kandemir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.16162">
<title>Scalable Hierarchical Over-the-Air Federated Learning. (arXiv:2211.16162v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2211.16162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose a communication-efficient hierarchical federated
learning algorithm for distributed setups including core servers and multiple
edge servers with clusters of devices. Assuming different learning tasks,
clusters with a same task collaborate. To implement the algorithm over wireless
links, we propose a scalable clustered over-the-air aggregation scheme for the
uplink with a bandwidth-limited broadcast scheme for the downlink that requires
only a single resource block for each algorithm iteration, independent of the
number of edge servers and devices. This setup is faced with interference of
devices in the uplink and interference of edge servers in the downlink that are
to be modeled rigorously. We first develop a spatial model for the setup by
modeling devices as a Poisson cluster process over the edge servers and
quantify uplink and downlink error terms due to the interference. Accordingly,
we present a comprehensive mathematical approach to derive the convergence
bound for the proposed algorithm including any number of collaborating clusters
and provide special cases and design remarks. Finally, we show that despite the
interference and data heterogeneity, the proposed algorithm not only achieves
high learning accuracy for a variety of parameters but also significantly
outperforms the conventional hierarchical learning algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azimi_Abarghouyi_S/0/1/0/all/0/1&quot;&gt;Seyed Mohammad Azimi-Abarghouyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fodor_V/0/1/0/all/0/1&quot;&gt;Viktoria Fodor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01495">
<title>iEnhancer-ELM: improve enhancer identification by extracting position-related multiscale contextual information based on enhancer language models. (arXiv:2212.01495v2 [q-bio.GN] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01495</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: Enhancers are important cis-regulatory elements that regulate a
wide range of biological functions and enhance the transcription of target
genes. Although many feature extraction methods have been proposed to improve
the performance of enhancer identification, they cannot learn position-related
multiscale contextual information from raw DNA sequences.
&lt;/p&gt;
&lt;p&gt;Results: In this article, we propose a novel enhancer identification method
(iEnhancer-ELM) based on BERT-like enhancer language models. iEnhancer-ELM
tokenizes DNA sequences with multi-scale k-mers and extracts contextual
information of different scale k-mers related with their positions via an
multi-head attention mechanism. We first evaluate the performance of different
scale k-mers, then ensemble them to improve the performance of enhancer
identification. The experimental results on two popular benchmark datasets show
that our model outperforms stateof-the-art methods. We further illustrate the
interpretability of iEnhancer-ELM. For a case study, we discover 30 enhancer
motifs via a 3-mer-based model, where 12 of motifs are verified by STREME and
JASPAR, demonstrating our model has a potential ability to unveil the
biological mechanism of enhancer.
&lt;/p&gt;
&lt;p&gt;Availability and implementation: The models and associated code are available
at https://github.com/chen-bioinfo/iEnhancer-ELM
&lt;/p&gt;
&lt;p&gt;Contact: junjiechen@hit.edu.cn
&lt;/p&gt;
&lt;p&gt;Supplementary information: Supplementary data are available at Bioinformatics
Advances online.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhourun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wenhao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiawei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingcai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junjie Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01793">
<title>kHGCN: Tree-likeness Modeling via Continuous and Discrete Curvature Learning. (arXiv:2212.01793v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01793</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence of tree-like structures, encompassing hierarchical structures
and power law distributions, exists extensively in real-world applications,
including recommendation systems, ecosystems, financial networks, social
networks, etc. Recently, the exploitation of hyperbolic space for tree-likeness
modeling has garnered considerable attention owing to its exponential growth
volume. Compared to the flat Euclidean space, the curved hyperbolic space
provides a more amenable and embeddable room, especially for datasets
exhibiting implicit tree-like architectures. However, the intricate nature of
real-world tree-like data presents a considerable challenge, as it frequently
displays a heterogeneous composition of tree-like, flat, and circular regions.
The direct embedding of such heterogeneous structures into a homogeneous
embedding space (i.e., hyperbolic space) inevitably leads to heavy distortions.
To mitigate the aforementioned shortage, this study endeavors to explore the
curvature between discrete structure and continuous learning space, aiming at
encoding the message conveyed by the network topology in the learning process,
thereby improving tree-likeness modeling. To the end, a curvature-aware
hyperbolic graph convolutional neural network, \{kappa}HGCN, is proposed, which
utilizes the curvature to guide message passing and improve long-range
propagation. Extensive experiments on node classification and link prediction
tasks verify the superiority of the proposal as it consistently outperforms
various competitive models by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Menglin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_M/0/1/0/all/0/1&quot;&gt;Min Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lujia Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+King_I/0/1/0/all/0/1&quot;&gt;Irwin King&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04471">
<title>Learning Quantum Processes and Hamiltonians via the Pauli Transfer Matrix. (arXiv:2212.04471v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04471</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning about physical systems from quantum-enhanced experiments, relying on
a quantum memory and quantum processing, can outperform learning from
experiments in which only classical memory and processing are available.
Whereas quantum advantages have been established for a variety of state
learning tasks, quantum process learning allows for comparable advantages only
with a careful problem formulation and is less understood. We establish an
exponential quantum advantage for learning an unknown $n$-qubit quantum process
$\mathcal{N}$. We show that a quantum memory allows to efficiently solve the
following tasks: (a) learning the Pauli transfer matrix of an arbitrary
$\mathcal{N}$, (b) predicting expectation values of bounded Pauli-sparse
observables measured on the output of an arbitrary $\mathcal{N}$ upon input of
a Pauli-sparse state, and (c) predicting expectation values of arbitrary
bounded observables measured on the output of an unknown $\mathcal{N}$ with
sparse Pauli transfer matrix upon input of an arbitrary state. With quantum
memory, these tasks can be solved using linearly-in-$n$ many copies of the Choi
state of $\mathcal{N}$, and even time-efficiently in the case of (b). In
contrast, any learner without quantum memory requires exponentially-in-$n$ many
queries, even when querying $\mathcal{N}$ on subsystems of adaptively chosen
states and performing adaptively chosen measurements. In proving this
separation, we extend existing shadow tomography upper and lower bounds from
states to channels via the Choi-Jamiolkowski isomorphism. Moreover, we combine
Pauli transfer matrix learning with polynomial interpolation techniques to
develop a procedure for learning arbitrary Hamiltonians, which may have
non-local all-to-all interactions, from short-time dynamics. Our results
highlight the power of quantum-enhanced experiments for learning highly complex
quantum dynamics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Caro_M/0/1/0/all/0/1&quot;&gt;Matthias C. Caro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07158">
<title>Establishing a stronger baseline for lightweight contrastive models. (arXiv:2212.07158v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07158</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research has reported a performance degradation in self-supervised
contrastive learning for specially designed efficient networks, such as
MobileNet and EfficientNet. A common practice to address this problem is to
introduce a pretrained contrastive teacher model and train the lightweight
networks with distillation signals generated by the teacher. However, it is
time and resource consuming to pretrain a teacher model when it is not
available. In this work, we aim to establish a stronger baseline for
lightweight contrastive models without using a pretrained teacher model.
Specifically, we show that the optimal recipe for efficient models is different
from that of larger models, and using the same training settings as ResNet50,
as previous research does, is inappropriate. Additionally, we observe a common
issu e in contrastive learning where either the positive or negative views can
be noisy, and propose a smoothed version of InfoNCE loss to alleviate this
problem. As a result, we successfully improve the linear evaluation results
from 36.3\% to 62.3\% for MobileNet-V3-Large and from 42.2\% to 65.8\% for
EfficientNet-B0 on ImageNet, closing the accuracy gap to ResNet50 with
$5\times$ fewer parameters. We hope our research will facilitate the usage of
lightweight contrastive models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Wenye Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Y/0/1/0/all/0/1&quot;&gt;Yifeng Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhixiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hai-tao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12532">
<title>Generalization Bounds for Few-Shot Transfer Learning with Pretrained Classifiers. (arXiv:2212.12532v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12532</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the ability of foundation models to learn representations for
classification that are transferable to new, unseen classes. Recent results in
the literature show that representations learned by a single classifier over
many classes are competitive on few-shot learning problems with representations
learned by special-purpose algorithms designed for such problems. We offer a
theoretical explanation for this behavior based on the recently discovered
phenomenon of class-feature-variability collapse, that is, that during the
training of deep classification networks the feature embeddings of samples
belonging to the same class tend to concentrate around their class means. More
specifically, we show that the few-shot error of the learned feature map on new
classes (defined as the classification error of the nearest class-center
classifier using centers learned from a small number of random samples from
each new class) is small in case of class-feature-variability collapse, under
the assumption that the classes are selected independently from a fixed
distribution. This suggests that foundation models can provide feature maps
that are transferable to new downstream tasks, even with very few samples; to
our knowledge, this is the first performance bound for transfer-learning that
is non-vacuous in the few-shot setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galanti_T/0/1/0/all/0/1&quot;&gt;Tomer Galanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gyorgy_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe1;s Gy&amp;#xf6;rgy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marcus Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.01947">
<title>StitchNet: Composing Neural Networks from Pre-Trained Fragments. (arXiv:2301.01947v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.01947</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose StitchNet, a novel neural network creation paradigm that stitches
together fragments (one or more consecutive network layers) from multiple
pre-trained neural networks. StitchNet allows the creation of high-performing
neural networks without the large compute and data requirements needed under
traditional model creation processes via backpropagation training. We leverage
Centered Kernel Alignment (CKA) as a compatibility measure to efficiently guide
the selection of these fragments in composing a network for a given task
tailored to specific accuracy needs and computing resource constraints. We then
show that these fragments can be stitched together to create neural networks
with comparable accuracy to traditionally trained networks at a fraction of
computing resource and data requirements. Finally, we explore a novel
on-the-fly personalized model creation and inference application enabled by
this new paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teerapittayanon_S/0/1/0/all/0/1&quot;&gt;Surat Teerapittayanon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comiter_M/0/1/0/all/0/1&quot;&gt;Marcus Comiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDanel_B/0/1/0/all/0/1&quot;&gt;Brad McDanel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kung_H/0/1/0/all/0/1&quot;&gt;H.T. Kung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07863">
<title>Discover governing differential equations from evolving systems. (arXiv:2301.07863v3 [physics.comp-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07863</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering the governing equations of evolving systems from available
observations is essential and challenging. In this paper, we consider a new
scenario: discovering governing equations from streaming data. Current methods
struggle to discover governing differential equations with considering
measurements as a whole, leading to failure to handle this task. We propose an
online modeling method capable of handling samples one by one sequentially by
modeling streaming data instead of processing the entire dataset. The proposed
method performs well in discovering ordinary differential equations (ODEs) and
partial differential equations (PDEs) from streaming data. Evolving systems are
changing over time, which invariably changes with system status. Thus, finding
the exact change points is critical. The measurement generated from a changed
system is distributed dissimilarly to before; hence, the difference can be
identified by the proposed method. Our proposal is competitive in identifying
the change points and discovering governing differential equations in three
hybrid systems and two switching linear systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12525">
<title>Composer&apos;s Assistant: An Interactive Transformer for Multi-Track MIDI Infilling. (arXiv:2301.12525v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12525</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce Composer&apos;s Assistant, a system for interactive human-computer
composition in the REAPER digital audio workstation. We consider the task of
multi-track MIDI infilling when arbitrary track-measures have been deleted from
a contiguous slice of measures from a MIDI file, and we train a T5-like model
to accomplish this task. Composer&apos;s Assistant consists of this model together
with scripts that enable interaction with the model in REAPER. We conduct
objective and subjective tests of our model. We release our complete system,
consisting of source code, pretrained models, and REAPER scripts. Our models
were trained only on permissively-licensed MIDI files.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malandro_M/0/1/0/all/0/1&quot;&gt;Martin E. Malandro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13192">
<title>Robust empirical risk minimization via Newton&apos;s method. (arXiv:2301.13192v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13192</link>
<description rdf:parseType="Literal">&lt;p&gt;A new variant of Newton&apos;s method for empirical risk minimization is studied,
where at each iteration of the optimization algorithm, the gradient and Hessian
of the objective function are replaced by robust estimators taken from existing
literature on robust mean estimation for multivariate data. After proving a
general theorem about the convergence of successive iterates to a small ball
around the population-level minimizer, consequences of the theory in
generalized linear models are studied when data are generated from Huber&apos;s
epsilon-contamination model and/or heavytailed distributions. An algorithm for
obtaining robust Newton directions based on the conjugate gradient method is
also proposed, which may be more appropriate for high-dimensional settings, and
conjectures about the convergence of the resulting algorithm are offered.
Compared to robust gradient descent, the proposed algorithm enjoys the faster
rates of convergence for successive iterates often achieved by second-order
algorithms for convex problems, i.e., quadratic convergence in a neighborhood
of the optimum, with a stepsize that may be chosen adaptively via backtracking
linesearch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ioannou_E/0/1/0/all/0/1&quot;&gt;Eirini Ioannou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pydi_M/0/1/0/all/0/1&quot;&gt;Muni Sreenivas Pydi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Loh_P/0/1/0/all/0/1&quot;&gt;Po-Ling Loh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02521">
<title>Exploiting Partial Common Information Microstructure for Multi-Modal Brain Tumor Segmentation. (arXiv:2302.02521v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02521</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning with multiple modalities is crucial for automated brain tumor
segmentation from magnetic resonance imaging data. Explicitly optimizing the
common information shared among all modalities (e.g., by maximizing the total
correlation) has been shown to achieve better feature representations and thus
enhance the segmentation performance. However, existing approaches are
oblivious to partial common information shared by subsets of the modalities. In
this paper, we show that identifying such partial common information can
significantly boost the discriminative power of image segmentation models. In
particular, we introduce a novel concept of partial common information mask
(PCI-mask) to provide a fine-grained characterization of what partial common
information is shared by which subsets of the modalities. By solving a masked
correlation maximization and simultaneously learning an optimal PCI-mask, we
identify the latent microstructure of partial common information and leverage
it in a self-attention module to selectively weight different feature
representations in multi-modal data. We implement our proposed framework on the
standard U-Net. Our experimental results on the Multi-modal Brain Tumor
Segmentation Challenge (BraTS) datasets outperform those of state-of-the-art
segmentation baselines, with validation Dice similarity coefficients of 0.920,
0.897, 0.837 for the whole tumor, tumor core, and enhancing tumor on
BraTS-2020.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venkataramani_G/0/1/0/all/0/1&quot;&gt;Guru Venkataramani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06235">
<title>A Simple Zero-shot Prompt Weighting Technique to Improve Prompt Ensembling in Text-Image Models. (arXiv:2302.06235v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06235</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastively trained text-image models have the remarkable ability to
perform zero-shot classification, that is, classifying previously unseen images
into categories that the model has never been explicitly trained to identify.
However, these zero-shot classifiers need prompt engineering to achieve high
accuracy. Prompt engineering typically requires hand-crafting a set of prompts
for individual downstream tasks. In this work, we aim to automate this prompt
engineering and improve zero-shot accuracy through prompt ensembling. In
particular, we ask &quot;Given a large pool of prompts, can we automatically score
the prompts and ensemble those that are most suitable for a particular
downstream dataset, without needing access to labeled validation data?&quot;. We
demonstrate that this is possible. In doing so, we identify several pathologies
in a naive prompt scoring method where the score can be easily overconfident
due to biases in pre-training and test data, and we propose a novel prompt
scoring method that corrects for the biases. Using our proposed scoring method
to create a weighted average prompt ensemble, our method outperforms equal
average ensemble, as well as hand-crafted prompts, on ImageNet, 4 of its
variants, and 11 fine-grained classification benchmarks, all while being fully
automatic, optimization-free, and not requiring access to labeled validation
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allingham_J/0/1/0/all/0/1&quot;&gt;James Urquhart Allingham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Jie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusenberry_M/0/1/0/all/0/1&quot;&gt;Michael W Dusenberry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_X/0/1/0/all/0/1&quot;&gt;Xiuye Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yin Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jeremiah Zhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1&quot;&gt;Balaji Lakshminarayanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06294">
<title>CholecTriplet2022: Show me a tool and tell me the triplet -- an endoscopic vision challenge for surgical action triplet detection. (arXiv:2302.06294v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06294</link>
<description rdf:parseType="Literal">&lt;p&gt;Formalizing surgical activities as triplets of the used instruments, actions
performed, and target anatomies is becoming a gold standard approach for
surgical activity modeling. The benefit is that this formalization helps to
obtain a more detailed understanding of tool-tissue interaction which can be
used to develop better Artificial Intelligence assistance for image-guided
surgery. Earlier efforts and the CholecTriplet challenge introduced in 2021
have put together techniques aimed at recognizing these triplets from surgical
footage. Estimating also the spatial locations of the triplets would offer a
more precise intraoperative context-aware decision support for
computer-assisted intervention. This paper presents the CholecTriplet2022
challenge, which extends surgical action triplet modeling from recognition to
detection. It includes weakly-supervised bounding box localization of every
visible surgical instrument (or tool), as the key actors, and the modeling of
each tool-activity in the form of &amp;lt;instrument, verb, target&amp;gt; triplet. The paper
describes a baseline method and 10 new deep learning algorithms presented at
the challenge to solve the task. It also provides thorough methodological
comparisons of the methods, an in-depth analysis of the obtained results across
multiple metrics, visual and procedural challenges; their significance, and
useful insights for future research directions and applications in surgery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nwoye_C/0/1/0/all/0/1&quot;&gt;Chinedu Innocent Nwoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Saurav Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Murali_A/0/1/0/all/0/1&quot;&gt;Aditya Murali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alapatt_D/0/1/0/all/0/1&quot;&gt;Deepak Alapatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vardazaryan_A/0/1/0/all/0/1&quot;&gt;Armine Vardazaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hajek_J/0/1/0/all/0/1&quot;&gt;Jonas Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reiter_W/0/1/0/all/0/1&quot;&gt;Wolfgang Reiter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yamlahi_A/0/1/0/all/0/1&quot;&gt;Amine Yamlahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Smidt_F/0/1/0/all/0/1&quot;&gt;Finn-Henri Smidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guoyan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oliveira_B/0/1/0/all/0/1&quot;&gt;Bruno Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Torres_H/0/1/0/all/0/1&quot;&gt;Helena R. Torres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kondo_S/0/1/0/all/0/1&quot;&gt;Satoshi Kondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kasai_S/0/1/0/all/0/1&quot;&gt;Satoshi Kasai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Holm_F/0/1/0/all/0/1&quot;&gt;Felix Holm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gui_S/0/1/0/all/0/1&quot;&gt;Shuangchun Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raviteja_S/0/1/0/all/0/1&quot;&gt;Sista Raviteja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sathish_R/0/1/0/all/0/1&quot;&gt;Rachana Sathish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poudel_P/0/1/0/all/0/1&quot;&gt;Pranav Poudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattarai_B/0/1/0/all/0/1&quot;&gt;Binod Bhattarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rui_G/0/1/0/all/0/1&quot;&gt;Guo Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schellenberg_M/0/1/0/all/0/1&quot;&gt;Melanie Schellenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vilaca_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o L. Vila&amp;#xe7;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Czempiel_T/0/1/0/all/0/1&quot;&gt;Tobias Czempiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheet_D/0/1/0/all/0/1&quot;&gt;Debdoot Sheet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thapa_S/0/1/0/all/0/1&quot;&gt;Shrawan Kumar Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Berniker_M/0/1/0/all/0/1&quot;&gt;Max Berniker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Godau_P/0/1/0/all/0/1&quot;&gt;Patrick Godau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Morais_P/0/1/0/all/0/1&quot;&gt;Pedro Morais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Regmi_S/0/1/0/all/0/1&quot;&gt;Sudarshan Regmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Thuy Nuong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fonseca_J/0/1/0/all/0/1&quot;&gt;Jaime Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nolke_J/0/1/0/all/0/1&quot;&gt;Jan-Hinrich N&amp;#xf6;lke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lima_E/0/1/0/all/0/1&quot;&gt;Estev&amp;#xe3;o Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vazquez_E/0/1/0/all/0/1&quot;&gt;Eduard Vazquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_L/0/1/0/all/0/1&quot;&gt;Lena Maier-Hein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mascagni_P/0/1/0/all/0/1&quot;&gt;Pietro Mascagni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Seeliger_B/0/1/0/all/0/1&quot;&gt;Barbara Seeliger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gonzalez_C/0/1/0/all/0/1&quot;&gt;Cristians Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mutter_D/0/1/0/all/0/1&quot;&gt;Didier Mutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06608">
<title>3D-aware Blending with Generative NeRFs. (arXiv:2302.06608v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06608</link>
<description rdf:parseType="Literal">&lt;p&gt;Image blending aims to combine multiple images seamlessly. It remains
challenging for existing 2D-based methods, especially when input images are
misaligned due to differences in 3D camera poses and object shapes. To tackle
these issues, we propose a 3D-aware blending method using generative Neural
Radiance Fields (NeRF), including two key components: 3D-aware alignment and
3D-aware blending. For 3D-aware alignment, we first estimate the camera pose of
the reference image with respect to generative NeRFs and then perform 3D local
alignment for each part. To further leverage 3D information of the generative
NeRF, we propose 3D-aware blending that directly blends images on the NeRF&apos;s
latent representation space, rather than raw pixel space. Collectively, our
method outperforms existing 2D baselines, as validated by extensive
quantitative and qualitative evaluations with FFHQ and AFHQ-Cat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gayoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yunjey Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin-Hwa Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jun-Yan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08942">
<title>PAC-Bayesian Generalization Bounds for Adversarial Generative Models. (arXiv:2302.08942v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08942</link>
<description rdf:parseType="Literal">&lt;p&gt;We extend PAC-Bayesian theory to generative models and develop generalization
bounds for models based on the Wasserstein distance and the total variation
distance. Our first result on the Wasserstein distance assumes the instance
space is bounded, while our second result takes advantage of dimensionality
reduction. Our results naturally apply to Wasserstein GANs and Energy-Based
GANs, and our bounds provide new training objectives for these two. Although
our work is mainly theoretical, we perform numerical experiments showing
non-vacuous generalization bounds for Wasserstein GANs on synthetic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mbacke_S/0/1/0/all/0/1&quot;&gt;Sokhna Diarra Mbacke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clerc_F/0/1/0/all/0/1&quot;&gt;Florence Clerc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Germain_P/0/1/0/all/0/1&quot;&gt;Pascal Germain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09921">
<title>Free-Form Variational Inference for Gaussian Process State-Space Models. (arXiv:2302.09921v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09921</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian process state-space models (GPSSMs) provide a principled and
flexible approach to modeling the dynamics of a latent state, which is observed
at discrete-time points via a likelihood model. However, inference in GPSSMs is
computationally and statistically challenging due to the large number of latent
variables in the model and the strong temporal dependencies between them. In
this paper, we propose a new method for inference in Bayesian GPSSMs, which
overcomes the drawbacks of previous approaches, namely over-simplified
assumptions, and high computational requirements. Our method is based on
free-form variational inference via stochastic gradient Hamiltonian Monte Carlo
within the inducing-variable formalism. Furthermore, by exploiting our proposed
variational distribution, we provide a collapsed extension of our method where
the inducing variables are marginalized analytically. We also showcase results
when combining our framework with particle MCMC methods. We show that, on six
real-world datasets, our approach can learn transition dynamics and latent
states more accurately than competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xuhui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonilla_E/0/1/0/all/0/1&quot;&gt;Edwin V. Bonilla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OKane_T/0/1/0/all/0/1&quot;&gt;Terence J. O&amp;#x27;Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sisson_S/0/1/0/all/0/1&quot;&gt;Scott A. Sisson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10893">
<title>Fair Diffusion: Instructing Text-to-Image Generation Models on Fairness. (arXiv:2302.10893v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10893</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI models have recently achieved astonishing results in quality
and are consequently employed in a fast-growing number of applications.
However, since they are highly data-driven, relying on billion-sized datasets
randomly scraped from the internet, they also suffer from degenerated and
biased human behavior, as we demonstrate. In fact, they may even reinforce such
biases. To not only uncover but also combat these undesired effects, we present
a novel strategy, called Fair Diffusion, to attenuate biases after the
deployment of generative text-to-image models. Specifically, we demonstrate
shifting a bias, based on human instructions, in any direction yielding
arbitrarily new proportions for, e.g., identity groups. As our empirical
evaluation demonstrates, this introduced control enables instructing generative
image models on fairness, with no data filtering and additional training
required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luccioni_S/0/1/0/all/0/1&quot;&gt;Sasha Luccioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12022">
<title>DoG is SGD&apos;s Best Friend: A Parameter-Free Dynamic Step Size Schedule. (arXiv:2302.12022v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12022</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a tuning-free dynamic SGD step size formula, which we call
Distance over Gradients (DoG). The DoG step sizes depend on simple empirical
quantities (distance from the initial point and norms of gradients) and have no
``learning rate&apos;&apos; parameter. Theoretically, we show that a slight variation of
the DoG formula enjoys strong parameter-free convergence guarantees for
stochastic convex optimization assuming only \emph{locally bounded} stochastic
gradients. Empirically, we consider a broad range of vision and language
transfer learning tasks, and show that DoG&apos;s performance is close to that of
SGD with tuned learning rate. We also propose a per-layer variant of DoG that
generally outperforms tuned SGD, approaching the performance of tuned Adam. A
PyTorch implementation is available at https://github.com/formll/dog
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivgi_M/0/1/0/all/0/1&quot;&gt;Maor Ivgi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hinder_O/0/1/0/all/0/1&quot;&gt;Oliver Hinder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carmon_Y/0/1/0/all/0/1&quot;&gt;Yair Carmon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.13262">
<title>Modulated Neural ODEs. (arXiv:2302.13262v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.13262</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural ordinary differential equations (NODEs) have been proven useful for
learning non-linear dynamics of arbitrary trajectories. However, current NODE
methods capture variations across trajectories only via the initial state value
or by auto-regressive encoder updates. In this work, we introduce Modulated
Neural ODEs (MoNODEs), a novel framework that sets apart dynamics states from
underlying static factors of variation and improves the existing NODE methods.
In particular, we introduce $\textit{time-invariant modulator variables}$ that
are learned from the data. We incorporate our proposed framework into four
existing NODE variants. We test MoNODE on oscillating systems, videos and human
walking trajectories, where each trajectory has trajectory-specific modulation.
Our framework consistently improves the existing model ability to generalize to
new dynamic parameterizations and to perform far-horizon forecasting. In
addition, we verify that the proposed modulator variables are informative of
the true unknown factors of variation as measured by $R^2$ scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auzina_I/0/1/0/all/0/1&quot;&gt;Ilze Amanda Auzina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yildiz_C/0/1/0/all/0/1&quot;&gt;&amp;#xc7;a&amp;#x11f;atay Y&amp;#x131;ld&amp;#x131;z&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magliacane_S/0/1/0/all/0/1&quot;&gt;Sara Magliacane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavves_E/0/1/0/all/0/1&quot;&gt;Efstratios Gavves&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.03323">
<title>CleanCLIP: Mitigating Data Poisoning Attacks in Multimodal Contrastive Learning. (arXiv:2303.03323v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.03323</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal contrastive pretraining has been used to train multimodal
representation models, such as CLIP, on large amounts of paired image-text
data. However, previous studies have revealed that such models are vulnerable
to backdoor attacks. Specifically, when trained on backdoored examples, CLIP
learns spurious correlations between the embedded backdoor trigger and the
target label, aligning their representations in the joint embedding space.
Injecting even a small number of poisoned examples, such as 75 examples in 3
million pretraining data, can significantly manipulate the model&apos;s behavior,
making it difficult to detect or unlearn such correlations. To address this
issue, we propose CleanCLIP, a finetuning framework that weakens the learned
spurious associations introduced by backdoor attacks by independently
re-aligning the representations for individual modalities. We demonstrate that
unsupervised finetuning using a combination of multimodal contrastive and
unimodal self-supervised objectives for individual modalities can significantly
reduce the impact of the backdoor attack. Additionally, we show that supervised
finetuning on task-specific labeled image data removes the backdoor trigger
from the CLIP vision encoder. We show empirically that CleanCLIP maintains
model performance on benign examples while erasing a range of backdoor attacks
on multimodal contrastive learning. The code and checkpoints are available at
https://github.com/nishadsinghi/CleanCLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_H/0/1/0/all/0/1&quot;&gt;Hritik Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhi_N/0/1/0/all/0/1&quot;&gt;Nishad Singhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_F/0/1/0/all/0/1&quot;&gt;Fan Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04185">
<title>Gradient-Free Structured Pruning with Unlabeled Data. (arXiv:2303.04185v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04185</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have achieved great success in solving difficult
tasks across many domains, but such success comes with a high computation cost,
and inference latency. As developers and third parties customize these models,
the need to provide efficient inference has increased. Many efforts have
attempted to reduce inference cost through model compression techniques such as
pruning and distillation. However, these techniques either require labeled
data, or are time-consuming as they require the compressed model to be
retrained to regain accuracy. In this paper, we propose a gradient-free
structured pruning framework that uses only unlabeled data. An evaluation on
the GLUE and SQuAD benchmarks using BERT$_{BASE}$ and DistilBERT illustrates
the effectiveness of the proposed approach. By only using the weights of the
pre-trained model and unlabeled data, in a matter of a few minutes on a single
GPU, up to 40% of the original FLOP count can be reduced with less than a 4%
accuracy loss across all tasks considered.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nova_A/0/1/0/all/0/1&quot;&gt;Azade Nova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Hanjun Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuurmans_D/0/1/0/all/0/1&quot;&gt;Dale Schuurmans&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04676">
<title>Considerations on the Theory of Training Models with Differential Privacy. (arXiv:2303.04676v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04676</link>
<description rdf:parseType="Literal">&lt;p&gt;In federated learning collaborative learning takes place by a set of clients
who each want to remain in control of how their local training data is used, in
particular, how can each client&apos;s local training data remain private?
Differential privacy is one method to limit privacy leakage. We provide a
general overview of its framework and provable properties, adopt the more
recent hypothesis based definition called Gaussian DP or $f$-DP, and discuss
Differentially Private Stochastic Gradient Descent (DP-SGD). We stay at a meta
level and attempt intuitive explanations and insights \textit{in this book
chapter}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dijk_M/0/1/0/all/0/1&quot;&gt;Marten van Dijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong Ha Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07679">
<title>Feature representations useful for predicting image memorability. (arXiv:2303.07679v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07679</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction of image memorability has attracted interest in various fields.
Consequently, the prediction accuracy of convolutional neural network (CNN)
models has been approaching the empirical upper bound estimated based on human
consistency. However, identifying which feature representations embedded in CNN
models are responsible for the high memorability prediction accuracy remains an
open question. To tackle this problem, we sought to identify
memorability-related feature representations in CNN models using brain
similarity. Specifically, memorability prediction accuracy and brain similarity
were examined across 16,860 layers in 64 CNN models pretrained for object
recognition. A clear tendency was observed in this comprehensive analysis that
layers with high memorability prediction accuracy had higher brain similarity
with the inferior temporal (IT) cortex, which is the highest stage in the
ventral visual pathway. Furthermore, fine-tuning of the 64 CNN models for
memorability prediction revealed that brain similarity with the IT cortex at
the penultimate layer positively correlated with the memorability prediction
accuracy of the models. This analysis also showed that the best fine-tuned
model provided accuracy comparable to state-of-the-art CNN models developed for
memorability prediction. Overall, the results of this study indicated that the
CNN models&apos; great success in predicting memorability relies on feature
representation acquisition, similar to the IT cortex. This study advances our
understanding of feature representations and their use in predicting image
memorability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Takumi Harada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakai_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Sakai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08691">
<title>Learning to Reconstruct Signals From Binary Measurements. (arXiv:2303.08691v2 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08691</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in unsupervised learning have highlighted the possibility of
learning to reconstruct signals from noisy and incomplete linear measurements
alone. These methods play a key role in medical and scientific imaging and
sensing, where ground truth data is often scarce or difficult to obtain.
However, in practice, measurements are not only noisy and incomplete but also
quantized. Here we explore the extreme case of learning from binary
observations and provide necessary and sufficient conditions on the number of
measurements required for identifying a set of signals from incomplete binary
data. Our results are complementary to existing bounds on signal recovery from
binary measurements. Furthermore, we introduce a novel self-supervised learning
approach, which we name SSBM, that only requires binary data for training. We
demonstrate in a series of experiments with real datasets that SSBM performs on
par with supervised learning and outperforms sparse reconstruction methods with
a fixed wavelet basis by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tachella_J/0/1/0/all/0/1&quot;&gt;Juli&amp;#xe1;n Tachella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jacques_L/0/1/0/all/0/1&quot;&gt;Laurent Jacques&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09522">
<title>P+: Extended Textual Conditioning in Text-to-Image Generation. (arXiv:2303.09522v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09522</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce an Extended Textual Conditioning space in text-to-image models,
referred to as $P+$. This space consists of multiple textual conditions,
derived from per-layer prompts, each corresponding to a layer of the denoising
U-net of the diffusion model.
&lt;/p&gt;
&lt;p&gt;We show that the extended space provides greater disentangling and control
over image synthesis. We further introduce Extended Textual Inversion (XTI),
where the images are inverted into $P+$, and represented by per-layer tokens.
&lt;/p&gt;
&lt;p&gt;We show that XTI is more expressive and precise, and converges faster than
the original Textual Inversion (TI) space. The extended inversion method does
not involve any noticeable trade-off between reconstruction and editability and
induces more regular inversions.
&lt;/p&gt;
&lt;p&gt;We conduct a series of extensive experiments to analyze and understand the
properties of the new space, and to showcase the effectiveness of our method
for personalizing text-to-image models. Furthermore, we utilize the unique
properties of this space to achieve previously unattainable results in
object-style mixing using text-to-image models. Project page:
https://prompt-plus.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1&quot;&gt;Andrey Voynov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1&quot;&gt;Qinghao Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aberman_K/0/1/0/all/0/1&quot;&gt;Kfir Aberman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09705">
<title>Batch Updating of a Posterior Tree Distribution over a Meta-Tree. (arXiv:2303.09705v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09705</link>
<description rdf:parseType="Literal">&lt;p&gt;Previously, we proposed a probabilistic data generation model represented by
an unobservable tree and a sequential updating method to calculate a posterior
distribution over a set of trees. The set is called a meta-tree. In this paper,
we propose a more efficient batch updating method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakahara_Y/0/1/0/all/0/1&quot;&gt;Yuta Nakahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsushima_T/0/1/0/all/0/1&quot;&gt;Toshiyasu Matsushima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10840">
<title>Ref-NeuS: Ambiguity-Reduced Neural Implicit Surface Learning for Multi-View Reconstruction with Reflection. (arXiv:2303.10840v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10840</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural implicit surface learning has shown significant progress in multi-view
3D reconstruction, where an object is represented by multilayer perceptrons
that provide continuous implicit surface representation and view-dependent
radiance. However, current methods often fail to accurately reconstruct
reflective surfaces, leading to severe ambiguity. To overcome this issue, we
propose Ref-NeuS, which aims to reduce ambiguity by attenuating the effect of
reflective surfaces. Specifically, we utilize an anomaly detector to estimate
an explicit reflection score with the guidance of multi-view context to
localize reflective surfaces. Afterward, we design a reflection-aware
photometric loss that adaptively reduces ambiguity by modeling rendered color
as a Gaussian distribution, with the reflection score representing the
variance. We show that together with a reflection direction-dependent radiance,
our model achieves high-quality surface reconstruction on reflective surfaces
and outperforms the state-of-the-arts by a large margin. Besides, our model is
also comparable on general surfaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Wenhang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-Cong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11165">
<title>Computationally Budgeted Continual Learning: What Does Matter?. (arXiv:2303.11165v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11165</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual Learning (CL) aims to sequentially train models on streams of
incoming data that vary in distribution by preserving previous knowledge while
adapting to new data. Current CL literature focuses on restricted access to
previously seen data, while imposing no constraints on the computational budget
for training. This is unreasonable for applications in-the-wild, where systems
are primarily constrained by computational and time budgets, not storage. We
revisit this problem with a large-scale benchmark and analyze the performance
of traditional CL approaches in a compute-constrained setting, where effective
memory samples used in training can be implicitly restricted as a consequence
of limited computation. We conduct experiments evaluating various CL sampling
strategies, distillation losses, and partial fine-tuning on two large-scale
datasets, namely ImageNet2K and Continual Google Landmarks V2 in data
incremental, class incremental, and time incremental settings. Through
extensive experiments amounting to a total of over 1500 GPU-hours, we find
that, under compute-constrained setting, traditional CL approaches, with no
exception, fail to outperform a simple minimal baseline that samples uniformly
from memory. Our conclusions are consistent in a different number of stream
time steps, e.g., 20 to 200, and under several computational budgets. This
suggests that most existing CL methods are particularly too computationally
expensive for realistic budgeted deployment. Code for this project is available
at: https://github.com/drimpossible/BudgetCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhu_A/0/1/0/all/0/1&quot;&gt;Ameya Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammoud_H/0/1/0/all/0/1&quot;&gt;Hasan Abed Al Kader Hammoud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokania_P/0/1/0/all/0/1&quot;&gt;Puneet Dokania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torr_P/0/1/0/all/0/1&quot;&gt;Philip H.S. Torr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_B/0/1/0/all/0/1&quot;&gt;Bernard Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bibi_A/0/1/0/all/0/1&quot;&gt;Adel Bibi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11369">
<title>Bridging Imitation and Online Reinforcement Learning: An Optimistic Tale. (arXiv:2303.11369v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11369</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the following problem: Given an offline
demonstration dataset from an imperfect expert, what is the best way to
leverage it to bootstrap online learning performance in MDPs. We first propose
an Informed Posterior Sampling-based RL (iPSRL) algorithm that uses the offline
dataset, and information about the expert&apos;s behavioral policy used to generate
the offline dataset. Its cumulative Bayesian regret goes down to zero
exponentially fast in N, the offline dataset size if the expert is competent
enough. Since this algorithm is computationally impractical, we then propose
the iRLSVI algorithm that can be seen as a combination of the RLSVI algorithm
for online RL, and imitation learning. Our empirical results show that the
proposed iRLSVI algorithm is able to achieve significant reduction in regret as
compared to two baselines: no offline data, and offline dataset but used
without information about the generative policy. Our algorithm bridges online
RL and imitation learning for the first time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_B/0/1/0/all/0/1&quot;&gt;Botao Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Rahul Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_D/0/1/0/all/0/1&quot;&gt;Dengwang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Z/0/1/0/all/0/1&quot;&gt;Zheng Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17765">
<title>Learning from Similar Linear Representations: Adaptivity, Minimaxity, and Robustness. (arXiv:2303.17765v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17765</link>
<description rdf:parseType="Literal">&lt;p&gt;Representation multi-task learning (MTL) and transfer learning (TL) have
achieved tremendous success in practice. However, the theoretical understanding
of these methods is still lacking. Most existing theoretical works focus on
cases where all tasks share the same representation, and claim that MTL and TL
almost always improve performance. However, as the number of tasks grows,
assuming all tasks share the same representation is unrealistic. Also, this
does not always match empirical findings, which suggest that a shared
representation may not necessarily improve single-task or target-only learning
performance. In this paper, we aim to understand how to learn from tasks with
\textit{similar but not exactly the same} linear representations, while dealing
with outlier tasks. With a known intrinsic dimension, we propose two algorithms
that are \textit{adaptive} to the similarity structure and \textit{robust} to
outlier tasks under both MTL and TL settings. Our algorithms outperform
single-task or target-only learning when representations across tasks are
sufficiently similar and the fraction of outlier tasks is small. Furthermore,
they always perform no worse than single-task learning or target-only learning,
even when the representations are dissimilar. We provide information-theoretic
lower bounds to show that our algorithms are nearly \textit{minimax} optimal in
a large regime. We also propose an algorithm to adapt to the unknown intrinsic
dimension. We conduct two simulation studies to verify our theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuqi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01906">
<title>Torch-Choice: A PyTorch Package for Large-Scale Choice Modelling with Python. (arXiv:2304.01906v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01906</link>
<description rdf:parseType="Literal">&lt;p&gt;The $\texttt{torch-choice}$ is an open-source library for flexible, fast
choice modeling with Python and PyTorch. $\texttt{torch-choice}$ provides a
$\texttt{ChoiceDataset}$ data structure to manage databases flexibly and
memory-efficiently. The paper demonstrates constructing a
$\texttt{ChoiceDataset}$ from databases of various formats and functionalities
of $\texttt{ChoiceDataset}$. The package implements two widely used models,
namely the multinomial logit and nested logit models, and supports
regularization during model estimation. The package incorporates the option to
take advantage of GPUs for estimation, allowing it to scale to massive datasets
while being computationally efficient. Models can be initialized using either
R-style formula strings or Python dictionaries. We conclude with a comparison
of the computational efficiencies of $\texttt{torch-choice}$ and
$\texttt{mlogit}$ in R as (1) the number of observations increases, (2) the
number of covariates increases, and (3) the expansion of item sets. Finally, we
demonstrate the scalability of $\texttt{torch-choice}$ on large-scale datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_T/0/1/0/all/0/1&quot;&gt;Tianyu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanodia_A/0/1/0/all/0/1&quot;&gt;Ayush Kanodia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athey_S/0/1/0/all/0/1&quot;&gt;Susan Athey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.03935">
<title>Last-Layer Fairness Fine-tuning is Simple and Effective for Neural Networks. (arXiv:2304.03935v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.03935</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning has been deployed ubiquitously across applications in
modern data science, algorithmic fairness has become a great concern. Among
them, imposing fairness constraints during learning, i.e. in-processing fair
training, has been a popular type of training method because they don&apos;t require
accessing sensitive attributes during test time in contrast to post-processing
methods. While this has been extensively studied in classical machine learning
models, their impact on deep neural networks remains unclear. Recent research
has shown that adding fairness constraints to the objective function leads to
severe over-fitting to fairness criteria in large models, and how to solve this
challenge is an important open question. To tackle this, we leverage the wisdom
and power of pre-training and fine-tuning and develop a simple but novel
framework to train fair neural networks in an efficient and inexpensive way --
last-layer fine-tuning alone can effectively promote fairness in deep neural
networks. This framework offers valuable insights into representation learning
for training fair neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuzhen Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_T/0/1/0/all/0/1&quot;&gt;Ting Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_J/0/1/0/all/0/1&quot;&gt;James Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04391">
<title>CAFIN: Centrality Aware Fairness inducing IN-processing for Unsupervised Representation Learning on Graphs. (arXiv:2304.04391v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04391</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Representation Learning on graphs is gaining traction due to the
increasing abundance of unlabelled network data and the compactness, richness,
and usefulness of the representations generated. In this context, the need to
consider fairness and bias constraints while generating the representations has
been well-motivated and studied to some extent in prior works. One major
limitation of most of the prior works in this setting is that they do not aim
to address the bias generated due to connectivity patterns in the graphs, such
as varied node centrality, which leads to a disproportionate performance across
nodes. In our work, we aim to address this issue of mitigating bias due to
inherent graph structure in an unsupervised setting. To this end, we propose
CAFIN, a centrality-aware fairness-inducing framework that leverages the
structural information of graphs to tune the representations generated by
existing frameworks. We deploy it on GraphSAGE (a popular framework in this
domain) and showcase its efficacy on two downstream tasks - Node Classification
and Link Prediction. Empirically, CAFIN consistently reduces the performance
disparity across popular datasets (varying from 18 to 80% reduction in
performance disparity) from various domains while incurring only a minimal cost
of fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arun_A/0/1/0/all/0/1&quot;&gt;Arvindh Arun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aanegola_A/0/1/0/all/0/1&quot;&gt;Aakash Aanegola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_A/0/1/0/all/0/1&quot;&gt;Amul Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanam_R/0/1/0/all/0/1&quot;&gt;Ramasuri Narayanam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumaraguru_P/0/1/0/all/0/1&quot;&gt;Ponnurangam Kumaraguru&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.13016">
<title>Subsample Ridge Ensembles: Equivalences and Generalized Cross-Validation. (arXiv:2304.13016v2 [math.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2304.13016</link>
<description rdf:parseType="Literal">&lt;p&gt;We study subsampling-based ridge ensembles in the proportional asymptotics
regime, where the feature size grows proportionally with the sample size such
that their ratio converges to a constant. By analyzing the squared prediction
risk of ridge ensembles as a function of the explicit penalty $\lambda$ and the
limiting subsample aspect ratio $\phi_s$ (the ratio of the feature size to the
subsample size), we characterize contours in the $(\lambda, \phi_s)$-plane at
any achievable risk. As a consequence, we prove that the risk of the optimal
full ridgeless ensemble (fitted on all possible subsamples) matches that of the
optimal ridge predictor. In addition, we prove strong uniform consistency of
generalized cross-validation (GCV) over the subsample sizes for estimating the
prediction risk of ridge ensembles. This allows for GCV-based tuning of full
ridgeless ensembles without sample splitting and yields a predictor whose risk
matches optimal ridge risk.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Du_J/0/1/0/all/0/1&quot;&gt;Jin-Hong Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Patil_P/0/1/0/all/0/1&quot;&gt;Pratik Patil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kuchibhotla_A/0/1/0/all/0/1&quot;&gt;Arun Kumar Kuchibhotla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03403">
<title>LLMs for Semi-Automated Data Science: Introducing CAAFE for Context-Aware Automated Feature Engineering. (arXiv:2305.03403v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03403</link>
<description rdf:parseType="Literal">&lt;p&gt;As the field of automated machine learning (AutoML) advances, it becomes
increasingly important to incorporate domain knowledge into these systems. We
present an approach for doing so by harnessing the power of large language
models (LLMs). Specifically, we introduce Context-Aware Automated Feature
Engineering (CAAFE), a feature engineering method for tabular datasets that
utilizes an LLM to iteratively generate additional semantically meaningful
features for tabular datasets based on the description of the dataset. The
method produces both Python code for creating new features and explanations for
the utility of the generated features.
&lt;/p&gt;
&lt;p&gt;Despite being methodologically simple, CAAFE improves performance on 11 out
of 14 datasets - boosting mean ROC AUC performance from 0.798 to 0.822 across
all dataset - similar to the improvement achieved by using a random forest
instead of logistic regression on our datasets.
&lt;/p&gt;
&lt;p&gt;Furthermore, CAAFE is interpretable by providing a textual explanation for
each generated feature. CAAFE paves the way for more extensive semi-automation
in data science tasks and emphasizes the significance of context-aware
solutions that can extend the scope of AutoML systems to semantic AutoML. We
release our $\href{https://github.com/automl/CAAFE}{code}$, a simple
$\href{https://colab.research.google.com/drive/1mCA8xOAJZ4MaB_alZvyARTMjhl6RZf0a}{demo}$
and a $\href{https://pypi.org/project/caafe/}{python\ package}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollmann_N/0/1/0/all/0/1&quot;&gt;Noah Hollmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_S/0/1/0/all/0/1&quot;&gt;Samuel M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05812">
<title>Assessment of Reinforcement Learning Algorithms for Nuclear Power Plant Fuel Optimization. (arXiv:2305.05812v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05812</link>
<description rdf:parseType="Literal">&lt;p&gt;The nuclear fuel loading pattern optimization problem belongs to the class of
large-scale combinatorial optimization. It is also characterized by multiple
objectives and constraints, which makes it impossible to solve explicitly.
Stochastic optimization methodologies including Genetic Algorithms and
Simulated Annealing are used by different nuclear utilities and vendors, but
hand-designed solutions continue to be the prevalent method in the industry. To
improve the state-of-the-art, Deep Reinforcement Learning (RL), in particular,
Proximal Policy Optimization is leveraged. This work presents a first-of-a-kind
approach to utilize deep RL to solve the loading pattern problem and could be
leveraged for any engineering design optimization. This paper is also to our
knowledge the first to propose a study of the behavior of several
hyper-parameters that influence the RL algorithm. The algorithm is highly
dependent on multiple factors such as the shape of the objective function
derived for the core design that behaves as a fudge factor that affects the
stability of the learning. But also, an exploration/exploitation trade-off that
manifests through different parameters such as the number of loading patterns
seen by the agents per episode, the number of samples collected before a policy
update nsteps, and an entropy factor ent_coef that increases the randomness of
the policy during training. We found that RL must be applied similarly to a
Gaussian Process in which the acquisition function is replaced by a
parametrized policy. Then, once an initial set of hyper-parameters is found,
reducing nsteps and ent_coef until no more learning is observed will result in
the highest sample efficiency robustly and stably. This resulted in an economic
benefit of 535,000- 642,000 $/year/plant.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seurin_P/0/1/0/all/0/1&quot;&gt;Paul Seurin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirvan_K/0/1/0/all/0/1&quot;&gt;Koroush Shirvan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06124">
<title>FedDWA: Personalized Federated Learning with Dynamic Weight Adjustment. (arXiv:2305.06124v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06124</link>
<description rdf:parseType="Literal">&lt;p&gt;Different from conventional federated learning, personalized federated
learning (PFL) is able to train a customized model for each individual client
according to its unique requirement. The mainstream approach is to adopt a kind
of weighted aggregation method to generate personalized models, in which
weights are determined by the loss value or model parameters among different
clients. However, such kinds of methods require clients to download others&apos;
models. It not only sheer increases communication traffic but also potentially
infringes data privacy. In this paper, we propose a new PFL algorithm called
\emph{FedDWA (Federated Learning with Dynamic Weight Adjustment)} to address
the above problem, which leverages the parameter server (PS) to compute
personalized aggregation weights based on collected models from clients. In
this way, FedDWA can capture similarities between clients with much less
communication overhead. More specifically, we formulate the PFL problem as an
optimization problem by minimizing the distance between personalized models and
guidance models, so as to customize aggregation weights for each client.
Guidance models are obtained by the local one-step ahead adaptation on
individual clients. Finally, we conduct extensive experiments using five real
datasets and the results demonstrate that FedDWA can significantly reduce the
communication traffic and achieve much higher model accuracy than the
state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiahao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Miao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yipeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12032">
<title>The Waymo Open Sim Agents Challenge. (arXiv:2305.12032v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12032</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation with realistic, interactive agents represents a key task for
autonomous vehicle software development. In this work, we introduce the Waymo
Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to
tackle this task and propose corresponding metrics. The goal of the challenge
is to stimulate the design of realistic simulators that can be used to evaluate
and train a behavior model for autonomous driving. We outline our evaluation
methodology, present results for a number of different baseline simulation
agent methods, and analyze several submissions to the 2023 competition which
ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains
open for submissions and we discuss open problems for the task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_N/0/1/0/all/0/1&quot;&gt;Nico Montali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_J/0/1/0/all/0/1&quot;&gt;John Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mougin_P/0/1/0/all/0/1&quot;&gt;Paul Mougin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuefler_A/0/1/0/all/0/1&quot;&gt;Alex Kuefler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhinehart_N/0/1/0/all/0/1&quot;&gt;Nick Rhinehart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Michelle Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulino_C/0/1/0/all/0/1&quot;&gt;Cole Gulino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emrich_T/0/1/0/all/0/1&quot;&gt;Tristan Emrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zoey Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1&quot;&gt;Brandyn White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1&quot;&gt;Dragomir Anguelov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12695">
<title>Systematic Literature Review on Application of Machine Learning in Continuous Integration. (arXiv:2305.12695v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12695</link>
<description rdf:parseType="Literal">&lt;p&gt;This research conducted a systematic review of the literature on machine
learning (ML)-based methods in the context of Continuous Integration (CI) over
the past 22 years. The study aimed to identify and describe the techniques used
in ML-based solutions for CI and analyzed various aspects such as data
engineering, feature engineering, hyper-parameter tuning, ML models, evaluation
methods, and metrics. In this paper, we have depicted the phases of CI testing,
the connection between them, and the employed techniques in training the ML
method phases. We presented nine types of data sources and four taken steps in
the selected studies for preparing the data. Also, we identified four feature
types and nine subsets of data features through thematic analysis of the
selected studies. Besides, five methods for selecting and tuning the
hyper-parameters are shown. In addition, we summarised the evaluation methods
used in the literature and identified fifteen different metrics. The most
commonly used evaluation methods were found to be precision, recall, and
F1-score, and we have also identified five methods for evaluating the
performance of trained ML models. Finally, we have presented the relationship
between ML model types, performance measurements, and CI phases. The study
provides valuable insights for researchers and practitioners interested in
ML-based methods in CI and emphasizes the need for further research in this
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arani_A/0/1/0/all/0/1&quot;&gt;Ali Kazemi Arani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Triet Huynh Minh Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahedi_M/0/1/0/all/0/1&quot;&gt;Mansooreh Zahedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babar_M/0/1/0/all/0/1&quot;&gt;Muhammad Ali Babar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14442">
<title>Optimal Preconditioning and Fisher Adaptive Langevin Sampling. (arXiv:2305.14442v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14442</link>
<description rdf:parseType="Literal">&lt;p&gt;We define an optimal preconditioning for the Langevin diffusion by
analytically optimizing the expected squared jumped distance. This yields as
the optimal preconditioning an inverse Fisher information covariance matrix,
where the covariance matrix is computed as the outer product of log target
gradients averaged under the target. We apply this result to the Metropolis
adjusted Langevin algorithm (MALA) and derive a computationally efficient
adaptive MCMC scheme that learns the preconditioning from the history of
gradients produced as the algorithm runs. We show in several experiments that
the proposed algorithm is very robust in high dimensions and significantly
outperforms other methods, including a closely related adaptive MALA scheme
that learns the preconditioning with standard adaptive MCMC as well as the
position-dependent Riemannian manifold MALA sampler.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Titsias_M/0/1/0/all/0/1&quot;&gt;Michalis K. Titsias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16148">
<title>Leveraging Human Feedback to Evolve and Discover Novel Emergent Behaviors in Robot Swarms. (arXiv:2305.16148v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16148</link>
<description rdf:parseType="Literal">&lt;p&gt;Robot swarms often exhibit emergent behaviors that are fascinating to
observe; however, it is often difficult to predict what swarm behaviors can
emerge under a given set of agent capabilities. We seek to efficiently leverage
human input to automatically discover a taxonomy of collective behaviors that
can emerge from a particular multi-agent system, without requiring the human to
know beforehand what behaviors are interesting or even possible. Our proposed
approach adapts to user preferences by learning a similarity space over swarm
collective behaviors using self-supervised learning and human-in-the-loop
queries. We combine our learned similarity metric with novelty search and
clustering to explore and categorize the space of possible swarm behaviors. We
also propose several general-purpose heuristics that improve the efficiency of
our novelty search by prioritizing robot controllers that are likely to lead to
interesting emergent behaviors. We test our approach in simulation on two robot
capability models and show that our methods consistently discover a richer set
of emergent behaviors than prior work. Code, videos, and datasets are available
at https://sites.google.com/view/evolving-novel-swarms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattson_C/0/1/0/all/0/1&quot;&gt;Connor Mattson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_D/0/1/0/all/0/1&quot;&gt;Daniel S. Brown&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18465">
<title>Federated Learning of Gboard Language Models with Differential Privacy. (arXiv:2305.18465v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18465</link>
<description rdf:parseType="Literal">&lt;p&gt;We train language models (LMs) with federated learning (FL) and differential
privacy (DP) in the Google Keyboard (Gboard). We apply the
DP-Follow-the-Regularized-Leader (DP-FTRL)~\citep{kairouz21b} algorithm to
achieve meaningfully formal DP guarantees without requiring uniform sampling of
client devices. To provide favorable privacy-utility trade-offs, we introduce a
new client participation criterion and discuss the implication of its
configuration in large scale systems. We show how quantile-based clip
estimation~\citep{andrew2019differentially} can be combined with DP-FTRL to
adaptively choose the clip norm during training or reduce the hyperparameter
tuning in preparation for training. With the help of pretraining on public
data, we train and deploy more than twenty Gboard LMs that achieve high utility
and $\rho-$zCDP privacy guarantees with $\rho \in (0.2, 2)$, with two models
additionally trained with secure aggregation~\citep{bonawitz2017practical}. We
are happy to announce that all the next word prediction neural network LMs in
Gboard now have DP guarantees, and all future launches of Gboard neural network
LMs will require DP guarantees. We summarize our experience and provide
concrete suggestions on DP training for practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andrew_G/0/1/0/all/0/1&quot;&gt;Galen Andrew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choquette_Choo_C/0/1/0/all/0/1&quot;&gt;Christopher A. Choquette-Choo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kairouz_P/0/1/0/all/0/1&quot;&gt;Peter Kairouz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMahan_H/0/1/0/all/0/1&quot;&gt;H. Brendan McMahan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosenstock_J/0/1/0/all/0/1&quot;&gt;Jesse Rosenstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanbo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18624">
<title>W-procer: Weighted Prototypical Contrastive Learning for Medical Few-Shot Named Entity Recognition. (arXiv:2305.18624v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18624</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has become a popular solution for few-shot Name Entity
Recognization (NER). The conventional configuration strives to reduce the
distance between tokens with the same labels and increase the distance between
tokens with different labels. The effect of this setup may, however, in the
medical domain, there are a lot of entities annotated as OUTSIDE (O), and they
are undesirably pushed apart to other entities that are not labeled as OUTSIDE
(O) by the current contrastive learning method end up with a noisy prototype
for the semantic representation of the label, though there are many OUTSIDE (O)
labeled entities are relevant to the labeled entities. To address this
challenge, we propose a novel method named Weighted Prototypical Contrastive
Learning for Medical Few Shot Named Entity Recognization (W-PROCER). Our
approach primarily revolves around constructing the prototype-based contractive
loss and weighting network. These components play a crucial role in assisting
the model in differentiating the negative samples from OUTSIDE (O) tokens and
enhancing the discrimination ability of contrastive learning. Experimental
results show that our proposed W-PROCER framework significantly outperforms the
strong baselines on the three medical benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_J/0/1/0/all/0/1&quot;&gt;Jeremy Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Huixue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_H/0/1/0/all/0/1&quot;&gt;Huaiyuan Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19779">
<title>Deep learning and MCMC with aggVAE for shifting administrative boundaries: mapping malaria prevalence in Kenya. (arXiv:2305.19779v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19779</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-based disease mapping remains a fundamental policy-informing tool in
the fields of public health and disease surveillance. Hierarchical Bayesian
models have emerged as the state-of-the-art approach for disease mapping since
they are able to both capture structure in the data and robustly characterise
uncertainty. When working with areal data, e.g.~aggregates at the
administrative unit level such as district or province, current models rely on
the adjacency structure of areal units to account for spatial correlations and
perform shrinkage. The goal of disease surveillance systems is to track disease
outcomes over time. This task is especially challenging in crisis situations
which often lead to redrawn administrative boundaries, meaning that data
collected before and after the crisis are no longer directly comparable.
Moreover, the adjacency-based approach ignores the continuous nature of spatial
processes and cannot solve the change-of-support problem, i.e.~when estimates
are required to be produced at different administrative levels or levels of
aggregation. We present a novel, practical, and easy to implement solution to
solve these problems relying on a methodology combining deep generative
modelling and fully Bayesian inference: we build on the recently proposed
PriorVAE method able to encode spatial priors over small areas with variational
autoencoders by encoding aggregates over administrative units. We map malaria
prevalence in Kenya, a country in which administrative boundaries changed in
2010.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semenova_E/0/1/0/all/0/1&quot;&gt;Elizaveta Semenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_S/0/1/0/all/0/1&quot;&gt;Swapnil Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_S/0/1/0/all/0/1&quot;&gt;Samir Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flaxman_S/0/1/0/all/0/1&quot;&gt;Seth Flaxman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unwin_H/0/1/0/all/0/1&quot;&gt;H Juliette T Unwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01007">
<title>Towards Fair Disentangled Online Learning for Changing Environments. (arXiv:2306.01007v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01007</link>
<description rdf:parseType="Literal">&lt;p&gt;In the problem of online learning for changing environments, data are
sequentially received one after another over time, and their distribution
assumptions may vary frequently. Although existing methods demonstrate the
effectiveness of their learning algorithms by providing a tight bound on either
dynamic regret or adaptive regret, most of them completely ignore learning with
model fairness, defined as the statistical parity across different
sub-population (e.g., race and gender). Another drawback is that when adapting
to a new environment, an online learner needs to update model parameters with a
global change, which is costly and inefficient. Inspired by the sparse
mechanism shift hypothesis, we claim that changing environments in online
learning can be attributed to partial changes in learned parameters that are
specific to environments and the rest remain invariant to changing
environments. To this end, in this paper, we propose a novel algorithm under
the assumption that data collected at each time can be disentangled with two
representations, an environment-invariant semantic factor and an
environment-specific variation factor. The semantic factor is further used for
fair prediction under a group fairness constraint. To evaluate the sequence of
model parameters generated by the learner, a novel regret is proposed in which
it takes a mixed form of dynamic and static regret metrics followed by a
fairness-aware long-term constraint. The detailed analysis provides theoretical
guarantees for loss regret and violation of cumulative fairness constraints.
Empirical evaluations on real-world datasets demonstrate our proposed method
sequentially outperforms baseline methods in model accuracy and fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_F/0/1/0/all/0/1&quot;&gt;Feng Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xintao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_L/0/1/0/all/0/1&quot;&gt;Latifur Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grant_C/0/1/0/all/0/1&quot;&gt;Christan Grant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01359">
<title>DWT-CompCNN: Deep Image Classification Network for High Throughput JPEG 2000 Compressed Documents. (arXiv:2306.01359v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01359</link>
<description rdf:parseType="Literal">&lt;p&gt;For any digital application with document images such as retrieval, the
classification of document images becomes an essential stage. Conventionally
for the purpose, the full versions of the documents, that is the uncompressed
document images make the input dataset, which poses a threat due to the big
volume required to accommodate the full versions of the documents. Therefore,
it would be novel, if the same classification task could be accomplished
directly (with some partial decompression) with the compressed representation
of documents in order to make the whole process computationally more efficient.
In this research work, a novel deep learning model, DWT CompCNN is proposed for
classification of documents that are compressed using High Throughput JPEG 2000
(HTJ2K) algorithm. The proposed DWT-CompCNN comprises of five convolutional
layers with filter sizes of 16, 32, 64, 128, and 256 consecutively for each
increasing layer to improve learning from the wavelet coefficients extracted
from the compressed images. Experiments are performed on two benchmark
datasets- Tobacco-3482 and RVL-CDIP, which demonstrate that the proposed model
is time and space efficient, and also achieves a better classification accuracy
in compressed domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisen_T/0/1/0/all/0/1&quot;&gt;Tejasvee Bisen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirtania_S/0/1/0/all/0/1&quot;&gt;Shashank Kirtania&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagabhushan_P/0/1/0/all/0/1&quot;&gt;P. Nagabhushan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02520">
<title>A Study of Situational Reasoning for Traffic Understanding. (arXiv:2306.02520v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02520</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent Traffic Monitoring (ITMo) technologies hold the potential for
improving road safety/security and for enabling smart city infrastructure.
Understanding traffic situations requires a complex fusion of perceptual
information with domain-specific and causal commonsense knowledge. Whereas
prior work has provided benchmarks and methods for traffic monitoring, it
remains unclear whether models can effectively align these information sources
and reason in novel scenarios. To address this assessment gap, we devise three
novel text-based tasks for situational reasoning in the traffic domain: i)
BDD-QA, which evaluates the ability of Language Models (LMs) to perform
situational decision-making, ii) TV-QA, which assesses LMs&apos; abilities to reason
about complex event causality, and iii) HDT-QA, which evaluates the ability of
models to solve human driving exams. We adopt four knowledge-enhanced methods
that have shown generalization capability across language reasoning tasks in
prior work, based on natural language inference, commonsense knowledge-graph
self-supervision, multi-QA joint training, and dense retrieval of domain
information. We associate each method with a relevant knowledge source,
including knowledge graphs, relevant benchmarks, and driving manuals. In
extensive experiments, we benchmark various knowledge-aware methods against the
three datasets, under zero-shot evaluation; we provide in-depth analyses of
model performance on data partitions and examine model predictions
categorically, to yield useful insights on traffic understanding, given
different background knowledge and reasoning strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiarui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilievski_F/0/1/0/all/0/1&quot;&gt;Filip Ilievski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kaixin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kollaa_A/0/1/0/all/0/1&quot;&gt;Aravinda Kollaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1&quot;&gt;Alessandro Oltramari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02786">
<title>Navigating Explanatory Multiverse Through Counterfactual Path Geometry. (arXiv:2306.02786v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02786</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual explanations are the de facto standard when tasked with
interpreting decisions of (opaque) predictive models. Their generation is often
subject to algorithmic and domain-specific constraints -- such as density-based
feasibility and attribute (im)mutability or directionality of change -- that
aim to maximise their real-life utility. In addition to desiderata with respect
to the counterfactual instance itself, existence of a viable path connecting it
with the factual data point, known as algorithmic recourse, has become an
important technical consideration. While both of these requirements ensure that
the steps of the journey as well as its destination are admissible, current
literature neglects the multiplicity of such counterfactual paths. To address
this shortcoming we introduce the novel concept of explanatory multiverse that
encompasses all the possible counterfactual journeys; we then show how to
navigate, reason about and compare the geometry of these trajectories -- their
affinity, branching, divergence and possible future convergence -- with two
methods: vector spaces and graphs. Implementing this (interactive) explanatory
process grants explainees agency by allowing them to select counterfactuals
based on the properties of the journey leading to them in addition to their
absolute differences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokol_K/0/1/0/all/0/1&quot;&gt;Kacper Sokol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Small_E/0/1/0/all/0/1&quot;&gt;Edward Small&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Y/0/1/0/all/0/1&quot;&gt;Yueqing Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02797">
<title>Modeling Human-like Concept Learning with Bayesian Inference over Natural Language. (arXiv:2306.02797v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02797</link>
<description rdf:parseType="Literal">&lt;p&gt;We model learning of abstract symbolic concepts by performing Bayesian
inference over utterances in natural language. For efficient inference, we use
a large language model as a proposal distribution. We fit a prior to human data
to better model human learners, and evaluate on both generative and logical
concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1&quot;&gt;Kevin Ellis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04904">
<title>An adaptive augmented Lagrangian method for training physics and equality constrained artificial neural networks. (arXiv:2306.04904v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04904</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics and equality constrained artificial neural networks (PECANN) are
grounded in methods of constrained optimization to properly constrain the
solution of partial differential equations (PDEs) with their boundary and
initial conditions and any high-fidelity data that may be available. To this
end, adoption of the augmented Lagrangian method within the PECANN framework is
paramount for learning the solution of PDEs without manually balancing the
individual loss terms in the objective function used for determining the
parameters of the neural network. Generally speaking, ALM combines the merits
of the penalty and Lagrange multiplier methods while avoiding the ill
conditioning and convergence issues associated singly with these methods . In
the present work, we apply our PECANN framework to solve forward and inverse
problems that have an expanded and diverse set of constraints. We show that ALM
with its conventional formulation to update its penalty parameter and Lagrange
multipliers stalls for such challenging problems. To address this issue, we
propose an adaptive ALM in which each constraint is assigned a unique penalty
parameter that evolve adaptively according to a rule inspired by the adaptive
subgradient method. Additionally, we revise our PECANN formulation for improved
computational efficiency and savings which allows for mini-batch training. We
demonstrate the efficacy of our proposed approach by solving several forward
and PDE-constrained inverse problems with noisy data, including simulation of
incompressible fluid flows with a primitive-variables formulation of the
Navier-Stokes equations up to a Reynolds number of 1000.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basir_S/0/1/0/all/0/1&quot;&gt;Shamsulhaq Basir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Senocak_I/0/1/0/all/0/1&quot;&gt;Inanc Senocak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04923">
<title>Unconstrained Online Learning with Unbounded Losses. (arXiv:2306.04923v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04923</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithms for online learning typically require one or more boundedness
assumptions: that the domain is bounded, that the losses are Lipschitz, or
both. In this paper, we develop a new setting for online learning with
unbounded domains and non-Lipschitz losses. For this setting we provide an
algorithm which guarantees $R_{T}(u)\le \tilde
O(G\|u\|\sqrt{T}+L\|u\|^{2}\sqrt{T})$ regret on any problem where the
subgradients satisfy $\|g_{t}\|\le G+L\|w_{t}\|$, and show that this bound is
unimprovable without further assumptions. We leverage this algorithm to develop
new saddle-point optimization algorithms that converge in duality gap in
unbounded domains, even in the absence of meaningful curvature. Finally, we
provide the first algorithm achieving non-trivial dynamic regret in an
unbounded domain for non-Lipschitz losses, as well as a matching lower bound.
The regret of our dynamic regret algorithm automatically improves to a novel
$L^{*}$ bound when the losses are smooth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobsen_A/0/1/0/all/0/1&quot;&gt;Andrew Jacobsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1&quot;&gt;Ashok Cutkosky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06206">
<title>PotatoPestNet: A CTInceptionV3-RS-Based Neural Network for Accurate Identification of Potato Pests. (arXiv:2306.06206v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06206</link>
<description rdf:parseType="Literal">&lt;p&gt;Potatoes are the third-largest food crop globally, but their production
frequently encounters difficulties because of aggressive pest infestations. The
aim of this study is to investigate the various types and characteristics of
these pests and propose an efficient PotatoPestNet AI-based automatic potato
pest identification system. To accomplish this, we curated a reliable dataset
consisting of eight types of potato pests. We leveraged the power of transfer
learning by employing five customized, pre-trained transfer learning models:
CMobileNetV2, CNASLargeNet, CXception, CDenseNet201, and CInceptionV3, in
proposing a robust PotatoPestNet model to accurately classify potato pests. To
improve the models&apos; performance, we applied various augmentation techniques,
incorporated a global average pooling layer, and implemented proper
regularization methods. To further enhance the performance of the models, we
utilized random search (RS) optimization for hyperparameter tuning. This
optimization technique played a significant role in fine-tuning the models and
achieving improved performance. We evaluated the models both visually and
quantitatively, utilizing different evaluation metrics. The robustness of the
models in handling imbalanced datasets was assessed using the Receiver
Operating Characteristic (ROC) curve. Among the models, the Customized Tuned
Inception V3 (CTInceptionV3) model, optimized through random search,
demonstrated outstanding performance. It achieved the highest accuracy (91%),
precision (91%), recall (91%), and F1-score (91%), showcasing its superior
ability to accurately identify and classify potato pests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukder_M/0/1/0/all/0/1&quot;&gt;Md. Simul Hasan Talukder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sulaiman_R/0/1/0/all/0/1&quot;&gt;Rejwan Bin Sulaiman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_M/0/1/0/all/0/1&quot;&gt;Mohammad Raziuddin Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nipun_M/0/1/0/all/0/1&quot;&gt;Musarrat Saberin Nipun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1&quot;&gt;Taminul Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06534">
<title>K-Tensors: Clustering Positive Semi-Definite Matrices. (arXiv:2306.06534v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06534</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel self-consistency clustering algorithm
($K$-Tensors) designed for {partitioning a distribution of}
positive-semidefinite matrices based on their eigenstructures. As positive
semi-definite matrices can be represented as ellipsoids in $\Re^p$, $p \ge 2$,
it is critical to maintain their structural information to perform effective
clustering. However, traditional clustering algorithms {applied to matrices}
often {involve vectorization of} the matrices, resulting in a loss of essential
structural information. To address this issue, we propose a distance metric
{for clustering} that is specifically based on the structural information of
positive semi-definite matrices. This distance metric enables the clustering
algorithm to consider the differences between positive semi-definite matrices
and their projections onto {a} common space spanned by \thadJulyTen{orthonormal
vectors defined from a set of} positive semi-definite matrices. This innovative
approach to clustering positive semi-definite matrices has broad applications
in several domains including financial and biomedical research, such as
analyzing functional connectivity data. By maintaining the structural
information of positive semi-definite matrices, our proposed algorithm promises
to cluster the positive semi-definite matrices in a more meaningful way,
thereby facilitating deeper insights into the underlying data in various
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanchao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarpey_T/0/1/0/all/0/1&quot;&gt;Thaddeus Tarpey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06613">
<title>Parameter-free version of Adaptive Gradient Methods for Strongly-Convex Functions. (arXiv:2306.06613v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06613</link>
<description rdf:parseType="Literal">&lt;p&gt;The optimal learning rate for adaptive gradient methods applied to
{\lambda}-strongly convex functions relies on the parameters {\lambda} and
learning rate {\eta}. In this paper, we adapt a universal algorithm along the
lines of Metagrad, to get rid of this dependence on {\lambda} and {\eta}. The
main idea is to concurrently run multiple experts and combine their predictions
to a master algorithm. This master enjoys O(d log T) regret bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouda_D/0/1/0/all/0/1&quot;&gt;Deepak Gouda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naveed_H/0/1/0/all/0/1&quot;&gt;Hassan Naveed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamath_S/0/1/0/all/0/1&quot;&gt;Salil Kamath&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08044">
<title>Pruning the Way to Reliable Policies: A Multi-Objective Deep Q-Learning Approach to Critical Care. (arXiv:2306.08044v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08044</link>
<description rdf:parseType="Literal">&lt;p&gt;Most medical treatment decisions are sequential in nature. Hence, there is
substantial hope that reinforcement learning may make it possible to formulate
precise data-driven treatment plans. However, a key challenge for most
applications in this field is the sparse nature of primarily mortality-based
reward functions, leading to decreased stability of offline estimates. In this
work, we introduce a deep Q-learning approach able to obtain more reliable
critical care policies. This method integrates relevant but noisy intermediate
biomarker signals into the reward specification, without compromising the
optimization of the main outcome of interest (e.g. patient survival). We
achieve this by first pruning the action set based on all available rewards,
and second training a final model based on the sparse main reward but with a
restricted action set. By disentangling accurate and approximated rewards
through action pruning, potential distortions of the main objective are
minimized, all while enabling the extraction of valuable information from
intermediate signals that can guide the learning process. We evaluate our
method in both off-policy and offline settings using simulated environments and
real health records of patients in intensive care units. Our empirical results
indicate that pruning significantly reduces the size of the action space while
staying mostly consistent with the actions taken by physicians, outperforming
the current state-of-the-art offline reinforcement learning method conservative
Q-learning. Our work is a step towards developing reliable policies by
effectively harnessing the wealth of available information in data-intensive
critical care environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirali_A/0/1/0/all/0/1&quot;&gt;Ali Shirali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schubert_A/0/1/0/all/0/1&quot;&gt;Alexander Schubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaa_A/0/1/0/all/0/1&quot;&gt;Ahmed Alaa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09862">
<title>DoubleAdapt: A Meta-learning Approach to Incremental Learning for Stock Trend Forecasting. (arXiv:2306.09862v2 [q-fin.ST] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09862</link>
<description rdf:parseType="Literal">&lt;p&gt;Stock trend forecasting is a fundamental task of quantitative investment
where precise predictions of price trends are indispensable. As an online
service, stock data continuously arrive over time. It is practical and
efficient to incrementally update the forecast model with the latest data which
may reveal some new patterns recurring in the future stock market. However,
incremental learning for stock trend forecasting still remains under-explored
due to the challenge of distribution shifts (a.k.a. concept drifts). With the
stock market dynamically evolving, the distribution of future data can slightly
or significantly differ from incremental data, hindering the effectiveness of
incremental updates. To address this challenge, we propose DoubleAdapt, an
end-to-end framework with two adapters, which can effectively adapt the data
and the model to mitigate the effects of distribution shifts. Our key insight
is to automatically learn how to adapt stock data into a locally stationary
distribution in favor of profitable updates. Complemented by data adaptation,
we can confidently adapt the model parameters under mitigated distribution
shifts. We cast each incremental learning task as a meta-learning task and
automatically optimize the adapters for desirable data adaptation and parameter
initialization. Experiments on real-world stock datasets demonstrate that
DoubleAdapt achieves state-of-the-art predictive performance and shows
considerable efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lifan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shuming Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-fin/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanyan Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10123">
<title>Dual Node and Edge Fairness-Aware Graph Partition. (arXiv:2306.10123v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;Fair graph partition of social networks is a crucial step toward ensuring
fair and non-discriminatory treatments in unsupervised user analysis. Current
fair partition methods typically consider node balance, a notion pursuing a
proportionally balanced number of nodes from all demographic groups, but ignore
the bias induced by imbalanced edges in each cluster. To address this gap, we
propose a notion edge balance to measure the proportion of edges connecting
different demographic groups in clusters. We analyze the relations between node
balance and edge balance, then with line graph transformations, we propose a
co-embedding framework to learn dual node and edge fairness-aware
representations for graph partition. We validate our framework through several
social network datasets and observe balanced partition in terms of both nodes
and edges along with good utility. Moreover, we demonstrate our fair partition
can be used as pseudo labels to facilitate graph neural networks to behave
fairly in node classification and link prediction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tingwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peizhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongfu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13253">
<title>Predicting Grokking Long Before it Happens: A look into the loss landscape of models which grok. (arXiv:2306.13253v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13253</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on predicting the occurrence of grokking in neural
networks, a phenomenon in which perfect generalization emerges long after signs
of overfitting or memorization are observed. It has been reported that grokking
can only be observed with certain hyper-parameters. This makes it critical to
identify the parameters that lead to grokking. However, since grokking occurs
after a large number of epochs, searching for the hyper-parameters that lead to
it is time-consuming. In this paper, we propose a low-cost method to predict
grokking without training for a large number of epochs. In essence, by studying
the learning curve of the first few epochs, we show that one can predict
whether grokking will occur later on. Specifically, if certain oscillations
occur in the early epochs, one can expect grokking to occur if the model is
trained for a much longer period of time. We propose using the spectral
signature of a learning curve derived by applying the Fourier transform to
quantify the amplitude of low-frequency components to detect the presence of
such oscillations. We also present additional experiments aimed at explaining
the cause of these oscillations and characterizing the loss landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Notsawo_P/0/1/0/all/0/1&quot;&gt;Pascal Jr. Tikeng Notsawo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hattie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pezeshki_M/0/1/0/all/0/1&quot;&gt;Mohammad Pezeshki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumas_G/0/1/0/all/0/1&quot;&gt;Guillaume Dumas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13800">
<title>A First Order Meta Stackelberg Method for Robust Federated Learning. (arXiv:2306.13800v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13800</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous research has shown that federated learning (FL) systems are exposed
to an array of security risks. Despite the proposal of several defensive
strategies, they tend to be non-adaptive and specific to certain types of
attacks, rendering them ineffective against unpredictable or adaptive threats.
This work models adversarial federated learning as a Bayesian Stackelberg
Markov game (BSMG) to capture the defender&apos;s incomplete information of various
attack types. We propose meta-Stackelberg learning (meta-SL), a provably
efficient meta-learning algorithm, to solve the equilibrium strategy in BSMG,
leading to an adaptable FL defense. We demonstrate that meta-SL converges to
the first-order $\varepsilon$-equilibrium point in $O(\varepsilon^{-2})$
gradient iterations, with $O(\varepsilon^{-4})$ samples needed per iteration,
matching the state of the art. Empirical evidence indicates that our
meta-Stackelberg framework performs exceptionally well against potent model
poisoning and backdoor attacks of an uncertain nature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yunian Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Henger Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zizhan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Quanyan Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16772">
<title>Learning from Synthetic Human Group Activities. (arXiv:2306.16772v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;The understanding of complex human interactions and group activities has
garnered attention in human-centric computer vision. However, the advancement
of the related tasks is hindered due to the difficulty of obtaining large-scale
labeled real-world datasets. To mitigate the issue, we propose M3Act, a
multi-view multi-group multi-person human atomic action and group activity data
generator. Powered by the Unity engine, M3Act contains simulation-ready 3D
scenes and human assets, configurable lighting and camera systems, highly
parameterized modular group activities, and a large degree of domain
randomization during the data generation process. Our data generator is capable
of generating large-scale datasets of human activities with multiple
viewpoints, modalities (RGB images, 2D poses, 3D motions), and high-quality
annotations for individual persons and multi-person groups (2D bounding boxes,
instance segmentation masks, individual actions and group activity categories).
Using M3Act, we perform synthetic data pre-training for 2D skeleton-based group
activity recognition and RGB-based multi-person pose tracking. The results
indicate that learning from our synthetic datasets largely improves the model
performances on real-world datasets, with the highest gain of 5.59% and 7.32%
respectively in group and person recognition accuracy on CAD2, as well as an
improvement of 6.63 in MOTP on HiEve. Pre-training with our synthetic data also
leads to faster model convergence on downstream tasks (up to 6.8% faster).
Moreover, M3Act opens new research problems for 3D group activity generation.
We release M3Act3D, an 87.6-hour 3D motion dataset of human activities with
larger group sizes and higher complexity of inter-person interactions than
previous multi-person datasets. We define multiple metrics and propose a
competitive baseline for the novel task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Che-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honglu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1&quot;&gt;Parth Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_A/0/1/0/all/0/1&quot;&gt;Aditya Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Samuel S. Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sejong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1&quot;&gt;Mubbasir Kapadia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00444">
<title>An Adaptive Optimization Approach to Personalized Financial Incentives in Mobile Behavioral Weight Loss Interventions. (arXiv:2307.00444v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00444</link>
<description rdf:parseType="Literal">&lt;p&gt;Obesity is a critical healthcare issue affecting the United States. The least
risky treatments available for obesity are behavioral interventions meant to
promote diet and exercise. Often these interventions contain a mobile component
that allows interventionists to collect participants level data and provide
participants with incentives and goals to promote long term behavioral change.
Recently, there has been interest in using direct financial incentives to
promote behavior change. However, adherence is challenging in these
interventions, as each participant will react differently to different
incentive structure and amounts, leading researchers to consider personalized
interventions. The key challenge for personalization, is that the clinicians do
not know a priori how best to administer incentives to participants, and given
finite intervention budgets how to disburse costly resources efficiently. In
this paper, we consider this challenge of designing personalized weight loss
interventions that use direct financial incentives to motivate weight loss
while remaining within a budget. We create a machine learning approach that is
able to predict how individuals may react to different incentive schedules
within the context of a behavioral intervention. We use this predictive model
in an adaptive framework that over the course of the intervention computes what
incentives to disburse to participants and remain within the study budget. We
provide both theoretical guarantees for our modeling and optimization
approaches as well as demonstrate their performance in a simulated weight loss
study. Our results highlight the cost efficiency and effectiveness of our
personalized intervention design for weight loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qiaomei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gavin_K/0/1/0/all/0/1&quot;&gt;Kara L. Gavin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voils_C/0/1/0/all/0/1&quot;&gt;Corrine I. Voils&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mintz_Y/0/1/0/all/0/1&quot;&gt;Yonatan Mintz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01066">
<title>PIGNet2: A Versatile Deep Learning-based Protein-Ligand Interaction Prediction Model for Binding Affinity Scoring and Virtual Screening. (arXiv:2307.01066v2 [q-bio.BM] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01066</link>
<description rdf:parseType="Literal">&lt;p&gt;Prediction of protein-ligand interactions (PLI) plays a crucial role in drug
discovery as it guides the identification and optimization of molecules that
effectively bind to target proteins. Despite remarkable advances in deep
learning-based PLI prediction, the development of a versatile model capable of
accurately scoring binding affinity and conducting efficient virtual screening
remains a challenge. The main obstacle in achieving this lies in the scarcity
of experimental structure-affinity data, which limits the generalization
ability of existing models. Here, we propose a viable solution to address this
challenge by introducing a novel data augmentation strategy combined with a
physics-informed graph neural network. The model showed significant
improvements in both scoring and screening, outperforming task-specific deep
learning models in various tests including derivative benchmarks, and notably
achieving results comparable to the state-of-the-art performance based on
distance likelihood learning. This demonstrates the potential of this approach
to drug discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seokhyun Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sang-Yeon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Jaechang Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Youn Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02631">
<title>An explainable model to support the decision about the therapy protocol for AML. (arXiv:2307.02631v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02631</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Myeloid Leukemia (AML) is one of the most aggressive types of
hematological neoplasm. To support the specialists&apos; decision about the
appropriate therapy, patients with AML receive a prognostic of outcomes
according to their cytogenetic and molecular characteristics, often divided
into three risk categories: favorable, intermediate, and adverse. However, the
current risk classification has known problems, such as the heterogeneity
between patients of the same risk group and no clear definition of the
intermediate risk category. Moreover, as most patients with AML receive an
intermediate-risk classification, specialists often demand other tests and
analyses, leading to delayed treatment and worsening of the patient&apos;s clinical
condition. This paper presents the data analysis and an explainable
machine-learning model to support the decision about the most appropriate
therapy protocol according to the patient&apos;s survival prediction. In addition to
the prediction model being explainable, the results obtained are promising and
indicate that it is possible to use it to support the specialists&apos; decisions
safely. Most importantly, the findings offered in this study have the potential
to open new avenues of research toward better treatments and prognostic
markers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1&quot;&gt;Jade M. Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_G/0/1/0/all/0/1&quot;&gt;Giovanna A. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_Neto_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o A. Machado-Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1&quot;&gt;Tiago A. Almeida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03093">
<title>Beyond Intuition, a Framework for Applying GPs to Real-World Data. (arXiv:2307.03093v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03093</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaussian Processes (GPs) offer an attractive method for regression over
small, structured and correlated datasets. However, their deployment is
hindered by computational costs and limited guidelines on how to apply GPs
beyond simple low-dimensional datasets. We propose a framework to identify the
suitability of GPs to a given problem and how to set up a robust and
well-specified GP model. The guidelines formalise the decisions of experienced
GP practitioners, with an emphasis on kernel design and options for
computational scalability. The framework is then applied to a case study of
glacier elevation change yielding more accurate results at test time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tazi_K/0/1/0/all/0/1&quot;&gt;Kenza Tazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jihao Andreas Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viljoen_R/0/1/0/all/0/1&quot;&gt;Ross Viljoen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gardner_A/0/1/0/all/0/1&quot;&gt;Alex Gardner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+John_S/0/1/0/all/0/1&quot;&gt;ST John&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_H/0/1/0/all/0/1&quot;&gt;Hong Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_R/0/1/0/all/0/1&quot;&gt;Richard E. Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04205">
<title>Extending the Forward Forward Algorithm. (arXiv:2307.04205v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04205</link>
<description rdf:parseType="Literal">&lt;p&gt;The Forward Forward algorithm, proposed by Geoffrey Hinton in November 2022,
is a novel method for training neural networks as an alternative to
backpropagation. In this project, we replicate Hinton&apos;s experiments on the
MNIST dataset, and subsequently extend the scope of the method with two
significant contributions. First, we establish a baseline performance for the
Forward Forward network on the IMDb movie reviews dataset. As far as we know,
our results on this sentiment analysis task marks the first instance of the
algorithm&apos;s extension beyond computer vision. Second, we introduce a novel
pyramidal optimization strategy for the loss threshold - a hyperparameter
specific to the Forward Forward method. Our pyramidal approach shows that a
good thresholding strategy causes a difference of up to 8% in test error.
Lastly, we perform visualizations of the trained parameters and derived several
significant insights, such as a notably larger (10-20x) mean and variance in
the weights acquired by the Forward Forward network.
&lt;/p&gt;
&lt;p&gt;Repository: https://github.com/Ads-cmu/ForwardForward
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandhi_S/0/1/0/all/0/1&quot;&gt;Saumya Gandhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gala_R/0/1/0/all/0/1&quot;&gt;Ritu Gala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornberg_J/0/1/0/all/0/1&quot;&gt;Jonah Kornberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_A/0/1/0/all/0/1&quot;&gt;Advaith Sridhar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04251">
<title>ChatGPT in the Age of Generative AI and Large Language Models: A Concise Survey. (arXiv:2307.04251v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04251</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT is a large language model (LLM) created by OpenAI that has been
carefully trained on a large amount of data. It has revolutionized the field of
natural language processing (NLP) and has pushed the boundaries of LLM
capabilities. ChatGPT has played a pivotal role in enabling widespread public
interaction with generative artificial intelligence (GAI) on a large scale. It
has also sparked research interest in developing similar technologies and
investigating their applications and implications. In this paper, our primary
goal is to provide a concise survey on the current lines of research on ChatGPT
and its evolution. We considered both the glass box and black box views of
ChatGPT, encompassing the components and foundational elements of the
technology, as well as its applications, impacts, and implications. The glass
box approach focuses on understanding the inner workings of the technology, and
the black box approach embraces it as a complex system, and thus examines its
inputs, outputs, and effects. This paves the way for a comprehensive
exploration of the technology and provides a road map for further research and
experimentation. We also lay out essential foundational literature on LLMs and
GAI in general and their connection with ChatGPT. This overview sheds light on
existing and missing research lines in the emerging field of LLMs, benefiting
both public users and developers. Furthermore, the paper delves into the broad
spectrum of applications and significant concerns in fields such as education,
research, healthcare, finance, etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohamadi_S/0/1/0/all/0/1&quot;&gt;Salman Mohamadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mujtaba_G/0/1/0/all/0/1&quot;&gt;Ghulam Mujtaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doretto_G/0/1/0/all/0/1&quot;&gt;Gianfranco Doretto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adjeroh_D/0/1/0/all/0/1&quot;&gt;Donald A. Adjeroh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04368">
<title>ECS -- an Interactive Tool for Data Quality Assurance. (arXiv:2307.04368v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04368</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing capabilities of machine learning systems and their
potential use in safety-critical systems, ensuring high-quality data is
becoming increasingly important. In this paper we present a novel approach for
the assurance of data quality. For this purpose, the mathematical basics are
first discussed and the approach is presented using multiple examples. This
results in the detection of data points with potentially harmful properties for
the use in safety-critical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sieberichs_C/0/1/0/all/0/1&quot;&gt;Christian Sieberichs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geerkens_S/0/1/0/all/0/1&quot;&gt;Simon Geerkens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braun_A/0/1/0/all/0/1&quot;&gt;Alexander Braun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waschulzik_T/0/1/0/all/0/1&quot;&gt;Thomas Waschulzik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04525">
<title>Cluster-Induced Mask Transformers for Effective Opportunistic Gastric Cancer Screening on Non-contrast CT Scans. (arXiv:2307.04525v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04525</link>
<description rdf:parseType="Literal">&lt;p&gt;Gastric cancer is the third leading cause of cancer-related mortality
worldwide, but no guideline-recommended screening test exists. Existing methods
can be invasive, expensive, and lack sensitivity to identify early-stage
gastric cancer. In this study, we explore the feasibility of using a deep
learning approach on non-contrast CT scans for gastric cancer detection. We
propose a novel cluster-induced Mask Transformer that jointly segments the
tumor and classifies abnormality in a multi-task manner. Our model incorporates
learnable clusters that encode the texture and shape prototypes of gastric
cancer, utilizing self- and cross-attention to interact with convolutional
features. In our experiments, the proposed method achieves a sensitivity of
85.0% and specificity of 92.6% for detecting gastric tumors on a hold-out test
set consisting of 100 patients with cancer and 148 normal. In comparison, two
radiologists have an average sensitivity of 73.5% and specificity of 84.3%. We
also obtain a specificity of 97.7% on an external test set with 903 normal
cases. Our approach performs comparably to established state-of-the-art gastric
cancer screening tools like blood testing and endoscopy, while also being more
sensitive in detecting early-stage cancer. This demonstrates the potential of
our approach as a novel, non-invasive, low-cost, and accurate method for
opportunistic gastric cancer screening.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingze Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yingda Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jiawen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiu_M/0/1/0/all/0/1&quot;&gt;Mingyan Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hexin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dong_B/0/1/0/all/0/1&quot;&gt;Bin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Le Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Ling Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04954">
<title>Hybrid hidden Markov LSTM for short-term traffic flow prediction. (arXiv:2307.04954v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04954</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) methods have outperformed parametric models such as
historical average, ARIMA and variants in predicting traffic variables into
short and near-short future, that are critical for traffic management.
Specifically, recurrent neural network (RNN) and its variants (e.g. long
short-term memory) are designed to retain long-term temporal correlations and
therefore are suitable for modeling sequences. However, multi-regime models
assume the traffic system to evolve through multiple states (say, free-flow,
congestion in traffic) with distinct characteristics, and hence, separate
models are trained to characterize the traffic dynamics within each regime. For
instance, Markov-switching models with a hidden Markov model (HMM) for regime
identification is capable of capturing complex dynamic patterns and
non-stationarity. Interestingly, both HMM and LSTM can be used for modeling an
observation sequence from a set of latent or, hidden state variables. In LSTM,
the latent variable is computed in a deterministic manner from the current
observation and the previous latent variable, while, in HMM, the set of latent
variables is a Markov chain. Inspired by research in natural language
processing, a hybrid hidden Markov-LSTM model that is capable of learning
complementary features in traffic data is proposed for traffic flow prediction.
Results indicate significant performance gains in using hybrid architecture
compared to conventional methods such as Markov switching ARIMA and LSTM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1&quot;&gt;Agnimitra Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Adway Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1&quot;&gt;S. Ilgin Guler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05217">
<title>Supervised Attention Using Homophily in Graph Neural Networks. (arXiv:2307.05217v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05217</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks have become the standard approach for dealing with
learning problems on graphs. Among the different variants of graph neural
networks, graph attention networks (GATs) have been applied with great success
to different tasks. In the GAT model, each node assigns an importance score to
its neighbors using an attention mechanism. However, similar to other graph
neural networks, GATs aggregate messages from nodes that belong to different
classes, and therefore produce node representations that are not well separated
with respect to the different classes, which might hurt their performance. In
this work, to alleviate this problem, we propose a new technique that can be
incorporated into any graph attention model to encourage higher attention
scores between nodes that share the same class label. We evaluate the proposed
method on several node classification datasets demonstrating increased
performance over standard baseline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chatzianastasis_M/0/1/0/all/0/1&quot;&gt;Michail Chatzianastasis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolentzos_G/0/1/0/all/0/1&quot;&gt;Giannis Nikolentzos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1&quot;&gt;Michalis Vazirgiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05358">
<title>Combating Data Imbalances in Federated Semi-supervised Learning with Dual Regulators. (arXiv:2307.05358v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05358</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning has become a popular method to learn from decentralized
heterogeneous data. Federated semi-supervised learning (FSSL) emerges to train
models from a small fraction of labeled data due to label scarcity on
decentralized clients. Existing FSSL methods assume independent and identically
distributed (IID) labeled data across clients and consistent class distribution
between labeled and unlabeled data within a client. This work studies a more
practical and challenging scenario of FSSL, where data distribution is
different not only across clients but also within a client between labeled and
unlabeled data. To address this challenge, we propose a novel FSSL framework
with dual regulators, FedDure.} FedDure lifts the previous assumption with a
coarse-grained regulator (C-reg) and a fine-grained regulator (F-reg): C-reg
regularizes the updating of the local model by tracking the learning effect on
labeled data distribution; F-reg learns an adaptive weighting scheme tailored
for unlabeled instances in each client. We further formulate the client model
training as bi-level optimization that adaptively optimizes the model in the
client with two regulators. Theoretically, we show the convergence guarantee of
the dual regulators. Empirically, we demonstrate that FedDure is superior to
the existing methods across a wide range of settings, notably by more than 11%
on CIFAR-10 and CINIC-10 datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Sikai Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kunlin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jun Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1&quot;&gt;Shuai Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05422">
<title>Differential Analysis of Triggers and Benign Features for Black-Box DNN Backdoor Detection. (arXiv:2307.05422v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05422</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a data-efficient detection method for deep neural
networks against backdoor attacks under a black-box scenario. The proposed
approach is motivated by the intuition that features corresponding to triggers
have a higher influence in determining the backdoored network output than any
other benign features. To quantitatively measure the effects of triggers and
benign features on determining the backdoored network output, we introduce five
metrics. To calculate the five-metric values for a given input, we first
generate several synthetic samples by injecting the input&apos;s partial contents
into clean validation samples. Then, the five metrics are computed by using the
output labels of the corresponding synthetic samples. One contribution of this
work is the use of a tiny clean validation dataset. Having the computed five
metrics, five novelty detectors are trained from the validation dataset. A meta
novelty detector fuses the output of the five trained novelty detectors to
generate a meta confidence score. During online testing, our method determines
if online samples are poisoned or not via assessing their meta confidence
scores output by the meta novelty detector. We show the efficacy of our
methodology through a broad range of backdoor attacks, including ablation
studies and comparison to existing approaches. Our methodology is promising
since the proposed five metrics quantify the inherent differences between clean
and poisoned samples. Additionally, our detection method can be incrementally
improved by appending more metrics that may be proposed to address future
advanced attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Hao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamurthy_P/0/1/0/all/0/1&quot;&gt;Prashanth Krishnamurthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Siddharth Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorrami_F/0/1/0/all/0/1&quot;&gt;Farshad Khorrami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05949">
<title>Newell&apos;s theory based feature transformations for spatio-temporal traffic prediction. (arXiv:2307.05949v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05949</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) models for spatio-temporal traffic flow forecasting employ
convolutional or graph-convolutional filters along with recurrent neural
networks to capture spatial and temporal dependencies in traffic data. These
models, such as CNN-LSTM, utilize traffic flows from neighboring detector
stations to predict flows at a specific location of interest. However, these
models are limited in their ability to capture the broader dynamics of the
traffic system, as they primarily learn features specific to the detector
configuration and traffic characteristics at the target location. Hence, the
transferability of these models to different locations becomes challenging,
particularly when data is unavailable at the new location for model training.
To address this limitation, we propose a traffic flow physics-based feature
transformation for spatio-temporal DL models. This transformation incorporates
Newell&apos;s uncongested and congested-state estimators of traffic flows at the
target locations, enabling the models to learn broader dynamics of the system.
Our methodology is empirically validated using traffic data from two different
locations. The results demonstrate that the proposed feature transformation
improves the models&apos; performance in predicting traffic flows over different
prediction horizons, as indicated by better goodness-of-fit statistics. An
important advantage of our framework is its ability to be transferred to new
locations where data is unavailable. This is achieved by appropriately
accounting for spatial dependencies based on station distances and various
traffic parameters. In contrast, regular DL models are not easily transferable
as their inputs remain fixed. It should be noted that due to data limitations,
we were unable to perform spatial sensitivity analysis, which calls for further
research using simulated data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1&quot;&gt;Agnimitra Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guler_S/0/1/0/all/0/1&quot;&gt;S. Ilgin Guler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06324">
<title>Provably Faster Gradient Descent via Long Steps. (arXiv:2307.06324v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06324</link>
<description rdf:parseType="Literal">&lt;p&gt;This work establishes provably faster convergence rates for gradient descent
via a computer-assisted analysis technique. Our theory allows nonconstant
stepsize policies with frequent long steps potentially violating descent by
analyzing the overall effect of many iterations at once rather than the typical
one-iteration inductions used in most first-order method analyses. We show that
long steps, which may increase the objective value in the short term, lead to
provably faster convergence in the long term. A conjecture towards proving a
faster $O(1/T\log T)$ rate for gradient descent is also motivated along with
simple numerical validation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Grimmer_B/0/1/0/all/0/1&quot;&gt;Benjamin Grimmer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07119">
<title>DataAssist: A Machine Learning Approach to Data Cleaning and Preparation. (arXiv:2307.07119v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07119</link>
<description rdf:parseType="Literal">&lt;p&gt;Current automated machine learning (ML) tools are model-centric, focusing on
model selection and parameter optimization. However, the majority of the time
in data analysis is devoted to data cleaning and wrangling, for which limited
tools are available. Here we present DataAssist, an automated data preparation
and cleaning platform that enhances dataset quality using ML-informed methods.
We show that DataAssist provides a pipeline for exploratory data analysis and
data cleaning, including generating visualization for user-selected variables,
unifying data annotation, suggesting anomaly removal, and preprocessing data.
The exported dataset can be readily integrated with other autoML tools or
user-specified model for downstream analysis. Our data-centric tool is
applicable to a variety of fields, including economics, business, and
forecasting applications saving over 50% time of the time spent on data
cleansing and preparation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_K/0/1/0/all/0/1&quot;&gt;Kartikay Goyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quin Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyle_V/0/1/0/all/0/1&quot;&gt;Vakul Goyle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07246">
<title>Knowledge Boosting: Rethinking Medical Contrastive Vision-Language Pre-Training. (arXiv:2307.07246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07246</link>
<description rdf:parseType="Literal">&lt;p&gt;The foundation models based on pre-training technology have significantly
advanced artificial intelligence from theoretical to practical applications.
These models have facilitated the feasibility of computer-aided diagnosis for
widespread use. Medical contrastive vision-language pre-training, which does
not require human annotations, is an effective approach for guiding
representation learning using description information in diagnostic reports.
However, the effectiveness of pre-training is limited by the large-scale
semantic overlap and shifting problems in medical field. To address these
issues, we propose the Knowledge-Boosting Contrastive Vision-Language
Pre-training framework (KoBo), which integrates clinical knowledge into the
learning of vision-language semantic consistency. The framework uses an
unbiased, open-set sample-wise knowledge representation to measure negative
sample noise and supplement the correspondence between vision-language mutual
information and clinical knowledge. Extensive experiments validate the effect
of our framework on eight tasks including classification, segmentation,
retrieval, and semantic relatedness, achieving comparable or better performance
with the zero-shot or few-shot settings. Our code is open on
https://github.com/ChenXiaoFei-CS/KoBo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaofei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuting He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Cheng Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_R/0/1/0/all/0/1&quot;&gt;Rongjun Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guanyu Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07322">
<title>A Context-Aware Cutting Plane Selection Algorithm for Mixed-Integer Programming. (arXiv:2307.07322v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07322</link>
<description rdf:parseType="Literal">&lt;p&gt;The current cut selection algorithm used in mixed-integer programming solvers
has remained largely unchanged since its creation. In this paper, we propose a
set of new cut scoring measures, cut filtering techniques, and stopping
criteria, extending the current state-of-the-art algorithm and obtaining a 5\%
performance improvement for SCIP over the MIPLIB 2017 benchmark set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Turner_M/0/1/0/all/0/1&quot;&gt;Mark Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Berthold_T/0/1/0/all/0/1&quot;&gt;Timo Berthold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Besancon_M/0/1/0/all/0/1&quot;&gt;Mathieu Besan&amp;#xe7;on&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07264">
<title>On Interpolating Experts and Multi-Armed Bandits. (arXiv:2307.07264v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.07264</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning with expert advice and multi-armed bandit are two classic online
decision problems which differ on how the information is observed in each round
of the game. We study a family of problems interpolating the two. For a vector
$\mathbf{m}=(m_1,\dots,m_K)\in \mathbb{N}^K$, an instance of $\mathbf{m}$-MAB
indicates that the arms are partitioned into $K$ groups and the $i$-th group
contains $m_i$ arms. Once an arm is pulled, the losses of all arms in the same
group are observed. We prove tight minimax regret bounds for $\mathbf{m}$-MAB
and design an optimal PAC algorithm for its pure exploration version,
$\mathbf{m}$-BAI, where the goal is to identify the arm with minimum loss with
as few rounds as possible. We show that the minimax regret of $\mathbf{m}$-MAB
is $\Theta\left(\sqrt{T\sum_{k=1}^K\log (m_k+1)}\right)$ and the minimum number
of pulls for an $(\epsilon,0.05)$-PAC algorithm of $\mathbf{m}$-BAI is
$\Theta\left(\frac{1}{\epsilon^2}\cdot \sum_{k=1}^K\log (m_k+1)\right)$. Both
our upper bounds and lower bounds for $\mathbf{m}$-MAB can be extended to a
more general setting, namely the bandit with graph feedback, in terms of the
clique cover and related graph parameters. As consequences, we obtained tight
minimax regret bounds for several families of feedback graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Houshuang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuchen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chihao Zhang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>