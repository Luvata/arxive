<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14909" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14911" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14920" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14922" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14939" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14993" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15027" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.10926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.14019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.03005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.05859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.14362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.00716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14456" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18285" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05697" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13959" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14084" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.14694">
<title>Standardized Analysis Ready (STAR) data cube for high-resolution Flood mapping using Sentinel-1 data. (arXiv:2311.14694v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14694</link>
<description rdf:parseType="Literal">&lt;p&gt;Floods are one of the most common disasters globally. Flood affects humans in
many ways. Therefore, rapid assessment is needed to assess the effect of floods
and to take early action to support the vulnerable community in time.
Sentinel-1 is one such Earth Observation (EO) mission widely used for mapping
the flooding conditions at a 10m scale. However, various preprocessing steps
are involved before analyses of the Sentinel-1 data. Researchers sometimes
avoid a few necessary corrections since it is time-consuming and complex.
Standardization of the Sentinel-1 data is the need of the hour, specifically
for supporting researchers to use the Standardized Analysis-Ready (STAR) data
cube without experiencing the complexity of the Sentinel-1 data processing. In
the present study, we proposed a workflow to use STAR in Google Earth Engine
(GEE) environment. The Nigeria Flood of 2022 has been used as a case study for
assessing the model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Surajit Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawn_A/0/1/0/all/0/1&quot;&gt;Arpan Dawn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kour_S/0/1/0/all/0/1&quot;&gt;Sneha Kour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Susmita Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14733">
<title>Thinking Outside the Box: Orthogonal Approach to Equalizing Protected Attributes. (arXiv:2311.14733v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14733</link>
<description rdf:parseType="Literal">&lt;p&gt;There is growing concern that the potential of black box AI may exacerbate
health-related disparities and biases such as gender and ethnicity in clinical
decision-making. Biased decisions can arise from data availability and
collection processes, as well as from the underlying confounding effects of the
protected attributes themselves. This work proposes a machine learning-based
orthogonal approach aiming to analyze and suppress the effect of the confounder
through discriminant dimensionality reduction and orthogonalization of the
protected attributes against the primary attribute information. By doing so,
the impact of the protected attributes on disease diagnosis can be realized,
undesirable feature correlations can be mitigated, and the model prediction
performance can be enhanced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiahui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xiaohao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niranjan_M/0/1/0/all/0/1&quot;&gt;Mahesan Niranjan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14746">
<title>All in One: RGB, RGB-D, and RGB-T Salient Object Detection. (arXiv:2311.14746v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14746</link>
<description rdf:parseType="Literal">&lt;p&gt;Salient object detection (SOD) aims to identify the most attractive objects
within an image. Depending on the type of data being detected, SOD can be
categorized into various forms, including RGB, RGB-D (Depth), RGB-T (Thermal)
and light field SOD. Previous researches have focused on saliency detection
with individual data type. If the RGB-D SOD model is forced to detect RGB-T
data it will perform poorly. We propose an innovative model framework that
provides a unified solution for the salient object detection task of three
types of data (RGB, RGB-D, and RGB-T). The three types of data can be handled
in one model (all in one) with the same weight parameters. In this framework,
the three types of data are concatenated in an ordered manner within a single
input batch, and features are extracted using a transformer network. Based on
this framework, we propose an efficient lightweight SOD model, namely AiOSOD,
which can detect any RGB, RGB-D, and RGB-T data with high speed (780FPS for RGB
data, 485FPS for RGB-D or RGB-T data). Notably, with only 6.25M parameters,
AiOSOD achieves excellent performance on RGB, RGB-D, and RGB-T datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xingzhao Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongqiu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dongye_C/0/1/0/all/0/1&quot;&gt;Changlei Dongye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14747">
<title>HOMOE: A Memory-Based and Composition-Aware Framework for Zero-Shot Learning with Hopfield Network and Soft Mixture of Experts. (arXiv:2311.14747v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14747</link>
<description rdf:parseType="Literal">&lt;p&gt;Compositional Zero-Shot Learning (CZSL) has emerged as an essential paradigm
in machine learning, aiming to overcome the constraints of traditional
zero-shot learning by incorporating compositional thinking into its
methodology. Conventional zero-shot learning has difficulty managing unfamiliar
combinations of seen and unseen classes because it depends on pre-defined class
embeddings. In contrast, Compositional Zero-Shot Learning uses the inherent
hierarchies and structural connections among classes, creating new class
representations by combining attributes, components, or other semantic
elements. In our paper, we propose a novel framework that for the first time
combines the Modern Hopfield Network with a Mixture of Experts (HOMOE) to
classify the compositions of previously unseen objects. Specifically, the
Modern Hopfield Network creates a memory that stores label prototypes and
identifies relevant labels for a given input image. Following this, the Mixture
of Expert models integrates the image with the fitting prototype to produce the
final composition classification. Our approach achieves SOTA performance on
several benchmarks, including MIT-States and UT-Zappos. We also examine how
each component contributes to improved generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dat_D/0/1/0/all/0/1&quot;&gt;Do Huu Dat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_P/0/1/0/all/0/1&quot;&gt;Po Yuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tien Hoang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buntine_W/0/1/0/all/0/1&quot;&gt;Wray Buntine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennamoun_M/0/1/0/all/0/1&quot;&gt;Mohammed Bennamoun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14749">
<title>Compositional Zero-shot Learning via Progressive Language-based Observations. (arXiv:2311.14749v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14749</link>
<description rdf:parseType="Literal">&lt;p&gt;Compositional zero-shot learning aims to recognize unseen state-object
compositions by leveraging known primitives (state and object) during training.
However, effectively modeling interactions between primitives and generalizing
knowledge to novel compositions remains a perennial challenge. There are two
key factors: object-conditioned and state-conditioned variance, i.e., the
appearance of states (or objects) can vary significantly when combined with
different objects (or states). For instance, the state &quot;old&quot; can signify a
vintage design for a &quot;car&quot; or an advanced age for a &quot;cat&quot;. In this paper, we
argue that these variances can be mitigated by predicting composition
categories based on pre-observed primitive. To this end, we propose Progressive
Language-based Observations (PLO), which can dynamically determine a better
observation order of primitives. These observations comprise a series of
concepts or languages that allow the model to understand image content in a
step-by-step manner. Specifically, PLO adopts pre-trained vision-language
models (VLMs) to empower the model with observation capabilities. We further
devise two variants: 1) PLO-VLM: a two-step method, where a pre-observing
classifier dynamically determines the observation order of two primitives. 2)
PLO-LLM: a multi-step scheme, which utilizes large language models (LLMs) to
craft composition-specific prompts for step-by-step observing. Extensive
ablations on three challenging datasets demonstrate the superiority of PLO
compared with state-of-the-art methods, affirming its abilities in
compositional recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guikun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14750">
<title>Attribute-Aware Representation Rectification for Generalized Zero-Shot Learning. (arXiv:2311.14750v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14750</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalized Zero-shot Learning (GZSL) has yielded remarkable performance by
designing a series of unbiased visual-semantics mappings, wherein, the
precision relies heavily on the completeness of extracted visual features from
both seen and unseen classes. However, as a common practice in GZSL, the
pre-trained feature extractor may easily exhibit difficulty in capturing
domain-specific traits of the downstream tasks/datasets to provide fine-grained
discriminative features, i.e., domain bias, which hinders the overall
recognition performance, especially for unseen classes. Recent studies
partially address this issue by fine-tuning feature extractors, while may
inevitably incur catastrophic forgetting and overfitting issues. In this paper,
we propose a simple yet effective Attribute-Aware Representation Rectification
framework for GZSL, dubbed $\mathbf{(AR)^{2}}$, to adaptively rectify the
feature extractor to learn novel features while keeping original valuable
features. Specifically, our method consists of two key components, i.e.,
Unseen-Aware Distillation (UAD) and Attribute-Guided Learning (AGL). During
training, UAD exploits the prior knowledge of attribute texts that are shared
by both seen/unseen classes with attention mechanisms to detect and maintain
unseen class-sensitive visual features in a targeted manner, and meanwhile, AGL
aims to steer the model to focus on valuable features and suppress them to fit
noisy elements in the seen classes by attribute-guided representation learning.
Extensive experiments on various benchmark datasets demonstrate the
effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhijie Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xiaocheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_K/0/1/0/all/0/1&quot;&gt;Kang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14757">
<title>PointOBB: Learning Oriented Object Detection via Single Point Supervision. (arXiv:2311.14757v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14757</link>
<description rdf:parseType="Literal">&lt;p&gt;Single point-supervised object detection is gaining attention due to its
cost-effectiveness. However, existing approaches focus on generating horizontal
bounding boxes (HBBs) while ignoring oriented bounding boxes (OBBs) commonly
used for objects in aerial images. This paper proposes PointOBB, the first
single Point-based OBB generation method, for oriented object detection.
PointOBB operates through the collaborative utilization of three distinctive
views: an original view, a resized view, and a rotated/flipped (rot/flp) view.
Upon the original view, we leverage the resized and rot/flp views to build a
scale augmentation module and an angle acquisition module, respectively. In the
former module, a Scale-Sensitive Consistency (SSC) loss is designed to enhance
the deep network&apos;s ability to perceive the object scale. For accurate object
angle predictions, the latter module incorporates self-supervised learning to
predict angles, which is associated with a scale-guided Dense-to-Sparse (DS)
matching strategy for aggregating dense angles corresponding to sparse objects.
The resized and rot/flp views are switched using a progressive multi-view
switching strategy during training to achieve coupled optimization of scale and
angle. Experimental results on the DIOR-R and DOTA-v1.0 datasets demonstrate
that PointOBB achieves promising performance, and significantly outperforms
potential point-supervised baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Junwei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yansheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14758">
<title>Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision. (arXiv:2311.14758v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14758</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapidly increasing demand for oriented object detection (OOD),
recent research involving weakly-supervised detectors for learning rotated box
(RBox) from the horizontal box (HBox) has attracted more and more attention. In
this paper, we explore a more challenging yet label-efficient setting, namely
single point-supervised OOD, and present our approach called Point2RBox.
Specifically, we propose to leverage two principles: 1) Synthetic pattern
knowledge combination: By sampling around each labelled point on the image, we
transfer the object feature to synthetic visual patterns with the known
bounding box to provide the knowledge for box regression. 2) Transform
self-supervision: With a transformed input image (e.g. scaled/rotated), the
output RBoxes are trained to follow the same transformation so that the network
can perceive the relative size/rotation between objects. The detector is
further enhanced by a few devised techniques to cope with peripheral issues,
e.g. the anchor/layer assignment as the size of the object is not available in
our point supervision setting. To our best knowledge, Point2RBox is the first
end-to-end solution for point-supervised OOD. In particular, our method uses a
lightweight paradigm, yet it achieves a competitive performance among
point-supervised alternatives, 41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1&quot;&gt;Yu Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_F/0/1/0/all/0/1&quot;&gt;Feipeng Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14760">
<title>SinSR: Diffusion-Based Image Super-Resolution in a Single Step. (arXiv:2311.14760v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14760</link>
<description rdf:parseType="Literal">&lt;p&gt;While super-resolution (SR) methods based on diffusion models exhibit
promising results, their practical application is hindered by the substantial
number of required inference steps. Recent methods utilize degraded images in
the initial state, thereby shortening the Markov chain. Nevertheless, these
solutions either rely on a precise formulation of the degradation process or
still necessitate a relatively lengthy generation path (e.g., 15 iterations).
To enhance inference speed, we propose a simple yet effective method for
achieving single-step SR generation, named SinSR. Specifically, we first derive
a deterministic sampling process from the most recent state-of-the-art (SOTA)
method for accelerating diffusion-based SR. This allows the mapping between the
input random noise and the generated high-resolution image to be obtained in a
reduced and acceptable number of inference steps during training. We show that
this deterministic mapping can be distilled into a student model that performs
SR within only one inference step. Additionally, we propose a novel
consistency-preserving loss to simultaneously leverage the ground-truth image
during the distillation process, ensuring that the performance of the student
model is not solely bound by the feature manifold of the teacher model,
resulting in further performance improvement. Extensive experiments conducted
on synthetic and real-world datasets demonstrate that the proposed method can
achieve comparable or even superior performance compared to both previous SOTA
methods and the teacher model, in just one sampling step, resulting in a
remarkable up to x10 speedup for inference. Our code will be released at
https://github.com/wyf0912/SinSR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaohui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Lanqing Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_L/0/1/0/all/0/1&quot;&gt;Lap-Pui Chau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1&quot;&gt;Alex C. Kot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bihan Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14762">
<title>The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024. (arXiv:2311.14762v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14762</link>
<description rdf:parseType="Literal">&lt;p&gt;The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024 addresses maritime
computer vision for Unmanned Aerial Vehicles (UAV) and Unmanned Surface
Vehicles (USV). Three challenges categories are considered: (i) UAV-based
Maritime Object Tracking with Re-identification, (ii) USV-based Maritime
Obstacle Segmentation and Detection, (iii) USV-based Maritime Boat Tracking.
The USV-based Maritime Obstacle Segmentation and Detection features three
sub-challenges, including a new embedded challenge addressing efficicent
inference on real-world embedded devices. This report offers a comprehensive
overview of the findings from the challenges. We provide both statistical and
qualitative analyses, evaluating trends from over 195 submissions. All
datasets, evaluation code, and the leaderboard are available to the public at
https://macvi.org/workshop/macvi24.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiefer_B/0/1/0/all/0/1&quot;&gt;Benjamin Kiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zust_L/0/1/0/all/0/1&quot;&gt;Lojze &amp;#x17d;ust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1&quot;&gt;Matej Kristan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pers_J/0/1/0/all/0/1&quot;&gt;Janez Per&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tersek_M/0/1/0/all/0/1&quot;&gt;Matija Ter&amp;#x161;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiliem_A/0/1/0/all/0/1&quot;&gt;Arnold Wiliem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messmer_M/0/1/0/all/0/1&quot;&gt;Martin Messmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1&quot;&gt;Heng-Cheng Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadler_D/0/1/0/all/0/1&quot;&gt;Daniel Stadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_L/0/1/0/all/0/1&quot;&gt;Lars Sommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaer Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1&quot;&gt;Aiguo Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1&quot;&gt;Weitu Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lertniphonphan_K/0/1/0/all/0/1&quot;&gt;Kanokphan Lertniphonphan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhepeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zedda_L/0/1/0/all/0/1&quot;&gt;Luca Zedda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loddo_A/0/1/0/all/0/1&quot;&gt;Andrea Loddo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruberto_C/0/1/0/all/0/1&quot;&gt;Cecilia Di Ruberto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Truong_H/0/1/0/all/0/1&quot;&gt;Hai Nguyen-Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1&quot;&gt;Tan-Sang Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1&quot;&gt;Quan-Dung Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Sai-Kit Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yuan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thien_N/0/1/0/all/0/1&quot;&gt;Nguyen Thanh Thien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lixin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuan_S/0/1/0/all/0/1&quot;&gt;Sheng-Yao Kuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_Y/0/1/0/all/0/1&quot;&gt;Yuan-Hao Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Angel Bueno Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrillo_Perez_B/0/1/0/all/0/1&quot;&gt;Borja Carrillo-Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1&quot;&gt;Alexander Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alex_A/0/1/0/all/0/1&quot;&gt;Antje Alex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiniger_Y/0/1/0/all/0/1&quot;&gt;Yannik Steiniger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattler_F/0/1/0/all/0/1&quot;&gt;Felix Sattler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solano_Carrillo_E/0/1/0/all/0/1&quot;&gt;Edgardo Solano-Carrillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabijanic_M/0/1/0/all/0/1&quot;&gt;Matej Fabijani&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumunec_M/0/1/0/all/0/1&quot;&gt;Magdalena &amp;#x160;umunec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapetanovic_N/0/1/0/all/0/1&quot;&gt;Nadir Kapetanovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michel_A/0/1/0/all/0/1&quot;&gt;Andreas Michel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_W/0/1/0/all/0/1&quot;&gt;Wolfgang Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1&quot;&gt;Martin Weinmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14764">
<title>SafeSea: Synthetic Data Generation for Adverse &amp; Low Probability Maritime Conditions. (arXiv:2311.14764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14764</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality training data is essential for enhancing the robustness of
object detection models. Within the maritime domain, obtaining a diverse real
image dataset is particularly challenging due to the difficulty of capturing
sea images with the presence of maritime objects , especially in stormy
conditions. These challenges arise due to resource limitations, in addition to
the unpredictable appearance of maritime objects. Nevertheless, acquiring data
from stormy conditions is essential for training effective maritime detection
models, particularly for search and rescue, where real-world conditions can be
unpredictable. In this work, we introduce SafeSea, which is a stepping stone
towards transforming actual sea images with various Sea State backgrounds while
retaining maritime objects. Compared to existing generative methods such as
Stable Diffusion Inpainting~\cite{stableDiffusion}, this approach reduces the
time and effort required to create synthetic datasets for training maritime
object detection models. The proposed method uses two automated filters to only
pass generated images that meet the criteria. In particular, these filters will
first classify the sea condition according to its Sea State level and then it
will check whether the objects from the input image are still preserved. This
method enabled the creation of the SafeSea dataset, offering diverse weather
condition backgrounds to supplement the training of maritime models. Lastly, we
observed that a maritime object detection model faced challenges in detecting
objects in stormy sea backgrounds, emphasizing the impact of weather conditions
on detection accuracy. The code, and dataset are available at
https://github.com/martin-3240/SafeSea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_M/0/1/0/all/0/1&quot;&gt;Martin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shipard_J/0/1/0/all/0/1&quot;&gt;Jordan Shipard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mulyono_H/0/1/0/all/0/1&quot;&gt;Hermawan Mulyono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiliem_A/0/1/0/all/0/1&quot;&gt;Arnold Wiliem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fookes_C/0/1/0/all/0/1&quot;&gt;Clinton Fookes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14768">
<title>AdaDiff: Adaptive Step Selection for Fast Diffusion. (arXiv:2311.14768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14768</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models, as a type of generative models, have achieved impressive
results in generating images and videos conditioned on textual conditions.
However, the generation process of diffusion models involves denoising for
dozens of steps to produce photorealistic images/videos, which is
computationally expensive. Unlike previous methods that design
``one-size-fits-all&apos;&apos; approaches for speed up, we argue denoising steps should
be sample-specific conditioned on the richness of input texts. To this end, we
introduce AdaDiff, a lightweight framework designed to learn instance-specific
step usage policies, which are then used by the diffusion model for generation.
AdaDiff is optimized using a policy gradient method to maximize a carefully
designed reward function, balancing inference time and generation quality. We
conduct experiments on three image generation and two video generation
benchmarks and demonstrate that our approach achieves similar results in terms
of visual quality compared to the baseline using a fixed 50 denoising steps
while reducing inference time by at least 33%, going as high as 40%.
Furthermore, our qualitative analysis shows that our method allocates more
steps to more informative text conditions and fewer steps to simpler text
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhen Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jie Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14772">
<title>Trainwreck: A damaging adversarial attack on image classifiers. (arXiv:2311.14772v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14772</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks are an important security concern for computer vision
(CV), as they enable malicious attackers to reliably manipulate CV models.
Existing attacks aim to elicit an output desired by the attacker, but keep the
model fully intact on clean data. With CV models becoming increasingly valuable
assets in applied practice, a new attack vector is emerging: disrupting the
models as a form of economic sabotage. This paper opens up the exploration of
damaging adversarial attacks (DAAs) that seek to damage the target model and
maximize the total cost incurred by the damage. As a pioneer DAA, this paper
proposes Trainwreck, a train-time attack that poisons the training data of
image classifiers to degrade their performance. Trainwreck conflates the data
of similar classes using stealthy ($\epsilon \leq 8/255$) class-pair universal
perturbations computed using a surrogate model. Trainwreck is a black-box,
transferable attack: it requires no knowledge of the target model&apos;s
architecture, and a single poisoned dataset degrades the performance of any
model trained on it. The experimental evaluation on CIFAR-10 and CIFAR-100
demonstrates that Trainwreck is indeed an effective attack across various model
architectures including EfficientNetV2, ResNeXt-101, and a finetuned ViT-L-16.
The strength of the attack can be customized by the poison rate parameter.
Finally, data redundancy with file hashing and/or pixel difference are
identified as a reliable defense technique against Trainwreck or similar DAAs.
The code is available at https://github.com/JanZahalka/trainwreck.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahalka_J/0/1/0/all/0/1&quot;&gt;Jan Zah&amp;#xe1;lka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14773">
<title>Set Features for Anomaly Detection. (arXiv:2311.14773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14773</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes set features for detecting anomalies in samples that
consist of unusual combinations of normal elements. Many leading methods
discover anomalies by detecting an unusual part of a sample. For example,
state-of-the-art segmentation-based approaches, first classify each element of
the sample (e.g., image patch) as normal or anomalous and then classify the
entire sample as anomalous if it contains anomalous elements. However, such
approaches do not extend well to scenarios where the anomalies are expressed by
an unusual combination of normal elements. In this paper, we overcome this
limitation by proposing set features that model each sample by the distribution
of its elements. We compute the anomaly score of each sample using a simple
density estimation method, using fixed features. Our approach outperforms the
previous state-of-the-art in image-level logical anomaly detection and
sequence-level time series anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1&quot;&gt;Niv Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzachor_I/0/1/0/all/0/1&quot;&gt;Issar Tzachor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoshen_Y/0/1/0/all/0/1&quot;&gt;Yedid Hoshen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14775">
<title>READS-V: Real-time Automated Detection of Epileptic Seizures from Surveillance Videos via Skeleton-based Spatiotemporal ViG. (arXiv:2311.14775v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14775</link>
<description rdf:parseType="Literal">&lt;p&gt;An accurate and efficient epileptic seizure onset detection system can
significantly benefit patients. Traditional diagnostic methods, primarily
relying on electroencephalograms (EEGs), often result in cumbersome and
non-portable solutions, making continuous patient monitoring challenging. The
video-based seizure detection system is expected to free patients from the
constraints of scalp or implanted EEG devices and enable remote monitoring in
residential settings. Previous video-based methods neither enable all-day
monitoring nor provide short detection latency due to insufficient resources
and ineffective patient action recognition techniques. Additionally,
skeleton-based action recognition approaches remain limitations in identifying
subtle seizure-related actions. To address these challenges, we propose a novel
skeleton-based spatiotemporal vision graph neural network (STViG) for
efficient, accurate, and timely REal-time Automated Detection of epileptic
Seizures from surveillance Videos (READS-V). Our experimental results indicate
STViG outperforms previous state-of-the-art action recognition models on our
collected patients&apos; video data with higher accuracy (5.9% error) and lower
FLOPs (0.4G). Furthermore, by integrating a decision-making rule that combines
output probabilities and an accumulative function, our READS-V system achieves
a 5.1 s EEG onset detection latency, a 13.1 s advance in clinical onset
detection, and zero false detection rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yankun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_W/0/1/0/all/0/1&quot;&gt;Wenjie Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawan_M/0/1/0/all/0/1&quot;&gt;Mohamad Sawan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14777">
<title>From Text to Image: Exploring GPT-4Vision&apos;s Potential in Advanced Radiological Analysis across Subspecialties. (arXiv:2311.14777v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.14777</link>
<description rdf:parseType="Literal">&lt;p&gt;The study evaluates and compares GPT-4 and GPT-4Vision for radiological
tasks, suggesting GPT-4Vision may recognize radiological features from images,
thereby enhancing its diagnostic potential over text-based descriptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Busch_F/0/1/0/all/0/1&quot;&gt;Felix Busch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_T/0/1/0/all/0/1&quot;&gt;Tianyu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Makowski_M/0/1/0/all/0/1&quot;&gt;Marcus Makowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Truhn_D/0/1/0/all/0/1&quot;&gt;Daniel Truhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bressem_K/0/1/0/all/0/1&quot;&gt;Keno Bressem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adams_L/0/1/0/all/0/1&quot;&gt;Lisa Adams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14786">
<title>GPT-4V Takes the Wheel: Evaluating Promise and Challenges for Pedestrian Behavior Prediction. (arXiv:2311.14786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14786</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing pedestrian behavior prediction methods rely primarily on deep neural
networks that utilize features extracted from video frame sequences. Although
these vision-based models have shown promising results, they face limitations
in effectively capturing and utilizing the dynamic spatio-temporal interactions
between the target pedestrian and its surrounding traffic elements, crucial for
accurate reasoning. Additionally, training these models requires manually
annotating domain-specific datasets, a process that is expensive,
time-consuming, and difficult to generalize to new environments and scenarios.
The recent emergence of Large Multimodal Models (LMMs) offers potential
solutions to these limitations due to their superior visual understanding and
causal reasoning capabilities, which can be harnessed through semi-supervised
training. GPT-4V(ision), the latest iteration of the state-of-the-art
Large-Language Model GPTs, now incorporates vision input capabilities. This
report provides a comprehensive evaluation of the potential of GPT-4V for
pedestrian behavior prediction in autonomous driving using publicly available
datasets: JAAD, PIE, and WiDEVIEW. Quantitative and qualitative evaluations
demonstrate GPT-4V(ision)&apos;s promise in zero-shot pedestrian behavior prediction
and driving scene understanding ability for autonomous driving. However, it
still falls short of the state-of-the-art traditional domain-specific models.
Challenges include difficulties in handling small pedestrians and vehicles in
motion. These limitations highlight the need for further research and
development in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Peng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1&quot;&gt;Alvika Gautam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1&quot;&gt;Srikanth Saripalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14822">
<title>Text and Click inputs for unambiguous open vocabulary instance segmentation. (arXiv:2311.14822v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14822</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation localizes objects in an image on a fine-grained per-pixel scale.
Segmentation benefits by humans-in-the-loop to provide additional input of
objects to segment using a combination of foreground or background clicks.
Tasks include photoediting or novel dataset annotation, where human annotators
leverage an existing segmentation model instead of drawing raw pixel level
annotations. We propose a new segmentation process, Text + Click segmentation,
where a model takes as input an image, a text phrase describing a class to
segment, and a single foreground click specifying the instance to segment.
Compared to previous approaches, we leverage open-vocabulary image-text models
to support a wide-range of text prompts. Conditioning segmentations on text
prompts improves the accuracy of segmentations on novel or unseen classes. We
demonstrate that the combination of a single user-specified foreground click
and a text prompt allows a model to better disambiguate overlapping or
co-occurring semantic categories, such as &quot;tie&quot;, &quot;suit&quot;, and &quot;person&quot;. We study
these results across common segmentation datasets such as refCOCO, COCO, VOC,
and OpenImages. Source code available here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warner_N/0/1/0/all/0/1&quot;&gt;Nikolai Warner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_M/0/1/0/all/0/1&quot;&gt;Meera Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jonathan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Essa_I/0/1/0/all/0/1&quot;&gt;Irfan Essa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Birodkar_V/0/1/0/all/0/1&quot;&gt;Vighnesh Birodkar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14824">
<title>A Reusable AI-Enabled Defect Detection System for Railway Using Ensembled CNN. (arXiv:2311.14824v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14824</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate Defect detection is crucial for ensuring the trustworthiness of
intelligent railway systems. Current approaches rely on single deep-learning
models, like CNNs, which employ a large amount of data to capture underlying
patterns. Training a new defect classifier with limited samples often leads to
overfitting and poor performance on unseen images. To address this, researchers
have advocated transfer learning and fine-tuning the pre-trained models.
However, using a single backbone network in transfer learning still may cause
bottleneck issues and inconsistent performance if it is not suitable for a
specific problem domain. To overcome these challenges, we propose a reusable
AI-enabled defect detection approach. By combining ensemble learning with
transfer learning models (VGG-19, MobileNetV3, and ResNet-50), we improved the
classification accuracy and achieved consistent performance at a certain phase
of training. Our empirical analysis demonstrates better and more consistent
performance compared to other state-of-the-art approaches. The consistency
substantiates the reusability of the defect detection system for newly evolved
defected rail parts. Therefore we anticipate these findings to benefit further
research and development of reusable AI-enabled solutions for railway systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdousi_R/0/1/0/all/0/1&quot;&gt;Rahatara Ferdousi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laamarti_F/0/1/0/all/0/1&quot;&gt;Fedwa Laamarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chunsheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14829">
<title>Proximal Algorithms for Accelerated Langevin Dynamics. (arXiv:2311.14829v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2311.14829</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a novel class of MCMC algorithms based on a stochastized Nesterov
scheme. With an appropriate addition of noise, the result is a
time-inhomogeneous underdamped Langevin equation, which we prove emits a
specified target distribution as its invariant measure. Convergence rates to
stationarity under Wasserstein-2 distance are established as well.
Metropolis-adjusted and stochastic gradient versions of the proposed Langevin
dynamics are also provided. Experimental illustrations show superior
performance of the proposed method over typical Langevin samplers for different
models in statistics and image processing including better mixing of the
resulting Markov chains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thai_D/0/1/0/all/0/1&quot;&gt;Duy H. Thai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1&quot;&gt;Alexander L. Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunson_D/0/1/0/all/0/1&quot;&gt;David B. Dunson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14837">
<title>Benchmarking Robustness of Text-Image Composed Retrieval. (arXiv:2311.14837v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14837</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-image composed retrieval aims to retrieve the target image through the
composed query, which is specified in the form of an image plus some text that
describes desired modifications to the input image. It has recently attracted
attention due to its ability to leverage both information-rich images and
concise language to precisely express the requirements for target images.
However, the robustness of these approaches against real-world corruptions or
further text understanding has never been studied. In this paper, we perform
the first robustness study and establish three new diversified benchmarks for
systematic analysis of text-image composed retrieval against natural
corruptions in both vision and text and further probe textural understanding.
For natural corruption analysis, we introduce two new large-scale benchmark
datasets, CIRR-C and FashionIQ-C for testing in open domain and fashion domain
respectively, both of which apply 15 visual corruptions and 7 textural
corruptions. For textural understanding analysis, we introduce a new diagnostic
dataset CIRR-D by expanding the original raw data with synthetic data, which
contains modified text to better probe textual understanding ability including
numerical variation, attribute variation, object removal, background variation,
and fine-grained evaluation. The code and benchmark datasets are available at
https://github.com/SunTongtongtong/Benchmark-Robustness-Text-Image-Compose-Retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shitong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jindong Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_S/0/1/0/all/0/1&quot;&gt;Shaogang Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14851">
<title>Unified Medical Image Pre-training in Language-Guided Common Semantic Space. (arXiv:2311.14851v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14851</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Pre-training (VLP) has shown the merits of analysing medical
images, by leveraging the semantic congruence between medical images and their
corresponding reports. It efficiently learns visual representations, which in
turn facilitates enhanced analysis and interpretation of intricate imaging
data. However, such observation is predominantly justified on single-modality
data (mostly 2D images like X-rays), adapting VLP to learning unified
representations for medical images in real scenario remains an open challenge.
This arises from medical images often encompass a variety of modalities,
especially modalities with different various number of dimensions (e.g., 3D
images like Computed Tomography). To overcome the aforementioned challenges, we
propose an Unified Medical Image Pre-training framework, namely UniMedI, which
utilizes diagnostic reports as common semantic space to create unified
representations for diverse modalities of medical images (especially for 2D and
3D images). Under the text&apos;s guidance, we effectively uncover visual modality
information, identifying the affected areas in 2D X-rays and slices containing
lesion in sophisticated 3D CT scans, ultimately enhancing the consistency
across various medical imaging modalities. To demonstrate the effectiveness and
versatility of UniMedI, we evaluate its performance on both 2D and 3D images
across 10 different datasets, covering a wide range of medical image tasks such
as classification, segmentation, and retrieval. UniMedI has demonstrated
superior performance in downstream tasks, showcasing its effectiveness in
establishing a universal medical visual representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xinyang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xufang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haoji Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Siyun Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Lili Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14875">
<title>Uncertainty Aware AI for MRI Segmentation. (arXiv:2311.14875v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.14875</link>
<description rdf:parseType="Literal">&lt;p&gt;Robust uncertainty estimations are necessary in safety-critical applications
of Deep Learning. One such example is the semantic segmentation of medical
images, whilst deep-learning approaches have high-performance in such tasks
they lack interpretability as they give no indication of their confidence when
making classification decisions. Robust and interpretable segmentation is a
critical first stage in automatically screening for pathologies hence the
optimal solution is one which can provide highly accuracy but also capture the
underlying uncertainty. In this work we present an uncertainty-aware
segmentation model, BA U-Net, for use on MRI data that incorporates Bayesian
Neural Networks and Attention Mechanisms to provide accurate and interpretable
segmentations. We evaluated our model on the publicly available BraTS 2020
dataset using F1 Score and Intersection Over Union (IoU) as evaluation metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Konathala_L/0/1/0/all/0/1&quot;&gt;Lohith Konathala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14897">
<title>Towards Scalable 3D Anomaly Detection and Localization: A Benchmark via 3D Anomaly Synthesis and A Self-Supervised Learning Network. (arXiv:2311.14897v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14897</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, 3D anomaly detection, a crucial problem involving fine-grained
geometry discrimination, is getting more attention. However, the lack of
abundant real 3D anomaly data limits the scalability of current models. To
enable scalable anomaly data collection, we propose a 3D anomaly synthesis
pipeline to adapt existing large-scale 3Dmodels for 3D anomaly detection.
Specifically, we construct a synthetic dataset, i.e., Anomaly-ShapeNet, basedon
ShapeNet. Anomaly-ShapeNet consists of 1600 point cloud samples under 40
categories, which provides a rich and varied collection of data, enabling
efficient training and enhancing adaptability to industrial scenarios.
Meanwhile,to enable scalable representation learning for 3D anomaly
localization, we propose a self-supervised method, i.e., Iterative Mask
Reconstruction Network (IMRNet). During training, we propose a geometry-aware
sample module to preserve potentially anomalous local regions during point
cloud down-sampling. Then, we randomly mask out point patches and sent the
visible patches to a transformer for reconstruction-based self-supervision.
During testing, the point cloud repeatedly goes through the Mask Reconstruction
Network, with each iteration&apos;s output becoming the next input. By merging and
contrasting the final reconstructed point cloud with the initial input, our
method successfully locates anomalies. Experiments show that IMRNet outperforms
previous state-of-the-art methods, achieving 66.1% in I-AUC on Anomaly-ShapeNet
dataset and 72.5% in I-AUC on Real3D-AD dataset. Our dataset will be released
at https://github.com/Chopper-233/Anomaly-ShapeNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenqiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaohao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14899">
<title>HyperDID: Hyperspectral Intrinsic Image Decomposition with Deep Feature Embedding. (arXiv:2311.14899v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14899</link>
<description rdf:parseType="Literal">&lt;p&gt;The dissection of hyperspectral images into intrinsic components through
hyperspectral intrinsic image decomposition (HIID) enhances the
interpretability of hyperspectral data, providing a foundation for more
accurate classification outcomes. However, the classification performance of
HIID is constrained by the model&apos;s representational ability. To address this
limitation, this study rethinks hyperspectral intrinsic image decomposition for
classification tasks by introducing deep feature embedding. The proposed
framework, HyperDID, incorporates the Environmental Feature Module (EFM) and
Categorical Feature Module (CFM) to extract intrinsic features. Additionally, a
Feature Discrimination Module (FDM) is introduced to separate
environment-related and category-related features. Experimental results across
three commonly used datasets validate the effectiveness of HyperDID in
improving hyperspectral image classification performance. This novel approach
holds promise for advancing the capabilities of hyperspectral image analysis by
leveraging deep feature embedding principles. The implementation of the
proposed method could be accessed soon at https://github.com/shendu-sw/HyperDID
for the sake of reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wen Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiaohu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1&quot;&gt;Ping Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14900">
<title>Resfusion: Prior Residual Noise embedded Denoising Diffusion Probabilistic Models. (arXiv:2311.14900v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14900</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Denoising Diffusion Probabilistic Models have been widely used in
image segmentation, by generating segmentation masks conditioned on the input
image. However, previous works can not seamlessly integrate existing end-to-end
models with denoising diffusion models. Existing research can only select
acceleration steps based on experience rather than calculating them
specifically. Moreover, most methods are limited to small models and
small-scale datasets, unable to generalize to general datasets and a wider
range of tasks. Therefore, we propose Resfusion with a novel resnoise-diffusion
process, which gradually generates segmentation masks or any type of target
image, seamlessly integrating state-of-the-art end-to-end models and denoising
diffusion models. Resfusion bridges the discrepancy between the likelihood
output and the ground truth output through a Markov process. Through the novel
smooth equivalence transformation in resnoise-diffusion process, we determine
the optimal acceleration step. Experimental results demonstrate that Resfusion
combines the capabilities of existing end-to-end models and denoising diffusion
models, further enhancing performance and achieving outstanding results.
Moreover, Resfusion is not limited to segmentation tasks, it can easily
generalize to any general tasks of image generation and exhibit strong
competitiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhenning_S/0/1/0/all/0/1&quot;&gt;Shi Zhenning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Changsheng_D/0/1/0/all/0/1&quot;&gt;Dong Changsheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_P/0/1/0/all/0/1&quot;&gt;Pan Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xueshuo_X/0/1/0/all/0/1&quot;&gt;Xie Xueshuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Along_H/0/1/0/all/0/1&quot;&gt;He Along&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiaoying_Q/0/1/0/all/0/1&quot;&gt;Qu Qiaoying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Li Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14902">
<title>Parkinson Disease classification Using Contrastive Graph Cross-View Learning with Multimodal Fusion of SPECT Images and Clinical Features. (arXiv:2311.14902v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14902</link>
<description rdf:parseType="Literal">&lt;p&gt;Parkinson&apos;s Disease (PD) is a neurodegenerative neurological disorder that
impacts movement and afflicts over 10 million people worldwide. Previous
researches have come up with deep learning models for predicting Parkinson&apos;s
disease primarily using medical images and didn&apos;t leverage the manifold
structure in the dataset. Our study introduces a multimodal approach with both
image and non-image features with a contrastive cross-view graph fusion for
Parkinson&apos;s disease classification. Specifically, we designed a multimodal
co-attention module to integrate embeddings from two distinct graph views
derived from low dimensional representation of images and clinical features,
enabling the extraction of more stable and structured features from the
multiview data. Additionally, we have devised a simplified fusion method
utilizing a contrastive loss for positive and negative pairs, to enhance the
model&apos;s overall cross-view fusion learning capabilities. In our experiments,
the graph-view multimodal approach can achieve an accuracy rate of 91% and an
AUC of 92.8% in five-fold cross-validation, and it also demonstrates superior
predictive capabilities on non-image data as compared to methods that rely
solely on machine learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_J/0/1/0/all/0/1&quot;&gt;Jun-En Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chien-Chin Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Feng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14905">
<title>Class Gradient Projection For Continual Learning. (arXiv:2311.14905v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14905</link>
<description rdf:parseType="Literal">&lt;p&gt;Catastrophic forgetting is one of the most critical challenges in Continual
Learning (CL). Recent approaches tackle this problem by projecting the gradient
update orthogonal to the gradient subspace of existing tasks. While the results
are remarkable, those approaches ignore the fact that these calculated
gradients are not guaranteed to be orthogonal to the gradient subspace of each
class due to the class deviation in tasks, e.g., distinguishing &quot;Man&quot; from
&quot;Sea&quot; v.s. differentiating &quot;Boy&quot; from &quot;Girl&quot;. Therefore, this strategy may
still cause catastrophic forgetting for some classes. In this paper, we propose
Class Gradient Projection (CGP), which calculates the gradient subspace from
individual classes rather than tasks. Gradient update orthogonal to the
gradient subspace of existing classes can be effectively utilized to minimize
interference from other classes. To improve the generalization and efficiency,
we further design a Base Refining (BR) algorithm to combine similar classes and
refine class bases dynamically. Moreover, we leverage a contrastive learning
method to improve the model&apos;s ability to handle unseen tasks. Extensive
experiments on benchmark datasets demonstrate the effectiveness of our proposed
approach. It improves the previous methods by 2.0% on the CIFAR-100 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Ji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14906">
<title>AutoEval-Video: An Automatic Benchmark for Assessing Large Vision Language Models in Open-Ended Video Question Answering. (arXiv:2311.14906v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14906</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel and challenging benchmark, AutoEval-Video, to
comprehensively evaluate large vision-language models in open-ended video
question answering. The comprehensiveness of AutoEval-Video is demonstrated in
two aspects: 1) AutoEval-Video constructs open-ended video-questions across 9
skill dimensions, addressing capabilities of perception, comprehension, and
generation. 2) AutoEval-Video contains newly collected videos that cover over
40 distinct themes. To efficiently evaluate responses to the open-ended
questions, we employ an LLM-based evaluation approach, but instead of merely
providing a reference answer, we annotate unique evaluation rules for every
single instance (video-question pair). To maximize the robustness of these
rules, we develop a novel adversarial annotation mechanism. By using
instance-specific rules as prompt, GPT-4, as an automatic evaluator, can
achieve a stable evaluation accuracy of around 97.0\%, comparable to the 94.9\%
- 97.5\% accuracy of a human evaluator. Furthermore, we assess the performance
of eight large vision-language models on AutoEval-Video. Among them,
GPT-4V(ision) significantly outperforms other models, achieving an accuracy of
32.2\%. However, there is still substantial room for improvement compared to
human accuracy of 72.8\%. By conducting an extensive case study, we uncover
several drawbacks of GPT-4V, such as limited temporal and dynamic
comprehension, and overly general responses. Code is available at
\href{https://github.com/Xiuyuan-Chen/AutoEval-Video}{\color{magenta}https://github.com/Xiuyuan-Chen/AutoEval-Video}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiuyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14909">
<title>Continual Referring Expression Comprehension via Dual Modular Memorization. (arXiv:2311.14909v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14909</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring Expression Comprehension (REC) aims to localize an image region of
a given object described by a natural-language expression. While promising
performance has been demonstrated, existing REC algorithms make a strong
assumption that training data feeding into a model are given upfront, which
degrades its practicality for real-world scenarios. In this paper, we propose
Continual Referring Expression Comprehension (CREC), a new setting for REC,
where a model is learning on a stream of incoming tasks. In order to
continuously improve the model on sequential tasks without forgetting prior
learned knowledge and without repeatedly re-training from a scratch, we propose
an effective baseline method named Dual Modular Memorization (DMM), which
alleviates the problem of catastrophic forgetting by two memorization modules:
Implicit-Memory and Explicit-Memory. Specifically, the former module aims to
constrain drastic changes to important parameters learned on old tasks when
learning a new task; while the latter module maintains a buffer pool to
dynamically select and store representative samples of each seen task for
future rehearsal. We create three benchmarks for the new CREC setting, by
respectively re-splitting three widely-used REC datasets RefCOCO, RefCOCO+ and
RefCOCOg into sequential tasks. Extensive experiments on the constructed
benchmarks demonstrate that our DMM method significantly outperforms other
alternatives, based on two popular REC backbones. We make the source code and
benchmarks publicly available to foster future progress in this field:
https://github.com/zackschen/DMM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Meng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14911">
<title>CUCL: Codebook for Unsupervised Continual Learning. (arXiv:2311.14911v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14911</link>
<description rdf:parseType="Literal">&lt;p&gt;The focus of this study is on Unsupervised Continual Learning (UCL), as it
presents an alternative to Supervised Continual Learning which needs
high-quality manual labeled data. The experiments under the UCL paradigm
indicate a phenomenon where the results on the first few tasks are suboptimal.
This phenomenon can render the model inappropriate for practical applications.
To address this issue, after analyzing the phenomenon and identifying the lack
of diversity as a vital factor, we propose a method named Codebook for
Unsupervised Continual Learning (CUCL) which promotes the model to learn
discriminative features to complete the class boundary. Specifically, we first
introduce a Product Quantization to inject diversity into the representation
and apply a cross quantized contrastive loss between the original
representation and the quantized one to capture discriminative information.
Then, based on the quantizer, we propose an effective Codebook Rehearsal to
address catastrophic forgetting. This study involves conducting extensive
experiments on CIFAR100, TinyImageNet, and MiniImageNet benchmark datasets. Our
method significantly boosts the performances of supervised and unsupervised
methods. For instance, on TinyImageNet, our method led to a relative
improvement of 12.76% and 7% when compared with Simsiam and BYOL, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jingkuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaosu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Junchen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Lianli Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hengtao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14918">
<title>Resolution- and Stimulus-agnostic Super-Resolution of Ultra-High-Field Functional MRI: Application to Visual Studies. (arXiv:2311.14918v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.14918</link>
<description rdf:parseType="Literal">&lt;p&gt;High-resolution fMRI provides a window into the brain&apos;s mesoscale
organization. Yet, higher spatial resolution increases scan times, to
compensate for the low signal and contrast-to-noise ratio. This work introduces
a deep learning-based 3D super-resolution (SR) method for fMRI. By
incorporating a resolution-agnostic image augmentation framework, our method
adapts to varying voxel sizes without retraining. We apply this innovative
technique to localize fine-scale motion-selective sites in the early visual
areas. Detection of these sites typically requires a resolution higher than 1
mm isotropic, whereas here, we visualize them based on lower resolution (2-3mm
isotropic) fMRI data. Remarkably, the super-resolved fMRI is able to recover
high-frequency detail of the interdigitated organization of these sites
(relative to the color-selective sites), even with training data sourced from
different subjects and experimental paradigms -- including non-visual
resting-state fMRI, underscoring its robustness and versatility. Quantitative
and qualitative results indicate that our method has the potential to enhance
the spatial resolution of fMRI, leading to a drastic reduction in acquisition
time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rosen_M/0/1/0/all/0/1&quot;&gt;Matthew S. Rosen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nasr_S/0/1/0/all/0/1&quot;&gt;Shahin Nasr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1&quot;&gt;Juan Eugenio Iglesias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14920">
<title>DECap: Towards Generalized Explicit Caption Editing via Diffusion Mechanism. (arXiv:2311.14920v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14920</link>
<description rdf:parseType="Literal">&lt;p&gt;Explicit Caption Editing (ECE) -- refining reference image captions through a
sequence of explicit edit operations (e.g., KEEP, DETELE) -- has raised
significant attention due to its explainable and human-like nature. After
training with carefully designed reference and ground-truth caption pairs,
state-of-the-art ECE models exhibit limited generalization ability beyond the
original training data distribution, i.e., they are tailored to refine content
details only in in-domain samples but fail to correct errors in out-of-domain
samples. To this end, we propose a new Diffusion-based Explicit Caption editing
method: DECap. Specifically, we reformulate the ECE task as a denoising process
under the diffusion mechanism, and introduce innovative edit-based noising and
denoising processes. Thanks to this design, the noising process can help to
eliminate the need for meticulous paired data selection by directly introducing
word-level noises for training, learning diverse distribution over input
reference caption. The denoising process involves the explicit predictions of
edit operations and corresponding content words, refining reference captions
through iterative step-wise editing. To further efficiently implement our
diffusion process and improve the inference speed, DECap discards the prevalent
multi-stage design and directly generates edit operations and content words
simultaneously. Extensive ablations have demonstrated the strong generalization
ability of DECap in various scenarios. More interestingly, it even shows great
potential in improving the quality and controllability of caption generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jun Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Long Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14922">
<title>GBD-TS: Goal-based Pedestrian Trajectory Prediction with Diffusion using Tree Sampling Algorithm. (arXiv:2311.14922v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14922</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting pedestrian trajectories is crucial for improving the safety and
effectiveness of autonomous driving and mobile robots. However, this task is
nontrivial due to the inherent stochasticity of human motion, which naturally
requires the predictor to generate multi-model prediction. Previous works have
used various generative methods, such as GAN and VAE, for pedestrian trajectory
prediction. Nevertheless, these methods may suffer from problems, including
mode collapse and relatively low-quality results. The denoising diffusion
probabilistic model (DDPM) has recently been applied to trajectory prediction
due to its simple training process and powerful reconstruction ability.
However, current diffusion-based methods are straightforward without fully
leveraging input information and usually require many denoising iterations
leading to a long inference time or an additional network for initialization.
To address these challenges and promote the application of diffusion models in
trajectory prediction, we propose a novel scene-aware multi-modal pedestrian
trajectory prediction framework called GBD. GBD combines goal prediction with
the diffusion network. First, the goal predictor produces multiple goals, and
then the diffusion network generates multi-modal trajectories conditioned on
these goals. Furthermore, we introduce a new diffusion sampling algorithm named
tree sampling (TS), which leverages common feature to reduce the inference time
and improve accuracy for multi-modal prediction. Experimental results
demonstrate that our GBD-TS method achieves state-of-the-art performance with
real-time inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Ge Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14925">
<title>Coordinate-based Neural Network for Fourier Phase Retrieval. (arXiv:2311.14925v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14925</link>
<description rdf:parseType="Literal">&lt;p&gt;Fourier phase retrieval is essential for high-definition imaging of nanoscale
structures across diverse fields, notably coherent diffraction imaging. This
study presents the Single impliCit neurAl Network (SCAN), a tool built upon
coordinate neural networks meticulously designed for enhanced phase retrieval
performance. Bypassing the pitfalls of conventional iterative methods, which
frequently face high computational loads and are prone to noise interference,
SCAN adeptly connects object coordinates to their amplitude and phase within a
unified network in an unsupervised manner. While many existing methods
primarily use Fourier magnitude in their loss function, our approach
incorporates both the predicted magnitude and phase, enhancing retrieval
accuracy. Comprehensive tests validate SCAN&apos;s superiority over traditional and
other deep learning models regarding accuracy and noise robustness. We also
demonstrate that SCAN excels in the ptychography setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tingyou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zixin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Y/0/1/0/all/0/1&quot;&gt;Yong S. Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaojing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jizhou Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14926">
<title>FreePIH: Training-Free Painterly Image Harmonization with Diffusion Model. (arXiv:2311.14926v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14926</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides an efficient training-free painterly image harmonization
(PIH) method, dubbed FreePIH, that leverages only a pre-trained diffusion model
to achieve state-of-the-art harmonization results. Unlike existing methods that
require either training auxiliary networks or fine-tuning a large pre-trained
backbone, or both, to harmonize a foreground object with a painterly-style
background image, our FreePIH tames the denoising process as a plug-in module
for foreground image style transfer. Specifically, we find that the very last
few steps of the denoising (i.e., generation) process strongly correspond to
the stylistic information of images, and based on this, we propose to augment
the latent features of both the foreground and background images with Gaussians
for a direct denoising-based harmonization. To guarantee the fidelity of the
harmonized image, we make use of multi-scale features to enforce the
consistency of the content and stability of the foreground objects in the
latent space, and meanwhile, aligning both fore-/back-grounds with the same
style. Moreover, to accommodate the generation with more structural and
textural details, we further integrate text prompts to attend to the latent
features, hence improving the generation quality. Quantitative and qualitative
evaluations on COCO and LAION 5B datasets demonstrate that our method can
surpass representative baselines by large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14927">
<title>View-Based Luminance Mapping in Open Workplace. (arXiv:2311.14927v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14927</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel computational method for mapping indoor
luminance values on the facade of an open workplace to improve its daylight
performance. 180-degree fisheye renderings from different indoor locations,
view positions, and times of the year are created. These renderings are then
transformed from two-dimensional (2D) images into three-dimensional (3D)
hemispheres. High luminance values are filtered and projected from the
hemisphere to the facade surface. This framework will highlight the areas of
the facade that allow too much light penetration into the interior environment.
The flexible workflow allows occupant centric lighting analysis that computes
multiple design parameters and synthesizes results for localized facade
optimization and daylight design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Guanzhou Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_T/0/1/0/all/0/1&quot;&gt;Tingsong Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawyer_A/0/1/0/all/0/1&quot;&gt;Azadeh O. Sawyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14939">
<title>OpenNet: Incremental Learning for Autonomous Driving Object Detection with Balanced Loss. (arXiv:2311.14939v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14939</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated driving object detection has always been a challenging task in
computer vision due to environmental uncertainties. These uncertainties include
significant differences in object sizes and encountering the class unseen. It
may result in poor performance when traditional object detection models are
directly applied to automated driving detection. Because they usually presume
fixed categories of common traffic participants, such as pedestrians and cars.
Worsely, the huge class imbalance between common and novel classes further
exacerbates performance degradation. To address the issues stated, we propose
OpenNet to moderate the class imbalance with the Balanced Loss, which is based
on Cross Entropy Loss. Besides, we adopt an inductive layer based on gradient
reshaping to fast learn new classes with limited samples during incremental
learning. To against catastrophic forgetting, we employ normalized feature
distillation. By the way, we improve multi-scale detection robustness and
unknown class recognition through FPN and energy-based detection, respectively.
The Experimental results upon the CODA dataset show that the proposed method
can obtain better performance than that of the existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zezhou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_G/0/1/0/all/0/1&quot;&gt;Guitao Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_X/0/1/0/all/0/1&quot;&gt;Xidong Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiangtao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14948">
<title>Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.14948</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach.
&lt;/p&gt;
&lt;p&gt;In this work, we demonstrate that the efficacy of CleanCLIP in mitigating
backdoors is highly dependent on the particular objective used during model
pre-training.
&lt;/p&gt;
&lt;p&gt;We observe that stronger pre-training objectives correlate with harder to
remove backdoors behaviors. We show this by training multimodal models on two
large datasets consisting of 3 million (CC3M) and 6 million (CC6M) datapoints,
under various pre-training objectives, followed by poison removal using
CleanCLIP. We find that CleanCLIP is ineffective when stronger pre-training
objectives are used, even with extensive hyperparameter tuning.
&lt;/p&gt;
&lt;p&gt;Our findings underscore critical considerations for ML practitioners who
pre-train models using large-scale web-curated data and are concerned about
potential backdoor threats. Notably, our results suggest that simpler
pre-training objectives are more amenable to effective backdoor removal. This
insight is pivotal for practitioners seeking to balance the trade-offs between
using stronger pre-training objectives and security against backdoor attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Sahil Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1&quot;&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1&quot;&gt;Soumye Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav Mohanty Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chirag Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1&quot;&gt;John P Dickerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeff Bilmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14960">
<title>Point Cloud Pre-training with Diffusion Models. (arXiv:2311.14960v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14960</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training a model and then fine-tuning it on downstream tasks has
demonstrated significant success in the 2D image and NLP domains. However, due
to the unordered and non-uniform density characteristics of point clouds, it is
non-trivial to explore the prior knowledge of point clouds and pre-train a
point cloud backbone. In this paper, we propose a novel pre-training method
called Point cloud Diffusion pre-training (PointDif). We consider the point
cloud pre-training task as a conditional point-to-point generation problem and
introduce a conditional point generator. This generator aggregates the features
extracted by the backbone and employs them as the condition to guide the
point-to-point recovery from the noisy point cloud, thereby assisting the
backbone in capturing both local and global geometric priors as well as the
global point density distribution of the object. We also present a recurrent
uniform sampling optimization strategy, which enables the model to uniformly
recover from various noise levels and learn from balanced supervision. Our
PointDif achieves substantial improvement across various real-world datasets
for diverse downstream tasks such as classification, segmentation and
detection. Specifically, PointDif attains 70.0% mIoU on S3DIS Area 5 for the
segmentation task and achieves an average improvement of 2.4% on ScanObjectNN
for the classification task compared to TAP. Furthermore, our pre-training
framework can be flexibly applied to diverse point cloud backbones and bring
considerable gains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xiao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaoshui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_G/0/1/0/all/0/1&quot;&gt;Guofeng Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yuenan Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yongshun Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14971">
<title>Segmentation of diagnostic tissue compartments on whole slide images with renal thrombotic microangiopathies (TMAs). (arXiv:2311.14971v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14971</link>
<description rdf:parseType="Literal">&lt;p&gt;The thrombotic microangiopathies (TMAs) manifest in renal biopsy histology
with a broad spectrum of acute and chronic findings. Precise diagnostic
criteria for a renal biopsy diagnosis of TMA are missing. As a first step
towards a machine learning- and computer vision-based analysis of wholes slide
images from renal biopsies, we trained a segmentation model for the decisive
diagnostic kidney tissue compartments artery, arteriole, glomerulus on a set of
whole slide images from renal biopsies with TMAs and Mimickers (distinct
diseases with a similar nephropathological appearance as TMA like severe benign
nephrosclerosis, various vasculitides, Bevacizumab-plug glomerulopathy,
arteriolar light chain deposition disease). Our segmentation model combines a
U-Net-based tissue detection with a Shifted windows-transformer architecture to
reach excellent segmentation results for even the most severely altered
glomeruli, arterioles and arteries, even on unseen staining domains from a
different nephropathology lab. With accurate automatic segmentation of the
decisive renal biopsy compartments in human renal vasculopathies, we have laid
the foundation for large-scale compartment-specific machine learning and
computer vision analysis of renal biopsy repositories with TMAs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_H/0/1/0/all/0/1&quot;&gt;Huy Q. Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cicalese_P/0/1/0/all/0/1&quot;&gt;Pietro A. Cicalese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshan_S/0/1/0/all/0/1&quot;&gt;Surya Seshan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizvi_S/0/1/0/all/0/1&quot;&gt;Syed A. Rizvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vathul_A/0/1/0/all/0/1&quot;&gt;Aneesh Vathul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bueno_G/0/1/0/all/0/1&quot;&gt;Gloria Bueno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dorado_A/0/1/0/all/0/1&quot;&gt;Anibal Pedraza Dorado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabe_N/0/1/0/all/0/1&quot;&gt;Niels Grabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stolle_K/0/1/0/all/0/1&quot;&gt;Katharina Stolle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pesce_F/0/1/0/all/0/1&quot;&gt;Francesco Pesce&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roelofs_J/0/1/0/all/0/1&quot;&gt;Joris J.T.H. Roelofs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kers_J/0/1/0/all/0/1&quot;&gt;Jesper Kers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bevilacqua_V/0/1/0/all/0/1&quot;&gt;Vitoantonio Bevilacqua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Altinini_N/0/1/0/all/0/1&quot;&gt;Nicola Altinini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schroppel_B/0/1/0/all/0/1&quot;&gt;Bernd Schr&amp;#xf6;ppel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roccatello_D/0/1/0/all/0/1&quot;&gt;Dario Roccatello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barreca_A/0/1/0/all/0/1&quot;&gt;Antonella Barreca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sciascia_S/0/1/0/all/0/1&quot;&gt;Savino Sciascia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_C/0/1/0/all/0/1&quot;&gt;Chandra Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hien V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_J/0/1/0/all/0/1&quot;&gt;Jan U. Becker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14977">
<title>Incorporating granularity bias as the margin into contrastive loss for video captioning. (arXiv:2311.14977v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14977</link>
<description rdf:parseType="Literal">&lt;p&gt;Video captioning models easily suffer from long-tail distribution of phrases,
which makes captioning models prone to generate vague sentences instead of
accurate ones. However, existing debiasing strategies tend to export external
knowledge to build dependency trees of words or refine frequency distribution
by complex losses and extra input features, which lack interpretability and are
hard to train. To mitigate the impact of granularity bias on the model, we
introduced a statistical-based bias extractor. This extractor quantifies the
information content within sentences and videos, providing an estimate of the
likelihood that a video-sentence pair is affected by granularity bias.
Furthermore, with the growing trend of integrating contrastive learning methods
into video captioning tasks, we use a bidirectional triplet loss to get more
negative samples in a batch. Subsequently, we incorporate the margin score into
the contrastive learning loss, establishing distinct training objectives for
head and tail sentences. This approach facilitates the model&apos;s training
effectiveness on tail samples. Our simple yet effective loss, incorporating
Granularity bias, is referred to as the Margin-Contrastive Loss (GMC Loss). The
proposed model demonstrates state-of-the-art performance on MSRVTT with a CIDEr
of 57.17, and MSVD, where CIDEr reaches up to 138.68.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiayang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_F/0/1/0/all/0/1&quot;&gt;Fengming Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14981">
<title>Multi-task Planar Reconstruction with Feature Warping Guidance. (arXiv:2311.14981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14981</link>
<description rdf:parseType="Literal">&lt;p&gt;Piece-wise planar 3D reconstruction simultaneously segments plane instances
and recovers their 3D plane parameters from an image, which is particularly
useful for indoor or man-made environments. Efficient reconstruction of 3D
planes coupled with semantic predictions offers advantages for a wide range of
applications requiring scene understanding and concurrent spatial mapping.
However, most existing planar reconstruction models either neglect semantic
predictions or do not run efficiently enough for real-time applications. We
introduce SoloPlanes, a real-time planar reconstruction model based on a
modified instance segmentation architecture which simultaneously predicts
semantics for each plane instance, along with plane parameters and piece-wise
plane instance masks. By providing multi-view guidance in feature space, we
achieve an improvement in instance mask segmentation despite only warping plane
features due to the nature of feature sharing in multi-task learning. Our model
simultaneously predicts semantics using single images at inference time, while
achieving real-time predictions at 43 FPS. The code will be released
post-publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Luan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14983">
<title>Neural Network Based Approach to Recognition of Meteor Tracks in the Mini-EUSO Telescope Data. (arXiv:2311.14983v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2311.14983</link>
<description rdf:parseType="Literal">&lt;p&gt;Mini-EUSO is a wide-angle fluorescence telescope that registers ultraviolet
(UV) radiation in the nocturnal atmosphere of Earth from the International
Space Station. Meteors are among multiple phenomena that manifest themselves
not only in the visible range but also in the UV. We present two simple
artificial neural networks that allow for recognizing meteor signals in the
Mini-EUSO data with high accuracy in terms of a binary classification problem.
We expect that similar architectures can be effectively used for signal
recognition in other fluorescence telescopes, regardless of the nature of the
signal. Due to their simplicity, the networks can be implemented in onboard
electronics of future orbital or balloon experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Zotov_M/0/1/0/all/0/1&quot;&gt;Mikhail Zotov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Anzhiganov_D/0/1/0/all/0/1&quot;&gt;Dmitry Anzhiganov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kryazhenkov_A/0/1/0/all/0/1&quot;&gt;Aleksandr Kryazhenkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Barghini_D/0/1/0/all/0/1&quot;&gt;Dario Barghini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Battisti_M/0/1/0/all/0/1&quot;&gt;Matteo Battisti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Belov_A/0/1/0/all/0/1&quot;&gt;Alexander Belov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bertaina_M/0/1/0/all/0/1&quot;&gt;Mario Bertaina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bianciotto_M/0/1/0/all/0/1&quot;&gt;Marta Bianciotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Bisconti_F/0/1/0/all/0/1&quot;&gt;Francesca Bisconti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Blaksley_C/0/1/0/all/0/1&quot;&gt;Carl Blaksley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Blin_S/0/1/0/all/0/1&quot;&gt;Sylvie Blin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Cambie_G/0/1/0/all/0/1&quot;&gt;Giorgio Cambi&amp;#xe8;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Capel_F/0/1/0/all/0/1&quot;&gt;Francesca Capel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Casolino_M/0/1/0/all/0/1&quot;&gt;Marco Casolino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ebisuzaki_T/0/1/0/all/0/1&quot;&gt;Toshikazu Ebisuzaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Eser_J/0/1/0/all/0/1&quot;&gt;Johannes Eser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Fenu_F/0/1/0/all/0/1&quot;&gt;Francesco Fenu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Franceschi_M/0/1/0/all/0/1&quot;&gt;Massimo Alberto Franceschi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Golzio_A/0/1/0/all/0/1&quot;&gt;Alessio Golzio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Gorodetzky_P/0/1/0/all/0/1&quot;&gt;Philippe Gorodetzky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kajino_F/0/1/0/all/0/1&quot;&gt;Fumiyoshi Kajino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Kasuga_H/0/1/0/all/0/1&quot;&gt;Hiroshi Kasuga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Klimov_P/0/1/0/all/0/1&quot;&gt;Pavel Klimov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Manfrin_M/0/1/0/all/0/1&quot;&gt;Massimiliano Manfrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Marcelli_L/0/1/0/all/0/1&quot;&gt;Laura Marcelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Miyamoto_H/0/1/0/all/0/1&quot;&gt;Hiroko Miyamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Murashov_A/0/1/0/all/0/1&quot;&gt;Alexey Murashov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Napolitano_T/0/1/0/all/0/1&quot;&gt;Tommaso Napolitano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ohmori_H/0/1/0/all/0/1&quot;&gt;Hiroshi Ohmori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Olinto_A/0/1/0/all/0/1&quot;&gt;Angela Olinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Parizot_E/0/1/0/all/0/1&quot;&gt;Etienne Parizot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Picozza_P/0/1/0/all/0/1&quot;&gt;Piergiorgio Picozza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Piotrowski_L/0/1/0/all/0/1&quot;&gt;Lech Wiktor Piotrowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Plebaniak_Z/0/1/0/all/0/1&quot;&gt;Zbigniew Plebaniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Prevot_G/0/1/0/all/0/1&quot;&gt;Guillaume Pr&amp;#xe9;v&amp;#xf4;t&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Reali_E/0/1/0/all/0/1&quot;&gt;Enzo Reali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ricci_M/0/1/0/all/0/1&quot;&gt;Marco Ricci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Romoli_G/0/1/0/all/0/1&quot;&gt;Giulia Romoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Sakaki_N/0/1/0/all/0/1&quot;&gt;Naoto Sakaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Shinozaki_K/0/1/0/all/0/1&quot;&gt;Kenji Shinozaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Taille_C/0/1/0/all/0/1&quot;&gt;Christophe De La Taille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Takizawa_Y/0/1/0/all/0/1&quot;&gt;Yoshiyuki Takizawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Vrabel_M/0/1/0/all/0/1&quot;&gt;Michal Vr&amp;#xe1;bel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Wiencke_L/0/1/0/all/0/1&quot;&gt;Lawrence Wiencke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14986">
<title>SAME++: A Self-supervised Anatomical eMbeddings Enhanced medical image registration framework using stable sampling and regularized transformation. (arXiv:2311.14986v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14986</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration is a fundamental medical image analysis task. Ideally,
registration should focus on aligning semantically corresponding voxels, i.e.,
the same anatomical locations. However, existing methods often optimize
similarity measures computed directly on intensities or on hand-crafted
features, which lack anatomical semantic information. These similarity measures
may lead to sub-optimal solutions where large deformations, complex anatomical
differences, or cross-modality imagery exist. In this work, we introduce a fast
and accurate method for unsupervised 3D medical image registration building on
top of a Self-supervised Anatomical eMbedding (SAM) algorithm, which is capable
of computing dense anatomical correspondences between two images at the voxel
level. We name our approach SAM-Enhanced registration (SAME++), which
decomposes image registration into four steps: affine transformation, coarse
deformation, deep non-parametric transformation, and instance optimization.
Using SAM embeddings, we enhance these steps by finding more coherent
correspondence and providing features with better semantic guidance. We
extensively evaluated SAME++ using more than 50 labeled organs on three
challenging inter-subject registration tasks of different body parts. As a
complete registration framework, SAME++ markedly outperforms leading methods by
$4.2\%$ - $8.2\%$ in terms of Dice score while being orders of magnitude faster
than numerical optimization-based methods. Code is available at
\url{https://github.com/alibaba-damo-academy/same}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fengze Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jia Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Le Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niethammer_M/0/1/0/all/0/1&quot;&gt;Marc Niethammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xianghua Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;Daikai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14990">
<title>View it like a radiologist: Shifted windows for deep learning augmentation of CT images. (arXiv:2311.14990v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.14990</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has the potential to revolutionize medical practice by
automating and performing important tasks like detecting and delineating the
size and locations of cancers in medical images. However, most deep learning
models rely on augmentation techniques that treat medical images as natural
images. For contrast-enhanced Computed Tomography (CT) images in particular,
the signals producing the voxel intensities have physical meaning, which is
lost during preprocessing and augmentation when treating such images as natural
images. To address this, we propose a novel preprocessing and intensity
augmentation scheme inspired by how radiologists leverage multiple viewing
windows when evaluating CT images. Our proposed method, window shifting,
randomly places the viewing windows around the region of interest during
training. This approach improves liver lesion segmentation performance and
robustness on images with poorly timed contrast agent. Our method outperforms
classical intensity augmentations as well as the intensity augmentation
pipeline of the popular nn-UNet on multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ostmo_E/0/1/0/all/0/1&quot;&gt;Eirik A. &amp;#xd8;stmo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wickstrom_K/0/1/0/all/0/1&quot;&gt;Kristoffer K. Wickstr&amp;#xf8;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Radiya_K/0/1/0/all/0/1&quot;&gt;Keyur Radiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kampffmeyer_M/0/1/0/all/0/1&quot;&gt;Michael C. Kampffmeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jenssen_R/0/1/0/all/0/1&quot;&gt;Robert Jenssen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14993">
<title>Coordinate-Aware Modulation for Neural Fields. (arXiv:2311.14993v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14993</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural fields, mapping low-dimensional input coordinates to corresponding
signals, have shown promising results in representing various signals. Numerous
methodologies have been proposed, and techniques employing MLPs and grid
representations have achieved substantial success. MLPs allow compact and high
expressibility, yet often suffer from spectral bias and slow convergence speed.
On the other hand, methods using grids are free from spectral bias and achieve
fast training speed, however, at the expense of high spatial complexity. In
this work, we propose a novel way for exploiting both MLPs and grid
representations in neural fields. Unlike the prevalent methods that combine
them sequentially (extract features from the grids first and feed them to the
MLP), we inject spectral bias-free grid representations into the intermediate
features in the MLP. More specifically, we suggest a Coordinate-Aware
Modulation (CAM), which modulates the intermediate features using scale and
shift parameters extracted from the grid representations. This can maintain the
strengths of MLPs while mitigating any remaining potential biases, facilitating
the rapid learning of high-frequency components. In addition, we empirically
found that the feature normalizations, which have not been successful in neural
filed literature, proved to be effective when applied in conjunction with the
proposed CAM. Experimental results demonstrate that CAM enhances the
performance of neural representation and improves learning stability across a
range of signals. Especially in the novel view synthesis task, we achieved
state-of-the-art performance with the least number of parameters and fast
training speed for dynamic scenes and the best performance under 1MB memory for
static scenes. CAM also outperforms the best-performing video compression
methods using neural fields by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joo Chan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rho_D/0/1/0/all/0/1&quot;&gt;Daniel Rho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Seungtae Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;Jong Hwan Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunbyung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15010">
<title>Adapter is All You Need for Tuning Visual Tasks. (arXiv:2311.15010v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.15010</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-training &amp;amp; fine-tuning can enhance the transferring efficiency and
performance in visual tasks. Recent delta-tuning methods provide more options
for visual classification tasks. Despite their success, existing visual
delta-tuning art fails to exceed the upper limit of full fine-tuning on
challenging tasks like instance segmentation and semantic segmentation. To find
a competitive alternative to full fine-tuning, we propose the Multi-cognitive
Visual Adapter (Mona) tuning, a novel adapter-based tuning method. First, we
introduce multiple vision-friendly filters into the adapter to enhance its
ability to process visual signals, while previous methods mainly rely on
language-friendly linear filters. Second, we add the scaled normalization layer
in the adapter to regulate the distribution of input features for visual
filters. To fully demonstrate the practicality and generality of Mona, we
conduct experiments on multiple representative visual tasks, including instance
segmentation on COCO, semantic segmentation on ADE20K, object detection on
Pascal VOC, and image classification on several common datasets. Exciting
results illustrate that Mona surpasses full fine-tuning on all these tasks and
is the only delta-tuning method outperforming full fine-tuning on instance
segmentation and semantic segmentation tasks. For example, Mona achieves a 1%
performance gain on the COCO dataset compared to full fine-tuning.
Comprehensive results suggest that Mona-tuning is more suitable for retaining
and utilizing the capabilities of pre-trained models than full fine-tuning. The
code will be released at https://github.com/Leiyi-Hu/mona.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dongshuo Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Leiyi Hu. Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youqun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15011">
<title>VSCode: General Visual Salient and Camouflaged Object Detection with 2D Prompt Learning. (arXiv:2311.15011v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.15011</link>
<description rdf:parseType="Literal">&lt;p&gt;Salient object detection (SOD) and camouflaged object detection (COD) are
related yet distinct binary mapping tasks. These tasks involve multiple
modalities, sharing commonalities and unique cues. Existing research often
employs intricate task-specific specialist models, potentially leading to
redundancy and suboptimal results. We introduce VSCode, a generalist model with
novel 2D prompt learning, to jointly address four SOD tasks and three COD
tasks. We utilize VST as the foundation model and introduce 2D prompts within
the encoder-decoder architecture to learn domain and task-specific knowledge on
two separate dimensions. A prompt discrimination loss helps disentangle
peculiarities to benefit model optimization. VSCode outperforms
state-of-the-art methods across six tasks on 26 datasets and exhibits zero-shot
generalization to unseen tasks by combining 2D prompts, such as RGB-D COD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziyang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Nian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wangbo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xuguang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dingwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Deng-Ping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Junwei Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15022">
<title>Occlusion Sensitivity Analysis with Augmentation Subspace Perturbation in Deep Feature Space. (arXiv:2311.15022v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.15022</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning of neural networks has gained prominence in multiple
life-critical applications like medical diagnoses and autonomous vehicle
accident investigations. However, concerns about model transparency and biases
persist. Explainable methods are viewed as the solution to address these
challenges. In this study, we introduce the Occlusion Sensitivity Analysis with
Deep Feature Augmentation Subspace (OSA-DAS), a novel perturbation-based
interpretability approach for computer vision. While traditional perturbation
methods make only use of occlusions to explain the model predictions, OSA-DAS
extends standard occlusion sensitivity analysis by enabling the integration
with diverse image augmentations. Distinctly, our method utilizes the output
vector of a DNN to build low-dimensional subspaces within the deep feature
vector space, offering a more precise explanation of the model prediction. The
structural similarity between these subspaces encompasses the influence of
diverse augmentations and occlusions. We test extensively on the ImageNet-1k,
and our class- and model-agnostic approach outperforms commonly used
interpreters, setting it apart in the realm of explainable AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valois_P/0/1/0/all/0/1&quot;&gt;Pedro Valois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niinuma_K/0/1/0/all/0/1&quot;&gt;Koichiro Niinuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fukui_K/0/1/0/all/0/1&quot;&gt;Kazuhiro Fukui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15027">
<title>Double-Flow-based Steganography without Embedding for Image-to-Image Hiding. (arXiv:2311.15027v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.15027</link>
<description rdf:parseType="Literal">&lt;p&gt;As an emerging concept, steganography without embedding (SWE) hides a secret
message without directly embedding it into a cover. Thus, SWE has the unique
advantage of being immune to typical steganalysis methods and can better
protect the secret message from being exposed. However, existing SWE methods
are generally criticized for their poor payload capacity and low fidelity of
recovered secret messages. In this paper, we propose a novel
steganography-without-embedding technique, named DF-SWE, which addresses the
aforementioned drawbacks and produces diverse and natural stego images.
Specifically, DF-SWE employs a reversible circulation of double flow to build a
reversible bijective transformation between the secret image and the generated
stego image. Hence, it provides a way to directly generate stego images from
secret images without a cover image. Besides leveraging the invertible
property, DF-SWE can invert a secret image from a generated stego image in a
nearly lossless manner and increases the fidelity of extracted secret images.
To the best of our knowledge, DF-SWE is the first SWE method that can hide
large images and multiple images into one image with the same size,
significantly enhancing the payload capacity. According to the experimental
results, the payload capacity of DF-SWE achieves 24-72 BPP is 8000-16000 times
compared to its competitors while producing diverse images to minimize the
exposure risk. Importantly, DF-SWE can be applied in the steganography of
secret images in various domains without requiring training data from the
corresponding domains. This domain-agnostic property suggests that DF-SWE can
1) be applied to hiding private data and 2) be deployed in resource-limited
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_B/0/1/0/all/0/1&quot;&gt;Bingbing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Derui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Renyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15038">
<title>Low-latency Visual Previews of Large Synchrotron Micro-CT Datasets. (arXiv:2311.15038v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.15038</link>
<description rdf:parseType="Literal">&lt;p&gt;The unprecedented rate at which synchrotron radiation facilities are
producing micro-computed (micro-CT) datasets has resulted in an overwhelming
amount of data that scientists struggle to browse and interact with in
real-time. Thousands of arthropods are scanned into micro-CT within the NOVA
project, producing a large collection of gigabyte-sized datasets. In this work,
we present methods to reduce the size of this data, scaling it from gigabytes
to megabytes, enabling the micro-CT dataset to be delivered in real-time. In
addition, arthropods can be identified by scientists even after implementing
data reduction methodologies. Our initial step is to devise three distinct
visual previews that comply with the best practices of data exploration.
Subsequently, each visual preview warrants its own design consideration,
thereby necessitating an individual data processing pipeline for each. We aim
to present data reduction algorithms applied across the data processing
pipelines. Particularly, we reduce size by using the multi-resolution
slicemaps, the server-side rendering, and the histogram filtering approaches.
In the evaluation, we examine the disparities of each method to identify the
most favorable arrangement for our operation, which can then be adjusted for
other experiments that have comparable necessities. Our demonstration proved
that reducing the dataset size to the megabyte range is achievable without
compromising the arthropod&apos;s geometry information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jerome_N/0/1/0/all/0/1&quot;&gt;Nicholas Tan Jerome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chilingaryan_S/0/1/0/all/0/1&quot;&gt;Suren Chilingaryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_T/0/1/0/all/0/1&quot;&gt;Thomas van de Kamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopmann_A/0/1/0/all/0/1&quot;&gt;Andreas Kopmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15040">
<title>InstaStyle: Inversion Noise of a Stylized Image is Secretly a Style Adviser. (arXiv:2311.15040v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.15040</link>
<description rdf:parseType="Literal">&lt;p&gt;Stylized text-to-image generation focuses on creating images from textual
descriptions while adhering to a style specified by a few reference images.
However, subtle style variations within different reference images can hinder
the model from accurately learning the target style. In this paper, we propose
InstaStyle, a novel approach that excels in generating high-fidelity stylized
images with only a single reference image. Our approach is based on the finding
that the inversion noise from a stylized reference image inherently carries the
style signal, as evidenced by their non-zero signal-to-noise ratio. We employ
DDIM inversion to extract this noise from the reference image and leverage a
diffusion model to generate new stylized images from the ``style&quot; noise.
Additionally, the inherent ambiguity and bias of textual prompts impede the
precise conveying of style. To address this, we introduce a learnable style
token via prompt refinement, which enhances the accuracy of the style
description for the reference image. Qualitative and quantitative experimental
results demonstrate that InstaStyle achieves superior performance compared to
current benchmarks. Furthermore, our approach also showcases its capability in
the creative task of style combination with mixed inversion noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xing Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zekun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Pei Pei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaibo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhaofeng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15053">
<title>Task adaption by biologically inspired stochastic comodulation. (arXiv:2311.15053v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.15053</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain representations must strike a balance between generalizability and
adaptability. Neural codes capture general statistical regularities in the
world, while dynamically adjusting to reflect current goals. One aspect of this
adaptation is stochastically co-modulating neurons&apos; gains based on their task
relevance. These fluctuations then propagate downstream to guide
decision-making. Here, we test the computational viability of such a scheme in
the context of multi-task learning. We show that fine-tuning convolutional
networks by stochastic gain modulation improves on deterministic gain
modulation, achieving state-of-the-art results on the CelebA dataset. To better
understand the mechanisms supporting this improvement, we explore how
fine-tuning performance is affected by architecture using Cifar-100. Overall,
our results suggest that stochastic comodulation can enhance learning
efficiency and performance in multi-task learning, without additional learnable
parameters. This offers a promising new direction for developing more flexible
and robust intelligent systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boeshertz_G/0/1/0/all/0/1&quot;&gt;Gauthier Boeshertz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haimerl_C/0/1/0/all/0/1&quot;&gt;Caroline Haimerl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savin_C/0/1/0/all/0/1&quot;&gt;Cristina Savin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.10926">
<title>Rethinking Global Context in Crowd Counting. (arXiv:2105.10926v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2105.10926</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates the role of global context for crowd counting.
Specifically, a pure transformer is used to extract features with global
information from overlapping image patches. Inspired by classification, we add
a context token to the input sequence, to facilitate information exchange with
tokens corresponding to image patches throughout transformer layers. Due to the
fact that transformers do not explicitly model the tried-and-true channel-wise
interactions, we propose a token-attention module (TAM) to recalibrate encoded
features through channel-wise attention informed by the context token. Beyond
that, it is adopted to predict the total person count of the image through
regression-token module (RTM). Extensive experiments on various datasets,
including ShanghaiTech, UCF-QNRF, JHU-CROWD++ and NWPU, demonstrate that the
proposed context extraction techniques can significantly improve the
performance over the baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_G/0/1/0/all/0/1&quot;&gt;Guolei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Probst_T/0/1/0/all/0/1&quot;&gt;Thomas Probst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1&quot;&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1&quot;&gt;Nikola Popovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.14019">
<title>Semi-supervised Salient Object Detection with Effective Confidence Estimation. (arXiv:2112.14019v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.14019</link>
<description rdf:parseType="Literal">&lt;p&gt;The success of existing salient object detection models relies on a large
pixel-wise labeled training dataset, which is time-consuming and expensive to
obtain. We study semi-supervised salient object detection, with access to a
small number of labeled samples and a large number of unlabeled samples.
Specifically, we present a pseudo label based learn-ing framework with a
Conditional Energy-based Model. We model the stochastic nature of human
saliency labels using the stochastic latent variable of the Conditional
Energy-based Model. It further enables generation of a high-quality pixel-wise
uncertainty map, highlighting the reliability of corresponding pseudo label
generated for the unlabeled sample. This minimises the contribution of
low-certainty pseudo labels in optimising the model, preventing the error
propagation. Experimental results show that the proposed strategy can
effectively explore the contribution of unlabeled data. With only 1/16 labeled
samples, our model achieves competitive performance compared with
state-of-the-art fully-supervised models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1&quot;&gt;Nick Barnes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.03005">
<title>Self-Supervised Face Image Restoration with a One-Shot Reference. (arXiv:2203.03005v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.03005</link>
<description rdf:parseType="Literal">&lt;p&gt;For image restoration, methods leveraging priors from generative models have
been proposed and demonstrated a promising capacity to robustly restore
photorealistic and high-quality results. However, these methods are susceptible
to semantic ambiguity, particularly with images that have obviously correct
semantics such as facial images. In this paper, we propose a semantic-aware
latent space exploration method for image restoration (SAIR). By explicitly
modeling semantics information from a given reference image, SAIR is able to
reliably restore severely degraded images not only to high-resolution and
highly realistic looks but also to correct semantics. Quantitative and
qualitative experiments collectively demonstrate the superior performance of
the proposed SAIR. Our code is available at https://github.com/Liamkuo/SAIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yanhui Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fangzhou Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shaoyuan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.05859">
<title>Bootstrap Motion Forecasting With Self-Consistent Constraints. (arXiv:2204.05859v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.05859</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework to bootstrap Motion forecasting with
Self-consistent Constraints (MISC). The motion forecasting task aims at
predicting future trajectories of vehicles by incorporating spatial and
temporal information from the past. A key design of MISC is the proposed Dual
Consistency Constraints that regularize the predicted trajectories under
spatial and temporal perturbation during training. Also, to model the
multi-modality in motion forecasting, we design a novel self-ensembling scheme
to obtain accurate teacher targets to enforce the self-constraints with
multi-modality supervision. With explicit constraints from multiple teacher
targets, we observe a clear improvement in the prediction performance.
Extensive experiments on the Argoverse motion forecasting benchmark and Waymo
Open Motion dataset show that MISC significantly outperforms the
state-of-the-art methods. As the proposed strategies are general and can be
easily incorporated into other motion forecasting approaches, we also
demonstrate that our proposed scheme consistently improves the prediction
performance of several existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Maosheng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiamiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xunnong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Tongyi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10089">
<title>Kernel Normalized Convolutional Networks. (arXiv:2205.10089v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10089</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing convolutional neural network architectures frequently rely upon
batch normalization (BatchNorm) to effectively train the model. BatchNorm,
however, performs poorly with small batch sizes, and is inapplicable to
differential privacy. To address these limitations, we propose kernel
normalization and kernel normalized convolutional layers, and incorporate them
into kernel normalized convolutional networks (KNConvNets) as the main building
blocks. We implement KNConvNets corresponding to the state-of-the-art ResNets
while forgoing BatchNorm layers. Through extensive experiments, we illustrate
KNConvNets achieve higher or competitive performance compared to the BatchNorm
counterparts in image classification and semantic segmentation. They also
significantly outperform their batch-independent competitors including layer
and group normalization in non-private and differentially private training.
Given that, KNConvNets combine the batch-independence property of layer and
group normalization with the performance advantage of BatchNorm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasirigerdeh_R/0/1/0/all/0/1&quot;&gt;Reza Nasirigerdeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torkzadehmahani_R/0/1/0/all/0/1&quot;&gt;Reihaneh Torkzadehmahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1&quot;&gt;Georgios Kaissis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.14362">
<title>AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels. (arXiv:2208.14362v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.14362</link>
<description rdf:parseType="Literal">&lt;p&gt;Weak supervision (WS) is a powerful method to build labeled datasets for
training supervised models in the face of little-to-no labeled data. It
replaces hand-labeling data with aggregating multiple noisy-but-cheap label
estimates expressed by labeling functions (LFs). While it has been used
successfully in many domains, weak supervision&apos;s application scope is limited
by the difficulty of constructing labeling functions for domains with complex
or high-dimensional features. To address this, a handful of methods have
proposed automating the LF design process using a small set of ground truth
labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating
automated WS (AutoWS) techniques in challenging WS settings -- a set of diverse
application domains on which it has been previously difficult or impossible to
apply traditional WS techniques. While AutoWS is a promising direction toward
expanding the application-scope of WS, the emergence of powerful methods such
as zero-shot foundation models reveals the need to understand how AutoWS
techniques compare or cooperate with modern zero-shot or few-shot learners.
This informs the central question of AutoWS-Bench-101: given an initial set of
100 labels for each task, we ask whether a practitioner should use an AutoWS
method to generate additional labels or use some simpler baseline, such as
zero-shot predictions from a foundation model or supervised learning. We
observe that in many settings, it is necessary for AutoWS methods to
incorporate signal from foundation models if they are to outperform simple
few-shot baselines, and AutoWS-Bench-101 promotes future research in this
direction. We conclude with a thorough ablation study of AutoWS methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1&quot;&gt;Nicholas Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xintong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzu-Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1&quot;&gt;Dyah Adila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenberg_S/0/1/0/all/0/1&quot;&gt;Spencer Schoenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cheng-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pick_L/0/1/0/all/0/1&quot;&gt;Lauren Pick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haotian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1&quot;&gt;Aws Albarghouthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00462">
<title>MA-RECON: Mask-aware deep-neural-network for robust fast MRI k-space interpolation. (arXiv:2209.00462v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00462</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality reconstruction of MRI images from under-sampled `k-space&apos; data,
which is in the Fourier domain, is crucial for shortening MRI acquisition times
and ensuring superior temporal resolution. Over recent years, a wealth of deep
neural network (DNN) methods have emerged, aiming to tackle the complex,
ill-posed inverse problem linked to this process. However, their instability
against variations in the acquisition process and anatomical distribution
exposes a deficiency in the generalization of relevant physical models within
these DNN architectures. The goal of our work is to enhance the generalization
capabilities of DNN methods for k-space interpolation by introducing
`MA-RECON&apos;, an innovative mask-aware DNN architecture and associated training
method. Unlike preceding approaches, our `MA-RECON&apos; architecture encodes not
only the observed data but also the under-sampling mask within the model
structure. It implements a tailored training approach that leverages data
generated with a variety of under-sampling masks to stimulate the model&apos;s
generalization of the under-sampled MRI reconstruction problem. Therefore,
effectively represents the associated inverse problem, akin to the classical
compressed sensing approach. The benefits of our MA-RECON approach were
affirmed through rigorous testing with the widely accessible fastMRI dataset.
Compared to standard DNN methods and DNNs trained with under-sampling mask
augmentation, our approach demonstrated superior generalization capabilities.
This resulted in a considerable improvement in robustness against variations in
both the acquisition process and anatomical distribution, especially in regions
with pathology. In conclusion, our mask-aware strategy holds promise for
enhancing the generalization capacity and robustness of DNN-based methodologies
for MRI reconstruction from undersampled k-space data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Avidan_N/0/1/0/all/0/1&quot;&gt;Nitzan Avidan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Freiman_M/0/1/0/all/0/1&quot;&gt;Moti Freiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07042">
<title>Efficient Perception, Planning, and Control Algorithms for Vision-Based Automated Vehicles. (arXiv:2209.07042v5 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07042</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles have limited computational resources; hence, their
control systems must be efficient. The cost and size of sensors have limited
the development of self-driving cars. To overcome these restrictions, this
study proposes an efficient framework for the operation of vision-based
automatic vehicles; the framework requires only a monocular camera and a few
inexpensive radars. The proposed algorithm comprises a multi-task UNet (MTUNet)
network for extracting image features and constrained iterative linear
quadratic regulator (CILQR) and vision predictive control (VPC) modules for
rapid motion planning and control. MTUNet is designed to simultaneously solve
lane line segmentation, the ego vehicle&apos;s heading angle regression, road type
classification, and traffic object detection tasks at approximately 40 FPS
(frames per second) for 228 x 228 pixel RGB input images. The CILQR controllers
then use the MTUNet outputs and radar data as inputs to produce driving
commands for lateral and longitudinal vehicle guidance within only 1 ms. In
particular, the VPC algorithm is included to reduce steering command latency to
below actuator latency to prevent self-driving vehicle performance degradation
during tight turns. The VPC algorithm uses road curvature data from MTUNet to
estimate the correction of the current steering angle at a look-ahead point to
adjust the turning amount. Including the VPC algorithm in a VPC-CILQR
controller on curvy roads leads to higher performance than CILQR alone. Our
experiments demonstrate that the proposed autonomous driving system, which does
not require high-definition maps, could be applied in current autonomous
vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Der-Hau Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13204">
<title>NEURAL MARIONETTE: A Transformer-based Multi-action Human Motion Synthesis System. (arXiv:2209.13204v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13204</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a neural network-based system for long-term, multi-action human
motion synthesis. The system, dubbed as NEURAL MARIONETTE, can produce
high-quality and meaningful motions with smooth transitions from simple user
input, including a sequence of action tags with expected action duration, and
optionally a hand-drawn moving trajectory if the user specifies. The core of
our system is a novel Transformer-based motion generation model, namely
MARIONET, which can generate diverse motions given action tags. Different from
existing motion generation models, MARIONET utilizes contextual information
from the past motion clip and future action tag, dedicated to generating
actions that can smoothly blend historical and future actions. Specifically,
MARIONET first encodes target action tag and contextual information into an
action-level latent code. The code is unfolded into frame-level control signals
via a time unrolling module, which could be then combined with other
frame-level control signals like the target trajectory. Motion frames are then
generated in an auto-regressive way. By sequentially applying MARIONET, the
system NEURAL MARIONETTE can robustly generate long-term, multi-action motions
with the help of two simple schemes, namely &quot;Shadow Start&quot; and &quot;Action
Revision&quot;. Along with the novel system, we also present a new dataset dedicated
to the multi-action motion synthesis task, which contains both action tags and
their contextual information. Extensive experiments are conducted to study the
action accuracy, naturalism, and transition smoothness of the motions generated
by our system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhe_X/0/1/0/all/0/1&quot;&gt;Xuefei Zhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_Q/0/1/0/all/0/1&quot;&gt;Qiuhong Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Di Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tingguang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruizhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_L/0/1/0/all/0/1&quot;&gt;Linchao Bao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.00716">
<title>rPPG-Toolbox: Deep Remote PPG Toolbox. (arXiv:2210.00716v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.00716</link>
<description rdf:parseType="Literal">&lt;p&gt;Camera-based physiological measurement is a fast growing field of computer
vision. Remote photoplethysmography (rPPG) utilizes imaging devices (e.g.,
cameras) to measure the peripheral blood volume pulse (BVP) via
photoplethysmography, and enables cardiac measurement via webcams and
smartphones. However, the task is non-trivial with important pre-processing,
modeling, and post-processing steps required to obtain state-of-the-art
results. Replication of results and benchmarking of new models is critical for
scientific progress; however, as with many other applications of deep learning,
reliable codebases are not easy to find or use. We present a comprehensive
toolbox, rPPG-Toolbox, that contains unsupervised and supervised rPPG models
with support for public benchmark datasets, data augmentation, and systematic
evaluation: \url{https://github.com/ubicomplab/rPPG-Toolbox}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narayanswamy_G/0/1/0/all/0/1&quot;&gt;Girish Narayanswamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paruchuri_A/0/1/0/all/0/1&quot;&gt;Akshay Paruchuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiankai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_S/0/1/0/all/0/1&quot;&gt;Soumyadip Sengupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1&quot;&gt;Daniel McDuff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03829">
<title>Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03829</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive review of past and current advances in
the early detection of bark beetle-induced tree mortality from three primary
perspectives: bark beetle &amp;amp; host interactions, RS, and ML/DL. In contrast to
prior efforts, this review encompasses all RS systems and emphasizes ML/DL
methods to investigate their strengths and weaknesses. We parse existing
literature based on multi- or hyper-spectral analyses and distill their
knowledge based on: bark beetle species &amp;amp; attack phases with a primary emphasis
on early stages of attacks, host trees, study regions, RS platforms &amp;amp; sensors,
spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation
indices (SVIs), ML approaches, learning schemes, task categories, models,
algorithms, classes/clusters, features, and DL networks &amp;amp; architectures.
Although DL-based methods and the random forest (RF) algorithm showed promising
results, highlighting their potential to detect subtle changes across visible,
thermal, and short-wave infrared (SWIR) spectral regions, they still have
limited effectiveness and high uncertainties. To inspire novel solutions to
these shortcomings, we delve into the principal challenges &amp;amp; opportunities from
different perspectives, enabling a deeper understanding of the current state of
research and guiding future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1&quot;&gt;Devin Goodsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1&quot;&gt;Nilanjan Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1&quot;&gt;Nadir Erbilgin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06462">
<title>Self-Guided Diffusion Models. (arXiv:2210.06462v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06462</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated remarkable progress in image generation
quality, especially when guidance is used to control the generative process.
However, guidance requires a large amount of image-annotation pairs for
training and is thus dependent on their availability, correctness and
unbiasedness. In this paper, we eliminate the need for such annotation by
instead leveraging the flexibility of self-supervision signals to design a
framework for self-guided diffusion models. By leveraging a feature extraction
function and a self-annotation function, our method provides guidance signals
at various image granularities: from the level of holistic images to object
boxes and even segmentation masks. Our experiments on single-label and
multi-label image datasets demonstrate that self-labeled guidance always
outperforms diffusion models without guidance and may even surpass guidance
based on ground-truth labels, especially on unbalanced data. When equipped with
self-supervised box or mask proposals, our method further generates visually
diverse yet semantically consistent images, without the need for any class,
box, or segment label annotation. Self-guided diffusion is simple, flexible and
expected to profit from deployment at scale. Source code will be at:
https://taohu.me/sgdm/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1&quot;&gt;Vincent Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David W Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki M. Asano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1&quot;&gt;Gertjan J. Burghouts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09846">
<title>G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System. (arXiv:2210.09846v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09846</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating dynamic physical environments without obstructing or damaging
human assets is of quintessential importance for social robots. In this work,
we solve autonomous drone navigation&apos;s sub-problem of predicting out-of-domain
human and agent trajectories using a deep generative model. Our method:
General-PECNet or G-PECNet observes an improvement of 9.5\% on the Final
Displacement Error (FDE) on 2020&apos;s benchmark: PECNet through a combination of
architectural improvements inspired by periodic activation functions and
synthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and
Reinforcement Learning (RL). Additionally, we propose a simple
geometry-inspired metric for trajectory non-linearity and outlier detection,
helpful for the task. Code available at
$\href{https://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git}{GitHub}$
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Aryan Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rameshan_R/0/1/0/all/0/1&quot;&gt;Renu M. Rameshan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14309">
<title>FutureHuman3D: Forecasting Complex Long-Term 3D Human Behavior from Video Observations. (arXiv:2211.14309v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14309</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a generative approach to forecast long-term future human behavior
in 3D, requiring only weak supervision from readily available 2D human action
data. This is a fundamental task enabling many downstream applications. The
required ground-truth data is hard to capture in 3D (mocap suits, expensive
setups) but easy to acquire in 2D (simple RGB cameras). Thus, we design our
method to only require 2D RGB data while being able to generate 3D human motion
sequences. We use a differentiable 2D projection scheme in an autoregressive
manner for weak supervision, and an adversarial loss for 3D regularization. Our
method predicts long and complex behavior sequences (e.g. cooking, assembly)
consisting of multiple sub-actions. We tackle this in a semantically
hierarchical manner, jointly predicting high-level coarse action labels
together with their low-level fine-grained realizations as characteristic 3D
human poses. We observe that these two action representations are coupled in
nature, and joint prediction benefits both action and pose forecasting. Our
experiments demonstrate the complementary nature of joint action and 3D pose
prediction: our joint approach outperforms each task treated individually,
enables robust longer-term sequence prediction, and outperforms alternative
approaches to forecast actions and characteristic 3D poses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diller_C/0/1/0/all/0/1&quot;&gt;Christian Diller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funkhouser_T/0/1/0/all/0/1&quot;&gt;Thomas Funkhouser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Angela Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14456">
<title>TetraSphere: A Neural Descriptor for O(3)-Invariant Point Cloud Analysis. (arXiv:2211.14456v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14456</link>
<description rdf:parseType="Literal">&lt;p&gt;In many practical applications, 3D point cloud analysis requires rotation
invariance. In this paper, we present a learnable descriptor invariant under 3D
rotations and reflections, i.e., the O(3) actions, utilizing the recently
introduced steerable 3D spherical neurons and vector neurons. Specifically, we
propose an embedding of the 3D spherical neurons into 4D vector neurons, which
leverages end-to-end training of the model. In our approach, we perform
TetraTransform--an equivariant embedding of the 3D input into 4D, constructed
from the steerable neurons--and extract deeper O(3)-equivariant features using
vector neurons. This integration of the TetraTransform into the VN-DGCNN
framework, termed TetraSphere, negligibly increases the number of parameters by
less than 0.0002%. TetraSphere sets a new state-of-the-art performance
classifying randomly rotated real-world object scans of the challenging subsets
of ScanObjectNN. Additionally, TetraSphere outperforms all equivariant methods
on randomly rotated synthetic data: classifying objects from ModelNet40 and
segmenting parts of the ShapeNet shapes. Thus, our results reveal the practical
value of steerable 3D spherical neurons for learning in 3D Euclidean space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnyk_P/0/1/0/all/0/1&quot;&gt;Pavlo Melnyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_A/0/1/0/all/0/1&quot;&gt;Andreas Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Felsberg_M/0/1/0/all/0/1&quot;&gt;Michael Felsberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadenback_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;rten Wadenb&amp;#xe4;ck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09068">
<title>Style-Hallucinated Dual Consistency Learning: A Unified Framework for Visual Domain Generalization. (arXiv:2212.09068v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09068</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain shift widely exists in the visual world, while modern deep neural
networks commonly suffer from severe performance degradation under domain shift
due to the poor generalization ability, which limits the real-world
applications. The domain shift mainly lies in the limited source environmental
variations and the large distribution gap between source and unseen target
data. To this end, we propose a unified framework, Style-HAllucinated Dual
consistEncy learning (SHADE), to handle such domain shift in various visual
tasks. Specifically, SHADE is constructed based on two consistency constraints,
Style Consistency (SC) and Retrospection Consistency (RC). SC enriches the
source situations and encourages the model to learn consistent representation
across style-diversified samples. RC leverages general visual knowledge to
prevent the model from overfitting to source data and thus largely keeps the
representation consistent between the source and general visual models.
Furthermore, we present a novel style hallucination module (SHM) to generate
style-diversified samples that are essential to consistency learning. SHM
selects basis styles from the source distribution, enabling the model to
dynamically generate diverse and realistic samples during training. Extensive
experiments demonstrate that our versatile SHADE can significantly enhance the
generalization in various visual recognition tasks, including image
classification, semantic segmentation and object detection, with different
models, i.e., ConvNets and Transformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_N/0/1/0/all/0/1&quot;&gt;Na Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05543">
<title>Adding Conditional Control to Text-to-Image Diffusion Models. (arXiv:2302.05543v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05543</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ControlNet, a neural network architecture to add spatial
conditioning controls to large, pretrained text-to-image diffusion models.
ControlNet locks the production-ready large diffusion models, and reuses their
deep and robust encoding layers pretrained with billions of images as a strong
backbone to learn a diverse set of conditional controls. The neural
architecture is connected with &quot;zero convolutions&quot; (zero-initialized
convolution layers) that progressively grow the parameters from zero and ensure
that no harmful noise could affect the finetuning. We test various conditioning
controls, eg, edges, depth, segmentation, human pose, etc, with Stable
Diffusion, using single or multiple conditions, with or without prompts. We
show that the training of ControlNets is robust with small (&amp;lt;50k) and large
(&amp;gt;1m) datasets. Extensive results show that ControlNet may facilitate wider
applications to control image diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lvmin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Anyi Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawala_M/0/1/0/all/0/1&quot;&gt;Maneesh Agrawala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06226">
<title>NeRFlame: FLAME-based conditioning of NeRF for 3D face rendering. (arXiv:2303.06226v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06226</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional 3D face models are based on mesh representations with texture.
One of the most important models is FLAME (Faces Learned with an Articulated
Model and Expressions), which produces meshes of human faces that are fully
controllable. Unfortunately, such models have problems with capturing geometric
and appearance details. In contrast to mesh representation, the neural radiance
field (NeRF) produces extremely sharp renders. However, implicit methods are
hard to animate and do not generalize well to unseen expressions. It is not
trivial to effectively control NeRF models to obtain face manipulation.
&lt;/p&gt;
&lt;p&gt;The present paper proposes a novel approach, named NeRFlame, which combines
the strengths of both NeRF and FLAME methods. Our method enables high-quality
rendering capabilities of NeRF while also offering complete control over the
visual appearance, similar to FLAME. In contrast to traditional NeRF-based
structures that use neural networks for RGB color and volume density modeling,
our approach utilizes the FLAME mesh as a distinct density volume.
Consequently, color values exist only in the vicinity of the FLAME mesh. This
FLAME framework is seamlessly incorporated into the NeRF architecture for
predicting RGB colors, enabling our model to explicitly represent volume
density and implicitly capture RGB colors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zajac_W/0/1/0/all/0/1&quot;&gt;Wojciech Zaj&amp;#x105;c&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waczynska_J/0/1/0/all/0/1&quot;&gt;Joanna Waczy&amp;#x144;ska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borycki_P/0/1/0/all/0/1&quot;&gt;Piotr Borycki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabor_J/0/1/0/all/0/1&quot;&gt;Jacek Tabor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1&quot;&gt;Maciej Zi&amp;#x119;ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spurek_P/0/1/0/all/0/1&quot;&gt;Przemys&amp;#x142;aw Spurek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08730">
<title>DiffusionAD: Norm-guided One-step Denoising Diffusion for Anomaly Detection. (arXiv:2303.08730v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08730</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection has garnered extensive applications in real industrial
manufacturing due to its remarkable effectiveness and efficiency. However,
previous generative-based models have been limited by suboptimal reconstruction
quality, hampering their overall performance. A fundamental enhancement lies in
our reformulation of the reconstruction process using a diffusion model into a
noise-to-norm paradigm. Here, anomalous regions are perturbed with Gaussian
noise and reconstructed as normal, overcoming the limitations of previous
models by facilitating anomaly-free restoration. Additionally, we propose a
rapid one-step denoising paradigm, significantly faster than the traditional
iterative denoising in diffusion models. Furthermore, the introduction of the
norm-guided paradigm elevates the accuracy and fidelity of reconstructions. The
segmentation sub-network predicts pixel-level anomaly scores using the input
image and its anomaly-free restoration. Comprehensive evaluations on four
standard and challenging benchmarks reveal that DiffusionAD outperforms current
state-of-the-art approaches, demonstrating the effectiveness and broad
applicability of the proposed pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10594">
<title>AdaptGuard: Defending Against Universal Attacks for Model Adaptation. (arXiv:2303.10594v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10594</link>
<description rdf:parseType="Literal">&lt;p&gt;Model adaptation aims at solving the domain transfer problem under the
constraint of only accessing the pretrained source models. With the increasing
considerations of data privacy and transmission efficiency, this paradigm has
been gaining recent popularity. This paper studies the vulnerability to
universal attacks transferred from the source domain during model adaptation
algorithms due to the existence of malicious providers. We explore both
universal adversarial perturbations and backdoor attacks as loopholes on the
source side and discover that they still survive in the target models after
adaptation. To address this issue, we propose a model preprocessing framework,
named AdaptGuard, to improve the security of model adaptation algorithms.
AdaptGuard avoids direct use of the risky source parameters through knowledge
distillation and utilizes the pseudo adversarial samples under adjusted radius
to enhance the robustness. AdaptGuard is a plug-and-play module that requires
neither robust pretrained models nor any changes for the following model
adaptation algorithms. Extensive results on three commonly used datasets and
two popular adaptation methods validate that AdaptGuard can effectively defend
against universal attacks and maintain clean accuracy in the target domain
simultaneously. We hope this research will shed light on the safety and
robustness of transfer learning. Code is available at
https://github.com/TomSheng21/AdaptGuard.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lijun Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jian Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Ran He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zilei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_T/0/1/0/all/0/1&quot;&gt;Tieniu Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15445">
<title>IRFL: Image Recognition of Figurative Language. (arXiv:2303.15445v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15445</link>
<description rdf:parseType="Literal">&lt;p&gt;Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge. In this work, we develop the Image Recognition of
Figurative Language (IRFL) dataset. We leverage human annotation and an
automatic pipeline we created to generate a multimodal dataset, and introduce
two novel tasks as a benchmark for multimodal figurative language
understanding. We experimented with state-of-the-art vision and language models
and found that the best (22%) performed substantially worse than humans (97%).
We release our dataset, benchmark, and code, in hopes of driving the
development of models that can better understand figurative language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1&quot;&gt;Ron Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1&quot;&gt;Dafna Shahaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00553">
<title>From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00553</link>
<description rdf:parseType="Literal">&lt;p&gt;As a vital step toward the intelligent agent, Action understanding matters
for intelligent agents and has attracted long-term attention. It can be formed
as the mapping from the action physical space to the semantic space. Typically,
researchers built action datasets according to idiosyncratic choices to define
classes and push the envelope of benchmarks respectively. Thus, datasets are
incompatible with each other like &quot;Isolated Islands&quot; due to semantic gaps and
various class granularities, e.g., do housework in dataset A and wash plate in
dataset B. We argue that a more principled semantic space is an urgent need to
concentrate the community efforts and enable us to use all datasets together to
pursue generalizable action learning. To this end, we design a structured
action semantic space in view of verb taxonomy hierarchy and covering massive
actions. By aligning the classes of previous datasets to our semantic space, we
gather (image/video/skeleton/MoCap) datasets into a unified database in a
unified label system, i.e., bridging ``isolated islands&apos;&apos; into a &quot;Pangea&quot;.
Accordingly, we propose a novel model mapping from the physical space to
semantic space to fully use Pangea. In extensive experiments, our new system
shows significant superiority, especially in transfer learning. Code and data
will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1&quot;&gt;Yiming Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yikun Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jingru Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xudong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02833">
<title>DoUnseen: Tuning-Free Class-Adaptive Object Detection of Unseen Objects for Robotic Grasping. (arXiv:2304.02833v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02833</link>
<description rdf:parseType="Literal">&lt;p&gt;How can we segment varying numbers of objects where each specific object
represents its own separate class? To make the problem even more realistic, how
can we add and delete classes on the fly without retraining or fine-tuning?
This is the case of robotic applications where no datasets of the objects exist
or application that includes thousands of objects (E.g., in logistics) where it
is impossible to train a single model to learn all of the objects. Most current
research on object segmentation for robotic grasping focuses on class-level
object segmentation (E.g., box, cup, bottle), closed sets (specific objects of
a dataset; for example, YCB dataset), or deep learning-based template matching.
In this work, we are interested in open sets where the number of classes is
unknown, varying, and without pre-knowledge about the objects&apos; types. We
consider each specific object as its own separate class. Our goal is to develop
an object detector that requires no fine-tuning and can add any object as a
class just by capturing a few images of the object. Our main idea is to break
the segmentation pipelines into two steps by combining unseen object
segmentation networks cascaded by class-adaptive classifiers. We evaluate our
class-adaptive object detector on unseen datasets and compare it to a trained
Mask R-CNN on those datasets. The results show that the performance varies from
practical to unsuitable depending on the environment setup and the objects
being handled. The code is available in our DoUnseen library repository.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gouda_A/0/1/0/all/0/1&quot;&gt;Anas Gouda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roidl_M/0/1/0/all/0/1&quot;&gt;Moritz Roidl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02970">
<title>A Closer Look at Audio-Visual Segmentation. (arXiv:2304.02970v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02970</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual segmentation (AVS) is a complex task that involves accurately
segmenting the corresponding sounding object based on audio-visual queries.
Successful audio-visual learning requires two essential components: 1) an
unbiased dataset with high-quality pixel-level multi-class labels, and 2) a
model capable of effectively linking audio information with its corresponding
visual object. However, these two requirements are only partially addressed by
current methods, with training sets containing biased audio-visual data, and
models that generalise poorly beyond this biased training set. In this work, we
propose a new strategy to build cost-effective and relatively unbiased
audio-visual semantic segmentation benchmarks. Our strategy, called Visual
Post-production (VPO), explores the observation that it is not necessary to
have explicit audio-visual pairs extracted from single video sources to build
such benchmarks. We also refine the previously proposed AVSBench to transform
it into the audio-visual semantic segmentation benchmark AVSBench-Single+.
Furthermore, this paper introduces a new pixel-wise audio-visual contrastive
learning method to enable a better generalisation of the model beyond the
training set. We verify the validity of the VPO strategy by showing that
state-of-the-art (SOTA) models trained with datasets built by matching audio
and visual data from different sources or with datasets containing audio and
visual data from the same video source produce almost the same accuracy. Then,
using the proposed VPO benchmarks and AVSBench-Single+, we show that our method
produces more accurate audio-visual semantic segmentation than SOTA models.
Code and dataset will be available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fengbei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05646">
<title>Breaking Modality Disparity: Harmonized Representation for Infrared and Visible Image Registration. (arXiv:2304.05646v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05646</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the differences in viewing range, resolution and relative position, the
multi-modality sensing module composed of infrared and visible cameras needs to
be registered so as to have more accurate scene perception. In practice, manual
calibration-based registration is the most widely used process, and it is
regularly calibrated to maintain accuracy, which is time-consuming and
labor-intensive. To cope with these problems, we propose a scene-adaptive
infrared and visible image registration. Specifically, in regard of the
discrepancy between multi-modality images, an invertible translation process is
developed to establish a modality-invariant domain, which comprehensively
embraces the feature intensity and distribution of both infrared and visible
modalities. We employ homography to simulate the deformation between different
planes and develop a hierarchical framework to rectify the deformation inferred
from the proposed latent representation in a coarse-to-fine manner. For that,
the advanced perception ability coupled with the residual estimation conducive
to the regression of sparse offsets, and the alternate correlation search
facilitates a more accurate correspondence matching. Moreover, we propose the
first ground truth available misaligned infrared and visible image dataset,
involving three synthetic sets and one real-world set. Extensive experiments
validate the effectiveness of the proposed method against the
state-of-the-arts, advancing the subsequent applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhiying Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zengxi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Risheng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10001">
<title>Weakly Supervised Detection of Baby Cry. (arXiv:2304.10001v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10001</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of baby cries is an important part of baby monitoring and health
care. Almost all existing methods use supervised SVM, CNN, or their varieties.
In this work, we propose to use weakly supervised anomaly detection to detect a
baby cry. In this weak supervision, we only need weak annotation if there is a
cry in an audio file. We design a data mining technique using the pre-trained
VGGish feature extractor and an anomaly detection network on long untrimmed
audio files. The obtained datasets are used to train a simple CNN feature
network for cry/non-cry classification. This CNN is then used as a feature
extractor in an anomaly detection framework to achieve better cry detection
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weijun Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Qi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingfeng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08946">
<title>Image Matching by Bare Homography. (arXiv:2305.08946v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08946</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents Slime, a novel non-deep image matching framework which
models the scene as rough local overlapping planes. This intermediate
representation sits in-between the local affine approximation of the keypoint
patches and the global matching based on both spatial and similarity
constraints, providing a progressive pruning of the correspondences, as planes
are easier to handle with respect to general scenes.
&lt;/p&gt;
&lt;p&gt;Slime decomposes the images into overlapping regions at different scales and
computes loose planar homographies. Planes are mutually extended by compatible
matches and the images are split into fixed tiles, with only the best
homographies retained for each pair of tiles. Stable matches are identified
according to the consensus of the admissible stereo configurations provided by
pairwise homographies. Within tiles, the rough planes are then merged according
to their overlap in terms of matches and further consistent correspondences are
extracted.
&lt;/p&gt;
&lt;p&gt;The whole process only involves homography constraints. As a result, both the
coverage and the stability of correct matches over the scene are amplified,
together with the ability to spot matches in challenging scenes, allowing
traditional hybrid matching pipelines to make up lost ground against recent
end-to-end deep matching methods.
&lt;/p&gt;
&lt;p&gt;In addition, the paper gives a thorough comparative analysis of recent
state-of-the-art in image matching represented by end-to-end deep networks and
hybrid pipelines. The evaluation considers both planar and non-planar scenes,
taking into account critical and challenging scenarios including abrupt
temporal image changes and strong variations in relative image rotations.
According to this analysis, although the impressive progress done in this
field, there is still a wide room for improvements to be investigated in future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellavia_F/0/1/0/all/0/1&quot;&gt;Fabio Bellavia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15617">
<title>ISLE: An Intelligent Streaming Framework for High-Throughput AI Inference in Medical Imaging. (arXiv:2305.15617v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15617</link>
<description rdf:parseType="Literal">&lt;p&gt;As the adoption of Artificial Intelligence (AI) systems within the clinical
environment grows, limitations in bandwidth and compute can create
communication bottlenecks when streaming imaging data, leading to delays in
patient care and increased cost. As such, healthcare providers and AI vendors
will require greater computational infrastructure, therefore dramatically
increasing costs. To that end, we developed ISLE, an intelligent streaming
framework for high-throughput, compute- and bandwidth- optimized, and cost
effective AI inference for clinical decision making at scale. In our
experiments, ISLE on average reduced data transmission by 98.02% and decoding
time by 98.09%, while increasing throughput by 2,730%. We show that ISLE
results in faster turnaround times, and reduced overall cost of data,
transmission, and compute, without negatively impacting clinical decision
making using AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kulkarni_P/0/1/0/all/0/1&quot;&gt;Pranav Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garin_S/0/1/0/all/0/1&quot;&gt;Sean Garin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kanhere_A/0/1/0/all/0/1&quot;&gt;Adway Kanhere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siegel_E/0/1/0/all/0/1&quot;&gt;Eliot Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yi_P/0/1/0/all/0/1&quot;&gt;Paul H. Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Parekh_V/0/1/0/all/0/1&quot;&gt;Vishwa S. Parekh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19599">
<title>RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine Semantic Re-alignment. (arXiv:2305.19599v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19599</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in text-to-image diffusion models have achieved remarkable
success in generating high-quality, realistic images from textual descriptions.
However, these approaches have faced challenges in precisely aligning the
generated visual content with the textual concepts described in the prompts. In
this paper, we propose a two-stage coarse-to-fine semantic re-alignment method,
named RealignDiff, aimed at improving the alignment between text and images in
text-to-image diffusion models. In the coarse semantic re-alignment phase, a
novel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the
semantic discrepancy between the generated image caption and the given text
prompt. Subsequently, the fine semantic re-alignment stage employs a local
dense caption generation module and a re-weighting attention modulation module
to refine the previously generated images from a local semantic view.
Experimental results on the MS-COCO benchmark demonstrate that the proposed
two-stage coarse-to-fine semantic re-alignment method outperforms other
baseline re-alignment techniques by a substantial margin in both visual quality
and semantic similarity with the input prompt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1&quot;&gt;Guian Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zutao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jianhua Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guansong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1&quot;&gt;Shengcai Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00349">
<title>CALICO: Self-Supervised Camera-LiDAR Contrastive Pre-training for BEV Perception. (arXiv:2306.00349v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00349</link>
<description rdf:parseType="Literal">&lt;p&gt;Perception is crucial in the realm of autonomous driving systems, where
bird&apos;s eye view (BEV)-based architectures have recently reached
state-of-the-art performance. The desirability of self-supervised
representation learning stems from the expensive and laborious process of
annotating 2D and 3D data. Although previous research has investigated
pretraining methods for both LiDAR and camera-based 3D object detection, a
unified pretraining framework for multimodal BEV perception is missing. In this
study, we introduce CALICO, a novel framework that applies contrastive
objectives to both LiDAR and camera backbones. Specifically, CALICO
incorporates two stages: point-region contrast (PRC) and region-aware
distillation (RAD). PRC better balances the region- and scene-level
representation learning on the LiDAR modality and offers significant
performance improvement compared to existing methods. RAD effectively achieves
contrastive distillation on our self-trained teacher model. CALICO&apos;s efficacy
is substantiated by extensive evaluations on 3D object detection and BEV map
segmentation tasks, where it delivers significant performance improvements.
Notably, CALICO outperforms the baseline method by 10.5% and 8.6% on NDS and
mAP. Moreover, CALICO boosts the robustness of multimodal 3D object detection
against adversarial attacks and corruption. Additionally, our framework can be
tailored to different backbones and heads, positioning it as a promising
approach for multimodal BEV perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiachen Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Haizhong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingzhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_A/0/1/0/all/0/1&quot;&gt;Atul Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Z. Morley Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05526">
<title>Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment. (arXiv:2306.05526v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05526</link>
<description rdf:parseType="Literal">&lt;p&gt;The egocentric and exocentric viewpoints of a human activity look
dramatically different, yet invariant representations to link them are
essential for many potential applications in robotics and augmented reality.
Prior work is limited to learning view-invariant features from paired
synchronized viewpoints. We relax that strong data assumption and propose to
learn fine-grained action features that are invariant to the viewpoints by
aligning egocentric and exocentric videos in time, even when not captured
simultaneously or in the same environment. To this end, we propose AE2, a
self-supervised embedding approach with two key designs: (1) an object-centric
encoder that explicitly focuses on regions corresponding to hands and active
objects; and (2) a contrastive-based alignment objective that leverages
temporally reversed frames as negative samples. For evaluation, we establish a
benchmark for fine-grained video understanding in the ego-exo context,
comprising four datasets -- including an ego tennis forehand dataset we
collected, along with dense per-frame labels we annotated for each dataset. On
the four datasets, our AE2 method strongly outperforms prior work in a variety
of fine-grained downstream tasks, both in regular and cross-view settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zihui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03017">
<title>RealLiFe: Real-Time Light Field Reconstruction via Hierarchical Sparse Gradient Descent. (arXiv:2307.03017v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03017</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of Extended Reality (XR) technology, there is a growing need
for real-time light field generation from sparse view inputs. Existing methods
can be classified into offline techniques, which can generate high-quality
novel views but at the cost of long inference/training time, and online
methods, which either lack generalizability or produce unsatisfactory results.
However, we have observed that the intrinsic sparse manifold of Multi-plane
Images (MPI) enables a significant acceleration of light field generation while
maintaining rendering quality. Based on this insight, we introduce EffLiFe, a
novel light field optimization method, which leverages the proposed
Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light
fields from sparse view images in real time. Technically, the coarse MPI of a
scene is first generated using a 3D CNN, and it is further sparsely optimized
by focusing only on important MPI gradients in a few iterations. Nevertheless,
relying solely on optimization can lead to artifacts at occlusion boundaries.
Therefore, we propose an occlusion-aware iterative refinement module that
removes visual artifacts in occluded regions by iteratively filtering the
input. Extensive experiments demonstrate that our method achieves comparable
visual quality while being 100x faster on average than state-of-the-art offline
methods and delivering better performance (about 2 dB higher in PSNR) compared
to other online approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yijie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianpeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Lu Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07187">
<title>Erasing, Transforming, and Noising Defense Network for Occluded Person Re-Identification. (arXiv:2307.07187v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07187</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion perturbation presents a significant challenge in person
re-identification (re-ID), and existing methods that rely on external visual
cues require additional computational resources and only consider the issue of
missing information caused by occlusion. In this paper, we propose a simple yet
effective framework, termed Erasing, Transforming, and Noising Defense Network
(ETNDNet), which treats occlusion as a noise disturbance and solves occluded
person re-ID from the perspective of adversarial defense. In the proposed
ETNDNet, we introduce three strategies: Firstly, we randomly erase the feature
map to create an adversarial representation with incomplete information,
enabling adversarial learning of identity loss to protect the re-ID system from
the disturbance of missing information. Secondly, we introduce random
transformations to simulate the position misalignment caused by occlusion,
training the extractor and classifier adversarially to learn robust
representations immune to misaligned information. Thirdly, we perturb the
feature map with random values to address noisy information introduced by
obstacles and non-target pedestrians, and employ adversarial gaming in the
re-ID system to enhance its resistance to occlusion noise. Without bells and
whistles, ETNDNet has three key highlights: (i) it does not require any
external modules with parameters, (ii) it effectively handles various issues
caused by occlusion from obstacles and non-target pedestrians, and (iii) it
designs the first GAN-based adversarial defense paradigm for occluded person
re-ID. Extensive experiments on five public datasets fully demonstrate the
effectiveness, superiority, and practicality of the proposed ETNDNet. The code
will be released at \url{https://github.com/nengdong96/ETNDNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_N/0/1/0/all/0/1&quot;&gt;Neng Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liyan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuanglin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jinhui Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10829">
<title>Exact Diffusion Inversion via Bi-directional Integration Approximation. (arXiv:2307.10829v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.10829</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, various methods have been proposed to address the inconsistency
issue of DDIM inversion to enable image editing, such as EDICT [36] and
Null-text inversion [22]. However, the above methods introduce considerable
computational overhead. In this paper, we propose a new technique, named
\emph{bi-directional integration approximation} (BDIA), to perform exact
diffusion inversion with neglible computational overhead. Suppose we would like
to estimate the next diffusion state $\boldsymbol{z}_{i-1}$ at timestep $t_i$
with the historical information $(i,\boldsymbol{z}_i)$ and
$(i+1,\boldsymbol{z}_{i+1})$. We first obtain the estimated Gaussian noise
$\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i)$, and then apply the DDIM
update procedure twice for approximating the ODE integration over the next
time-slot $[t_i, t_{i-1}]$ in the forward manner and the previous time-slot
$[t_i, t_{t+1}]$ in the backward manner. The DDIM step for the previous
time-slot is used to refine the integration approximation made earlier when
computing $\boldsymbol{z}_i$. A nice property of BDIA-DDIM is that the update
expression for $\boldsymbol{z}_{i-1}$ is a linear combination of
$(\boldsymbol{z}_{i+1}, \boldsymbol{z}_i,
\hat{\boldsymbol{\epsilon}}(\boldsymbol{z}_i,i))$. This allows for exact
backward computation of $\boldsymbol{z}_{i+1}$ given $(\boldsymbol{z}_i,
\boldsymbol{z}_{i-1})$, thus leading to exact diffusion inversion. It is
demonstrated with experiments that (round-trip) BDIA-DDIM is particularly
effective for image editing. Our experiments further show that BDIA-DDIM
produces markedly better image sampling qualities than DDIM for text-to-image
generation.
&lt;/p&gt;
&lt;p&gt;BDIA can also be applied to improve the performance of other ODE solvers in
addition to DDIM. In our work, it is found that applying BDIA to the EDM
sampling procedure produces consistently better performance over four
pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_J/0/1/0/all/0/1&quot;&gt;J. P. Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleijn_W/0/1/0/all/0/1&quot;&gt;W. Bastiaan Kleijn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11845">
<title>Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11845</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional banks face increasing competition from FinTechs in the rapidly
evolving financial ecosystem. Raising operational efficiency is vital to
address this challenge. Our study aims to improve the efficiency of
document-intensive business processes in banking. To that end, we first review
the landscape of business documents in the retail segment. Banking documents
often contain text, layout, and visuals, suggesting that document analytics and
process automation require more than plain natural language processing (NLP).
To verify this and assess the incremental value of visual cues when processing
business documents, we compare a recently proposed multimodal model called
LayoutXLM to powerful text classifiers (e.g., BERT) and large language models
(e.g., GPT) in a case study related to processing company register extracts.
The results confirm that incorporating layout information in a model
substantially increases its performance. Interestingly, we also observed that
more than 75% of the best model performance (in terms of the F1 score) can be
achieved with as little as 30% of the training data. This shows that the demand
for data labeled data to set up a multi-modal model can be moderate, which
simplifies real-world applications of multimodal document analytics. Our study
also sheds light on more specific practices in the scope of calibrating a
multimodal banking document classifier, including the need for fine-tuning. In
sum, the paper contributes original empirical evidence on the effectiveness and
efficiency of multi-model models for document processing in the banking
business and offers practical guidance on how to unlock this potential in
day-to-day operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerling_C/0/1/0/all/0/1&quot;&gt;Christopher Gerling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lessmann_S/0/1/0/all/0/1&quot;&gt;Stefan Lessmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09084">
<title>MovePose: A High-performance Human Pose Estimation Algorithm on Mobile and Edge Devices. (arXiv:2308.09084v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09084</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MovePose, an optimized lightweight convolutional neural network
designed specifically for real-time body pose estimation on CPU-based mobile
devices. The current solutions do not provide satisfactory accuracy and speed
for human posture estimation, and MovePose addresses this gap. It aims to
maintain real-time performance while improving the accuracy of human posture
estimation for mobile devices. The network produces 17 keypoints for each
individual at a rate exceeding 11 frames per second, making it suitable for
real-time applications such as fitness tracking, sign language interpretation,
and advanced mobile human posture estimation. Our MovePose algorithm has
attained an Mean Average Precision (mAP) score of 67.7 on the COCO
\cite{cocodata} validation dataset. The MovePose algorithm displayed efficiency
with a performance of 69+ frames per second (fps) when run on an Intel
i9-10920x CPU. Additionally, it showcased an increased performance of 452+ fps
on an NVIDIA RTX3090 GPU. On an Android phone equipped with a Snapdragon 8 + 4G
processor, the fps reached above 11. To enhance accuracy, we incorporated three
techniques: deconvolution, large kernel convolution, and coordinate
classification methods. Compared to basic upsampling, deconvolution is
trainable, improves model capacity, and enhances the receptive field. Large
kernel convolution strengthens these properties at a decreased computational
cost. In summary, MovePose provides high accuracy and real-time performance,
marking it a potential tool for a variety of applications, including those
focused on mobile-side human posture estimation. The code and models for this
algorithm will be made publicly accessible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Dongyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyue Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhirui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_W/0/1/0/all/0/1&quot;&gt;Wangpeng An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yanhong Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12532">
<title>FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12532</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gihun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1&quot;&gt;Minchan Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangmook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jaehoon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Se-Young Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14969">
<title>Uncovering the Hidden Cost of Model Compression. (arXiv:2308.14969v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14969</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of resource-intensive foundation models, efficient adaptation in
downstream tasks has become paramount. Visual Prompting (VP), inspired by
prompting in Large Language Models (LLMs), has emerged as a key transfer
learning method in computer vision. Aligned with the growing significance of
efficiency, research in model compression has become pivotal to alleviate the
computational burden in both training and deploying over-parameterized neural
networks. A key goal in model compression is the development of sparse models
capable of matching or surpassing the performance of their over-parameterized,
dense counterparts. While prior research has explored the impact of model
sparsity on transfer learning, its effects on visual prompting-based transfer
remain unclear. This study addresses this gap, revealing that model sparsity
adversely affects the performance of visual prompting-based transfer,
particularly in low-data-volume scenarios. Furthermore, our findings highlight
the negative influence of sparsity on the calibration of downstream
visual-prompted models. This empirical exploration calls for a nuanced
understanding beyond accuracy in sparse settings, opening avenues for further
research in Visual Prompting for sparse models. Code and logs can be accessed
at https://github.com/landskape-ai/Reprogram_LT .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_D/0/1/0/all/0/1&quot;&gt;Diganta Misra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Agam Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Runwal_B/0/1/0/all/0/1&quot;&gt;Bharat Runwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04422">
<title>Video Task Decathlon: Unifying Image and Video Tasks in Autonomous Driving. (arXiv:2309.04422v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04422</link>
<description rdf:parseType="Literal">&lt;p&gt;Performing multiple heterogeneous visual tasks in dynamic scenes is a
hallmark of human perception capability. Despite remarkable progress in image
and video recognition via representation learning, current research still
focuses on designing specialized networks for singular, homogeneous, or simple
combination of tasks. We instead explore the construction of a unified model
for major image and video recognition tasks in autonomous driving with diverse
input and output structures. To enable such an investigation, we design a new
challenge, Video Task Decathlon (VTD), which includes ten representative image
and video tasks spanning classification, segmentation, localization, and
association of objects and pixels. On VTD, we develop our unified network,
VTDNet, that uses a single structure and a single set of weights for all ten
tasks. VTDNet groups similar tasks and employs task interaction stages to
exchange information within and between task groups. Given the impracticality
of labeling all tasks on all frames, and the performance degradation associated
with joint training of many tasks, we design a Curriculum training,
Pseudo-labeling, and Fine-tuning (CPF) scheme to successfully train VTDNet on
all tasks and mitigate performance loss. Armed with CPF, VTDNet significantly
outperforms its single-task counterparts on most tasks with only 20% overall
computations. VTD is a promising new direction for exploring the unification of
perception tasks in autonomous driving.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Thomas E. Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09777">
<title>DriveDreamer: Towards Real-world-driven World Models for Autonomous Driving. (arXiv:2309.09777v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09777</link>
<description rdf:parseType="Literal">&lt;p&gt;World models, especially in autonomous driving, are trending and drawing
extensive attention due to their capacity for comprehending driving
environments. The established world model holds immense potential for the
generation of high-quality driving videos, and driving policies for safe
maneuvering. However, a critical limitation in relevant research lies in its
predominant focus on gaming environments or simulated settings, thereby lacking
the representation of real-world driving scenarios. Therefore, we introduce
DriveDreamer, a pioneering world model entirely derived from real-world driving
scenarios. Regarding that modeling the world in intricate driving scenes
entails an overwhelming search space, we propose harnessing the powerful
diffusion model to construct a comprehensive representation of the complex
environment. Furthermore, we introduce a two-stage training pipeline. In the
initial phase, DriveDreamer acquires a deep understanding of structured traffic
constraints, while the subsequent stage equips it with the ability to
anticipate future states. The proposed DriveDreamer is the first world model
established from real-world driving scenarios. We instantiate DriveDreamer on
the challenging nuScenes benchmark, and extensive experiments verify that
DriveDreamer empowers precise, controllable video generation that faithfully
captures the structural constraints of real-world traffic scenarios.
Additionally, DriveDreamer enables the generation of realistic and reasonable
driving policies, opening avenues for interaction and practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_G/0/1/0/all/0/1&quot;&gt;Guan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiagang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12303">
<title>PanoVOS: Bridging Non-panoramic and Panoramic Views with Transformer for Video Segmentation. (arXiv:2309.12303v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12303</link>
<description rdf:parseType="Literal">&lt;p&gt;Panoramic videos contain richer spatial information and have attracted
tremendous amounts of attention due to their exceptional experience in some
fields such as autonomous driving and virtual reality. However, existing
datasets for video segmentation only focus on conventional planar images. To
address the challenge, in this paper, we present a panoramic video dataset,
PanoVOS. The dataset provides 150 videos with high video resolutions and
diverse motions. To quantify the domain gap between 2D planar videos and
panoramic videos, we evaluate 15 off-the-shelf video object segmentation (VOS)
models on PanoVOS. Through error analysis, we found that all of them fail to
tackle pixel-level content discontinues of panoramic videos. Thus, we present a
Panoramic Space Consistency Transformer (PSCFormer), which can effectively
utilize the semantic boundary information of the previous frame for pixel-level
matching with the current frame. Extensive experiments demonstrate that
compared with the previous SOTA models, our PSCFormer network exhibits a great
advantage in terms of segmentation results under the panoramic setting. Our
dataset poses new challenges in panoramic VOS and we hope that our PanoVOS can
advance the development of panoramic segmentation/tracking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shilin Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaohao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Renrui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;Lingyi Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14809">
<title>ENIGMA-51: Towards a Fine-Grained Understanding of Human-Object Interactions in Industrial Scenarios. (arXiv:2309.14809v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14809</link>
<description rdf:parseType="Literal">&lt;p&gt;ENIGMA-51 is a new egocentric dataset acquired in an industrial scenario by
19 subjects who followed instructions to complete the repair of electrical
boards using industrial tools (e.g., electric screwdriver) and equipments
(e.g., oscilloscope). The 51 egocentric video sequences are densely annotated
with a rich set of labels that enable the systematic study of human behavior in
the industrial domain. We provide benchmarks on four tasks related to human
behavior: 1) untrimmed temporal detection of human-object interactions, 2)
egocentric human-object interaction detection, 3) short-term object interaction
anticipation and 4) natural language understanding of intents and entities.
Baseline results show that the ENIGMA-51 dataset poses a challenging benchmark
to study human behavior in industrial scenarios. We publicly release the
dataset at https://iplab.dmi.unict.it/ENIGMA-51.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragusa_F/0/1/0/all/0/1&quot;&gt;Francesco Ragusa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonardi_R/0/1/0/all/0/1&quot;&gt;Rosario Leonardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzamuto_M/0/1/0/all/0/1&quot;&gt;Michele Mazzamuto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonanno_C/0/1/0/all/0/1&quot;&gt;Claudia Bonanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scavo_R/0/1/0/all/0/1&quot;&gt;Rosario Scavo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furnari_A/0/1/0/all/0/1&quot;&gt;Antonino Furnari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farinella_G/0/1/0/all/0/1&quot;&gt;Giovanni Maria Farinella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16899">
<title>On the Contractivity of Plug-and-Play Operators. (arXiv:2309.16899v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16899</link>
<description rdf:parseType="Literal">&lt;p&gt;In plug-and-play (PnP) regularization, the proximal operator in algorithms
such as ISTA and ADMM is replaced by a powerful denoiser. This formal
substitution works surprisingly well in practice. In fact, PnP has been shown
to give state-of-the-art results for various imaging applications. The
empirical success of PnP has motivated researchers to understand its
theoretical underpinnings and, in particular, its convergence. It was shown in
prior work that for kernel denoisers such as the nonlocal means, PnP-ISTA
provably converges under some strong assumptions on the forward model. The
present work is motivated by the following questions: Can we relax the
assumptions on the forward model? Can the convergence analysis be extended to
PnP-ADMM? Can we estimate the convergence rate? In this letter, we resolve
these questions using the contraction mapping theorem: (i) for symmetric
denoisers, we show that (under mild conditions) PnP-ISTA and PnP-ADMM exhibit
linear convergence; and (ii) for kernel denoisers, we show that PnP-ISTA and
PnP-ADMM converge linearly for image inpainting. We validate our theoretical
findings using reconstruction experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athalye_C/0/1/0/all/0/1&quot;&gt;Chirayu D. Athalye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhury_K/0/1/0/all/0/1&quot;&gt;Kunal N. Chaudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_B/0/1/0/all/0/1&quot;&gt;Bhartendu Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17338">
<title>Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17338</link>
<description rdf:parseType="Literal">&lt;p&gt;The inherently diverse and uncertain nature of trajectories presents a
formidable challenge in accurately modeling them. Motion prediction systems
must effectively learn spatial and temporal information from the past to
forecast the future trajectories of the agent. Many existing methods learn
temporal motion via separate components within stacked models to capture
temporal features. Furthermore, prediction methods often operate under the
assumption that observed trajectory waypoint sequences are complete,
disregarding scenarios where missing values may occur, which can influence
their performance. Moreover, these models may be biased toward particular
waypoint sequences when making predictions. We propose a novel approach called
Temporal Waypoint Dropping (TWD) that explicitly incorporates temporal
dependencies during the training of a trajectory prediction model. By
stochastically dropping waypoints from past observed trajectories, the model is
forced to learn the underlying temporal representation from the remaining
waypoints, resulting in an improved model. Incorporating stochastic temporal
waypoint dropping into the model learning process significantly enhances its
performance in scenarios with missing values. Experimental results demonstrate
our approach&apos;s substantial improvement in trajectory prediction capabilities.
Our approach can complement existing trajectory prediction methods to improve
their prediction accuracy. We evaluate our proposed approach on three datasets:
NBA Sports VU, ETH-UCY, and TrajNet++.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chib_P/0/1/0/all/0/1&quot;&gt;Pranav Singh Chib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pravendra Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17389">
<title>Prompt-based test-time real image dehazing: a novel pipeline. (arXiv:2309.17389v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17389</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods attempt to improve models&apos; generalization ability on
real-world hazy images by exploring well-designed training schemes (e.g.,
CycleGAN, prior loss). However, most of them need very complicated training
procedures to achieve satisfactory results. In this work, we present a totally
novel testing pipeline called Prompt-based Test-Time Dehazing (PTTD) to help
generate visually pleasing results of real-captured hazy images during the
inference phase. We experimentally find that given a dehazing model trained on
synthetic data, by fine-tuning the statistics (i.e., mean and standard
deviation) of encoding features, PTTD is able to narrow the domain gap,
boosting the performance of real image dehazing. Accordingly, we first apply a
prompt generation module (PGM) to generate a visual prompt, which is the source
of appropriate statistical perturbations for mean and standard deviation. And
then, we employ the feature adaptation module (FAM) into the existing dehazing
models for adjusting the original statistics with the guidance of the generated
prompt. Note that, PTTD is model-agnostic and can be equipped with various
state-of-the-art dehazing models trained on synthetic hazy-clean pairs.
Extensive experimental results demonstrate that our PTTD is flexible meanwhile
achieves superior performance against state-of-the-art dehazing methods in
real-world scenarios. The source code of our PTTD will be made available at
https://github.com/cecret3350/PTTD-Dehazing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zewei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Ziqian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xuecheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhe-Ming Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01825">
<title>Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced
significant growth and have been extensively employed to adapt large vision and
language models to various domains, enabling satisfactory model performance
with minimal computational needs. Despite these advances, more research has yet
to delve into potential PEFT applications in real-life scenarios, particularly
in the critical domains of remote sensing and crop monitoring. The diversity of
climates across different regions and the need for comprehensive large-scale
datasets have posed significant obstacles to accurately identify crop types
across varying geographic locations and changing growing seasons. This study
seeks to bridge this gap by comprehensively exploring the feasibility of
cross-area and cross-year out-of-distribution generalization using the
State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to
explore PEFT approaches for crop monitoring. Specifically, we focus on adapting
the SOTA TSViT model to address winter wheat field segmentation, a critical
task for crop monitoring and food security. This adaptation process involves
integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and
prompt tuning. Using PEFT techniques, we achieved notable results comparable to
those achieved using full fine-tuning methods while training only a mere 0.7%
parameters of the whole TSViT architecture. The in-house labeled data-set,
referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated
polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over
five consecutive years. Using Sentinel-2 images, our model achieved a 84%
F1-score. We intend to publicly release the Lebanese winter wheat data set,
code repository, and model weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahweh_M/0/1/0/all/0/1&quot;&gt;Mohamad Hasan Zahweh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1&quot;&gt;Hasan Nasrallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1&quot;&gt;Mustafa Shukor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1&quot;&gt;Ghaleb Faour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1&quot;&gt;Ali J. Ghandour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01828">
<title>Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01828</link>
<description rdf:parseType="Literal">&lt;p&gt;eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shreim_H/0/1/0/all/0/1&quot;&gt;Hossein Shreim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gizzini_A/0/1/0/all/0/1&quot;&gt;Abdul Karim Gizzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1&quot;&gt;Ali J. Ghandour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01852">
<title>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01852</link>
<description rdf:parseType="Literal">&lt;p&gt;The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N&amp;gt;=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 5.8%
R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot
video-text retrieval task. Beyond this, our LanguageBind has greatly improved
in the zero-shot video, audio, depth, and infrared understanding tasks. For
instance, LanguageBind surpassing InterVideo by 1.9% on MSR-VTT, 8.8% on MSVD,
6.3% on DiDeMo, and 4.4% on ActivityNet. On the LLVIP and NYU-D datasets,
LanguageBind outperforms ImageBind with 23.8% and 11.1% top-1 accuracy. Code
address: https://github.com/PKU-YuanGroup/LanguageBind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaxi Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;HongFa Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yatian Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junwu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wancai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03335">
<title>Continual Test-time Domain Adaptation via Dynamic Sample Selection. (arXiv:2310.03335v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03335</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of Continual Test-time Domain Adaptation (CTDA) is to gradually
adapt a pre-trained model to a sequence of target domains without accessing the
source data. This paper proposes a Dynamic Sample Selection (DSS) method for
CTDA. DSS consists of dynamic thresholding, positive learning, and negative
learning processes. Traditionally, models learn from unlabeled unknown
environment data and equally rely on all samples&apos; pseudo-labels to update their
parameters through self-training. However, noisy predictions exist in these
pseudo-labels, so all samples are not equally trustworthy. Therefore, in our
method, a dynamic thresholding module is first designed to select suspected
low-quality from high-quality samples. The selected low-quality samples are
more likely to be wrongly predicted. Therefore, we apply joint positive and
negative learning on both high- and low-quality samples to reduce the risk of
using wrong information. We conduct extensive experiments that demonstrate the
effectiveness of our proposed method for CTDA in the image domain,
outperforming the state-of-the-art results. Furthermore, our approach is also
evaluated in the 3D point cloud domain, showcasing its versatility and
potential for broader applicability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanshuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Jie Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheraghian_A/0/1/0/all/0/1&quot;&gt;Ali Cheraghian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_S/0/1/0/all/0/1&quot;&gt;Shafin Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmedt_Aristizabal_D/0/1/0/all/0/1&quot;&gt;David Ahmedt-Aristizabal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersson_L/0/1/0/all/0/1&quot;&gt;Lars Petersson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harandi_M/0/1/0/all/0/1&quot;&gt;Mehrtash Harandi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04914">
<title>Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks. (arXiv:2310.04914v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04914</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundational multimodal models pre-trained on large scale image-text pairs or
video-text pairs or both have shown strong generalization abilities on
downstream tasks. However unlike image-text models, pretraining video-text
models is always not feasible due to the difficulty in collecting large-scale
clean and aligned data, and exponential computational costs involved in the
pretraining phase. Therefore, the pertinent question to ask is: Can image-text
models be adapted to video tasks and is there any benefit to using these models
over pretraining directly on videos? In this work, we focus on this question by
proposing a detailed study on the generalization abilities of image-text models
when evaluated on video understanding tasks in a zero-shot setting. We
investigate 9 foundational image-text models on a diverse set of video tasks
that include video action recognition (video AR), video retrieval (video RT),
video question answering (video QA), video multiple choice (video MC) and video
captioning (video CP). Our experiments show that image-text models exhibit
impressive performance on video AR, video RT and video MC. Furthermore, they
perform moderately on video captioning and poorly on video QA. These findings
shed a light on the benefits of adapting foundational image-text models to an
array of video tasks while avoiding the costly pretraining step.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1&quot;&gt;Avinash Madasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhiwandiwalla_A/0/1/0/all/0/1&quot;&gt;Anahita Bhiwandiwalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1&quot;&gt;Vasudev Lal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06627">
<title>What If the TV Was Off? Examining Counterfactual Reasoning Abilities of Multi-modal Language Models. (arXiv:2310.06627v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06627</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual reasoning, a fundamental aspect of human cognition, involves
contemplating alternatives to established facts or past events, significantly
enhancing our abilities in planning and decision-making. In light of the
advancements in current multi-modal large language models, we explore their
effectiveness in counterfactual reasoning. To facilitate this investigation, we
introduce a novel dataset, C-VQA, specifically designed to test the
counterfactual reasoning capabilities of modern multi-modal large language
models. This dataset is constructed by infusing original questions with
counterfactual presuppositions, spanning various types such as numerical and
boolean queries. It encompasses a mix of real and synthetic data, representing
a wide range of difficulty levels. Our thorough evaluations of contemporary
vision-language models using this dataset have revealed substantial performance
drops, with some models showing up to a 40\% decrease, highlighting a
significant gap between current models and human-like vision reasoning
capabilities. We hope our dataset will serve as a vital benchmark for
evaluating the counterfactual reasoning capabilities of models. Code and
dataset are publicly available at https://bzhao.me/C-VQA/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Letian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaotong Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongkai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Y/0/1/0/all/0/1&quot;&gt;Yongshuo Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bingchen Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07206">
<title>DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation. (arXiv:2310.07206v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07206</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the task of 3D pose estimation for a hand interacting
with an object from a single image observation. When modeling hand-object
interaction, previous works mainly exploit proximity cues, while overlooking
the dynamical nature that the hand must stably grasp the object to counteract
gravity and thus preventing the object from slipping or falling. These works
fail to leverage dynamical constraints in the estimation and consequently often
produce unstable results. Meanwhile, refining unstable configurations with
physics-based reasoning remains challenging, both by the complexity of contact
dynamics and by the lack of effective and efficient physics inference in the
data-driven learning framework. To address both issues, we present DeepSimHO: a
novel deep-learning pipeline that combines forward physics simulation and
backward gradient approximation with a neural network. Specifically, for an
initial hand-object pose estimated by a base network, we forward it to a
physics simulator to evaluate its stability. However, due to non-smooth contact
geometry and penetration, existing differentiable simulators can not provide
reliable state gradient. To remedy this, we further introduce a deep network to
learn the stability evaluation process from the simulator, while smoothly
approximating its gradient and thus enabling effective back-propagation.
Extensive experiments show that our method noticeably improves the stability of
the estimation and achieves superior efficiency over test-time optimization.
The code is available at https://github.com/rongakowang/DeepSimHO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_W/0/1/0/all/0/1&quot;&gt;Wei Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07997">
<title>PG-NeuS: Robust and Efficient Point Guidance for Multi-View Neural Surface Reconstruction. (arXiv:2310.07997v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07997</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learning multi-view neural surface reconstruction with the
supervision of point clouds or depth maps has been a promising way. However,
due to the underutilization of prior information, current methods still
struggle with the challenges of limited accuracy and excessive time complexity.
In addition, prior data perturbation is also an important but rarely considered
issue. To address these challenges, we propose a novel point-guided method
named PG-NeuS, which achieves accurate and efficient reconstruction while
robustly coping with point noise. Specifically, aleatoric uncertainty of the
point cloud is modeled to capture the distribution of noise, leading to noise
robustness. Furthermore, a Neural Projection module connecting points and
images is proposed to add geometric constraints to implicit surface, achieving
precise point guidance. To better compensate for geometric bias between volume
rendering and point modeling, high-fidelity points are filtered into a Bias
Network to further improve details representation. Benefiting from the
effective point guidance, even with a lightweight network, the proposed PG-NeuS
achieves fast convergence with an impressive 11x speedup compared to NeuS.
Extensive experiments show that our method yields high-quality surfaces with
high efficiency, especially for fine-grained details and smooth regions,
outperforming the state-of-the-art methods. Moreover, it exhibits strong
robustness to noisy data and sparse data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Wanjuan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingshan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wenbing Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08872">
<title>R&amp;B: Region and Boundary Aware Zero-shot Grounded Text-to-image Generation. (arXiv:2310.08872v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08872</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image (T2I) diffusion models have achieved remarkable progress
in generating high-quality images given text-prompts as input. However, these
models fail to convey appropriate spatial composition specified by a layout
instruction. In this work, we probe into zero-shot grounded T2I generation with
diffusion models, that is, generating images corresponding to the input layout
information without training auxiliary modules or finetuning diffusion models.
We propose a Region and Boundary (R&amp;amp;B) aware cross-attention guidance approach
that gradually modulates the attention maps of diffusion model during
generative process, and assists the model to synthesize images (1) with high
fidelity, (2) highly compatible with textual input, and (3) interpreting layout
instructions accurately. Specifically, we leverage the discrete sampling to
bridge the gap between consecutive attention maps and discrete layout
constraints, and design a region-aware loss to refine the generative layout
during diffusion process. We further propose a boundary-aware loss to
strengthen object discriminability within the corresponding regions.
Experimental results show that our method outperforms existing state-of-the-art
zero-shot grounded T2I generation methods by a large margin both qualitatively
and quantitatively on several benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jiayu Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_H/0/1/0/all/0/1&quot;&gt;Henglei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Liang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09912">
<title>Unsupervised Discovery of Interpretable Directions in h-space of Pre-trained Diffusion Models. (arXiv:2310.09912v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09912</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose the first unsupervised and learning-based method to identify
interpretable directions in h-space of pre-trained diffusion models. Our method
is derived from an existing technique that operates on the GAN latent space.
Specifically, we employ a shift control module that works on h-space of
pre-trained diffusion models to manipulate a sample into a shifted version of
itself, followed by a reconstructor to reproduce both the type and the strength
of the manipulation. By jointly optimizing them, the model will spontaneously
discover disentangled and interpretable directions. To prevent the discovery of
meaningless and destructive directions, we employ a discriminator to maintain
the fidelity of shifted sample. Due to the iterative generative process of
diffusion models, our training requires a substantial amount of GPU VRAM to
store numerous intermediate tensors for back-propagating gradient. To address
this issue, we propose a general VRAM-efficient training algorithm based on
gradient checkpointing technique to back-propagate any gradient through the
whole generative process, with acceptable occupancy of VRAM and sacrifice of
training efficiency. Compared with existing related works on diffusion models,
our method inherently identifies global and scalable directions, without
necessitating any other complicated procedures. Extensive experiments on
various datasets demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zijian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Luping Liu. Zhijie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10541">
<title>AST: Effective Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10541</link>
<description rdf:parseType="Literal">&lt;p&gt;Training large AI models typically requires large-scale datasets in the
machine learning process, making training and parameter-tuning process both
time-consuming and costly. Some researchers address this problem by carefully
synthesizing a very small number of highly representative and informative
samples from real-world datasets. This approach, known as Dataset Distillation
(DD), proposes a perspective for data-efficient learning. Despite recent
progress in this field, the performance of existing methods still cannot meet
expectations, and distilled datasets cannot effectively replace original
datasets. In this paper, unlike previous methods that focus solely on improving
the effectiveness of student distillation, we recognize and leverage the
important mutual influence between expert and student models. We observed that
the smoothness of expert trajectories has a significant impact on subsequent
student parameter alignment. Based on this, we propose an effective DD
framework named AST, standing for Alignment with Smooth and high-quality expert
Trajectories. We devise the integration of clipping loss and gradient penalty
to regulate the rate of parameter changes in expert trajectory generation. To
further refine the student parameter alignment with expert trajectory, we put
forward representative initialization for the synthetic dataset and balanced
inner-loop loss in response to the sensitivity exhibited towards randomly
initialized variables during distillation. We also propose two enhancement
strategies, namely intermediate matching loss and weight perturbation, to
mitigate the potential occurrence of cumulative errors. We conduct extensive
experiments on datasets of different scales, sizes, and resolutions. The
results demonstrate that the proposed method significantly outperforms prior
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiyuan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenzhuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kwok-Yan Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12190">
<title>DynamiCrafter: Animating Open-domain Images with Video Diffusion Priors. (arXiv:2310.12190v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12190</link>
<description rdf:parseType="Literal">&lt;p&gt;Animating a still image offers an engaging visual experience. Traditional
image animation techniques mainly focus on animating natural scenes with
stochastic dynamics (e.g. clouds and fluid) or domain-specific motions (e.g.
human hair or body motions), and thus limits their applicability to more
general visual content. To overcome this limitation, we explore the synthesis
of dynamic content for open-domain images, converting them into animated
videos. The key idea is to utilize the motion prior of text-to-video diffusion
models by incorporating the image into the generative process as guidance.
Given an image, we first project it into a text-aligned rich context
representation space using a query transformer, which facilitates the video
model to digest the image content in a compatible fashion. However, some visual
details still struggle to be preserved in the resultant videos. To supplement
with more precise image information, we further feed the full image to the
diffusion model by concatenating it with the initial noises. Experimental
results show that our proposed method can produce visually convincing and more
logical &amp;amp; natural motions, as well as higher conformity to the input image.
Comparative evaluation demonstrates the notable superiority of our approach
over existing competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Jinbo Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Menghan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wangbo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_T/0/1/0/all/0/1&quot;&gt;Tien-Tsin Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12877">
<title>Perceptual Assessment and Optimization of High Dynamic Range Image Rendering. (arXiv:2310.12877v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12877</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing popularity of high dynamic range (HDR) imaging stems from its
ability to faithfully capture luminance levels in natural scenes. However, HDR
image quality assessment has been insufficiently addressed. Existing models are
mostly designed for low dynamic range (LDR) images, which exhibit poorly
correlated with human perception of HDR image quality. To fill this gap, we
propose a family of HDR quality metrics by transferring the recent advancements
in LDR domain. The key step in our approach is to employ a simple inverse
display model to decompose an HDR image into a stack of LDR images with varying
exposures. Subsequently, these LDR images are evaluated using state-of-the-art
LDR quality metrics. Our family of HDR quality models offer three notable
advantages. First, specific exposures (i.e., luminance ranges) can be weighted
to emphasize their assessment when calculating the overall quality score.
Second, our HDR quality metrics directly inherit the capabilities of their base
LDR quality models in assessing LDR images. Third, our metrics do not rely on
human perceptual data of HDR image quality for re-calibration. Experiments
conducted on four human-rated HDR image quality datasets indicate that our HDR
quality metrics consistently outperform existing methods, including the HDR-VDP
family. Furthermore, we demonstrate the promise of our models in the perceptual
optimization of HDR novel view synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_P/0/1/0/all/0/1&quot;&gt;Peibei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mantiuk_R/0/1/0/all/0/1&quot;&gt;Rafal K. Mantiuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kede Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15848">
<title>On Responsible Machine Learning Datasets with Fairness, Privacy, and Regulatory Norms. (arXiv:2310.15848v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15848</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) has made its way into various scientific fields,
providing astonishing improvements over existing algorithms for a wide variety
of tasks. In recent years, there have been severe concerns over the
trustworthiness of AI technologies. The scientific community has focused on the
development of trustworthy AI algorithms. However, machine and deep learning
algorithms, popular in the AI community today, depend heavily on the data used
during their development. These learning algorithms identify patterns in the
data, learning the behavioral objective. Any flaws in the data have the
potential to translate directly into algorithms. In this study, we discuss the
importance of Responsible Machine Learning Datasets and propose a framework to
evaluate the datasets through a responsible rubric. While existing work focuses
on the post-hoc evaluation of algorithms for their trustworthiness, we provide
a framework that considers the data component separately to understand its role
in the algorithm. We discuss responsible datasets through the lens of fairness,
privacy, and regulatory compliance and provide recommendations for constructing
future datasets. After surveying over 100 datasets, we use 60 datasets for
analysis and demonstrate that none of these datasets is immune to issues of
fairness, privacy preservation, and regulatory compliance. We provide
modifications to the ``datasheets for datasets&quot; with important additions for
improved dataset documentation. With governments around the world regularizing
data protection laws, the method for the creation of datasets in the scientific
community requires revision. We believe this study is timely and relevant in
today&apos;s era of AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Surbhi Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thakral_K/0/1/0/all/0/1&quot;&gt;Kartik Thakral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Richa Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vatsa_M/0/1/0/all/0/1&quot;&gt;Mayank Vatsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glaser_T/0/1/0/all/0/1&quot;&gt;Tamar Glaser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_C/0/1/0/all/0/1&quot;&gt;Cristian Canton Ferrer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassner_T/0/1/0/all/0/1&quot;&gt;Tal Hassner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17294">
<title>Scale-Adaptive Feature Aggregation for Efficient Space-Time Video Super-Resolution. (arXiv:2310.17294v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17294</link>
<description rdf:parseType="Literal">&lt;p&gt;The Space-Time Video Super-Resolution (STVSR) task aims to enhance the visual
quality of videos, by simultaneously performing video frame interpolation (VFI)
and video super-resolution (VSR). However, facing the challenge of the
additional temporal dimension and scale inconsistency, most existing STVSR
methods are complex and inflexible in dynamically modeling different motion
amplitudes. In this work, we find that choosing an appropriate processing scale
achieves remarkable benefits in flow-based feature propagation. We propose a
novel Scale-Adaptive Feature Aggregation (SAFA) network that adaptively selects
sub-networks with different processing scales for individual samples.
Experiments on four public STVSR benchmarks demonstrate that SAFA achieves
state-of-the-art performance. Our SAFA network outperforms recent
state-of-the-art methods such as TMNet and VideoINR by an average improvement
of over 0.5dB on PSNR, while requiring less than half the number of parameters
and only 1/3 computational costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhewei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_A/0/1/0/all/0/1&quot;&gt;Ailin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xiaotao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shuchang Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18285">
<title>Unlocking the Potential of Prompt-Tuning in Bridging Generalized and Personalized Federated Learning. (arXiv:2310.18285v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18285</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViT) and Visual Prompt Tuning (VPT) achieve
state-of-the-art performance with improved efficiency in various computer
vision tasks. This suggests a promising paradigm shift of adapting pre-trained
ViT models to Federated Learning (FL) settings. However, the challenge of data
heterogeneity among FL clients presents a significant hurdle in effectively
deploying ViT models. Existing Generalized FL (GFL) and Personalized FL (PFL)
methods have limitations in balancing performance across both global and local
data distributions. In this paper, we present a novel algorithm, SGPT, that
integrates GFL and PFL approaches by employing a unique combination of both
shared and group-specific prompts. This design enables SGPT to capture both
common and group-specific features. A key feature of SGPT is its prompt
selection module, which facilitates the training of a single global model
capable of automatically adapting to diverse local client data distributions
without the need for local fine-tuning. To effectively train the prompts, we
utilize block coordinate descent (BCD), learning from common feature
information (shared prompts), and then more specialized knowledge (group
prompts) iteratively. Theoretically, we justify that learning the proposed
prompts can reduce the gap between global and local performance. Empirically,
we conduct experiments on both label and feature heterogeneity settings in
comparison with state-of-the-art baselines, along with extensive ablation
studies, to substantiate the superior performance of SGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Wenlong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18332">
<title>WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models. (arXiv:2310.18332v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18332</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces WordArt Designer, a user-driven framework for artistic
typography synthesis, relying on the Large Language Model (LLM). The system
incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo
modules. 1) The LLM Engine, empowered by the LLM (e.g., GPT-3.5), interprets
user inputs and generates actionable prompts for the other modules, thereby
transforming abstract concepts into tangible designs. 2) The SemTypo module
optimizes font designs using semantic concepts, striking a balance between
artistic transformation and readability. 3) Building on the semantic layout
provided by the SemTypo module, the StyTypo module creates smooth, refined
images. 4) The TexTypo module further enhances the design&apos;s aesthetics through
texture rendering, enabling the generation of inventive textured fonts.
Notably, WordArt Designer highlights the fusion of generative AI with artistic
typography. Experience its capabilities on ModelScope:
https://www.modelscope.cn/studios/WordArt/WordArt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingdong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wangmeng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xianhui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zengke Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yusen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00397">
<title>Towards Omni-supervised Referring Expression Segmentation. (arXiv:2311.00397v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00397</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring Expression Segmentation (RES) is an emerging task in computer
vision, which segments the target instances in images based on text
descriptions. However, its development is plagued by the expensive segmentation
labels. To address this issue, we propose a new learning task for RES called
Omni-supervised Referring Expression Segmentation (Omni-RES), which aims to
make full use of unlabeled, fully labeled and weakly labeled data, e.g.,
referring points or grounding boxes, for efficient RES training. To accomplish
this task, we also propose a novel yet strong baseline method for Omni-RES
based on the recently popular teacher-student learning, where the weak labels
are not directly transformed into supervision signals but used as a yardstick
to select and refine high-quality pseudo-masks for teacher-student learning. To
validate the proposed Omni-RES method, we apply it to a set of state-of-the-art
RES models and conduct extensive experiments on a bunch of RES datasets. The
experimental results yield the obvious merits of Omni-RES than the
fully-supervised and semi-supervised training schemes. For instance, with only
10% fully labeled data, Omni-RES can help the base model achieve 100% fully
supervised performance, and it also outperform the semi-supervised alternative
by a large margin, e.g., +14.93% on RefCOCO and +14.95% on RefCOCO+,
respectively. More importantly, Omni-RES also enable the use of large-scale
vision-langauges like Visual Genome to facilitate low-cost RES training, and
achieve new SOTA performance of RES, e.g., 80.66 on RefCOCO.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_M/0/1/0/all/0/1&quot;&gt;Minglang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yiyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Gen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_G/0/1/0/all/0/1&quot;&gt;Guannan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_W/0/1/0/all/0/1&quot;&gt;Weilin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xiaoshuai Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01066">
<title>Dynamic Multimodal Information Bottleneck for Multimodality Classification. (arXiv:2311.01066v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01066</link>
<description rdf:parseType="Literal">&lt;p&gt;Effectively leveraging multimodal data such as various images, laboratory
tests and clinical information is gaining traction in a variety of AI-based
medical diagnosis and prognosis tasks. Most existing multi-modal techniques
only focus on enhancing their performance by leveraging the differences or
shared features from various modalities and fusing feature across different
modalities. These approaches are generally not optimal for clinical settings,
which pose the additional challenges of limited training data, as well as being
rife with redundant data or noisy modality channels, leading to subpar
performance. To address this gap, we study the robustness of existing methods
to data redundancy and noise and propose a generalized dynamic multimodal
information bottleneck framework for attaining a robust fused feature
representation. Specifically, our information bottleneck module serves to
filter out the task-irrelevant information and noises in the fused feature, and
we further introduce a sufficiency loss to prevent dropping of task-relevant
information, thus explicitly preserving the sufficiency of prediction
information in the distilled feature. We validate our model on an in-house and
a public COVID19 dataset for mortality prediction as well as two public
biomedical datasets for diagnostic tasks. Extensive experiments show that our
method surpasses the state-of-the-art and is significantly more robust, being
the only method to remain performance when large-scale noisy channels exist.
Our code is publicly available at https://github.com/ayanglab/DMIB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yingying Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shuang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chaoyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tieyong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xiaodan Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Walsh_S/0/1/0/all/0/1&quot;&gt;Simon Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01703">
<title>Taking a PEEK into YOLOv5 for Satellite Component Recognition via Entropy-based Visual Explanations. (arXiv:2311.01703v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01703</link>
<description rdf:parseType="Literal">&lt;p&gt;The escalating risk of collisions and the accumulation of space debris in Low
Earth Orbit (LEO) has reached critical concern due to the ever increasing
number of spacecraft. Addressing this crisis, especially in dealing with
non-cooperative and unidentified space debris, is of paramount importance. This
paper contributes to efforts in enabling autonomous swarms of small chaser
satellites for target geometry determination and safe flight trajectory
planning for proximity operations in LEO. Our research explores on-orbit use of
the You Only Look Once v5 (YOLOv5) object detection model trained to detect
satellite components. While this model has shown promise, its inherent lack of
interpretability hinders human understanding, a critical aspect of validating
algorithms for use in safety-critical missions. To analyze the decision
processes, we introduce Probabilistic Explanations for Entropic Knowledge
extraction (PEEK), a method that utilizes information theoretic analysis of the
latent representations within the hidden layers of the model. Through both
synthetic in hardware-in-the-loop experiments, PEEK illuminates the
decision-making processes of the model, helping identify its strengths,
limitations and biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meni_M/0/1/0/all/0/1&quot;&gt;Mackenzie J. Meni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahendrakar_T/0/1/0/all/0/1&quot;&gt;Trupti Mahendrakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raney_O/0/1/0/all/0/1&quot;&gt;Olivia D. M. Raney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_R/0/1/0/all/0/1&quot;&gt;Ryan T. White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayo_M/0/1/0/all/0/1&quot;&gt;Michael L. Mayo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pilkiewicz_K/0/1/0/all/0/1&quot;&gt;Kevin Pilkiewicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01815">
<title>Estimating 3D Uncertainty Field: Quantifying Uncertainty for Neural Radiance Fields. (arXiv:2311.01815v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01815</link>
<description rdf:parseType="Literal">&lt;p&gt;Current methods based on Neural Radiance Fields (NeRF) significantly lack the
capacity to quantify uncertainty in their predictions, particularly on the
unseen space including the occluded and outside scene content. This limitation
hinders their extensive applications in robotics, where the reliability of
model predictions has to be considered for tasks such as robotic exploration
and planning in unknown environments. To address this, we propose a novel
approach to estimate a 3D Uncertainty Field based on the learned incomplete
scene geometry, which explicitly identifies these unseen regions. By
considering the accumulated transmittance along each camera ray, our
Uncertainty Field infers 2D pixel-wise uncertainty, exhibiting high values for
rays directly casting towards occluded or outside the scene content. To
quantify the uncertainty on the learned surface, we model a stochastic radiance
field. Our experiments demonstrate that our approach is the only one that can
explicitly reason about high uncertainty both on 3D unseen regions and its
involved 2D rendered pixels, compared with recent methods. Furthermore, we
illustrate that our designed uncertainty field is ideally suited for real-world
robotics tasks, such as next-best-view selection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jianxiong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_R/0/1/0/all/0/1&quot;&gt;Ruijie Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_A/0/1/0/all/0/1&quot;&gt;Adria Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01908">
<title>LLM-driven Multimodal Target Volume Contouring in Radiation Oncology. (arXiv:2311.01908v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01908</link>
<description rdf:parseType="Literal">&lt;p&gt;Target volume contouring for radiation therapy is considered significantly
more challenging than the normal organ segmentation tasks as it necessitates
the utilization of both image and text-based clinical information. Inspired by
the recent advancement of large language models (LLMs) that can facilitate the
integration of the textural information and images, here we present a novel
LLM-driven multi-modal AI that utilizes the clinical text information and is
applicable to the challenging task of target volume contouring for radiation
therapy, and validate it within the context of breast cancer radiation therapy
target volume contouring. Using external validation and data-insufficient
environments, which attributes highly conducive to real-world applications, we
demonstrate that the proposed model exhibits markedly improved performance
compared to conventional vision-only AI models, particularly exhibiting robust
generalization performance and data-efficiency. To our best knowledge, this is
the first LLM-driven multimodal AI model that integrates the clinical text
information into target volume delineation for radiation oncology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oh_Y/0/1/0/all/0/1&quot;&gt;Yujin Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sangjoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hwa Kyung Byun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin Sung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05697">
<title>3DGAUnet: 3D generative adversarial networks with a 3D U-Net based generator to achieve the accurate and effective synthesis of clinical tumor image data for pancreatic cancer. (arXiv:2311.05697v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05697</link>
<description rdf:parseType="Literal">&lt;p&gt;Pancreatic ductal adenocarcinoma (PDAC) presents a critical global health
challenge, and early detection is crucial for improving the 5-year survival
rate. Recent medical imaging and computational algorithm advances offer
potential solutions for early diagnosis. Deep learning, particularly in the
form of convolutional neural networks (CNNs), has demonstrated success in
medical image analysis tasks, including classification and segmentation.
However, the limited availability of clinical data for training purposes
continues to provide a significant obstacle. Data augmentation, generative
adversarial networks (GANs), and cross-validation are potential techniques to
address this limitation and improve model performance, but effective solutions
are still rare for 3D PDAC, where contrast is especially poor owing to the high
heterogeneity in both tumor and background tissues. In this study, we developed
a new GAN-based model, named 3DGAUnet, for generating realistic 3D CT images of
PDAC tumors and pancreatic tissue, which can generate the interslice connection
data that the existing 2D CT image synthesis models lack. Our innovation is to
develop a 3D U-Net architecture for the generator to improve shape and texture
learning for PDAC tumors and pancreatic tissue. Our approach offers a promising
path to tackle the urgent requirement for creative and synergistic methods to
combat PDAC. The development of this GAN-based model has the potential to
alleviate data scarcity issues, elevate the quality of synthesized data, and
thereby facilitate the progression of deep learning models to enhance the
accuracy and early detection of PDAC tumors, which could profoundly impact
patient outcomes. Furthermore, this model has the potential to be adapted to
other types of solid tumors, hence making significant contributions to the
field of medical imaging in terms of image processing models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Hannah Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baine_M/0/1/0/all/0/1&quot;&gt;Michael Baine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hollingsworth_M/0/1/0/all/0/1&quot;&gt;Michael A. Hollingsworth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Huijing Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Dandan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongfeng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05858">
<title>Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation. (arXiv:2311.05858v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05858</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Hyeongjun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1&quot;&gt;Ilhoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1&quot;&gt;Kwanghoon Sohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07113">
<title>SpectralGPT: Spectral Foundation Model. (arXiv:2311.07113v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07113</link>
<description rdf:parseType="Literal">&lt;p&gt;The foundation model has recently garnered significant attention due to its
potential to revolutionize the field of visual representation learning in a
self-supervised manner. While most foundation models are tailored to
effectively process RGB images for various visual tasks, there is a noticeable
gap in research focused on spectral data, which offers valuable information for
scene understanding, especially in remote sensing (RS) applications. To fill
this gap, we created for the first time a universal RS foundation model, named
SpectralGPT, which is purpose-built to handle spectral RS images using a novel
3D generative pretrained transformer (GPT). Compared to existing foundation
models, SpectralGPT 1) accommodates input images with varying sizes,
resolutions, time series, and regions in a progressive training fashion,
enabling full utilization of extensive RS big data; 2) leverages 3D token
generation for spatial-spectral coupling; 3) captures spectrally sequential
patterns via multi-target reconstruction; 4) trains on one million spectral RS
images, yielding models with over 600 million parameters. Our evaluation
highlights significant performance improvements with pretrained SpectralGPT
models, signifying substantial potential in advancing spectral RS big data
applications within the field of geoscience across four downstream tasks:
single/multi-label scene classification, semantic segmentation, and change
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1&quot;&gt;Danfeng Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1&quot;&gt;Naoto Yokoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghamisi_P/0/1/0/all/0/1&quot;&gt;Pedram Ghamisi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiuping Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plaza_A/0/1/0/all/0/1&quot;&gt;Antonio Plaza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paolo_G/0/1/0/all/0/1&quot;&gt;Gamba Paolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benediktsson_J/0/1/0/all/0/1&quot;&gt;Jon Atli Benediktsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1&quot;&gt;Jocelyn Chanussot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08172">
<title>Vision-Language Instruction Tuning: A Review and Analysis. (arXiv:2311.08172v2 [cs.MM] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08172</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction tuning is a crucial supervised training phase in Large Language
Models (LLMs), aiming to enhance the LLM&apos;s ability to generalize instruction
execution and adapt to user preferences. With the increasing integration of
multi-modal data into LLMs, there is growing interest in Vision-Language
Instruction Tuning (VLIT), which presents more complex characteristics compared
to pure text instruction tuning. In this paper, we systematically review the
latest VLIT settings and corresponding datasets in multi-modal LLMs and provide
insights into the intrinsic motivations behind their design. For the first
time, we offer a detailed multi-perspective categorization for existing VLIT
datasets and identify the characteristics that high-quality VLIT data should
possess. By incorporating these characteristics as guiding principles into the
existing VLIT data construction process, we conduct extensive experiments and
verify their positive impact on the performance of tuned multi-modal LLMs.
Furthermore, we discuss the current challenges and future research directions
of VLIT, providing insights for the continuous development of this field. The
code and dataset related to this paper have been open-sourced at
https://github.com/palchenli/VL-Instruction-Tuning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09257">
<title>UFOGen: You Forward Once Large Scale Text-to-Image Generation via Diffusion GANs. (arXiv:2311.09257v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09257</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models have demonstrated remarkable capabilities in
transforming textual prompts into coherent images, yet the computational cost
of their inference remains a persistent challenge. To address this issue, we
present UFOGen, a novel generative model designed for ultra-fast, one-step
text-to-image synthesis. In contrast to conventional approaches that focus on
improving samplers or employing distillation techniques for diffusion models,
UFOGen adopts a hybrid methodology, integrating diffusion models with a GAN
objective. Leveraging a newly introduced diffusion-GAN objective and
initialization with pre-trained diffusion models, UFOGen excels in efficiently
generating high-quality images conditioned on textual descriptions in a single
step. Beyond traditional text-to-image generation, UFOGen showcases versatility
in applications. Notably, UFOGen stands among the pioneering models enabling
one-step text-to-image generation and diverse downstream tasks, presenting a
significant advancement in the landscape of efficient generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_T/0/1/0/all/0/1&quot;&gt;Tingbo Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09680">
<title>Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09680</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
by reliable methods. Despite the abundance of literature on trustworthy LMs in
NLP, a systematic survey specifically delving into the trustworthiness of LMs
in CV remains absent. In order to mitigate this gap, we summarize four relevant
concerns that obstruct the trustworthy usage in vision of LMs in this survey,
including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers&apos;
understanding of this field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Li Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10093">
<title>The Chosen One: Consistent Characters in Text-to-Image Diffusion Models. (arXiv:2311.10093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10093</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in text-to-image generation models have unlocked vast
potential for visual creativity. However, these models struggle with generation
of consistent characters, a crucial aspect for numerous real-world applications
such as story visualization, game development asset design, advertising, and
more. Current methods typically rely on multiple pre-existing images of the
target character or involve labor-intensive manual processes. In this work, we
propose a fully automated solution for consistent character generation, with
the sole input being a text prompt. We introduce an iterative procedure that,
at each stage, identifies a coherent set of images sharing a similar identity
and extracts a more consistent identity from this set. Our quantitative
analysis demonstrates that our method strikes a better balance between prompt
alignment and identity consistency compared to the baseline methods, and these
findings are reinforced by a user study. To conclude, we showcase several
practical applications of our approach. Project page is available at
https://omriavrahami.com/the-chosen-one
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avrahami_O/0/1/0/all/0/1&quot;&gt;Omri Avrahami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1&quot;&gt;Amir Hertz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinker_Y/0/1/0/all/0/1&quot;&gt;Yael Vinker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1&quot;&gt;Moab Arar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1&quot;&gt;Shlomi Fruchter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1&quot;&gt;Ohad Fried&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lischinski_D/0/1/0/all/0/1&quot;&gt;Dani Lischinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10522">
<title>Enhancing Object Coherence in Layout-to-Image Synthesis. (arXiv:2311.10522v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10522</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel&apos;s generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10902">
<title>OCT2Confocal: 3D CycleGAN based Translation of Retinal OCT Images to Confocal Microscopy. (arXiv:2311.10902v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10902</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical coherence tomography (OCT) and confocal microscopy are pivotal in
retinal imaging, each presenting unique benefits and limitations. In vivo OCT
offers rapid, non-invasive imaging but can be hampered by clarity issues and
motion artifacts. Ex vivo confocal microscopy provides high-resolution,
cellular detailed color images but is invasive and poses ethical concerns and
potential tissue damage. To bridge these modalities, we developed a 3D CycleGAN
framework for unsupervised translation of in vivo OCT to ex vivo confocal
microscopy images. Applied to our OCT2Confocal dataset, this framework
effectively translates between 3D medical data domains, capturing vascular,
textural, and cellular details with precision. This marks the first attempt to
exploit the inherent 3D information of OCT and translate it into the rich,
detailed color domain of confocal microscopy. Assessed through quantitative and
qualitative metrics, the 3D CycleGAN framework demonstrates commendable image
fidelity and quality, outperforming existing methods despite the constraints of
limited data. This non-invasive generation of retinal confocal images has the
potential to further enhance diagnostic and monitoring capabilities in
ophthalmology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_X/0/1/0/all/0/1&quot;&gt;Xin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anantrasirichai_N/0/1/0/all/0/1&quot;&gt;Nantheera Anantrasirichai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nicholson_L/0/1/0/all/0/1&quot;&gt;Lindsay Nicholson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Achim_A/0/1/0/all/0/1&quot;&gt;Alin Achim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11178">
<title>Active Prompt Learning in Vision Language Models. (arXiv:2311.11178v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11178</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained Vision Language Models (VLMs) have demonstrated notable progress
in various zero-shot tasks, such as classification and retrieval. Despite their
performance, because improving performance on new tasks requires task-specific
knowledge, their adaptation is essential. While labels are needed for the
adaptation, acquiring them is typically expensive. To overcome this challenge,
active learning, a method of achieving a high performance by obtaining labels
for a small number of samples from experts, has been studied. Active learning
primarily focuses on selecting unlabeled samples for labeling and leveraging
them to train models. In this study, we pose the question, &quot;how can the
pre-trained VLMs be adapted under the active learning framework?&quot; In response
to this inquiry, we observe that (1) simply applying a conventional active
learning framework to pre-trained VLMs even may degrade performance compared to
random selection because of the class imbalance in labeling candidates, and (2)
the knowledge of VLMs can provide hints for achieving the balance before
labeling. Based on these observations, we devise a novel active learning
framework for VLMs, denoted as PCB. To assess the effectiveness of our
approach, we conduct experiments on seven different real-world datasets, and
the results demonstrate that PCB surpasses conventional active learning and
random sampling methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1&quot;&gt;Jihwan Bang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sumyeong Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae-Gil Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11317">
<title>Discrete approximations of Gaussian smoothing and Gaussian derivatives. (arXiv:2311.11317v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11317</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper develops an in-depth treatment concerning the problem of
approximating the Gaussian smoothing and Gaussian derivative computations in
scale-space theory for application on discrete data. With close connections to
previous axiomatic treatments of continuous and discrete scale-space theory, we
consider three main ways discretizing these scale-space operations in terms of
explicit discrete convolutions, based on either (i) sampling the Gaussian
kernels and the Gaussian derivative kernels, (ii) locally integrating the
Gaussian kernels and the Gaussian derivative kernels over each pixel support
region and (iii) basing the scale-space analysis on the discrete analogue of
the Gaussian kernel, and then computing derivative approximations by applying
small-support central difference operators to the spatially smoothed image
data.
&lt;/p&gt;
&lt;p&gt;We study the properties of these three main discretization methods both
theoretically and experimentally, and characterize their performance by
quantitative measures, including the results they give rise to with respect to
the task of scale selection, investigated for four different use cases, and
with emphasis on the behaviour at fine scales. The results show that the
sampled Gaussian kernels and derivatives as well as the integrated Gaussian
kernels and derivatives perform very poorly at very fine scales. At very fine
scales, the discrete analogue of the Gaussian kernel with its corresponding
discrete derivative approximations performs substantially better. The sampled
Gaussian kernel and the sampled Gaussian derivatives do, on the other hand,
lead to numerically very good approximations of the corresponding continuous
results, when the scale parameter is sufficiently large, in the experiments
presented in the paper, when the scale parameter is greater than a value of
about 1, in units of the grid spacing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindeberg_T/0/1/0/all/0/1&quot;&gt;Tony Lindeberg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11587">
<title>AKConv: Convolutional Kernel with Arbitrary Sampled Shapes and Arbitrary Number of Parameters. (arXiv:2311.11587v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11587</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks based on convolutional operations have achieved remarkable
results in the field of deep learning, but there are two inherent flaws in
standard convolutional operations. On the one hand, the convolution operation
be confined to a local window and cannot capture information from other
locations, and its sampled shapes is fixed. On the other hand, the size of the
convolutional kernel is fixed to k $\times$ k, which is a fixed square shape,
and the number of parameters tends to grow squarely with size. It is obvious
that the shape and size of targets are various in different datasets and at
different locations. Convolutional kernels with fixed sample shapes and squares
do not adapt well to changing targets. In response to the above questions, the
Alterable Kernel Convolution (AKConv) is explored in this work, which gives the
convolution kernel an arbitrary number of parameters and arbitrary sampled
shapes to provide richer options for the trade-off between network overhead and
performance. In AKConv, we define initial positions for convolutional kernels
of arbitrary size by means of a new coordinate generation algorithm. To adapt
to changes for targets, we introduce offsets to adjust the shape of the samples
at each position. Moreover, we explore the effect of the neural network by
using the AKConv with the same size and different initial sampled shapes.
AKConv completes the process of efficient feature extraction by irregular
convolutional operations and brings more exploration options for convolutional
sampling shapes. Object detection experiments on representative datasets
COCO2017, VOC 7+12 and VisDrone-DET2021 fully demonstrate the advantages of
AKConv. AKConv can be used as a plug-and-play convolutional operation to
replace convolutional operations to improve network performance. The code for
the relevant tasks can be found at https://github.com/CV-ZhangXin/AKConv.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yingze Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tingting Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Degang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yichen Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11602">
<title>A Multi-In-Single-Out Network for Video Frame Interpolation without Optical Flow. (arXiv:2311.11602v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11602</link>
<description rdf:parseType="Literal">&lt;p&gt;In general, deep learning-based video frame interpolation (VFI) methods have
predominantly focused on estimating motion vectors between two input frames and
warping them to the target time. While this approach has shown impressive
performance for linear motion between two input frames, it exhibits limitations
when dealing with occlusions and nonlinear movements. Recently, generative
models have been applied to VFI to address these issues. However, as VFI is not
a task focused on generating plausible images, but rather on predicting
accurate intermediate frames between two given frames, performance limitations
still persist. In this paper, we propose a multi-in-single-out (MISO) based VFI
method that does not rely on motion vector estimation, allowing it to
effectively model occlusions and nonlinear motion. Additionally, we introduce a
novel motion perceptual loss that enables MISO-VFI to better capture the
spatio-temporal correlations within the video frames. Our MISO-VFI method
achieves state-of-the-art results on VFI benchmarks Vimeo90K, Middlebury, and
UCF101, with a significant performance gap compared to existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaemin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minseok Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyobin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dong-Geol Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11810">
<title>DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding. (arXiv:2311.11810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11810</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents DocPedia, a novel large multimodal model (LMM) for
versatile OCR-free document understanding, capable of parsing images up to
2,560$\times$2,560 resolution. Unlike existing work either struggle with
high-resolution documents or give up the large language model thus vision or
language ability constrained, our DocPedia directly processes visual input in
the frequency domain rather than the pixel space. The unique characteristic
enables DocPedia to capture a greater amount of visual and textual information
using a limited number of visual tokens. To consistently enhance both
perception and comprehension abilities of our model, we develop a dual-stage
training strategy and enrich instructions/annotations of all training tasks
covering multiple document types. Extensive quantitative and qualitative
experiments conducted on various publicly available benchmarks confirm the
mutual benefits of jointly learning perception and comprehension tasks. The
results provide further evidence of the effectiveness and superior performance
of our DocPedia over other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wengang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Can Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11860">
<title>LION : Empowering Multimodal Large Language Model with Dual-Level Visual Knowledge. (arXiv:2311.11860v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11860</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have endowed LLMs with the ability
to perceive and understand multi-modal signals. However, most of the existing
MLLMs mainly adopt vision encoders pretrained on coarsely aligned image-text
pairs, leading to insufficient extraction and reasoning of visual knowledge. To
address this issue, we devise a dual-Level vIsual knOwledge eNhanced Multimodal
Large Language Model (LION), which empowers the MLLM by injecting visual
knowledge in two levels. 1) Progressive incorporation of fine-grained
spatial-aware visual knowledge. We design a vision aggregator cooperated with
region-level vision-language (VL) tasks to incorporate fine-grained
spatial-aware visual knowledge into the MLLM. To alleviate the conflict between
image-level and region-level VL tasks during incorporation, we devise a
dedicated stage-wise instruction-tuning strategy with mixture-of-adapters. This
progressive incorporation scheme contributes to the mutual promotion between
these two kinds of VL tasks. 2) Soft prompting of high-level semantic visual
evidence. We facilitate the MLLM with high-level semantic visual evidence by
leveraging diverse image tags. To mitigate the potential influence caused by
imperfect predicted tags, we propose a soft prompting method by embedding a
learnable token into the tailored text instruction. Comprehensive experiments
on several multi-modal benchmarks demonstrate the superiority of our model
(e.g., improvement of 5% accuracy on VSR and 3% CIDEr on TextCaps over
InstructBLIP, 5% accuracy on RefCOCOg over Kosmos-2).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gongwei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Leyang Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_R/0/1/0/all/0/1&quot;&gt;Rui Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_L/0/1/0/all/0/1&quot;&gt;Liqiang Nie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12050">
<title>3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing. (arXiv:2311.12050v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12050</link>
<description rdf:parseType="Literal">&lt;p&gt;The current GAN inversion methods typically can only edit the appearance and
shape of a single object and background while overlooking spatial information.
In this work, we propose a 3D editing framework, 3D-GOI, to enable multifaceted
editing of affine information (scale, translation, and rotation) on multiple
objects. 3D-GOI realizes the complex editing function by inverting the
abundance of attribute codes (object
shape/appearance/scale/rotation/translation, background shape/appearance, and
camera pose) controlled by GIRAFFE, a renowned 3D GAN. Accurately inverting all
the codes is challenging, 3D-GOI solves this challenge following three main
steps. First, we segment the objects and the background in a multi-object
image. Second, we use a custom Neural Inversion Encoder to obtain coarse codes
of each object. Finally, we use a round-robin optimization algorithm to get
precise codes to reconstruct the image. To the best of our knowledge, 3D-GOI is
the first framework to enable multifaceted editing on multiple objects. Both
qualitative and quantitative experiments demonstrate that 3D-GOI holds immense
potential for flexible, multifaceted editing in complex multi-object scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Long Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yanbin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12092">
<title>Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models. (arXiv:2311.12092v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12092</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method to create interpretable concept sliders that enable
precise control over attributes in image generations from diffusion models. Our
approach identifies a low-rank parameter direction corresponding to one concept
while minimizing interference with other attributes. A slider is created using
a small set of prompts or sample images; thus slider directions can be created
for either textual or visual concepts. Concept Sliders are plug-and-play: they
can be composed efficiently and continuously modulated, enabling precise
control over image generation. In quantitative experiments comparing to
previous editing techniques, our sliders exhibit stronger targeted edits with
lower interference. We showcase sliders for weather, age, styles, and
expressions, as well as slider compositions. We show how sliders can transfer
latents from StyleGAN for intuitive editing of visual concepts for which
textual description is difficult. We also find that our method can help address
persistent quality issues in Stable Diffusion XL including repair of object
deformations and fixing distorted hands. Our code, data, and trained sliders
are available at https://sliders.baulab.info/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandikota_R/0/1/0/all/0/1&quot;&gt;Rohit Gandikota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1&quot;&gt;Joanna Materzynska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tingrui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12144">
<title>Applications of Large Scale Foundation Models for Autonomous Driving. (arXiv:2311.12144v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12144</link>
<description rdf:parseType="Literal">&lt;p&gt;Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12386">
<title>Point, Segment and Count: A Generalized Framework for Object Counting. (arXiv:2311.12386v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12386</link>
<description rdf:parseType="Literal">&lt;p&gt;Class-agnostic object counting aims to count all objects in an image with
respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot
counting. Current state-of-the-art methods highly rely on density maps to
predict object counts, which lacks model interpretability. In this paper, we
propose a generalized framework for both few-shot and zero-shot object counting
based on detection. Our framework combines the superior advantages of two
foundation models without compromising their zero-shot capability: (\textbf{i})
SAM to segment all possible objects as mask proposals, and (\textbf{ii}) CLIP
to classify proposals to obtain accurate object counts. However, this strategy
meets the obstacles of efficiency overhead and the small crowded objects that
cannot be localized and distinguished. To address these issues, our framework,
termed PseCo, follows three steps: point, segment, and count. Specifically, we
first propose a class-agnostic object localization to provide accurate but
least point prompts for SAM, which consequently not only reduces computation
costs but also avoids missing small objects. Furthermore, we propose a
generalized object classification that leverages CLIP image/text embeddings as
the classifier, following a hierarchical knowledge distillation to obtain
discriminative classifications among hierarchical mask proposals. Extensive
experimental results on FSC-147 dataset demonstrate that PseCo achieves
state-of-the-art performance in both few-shot/zero-shot object
counting/detection, with additional results on large-scale COCO and LVIS
datasets. The source code is available at
\url{https://github.com/Hzzone/PseCo}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhizhong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1&quot;&gt;Mingliang Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13372">
<title>MRGazer: Decoding Eye Gaze Points from Functional Magnetic Resonance Imaging in Individual Space. (arXiv:2311.13372v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13372</link>
<description rdf:parseType="Literal">&lt;p&gt;Eye-tracking research has proven valuable in understanding numerous cognitive
functions. Recently, Frey et al. provided an exciting deep learning method for
learning eye movements from fMRI data. However, it needed to co-register fMRI
into standard space to obtain eyeballs masks, and thus required additional
templates and was time consuming. To resolve this issue, in this paper, we
propose a framework named MRGazer for predicting eye gaze points from fMRI in
individual space. The MRGazer consisted of eyeballs extraction module and a
residual network-based eye gaze prediction. Compared to the previous method,
the proposed framework skips the fMRI co-registration step, simplifies the
processing protocol and achieves end-to-end eye gaze regression. The proposed
method achieved superior performance in a variety of eye movement tasks than
the co-registration-based method, and delivered objective results within a
shorter time (~ 0.02 Seconds for each volume) than prior method (~0.3 Seconds
for each volume).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiuwen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Rongjie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jie Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_B/0/1/0/all/0/1&quot;&gt;Bensheng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13404">
<title>Animatable 3D Gaussians for High-fidelity Synthesis of Human Motions. (arXiv:2311.13404v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13404</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel animatable 3D Gaussian model for rendering high-fidelity
free-view human motions in real time. Compared to existing NeRF-based methods,
the model owns better capability in synthesizing high-frequency details without
the jittering problem across video frames. The core of our model is a novel
augmented 3D Gaussian representation, which attaches each Gaussian with a
learnable code. The learnable code serves as a pose-dependent appearance
embedding for refining the erroneous appearance caused by geometric
transformation of Gaussians, based on which an appearance refinement model is
learned to produce residual Gaussian properties to match the appearance in
target pose. To force the Gaussians to learn the foreground human only without
background interference, we further design a novel alpha loss to explicitly
constrain the Gaussians within the human body. We also propose to jointly
optimize the human joint parameters to improve the appearance accuracy. The
animatable 3D Gaussian model can be learned with shallow MLPs, so new human
motions can be synthesized in real time (66 fps on avarage). Experiments show
that our model has superior performance over NeRF-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_K/0/1/0/all/0/1&quot;&gt;Keyang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_T/0/1/0/all/0/1&quot;&gt;Tianjia Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13959">
<title>RankFeat&amp;RankWeight: Rank-1 Feature/Weight Removal for Out-of-distribution Detection. (arXiv:2311.13959v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13959</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of out-of-distribution (OOD) detection is crucial for deploying
machine learning models in real-world settings. In this paper, we observe that
the singular value distributions of the in-distribution (ID) and OOD features
are quite different: the OOD feature matrix tends to have a larger dominant
singular value than the ID feature, and the class predictions of OOD samples
are largely determined by it. This observation motivates us to propose
\texttt{RankFeat}, a simple yet effective \emph{post hoc} approach for OOD
detection by removing the rank-1 matrix composed of the largest singular value
and the associated singular vectors from the high-level feature.
\texttt{RankFeat} achieves \emph{state-of-the-art} performance and reduces the
average false positive rate (FPR95) by 17.90\% compared with the previous best
method. The success of \texttt{RankFeat} motivates us to investigate whether a
similar phenomenon would exist in the parameter matrices of neural networks. We
thus propose \texttt{RankWeight} which removes the rank-1 weight from the
parameter matrices of a single deep layer. Our \texttt{RankWeight}is also
\emph{post hoc} and only requires computing the rank-1 matrix once. As a
standalone approach, \texttt{RankWeight} has very competitive performance
against other methods across various backbones. Moreover, \texttt{RankWeight}
enjoys flexible compatibility with a wide range of OOD detection methods. The
combination of \texttt{RankWeight} and \texttt{RankFeat} refreshes the new
\emph{state-of-the-art} performance, achieving the FPR95 as low as 16.13\% on
the ImageNet-1k benchmark. Extensive ablation studies and comprehensive
theoretical analyses are presented to support the empirical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yue Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14084">
<title>AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14084</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon has elevated the issue of source bias in text retrieval for web
searches. Specifically, neural retrieval models tend to rank generated texts
higher than human-written texts. In this paper, we extend the study of this
bias to cross-modal retrieval. Firstly, we successfully construct a suitable
benchmark to explore the existence of the bias. Subsequent extensive
experiments on this benchmark reveal that AI-generated images introduce an
invisible relevance bias to text-image retrieval models. Specifically, our
experiments show that text-image retrieval models tend to rank the AI-generated
images higher than the real images, even though the AI-generated images do not
exhibit more visually relevant features to the query than real images. This
invisible relevance bias is prevalent across retrieval models with varying
training data and architectures. Furthermore, our subsequent exploration
reveals that the inclusion of AI-generated images in the training data of the
retrieval models exacerbates the invisible relevance bias. The above phenomenon
triggers a vicious cycle, which makes the invisible relevance bias become more
and more serious. To elucidate the potential causes of invisible relevance and
address the aforementioned issues, we introduce an effective training method
aimed at alleviating the invisible relevance bias. Subsequently, we apply our
proposed debiasing method to retroactively identify the causes of invisible
relevance, revealing that the AI-generated images induce the image encoder to
embed additional information into their representation. This information
exhibits a certain consistency across generated images with different semantics
and can make the retriever estimate a higher relevance score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shicheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1&quot;&gt;Danyang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1&quot;&gt;Liang Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jingcheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>