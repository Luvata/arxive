<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12064" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12092" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12398" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12430" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12490" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12603" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12623" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12655" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12751" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1810.12813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2004.07780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.12311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.13788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.11057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.14372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08660" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13807" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15513" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.05246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01351" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18183" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12320" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.04342" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06504" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07784" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11908" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.12033">
<title>An improved two-threshold quantum segmentation algorithm for NEQR image. (arXiv:2311.12033v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.12033</link>
<description rdf:parseType="Literal">&lt;p&gt;The quantum image segmentation algorithm is to divide a quantum image into
several parts, but most of the existing algorithms use more quantum
resource(qubit) or cannot process the complex image. In this paper, an improved
two-threshold quantum segmentation algorithm for NEQR image is proposed, which
can segment the complex gray-scale image into a clear ternary image by using
fewer qubits and can be scaled to use n thresholds for n + 1 segmentations. In
addition, a feasible quantum comparator is designed to distinguish the
gray-scale values with two thresholds, and then a scalable quantum circuit is
designed to segment the NEQR image. For a 2^(n)*2^(n) image with q gray-scale
levels, the quantum cost of our algorithm can be reduced to 60q-6, which is
lower than other existing quantum algorithms and does not increase with the
image&apos;s size increases. The experiment on IBM Q demonstrates that our algorithm
can effectively segment the image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhiliang Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenjie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12041">
<title>Automated Detection of hidden Damages and Impurities in Aluminum Die Casting Materials and Fibre-Metal Laminates using Low-quality X-ray Radiography, Synthetic X-ray Data Augmentation by Simulation, and Machine Learning. (arXiv:2311.12041v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12041</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection and characterization of hidden defects, impurities, and damages in
layered composites like Fibre laminates, e.g., Fibre Metal Laminates (FML), as
well as in monolithic materials, e.g., aluminum die casting materials, is still
a challenge. This work discusses methods and challenges in data-driven modeling
of automated damage and defect detectors using X-ray single- and
multi-projection (CT) images. Three main issues are identified: Data and
feature variance, data feature labeling (for supervised machine learning), and
the missing ground truth. It will be shown that only simulation of data can
deliver a ground truth data set and accurate labeling. Noise has significant
impact on the feature detection and will be discussed. Data-driven feature
detectors are implemented with semantic pixel- or z-profile Convolutional
Neural Networks and LSTM Auto-encoders. Data is measured with three different
devices: A low-quality and low-cost (Low-Q), a mid- and a high-quality
(micro-CT, Mid-/High-Q) device. The goals of this work are the training of
robust and generalized feature detectors with synthetic data and the transition
from High- and Mid-Q laboratory measuring technologies towards in-field usable
technologies and methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosse_S/0/1/0/all/0/1&quot;&gt;Stefan Bosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehmhus_D/0/1/0/all/0/1&quot;&gt;Dirk Lehmhus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12043">
<title>Efficient Domain Adaptation via Generative Prior for 3D Infant Pose Estimation. (arXiv:2311.12043v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12043</link>
<description rdf:parseType="Literal">&lt;p&gt;Although 3D human pose estimation has gained impressive development in recent
years, only a few works focus on infants, that have different bone lengths and
also have limited data. Directly applying adult pose estimation models
typically achieves low performance in the infant domain and suffers from
out-of-distribution issues. Moreover, the limitation of infant pose data
collection also heavily constrains the efficiency of learning-based models to
lift 2D poses to 3D. To deal with the issues of small datasets, domain
adaptation and data augmentation are commonly used techniques. Following this
paradigm, we take advantage of an optimization-based method that utilizes
generative priors to predict 3D infant keypoints from 2D keypoints without the
need of large training data. We further apply a guided diffusion model to
domain adapt 3D adult pose to infant pose to supplement small datasets.
Besides, we also prove that our method, ZeDO-i, could attain efficient domain
adaptation, even if only a small number of data is given. Quantitatively, we
claim that our model attains state-of-the-art MPJPE performance of 43.6 mm on
the SyRIP dataset and 21.2 mm on the MINI-RGBD dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Wenhao Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12046">
<title>LATIS: Lambda Abstraction-based Thermal Image Super-resolution. (arXiv:2311.12046v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12046</link>
<description rdf:parseType="Literal">&lt;p&gt;Single image super-resolution (SISR) is an effective technique to improve the
quality of low-resolution thermal images. Recently, transformer-based methods
have achieved significant performance in SISR. However, in the SR task, only a
small number of pixels are involved in the transformers self-attention (SA)
mechanism due to the computational complexity of the attention mechanism. The
lambda abstraction is a promising alternative to SA in modeling long-range
interactions while being computationally more efficient. This paper presents
lambda abstraction-based thermal image super-resolution (LATIS), a novel
lightweight architecture for SISR of thermal images. LATIS sequentially
captures local and global information using the local and global feature block
(LGFB). In LGFB, we introduce a global feature extraction (GFE) module based on
the lambda abstraction mechanism, channel-shuffle and convolution (CSConv)
layer to encode local context. Besides, to improve the performance further, we
propose a differentiable patch-wise histogram-based loss function. Experimental
results demonstrate that our LATIS, with the least model parameters and
complexity, achieves better or comparable performance with state-of-the-art
methods across multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Panda_G/0/1/0/all/0/1&quot;&gt;Gargi Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kundu_S/0/1/0/all/0/1&quot;&gt;Soumitra Kundu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Saumik Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Routray_A/0/1/0/all/0/1&quot;&gt;Aurobinda Routray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12049">
<title>Energizing Federated Learning via Filter-Aware Attention. (arXiv:2311.12049v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12049</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is a promising distributed paradigm, eliminating the
need for data sharing but facing challenges from data heterogeneity.
Personalized parameter generation through a hypernetwork proves effective, yet
existing methods fail to personalize local model structures. This leads to
redundant parameters struggling to adapt to diverse data distributions. To
address these limitations, we propose FedOFA, utilizing personalized orthogonal
filter attention for parameter recalibration. The core is the Two-stream
Filter-aware Attention (TFA) module, meticulously designed to extract
personalized filter-aware attention maps, incorporating Intra-Filter Attention
(IntraFa) and Inter-Filter Attention (InterFA) streams. These streams enhance
representation capability and explore optimal implicit structures for local
models. Orthogonal regularization minimizes redundancy by averting
inter-correlation between filters. Furthermore, we introduce an
Attention-Guided Pruning Strategy (AGPS) for communication efficiency. AGPS
selectively retains crucial neurons while masking redundant ones, reducing
communication costs without performance sacrifice. Importantly, FedOFA operates
on the server side, incurring no additional computational cost on the client,
making it advantageous in communication-constrained scenarios. Extensive
experiments validate superior performance over state-of-the-art approaches,
with code availability upon paper acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zerui Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huangfu_H/0/1/0/all/0/1&quot;&gt;Huijie Huangfu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teoh_A/0/1/0/all/0/1&quot;&gt;Andrew Beng Jin Teoh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_H/0/1/0/all/0/1&quot;&gt;Hongming Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12050">
<title>3D-GOI: 3D GAN Omni-Inversion for Multifaceted and Multi-object Editing. (arXiv:2311.12050v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12050</link>
<description rdf:parseType="Literal">&lt;p&gt;The current GAN inversion methods typically can only edit the appearance and
shape of a single object and background while overlooking spatial information.
In this work, we propose a 3D editing framework, 3D-GOI, to enable multifaceted
editing of affine information (scale, translation, and rotation) on multiple
objects. 3D-GOI realizes the complex editing function by inverting the
abundance of attribute codes (object
shape/appearance/scale/rotation/translation, background shape/appearance, and
camera pose) controlled by GIRAFFE, a renowned 3D GAN. Accurately inverting all
the codes is challenging, 3D-GOI solves this challenge following three main
steps. First, we segment the objects and the background in a multi-object
image. Second, we use a custom Neural Inversion Encoder to obtain coarse codes
of each object. Finally, we use a round-robin optimization algorithm to get
precise codes to reconstruct the image. To the best of our knowledge, 3D-GOI is
the first framework to enable multifaceted editing on multiple objects. Both
qualitative and quantitative experiments demonstrate that 3D-GOI holds immense
potential for flexible, multifaceted editing in complex multi-object scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Long Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Yong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lechao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yanbin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengyuan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12051">
<title>Boost Adversarial Transferability by Uniform Scale and Mix Mask Method. (arXiv:2311.12051v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12051</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples generated from surrogate models often possess the
ability to deceive other black-box models, a property known as transferability.
Recent research has focused on enhancing adversarial transferability, with
input transformation being one of the most effective approaches. However,
existing input transformation methods suffer from two issues. Firstly, certain
methods, such as the Scale-Invariant Method, employ exponentially decreasing
scale invariant parameters that decrease the adaptability in generating
effective adversarial examples across multiple scales. Secondly, most mixup
methods only linearly combine candidate images with the source image, leading
to reduced features blending effectiveness. To address these challenges, we
propose a framework called Uniform Scale and Mix Mask Method (US-MM) for
adversarial example generation. The Uniform Scale approach explores the upper
and lower boundaries of perturbation with a linear factor, minimizing the
negative impact of scale copies. The Mix Mask method introduces masks into the
mixing process in a nonlinear manner, significantly improving the effectiveness
of mixing strategies. Ablation experiments are conducted to validate the
effectiveness of each component in US-MM and explore the effect of
hyper-parameters. Empirical evaluations on standard ImageNet datasets
demonstrate that US-MM achieves an average of 7% better transfer attack success
rate compared to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_Z/0/1/0/all/0/1&quot;&gt;Zijian Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qianmu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_z/0/1/0/all/0/1&quot;&gt;zhichao Lian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12052">
<title>MagicDance: Realistic Human Dance Video Generation with Motions &amp; Facial Expressions Transfer. (arXiv:2311.12052v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12052</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we propose MagicDance, a diffusion-based model for 2D human
motion and facial expression transfer on challenging human dance videos.
Specifically, we aim to generate human dance videos of any target identity
driven by novel pose sequences while keeping the identity unchanged. To this
end, we propose a two-stage training strategy to disentangle human motions and
appearance (e.g., facial expressions, skin tone and dressing), consisting of
the pretraining of an appearance-control block and fine-tuning of an
appearance-pose-joint-control block over human dance poses of the same dataset.
Our novel design enables robust appearance control with temporally consistent
upper body, facial attributes, and even background. The model also generalizes
well on unseen human identities and complex motion sequences without the need
for any fine-tuning with additional data with diverse human attributes by
leveraging the prior knowledge of image diffusion models. Moreover, the
proposed model is easy to use and can be considered as a plug-in
module/extension to Stable Diffusion. We also demonstrate the model&apos;s ability
for zero-shot 2D animation generation, enabling not only the appearance
transfer from one identity to another but also allowing for cartoon-like
stylization given only pose inputs. Extensive experiments demonstrate our
superior performance on the TikTok dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_D/0/1/0/all/0/1&quot;&gt;Di Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yichun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Q/0/1/0/all/0/1&quot;&gt;Quankai Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jessica Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guoxian Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Q/0/1/0/all/0/1&quot;&gt;Qing Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soleymani_M/0/1/0/all/0/1&quot;&gt;Mohammad Soleymani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12056">
<title>Kuro Siwo: 12.1 billion $m^2$ under the water. A global multi-temporal satellite dataset for rapid flood mapping. (arXiv:2311.12056v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12056</link>
<description rdf:parseType="Literal">&lt;p&gt;Global floods, exacerbated by climate change, pose severe threats to human
life, infrastructure, and the environment. This urgency is highlighted by
recent catastrophic events in Pakistan and New Zealand, underlining the
critical need for precise flood mapping for guiding restoration efforts,
understanding vulnerabilities, and preparing for future events. While Synthetic
Aperture Radar (SAR) offers day-and-night, all-weather imaging capabilities,
harnessing it for deep learning is hindered by the absence of a large annotated
dataset. To bridge this gap, we introduce Kuro Siwo, a meticulously curated
multi-temporal dataset, spanning 32 flood events globally. Our dataset maps
more than 63 billion m2 of land, with 12.1 billion of them being either a
flooded area or a permanent water body. Kuro Siwo stands out for its
unparalleled annotation quality to facilitate rapid flood mapping in a
supervised setting. We also augment learning by including a large unlabeled set
of SAR samples, aimed at self-supervised pretraining. We provide an extensive
benchmark and strong baselines for a diverse set of flood events from Europe,
America, Africa and Australia. Our benchmark demonstrates the quality of Kuro
Siwo annotations, training models that can achieve $\approx$ 85% and $\approx$
87% in F1-score for flooded areas and general water detection respectively.
This work calls on the deep learning community to develop solution-driven
algorithms for rapid flood mapping, with the potential to aid civil protection
and humanitarian agencies amid climate change challenges. Our code and data
will be made available at https://github.com/Orion-AI-Lab/KuroSiwo
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bountos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Ioannis Bountos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sdraka_M/0/1/0/all/0/1&quot;&gt;Maria Sdraka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavras_A/0/1/0/all/0/1&quot;&gt;Angelos Zavras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karasante_I/0/1/0/all/0/1&quot;&gt;Ilektra Karasante&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karavias_A/0/1/0/all/0/1&quot;&gt;Andreas Karavias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herekakis_T/0/1/0/all/0/1&quot;&gt;Themistocles Herekakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thanasou_A/0/1/0/all/0/1&quot;&gt;Angeliki Thanasou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michail_D/0/1/0/all/0/1&quot;&gt;Dimitrios Michail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papoutsis_I/0/1/0/all/0/1&quot;&gt;Ioannis Papoutsis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12058">
<title>FlashOcc: Fast and Memory-Efficient Occupancy Prediction via Channel-to-Height Plugin. (arXiv:2311.12058v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12058</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the capability of mitigating the long-tail deficiencies and
intricate-shaped absence prevalent in 3D object detection, occupancy prediction
has become a pivotal component in autonomous driving systems. However, the
procession of three-dimensional voxel-level representations inevitably
introduces large overhead in both memory and computation, obstructing the
deployment of to-date occupancy prediction approaches. In contrast to the trend
of making the model larger and more complicated, we argue that a desirable
framework should be deployment-friendly to diverse chips while maintaining high
precision. To this end, we propose a plug-and-play paradigm, namely FlashOCC,
to consolidate rapid and memory-efficient occupancy prediction while
maintaining high precision. Particularly, our FlashOCC makes two improvements
based on the contemporary voxel-level occupancy prediction approaches. Firstly,
the features are kept in the BEV, enabling the employment of efficient 2D
convolutional layers for feature extraction. Secondly, a channel-to-height
transformation is introduced to lift the output logits from the BEV into the 3D
space. We apply the FlashOCC to diverse occupancy prediction baselines on the
challenging Occ3D-nuScenes benchmarks and conduct extensive experiments to
validate the effectiveness. The results substantiate the superiority of our
plug-and-play paradigm over previous state-of-the-art methods in terms of
precision, runtime efficiency, and memory costs, demonstrating its potential
for deployment. The code will be made available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zichen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1&quot;&gt;Changyong Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jiajun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Kangjie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zongdai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiangyong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dawei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12059">
<title>Towards Function Space Mesh Watermarking: Protecting the Copyright of Signed Distance Fields. (arXiv:2311.12059v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12059</link>
<description rdf:parseType="Literal">&lt;p&gt;The signed distance field (SDF) represents 3D geometries in continuous
function space. Due to its continuous nature, explicit 3D models (e.g., meshes)
can be extracted from it at arbitrary resolution, which means losing the SDF is
equivalent to losing the mesh. Recent research has shown meshes can also be
extracted from SDF-enhanced neural radiance fields (NeRF). Such a signal raises
an alarm that any implicit neural representation with SDF enhancement can
extract the original mesh, which indicates identifying the SDF&apos;s intellectual
property becomes an urgent issue. This paper proposes FuncMark, a robust and
invisible watermarking method to protect the copyright of signed distance
fields by leveraging analytic on-surface deformations to embed binary watermark
messages. Such deformation can survive isosurfacing and thus be inherited by
the extracted meshes for further watermark message decoding. Our method can
recover the message with high-resolution meshes extracted from SDFs and detect
the watermark even when mesh vertices are extremely sparse. Furthermore, our
method is robust even when various distortions (including remeshing) are
encountered. Extensive experiments demonstrate that our \tool significantly
outperforms state-of-the-art approaches and the message is still detectable
even when only 50 vertex samples are given.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xingyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1&quot;&gt;Guanhui Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_C/0/1/0/all/0/1&quot;&gt;Chengdong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiapu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xuetao Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12062">
<title>PBWR: Parametric Building Wireframe Reconstruction from Aerial LiDAR Point Clouds. (arXiv:2311.12062v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12062</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present an end-to-end 3D building wireframe reconstruction
method to regress edges directly from aerial LiDAR point clouds.Our method,
named Parametric Building Wireframe Reconstruction (PBWR), takes aerial LiDAR
point clouds and initial edge entities as input, and fully uses self-attention
mechanism of transformers to regress edge parameters without any intermediate
steps such as corner prediction. We propose an edge non-maximum suppression
(E-NMS) module based on edge similarityto remove redundant edges. Additionally,
a dedicated edge loss function is utilized to guide the PBWR in regressing
edges parameters, where simple use of edge distance loss isn&apos;t suitable. In our
experiments, we demonstrate state-of-the-art results on the Building3D dataset,
achieving an improvement of approximately 36% in entry-level dataset edge
accuracy and around 42% improvement in the Tallinn dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shangfeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruisheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Bo Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxin Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12063">
<title>DatasetNeRF: Efficient 3D-aware Data Factory with Generative Radiance Fields. (arXiv:2311.12063v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12063</link>
<description rdf:parseType="Literal">&lt;p&gt;Progress in 3D computer vision tasks demands a huge amount of data, yet
annotating multi-view images with 3D-consistent annotations, or point clouds
with part segmentation is both time-consuming and challenging. This paper
introduces DatasetNeRF, a novel approach capable of generating infinite,
high-quality 3D-consistent 2D annotations alongside 3D point cloud
segmentations, while utilizing minimal 2D human-labeled annotations.
Specifically, we leverage the strong semantic prior within a 3D generative
model to train a semantic decoder, requiring only a handful of fine-grained
labeled samples. Once trained, the decoder efficiently generalizes across the
latent space, enabling the generation of infinite data. The generated data is
applicable across various computer vision tasks, including video segmentation
and 3D point cloud segmentation. Our approach not only surpasses baseline
models in segmentation quality, achieving superior 3D consistency and
segmentation precision on individual images, but also demonstrates versatility
by being applicable to both articulated and non-articulated generative models.
Furthermore, we explore applications stemming from our approach, such as
3D-aware semantic editing and 3D inversion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yu Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1&quot;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sibo Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1&quot;&gt;Adam Kortylewski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12064">
<title>Security Fence Inspection at Airports Using Object Detection. (arXiv:2311.12064v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12064</link>
<description rdf:parseType="Literal">&lt;p&gt;To ensure the security of airports, it is essential to protect the airside
from unauthorized access. For this purpose, security fences are commonly used,
but they require regular inspection to detect damages. However, due to the
growing shortage of human specialists and the large manual effort, there is the
need for automated methods. The aim is to automatically inspect the fence for
damage with the help of an autonomous robot. In this work, we explore object
detection methods to address the fence inspection task and localize various
types of damages. In addition to evaluating four State-of-the-Art (SOTA) object
detection models, we analyze the impact of several design criteria, aiming at
adapting to the task-specific challenges. This includes contrast adjustment,
optimization of hyperparameters, and utilization of modern backbones. The
experimental results indicate that our optimized You Only Look Once v5 (YOLOv5)
model achieves the highest accuracy of the four methods with an increase of
6.9% points in Average Precision (AP) compared to the baseline. Moreover, we
show the real-time capability of the model. The trained models are published on
GitHub: https://github.com/N-Friederich/airport_fence_inspection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friederich_N/0/1/0/all/0/1&quot;&gt;Nils Friederich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Specker_A/0/1/0/all/0/1&quot;&gt;Andreas Specker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyerer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Beyerer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12065">
<title>Few-Shot Classification &amp; Segmentation Using Large Language Models Agent. (arXiv:2311.12065v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12065</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of few-shot image classification and segmentation (FS-CS) requires
the classification and segmentation of target objects in a query image, given
only a few examples of the target classes. We introduce a method that utilises
large language models (LLM) as an agent to address the FS-CS problem in a
training-free manner. By making the LLM the task planner and off-the-shelf
vision models the tools, the proposed method is capable of classifying and
segmenting target objects using only image-level labels. Specifically,
chain-of-thought prompting and in-context learning guide the LLM to observe
support images like human; vision models such as Segment Anything Model (SAM)
and GPT-4Vision assist LLM understand spatial and semantic information at the
same time. Ultimately, the LLM uses its summarizing and reasoning capabilities
to classify and segment the query image. The proposed method&apos;s modular
framework makes it easily extendable. Our approach achieves state-of-the-art
performance on the Pascal-5i dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_T/0/1/0/all/0/1&quot;&gt;Tian Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wuliang Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12067">
<title>Quality and Quantity: Unveiling a Million High-Quality Images for Text-to-Image Synthesis in Fashion Design. (arXiv:2311.12067v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12067</link>
<description rdf:parseType="Literal">&lt;p&gt;The fusion of AI and fashion design has emerged as a promising research area.
However, the lack of extensive, interrelated data on clothing and try-on stages
has hindered the full potential of AI in this domain. Addressing this, we
present the Fashion-Diffusion dataset, a product of multiple years&apos; rigorous
effort. This dataset, the first of its kind, comprises over a million
high-quality fashion images, paired with detailed text descriptions. Sourced
from a diverse range of geographical locations and cultural backgrounds, the
dataset encapsulates global fashion trends. The images have been meticulously
annotated with fine-grained attributes related to clothing and humans,
simplifying the fashion design process into a Text-to-Image (T2I) task. The
Fashion-Diffusion dataset not only provides high-quality text-image pairs and
diverse human-garment pairs but also serves as a large-scale resource about
humans, thereby facilitating research in T2I generation. Moreover, to foster
standardization in the T2I-based fashion design field, we propose a new
benchmark comprising multiple datasets for evaluating the performance of
fashion design models. This work represents a significant leap forward in the
realm of AI-driven fashion design, setting a new standard for future research
in this field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jia Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lichao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zijie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1&quot;&gt;Fayu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_M/0/1/0/all/0/1&quot;&gt;MiaoMiao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_F/0/1/0/all/0/1&quot;&gt;Fangsheng Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lili Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Z/0/1/0/all/0/1&quot;&gt;Zhenzhong Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12068">
<title>Enhancing Novel Object Detection via Cooperative Foundational Models. (arXiv:2311.12068v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12068</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we address the challenging and emergent problem of novel object
detection (NOD), focusing on the accurate detection of both known and novel
object categories during inference. Traditional object detection algorithms are
inherently closed-set, limiting their capability to handle NOD. We present a
novel approach to transform existing closed-set detectors into open-set
detectors. This transformation is achieved by leveraging the complementary
strengths of pre-trained foundational models, specifically CLIP and SAM,
through our cooperative mechanism. Furthermore, by integrating this mechanism
with state-of-the-art open-set detectors such as GDINO, we establish new
benchmarks in object detection performance. Our method achieves 17.42 mAP in
novel object detection and 42.08 mAP for known objects on the challenging LVIS
dataset. Adapting our approach to the COCO OVD split, we surpass the current
state-of-the-art by a margin of 7.2 $ \text{AP}_{50} $ for novel classes. Our
code is available at
https://github.com/rohit901/cooperative-foundational-models .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadwaj_R/0/1/0/all/0/1&quot;&gt;Rohit Bharadwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12070">
<title>FDDM: Unsupervised Medical Image Translation with a Frequency-Decoupled Diffusion Model. (arXiv:2311.12070v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12070</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated significant potential in producing
high-quality images for medical image translation to aid disease diagnosis,
localization, and treatment. Nevertheless, current diffusion models have
limited success in achieving faithful image translations that can accurately
preserve the anatomical structures of medical images, especially for unpaired
datasets. The preservation of structural and anatomical details is essential to
reliable medical diagnosis and treatment planning, as structural mismatches can
lead to disease misidentification and treatment errors. In this study, we
introduced a frequency-decoupled diffusion model (FDDM), a novel framework that
decouples the frequency components of medical images in the Fourier domain
during the translation process, to allow structure-preserved high-quality image
conversion. FDDM applies an unsupervised frequency conversion module to
translate the source medical images into frequency-specific outputs and then
uses the frequency-specific information to guide a following diffusion model
for final source-to-target image translation. We conducted extensive
evaluations of FDDM using a public brain MR-to-CT translation dataset, showing
its superior performance against other GAN-, VAE-, and diffusion-based models.
Metrics including the Frechet inception distance (FID), the peak
signal-to-noise ratio (PSNR), and the structural similarity index measure
(SSIM) were assessed. FDDM achieves an FID of 29.88, less than half of the
second best. These results demonstrated FDDM&apos;s prowess in generating
highly-realistic target-domain images while maintaining the faithfulness of
translated anatomical structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Hua-Chieh Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaoxue Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;You Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12071">
<title>Enhancing Low-dose CT Image Reconstruction by Integrating Supervised and Unsupervised Learning. (arXiv:2311.12071v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12071</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional model-based image reconstruction (MBIR) methods combine forward
and noise models with simple object priors. Recent application of deep learning
methods for image reconstruction provides a successful data-driven approach to
addressing the challenges when reconstructing images with undersampled
measurements or various types of noise. In this work, we propose a hybrid
supervised-unsupervised learning framework for X-ray computed tomography (CT)
image reconstruction. The proposed learning formulation leverages both sparsity
or unsupervised learning-based priors and neural network reconstructors to
simulate a fixed-point iteration process. Each proposed trained block consists
of a deterministic MBIR solver and a neural network. The information flows in
parallel through these two reconstructors and is then optimally combined.
Multiple such blocks are cascaded to form a reconstruction pipeline. We
demonstrate the efficacy of this learned hybrid model for low-dose CT image
reconstruction with limited training data, where we use the NIH AAPM Mayo
Clinic Low Dose CT Grand Challenge dataset for training and testing. In our
experiments, we study combinations of supervised deep network reconstructors
and MBIR solver with learned sparse representation-based priors or analytical
priors. Our results demonstrate the promising performance of the proposed
framework compared to recent low-dose CT reconstruction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Ling Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhishen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Long_Y/0/1/0/all/0/1&quot;&gt;Yong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravishankar_S/0/1/0/all/0/1&quot;&gt;Saiprasad Ravishankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12075">
<title>BadCLIP: Dual-Embedding Guided Backdoor Attack on Multimodal Contrastive Learning. (arXiv:2311.12075v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12075</link>
<description rdf:parseType="Literal">&lt;p&gt;Studying backdoor attacks is valuable for model copyright protection and
enhancing defenses. While existing backdoor attacks have successfully infected
multimodal contrastive learning models such as CLIP, they can be easily
countered by specialized backdoor defenses for MCL models. This paper reveals
the threats in this practical scenario that backdoor attacks can remain
effective even after defenses and introduces the \emph{\toolns} attack, which
is resistant to backdoor detection and model fine-tuning defenses. To achieve
this, we draw motivations from the perspective of the Bayesian rule and propose
a dual-embedding guided framework for backdoor attacks. Specifically, we ensure
that visual trigger patterns approximate the textual target semantics in the
embedding space, making it challenging to detect the subtle parameter
variations induced by backdoor learning on such natural trigger patterns.
Additionally, we optimize the visual trigger patterns to align the poisoned
samples with target vision features in order to hinder the backdoor unlearning
through clean fine-tuning. Extensive experiments demonstrate that our attack
significantly outperforms state-of-the-art baselines (+45.3% ASR) in the
presence of SoTA backdoor defenses, rendering these mitigation and detection
strategies virtually ineffective. Furthermore, our approach effectively attacks
some more rigorous scenarios like downstream tasks. We believe that this paper
raises awareness regarding the potential threats associated with the practical
application of multimodal contrastive learning and encourages the development
of more robust defense mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingli Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Ee-Chien Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12076">
<title>Towards Few-shot Out-of-Distribution Detection. (arXiv:2311.12076v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12076</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is critical for ensuring the reliability
of open-world intelligent systems. Despite the notable advancements in existing
OOD detection methodologies, our study identifies a significant performance
drop under the scarcity of training samples. In this context, we introduce a
novel few-shot OOD detection benchmark, carefully constructed to address this
gap. Our empirical analysis reveals the superiority of ParameterEfficient
Fine-Tuning (PEFT) strategies, such as visual prompt tuning and visual adapter
tuning, over conventional techniques, including fully fine-tuning and linear
probing tuning in the few-shot OOD detection task. Recognizing some crucial
information from the pre-trained model, which is pivotal for OOD detection, may
be lost during the fine-tuning process, we propose a method termed
DomainSpecific and General Knowledge Fusion (DSGF). This approach is designed
to be compatible with diverse fine-tuning frameworks. Our experiments show that
the integration of DSGF significantly enhances the few-shot OOD detection
capabilities across various methods and fine-tuning methodologies, including
fully fine-tuning, visual adapter tuning, and visual prompt tuning. The code
will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jiuqing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yongbin Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Heng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1&quot;&gt;Jun Cen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yifan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sook Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Park Dong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12077">
<title>Efficient Model Agnostic Approach for Implicit Neural Representation Based Arbitrary-Scale Image Super-Resolution. (arXiv:2311.12077v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12077</link>
<description rdf:parseType="Literal">&lt;p&gt;Single image super-resolution (SISR) has experienced significant
advancements, primarily driven by deep convolutional networks. Traditional
networks, however, are limited to upscaling images to a fixed scale, leading to
the utilization of implicit neural functions for generating arbitrarily scaled
images. Nevertheless, these methodologies have imposed substantial
computational demands as they involve querying every target pixel to a single
resource-intensive decoder. In this paper, we introduce a novel and efficient
framework, the Mixture of Experts Implicit Super-Resolution (MoEISR), which
enables super-resolution at arbitrary scales with significantly increased
computational efficiency without sacrificing reconstruction quality. MoEISR
dynamically allocates the most suitable decoding expert to each pixel using a
lightweight mapper module, allowing experts with varying capacities to
reconstruct pixels across regions with diverse complexities. Our experiments
demonstrate that MoEISR successfully reduces up to 73% in floating point
operations (FLOPs) while delivering comparable or superior peak signal-to-noise
ratio (PSNR).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1&quot;&gt;Young Jae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jihun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tae Hyun Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12079">
<title>FreeKD: Knowledge Distillation via Semantic Frequency Prompt. (arXiv:2311.12079v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12079</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation (KD) has been applied to various tasks successfully,
and mainstream methods typically boost the student model via spatial imitation
losses. However, the consecutive downsamplings induced in the spatial domain of
teacher model is a type of corruption, hindering the student from analyzing
what specific information needs to be imitated, which results in accuracy
degradation. To better understand the underlying pattern of corrupted feature
maps, we shift our attention to the frequency domain. During frequency
distillation, we encounter a new challenge: the low-frequency bands convey
general but minimal context, while the high are more informative but also
introduce noise. Not each pixel within the frequency bands contributes equally
to the performance. To address the above problem: (1) We propose the Frequency
Prompt plugged into the teacher model, absorbing the semantic frequency context
during finetuning. (2) During the distillation period, a pixel-wise frequency
mask is generated via Frequency Prompt, to localize those pixel of interests
(PoIs) in various frequency bands. Additionally, we employ a position-aware
relational frequency loss for dense prediction tasks, delivering a high-order
spatial enhancement to the student model. We dub our Frequency Knowledge
Distillation method as FreeKD, which determines the optimal localization and
extent for the frequency distillation. Extensive experiments demonstrate that
FreeKD not only outperforms spatial-based distillation methods consistently on
dense prediction tasks (e.g., FreeKD brings 3.8 AP gains for RepPoints-R50 on
COCO2017 and 4.55 mIoU gains for PSPNet-R18 on Cityscapes), but also conveys
more robustness to the student. Notably, we also validate the generalization of
our approach on large-scale vision models (e.g., DINO and SAM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Tao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12083">
<title>PanBench: Towards High-Resolution and High-Performance Pansharpening. (arXiv:2311.12083v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12083</link>
<description rdf:parseType="Literal">&lt;p&gt;Pansharpening, a pivotal task in remote sensing, involves integrating
low-resolution multispectral images with high-resolution panchromatic images to
synthesize an image that is both high-resolution and retains multispectral
information. These pansharpened images enhance precision in land cover
classification, change detection, and environmental monitoring within remote
sensing data analysis. While deep learning techniques have shown significant
success in pansharpening, existing methods often face limitations in their
evaluation, focusing on restricted satellite data sources, single scene types,
and low-resolution images. This paper addresses this gap by introducing
PanBench, a high-resolution multi-scene dataset containing all mainstream
satellites and comprising 5,898 pairs of samples. Each pair includes a
four-channel (RGB + near-infrared) multispectral image of 256x256 pixels and a
mono-channel panchromatic image of 1,024x1,024 pixels. To achieve high-fidelity
synthesis, we propose a Cascaded Multiscale Fusion Network (CMFNet) for
Pansharpening. Extensive experiments validate the effectiveness of CMFNet. We
have released the dataset, source code, and pre-trained models in the
supplementary, fostering further research in remote sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xuechao Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_J/0/1/0/all/0/1&quot;&gt;Junliang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_P/0/1/0/all/0/1&quot;&gt;Pin Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12084">
<title>ODDR: Outlier Detection &amp; Dimension Reduction Based Defense Against Adversarial Patches. (arXiv:2311.12084v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.12084</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks are a major deterrent towards the reliable use of machine
learning models. A powerful type of adversarial attacks is the patch-based
attack, wherein the adversarial perturbations modify localized patches or
specific areas within the images to deceive the trained machine learning model.
In this paper, we introduce Outlier Detection and Dimension Reduction (ODDR), a
holistic defense mechanism designed to effectively mitigate patch-based
adversarial attacks. In our approach, we posit that input features
corresponding to adversarial patches, whether naturalistic or otherwise,
deviate from the inherent distribution of the remaining image sample and can be
identified as outliers or anomalies. ODDR employs a three-stage pipeline:
Fragmentation, Segregation, and Neutralization, providing a model-agnostic
solution applicable to both image classification and object detection tasks.
The Fragmentation stage parses the samples into chunks for the subsequent
Segregation process. Here, outlier detection techniques identify and segregate
the anomalous features associated with adversarial perturbations. The
Neutralization stage utilizes dimension reduction methods on the outliers to
mitigate the impact of adversarial perturbations without sacrificing pertinent
information necessary for the machine learning task. Extensive testing on
benchmark datasets and state-of-the-art adversarial patches demonstrates the
effectiveness of ODDR. Results indicate robust accuracies matching and lying
within a small range of clean accuracies (1%-3% for classification and 3%-5%
for object detection), with only a marginal compromise of 1%-2% in performance
on clean samples, thereby significantly outperforming other defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chattopadhyay_N/0/1/0/all/0/1&quot;&gt;Nandish Chattopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1&quot;&gt;Amira Guesmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdullah Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouni_B/0/1/0/all/0/1&quot;&gt;Bassem Ouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12085">
<title>Pyramid Diffusion for Fine 3D Large Scene Generation. (arXiv:2311.12085v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12085</link>
<description rdf:parseType="Literal">&lt;p&gt;Directly transferring the 2D techniques to 3D scene generation is challenging
due to significant resolution reduction and the scarcity of comprehensive
real-world 3D scene datasets. To address these issues, our work introduces the
Pyramid Discrete Diffusion model (PDD) for 3D scene generation. This novel
approach employs a multi-scale model capable of progressively generating
high-quality 3D scenes from coarse to fine. In this way, the PDD can generate
high-quality scenes within limited resource constraints and does not require
additional data sources. To the best of our knowledge, we are the first to
adopt the simple but effective coarse-to-fine strategy for 3D large scene
generation. Our experiments, covering both unconditional and conditional
generation, have yielded impressive results, showcasing the model&apos;s
effectiveness and robustness in generating realistic and detailed 3D scenes.
Our code will be available to the public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinke Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xueting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chongshou Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12088">
<title>PhytNet -- Tailored Convolutional Neural Networks for Custom Botanical Data. (arXiv:2311.12088v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12088</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated disease, weed and crop classification with computer vision will be
invaluable in the future of agriculture. However, existing model architectures
like ResNet, EfficientNet and ConvNeXt often underperform on smaller,
specialised datasets typical of such projects. We address this gap with
informed data collection and the development of a new CNN architecture,
PhytNet. Utilising a novel dataset of infrared cocoa tree images, we
demonstrate PhytNet&apos;s development and compare its performance with existing
architectures. Data collection was informed by analysis of spectroscopy data,
which provided useful insights into the spectral characteristics of cocoa
trees. Such information could inform future data collection and model
development. Cocoa was chosen as a focal species due to the diverse pathology
of its diseases, which pose significant challenges for detection. ResNet18
showed some signs of overfitting, while EfficientNet variants showed distinct
signs of overfitting. By contrast, PhytNet displayed excellent attention to
relevant features, no overfitting, and an exceptionally low computation cost
(1.19 GFLOPS). As such PhytNet is a promising candidate for rapid disease or
plant classification, or precise localisation of disease symptoms for
autonomous systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sykes_J/0/1/0/all/0/1&quot;&gt;Jamie R. Sykes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denby_K/0/1/0/all/0/1&quot;&gt;Katherine Denby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franks_D/0/1/0/all/0/1&quot;&gt;Daniel W. Franks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12090">
<title>FrePolad: Frequency-Rectified Point Latent Diffusion for Point Cloud Generation. (arXiv:2311.12090v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12090</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose FrePolad: frequency-rectified point latent diffusion, a point
cloud generation pipeline integrating a variational autoencoder (VAE) with a
denoising diffusion probabilistic model (DDPM) for the latent distribution.
FrePolad simultaneously achieves high quality, diversity, and flexibility in
point cloud cardinality for generation tasks while maintaining high
computational efficiency. The improvement in generation quality and diversity
is achieved through (1) a novel frequency rectification module via spherical
harmonics designed to retain high-frequency content while learning the point
cloud distribution; and (2) a latent DDPM to learn the regularized yet complex
latent distribution. In addition, FrePolad supports variable point cloud
cardinality by formulating the sampling of points as conditional distributions
over a latent shape distribution. Finally, the low-dimensional latent space
encoded by the VAE contributes to FrePolad&apos;s fast and scalable sampling. Our
quantitative and qualitative results demonstrate the state-of-the-art
performance of FrePolad in terms of quality, diversity, and computational
efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chenliang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1&quot;&gt;Fangcheng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanji_P/0/1/0/all/0/1&quot;&gt;Param Hanji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhilin Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fogarty_K/0/1/0/all/0/1&quot;&gt;Kyle Fogarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sztrajman_A/0/1/0/all/0/1&quot;&gt;Alejandro Sztrajman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hongyun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1&quot;&gt;Cengiz Oztireli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12091">
<title>DAS: A Deformable Attention to Capture Salient Information in CNNs. (arXiv:2311.12091v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12091</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) excel in local spatial pattern
recognition. For many vision tasks, such as object recognition and
segmentation, salient information is also present outside CNN&apos;s kernel
boundaries. However, CNNs struggle in capturing such relevant information due
to their confined receptive fields. Self-attention can improve a model&apos;s access
to global information but increases computational overhead. We present a fast
and simple fully convolutional method called DAS that helps focus attention on
relevant information. It uses deformable convolutions for the location of
pertinent image regions and separable convolutions for efficiency. DAS plugs
into existing CNNs and propagates relevant information using a gating
mechanism. Compared to the O(n^2) computational complexity of transformer-style
attention, DAS is O(n). Our claim is that DAS&apos;s ability to pay increased
attention to relevant features results in performance improvements when added
to popular CNNs for Image Classification and Object Detection. For example, DAS
yields an improvement on Stanford Dogs (4.47%), ImageNet (1.91%), and COCO AP
(3.3%) with base ResNet50 backbone. This outperforms other CNN attention
mechanisms while using similar or less FLOPs. Our code will be publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salajegheh_F/0/1/0/all/0/1&quot;&gt;Farzad Salajegheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadi_N/0/1/0/all/0/1&quot;&gt;Nader Asadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saryazdi_S/0/1/0/all/0/1&quot;&gt;Soroush Saryazdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mudur_S/0/1/0/all/0/1&quot;&gt;Sudhir Mudur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12092">
<title>Concept Sliders: LoRA Adaptors for Precise Control in Diffusion Models. (arXiv:2311.12092v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12092</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method to create interpretable concept sliders that enable
precise control over attributes in image generations from diffusion models. Our
approach identifies a low-rank parameter direction corresponding to one concept
while minimizing interference with other attributes. A slider is created using
a small set of prompts or sample images; thus slider directions can be created
for either textual or visual concepts. Concept Sliders are plug-and-play: they
can be composed efficiently and continuously modulated, enabling precise
control over image generation. In quantitative experiments comparing to
previous editing techniques, our sliders exhibit stronger targeted edits with
lower interference. We showcase sliders for weather, age, styles, and
expressions, as well as slider compositions. We show how sliders can transfer
latents from StyleGAN for intuitive editing of visual concepts for which
textual description is difficult. We also find that our method can help address
persistent quality issues in Stable Diffusion XL including repair of object
deformations and fixing distorted hands. Our code, data, and trained sliders
are available at https://sliders.baulab.info/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandikota_R/0/1/0/all/0/1&quot;&gt;Rohit Gandikota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1&quot;&gt;Joanna Materzynska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tingrui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12125">
<title>Mixing-Denoising Generalizable Occupancy Networks. (arXiv:2311.12125v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12125</link>
<description rdf:parseType="Literal">&lt;p&gt;While current state-of-the-art generalizable implicit neural shape models
rely on the inductive bias of convolutions, it is still not entirely clear how
properties emerging from such biases are compatible with the task of 3D
reconstruction from point cloud. We explore an alternative approach to
generalizability in this context. We relax the intrinsic model bias (i.e. using
MLPs to encode local features as opposed to convolutions) and constrain the
hypothesis space instead with an auxiliary regularization related to the
reconstruction task, i.e. denoising. The resulting model is the first only-MLP
locally conditioned implicit shape reconstruction from point cloud network with
fast feed forward inference. Point cloud borne features and denoising offsets
are predicted from an exclusively MLP-made network in a single forward pass. A
decoder predicts occupancy probabilities for queries anywhere in space by
pooling nearby features from the point cloud order-invariantly, guided by
denoised relative positional encoding. We outperform the state-of-the-art
convolutional method while using half the number of model parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouasfi_A/0/1/0/all/0/1&quot;&gt;Amine Ouasfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boukhayma_A/0/1/0/all/0/1&quot;&gt;Adnane Boukhayma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12128">
<title>Fingerspelling PoseNet: Enhancing Fingerspelling Translation with Pose-Based Transformer Models. (arXiv:2311.12128v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12128</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the task of American Sign Language fingerspelling translation
using videos in the wild. We exploit advances in more accurate hand pose
estimation and propose a novel architecture that leverages the transformer
based encoder-decoder model enabling seamless contextual word translation. The
translation model is augmented by a novel loss term that accurately predicts
the length of the finger-spelled word, benefiting both training and inference.
We also propose a novel two-stage inference approach that re-ranks the
hypotheses using the language model capabilities of the decoder. Through
extensive experiments, we demonstrate that our proposed method outperforms the
state-of-the-art models on ChicagoFSWild and ChicagoFSWild+ achieving more than
10% relative improvement in performance. Our findings highlight the
effectiveness of our approach and its potential to advance fingerspelling
recognition in sign language translation. Code is also available at
https://github.com/pooyafayyaz/Fingerspelling-PoseNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayyazsanavi_P/0/1/0/all/0/1&quot;&gt;Pooya Fayyazsanavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nejatishahidin_N/0/1/0/all/0/1&quot;&gt;Negar Nejatishahidin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosecka_J/0/1/0/all/0/1&quot;&gt;Jana Kosecka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12144">
<title>Applications of Large Scale Foundation Models for Autonomous Driving. (arXiv:2311.12144v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12144</link>
<description rdf:parseType="Literal">&lt;p&gt;Since DARPA Grand Challenges (rural) in 2004/05 and Urban Challenges in 2007,
autonomous driving has been the most active field of AI applications. Recently
powered by large language models (LLMs), chat systems, such as chatGPT and
PaLM, emerge and rapidly become a promising direction to achieve artificial
general intelligence (AGI) in natural language processing (NLP). There comes a
natural thinking that we could employ these abilities to reformulate autonomous
driving. By combining LLM with foundation models, it is possible to utilize the
human knowledge, commonsense and reasoning to rebuild autonomous driving
systems from the current long-tailed AI dilemma. In this paper, we investigate
the techniques of foundation models and LLMs applied for autonomous driving,
categorized as simulation, world model, data annotation and planning or E2E
solutions etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yue Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12151">
<title>Teaching Robots to Build Simulations of Themselves. (arXiv:2311.12151v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12151</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation enables robots to plan and estimate the outcomes of prospective
actions without the need to physically execute them. We introduce a
self-supervised learning framework to enable robots model and predict their
morphology, kinematics and motor control using only brief raw video data,
eliminating the need for extensive real-world data collection and kinematic
priors. By observing their own movements, akin to humans watching their
reflection in a mirror, robots learn an ability to simulate themselves and
predict their spatial motion for various tasks. Our results demonstrate that
this self-learned simulation not only enables accurate motion planning but also
allows the robot to detect abnormalities and recover from damage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yuhang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jiong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipson_H/0/1/0/all/0/1&quot;&gt;Hod Lipson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12153">
<title>Uncertainty Estimation in Contrast-Enhanced MR Image Translation with Multi-Axis Fusion. (arXiv:2311.12153v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12153</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep learning has been applied to a wide range of medical
imaging and image processing tasks. In this work, we focus on the estimation of
epistemic uncertainty for 3D medical image-to-image translation. We propose a
novel model uncertainty quantification method, Multi-Axis Fusion (MAF), which
relies on the integration of complementary information derived from multiple
views on volumetric image data. The proposed approach is applied to the task of
synthesizing contrast enhanced T1-weighted images based on native T1, T2 and
T2-FLAIR scans. The quantitative findings indicate a strong correlation
($\rho_{\text healthy} = 0.89$) between the mean absolute image synthetization
error and the mean uncertainty score for our MAF method. Hence, we consider MAF
as a promising approach to solve the highly relevant task of detecting
synthetization failures at inference time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baltruschat_I/0/1/0/all/0/1&quot;&gt;Ivo M. Baltruschat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janbakhshi_P/0/1/0/all/0/1&quot;&gt;Parvaneh Janbakhshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dohmen_M/0/1/0/all/0/1&quot;&gt;Melanie Dohmen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lenga_M/0/1/0/all/0/1&quot;&gt;Matthias Lenga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12157">
<title>Model-aware 3D Eye Gaze from Weak and Few-shot Supervisions. (arXiv:2311.12157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12157</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of predicting 3D eye gaze from eye images can be performed either by
(a) end-to-end learning for image-to-gaze mapping or by (b) fitting a 3D eye
model onto images. The former case requires 3D gaze labels, while the latter
requires eye semantics or landmarks to facilitate the model fitting. Although
obtaining eye semantics and landmarks is relatively easy, fitting an accurate
3D eye model on them remains to be very challenging due to its ill-posed nature
in general. On the other hand, obtaining large-scale 3D gaze data is cumbersome
due to the required hardware setups and computational demands. In this work, we
propose to predict 3D eye gaze from weak supervision of eye semantic
segmentation masks and direct supervision of a few 3D gaze vectors. The
proposed method combines the best of both worlds by leveraging large amounts of
weak annotations--which are easy to obtain, and only a few 3D gaze
vectors--which alleviate the difficulty of fitting 3D eye models on the
semantic segmentation of eye images. Thus, the eye gaze vectors, used in the
model fitting, are directly supervised using the few-shot gaze labels.
Additionally, we propose a transformer-based network architecture, that serves
as a solid baseline for our improvements. Our experiments in diverse settings
illustrate the significant benefits of the proposed method, achieving about 5
degrees lower angular gaze error over the baseline, when only 0.05% 3D
annotations of the training images are used. The source code is available at
https://github.com/dimitris-christodoulou57/Model-aware_3D_Eye_Gaze.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovic_N/0/1/0/all/0/1&quot;&gt;Nikola Popovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christodoulou_D/0/1/0/all/0/1&quot;&gt;Dimitrios Christodoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paudel_D/0/1/0/all/0/1&quot;&gt;Danda Pani Paudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gool_L/0/1/0/all/0/1&quot;&gt;Luc Van Gool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12159">
<title>Conditional Modeling Based Automatic Video Summarization. (arXiv:2311.12159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12159</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of video summarization is to shorten videos automatically while
retaining the key information necessary to convey the overall story. Video
summarization methods mainly rely on visual factors, such as visual
consecutiveness and diversity, which may not be sufficient to fully understand
the content of the video. There are other non-visual factors, such as
interestingness, representativeness, and storyline consistency that should also
be considered for generating high-quality video summaries. Current methods do
not adequately take into account these non-visual factors, resulting in
suboptimal performance. In this work, a new approach to video summarization is
proposed based on insights gained from how humans create ground truth video
summaries. The method utilizes a conditional modeling perspective and
introduces multiple meaningful random variables and joint distributions to
characterize the key components of video summarization. Helper distributions
are employed to improve the training of the model. A conditional attention
module is designed to mitigate potential performance degradation in the
presence of multi-modal input. The proposed video summarization method
incorporates the above innovative design choices that aim to narrow the gap
between human-generated and machine-generated video summaries. Extensive
experiments show that the proposed approach outperforms existing methods and
achieves state-of-the-art performance on commonly used video summarization
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia-Hong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Min-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worring_M/0/1/0/all/0/1&quot;&gt;Marcel Worring&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12161">
<title>ChemScraper: Graphics Extraction, Molecular Diagram Parsing, and Annotated Data Generation for PDF Images. (arXiv:2311.12161v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12161</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing visual parsers for molecule diagrams translate pixel-based raster
images such as PNGs to chemical structure representations (e.g., SMILES).
However, PDFs created by word processors including \LaTeX{} and Word provide
explicit locations and shapes for characters, lines, and polygons. We
%introduce a method to extract symbols from born-digital PDF molecule images
and then apply simple graph transformations to capture both visual and chemical
structure in editable ChemDraw files (CDXML). Our fast ( PDF $\rightarrow$
visual graph $\rightarrow$ chemical graph ) pipeline does not require GPUs,
Optical Character Recognition (OCR) or vectorization. We evaluate on standard
benchmarks using SMILES strings, along with a novel evaluation that provides
graph-based metrics and error compilation using LgEval. The geometric
information in born-digital PDFs produces a highly accurate parser, motivating
generating training data for visual parsers that recognize from raster images,
with extracted graphics, visual structure, and chemical structure as
annotations. To do this we render SMILES strings in Indigo, parse molecule
structure, and then validate recognized structure to select correct files.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Ayush Kumar Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amador_B/0/1/0/all/0/1&quot;&gt;Bryan Manrique Amador&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_A/0/1/0/all/0/1&quot;&gt;Abhisek Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creekmore_M/0/1/0/all/0/1&quot;&gt;Ming Creekmore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ocampo_B/0/1/0/all/0/1&quot;&gt;Blake Ocampo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denmark_S/0/1/0/all/0/1&quot;&gt;Scott Denmark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanibbi_R/0/1/0/all/0/1&quot;&gt;Richard Zanibbi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12174">
<title>LABELMAKER: Automatic Semantic Label Generation from RGB-D Trajectories. (arXiv:2311.12174v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12174</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic annotations are indispensable to train or evaluate perception
models, yet very costly to acquire. This work introduces a fully automated
2D/3D labeling framework that, without any human intervention, can generate
labels for RGB-D scans at equal (or better) level of accuracy than comparable
manually annotated datasets such as ScanNet. Our approach is based on an
ensemble of state-of-the-art segmentation models and 3D lifting through neural
rendering. We demonstrate the effectiveness of our LabelMaker pipeline by
generating significantly better labels for the ScanNet datasets and
automatically labelling the previously unlabeled ARKitScenes dataset. Code and
models are available at https://labelmaker.org
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weder_S/0/1/0/all/0/1&quot;&gt;Silvan Weder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blum_H/0/1/0/all/0/1&quot;&gt;Hermann Blum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engelmann_F/0/1/0/all/0/1&quot;&gt;Francis Engelmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12193">
<title>Disentangling Structure and Appearance in ViT Feature Space. (arXiv:2311.12193v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12193</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for semantically transferring the visual appearance of
one natural image to another. Specifically, our goal is to generate an image in
which objects in a source structure image are &quot;painted&quot; with the visual
appearance of their semantically related objects in a target appearance image.
To integrate semantic information into our framework, our key idea is to
leverage a pre-trained and fixed Vision Transformer (ViT) model. Specifically,
we derive novel disentangled representations of structure and appearance
extracted from deep ViT features. We then establish an objective function that
splices the desired structure and appearance representations, interweaving them
together in the space of ViT features. Based on our objective function, we
propose two frameworks of semantic appearance transfer -- &quot;Splice&quot;, which works
by training a generator on a single and arbitrary pair of structure-appearance
images, and &quot;SpliceNet&quot;, a feed-forward real-time appearance transfer model
trained on a dataset of images from a specific domain. Our frameworks do not
involve adversarial training, nor do they require any additional input
information such as semantic segmentation or correspondences. We demonstrate
high-resolution results on a variety of in-the-wild image pairs, under
significant variations in the number of objects, pose, and appearance. Code and
supplementary material are available in our project page: splice-vit.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tumanyan_N/0/1/0/all/0/1&quot;&gt;Narek Tumanyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bar_Tal_O/0/1/0/all/0/1&quot;&gt;Omer Bar-Tal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1&quot;&gt;Shir Amir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagon_S/0/1/0/all/0/1&quot;&gt;Shai Bagon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dekel_T/0/1/0/all/0/1&quot;&gt;Tali Dekel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12194">
<title>DiffAvatar: Simulation-Ready Garment Optimization with Differentiable Simulation. (arXiv:2311.12194v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12194</link>
<description rdf:parseType="Literal">&lt;p&gt;The realism of digital avatars is crucial in enabling telepresence
applications with self-expression and customization. A key aspect of this
realism originates from the physical accuracy of both a true-to-life body shape
and clothing. While physical simulations can produce high-quality, realistic
motions for clothed humans, they require precise estimation of body shape and
high-quality garment assets with associated physical parameters for cloth
simulations. However, manually creating these assets and calibrating their
parameters is labor-intensive and requires specialized expertise. To address
this gap, we propose DiffAvatar, a novel approach that performs body and
garment co-optimization using differentiable simulation. By integrating
physical simulation into the optimization loop and accounting for the complex
nonlinear behavior of cloth and its intricate interaction with the body, our
framework recovers body and garment geometry and extracts important material
parameters in a physically plausible way. Our experiments demonstrate that our
approach generates realistic clothing and body shape that can be easily used in
downstream applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hsiao-yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larionov_E/0/1/0/all/0/1&quot;&gt;Egor Larionov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarafianos_N/0/1/0/all/0/1&quot;&gt;Nikolaos Sarafianos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matusik_W/0/1/0/all/0/1&quot;&gt;Wojciech Matusik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stuyck_T/0/1/0/all/0/1&quot;&gt;Tuur Stuyck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12198">
<title>PhysGaussian: Physics-Integrated 3D Gaussians for Generative Dynamics. (arXiv:2311.12198v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2311.12198</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce PhysGaussian, a new method that seamlessly integrates physically
grounded Newtonian dynamics within 3D Gaussians to achieve high-quality novel
motion synthesis. Employing a custom Material Point Method (MPM), our approach
enriches 3D Gaussian kernels with physically meaningful kinematic deformation
and mechanical stress attributes, all evolved in line with continuum mechanics
principles. A defining characteristic of our method is the seamless integration
between physical simulation and visual rendering: both components utilize the
same 3D Gaussian kernels as their discrete representations. This negates the
necessity for triangle/tetrahedron meshing, marching cubes, &quot;cage meshes,&quot; or
any other geometry embedding, highlighting the principle of &quot;what you see is
what you simulate (WS$^2$).&quot; Our method demonstrates exceptional versatility
across a wide variety of materials--including elastic entities, metals,
non-Newtonian fluids, and granular materials--showcasing its strong
capabilities in creating diverse visual content with novel viewpoints and
movements. Our project page is at: https://xpandora.github.io/PhysGaussian/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tianyi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_Z/0/1/0/all/0/1&quot;&gt;Zeshun Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yuxin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yutao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chenfanfu Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12202">
<title>Nepotistically Trained Generative-AI Models Collapse. (arXiv:2311.12202v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.12202</link>
<description rdf:parseType="Literal">&lt;p&gt;Trained on massive amounts of human-generated content, AI (artificial
intelligence) image synthesis is capable of reproducing semantically coherent
images that match the visual appearance of its training data. We show that when
retrained on even small amounts of their own creation, these generative-AI
models produce highly distorted images. We also show that this distortion
extends beyond the text prompts used in retraining, and that once poisoned, the
models struggle to fully heal even after retraining on only real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohacek_M/0/1/0/all/0/1&quot;&gt;Matyas Bohacek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1&quot;&gt;Hany Farid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12225">
<title>HandSight: DeCAF &amp; Improved Fisher Vectors to Classify Clothing Color and Texture with a Finger-Mounted Camera. (arXiv:2311.12225v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12225</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate the use of DeCAF and Improved Fisher Vector image features to
classify clothing texture. The issue of choosing clothes is a problem for the
blind every day. This work attempts to solve the issue with a finger-mounted
camera and state-of-the-art classification algorithms. To evaluate our
solution, we collected 520 close-up images across 29 pieces of clothing. We
contribute (1) the HCTD, an image dataset taken with a NanEyeGS camera, a
camera small enough to be mounted on the finger, and (2) evaluations of
state-of-the-art recognition algorithms applied to our dataset - achieving an
accuracy &amp;gt;95%. Throughout the paper, we will discuss previous work, evaluate
the current work, and finally, suggest the project&apos;s future direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medeiros_A/0/1/0/all/0/1&quot;&gt;Alexander J. Medeiros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stearns_L/0/1/0/all/0/1&quot;&gt;Lee Stearns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Froehlich_J/0/1/0/all/0/1&quot;&gt;Jon E. Froehlich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12265">
<title>Virtual Home Staging: Inverse Rendering and Editing an Indoor Panorama under Natural Illumination. (arXiv:2311.12265v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12265</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel inverse rendering method that enables the transformation
of existing indoor panoramas with new indoor furniture layouts under natural
illumination. To achieve this, we captured indoor HDR panoramas along with
real-time outdoor hemispherical HDR photographs. Indoor and outdoor HDR images
were linearly calibrated with measured absolute luminance values for accurate
scene relighting. Our method consists of three key components: (1) panoramic
furniture detection and removal, (2) automatic floor layout design, and (3)
global rendering with scene geometry, new furniture objects, and a real-time
outdoor photograph. We demonstrate the effectiveness of our workflow in
rendering indoor scenes under different outdoor illumination conditions.
Additionally, we contribute a new calibrated HDR (Cali-HDR) dataset that
consists of 137 calibrated indoor panoramas and their associated outdoor
photographs. The source code and dataset are available:
https://github.com/Gzhji/Cali-HDR-Dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_G/0/1/0/all/0/1&quot;&gt;Guanzhou Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawyer_A/0/1/0/all/0/1&quot;&gt;Azadeh O. Sawyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_S/0/1/0/all/0/1&quot;&gt;Srinivasa G. Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12268">
<title>Boosting Audio-visual Zero-shot Learning with Large Language Models. (arXiv:2311.12268v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12268</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-visual zero-shot learning aims to recognize unseen categories based on
paired audio-visual sequences. Recent methods mainly focus on learning aligned
and discriminative multi-modal features to boost generalization towards unseen
categories. However, these approaches ignore the obscure action concepts in
category names and may inevitably introduce complex network structures with
difficult training objectives. In this paper, we propose a simple yet effective
framework named Knowledge-aware Distribution Adaptation (KDA) to help the model
better grasp the novel action contents with an external knowledge base.
Specifically, we first propose using large language models to generate rich
descriptions from category names, which leads to a better understanding of
unseen categories. Additionally, we propose a distribution alignment loss as
well as a knowledge-aware adaptive margin loss to further improve the
generalization ability towards unseen categories. Extensive experimental
results demonstrate that our proposed KDA can outperform state-of-the-art
methods on three popular audio-visual zero-shot learning datasets. Our code
will be avaliable at \url{https://github.com/chenhaoxing/KDA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoxing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaohui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Yan Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zizheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuoer Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Z/0/1/0/all/0/1&quot;&gt;Zhangxuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Jun Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huijia Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiqiang Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12272">
<title>Procedural Generation of Grain Orientations using the Wave Function Collapse Algorithm. (arXiv:2311.12272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12272</link>
<description rdf:parseType="Literal">&lt;p&gt;Statistics of grain sizes and orientations in metals correlate to the
material&apos;s mechanical properties. Reproducing representative volume elements
for further analysis of deformation and failure in metals, like 316L stainless
steel, is particularly important due to their wide use in manufacturing goods
today. Two approaches, initially created for video games, were considered for
the procedural generation of representative grain microstructures. The first is
the Wave Function Collapse (WFC) algorithm, and the second is constraint
propagation and probabilistic inference through Markov Junior, a free and
open-source software. This study aimed to investigate these two algorithms&apos;
effectiveness in using reference electron backscatter diffraction (EBSD) maps
and recreating a statistically similar one that could be used in further
research. It utilized two stainless steel EBSD maps as references to test both
algorithms. First, the WFC algorithm was too constricting and, thus, incapable
of producing images that resembled EBSDs. The second, MarkovJunior, was much
more effective in creating a Voronoi tessellation that could be used to create
an EBSD map in Python. When comparing the results between the reference and the
generated EBSD, we discovered that the orientation and volume fractions were
extremely similar. With the study, it was concluded that MarkovJunior is an
effective machine learning tool that can reproduce representative grain
microstructures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magny_Fokam_G/0/1/0/all/0/1&quot;&gt;G. Magny-Fokam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madisetti_D/0/1/0/all/0/1&quot;&gt;D. Madisetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Awady_J/0/1/0/all/0/1&quot;&gt;J. El-Awady&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12291">
<title>Instance-aware 3D Semantic Segmentation powered by Shape Generators and Classifiers. (arXiv:2311.12291v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12291</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing 3D semantic segmentation methods rely on point-wise or voxel-wise
feature descriptors to output segmentation predictions. However, these
descriptors are often supervised at point or voxel level, leading to
segmentation models that can behave poorly at instance-level. In this paper, we
proposed a novel instance-aware approach for 3D semantic segmentation. Our
method combines several geometry processing tasks supervised at instance-level
to promote the consistency of the learned feature representation. Specifically,
our methods use shape generators and shape classifiers to perform shape
reconstruction and classification tasks for each shape instance. This enforces
the feature representation to faithfully encode both structural and local shape
information, with an awareness of shape instances. In the experiments, our
method significantly outperform existing approaches in 3D semantic segmentation
on several public benchmarks, such as Waymo Open Dataset, SemanticKITTI and
ScanNetV2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qixing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiangru Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12300">
<title>Challenges in Video-Based Infant Action Recognition: A Critical Examination of the State of the Art. (arXiv:2311.12300v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12300</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated human action recognition, a burgeoning field within computer
vision, boasts diverse applications spanning surveillance, security,
human-computer interaction, tele-health, and sports analysis. Precise action
recognition in infants serves a multitude of pivotal purposes, encompassing
safety monitoring, developmental milestone tracking, early intervention for
developmental delays, fostering parent-infant bonds, advancing computer-aided
diagnostics, and contributing to the scientific comprehension of child
development. This paper delves into the intricacies of infant action
recognition, a domain that has remained relatively uncharted despite the
accomplishments in adult action recognition. In this study, we introduce a
groundbreaking dataset called ``InfActPrimitive&apos;&apos;, encompassing five
significant infant milestone action categories, and we incorporate specialized
preprocessing for infant data. We conducted an extensive comparative analysis
employing cutting-edge skeleton-based action recognition models using this
dataset. Our findings reveal that, although the PoseC3D model achieves the
highest accuracy at approximately 71%, the remaining models struggle to
accurately capture the dynamics of infant actions. This highlights a
substantial knowledge gap between infant and adult action recognition domains
and the urgent need for data-efficient pipeline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatamimajoumerd_E/0/1/0/all/0/1&quot;&gt;Elaheh Hatamimajoumerd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakhaki_P/0/1/0/all/0/1&quot;&gt;Pooria Daneshvar Kakhaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiaofei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_L/0/1/0/all/0/1&quot;&gt;Lingfei Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amraee_S/0/1/0/all/0/1&quot;&gt;Somaieh Amraee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostadabbas_S/0/1/0/all/0/1&quot;&gt;Sarah Ostadabbas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12311">
<title>ABFL: Angular Boundary Discontinuity Free Loss for Arbitrary Oriented Object Detection in Aerial Images. (arXiv:2311.12311v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12311</link>
<description rdf:parseType="Literal">&lt;p&gt;Arbitrary oriented object detection (AOOD) in aerial images is a widely
concerned and highly challenging task, and plays an important role in many
scenarios. The core of AOOD involves the representation, encoding, and feature
augmentation of oriented bounding-boxes (Bboxes). Existing methods lack
intuitive modeling of angle difference measurement in oriented Bbox
representations. Oriented Bboxes under different representations exhibit
rotational symmetry with varying periods due to angle periodicity. The angular
boundary discontinuity (ABD) problem at periodic boundary positions is caused
by rotational symmetry in measuring angular differences. In addition, existing
methods also use additional encoding-decoding structures for oriented Bboxes.
In this paper, we design an angular boundary free loss (ABFL) based on the von
Mises distribution. The ABFL aims to solve the ABD problem when detecting
oriented objects. Specifically, ABFL proposes to treat angles as circular data
rather than linear data when measuring angle differences, aiming to introduce
angle periodicity to alleviate the ABD problem and improve the accuracy of
angle difference measurement. In addition, ABFL provides a simple and effective
solution for various periodic boundary discontinuities caused by rotational
symmetry in AOOD tasks, as it does not require additional encoding-decoding
structures for oriented Bboxes. Extensive experiments on the DOTA and HRSC2016
datasets show that the proposed ABFL loss outperforms some state-of-the-art
methods focused on addressing the ABD problem.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zifei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shengyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12316">
<title>Overcoming Pathology Image Data Deficiency: Generating Images from Pathological Transformation Process. (arXiv:2311.12316v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12316</link>
<description rdf:parseType="Literal">&lt;p&gt;Histopathology serves as the gold standard for medical diagnosis but faces
application limitations due to the shortage of medical resources. Leveraging
deep learning, computer-aided diagnosis has the potential to alleviate the
pathologist scarcity and provide timely clinical analysis. However, developing
a reliable model generally necessitates substantial data for training, which is
challenging in pathological field. In response, we propose an adaptive
depth-controlled bidirectional diffusion (ADBD) network for image data
generation. The domain migration approach can work with small trainset and
overcome the diffusion overfitting by source information guidance.
Specifically, we developed a hybrid attention strategy to blend global and
local attention priorities, which guides the bidirectional diffusion and
ensures the migration success. In addition, we developed the adaptive
depth-controlled strategy to simulate physiological transformations, capable of
yielding unlimited cross-domain intermediate images with corresponding soft
labels. ADBD is effective for overcoming pathological image data deficiency and
supportable for further pathology-related research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zeyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yufang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yunlu Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guanglei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12327">
<title>ViLaM: A Vision-Language Model with Enhanced Visual Grounding and Generalization Capability. (arXiv:2311.12327v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12327</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language models have revolutionized human-computer interaction and
shown significant progress in multi-modal tasks. However, applying these models
to complex visual tasks like medical image analysis remains challenging. In
this study, we propose ViLaM, a unified Vision-Language transformer model that
integrates instruction tuning predicated on a large language model. This
approach enables us to optimally utilize the knowledge and reasoning capacities
of large pre-trained language models for an array of tasks encompassing both
language and vision. We employ frozen pre-trained encoders to encode and align
both image and text features, enabling ViLaM to handle a variety of visual
tasks following textual instructions. Besides, we&apos;ve designed cycle training
for referring expressions to address the need for high-quality, paired
referring expression datasets for training large models in terms of both
quantity and quality. We evaluated ViLaM&apos;s exceptional performance on public
general datasets and further confirmed its generalizability on medical
datasets. Importantly, we&apos;ve observed the model&apos;s impressive zero-shot learning
ability, indicating the potential future application of ViLaM in the medical
field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lijian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12342">
<title>LoCo: Locally Constrained Training-Free Layout-to-Image Synthesis. (arXiv:2311.12342v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12342</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent text-to-image diffusion models have reached an unprecedented level in
generating high-quality images. However, their exclusive reliance on textual
prompts often falls short in accurately conveying fine-grained spatial
compositions. In this paper, we propose LoCo, a training-free approach for
layout-to-image synthesis that excels in producing high-quality images aligned
with both textual prompts and spatial layouts. Our method introduces a
Localized Attention Constraint to refine cross-attention for individual
objects, ensuring their precise placement in designated regions. We further
propose a Padding Token Constraint to leverage the semantic information
embedded in previously neglected padding tokens, thereby preventing the
undesired fusion of synthesized objects. LoCo seamlessly integrates into
existing text-to-image and layout-to-image models, significantly amplifying
their performance and effectively addressing semantic failures observed in
prior methods. Through extensive experiments, we showcase the superiority of
our approach, surpassing existing state-of-the-art training-free
layout-to-image methods both qualitatively and quantitatively across multiple
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_P/0/1/0/all/0/1&quot;&gt;Peiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Ruiyang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12344">
<title>Modality Mixer Exploiting Complementary Information for Multi-modal Action Recognition. (arXiv:2311.12344v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12344</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the distinctive characteristics of sensors, each modality exhibits
unique physical properties. For this reason, in the context of multi-modal
action recognition, it is important to consider not only the overall action
content but also the complementary nature of different modalities. In this
paper, we propose a novel network, named Modality Mixer (M-Mixer) network,
which effectively leverages and incorporates the complementary information
across modalities with the temporal context of actions for action recognition.
A key component of our proposed M-Mixer is the Multi-modal Contextualization
Unit (MCU), a simple yet effective recurrent unit. Our MCU is responsible for
temporally encoding a sequence of one modality (e.g., RGB) with action content
features of other modalities (e.g., depth and infrared modalities). This
process encourages M-Mixer network to exploit global action content and also to
supplement complementary information of other modalities. Furthermore, to
extract appropriate complementary information regarding to the given modality
settings, we introduce a new module, named Complementary Feature Extraction
Module (CFEM). CFEM incorporates sepearte learnable query embeddings for each
modality, which guide CFEM to extract complementary information and global
action content from the other modalities. As a result, our proposed method
outperforms state-of-the-art methods on NTU RGB+D 60, NTU RGB+D 120, and
NW-UCLA datasets. Moreover, through comprehensive ablation studies, we further
validate the effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sumin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Sangmin Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nugroho_M/0/1/0/all/0/1&quot;&gt;Muhammad Adi Nugroho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Changick Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12345">
<title>Stable Diffusion For Aerial Object Detection. (arXiv:2311.12345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12345</link>
<description rdf:parseType="Literal">&lt;p&gt;Aerial object detection is a challenging task, in which one major obstacle
lies in the limitations of large-scale data collection and the long-tail
distribution of certain classes. Synthetic data offers a promising solution,
especially with recent advances in diffusion-based methods like stable
diffusion (SD). However, the direct application of diffusion methods to aerial
domains poses unique challenges: stable diffusion&apos;s optimization for rich
ground-level semantics doesn&apos;t align with the sparse nature of aerial objects,
and the extraction of post-synthesis object coordinates remains problematic. To
address these challenges, we introduce a synthetic data augmentation framework
tailored for aerial images. It encompasses sparse-to-dense region of interest
(ROI) extraction to bridge the semantic gap, fine-tuning the diffusion model
with low-rank adaptation (LORA) to circumvent exhaustive retraining, and
finally, a Copy-Paste method to compose synthesized objects with backgrounds,
providing a nuanced approach to aerial object detection through synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_Y/0/1/0/all/0/1&quot;&gt;Yanan Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fuxun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Simranjit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stamoulis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Stamoulis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12359">
<title>Post-Training Quantization with Low-precision Minifloats and Integers on FPGAs. (arXiv:2311.12359v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12359</link>
<description rdf:parseType="Literal">&lt;p&gt;Post-Training Quantization (PTQ) is a powerful technique for model
compression, reducing the precision of neural networks without additional
training overhead. Recent works have investigated adopting 8-bit floating-point
quantization (FP8) in the context of PTQ for model inference. However, the
exploration of floating-point formats smaller than 8 bits and their comparison
with integer quantization remains relatively limited. In this work, we present
minifloats, which are reduced-precision floating-point formats capable of
further reducing the memory footprint, latency, and energy cost of a model
while approaching full-precision model accuracy. Our work presents a novel PTQ
design-space exploration, comparing minifloat and integer quantization schemes
across a range of 3 to 8 bits for both weights and activations. We examine the
applicability of various PTQ techniques to minifloats, including weight
equalization, bias correction, SmoothQuant, gradient-based learned rounding,
and the GPTQ method. Our experiments validate the effectiveness of
low-precision minifloats when compared to their integer counterparts across a
spectrum of accuracy-precision trade-offs on a set of reference deep learning
vision workloads. Finally, we evaluate our results against an FPGA-based
hardware cost model, showing that integer quantization often remains the
Pareto-optimal option, given its relatively smaller hardware resource
footprint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_S/0/1/0/all/0/1&quot;&gt;Shivam Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappalardo_A/0/1/0/all/0/1&quot;&gt;Alessandro Pappalardo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damsgaard_H/0/1/0/all/0/1&quot;&gt;Hans Jakob Damsgaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franco_G/0/1/0/all/0/1&quot;&gt;Giuseppe Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preusser_T/0/1/0/all/0/1&quot;&gt;Thomas B. Preu&amp;#xdf;er&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blott_M/0/1/0/all/0/1&quot;&gt;Michaela Blott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_T/0/1/0/all/0/1&quot;&gt;Tulika Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12364">
<title>Semi-supervised Medical Image Segmentation via Query Distribution Consistency. (arXiv:2311.12364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12364</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning is increasingly popular in medical image
segmentation due to its ability to leverage large amounts of unlabeled data to
extract additional information. However, most existing semi-supervised
segmentation methods focus only on extracting information from unlabeled data.
In this paper, we propose a novel Dual KMax UX-Net framework that leverages
labeled data to guide the extraction of information from unlabeled data. Our
approach is based on a mutual learning strategy that incorporates two modules:
3D UX-Net as our backbone meta-architecture and KMax decoder to enhance the
segmentation performance. Extensive experiments on the Atrial Segmentation
Challenge dataset have shown that our method can significantly improve
performance by merging unlabeled data. Meanwhile, our framework outperforms
state-of-the-art semi-supervised learning methods on 10\% and 20\% labeled
settings. Code located at: https://github.com/Rows21/DK-UXNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Rong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dehua Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12386">
<title>Point, Segment and Count: A Generalized Framework for Object Counting. (arXiv:2311.12386v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12386</link>
<description rdf:parseType="Literal">&lt;p&gt;Class-agnostic object counting aims to count all objects in an image with
respect to example boxes or class names, \emph{a.k.a} few-shot and zero-shot
counting. Current state-of-the-art methods highly rely on density maps to
predict object counts, which lacks model interpretability. In this paper, we
propose a generalized framework for both few-shot and zero-shot object counting
based on detection. Our framework combines the superior advantages of two
foundation models without compromising their zero-shot capability: (\textbf{i})
SAM to segment all possible objects as mask proposals, and (\textbf{ii}) CLIP
to classify proposals to obtain accurate object counts. However, this strategy
meets the obstacles of efficiency overhead and the small crowded objects that
cannot be localized and distinguished. To address these issues, our framework,
termed PseCo, follows three steps: point, segment, and count. Specifically, we
first propose a class-agnostic object localization to provide accurate but
least point prompts for SAM, which consequently not only reduces computation
costs but also avoids missing small objects. Furthermore, we propose a
generalized object classification that leverages CLIP image/text embeddings as
the classifier, following a hierarchical knowledge distillation to obtain
discriminative classifications among hierarchical mask proposals. Extensive
experimental results on FSC-147 dataset demonstrate that PseCo achieves
state-of-the-art performance in both few-shot/zero-shot object
counting/detection, with additional results on large-scale COCO and LVIS
datasets. The source code is available at
\url{https://github.com/Hzzone/PseCo}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhizhong_H/0/1/0/all/0/1&quot;&gt;Huang Zhizhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mingliang_D/0/1/0/all/0/1&quot;&gt;Dai Mingliang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1&quot;&gt;Zhang Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junping_Z/0/1/0/all/0/1&quot;&gt;Zhang Junping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hongming_S/0/1/0/all/0/1&quot;&gt;Shan Hongming&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12391">
<title>From Wrong To Right: A Recursive Approach Towards Vision-Language Explanation. (arXiv:2311.12391v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12391</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the challenge of adapting pre-trained vision-language models for
generating insightful explanations for visual reasoning tasks with limited
annotations, we present ReVisE: a $\textbf{Re}$cursive $\textbf{Vis}$ual
$\textbf{E}$xplanation algorithm. Our method iteratively computes visual
features (conditioned on the text input), an answer, and an explanation, to
improve the explanation quality step by step until the answer converges. We
find that this multi-step approach guides the model to correct its own answers
and outperforms single-step explanation generation. Furthermore, explanations
generated by ReVisE also serve as valuable annotations for few-shot
self-training. Our approach outperforms previous methods while utilizing merely
5% of the human-annotated explanations across 10 metrics, demonstrating up to a
4.2 and 1.3 increase in BLEU-1 score on the VCR and VQA-X datasets,
underscoring the efficacy and data-efficiency of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jiaxin Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1&quot;&gt;Sanjay Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12397">
<title>Rich and Poor Texture Contrast: A Simple yet Effective Approach for AI-generated Image Detection. (arXiv:2311.12397v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12397</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent generative models show impressive performance in generating
photographic images. Humans can hardly distinguish such incredibly
realistic-looking AI-generated images from real ones. AI-generated images may
lead to ubiquitous disinformation dissemination. Therefore, it is of utmost
urgency to develop a detector to identify AI-generated images. Most existing
detectors suffer from sharp performance drops over unseen generative models. In
this paper, we propose a novel AI-generated image detector capable of
identifying fake images created by a wide range of generative models. Our
approach leverages the inter-pixel correlation contrast between rich and poor
texture regions within an image. Pixels in rich texture regions exhibit more
significant fluctuations than those in poor texture regions. This discrepancy
reflects that the entropy of rich texture regions is larger than that of poor
ones. Consequently, synthesizing realistic rich texture regions proves to be
more challenging for existing generative models. Based on this principle, we
divide an image into multiple patches and reconstruct them into two images,
comprising rich-texture and poor-texture patches respectively. Subsequently, we
extract the inter-pixel correlation discrepancy feature between rich and poor
texture regions. This feature serves as a universal fingerprint used for
AI-generated image forensics across different generative models. In addition,
we build a comprehensive AI-generated image detection benchmark, which includes
16 kinds of prevalent generative models, to evaluate the effectiveness of
existing baselines and our approach. Our benchmark provides a leaderboard for
follow-up studies. Extensive experimental results show that our approach
outperforms state-of-the-art baselines by a significant margin. Our project:
https://fdmas.github.io/AIGCDetect/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_N/0/1/0/all/0/1&quot;&gt;Nan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yiran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhenxing Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinpeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12398">
<title>RFTrans: Leveraging Refractive Flow of Transparent Objects for Surface Normal Estimation and Manipulation. (arXiv:2311.12398v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12398</link>
<description rdf:parseType="Literal">&lt;p&gt;Transparent objects are widely used in our daily lives, making it important
to teach robots to interact with them. However, it&apos;s not easy because the
reflective and refractive effects can make RGB-D cameras fail to give accurate
geometry measurements. To solve this problem, this paper introduces RFTrans, an
RGB-D-based method for surface normal estimation and manipulation of
transparent objects. By leveraging refractive flow as an intermediate
representation, RFTrans circumvents the drawbacks of directly predicting the
geometry (e.g. surface normal) from RGB images and helps bridge the sim-to-real
gap. RFTrans integrates the RFNet, which predicts refractive flow, object mask,
and boundaries, followed by the F2Net, which estimates surface normal from the
refractive flow. To make manipulation possible, a global optimization module
will take in the predictions, refine the raw depth, and construct the point
cloud with normal. An analytical grasp planning algorithm, ISF, is followed to
generate the grasp poses. We build a synthetic dataset with physically
plausible ray-tracing rendering techniques to train the networks. Results show
that the RFTrans trained on the synthetic dataset can consistently outperform
the baseline ClearGrasp in both synthetic and real-world benchmarks by a large
margin. Finally, a real-world robot grasping task witnesses an 83% success
rate, proving that refractive flow can help enable direct sim-to-real transfer.
The code, data, and supplementary materials are available at
https://rftrans.robotflow.ai.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_T/0/1/0/all/0/1&quot;&gt;Tutian Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jieyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Haoyuan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenqiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12401">
<title>CASR: Refining Action Segmentation via Magrinalizing Frame-levle Causal Relationships. (arXiv:2311.12401v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12401</link>
<description rdf:parseType="Literal">&lt;p&gt;Integrating deep learning and causal discovery has increased the
interpretability of Temporal Action Segmentation (TAS) tasks. However,
frame-level causal relationships exist many complicated noises outside the
segment-level, making it infeasible to directly express macro action semantics.
Thus, we propose \textit{\textbf{Causal Abstraction Segmentation Refiner
(CASR)}}, which can refine TAS results from various models by enhancing video
causality in marginalizing frame-level casual relationships. Specifically, we
define the equivalent frame-level casual model and segment-level causal model,
so that the causal adjacency matrix constructed from marginalized frame-level
causal relationships has the ability to represent the segmnet-level causal
relationships. CASR works out by reducing the difference in the causal
adjacency matrix between we constructed and pre-segmentation results of
backbone models. In addition, we propose a novel evaluation metric Causal Edit
Distance (CED) to evaluate the causal interpretability. Extensive experimental
results on mainstream datasets indicate that CASR significantly surpasses
existing various methods in action segmentation performance, as well as in
causal explainability and generalization. Our code will be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1&quot;&gt;Keqing Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12407">
<title>Learning Part Motion of Articulated Objects Using Spatially Continuous Neural Implicit Representations. (arXiv:2311.12407v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12407</link>
<description rdf:parseType="Literal">&lt;p&gt;Articulated objects (e.g., doors and drawers) exist everywhere in our life.
Different from rigid objects, articulated objects have higher degrees of
freedom and are rich in geometries, semantics, and part functions. Modeling
different kinds of parts and articulations with nerual networks plays an
essential role in articulated object understanding and manipulation, and will
further benefit 3D vision and robotics communities. To model articulated
objects, most previous works directly encode articulated objects into feature
representations, without specific designs for parts, articulations and part
motions. In this paper, we introduce a novel framework that explicitly
disentangles the part motion of articulated objects by predicting the
transformation matrix of points on the part surface, using spatially continuous
neural implicit representations to model the part motion smoothly in the space.
More importantly, while many methods could only model a certain kind of joint
motion (such as the revolution in the clockwise order), our proposed framework
is generic to different kinds of joint motions in that transformation matrix
can model diverse kinds of joint motions in the space. Quantitative and
qualitative results of experiments over diverse categories of articulated
objects demonstrate the effectiveness of our proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yushi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruihai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12419">
<title>Board-to-Board: Evaluating Moonboard Grade Prediction Generalization. (arXiv:2311.12419v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.12419</link>
<description rdf:parseType="Literal">&lt;p&gt;Bouldering is a sport where athletes aim to climb up an obstacle using a set
of defined holds called a route. Typically routes are assigned a grade to
inform climbers of its difficulty and allow them to more easily track their
progression. However, the variation in individual climbers technical and
physical attributes and many nuances of an individual route make grading a
difficult and often biased task. In this work, we apply classical and
deep-learning modelling techniques to the 2016, 2017 and 2019 Moonboard
datasets, achieving state of the art grade prediction performance with 0.87 MAE
and 1.12 RMSE. We achieve this performance on a feature-set that does not
require decomposing routes into individual moves, which is a method common in
literature and introduces bias. We also demonstrate the generalization
capability of this model between editions and introduce a novel vision-based
method of grade prediction. While the generalization performance of these
techniques is below human level performance currently, we propose these methods
as a basis for future work. Such a tool could be implemented in pre-existing
mobile applications and would allow climbers to better track their progress and
assess new routes with reduced bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petashvili_D/0/1/0/all/0/1&quot;&gt;Daniel Petashvili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodda_M/0/1/0/all/0/1&quot;&gt;Matthew Rodda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12421">
<title>Two Views Are Better than One: Monocular 3D Pose Estimation with Multiview Consistency. (arXiv:2311.12421v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12421</link>
<description rdf:parseType="Literal">&lt;p&gt;Deducing a 3D human pose from a single 2D image or 2D keypoints is inherently
challenging, given the fundamental ambiguity wherein multiple 3D poses can
correspond to the same 2D representation. The acquisition of 3D data, while
invaluable for resolving pose ambiguity, is expensive and requires an intricate
setup, often restricting its applicability to controlled lab environments. We
improve performance of monocular human pose estimation models using multiview
data for fine-tuning. We propose a novel loss function, multiview consistency,
to enable adding additional training data with only 2D supervision. This loss
enforces that the inferred 3D pose from one view aligns with the inferred 3D
pose from another view under similarity transformations. Our consistency loss
substantially improves performance for fine-tuning with no available 3D data.
Our experiments demonstrate that two views offset by 90 degrees are enough to
obtain good performance, with only marginal improvements by adding more views.
Thus, we enable the acquisition of domain-specific data by capturing activities
with off-the-shelf cameras, eliminating the need for elaborate calibration
procedures. This research introduces new possibilities for domain adaptation in
3D pose estimation, providing a practical and cost-effective solution to
customize models for specific applications. The used dataset, featuring
additional views, will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingwersen_C/0/1/0/all/0/1&quot;&gt;Christian Keilstrup Ingwersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dahl_A/0/1/0/all/0/1&quot;&gt;Anders Bjorholm Dahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_J/0/1/0/all/0/1&quot;&gt;Janus N&amp;#xf8;rtoft Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hannemose_M/0/1/0/all/0/1&quot;&gt;Morten Rieger Hannemose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12430">
<title>AR Visualization System for Ship Detection and Recognition Based on AI. (arXiv:2311.12430v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12430</link>
<description rdf:parseType="Literal">&lt;p&gt;Augmented reality technology has been widely used in industrial design
interaction, exhibition guide, information retrieval and other fields. The
combination of artificial intelligence and augmented reality technology has
also become a future development trend. This project is an AR visualization
system for ship detection and recognition based on AI, which mainly includes
three parts: artificial intelligence module, Unity development module and
Hololens2AR module. This project is based on R3Det algorithm to complete the
detection and recognition of ships in remote sensing images. The recognition
rate of model detection trained on RTX 2080Ti can reach 96%. Then, the 3D model
of the ship is obtained by ship categories and information and generated in the
virtual scene. At the same time, voice module and UI interaction module are
added. Finally, we completed the deployment of the project on Hololens2 through
MRTK. The system realizes the fusion of computer vision and augmented reality
technology, which maps the results of object detection to the AR field, and
makes a brave step toward the future technological trend and intelligent
application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Z/0/1/0/all/0/1&quot;&gt;Ziqi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Limin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongji Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Min Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12437">
<title>Learning Site-specific Styles for Multi-institutional Unsupervised Cross-modality Domain Adaptation. (arXiv:2311.12437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12437</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised cross-modality domain adaptation is a challenging task in
medical image analysis, and it becomes more challenging when source and target
domain data are collected from multiple institutions. In this paper, we present
our solution to tackle the multi-institutional unsupervised domain adaptation
for the crossMoDA 2023 challenge. First, we perform unpaired image translation
to translate the source domain images to the target domain, where we design a
dynamic network to generate synthetic target domain images with controllable,
site-specific styles. Afterwards, we train a segmentation model using the
synthetic images and further reduce the domain gap by self-training. Our
solution achieved the 1st place during both the validation and testing phases
of the challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yubo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhoubing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dawant_B/0/1/0/all/0/1&quot;&gt;Benoit M. Dawant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12457">
<title>LIP-RTVE: An Audiovisual Database for Continuous Spanish in the Wild. (arXiv:2311.12457v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12457</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech is considered as a multi-modal process where hearing and vision are
two fundamentals pillars. In fact, several studies have demonstrated that the
robustness of Automatic Speech Recognition systems can be improved when audio
and visual cues are combined to represent the nature of speech. In addition,
Visual Speech Recognition, an open research problem whose purpose is to
interpret speech by reading the lips of the speaker, has been a focus of
interest in the last decades. Nevertheless, in order to estimate these systems
in the currently Deep Learning era, large-scale databases are required. On the
other hand, while most of these databases are dedicated to English, other
languages lack sufficient resources. Thus, this paper presents a
semi-automatically annotated audiovisual database to deal with unconstrained
natural Spanish, providing 13 hours of data extracted from Spanish television.
Furthermore, baseline results for both speaker-dependent and
speaker-independent scenarios are reported using Hidden Markov Models, a
traditional paradigm that has been widely used in the field of Speech
Technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_Gomez_D/0/1/0/all/0/1&quot;&gt;David Gimeno-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Hinarejos_C/0/1/0/all/0/1&quot;&gt;Carlos-D. Mart&amp;#xed;nez-Hinarejos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12461">
<title>HiFi-Syn: Hierarchical Granularity Discrimination for High-Fidelity Synthesis of MR Images with Structure Preservation. (arXiv:2311.12461v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12461</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizing medical images while preserving their structural information is
crucial in medical research. In such scenarios, the preservation of anatomical
content becomes especially important. Although recent advances have been made
by incorporating instance-level information to guide translation, these methods
overlook the spatial coherence of structural-level representation and the
anatomical invariance of content during translation. To address these issues,
we introduce hierarchical granularity discrimination, which exploits various
levels of semantic information present in medical images. Our strategy utilizes
three levels of discrimination granularity: pixel-level discrimination using a
Brain Memory Bank, structure-level discrimination on each brain structure with
a re-weighting strategy to focus on hard samples, and global-level
discrimination to ensure anatomical consistency during translation. The image
translation performance of our strategy has been evaluated on three independent
datasets (UK Biobank, IXI, and BraTS 2018), and it has outperformed
state-of-the-art algorithms. Particularly, our model excels not only in
synthesizing normal structures but also in handling abnormal (pathological)
structures, such as brain tumors, despite the variations in contrast observed
across different imaging modalities due to their pathological characteristics.
The diagnostic value of synthesized MR images containing brain tumors has been
evaluated by radiologists. This indicates that our model may offer an
alternative solution in scenarios where specific MR modalities of patients are
unavailable. Extensive experiments further demonstrate the versatility of our
method, providing unique insights into medical image translation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Ziqi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Botao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jianfeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao-Yong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12467">
<title>GLAD: Global-Local View Alignment and Background Debiasing for Unsupervised Video Domain Adaptation with Large Domain Gap. (arXiv:2311.12467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12467</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we tackle the challenging problem of unsupervised video domain
adaptation (UVDA) for action recognition. We specifically focus on scenarios
with a substantial domain gap, in contrast to existing works primarily deal
with small domain gaps between labeled source domains and unlabeled target
domains. To establish a more realistic setting, we introduce a novel UVDA
scenario, denoted as Kinetics-&amp;gt;BABEL, with a more considerable domain gap in
terms of both temporal dynamics and background shifts. To tackle the temporal
shift, i.e., action duration difference between the source and target domains,
we propose a global-local view alignment approach. To mitigate the background
shift, we propose to learn temporal order sensitive representations by temporal
order learning and background invariant representations by background
augmentation. We empirically validate that the proposed method shows
significant improvement over the existing methods on the Kinetics-&amp;gt;BABEL
dataset with a large domain gap. The code is available at
https://github.com/KHUVLL/GLAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyogun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1&quot;&gt;Kyungho Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Seongjong Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_Y/0/1/0/all/0/1&quot;&gt;Yumin Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_G/0/1/0/all/0/1&quot;&gt;Gyeongmoon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jinwoo Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12468">
<title>Analysis of Visual Features for Continuous Lipreading in Spanish. (arXiv:2311.12468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12468</link>
<description rdf:parseType="Literal">&lt;p&gt;During a conversation, our brain is responsible for combining information
obtained from multiple senses in order to improve our ability to understand the
message we are perceiving. Different studies have shown the importance of
presenting visual information in these situations. Nevertheless, lipreading is
a complex task whose objective is to interpret speech when audio is not
available. By dispensing with a sense as crucial as hearing, it will be
necessary to be aware of the challenge that this lack presents. In this paper,
we propose an analysis of different speech visual features with the intention
of identifying which of them is the best approach to capture the nature of lip
movements for natural Spanish and, in this way, dealing with the automatic
visual speech recognition task. In order to estimate our system, we present an
audiovisual corpus compiled from a subset of the RTVE database, which has been
used in the Albayz\&apos;in evaluations. We employ a traditional system based on
Hidden Markov Models with Gaussian Mixture Models. Results show that, although
the task is difficult, in restricted conditions we obtain recognition results
which determine that using eigenlips in combination with deep features is the
best visual approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_Gomez_D/0/1/0/all/0/1&quot;&gt;David Gimeno-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Hinarejos_C/0/1/0/all/0/1&quot;&gt;Carlos-D. Mart&amp;#xed;nez-Hinarejos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12476">
<title>MaskFlow: Object-Aware Motion Estimation. (arXiv:2311.12476v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12476</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel motion estimation method, MaskFlow, that is capable of
estimating accurate motion fields, even in very challenging cases with small
objects, large displacements and drastic appearance changes. In addition to
lower-level features, that are used in other Deep Neural Network (DNN)-based
motion estimation methods, MaskFlow draws from object-level features and
segmentations. These features and segmentations are used to approximate the
objects&apos; translation motion field. We propose a novel and effective way of
incorporating the incomplete translation motion field into a subsequent motion
estimation network for refinement and completion. We also produced a new
challenging synthetic dataset with motion field ground truth, and also provide
extra ground truth for the object-instance matchings and corresponding
segmentation masks. We demonstrate that MaskFlow outperforms state of the art
methods when evaluated on our new challenging dataset, whilst still producing
comparable results on the popular FlyingThings3D benchmark dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_A/0/1/0/all/0/1&quot;&gt;Aria Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walton_D/0/1/0/all/0/1&quot;&gt;David R. Walton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atherton_T/0/1/0/all/0/1&quot;&gt;Tim Atherton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dikici_C/0/1/0/all/0/1&quot;&gt;Cagatay Dikici&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12480">
<title>Speaker-Adapted End-to-End Visual Speech Recognition for Continuous Spanish. (arXiv:2311.12480v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12480</link>
<description rdf:parseType="Literal">&lt;p&gt;Different studies have shown the importance of visual cues throughout the
speech perception process. In fact, the development of audiovisual approaches
has led to advances in the field of speech technologies. However, although
noticeable results have recently been achieved, visual speech recognition
remains an open research problem. It is a task in which, by dispensing with the
auditory sense, challenges such as visual ambiguities and the complexity of
modeling silence must be faced. Nonetheless, some of these challenges can be
alleviated when the problem is approached from a speaker-dependent perspective.
Thus, this paper studies, using the Spanish LIP-RTVE database, how the
estimation of specialized end-to-end systems for a specific person could affect
the quality of speech recognition. First, different adaptation strategies based
on the fine-tuning technique were proposed. Then, a pre-trained CTC/Attention
architecture was used as a baseline throughout our experiments. Our findings
showed that a two-step fine-tuning process, where the VSR system is first
adapted to the task domain, provided significant improvements when the speaker
adaptation was addressed. Furthermore, results comparable to the current state
of the art were reached even when only a limited amount of data was available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gimeno_Gomez_D/0/1/0/all/0/1&quot;&gt;David Gimeno-G&amp;#xf3;mez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Hinarejos_C/0/1/0/all/0/1&quot;&gt;Carlos-D. Mart&amp;#xed;nez-Hinarejos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12486">
<title>HCA-Net: Hierarchical Context Attention Network for Intervertebral Disc Semantic Labeling. (arXiv:2311.12486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12486</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate and automated segmentation of intervertebral discs (IVDs) in medical
images is crucial for assessing spine-related disorders, such as osteoporosis,
vertebral fractures, or IVD herniation. We present HCA-Net, a novel contextual
attention network architecture for semantic labeling of IVDs, with a special
focus on exploiting prior geometric information. Our approach excels at
processing features across different scales and effectively consolidating them
to capture the intricate spatial relationships within the spinal cord. To
achieve this, HCA-Net models IVD labeling as a pose estimation problem, aiming
to minimize the discrepancy between each predicted IVD location and its
corresponding actual joint location. In addition, we introduce a skeletal loss
term to reinforce the model&apos;s geometric dependence on the spine. This loss
function is designed to constrain the model&apos;s predictions to a range that
matches the general structure of the human vertebral skeleton. As a result, the
network learns to reduce the occurrence of false predictions and adaptively
improves the accuracy of IVD location estimation. Through extensive
experimental evaluation on multi-center spine datasets, our approach
consistently outperforms previous state-of-the-art methods on both MRI T1w and
T2w modalities. The codebase is accessible to the public on
\href{https://github.com/xmindflow/HCA-Net}{GitHub}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozorgpour_A/0/1/0/all/0/1&quot;&gt;Afshin Bozorgpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_B/0/1/0/all/0/1&quot;&gt;Bobby Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1&quot;&gt;Reza Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velichko_Y/0/1/0/all/0/1&quot;&gt;Yury Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1&quot;&gt;Dorit Merhof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12490">
<title>Hyb-NeRF: A Multiresolution Hybrid Encoding for Neural Radiance Fields. (arXiv:2311.12490v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12490</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Neural radiance fields (NeRF) have enabled high-fidelity
scene reconstruction for novel view synthesis. However, NeRF requires hundreds
of network evaluations per pixel to approximate a volume rendering integral,
making it slow to train. Caching NeRFs into explicit data structures can
effectively enhance rendering speed but at the cost of higher memory usage. To
address these issues, we present Hyb-NeRF, a novel neural radiance field with a
multi-resolution hybrid encoding that achieves efficient neural modeling and
fast rendering, which also allows for high-quality novel view synthesis. The
key idea of Hyb-NeRF is to represent the scene using different encoding
strategies from coarse-to-fine resolution levels. Hyb-NeRF exploits
memory-efficiency learnable positional features at coarse resolutions and the
fast optimization speed and local details of hash-based feature grids at fine
resolutions. In addition, to further boost performance, we embed cone
tracing-based features in our learnable positional encoding that eliminates
encoding ambiguity and reduces aliasing artifacts. Extensive experiments on
both synthetic and real-world datasets show that Hyb-NeRF achieves faster
rendering speed with better rending quality and even a lower memory footprint
in comparison to previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yi Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yuan Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12539">
<title>GMISeg: General Medical Image Segmentation without Re-Training. (arXiv:2311.12539v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12539</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep learning models have become the main method for medical image
segmentation, they often cannot be extended to unknown segmentation tasks
involving new anatomical structures, image shapes, or labels. For new
segmentation tasks, researchers often have to retrain or fine-tune the model,
which is time-consuming and poses a significant obstacle to clinical
researchers, who often lack the resources and professional knowledge to train
neural networks. Therefore, we proposed a general method that can solve unknown
medical image segmentation tasks without requiring additional training. Given
an example set of images and prompts for defining new segmentation tasks,
GMISeg applies a novel low-rank fine-tuning strategy based on the proposed
approach to the SAM (Segment Anything Model) image encoder, and works with the
prompt encoder and mask decoder to fine-tune the labeled dataset without the
need for additional training. To achieve generalization of new tasks, we used
medical image datasets with different imaging modes for different parts. We
trained and generalized GMISeg on a different set of anatomical and imaging
modes using cardiac images on other site datasets. We have demonstrated that
GMISeg outperforms the latest methods on unknown tasks and have conducted a
comprehensive analysis and summary of the important performance of the proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12553">
<title>&quot;HoVer-UNet&quot;: Accelerating HoVerNet with UNet-based multi-class nuclei segmentation via knowledge distillation. (arXiv:2311.12553v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12553</link>
<description rdf:parseType="Literal">&lt;p&gt;We present &quot;HoVer-UNet&quot;, an approach to distill the knowledge of the
multi-branch HoVerNet framework for nuclei instance segmentation and
classification in histopathology. We propose a compact, streamlined single UNet
network with a Mix Vision Transformer backbone, and equip it with a custom loss
function to optimally encode the distilled knowledge of HoVerNet, reducing
computational requirements without compromising performances. We show that our
model achieved results comparable to HoVerNet on the public PanNuke and Consep
datasets with a three-fold reduction in inference time. We make the code of our
model publicly available at https://github.com/DIAGNijmegen/HoVer-UNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tommasino_C/0/1/0/all/0/1&quot;&gt;Cristian Tommasino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Russo_C/0/1/0/all/0/1&quot;&gt;Cristiano Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rinaldi_A/0/1/0/all/0/1&quot;&gt;Antonio Maria Rinaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ciompi_F/0/1/0/all/0/1&quot;&gt;Francesco Ciompi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12560">
<title>Benchmarking bias: Expanding clinical AI model card to incorporate bias reporting of social and non-social factors. (arXiv:2311.12560v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12560</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical AI model reporting cards should be expanded to incorporate a broad
bias reporting of both social and non-social factors. Non-social factors
consider the role of other factors, such as disease dependent, anatomic, or
instrument factors on AI model bias, which are essential to ensure safe
deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heming_C/0/1/0/all/0/1&quot;&gt;Carolina A. M. Heming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdalla_M/0/1/0/all/0/1&quot;&gt;Mohamed Abdalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahluwalia_M/0/1/0/all/0/1&quot;&gt;Monish Ahluwalia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Linglin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_H/0/1/0/all/0/1&quot;&gt;Hari Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_M/0/1/0/all/0/1&quot;&gt;MinJae Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fine_B/0/1/0/all/0/1&quot;&gt;Benjamin Fine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gichoya_J/0/1/0/all/0/1&quot;&gt;Judy Wawira Gichoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celi_L/0/1/0/all/0/1&quot;&gt;Leo Anthony Celi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seyyed_Kalantari_L/0/1/0/all/0/1&quot;&gt;Laleh Seyyed-Kalantari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12561">
<title>Convolutional Neural Networks for Neuroimaging in Parkinson&apos;s Disease: Is Preprocessing Needed?. (arXiv:2311.12561v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12561</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial and intensity normalization are nowadays a prerequisite for
neuroimaging analysis. Influenced by voxel-wise and other univariate
comparisons, where these corrections are key, they are commonly applied to any
type of analysis and imaging modalities. Nuclear imaging modalities such as
PET-FDG or FP-CIT SPECT, a common modality used in Parkinson&apos;s Disease
diagnosis, are especially dependent on intensity normalization. However, these
steps are computationally expensive and furthermore, they may introduce
deformations in the images, altering the information contained in them.
Convolutional Neural Networks (CNNs), for their part, introduce position
invariance to pattern recognition, and have been proven to classify objects
regardless of their orientation, size, angle, etc. Therefore, a question
arises: how well can CNNs account for spatial and intensity differences when
analysing nuclear brain imaging? Are spatial and intensity normalization still
needed? To answer this question, we have trained four different CNN models
based on well-established architectures, using or not different spatial and
intensity normalization preprocessing. The results show that a sufficiently
complex model such as our three-dimensional version of the ALEXNET can
effectively account for spatial differences, achieving a diagnosis accuracy of
94.1% with an area under the ROC curve of 0.984. The visualization of the
differences via saliency maps shows that these models are correctly finding
patterns that match those found in the literature, without the need of applying
any complex spatial normalization procedure. However, the intensity
normalization -- and its type -- is revealed as very influential in the results
and accuracy of the trained model, and therefore must be well accounted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_Murcia_F/0/1/0/all/0/1&quot;&gt;Francisco J. Martinez-Murcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorriz_J/0/1/0/all/0/1&quot;&gt;Juan M. G&amp;#xf3;rriz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_J/0/1/0/all/0/1&quot;&gt;Javier Ram&amp;#xed;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Ortiz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12562">
<title>Multi-Resolution Planar Region Extraction for Uneven Terrains. (arXiv:2311.12562v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12562</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of extracting planar regions in uneven
terrains from unordered point cloud measurements. Such a problem is critical in
various robotic applications such as robotic perceptive locomotion. While
existing approaches have shown promising results in effectively extracting
planar regions from the environment, they often suffer from issues such as low
computational efficiency or loss of resolution. To address these issues, we
propose a multi-resolution planar region extraction strategy in this paper that
balances the accuracy in boundaries and computational efficiency. Our method
begins with a pointwise classification preprocessing module, which categorizes
all sampled points according to their local geometric properties to facilitate
multi-resolution segmentation. Subsequently, we arrange the categorized points
using an octree, followed by an in-depth analysis of nodes to finish
multi-resolution plane segmentation. The efficiency and robustness of the
proposed approach are verified via synthetic and real-world experiments,
demonstrating our method&apos;s ability to generalize effectively across various
uneven terrains while maintaining real-time performance, achieving frame rates
exceeding 35 FPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yinghan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Linfang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hua Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12581">
<title>A Region of Interest Focused Triple UNet Architecture for Skin Lesion Segmentation. (arXiv:2311.12581v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12581</link>
<description rdf:parseType="Literal">&lt;p&gt;Skin lesion segmentation is of great significance for skin lesion analysis
and subsequent treatment. It is still a challenging task due to the irregular
and fuzzy lesion borders, and diversity of skin lesions. In this paper, we
propose Triple-UNet to automatically segment skin lesions. It is an organic
combination of three UNet architectures with suitable modules. In order to
concatenate the first and second sub-networks more effectively, we design a
region of interest enhancement module (ROIE). The ROIE enhances the target
object region of the image by using the predicted score map of the first UNet.
The features learned by the first UNet and the enhanced image help the second
UNet obtain a better score map. Finally, the results are fine-tuned by the
third UNet. We evaluate our algorithm on a publicly available dataset of skin
lesion segmentation. Experiments show that Triple-UNet outperforms the
state-of-the-art on skin lesion segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Guoqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Caiying Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guoqing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saheya_B/0/1/0/all/0/1&quot;&gt;Barintag Saheya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qiyu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12582">
<title>Echocardiogram Foundation Model -- Application 1: Estimating Ejection Fraction. (arXiv:2311.12582v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12582</link>
<description rdf:parseType="Literal">&lt;p&gt;Cardiovascular diseases stand as the primary global cause of mortality. Among
the various imaging techniques available for visualising the heart and
evaluating its function, echocardiograms emerge as the preferred choice due to
their safety and low cost. Quantifying cardiac function based on
echocardiograms is very laborious, time-consuming and subject to high
interoperator variability. In this work, we introduce EchoAI, an echocardiogram
foundation model, that is trained using self-supervised learning (SSL) on 1.5
million echocardiograms. We evaluate our approach by fine-tuning EchoAI to
estimate the ejection fraction achieving a mean absolute percentage error of
9.40%. This level of accuracy aligns with the performance of expert
sonographers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dahlan_A/0/1/0/all/0/1&quot;&gt;Adil Dahlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zakka_C/0/1/0/all/0/1&quot;&gt;Cyril Zakka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Abhinav Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Laura Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shad_R/0/1/0/all/0/1&quot;&gt;Rohan Shad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fong_R/0/1/0/all/0/1&quot;&gt;Robyn Fong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hiesinger_W/0/1/0/all/0/1&quot;&gt;William Hiesinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12588">
<title>HiPose: Hierarchical Binary Surface Encoding and Correspondence Pruning for RGB-D 6DoF Object Pose Estimation. (arXiv:2311.12588v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12588</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a novel dense-correspondence method for 6DoF object
pose estimation from a single RGB-D image. While many existing data-driven
methods achieve impressive performance, they tend to be time-consuming due to
their reliance on rendering-based refinement approaches. To circumvent this
limitation, we present HiPose, which establishes 3D-3D correspondences in a
coarse-to-fine manner with a hierarchical binary surface encoding. Unlike
previous dense-correspondence methods, we estimate the correspondence surface
by employing point-to-surface matching and iteratively constricting the surface
until it becomes a correspondence point while gradually removing outliers.
Extensive experiments on public benchmarks LM-O, YCB-V, and T-Less demonstrate
that our method surpasses all refinement-free methods and is even on par with
expensive refinement-based approaches. Crucially, our approach is
computationally efficient and enables real-time critical applications with high
accuracy requirements. Code and models will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yongliang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Y/0/1/0/all/0/1&quot;&gt;Yongzhi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nathan_P/0/1/0/all/0/1&quot;&gt;Praveen Nathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inuganti_S/0/1/0/all/0/1&quot;&gt;Sandeep Inuganti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Di_Y/0/1/0/all/0/1&quot;&gt;Yan Di&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sundermeyer_M/0/1/0/all/0/1&quot;&gt;Martin Sundermeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manhardt_F/0/1/0/all/0/1&quot;&gt;Fabian Manhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stricke_D/0/1/0/all/0/1&quot;&gt;Didier Stricke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rambach_J/0/1/0/all/0/1&quot;&gt;Jason Rambach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12589">
<title>Improving Source-Free Target Adaptation with Vision Transformers Leveraging Domain Representation Images. (arXiv:2311.12589v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12589</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised Domain Adaptation (UDA) methods facilitate knowledge transfer
from a labeled source domain to an unlabeled target domain, navigating the
obstacle of domain shift. While Convolutional Neural Networks (CNNs) are a
staple in UDA, the rise of Vision Transformers (ViTs) provides new avenues for
domain generalization. This paper presents an innovative method to bolster ViT
performance in source-free target adaptation, beginning with an evaluation of
how key, query, and value elements affect ViT outcomes. Experiments indicate
that altering the key component has negligible effects on Transformer
performance. Leveraging this discovery, we introduce Domain Representation
Images (DRIs), feeding embeddings through the key element. DRIs act as
domain-specific markers, effortlessly merging with the training regimen. To
assess our method, we perform target adaptation tests on the Cross Instance DRI
source-only (SO) control. We measure the efficacy of target adaptation with and
without DRIs, against existing benchmarks like SHOT-B* and adaptations via
CDTrans. Findings demonstrate that excluding DRIs offers limited gains over
SHOT-B*, while their inclusion in the key segment boosts average precision
promoting superior domain generalization. This research underscores the vital
role of DRIs in enhancing ViT efficiency in UDA scenarios, setting a precedent
for further domain adaptation explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawhney_G/0/1/0/all/0/1&quot;&gt;Gauransh Sawhney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_D/0/1/0/all/0/1&quot;&gt;Daksh Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_A/0/1/0/all/0/1&quot;&gt;Adeel Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jiechao Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saleem_K/0/1/0/all/0/1&quot;&gt;Khalid Saleem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12601">
<title>Deep learning-based detection of morphological features associated with hypoxia in H&amp;E breast cancer whole slide images. (arXiv:2311.12601v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12601</link>
<description rdf:parseType="Literal">&lt;p&gt;Hypoxia occurs when tumour cells outgrow their blood supply, leading to
regions of low oxygen levels within the tumour. Calculating hypoxia levels can
be an important step in understanding the biology of tumours, their clinical
progression and response to treatment. This study demonstrates a novel
application of deep learning to evaluate hypoxia in the context of breast
cancer histomorphology. More precisely, we show that Weakly Supervised Deep
Learning (WSDL) models can accurately detect hypoxia associated features in
routine Hematoxylin and Eosin (H&amp;amp;E) whole slide images (WSI). We trained and
evaluated a deep Multiple Instance Learning model on tiles from WSI H&amp;amp;E tissue
from breast cancer primary sites (n=240) obtaining on average an AUC of 0.87 on
a left-out test set. We also showed significant differences between features of
hypoxic and normoxic tissue regions as distinguished by the WSDL models. Such
DL hypoxia H&amp;amp;E WSI detection models could potentially be extended to other
tumour types and easily integrated into the pathology workflow without
requiring additional costly assays.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manescu_P/0/1/0/all/0/1&quot;&gt;Petru Manescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geradts_J/0/1/0/all/0/1&quot;&gt;Joseph Geradts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Reyes_D/0/1/0/all/0/1&quot;&gt;Delmiro Fernandez-Reyes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12602">
<title>TouchSDF: A DeepSDF Approach for 3D Shape Reconstruction using Vision-Based Tactile Sensing. (arXiv:2311.12602v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12602</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans rely on their visual and tactile senses to develop a comprehensive 3D
understanding of their physical environment. Recently, there has been a growing
interest in exploring and manipulating objects using data-driven approaches
that utilise high-resolution vision-based tactile sensors. However, 3D shape
reconstruction using tactile sensing has lagged behind visual shape
reconstruction because of limitations in existing techniques, including the
inability to generalise over unseen shapes, the absence of real-world testing,
and limited expressive capacity imposed by discrete representations. To address
these challenges, we propose TouchSDF, a Deep Learning approach for tactile 3D
shape reconstruction that leverages the rich information provided by a
vision-based tactile sensor and the expressivity of the implicit neural
representation DeepSDF. Our technique consists of two components: (1) a
Convolutional Neural Network that maps tactile images into local meshes
representing the surface at the touch location, and (2) an implicit neural
function that predicts a signed distance function to extract the desired 3D
shape. This combination allows TouchSDF to reconstruct smooth and continuous 3D
shapes from tactile inputs in simulation and real-world settings, opening up
research avenues for robust 3D-aware representations and improved multimodal
perception in robotics. Code and supplementary material are available at:
https://touchsdf.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Comi_M/0/1/0/all/0/1&quot;&gt;Mauro Comi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yijiong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Church_A/0/1/0/all/0/1&quot;&gt;Alex Church&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonioni_A/0/1/0/all/0/1&quot;&gt;Alessio Tonioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aitchison_L/0/1/0/all/0/1&quot;&gt;Laurence Aitchison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepora_N/0/1/0/all/0/1&quot;&gt;Nathan F. Lepora&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12603">
<title>Surgical Temporal Action-aware Network with Sequence Regularization for Phase Recognition. (arXiv:2311.12603v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12603</link>
<description rdf:parseType="Literal">&lt;p&gt;To assist surgeons in the operating theatre, surgical phase recognition is
critical for developing computer-assisted surgical systems, which requires
comprehensive understanding of surgical videos. Although existing studies made
great progress, there are still two significant limitations worthy of
improvement. First, due to the compromise of resource consumption, frame-wise
visual features are extracted by 2D networks and disregard spatial and temporal
knowledge of surgical actions, which hinders subsequent inter-frame modeling
for phase prediction. Second, these works simply utilize ordinary
classification loss with one-hot phase labels to optimize the phase
predictions, and cannot fully explore surgical videos under inadequate
supervision. To overcome these two limitations, we propose a Surgical Temporal
Action-aware Network with sequence Regularization, named STAR-Net, to recognize
surgical phases more accurately from input videos. Specifically, we propose an
efficient multi-scale surgical temporal action (MS-STA) module, which
integrates visual features with spatial and temporal knowledge of surgical
actions at the cost of 2D networks. Moreover, we devise the dual-classifier
sequence regularization (DSR) to facilitate the training of STAR-Net by the
sequence guidance of an auxiliary classifier with a smaller capacity. Our
STAR-Net with MS-STA and DSR can exploit visual features of surgical actions
with effective regularization, thereby leading to the superior performance of
surgical phase recognition. Extensive experiments on a large-scale gastrectomy
surgery dataset and the public Cholec80 benchmark prove that our STAR-Net
significantly outperforms state-of-the-arts of surgical phase recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Y/0/1/0/all/0/1&quot;&gt;Yuhao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinqiao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12608">
<title>Adaptive Dense Pseudo Label Selection for Semi-supervised Oriented Object Detection. (arXiv:2311.12608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12608</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, dense pseudo-label, which directly selects pseudo labels from the
original output of the teacher model without any complicated post-processing
steps, has received considerable attention in semi-supervised object detection
(SSOD). However, for the multi-oriented and dense objects that are common in
aerial scenes, existing dense pseudo-label selection methods are inefficient
and impede the performance in semi-supervised oriented object detection.
Therefore, we propose Adaptive Dense Pseudo Label Selection (ADPLS) for
semi-supervised oriented object detection. In ADPLS, we design a simple but
effective adaptive mechanism to guide the selection of dense pseudo labels.
Specifically, we propose the mean Feature-Richness Score (mFRS) to estimate the
density of potential objects and use this score to adjust the number of dense
pseudo labels. On the DOTA-v1.5 benchmark, the proposed method outperforms
previous methods especially when labeled data are scarce. For example, it
achieves 49.78 mAP given only 5% of annotated data, which surpasses previous
state-of-the-art method given 10% of annotated data by 1.15 mAP. Our codes will
be available soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Q/0/1/0/all/0/1&quot;&gt;Qiang Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_S/0/1/0/all/0/1&quot;&gt;Shuohao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12610">
<title>ChessVision -- A Dataset for Logically Coherent Multi-label Classification. (arXiv:2311.12610v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12610</link>
<description rdf:parseType="Literal">&lt;p&gt;Starting with early successes in computer vision tasks, deep learning based
techniques have since overtaken state of the art approaches in a multitude of
domains. However, it has been demonstrated time and again that these techniques
fail to capture semantic context and logical constraints, instead often relying
on spurious correlations to arrive at the answer. Since application of deep
learning techniques to critical scenarios are dependent on adherence to domain
specific constraints, several attempts have been made to address this issue.
One limitation holding back a thorough exploration of this area, is a lack of
suitable datasets which feature a rich set of rules. In order to address this,
we present the ChessVision Dataset, consisting of 200,000+ images of annotated
chess games in progress, requiring recreation of the game state from its
corresponding image. This is accompanied by a curated set of rules which
constrains the set of predictions to &quot;reasonable&quot; game states, and are designed
to probe key semantic abilities like localization and enumeration. Alongside
standard metrics, additional metrics to measure performance with regards to
logical consistency is presented. We analyze several popular and state of the
art vision models on this task, and show that, although their performance on
standard metrics are laudable, they produce a plethora of incoherent results,
indicating that this dataset presents a significant challenge for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Soumadeep Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garain_U/0/1/0/all/0/1&quot;&gt;Utpal Garain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12617">
<title>Leveraging Unlabeled Data for 3D Medical Image Segmentation through Self-Supervised Contrastive Learning. (arXiv:2311.12617v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12617</link>
<description rdf:parseType="Literal">&lt;p&gt;Current 3D semi-supervised segmentation methods face significant challenges
such as limited consideration of contextual information and the inability to
generate reliable pseudo-labels for effective unsupervised data use. To address
these challenges, we introduce two distinct subnetworks designed to explore and
exploit the discrepancies between them, ultimately correcting the erroneous
prediction results. More specifically, we identify regions of inconsistent
predictions and initiate a targeted verification training process. This
procedure strategically fine-tunes and harmonizes the predictions of the
subnetworks, leading to enhanced utilization of contextual information.
Furthermore, to adaptively fine-tune the network&apos;s representational capacity
and reduce prediction uncertainty, we employ a self-supervised contrastive
learning paradigm. For this, we use the network&apos;s confidence to distinguish
between reliable and unreliable predictions. The model is then trained to
effectively minimize unreliable predictions. Our experimental results for organ
segmentation, obtained from clinical MRI and CT scans, demonstrate the
effectiveness of our approach when compared to state-of-the-art methods. The
codebase is accessible on
\href{https://github.com/xmindflow/SSL-contrastive}{GitHub}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimijafarbigloo_S/0/1/0/all/0/1&quot;&gt;Sanaz Karimijafarbigloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azad_R/0/1/0/all/0/1&quot;&gt;Reza Azad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velichko_Y/0/1/0/all/0/1&quot;&gt;Yury Velichko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merhof_D/0/1/0/all/0/1&quot;&gt;Dorit Merhof&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12621">
<title>Crowd management, crime detection, work monitoring using aiml. (arXiv:2311.12621v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12621</link>
<description rdf:parseType="Literal">&lt;p&gt;This research endeavors to harness the potential of existing Closed-Circuit
Television (CCTV) networks for a comprehensive approach to crowd management,
crime prevention, and workplace monitoring through the integration of
Artificial Intelligence (AI) and Machine Learning (ML) technologies. The
primary objective is to develop and implement advanced algorithms capable of
real-time analysis of video feeds, enabling the identification and assessment
of crowd dynamics, early detection of potential criminal activities, and
continuous monitoring of workplace environments. By leveraging AI/ML, the
project aims to optimize surveillance capabilities, thereby enhancing public
safety measures and improving organizational productivity. This initiative
underscores the transformative impact that intelligent video analytics can have
on existing infrastructure, mitigating the need for extensive system overhauls
while significantly advancing security and operational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adithya_P/0/1/0/all/0/1&quot;&gt;P.R.Adithya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_D/0/1/0/all/0/1&quot;&gt;Dheepak.S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akash_B/0/1/0/all/0/1&quot;&gt;B.Akash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+V_H/0/1/0/all/0/1&quot;&gt;Harshini.V&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshana_S/0/1/0/all/0/1&quot;&gt;Sai Lakshana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12623">
<title>Bridging Generalization Gaps in High Content Imaging Through Online Self-Supervised Domain Adaptation. (arXiv:2311.12623v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12623</link>
<description rdf:parseType="Literal">&lt;p&gt;High Content Imaging (HCI) plays a vital role in modern drug discovery and
development pipelines, facilitating various stages from hit identification to
candidate drug characterization. Applying machine learning models to these
datasets can prove challenging as they typically consist of multiple batches,
affected by experimental variation, especially if different imaging equipment
have been used. Moreover, as new data arrive, it is preferable that they are
analyzed in an online fashion. To overcome this, we propose CODA, an online
self-supervised domain adaptation approach. CODA divides the classifier&apos;s role
into a generic feature extractor and a task-specific model. We adapt the
feature extractor&apos;s weights to the new domain using cross-batch
self-supervision while keeping the task-specific model unchanged. Our results
demonstrate that this strategy significantly reduces the generalization gap,
achieving up to a 300% improvement when applied to data from different labs
utilizing different microscopes. CODA can be applied to new, unlabeled
out-of-domain data sources of different sizes, from a single plate to multiple
experimental batches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haslum_J/0/1/0/all/0/1&quot;&gt;Johan Fredin Haslum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsoukas_C/0/1/0/all/0/1&quot;&gt;Christos Matsoukas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leuchowius_K/0/1/0/all/0/1&quot;&gt;Karl-Johan Leuchowius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_K/0/1/0/all/0/1&quot;&gt;Kevin Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12631">
<title>GPT4Motion: Scripting Physical Motions in Text-to-Video Generation via Blender-Oriented GPT Planning. (arXiv:2311.12631v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12631</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in text-to-video generation have harnessed the power of
diffusion models to create visually compelling content conditioned on text
prompts. However, they usually encounter high computational costs and often
struggle to produce videos with coherent physical motions. To tackle these
issues, we propose GPT4Motion, a training-free framework that leverages the
planning capability of large language models such as GPT, the physical
simulation strength of Blender, and the excellent image generation ability of
text-to-image diffusion models to enhance the quality of video synthesis.
Specifically, GPT4Motion employs GPT-4 to generate a Blender script based on a
user textual prompt, which commands Blender&apos;s built-in physics engine to craft
fundamental scene components that encapsulate coherent physical motions across
frames. Then these components are inputted into Stable Diffusion to generate a
video aligned with the textual prompt. Experimental results on three basic
physical motion scenarios, including rigid object drop and collision, cloth
draping and swinging, and liquid flow, demonstrate that GPT4Motion can generate
high-quality videos efficiently in maintaining motion coherency and entity
consistency. GPT4Motion offers new insights in text-to-video research,
enhancing its quality and broadening its horizon for future explorations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_J/0/1/0/all/0/1&quot;&gt;Jiaxi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Mingfu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiancheng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianzhuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yifan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yafei Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoxin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12639">
<title>KNVQA: A Benchmark for evaluation knowledge-based VQA. (arXiv:2311.12639v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12639</link>
<description rdf:parseType="Literal">&lt;p&gt;Within the multimodal field, large vision-language models (LVLMs) have made
significant progress due to their strong perception and reasoning capabilities
in the visual and language systems. However, LVLMs are still plagued by the two
critical issues of object hallucination and factual accuracy, which limit the
practicality of LVLMs in different scenarios. Furthermore, previous evaluation
methods focus more on the comprehension and reasoning of language content but
lack a comprehensive evaluation of multimodal interactions, thereby resulting
in potential limitations. To this end, we propose a novel KNVQA-Eval, which is
devoted to knowledge-based VQA task evaluation to reflect the factuality of
multimodal LVLMs. To ensure the robustness and scalability of the evaluation,
we develop a new KNVQA dataset by incorporating human judgment and perception,
aiming to evaluate the accuracy of standard answers relative to AI-generated
answers in knowledge-based VQA. This work not only comprehensively evaluates
the contextual information of LVLMs using reliable human annotations, but also
further analyzes the fine-grained capabilities of current methods to reveal
potential avenues for subsequent optimization of LVLMs-based estimators. Our
proposed VQA-Eval and corresponding dataset KNVQA will facilitate the
development of automatic evaluation tools with the advantages of low cost,
privacy protection, and reproducibility. Our code will be released upon
publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sirui Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiayi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_M/0/1/0/all/0/1&quot;&gt;Muchen Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12641">
<title>Polyhedral Object Recognition by Indexing. (arXiv:2311.12641v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12641</link>
<description rdf:parseType="Literal">&lt;p&gt;In computer vision, the indexing problem is the problem of recognizing a few
objects in a large database of objects while avoiding the help of the classical
image-feature-to-object-feature matching paradigm. In this paper we address the
problem of recognizing 3-D polyhedral objects from 2-D images by indexing. Both
the objects to be recognized and the images are represented by weighted graphs.
The indexing problem is therefore the problem of determining whether a graph
extracted from the image is present or absent in a database of model graphs. We
introduce a novel method for performing this graph indexing process which is
based both on polynomial characterization of binary and weighted graphs and on
hashing. We describe in detail this polynomial characterization and then we
show how it can be used in the context of polyhedral object recognition. Next
we describe a practical recognition-by-indexing system that includes the
organization of the database, the representation of polyhedral objects in terms
of 2-D characteristic views, the representation of this views in terms of
weighted graphs, and the associated image processing. Finally, some
experimental results allow the evaluation of the system performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sossa_H/0/1/0/all/0/1&quot;&gt;Humberto Sossa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12651">
<title>Mobile-Seed: Joint Semantic Segmentation and Boundary Detection for Mobile Robots. (arXiv:2311.12651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12651</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise and rapid delineation of sharp boundaries and robust semantics is
essential for numerous downstream robotic tasks, such as robot grasping and
manipulation, real-time semantic mapping, and online sensor calibration
performed on edge computing units. Although boundary detection and semantic
segmentation are complementary tasks, most studies focus on lightweight models
for semantic segmentation but overlook the critical role of boundary detection.
In this work, we introduce Mobile-Seed, a lightweight, dual-task framework
tailored for simultaneous semantic segmentation and boundary detection. Our
framework features a two-stream encoder, an active fusion decoder (AFD) and a
dual-task regularization approach. The encoder is divided into two pathways:
one captures category-aware semantic information, while the other discerns
boundaries from multi-scale features. The AFD module dynamically adapts the
fusion of semantic and boundary information by learning channel-wise
relationships, allowing for precise weight assignment of each channel.
Furthermore, we introduce a regularization loss to mitigate the conflicts in
dual-task learning and deep diversity supervision. Compared to existing
methods, the proposed Mobile-Seed offers a lightweight framework to
simultaneously improve semantic segmentation performance and accurately locate
object boundaries. Experiments on the Cityscapes dataset have shown that
Mobile-Seed achieves notable improvement over the state-of-the-art (SOTA)
baseline by 2.2 percentage points (pp) in mIoU and 4.2 pp in mF-score, while
maintaining an online inference speed of 23.9 frames-per-second (FPS) with
1024x2048 resolution input on an RTX 2080 Ti GPU. Additional experiments on
CamVid and PASCAL Context datasets confirm our method&apos;s generalizability. Code
and additional results are publicly available at
\url{https://martin-liao.github.io/Mobile-Seed/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_Y/0/1/0/all/0/1&quot;&gt;Youqi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Shuhao Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bisheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xieyuanli Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12655">
<title>Hand-Eye Calibration. (arXiv:2311.12655v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12655</link>
<description rdf:parseType="Literal">&lt;p&gt;Whenever a sensor is mounted on a robot hand it is important to know the
relationship between the sensor and the hand. The problem of determining this
relationship is referred to as hand-eye calibration, which is important in at
least two types of tasks: (i) map sensor centered measurements into the robot
workspace and (ii) allow the robot to precisely move the sensor. In the past
some solutions were proposed in the particular case of a camera. With almost no
exception, all existing solutions attempt to solve the homogeneous matrix
equation AX=XB. First we show that there are two possible formulations of the
hand-eye calibration problem. One formulation is the classical one that we just
mentioned. A second formulation takes the form of the following homogeneous
matrix equation: MY=M&apos;YB. The advantage of the latter is that the extrinsic and
intrinsic camera parameters need not be made explicit. Indeed, this formulation
directly uses the 3 by 4 perspective matrices (M and M&apos;) associated with two
positions of the camera. Moreover, this formulation together with the classical
one cover a wider range of camera-based sensors to be calibrated with respect
to the robot hand. Second, we develop a common mathematical framework to solve
for the hand-eye calibration problem using either of the two formulations. We
present two methods, (i) a rotation then translation and (ii) a non-linear
solver for rotation and translation. Third, we perform a stability analysis
both for our two methods and for the classical linear method developed. In the
light of this comparison, the non-linear optimization method, that solves for
rotation and translation simultaneously, seems to be the most robust one with
respect to noise and to measurement errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dornaika_F/0/1/0/all/0/1&quot;&gt;Fadi Dornaika&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12660">
<title>Visually Guided Object Grasping. (arXiv:2311.12660v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12660</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present a visual servoing approach to the problem of object
grasping and more generally, to the problem of aligning an end-effector with an
object. First we extend the method proposed by Espiau et al. [1] to the case of
a camera which is not mounted onto the robot being controlled and we stress the
importance of the real-time estimation of the image Jacobian. Second, we show
how to represent a grasp or more generally, an alignment between two solids in
3-D projective space using an uncalibrated stereo rig. Such a 3-D projective
representation is view-invariant in the sense that it can be easily mapped into
an image set-point without any knowledge about the camera parameters. Third, we
perform an analysis of the performances of the visual servoing algorithm and of
the grasping precision that can be expected from this type of approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dornaika_F/0/1/0/all/0/1&quot;&gt;Fadi Dornaika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espiau_B/0/1/0/all/0/1&quot;&gt;Bernard Espiau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12663">
<title>Similar Document Template Matching Algorithm. (arXiv:2311.12663v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12663</link>
<description rdf:parseType="Literal">&lt;p&gt;This study outlines a comprehensive methodology for verifying medical
documents, integrating advanced techniques in template extraction, comparison,
and fraud detection. It begins with template extraction using sophisticated
region-of-interest (ROI) methods, incorporating contour analysis and edge
identification. Pre-processing steps ensure template clarity through
morphological operations and adaptive thresholding. The template comparison
algorithm utilizes advanced feature matching with key points and descriptors,
enhancing robustness through histogram-based analysis for accounting
variations. Fraud detection involves the SSIM computation and OCR for textual
information extraction. The SSIM quantifies structural similarity, aiding in
potential match identification. OCR focuses on critical areas like patient
details, provider information, and billing amounts. Extracted information is
compared with a reference dataset, and confidence thresholding ensures reliable
fraud detection. Adaptive parameters enhance system flexibility for dynamic
adjustments to varying document layouts. This methodology provides a robust
approach to medical document verification, addressing complexities in template
extraction, comparison, fraud detection, and adaptability to diverse document
structures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yenigalla_H/0/1/0/all/0/1&quot;&gt;Harshitha Yenigalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_B/0/1/0/all/0/1&quot;&gt;Bommareddy Revanth Srinivasa Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahul_B/0/1/0/all/0/1&quot;&gt;Batta Venkata Rahul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raju_N/0/1/0/all/0/1&quot;&gt;Nannapuraju Hemanth Raju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12679">
<title>BundleMoCap: Efficient, Robust and Smooth Motion Capture from Sparse Multiview Videos. (arXiv:2311.12679v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12679</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing smooth motions from videos using markerless techniques typically
involves complex processes such as temporal constraints, multiple stages with
data-driven regression and optimization, and bundle solving over temporal
windows. These processes can be inefficient and require tuning multiple
objectives across stages. In contrast, BundleMoCap introduces a novel and
efficient approach to this problem. It solves the motion capture task in a
single stage, eliminating the need for temporal smoothness objectives while
still delivering smooth motions. BundleMoCap outperforms the state-of-the-art
without increasing complexity. The key concept behind BundleMoCap is manifold
interpolation between latent keyframes. By relying on a local manifold
smoothness assumption, we can efficiently solve a bundle of frames using a
single code. Additionally, the method can be implemented as a sliding window
optimization and requires only the first frame to be properly initialized,
reducing the overall computational burden. BundleMoCap&apos;s strength lies in its
ability to achieve high-quality motion capture results with simplicity and
efficiency. More details can be found at https://moverseai.github.io/bundle/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albanis_G/0/1/0/all/0/1&quot;&gt;Georgios Albanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zioulis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Zioulis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolomvatsos_K/0/1/0/all/0/1&quot;&gt;Kostas Kolomvatsos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12682">
<title>Transferring to Real-World Layouts: A Depth-aware Framework for Scene Adaptation. (arXiv:2311.12682v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12682</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene segmentation via unsupervised domain adaptation (UDA) enables the
transfer of knowledge acquired from source synthetic data to real-world target
data, which largely reduces the need for manual pixel-level annotations in the
target domain. To facilitate domain-invariant feature learning, existing
methods typically mix data from both the source domain and target domain by
simply copying and pasting the pixels. Such vanilla methods are usually
sub-optimal since they do not take into account how well the mixed layouts
correspond to real-world scenarios. Real-world scenarios are with an inherent
layout. We observe that semantic categories, such as sidewalks, buildings, and
sky, display relatively consistent depth distributions, and could be clearly
distinguished in a depth map. Based on such observation, we propose a
depth-aware framework to explicitly leverage depth estimation to mix the
categories and facilitate the two complementary tasks, i.e., segmentation and
depth learning in an end-to-end manner. In particular, the framework contains a
Depth-guided Contextual Filter (DCF) forndata augmentation and a cross-task
encoder for contextual learning. DCF simulates the real-world layouts, while
the cross-task encoder further adaptively fuses the complementing features
between two tasks. Besides, it is worth noting that several public datasets do
not provide depth annotation. Therefore, we leverage the off-the-shelf depth
estimation network to generate the pseudo depth. Extensive experiments show
that our proposed methods, even with pseudo depth, achieve competitive
performance on two widely-used bench-marks, i.e. 77.7 mIoU on GTA to Cityscapes
and 69.3 mIoU on Synthia to Cityscapes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12704">
<title>Cascade Learning Localises Discriminant Features in Visual Scene Classification. (arXiv:2311.12704v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12704</link>
<description rdf:parseType="Literal">&lt;p&gt;Lack of interpretability of deep convolutional neural networks (DCNN) is a
well-known problem particularly in the medical domain as clinicians want
trustworthy automated decisions. One way to improve trust is to demonstrate the
localisation of feature representations with respect to expert labeled regions
of interest. In this work, we investigate the localisation of features learned
via two varied learning paradigms and demonstrate the superiority of one
learning approach with respect to localisation. Our analysis on medical and
natural datasets show that the traditional end-to-end (E2E) learning strategy
has a limited ability to localise discriminative features across multiple
network layers. We show that a layer-wise learning strategy, namely cascade
learning (CL), results in more localised features. Considering localisation
accuracy, we not only show that CL outperforms E2E but that it is a promising
method of predicting regions. On the YOLO object detection framework, our best
result shows that CL outperforms the E2E scheme by $2\%$ in mAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrahi_K/0/1/0/all/0/1&quot;&gt;Katayoun Farrahi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12722">
<title>Attacking Motion Planners Using Adversarial Perception Errors. (arXiv:2311.12722v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.12722</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving (AD) systems are often built and tested in a modular
fashion, where the performance of different modules is measured using
task-specific metrics. These metrics should be chosen so as to capture the
downstream impact of each module and the performance of the system as a whole.
For example, high perception quality should enable prediction and planning to
be performed safely. Even though this is true in general, we show here that it
is possible to construct planner inputs that score very highly on various
perception quality metrics but still lead to planning failures. In an analogy
to adversarial attacks on image classifiers, we call such inputs
\textbf{adversarial perception errors} and show they can be systematically
constructed using a simple boundary-attack algorithm. We demonstrate the
effectiveness of this algorithm by finding attacks for two different black-box
planners in several urban and highway driving scenarios using the CARLA
simulator. Finally, we analyse the properties of these attacks and show that
they are isolated in the input space of the planner, and discuss their
implications for AD system deployment and testing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadeghi_J/0/1/0/all/0/1&quot;&gt;Jonathan Sadeghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lord_N/0/1/0/all/0/1&quot;&gt;Nicholas A. Lord&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redford_J/0/1/0/all/0/1&quot;&gt;John Redford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_R/0/1/0/all/0/1&quot;&gt;Romain Mueller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12751">
<title>Towards Natural Language-Guided Drones: GeoText-1652 Benchmark with Spatially Relation Matching. (arXiv:2311.12751v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12751</link>
<description rdf:parseType="Literal">&lt;p&gt;Drone navigation through natural language commands remains a significant
challenge due to the lack of publicly available multi-modal datasets and the
intricate demands of fine-grained visual-text alignment. In response to this
pressing need, we present a new human-computer interaction annotation benchmark
called GeoText-1652, meticulously curated through a robust Large Language Model
(LLM)-based data generation framework and the expertise of pre-trained vision
models. This new dataset seamlessly extends the existing image dataset, \ie,
University-1652, with spatial-aware text annotations, encompassing intricate
image-text-bounding box associations. Besides, we introduce a new optimization
objective to leverage fine-grained spatial associations, called blending
spatial matching, for region-level spatial relation matching. Extensive
experiments reveal that our approach maintains an exceptional recall rate under
varying description complexities. This underscores the promising potential of
our approach in elevating drone control and navigation through the seamless
integration of natural language commands in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_M/0/1/0/all/0/1&quot;&gt;Meng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhedong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12754">
<title>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction. (arXiv:2311.12754v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12754</link>
<description rdf:parseType="Literal">&lt;p&gt;3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird&apos;s eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on Occ3D. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12760">
<title>High-resolution Image-based Malware Classification using Multiple Instance Learning. (arXiv:2311.12760v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2311.12760</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel method of classifying malware into families using
high-resolution greyscale images and multiple instance learning to overcome
adversarial binary enlargement. Current methods of visualisation-based malware
classification largely rely on lossy transformations of inputs such as resizing
to handle the large, variable-sized images. Through empirical analysis and
experimentation, it is shown that these approaches cause crucial information
loss that can be exploited. The proposed solution divides the images into
patches and uses embedding-based multiple instance learning with a
convolutional neural network and an attention aggregation function for
classification. The implementation is evaluated on the Microsoft Malware
Classification dataset and achieves accuracies of up to $96.6\%$ on
adversarially enlarged samples compared to the baseline of $22.8\%$. The Python
code is available online at https://github.com/timppeters/MIL-Malware-Images .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_T/0/1/0/all/0/1&quot;&gt;Tim Peters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhat_H/0/1/0/all/0/1&quot;&gt;Hikmat Farhat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12764">
<title>Investigating Weight-Perturbed Deep Neural Networks With Application in Iris Presentation Attack Detection. (arXiv:2311.12764v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12764</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) exhibit superior performance in various machine
learning tasks, e.g., image classification, speech recognition, biometric
recognition, object detection, etc. However, it is essential to analyze their
sensitivity to parameter perturbations before deploying them in real-world
applications. In this work, we assess the sensitivity of DNNs against
perturbations to their weight and bias parameters. The sensitivity analysis
involves three DNN architectures (VGG, ResNet, and DenseNet), three types of
parameter perturbations (Gaussian noise, weight zeroing, and weight scaling),
and two settings (entire network and layer-wise). We perform experiments in the
context of iris presentation attack detection and evaluate on two publicly
available datasets: LivDet-Iris-2017 and LivDet-Iris-2020. Based on the
sensitivity analysis, we propose improved models simply by perturbing
parameters of the network without undergoing training. We further combine these
perturbed models at the score-level and at the parameter-level to improve the
performance over the original model. The ensemble at the parameter-level shows
an average improvement of 43.58% on the LivDet-Iris-2017 dataset and 9.25% on
the LivDet-Iris-2020 dataset. The source code is available at
\href{https://github.com/redwankarimsony/WeightPerturbation-MSU}{https://github.com/redwankarimsony/WeightPerturbation-MSU}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Renu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sony_R/0/1/0/all/0/1&quot;&gt;Redwan Sony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12770">
<title>Swift Parameter-free Attention Network for Efficient Super-Resolution. (arXiv:2311.12770v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.12770</link>
<description rdf:parseType="Literal">&lt;p&gt;Single Image Super-Resolution (SISR) is a crucial task in low-level computer
vision, aiming to reconstruct high-resolution images from low-resolution
counterparts. Conventional attention mechanisms have significantly improved
SISR performance but often result in complex network structures and large
number of parameters, leading to slow inference speed and large model size. To
address this issue, we propose the Swift Parameter-free Attention Network
(SPAN), a highly efficient SISR model that balances parameter count, inference
speed, and image quality. SPAN employs a novel parameter-free attention
mechanism, which leverages symmetric activation functions and residual
connections to enhance high-contribution information and suppress redundant
information. Our theoretical analysis demonstrates the effectiveness of this
design in achieving the attention mechanism&apos;s purpose. We evaluate SPAN on
multiple benchmarks, showing that it outperforms existing efficient
super-resolution models in terms of both image quality and inference speed,
achieving a significant quality-speed trade-off. This makes SPAN highly
suitable for real-world applications, particularly in resource-constrained
scenarios. Notably, our model attains the best PSNR of 27.09 dB, and the test
runtime of our team is reduced by 7.08ms in the NTIRE 2023 efficient
super-resolution challenge. Our code and models are made publicly available at
\url{https://github.com/hongyuanyu/SPAN}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_C/0/1/0/all/0/1&quot;&gt;Cheng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongyuan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yihang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yajun Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuqing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xuanwu Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zuo_K/0/1/0/all/0/1&quot;&gt;Kunlong Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12773">
<title>Iris Presentation Attack: Assessing the Impact of Combining Vanadium Dioxide Films with Artificial Eyes. (arXiv:2311.12773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12773</link>
<description rdf:parseType="Literal">&lt;p&gt;Iris recognition systems, operating in the near infrared spectrum (NIR), have
demonstrated vulnerability to presentation attacks, where an adversary uses
artifacts such as cosmetic contact lenses, artificial eyes or printed iris
images in order to circumvent the system. At the same time, a number of
effective presentation attack detection (PAD) methods have been developed.
These methods have demonstrated success in detecting artificial eyes (e.g.,
fake Van Dyke eyes) as presentation attacks. In this work, we seek to alter the
optical characteristics of artificial eyes by affixing Vanadium Dioxide (VO2)
films on their surface in various spatial configurations. VO2 films can be used
to selectively transmit NIR light and can, therefore, be used to regulate the
amount of NIR light from the object that is captured by the iris sensor. We
study the impact of such images produced by the sensor on two state-of-the-art
iris PA detection methods. We observe that the addition of VO2 films on the
surface of artificial eyes can cause the PA detection methods to misclassify
them as bonafide eyes in some cases. This represents a vulnerability that must
be systematically analyzed and effectively addressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jauhari_D/0/1/0/all/0/1&quot;&gt;Darshika Jauhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_R/0/1/0/all/0/1&quot;&gt;Renu Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cunjian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sepulveda_N/0/1/0/all/0/1&quot;&gt;Nelson Sepulveda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12775">
<title>SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering. (arXiv:2311.12775v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2311.12775</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method to allow precise and extremely fast mesh extraction from
3D Gaussian Splatting. Gaussian Splatting has recently become very popular as
it yields realistic rendering while being significantly faster to train than
NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D
gaussians as these gaussians tend to be unorganized after optimization and no
method has been proposed so far. Our first key contribution is a regularization
term that encourages the gaussians to align well with the surface of the scene.
We then introduce a method that exploits this alignment to extract a mesh from
the Gaussians using Poisson reconstruction, which is fast, scalable, and
preserves details, in contrast to the Marching Cubes algorithm usually applied
to extract meshes from Neural SDFs. Finally, we introduce an optional
refinement strategy that binds gaussians to the surface of the mesh, and
jointly optimizes these Gaussians and the mesh through Gaussian splatting
rendering. This enables easy editing, sculpting, rigging, animating,
compositing and relighting of the Gaussians using traditional softwares by
manipulating the mesh instead of the gaussians themselves. Retrieving such an
editable mesh for realistic rendering is done within minutes with our method,
compared to hours with the state-of-the-art methods on neural SDFs, while
providing a better rendering quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guedon_A/0/1/0/all/0/1&quot;&gt;Antoine Gu&amp;#xe9;don&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepetit_V/0/1/0/all/0/1&quot;&gt;Vincent Lepetit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12792">
<title>Intrinsic Image Decomposition via Ordinal Shading. (arXiv:2311.12792v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12792</link>
<description rdf:parseType="Literal">&lt;p&gt;Intrinsic decomposition is a fundamental mid-level vision problem that plays
a crucial role in various inverse rendering and computational photography
pipelines. Generating highly accurate intrinsic decompositions is an inherently
under-constrained task that requires precisely estimating continuous-valued
shading and albedo. In this work, we achieve high-resolution intrinsic
decomposition by breaking the problem into two parts. First, we present a dense
ordinal shading formulation using a shift- and scale-invariant loss in order to
estimate ordinal shading cues without restricting the predictions to obey the
intrinsic model. We then combine low- and high-resolution ordinal estimations
using a second network to generate a shading estimate with both global
coherency and local details. We encourage the model to learn an accurate
decomposition by computing losses on the estimated shading as well as the
albedo implied by the intrinsic model. We develop a straightforward method for
generating dense pseudo ground truth using our model&apos;s predictions and
multi-illumination data, enabling generalization to in-the-wild imagery. We
present an exhaustive qualitative and quantitative analysis of our predicted
intrinsic components against state-of-the-art methods. Finally, we demonstrate
the real-world applicability of our estimations by performing otherwise
difficult editing tasks such as recoloring and relighting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Careaga_C/0/1/0/all/0/1&quot;&gt;Chris Careaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksoy_Y/0/1/0/all/0/1&quot;&gt;Ya&amp;#x11f;&amp;#x131;z Aksoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12793">
<title>ShareGPT4V: Improving Large Multi-Modal Models with Better Captions. (arXiv:2311.12793v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.12793</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of large multi-modal models (LMMs), efficient modality alignment
is crucial yet often constrained by the scarcity of high-quality image-text
data. To address this bottleneck, we introduce the ShareGPT4V dataset, a
pioneering large-scale resource featuring 1.2 million highly descriptive
captions, which surpasses existing datasets in diversity and information
content, covering world knowledge, object properties, spatial relationships,
and aesthetic evaluations. Specifically, ShareGPT4V originates from a curated
100K high-quality captions collected from advanced GPT4-Vision and has been
expanded to 1.2M with a superb caption model trained on this subset. ShareGPT4V
first demonstrates its effectiveness for the Supervised Fine-Tuning (SFT)
phase, by substituting an equivalent quantity of detailed captions in existing
SFT datasets with a subset of our high-quality captions, significantly
enhancing the LMMs like LLaVA-7B, LLaVA-1.5-13B, and Qwen-VL-Chat-7B on the MME
and MMBench benchmarks, with respective gains of 222.8/22.0/22.3 and
2.7/1.3/1.5. We further incorporate ShareGPT4V data into both the pre-training
and SFT phases, obtaining ShareGPT4V-7B, a superior LMM based on a simple
architecture that has remarkable performance across a majority of the
multi-modal benchmarks. This project is available at
https://ShareGPT4V.github.io to serve as a pivotal resource for advancing the
LMMs community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jisong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaoyi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Conghui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Feng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1810.12813">
<title>Contextual Hourglass Network for Semantic Segmentation of High Resolution Aerial Imagery. (arXiv:1810.12813v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1810.12813</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentation for aerial imagery is a challenging and important
problem in remotely sensed imagery analysis. In recent years, with the success
of deep learning, various convolutional neural network (CNN) based models have
been developed. However, due to the varying sizes of the objects and imbalanced
class labels, it can be challenging to obtain accurate pixel-wise semantic
segmentation results. To address those challenges, we develop a novel semantic
segmentation method and call it Contextual Hourglass Network. In our method, in
order to improve the robustness of the prediction, we design a new contextual
hourglass module which incorporates attention mechanism on processed
low-resolution featuremaps to exploit the contextual semantics. We further
exploit the stacked encoder-decoder structure by connecting multiple contextual
hourglass modules from end to end. This architecture can effectively extract
rich multi-scale features and add more feedback loops for better learning
contextual semantics through intermediate supervision. To demonstrate the
efficacy of our semantic segmentation method, we test it on Potsdam and
Vaihingen datasets. Through the comparisons to other baseline methods, our
method yields the best results on overall performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Panfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Youzuo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schultz_Fellenz_E/0/1/0/all/0/1&quot;&gt;Emily Schultz-Fellenz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2004.07780">
<title>Shortcut Learning in Deep Neural Networks. (arXiv:2004.07780v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2004.07780</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has triggered the current rise of artificial intelligence and
is the workhorse of today&apos;s machine intelligence. Numerous success stories have
rapidly spread all over science, industry and society, but its limitations have
only recently come into focus. In this perspective we seek to distill how many
of deep learning&apos;s problems can be seen as different symptoms of the same
underlying problem: shortcut learning. Shortcuts are decision rules that
perform well on standard benchmarks but fail to transfer to more challenging
testing conditions, such as real-world scenarios. Related issues are known in
Comparative Psychology, Education and Linguistics, suggesting that shortcut
learning may be a common characteristic of learning systems, biological and
artificial alike. Based on these observations, we develop a set of
recommendations for model interpretation and benchmarking, highlighting recent
advances in machine learning to improve robustness and transferability from the
lab to real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geirhos_R/0/1/0/all/0/1&quot;&gt;Robert Geirhos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobsen_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rn-Henrik Jacobsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michaelis_C/0/1/0/all/0/1&quot;&gt;Claudio Michaelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zemel_R/0/1/0/all/0/1&quot;&gt;Richard Zemel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wichmann_F/0/1/0/all/0/1&quot;&gt;Felix A. Wichmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.12311">
<title>Influencer Videos: Unboxing the Mystique. (arXiv:2012.12311v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2012.12311</link>
<description rdf:parseType="Literal">&lt;p&gt;Influencer marketing has become a very popular tool to reach customers.
Despite the rapid growth in influencer videos, there has been little research
on the effectiveness of their constituent features in explaining video
engagement. We study YouTube influencers and analyze their unstructured video
data across text, audio and images using an &quot;interpretable deep learning&quot;
framework that accomplishes both goals of prediction and interpretation. Our
prediction-based approach analyzes unstructured data and finds that &quot;what is
said&quot; in words (text) is more influential than &quot;how it is said&quot; in imagery
(images) or acoustics (audio). Our novel interpretation-based approach is
implemented after completion of model prediction by analyzing the same source
of unstructured data to measure importance attributed to the video features. We
eliminate several spurious relationships in two steps, identifying a subset of
relationships which are confirmed using theory. We uncover novel findings that
establish distinct associations for measures of shallow and deep engagement
based on the dual-system framework of human thinking. Our approach is validated
using simulated data, and we discuss the learnings from our findings for
influencers and brands.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajaram_P/0/1/0/all/0/1&quot;&gt;Prashant Rajaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manchanda_P/0/1/0/all/0/1&quot;&gt;Puneet Manchanda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.13788">
<title>PFENet++: Boosting Few-shot Semantic Segmentation with the Noise-filtered Context-aware Prior Mask. (arXiv:2109.13788v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.13788</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we revisit the prior mask guidance proposed in ``Prior Guided
Feature Enrichment Network for Few-Shot Segmentation&apos;&apos;. The prior mask serves
as an indicator that highlights the region of interests of unseen categories,
and it is effective in achieving better performance on different frameworks of
recent studies. However, the current method directly takes the maximum
element-to-element correspondence between the query and support features to
indicate the probability of belonging to the target class, thus the broader
contextual information is seldom exploited during the prior mask generation. To
address this issue, first, we propose the Context-aware Prior Mask (CAPM) that
leverages additional nearby semantic cues for better locating the objects in
query images. Second, since the maximum correlation value is vulnerable to
noisy features, we take one step further by incorporating a lightweight Noise
Suppression Module (NSM) to screen out the unnecessary responses, yielding
high-quality masks for providing the prior knowledge. Both two contributions
are experimentally shown to have substantial practical merit, and the new model
named PFENet++ significantly outperforms the baseline PFENet as well as all
other competitors on three challenging benchmarks PASCAL-5$^i$, COCO-20$^i$ and
FSS-1000. The new state-of-the-art performance is achieved without compromising
the efficiency, manifesting the potential for being a new strong baseline in
few-shot semantic segmentation. Our code will be available at
https://github.com/luoxiaoliu/PFENet2Plus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiaoliu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Z/0/1/0/all/0/1&quot;&gt;Zhuotao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Taiping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yuan Yan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiaya Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.11057">
<title>Learning to Aggregate Multi-Scale Context for Instance Segmentation in Remote Sensing Images. (arXiv:2111.11057v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.11057</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of instance segmentation in remote sensing images, aiming at
performing per-pixel labeling of objects at instance level, is of great
importance for various civil applications. Despite previous successes, most
existing instance segmentation methods designed for natural images encounter
sharp performance degradations when they are directly applied to top-view
remote sensing images. Through careful analysis, we observe that the challenges
mainly come from the lack of discriminative object features due to severe scale
variations, low contrasts, and clustered distributions. In order to address
these problems, a novel context aggregation network (CATNet) is proposed to
improve the feature extraction process. The proposed model exploits three
lightweight plug-and-play modules, namely dense feature pyramid network
(DenseFPN), spatial context pyramid (SCP), and hierarchical region of interest
extractor (HRoIE), to aggregate global visual context at feature, spatial, and
instance domains, respectively. DenseFPN is a multi-scale feature propagation
module that establishes more flexible information flows by adopting inter-level
residual connections, cross-level dense connections, and feature re-weighting
strategy. Leveraging the attention mechanism, SCP further augments the features
by aggregating global spatial context into local regions. For each instance,
HRoIE adaptively generates RoI features for different downstream tasks.
Extensive evaluations of the proposed scheme on iSAID, DIOR, NWPU VHR-10, and
HRSID datasets demonstrate that the proposed approach outperforms
state-of-the-arts under similar computational costs. Source code and
pre-trained models are available at https://github.com/yeliudev/CATNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huifang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_C/0/1/0/all/0/1&quot;&gt;Chao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shuang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chang Wen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10996">
<title>ProtoCLIP: Prototypical Contrastive Language Image Pretraining. (arXiv:2206.10996v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10996</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language Image Pretraining (CLIP) has received widespread
attention, since its learned representations can be transferred well to various
downstream tasks. During the training process of the CLIP model, the InfoNCE
objective aligns positive image-text pairs and separates negative ones. We show
an underlying representation grouping effect during this process: the InfoNCE
objective indirectly groups semantically similar representations together via
randomly emerged within-modal anchors. Based on this understanding, in this
paper, Prototypical Contrastive Language Image Pretraining (ProtoCLIP) is
introduced to enhance such grouping by boosting its efficiency and increasing
its robustness against the modality gap. Specifically, ProtoCLIP sets up
prototype-level discrimination between image and text spaces, which efficiently
transfers higher-level structural knowledge. Further, Prototypical Back
Translation (PBT) is proposed to decouple representation grouping from
representation alignment, resulting in effective learning of meaningful
representations under large modality gap. The PBT also enables us to introduce
additional external teachers with richer prior language knowledge. ProtoCLIP is
trained with an online episodic training strategy, which makes it can be scaled
up to unlimited amounts of data. We train our ProtoCLIP on Conceptual Captions
and achieved an +5.81% ImageNet linear probing improvement and an +2.01%
ImageNet zero-shot classification improvement. On the larger YFCC-15M dataset,
ProtoCLIP matches the performance of CLIP with 33% of training time. Codes are
available at https://github.com/megvii-research/protoclip.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Delong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zaiquan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Huaxi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Ying Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_E/0/1/0/all/0/1&quot;&gt;Erjin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.14372">
<title>Formalizing and Evaluating Requirements of Perception Systems for Automated Vehicles using Spatio-Temporal Perception Logic. (arXiv:2206.14372v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2206.14372</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated vehicles (AV) heavily depend on robust perception systems. Current
methods for evaluating vision systems focus mainly on frame-by-frame
performance. Such evaluation methods appear to be inadequate in assessing the
performance of a perception subsystem when used within an AV. In this paper, we
present a logic -- referred to as Spatio-Temporal Perception Logic (STPL) --
which utilizes both spatial and temporal modalities. STPL enables reasoning
over perception data using spatial and temporal operators. One major advantage
of STPL is that it facilitates basic sanity checks on the functional
performance of the perception system, even without ground-truth data in some
cases. We identify a fragment of STPL which is efficiently monitorable offline
in polynomial time. Finally, we present a range of specifications for AV
perception systems to highlight the types of requirements that can be expressed
and analyzed through offline monitoring with STPL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hekmatnejad_M/0/1/0/all/0/1&quot;&gt;Mohammad Hekmatnejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoxha_B/0/1/0/all/0/1&quot;&gt;Bardh Hoxha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_J/0/1/0/all/0/1&quot;&gt;Jyotirmoy V. Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yezhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fainekos_G/0/1/0/all/0/1&quot;&gt;Georgios Fainekos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04934">
<title>Multi-level Geometric Optimization for Regularised Constrained Linear Inverse Problems. (arXiv:2207.04934v2 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04934</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a geometric multilevel optimization approach that smoothly
incorporates box constraints. Given a box constrained optimization problem, we
consider a hierarchy of models with varying discretization levels. Finer models
are accurate but expensive to compute, while coarser models are less accurate
but cheaper to compute. When working at the fine level, multilevel optimisation
computes the search direction based on a coarser model which speeds up updates
at the fine level. Moreover, exploiting geometry induced by the hierarchy the
feasibility of the updates is preserved. In particular, our approach extends
classical components of multigrid methods like restriction and prolongation to
the Riemannian structure of our constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Muller_S/0/1/0/all/0/1&quot;&gt;Sebastian M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Petra_S/0/1/0/all/0/1&quot;&gt;Stefania Petra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zisler_M/0/1/0/all/0/1&quot;&gt;Matthias Zisler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04717">
<title>Cascaded and Generalizable Neural Radiance Fields for Fast View Synthesis. (arXiv:2208.04717v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04717</link>
<description rdf:parseType="Literal">&lt;p&gt;We present CG-NeRF, a cascade and generalizable neural radiance fields method
for view synthesis. Recent generalizing view synthesis methods can render
high-quality novel views using a set of nearby input views. However, the
rendering speed is still slow due to the nature of uniformly-point sampling of
neural radiance fields. Existing scene-specific methods can train and render
novel views efficiently but can not generalize to unseen data. Our approach
addresses the problems of fast and generalizing view synthesis by proposing two
novel modules: a coarse radiance fields predictor and a convolutional-based
neural renderer. This architecture infers consistent scene geometry based on
the implicit neural fields and renders new views efficiently using a single
GPU. We first train CG-NeRF on multiple 3D scenes of the DTU dataset, and the
network can produce high-quality and accurate novel views on unseen real and
synthetic data using only photometric losses. Moreover, our method can leverage
a denser set of reference images of a single scene to produce accurate novel
views without relying on additional explicit representations and still
maintains the high-speed rendering of the pre-trained model. Experimental
results show that CG-NeRF outperforms state-of-the-art generalizable neural
rendering methods on various synthetic and real datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Ha_P/0/1/0/all/0/1&quot;&gt;Phong Nguyen-Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_L/0/1/0/all/0/1&quot;&gt;Lam Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahtu_E/0/1/0/all/0/1&quot;&gt;Esa Rahtu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matas_J/0/1/0/all/0/1&quot;&gt;Jiri Matas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heikkila_J/0/1/0/all/0/1&quot;&gt;Janne Heikkila&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09688">
<title>Learning Sub-Pixel Disparity Distribution for Light Field Depth Estimation. (arXiv:2208.09688v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09688</link>
<description rdf:parseType="Literal">&lt;p&gt;Light field (LF) depth estimation plays a crucial role in many LF-based
applications. Existing LF depth estimation methods consider depth estimation as
a regression problem, where a pixel-wise L1 loss is employed to supervise the
training process. However, the disparity map is only a sub-space projection
(i.e., an expectation) of the disparity distribution, which is essential for
models to learn. In this paper, we propose a simple yet effective method to
learn the sub-pixel disparity distribution by fully utilizing the power of deep
networks, especially for LF of narrow baselines. We construct the cost volume
at the sub-pixel level to produce a finer disparity distribution and design an
uncertainty-aware focal loss to supervise the predicted disparity distribution
toward the ground truth. Extensive experimental results demonstrate the
effectiveness of our method.Our method significantly outperforms recent
state-of-the-art LF depth algorithms on the HCI 4D LF Benchmark in terms of all
four accuracy metrics (i.e., BadPix 0.01, BadPix 0.03, BadPix 0.07, and MSE
$\times$100). The code and model of the proposed method are available at
\url{https://github.com/chaowentao/SubFocal}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_W/0/1/0/all/0/1&quot;&gt;Wentao Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuechun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yingqian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guanghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_F/0/1/0/all/0/1&quot;&gt;Fuqing Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08660">
<title>Learn the Time to Learn: Replay Scheduling in Continual Learning. (arXiv:2209.08660v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08660</link>
<description rdf:parseType="Literal">&lt;p&gt;Replay methods are known to be successful at mitigating catastrophic
forgetting in continual learning scenarios despite having limited access to
historical data. However, storing historical data is cheap in many real-world
settings, yet replaying all historical data is often prohibited due to
processing time constraints. In such settings, we propose that continual
learning systems should learn the time to learn and schedule which tasks to
replay at different time steps. We first demonstrate the benefits of our
proposal by using Monte Carlo tree search to find a proper replay schedule, and
show that the found replay schedules can outperform fixed scheduling policies
when combined with various replay methods in different continual learning
settings. Additionally, we propose a framework for learning replay scheduling
policies with reinforcement learning. We show that the learned policies can
generalize better in new continual learning scenarios compared to equally
replaying all seen tasks, without added computational cost. Our study reveals
the importance of learning the time to learn in continual learning, which
brings current research closer to real-world needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klasson_M/0/1/0/all/0/1&quot;&gt;Marcus Klasson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kjellstrom_H/0/1/0/all/0/1&quot;&gt;Hedvig Kjellstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cheng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08942">
<title>Differentially Private Optimizers Can Learn Adversarially Robust Models. (arXiv:2211.08942v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08942</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models have shone in a variety of domains and attracted
increasing attention from both the security and the privacy communities. One
important yet worrying question is: Will training models under the differential
privacy (DP) constraint have an unfavorable impact on their adversarial
robustness? While previous works have postulated that privacy comes at the cost
of worse robustness, we give the first theoretical analysis to show that DP
models can indeed be robust and accurate, even sometimes more robust than their
naturally-trained non-private counterparts. We observe three key factors that
influence the privacy-robustness-accuracy tradeoff: (1) hyper-parameters for DP
optimizers are critical; (2) pre-training on public data significantly
mitigates the accuracy and robustness drop; (3) choice of DP optimizers makes a
difference. With these factors set properly, we achieve 90\% natural accuracy,
72\% robust accuracy ($+9\%$ than the non-private model) under $l_2(0.5)$
attack, and 69\% robust accuracy ($+16\%$ than the non-private model) with
pre-trained SimCLRv2 model under $l_\infty(4/255)$ attack on CIFAR10 with
$\epsilon=2$. In fact, we show both theoretically and empirically that DP
models are Pareto optimal on the accuracy-robustness tradeoff. Empirically, the
robustness of DP models is consistently observed across various datasets and
models. We believe our encouraging results are a significant step towards
training models that are private as well as robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09945">
<title>VeriCompress: A Tool to Streamline the Synthesis of Verified Robust Compressed Neural Networks from Scratch. (arXiv:2211.09945v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.09945</link>
<description rdf:parseType="Literal">&lt;p&gt;AI&apos;s widespread integration has led to neural networks (NNs) deployment on
edge and similar limited-resource platforms for safety-critical scenarios. Yet,
NN&apos;s fragility raises concerns about reliable inference. Moreover, constrained
platforms demand compact networks. This study introduces VeriCompress, a tool
that automates the search and training of compressed models with robustness
guarantees. These models are well-suited for safety-critical applications and
adhere to predefined architecture and size limitations, making them deployable
on resource-restricted platforms. The method trains models 2-3 times faster
than the state-of-the-art approaches, surpassing relevant baseline approaches
by average accuracy and robustness gains of 15.1 and 9.8 percentage points,
respectively. When deployed on a resource-restricted generic platform, these
models require 5-8 times less memory and 2-4 times less inference time than
models used in verified robustness literature. Our comprehensive evaluation
across various model architectures and datasets, including MNIST, CIFAR, SVHN,
and a relevant pedestrian detection dataset, showcases VeriCompress&apos;s capacity
to identify compressed verified robust models with reduced computation overhead
compared to current standards. This underscores its potential as a valuable
tool for end users, such as developers of safety-critical applications on edge
or Internet of Things platforms, empowering them to create suitable models for
safety-critical, resource-constrained platforms in their respective domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaur_S/0/1/0/all/0/1&quot;&gt;Sawinder Kaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salekin_A/0/1/0/all/0/1&quot;&gt;Asif Salekin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13807">
<title>GEFF: Improving Any Clothes-Changing Person ReID Model using Gallery Enrichment with Face Features. (arXiv:2211.13807v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13807</link>
<description rdf:parseType="Literal">&lt;p&gt;In the Clothes-Changing Re-Identification (CC-ReID) problem, given a query
sample of a person, the goal is to determine the correct identity based on a
labeled gallery in which the person appears in different clothes. Several
models tackle this challenge by extracting clothes-independent features.
However, the performance of these models is still lower for the
clothes-changing setting compared to the same-clothes setting in which the
person appears with the same clothes in the labeled gallery. As
clothing-related features are often dominant features in the data, we propose a
new process we call Gallery Enrichment, to utilize these features. In this
process, we enrich the original gallery by adding to it query samples based on
their face features, using an unsupervised algorithm. Additionally, we show
that combining ReID and face feature extraction modules alongside an enriched
gallery results in a more accurate ReID model, even for query samples with new
outfits that do not include faces. Moreover, we claim that existing CC-ReID
benchmarks do not fully represent real-world scenarios, and propose a new video
CC-ReID dataset called 42Street, based on a theater play that includes crowded
scenes and numerous clothes changes. When applied to multiple ReID models, our
method (GEFF) achieves an average improvement of 33.5% and 6.7% in the Top-1
clothes-changing metric on the PRCC and LTCC benchmarks. Combined with the
latest ReID models, our method achieves new SOTA results on the PRCC, LTCC,
CCVID, LaST and VC-Clothes benchmarks and the proposed 42Street dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arkushin_D/0/1/0/all/0/1&quot;&gt;Daniel Arkushin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_B/0/1/0/all/0/1&quot;&gt;Bar Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peleg_S/0/1/0/all/0/1&quot;&gt;Shmuel Peleg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fried_O/0/1/0/all/0/1&quot;&gt;Ohad Fried&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15513">
<title>Composite Score for Anomaly Detection in Imbalanced Real-World Industrial Dataset. (arXiv:2211.15513v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15513</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the industrial sector has evolved towards its fourth
revolution. The quality control domain is particularly interested in advanced
machine learning for computer vision anomaly detection. Nevertheless, several
challenges have to be faced, including imbalanced datasets, the image
complexity, and the zero-false-negative (ZFN) constraint to guarantee the
high-quality requirement. This paper illustrates a use case for an industrial
partner, where Printed Circuit Board Assembly (PCBA) images are first
reconstructed with a Vector Quantized Generative Adversarial Network (VQGAN)
trained on normal products. Then, several multi-level metrics are extracted on
a few normal and abnormal images, highlighting anomalies through reconstruction
differences. Finally, a classifer is trained to build a composite anomaly score
thanks to the metrics extracted. This three-step approach is performed on the
public MVTec-AD datasets and on the partner PCBA dataset, where it achieves a
regular accuracy of 95.69% and 87.93% under the ZFN constraint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bougaham_A/0/1/0/all/0/1&quot;&gt;Arnaud Bougaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adoui_M/0/1/0/all/0/1&quot;&gt;Mohammed El Adoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linden_I/0/1/0/all/0/1&quot;&gt;Isabelle Linden&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frenay_B/0/1/0/all/0/1&quot;&gt;Beno&amp;#xee;t Fr&amp;#xe9;nay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02081">
<title>YolOOD: Utilizing Object Detection Concepts for Multi-Label Out-of-Distribution Detection. (arXiv:2212.02081v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02081</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection has attracted a large amount of attention
from the machine learning research community in recent years due to its
importance in deployed systems. Most of the previous studies focused on the
detection of OOD samples in the multi-class classification task. However, OOD
detection in the multi-label classification task, a more common real-world use
case, remains an underexplored domain. In this research, we propose YolOOD - a
method that utilizes concepts from the object detection domain to perform OOD
detection in the multi-label classification task. Object detection models have
an inherent ability to distinguish between objects of interest
(in-distribution) and irrelevant objects (e.g., OOD objects) in images that
contain multiple objects belonging to different class categories. These
abilities allow us to convert a regular object detection model into an image
classifier with inherent OOD detection capabilities with just minor changes. We
compare our approach to state-of-the-art OOD detection methods and demonstrate
YolOOD&apos;s ability to outperform these methods on a comprehensive suite of
in-distribution and OOD benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zolfi_A/0/1/0/all/0/1&quot;&gt;Alon Zolfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amit_G/0/1/0/all/0/1&quot;&gt;Guy Amit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baras_A/0/1/0/all/0/1&quot;&gt;Amit Baras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koda_S/0/1/0/all/0/1&quot;&gt;Satoru Koda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morikawa_I/0/1/0/all/0/1&quot;&gt;Ikuya Morikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabtai_A/0/1/0/all/0/1&quot;&gt;Asaf Shabtai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05258">
<title>Image augmentation with conformal mappings for a convolutional neural network. (arXiv:2212.05258v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05258</link>
<description rdf:parseType="Literal">&lt;p&gt;For augmentation of the square-shaped image data of a convolutional neural
network (CNN), we introduce a new method, in which the original images are
mapped onto a disk with a conformal mapping, rotated around the center of this
disk and mapped under such a M\&quot;obius transformation that preserves the disk,
and then mapped back onto their original square shape. This process does not
result the loss of information caused by removing areas from near the edges of
the original images unlike the typical transformations used in the data
augmentation for a CNN. We offer here the formulas of all the mappings needed
together with detailed instructions how to write a code for transforming the
images. The new method is also tested with simulated data and, according the
results, using this method to augment the training data of 10 images into 40
images decreases the amount of the error in the predictions by a CNN for a test
set of 160 images in a statistically significant way (p-value=0.0360).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rainio_O/0/1/0/all/0/1&quot;&gt;Oona Rainio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasser_M/0/1/0/all/0/1&quot;&gt;Mohamed M.S. Nasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuorinen_M/0/1/0/all/0/1&quot;&gt;Matti Vuorinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klen_R/0/1/0/all/0/1&quot;&gt;Riku Kl&amp;#xe9;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.05246">
<title>Online Class-Incremental Learning For Real-World Food Classification. (arXiv:2301.05246v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.05246</link>
<description rdf:parseType="Literal">&lt;p&gt;Food image classification is essential for monitoring health and tracking
dietary in image-based dietary assessment methods. However, conventional
systems often rely on static datasets with fixed classes and uniform
distribution. In contrast, real-world food consumption patterns, shaped by
cultural, economic, and personal influences, involve dynamic and evolving data.
Thus, require the classification system to cope with continuously evolving
data. Online Class Incremental Learning (OCIL) addresses the challenge of
learning continuously from a single-pass data stream while adapting to the new
knowledge and reducing catastrophic forgetting. Experience Replay (ER) based
OCIL methods store a small portion of previous data and have shown encouraging
performance. However, most existing OCIL works assume that the distribution of
encountered data is perfectly balanced, which rarely happens in real-world
scenarios. In this work, we explore OCIL for real-world food image
classification by first introducing a probabilistic framework to simulate
realistic food consumption scenarios. Subsequently, we present an attachable
Dynamic Model Update (DMU) module designed for existing ER methods, which
enables the selection of relevant images for model training, addressing
challenges arising from data repetition and imbalanced sample occurrences
inherent in realistic food consumption patterns within the OCIL framework. Our
performance evaluation demonstrates significant enhancements compared to
established ER methods, showing great potential for lifelong learning in
real-world food image classification scenarios. The code of our method is
publicly accessible at
\href{https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification}{https://gitlab.com/viper-purdue/OCIL-real-world-food-image-classification}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_S/0/1/0/all/0/1&quot;&gt;Siddeshwar Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiangpeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_F/0/1/0/all/0/1&quot;&gt;Fengqing Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03868">
<title>A Generalized Surface Loss for Reducing the Hausdorff Distance in Medical Imaging Segmentation. (arXiv:2302.03868v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03868</link>
<description rdf:parseType="Literal">&lt;p&gt;Within medical imaging segmentation, the Dice coefficient and Hausdorff-based
metrics are standard measures of success for deep learning models. However,
modern loss functions for medical image segmentation often only consider the
Dice coefficient or similar region-based metrics during training. As a result,
segmentation architectures trained over such loss functions run the risk of
achieving high accuracy for the Dice coefficient but low accuracy for
Hausdorff-based metrics. Low accuracy on Hausdorff-based metrics can be
problematic for applications such as tumor segmentation, where such benchmarks
are crucial. For example, high Dice scores accompanied by significant Hausdorff
errors could indicate that the predictions fail to detect small tumors. We
propose the Generalized Surface Loss function, a novel loss function to
minimize Hausdorff-based metrics with more desirable numerical properties than
current methods and with weighting terms for class imbalance. Our loss function
outperforms other losses when tested on the LiTS and BraTS datasets using the
state-of-the-art nnUNet architecture. These results suggest we can improve
medical imaging segmentation accuracy with our novel loss function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Celaya_A/0/1/0/all/0/1&quot;&gt;Adrian Celaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Riviere_B/0/1/0/all/0/1&quot;&gt;Beatrice Riviere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fuentes_D/0/1/0/all/0/1&quot;&gt;David Fuentes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10396">
<title>Assessing Domain Gap for Continual Domain Adaptation in Object Detection. (arXiv:2302.10396v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10396</link>
<description rdf:parseType="Literal">&lt;p&gt;To ensure reliable object detection in autonomous systems, the detector must
be able to adapt to changes in appearance caused by environmental factors such
as time of day, weather, and seasons. Continually adapting the detector to
incorporate these changes is a promising solution, but it can be
computationally costly. Our proposed approach is to selectively adapt the
detector only when necessary, using new data that does not have the same
distribution as the current training data. To this end, we investigate three
popular metrics for domain gap evaluation and find that there is a correlation
between the domain gap and detection accuracy. Therefore, we apply the domain
gap as a criterion to decide when to adapt the detector. Our experiments show
that our approach has the potential to improve the efficiency of the detector&apos;s
operation in real-world scenarios, where environmental conditions change in a
cyclical manner, without sacrificing the overall performance of the detector.
Our code is publicly available at https://github.com/dadung/DGE-CDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doan_A/0/1/0/all/0/1&quot;&gt;Anh-Dzung Doan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Bach Long Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Surabhi Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reid_I/0/1/0/all/0/1&quot;&gt;Ian Reid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Markus Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chin_T/0/1/0/all/0/1&quot;&gt;Tat-Jun Chin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01351">
<title>APARATE: Adaptive Adversarial Patch for CNN-based Monocular Depth Estimation for Autonomous Navigation. (arXiv:2303.01351v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01351</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent times, monocular depth estimation (MDE) has experienced significant
advancements in performance, largely attributed to the integration of
innovative architectures, i.e., convolutional neural networks (CNNs) and
Transformers. Nevertheless, the susceptibility of these models to adversarial
attacks has emerged as a noteworthy concern, especially in domains where safety
and security are paramount. This concern holds particular weight for MDE due to
its critical role in applications like autonomous driving and robotic
navigation, where accurate scene understanding is pivotal. To assess the
vulnerability of CNN-based depth prediction methods, recent work tries to
design adversarial patches against MDE. However, the existing approaches fall
short of inducing a comprehensive and substantially disruptive impact on the
vision system. Instead, their influence is partial and confined to specific
local areas. These methods lead to erroneous depth predictions only within the
overlapping region with the input image, without considering the
characteristics of the target object, such as its size, shape, and position. In
this paper, we introduce a novel adversarial patch named APARATE. This patch
possesses the ability to selectively undermine MDE in two distinct ways: by
distorting the estimated distances or by creating the illusion of an object
disappearing from the perspective of the autonomous system. Notably, APARATE is
designed to be sensitive to the shape and scale of the target object, and its
influence extends beyond immediate proximity. APARATE, results in a mean depth
estimation error surpassing $0.5$, significantly impacting as much as $99\%$ of
the targeted region when applied to CNN-based MDE models. Furthermore, it
yields a significant error of $0.34$ and exerts substantial influence over
$94\%$ of the target region in the context of Transformer-based MDE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1&quot;&gt;Amira Guesmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdullah Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1&quot;&gt;Ihsen Alouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17646">
<title>XPert: Peripheral Circuit &amp; Neural Architecture Co-search for Area and Energy-efficient Xbar-based Computing. (arXiv:2303.17646v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17646</link>
<description rdf:parseType="Literal">&lt;p&gt;The hardware-efficiency and accuracy of Deep Neural Networks (DNNs)
implemented on In-memory Computing (IMC) architectures primarily depend on the
DNN architecture and the peripheral circuit parameters. It is therefore
essential to holistically co-search the network and peripheral parameters to
achieve optimal performance. To this end, we propose XPert, which co-searches
network architecture in tandem with peripheral parameters such as the type and
precision of analog-to-digital converters, crossbar column sharing and the
layer-specific input precision using an optimization-based design space
exploration. Compared to VGG16 baselines, XPert achieves 10.24x (4.7x) lower
EDAP, 1.72x (1.62x) higher TOPS/W,1.93x (3x) higher TOPS/mm2 at 92.46% (56.7%)
accuracy for CIFAR10 (TinyImagenet) datasets. The code for this paper is
available at https://github.com/Intelligent-Computing-Lab-Yale/XPert.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moitra_A/0/1/0/all/0/1&quot;&gt;Abhishek Moitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_A/0/1/0/all/0/1&quot;&gt;Abhiroop Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngeun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_P/0/1/0/all/0/1&quot;&gt;Priyadarshini Panda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01716">
<title>Decoupling Dynamic Monocular Videos for Dynamic View Synthesis. (arXiv:2304.01716v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01716</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenge of dynamic view synthesis from dynamic monocular videos, i.e.,
synthesizing novel views for free viewpoints given a monocular video of a
dynamic scene captured by a moving camera, mainly lies in accurately modeling
the dynamic objects of a scene using limited 2D frames, each with a varying
timestamp and viewpoint. Existing methods usually require pre-processed 2D
optical flow and depth maps by off-the-shelf methods to supervise the network,
making them suffer from the inaccuracy of the pre-processed supervision and the
ambiguity when lifting the 2D information to 3D. In this paper, we tackle this
challenge in an unsupervised fashion. Specifically, we decouple the motion of
the dynamic objects into object motion and camera motion, respectively
regularized by proposed unsupervised surface consistency and patch-based
multi-view constraints. The former enforces the 3D geometric surfaces of moving
objects to be consistent over time, while the latter regularizes their
appearances to be consistent across different viewpoints. Such a fine-grained
motion formulation can alleviate the learning difficulty for the network, thus
enabling it to produce not only novel views with higher quality but also more
accurate scene flows and depth than existing methods requiring extra
supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_M/0/1/0/all/0/1&quot;&gt;Meng You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05088">
<title>WEAR: An Outdoor Sports Dataset for Wearable and Egocentric Activity Recognition. (arXiv:2304.05088v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05088</link>
<description rdf:parseType="Literal">&lt;p&gt;Though research has shown the complementarity of camera- and inertial-based
data, datasets which offer both egocentric video and inertial-based sensor data
remain scarce. In this paper, we introduce WEAR, an outdoor sports dataset for
both vision- and inertial-based human activity recognition (HAR). The dataset
comprises data from 18 participants performing a total of 18 different workout
activities with untrimmed inertial (acceleration) and camera (egocentric video)
data recorded at 10 different outside locations. Unlike previous egocentric
datasets, WEAR provides a challenging prediction scenario marked by purposely
introduced activity variations as well as an overall small information overlap
across modalities. Benchmark results obtained using each modality separately
show that each modality interestingly offers complementary strengths and
weaknesses in their prediction performance. Further, in light of the recent
success of temporal action localization models following the architecture
design of the ActionFormer, we demonstrate their versatility by applying them
in a plain fashion using vision, inertial and combined (vision + inertial)
features as input. Results demonstrate both the applicability of vision-based
temporal action localization models for inertial data and fusing both
modalities by means of simple concatenation, with the combined approach (vision
+ inertial features) being able to produce the highest mean average precision
and close-to-best F1-score. The dataset and code to reproduce experiments is
publicly available via: https://mariusbock.github.io/wear/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bock_M/0/1/0/all/0/1&quot;&gt;Marius Bock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1&quot;&gt;Hilde Kuehne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laerhoven_K/0/1/0/all/0/1&quot;&gt;Kristof Van Laerhoven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moeller_M/0/1/0/all/0/1&quot;&gt;Michael Moeller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07647">
<title>LASER: A Neuro-Symbolic Framework for Learning Spatial-Temporal Scene Graphs with Weak Supervision. (arXiv:2304.07647v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07647</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose LASER, a neuro-symbolic approach to learn semantic video
representations that capture rich spatial and temporal properties in video data
by leveraging high-level logic specifications. In particular, we formulate the
problem in terms of alignment between raw videos and spatio-temporal logic
specifications. The alignment algorithm leverages a differentiable symbolic
reasoner and a combination of contrastive, temporal, and semantics losses. It
effectively and efficiently trains low-level perception models to extract
fine-grained video representation in the form of a spatio-temporal scene graph
that conforms to the desired high-level specification. In doing so, we explore
a novel methodology that weakly supervises the learning of video semantic
representations through logic specifications. We evaluate our method on two
datasets with rich spatial and temporal specifications:
20BN-Something-Something and MUGEN. We demonstrate that our method learns
better fine-grained video semantics than existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiani Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naik_M/0/1/0/all/0/1&quot;&gt;Mayur Naik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Ser-Nam Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09248">
<title>Real-Time Helmet Violation Detection in AI City Challenge 2023 with Genetic Algorithm-Enhanced YOLOv5. (arXiv:2304.09248v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09248</link>
<description rdf:parseType="Literal">&lt;p&gt;This research focuses on real-time surveillance systems as a means for
tackling the issue of non-compliance with helmet regulations, a practice that
considerably amplifies the risk for motorcycle drivers or riders. Despite the
well-established advantages of helmet usage, achieving widespread compliance
remains challenging due to diverse contributing factors. To effectively address
this concern, real-time monitoring and enforcement of helmet laws have been
proposed as a plausible solution. However, previous attempts at real-time
helmet violation detection have been hindered by their limited ability to
operate in real-time. To overcome this limitation, the current paper introduces
a novel real-time helmet violation detection system that utilizes the YOLOv5
single-stage object detection model. This model is trained on the 2023 NVIDIA
AI City Challenge 2023 Track 5 dataset. The optimal hyperparameters for
training the model are determined using genetic algorithms. Additionally, data
augmentation and various sampling techniques are implemented to enhance the
model&apos;s performance. The efficacy of the models is evaluated using precision,
recall, and mean Average Precision (mAP) metrics. The results demonstrate
impressive precision, recall, and mAP scores of 0.848, 0.599, and 0.641,
respectively for the training data. Furthermore, the model achieves notable mAP
score of 0.6667 for the test datasets, leading to a commendable 4th place rank
in the public leaderboard. This innovative approach represents a notable
breakthrough in the field and holds immense potential to substantially enhance
motorcycle safety. By enabling real-time monitoring and enforcement
capabilities, this system has the capacity to contribute towards increased
compliance with helmet laws, thereby effectively reducing the risks faced by
motorcycle riders and passengers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanikazemi_E/0/1/0/all/0/1&quot;&gt;Elham Soltanikazemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhakal_A/0/1/0/all/0/1&quot;&gt;Ashwin Dhakal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatuwal_B/0/1/0/all/0/1&quot;&gt;Bijaya Kumar Hatuwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toubal_I/0/1/0/all/0/1&quot;&gt;Imad Eddine Toubal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aboah_A/0/1/0/all/0/1&quot;&gt;Armstrong Aboah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniappan_K/0/1/0/all/0/1&quot;&gt;Kannappan Palaniappan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14408">
<title>Using Scalable Computer Vision to Automate High-throughput Semiconductor Characterization. (arXiv:2304.14408v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14408</link>
<description rdf:parseType="Literal">&lt;p&gt;High-throughput materials synthesis methods have risen in popularity due to
their potential to accelerate the design and discovery of novel functional
materials, such as solution-processed semiconductors. After synthesis, key
material properties must be measured and characterized to validate discovery
and provide feedback to optimization cycles. However, with the boom in
development of high-throughput synthesis tools that champion production rates
up to $10^4$ samples per hour with flexible form factors, most sample
characterization methods are either slow (conventional rates of $10^1$ samples
per hour, approximately 1000x slower) or rigid (e.g., designed for
standard-size microplates), resulting in a bottleneck that impedes the
materials-design process. To overcome this challenge, we propose a set of
automated material property characterization (autocharacterization) tools that
leverage the adaptive, parallelizable, and scalable nature of computer vision
to accelerate the throughput of characterization by 85x compared to the
non-automated workflow. We demonstrate a generalizable composition mapping tool
for high-throughput synthesized binary material systems as well as two scalable
autocharacterization algorithms that (1) autonomously compute the band gap of
200 unique compositions in 6 minutes and (2) autonomously compute the degree of
degradation in 200 unique compositions in 20 minutes, generating ultra-high
compositional resolution trends of band gap and stability. We demonstrate that
the developed band gap and degradation detection autocharacterization methods
achieve 98.5% accuracy and 96.9% accuracy, respectively, on the
FA$_{1-x}$MA$_{x}$PbI$_3$, $0\leq x \leq 1$ perovskite semiconductor system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siemenn_A/0/1/0/all/0/1&quot;&gt;Alexander E. Siemenn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aissi_E/0/1/0/all/0/1&quot;&gt;Eunice Aissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheng_F/0/1/0/all/0/1&quot;&gt;Fang Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tiihonen_A/0/1/0/all/0/1&quot;&gt;Armi Tiihonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kavak_H/0/1/0/all/0/1&quot;&gt;Hamide Kavak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Das_B/0/1/0/all/0/1&quot;&gt;Basita Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Buonassisi_T/0/1/0/all/0/1&quot;&gt;Tonio Buonassisi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05546">
<title>ColonMapper: topological mapping and localization for colonoscopy. (arXiv:2305.05546v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05546</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a topological mapping and localization system able to operate on
real human colonoscopies, despite significant shape and illumination changes.
The map is a graph where each node codes a colon location by a set of real
images, while edges represent traversability between nodes. For close-in-time
images, where scene changes are minor, place recognition can be successfully
managed with the recent transformers-based local feature matching algorithms.
However, under long-term changes -- such as different colonoscopies of the same
patient -- feature-based matching fails. To address this, we train on real
colonoscopies a deep global descriptor achieving high recall with significant
changes in the scene. The addition of a Bayesian filter boosts the accuracy of
long-term place recognition, enabling relocalization in a previously built map.
Our experiments show that ColonMapper is able to autonomously build a map and
localize against it in two important use cases: localization within the same
colonoscopy or within different colonoscopies of the same patient. Code will be
available upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morlana_J/0/1/0/all/0/1&quot;&gt;Javier Morlana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tardos_J/0/1/0/all/0/1&quot;&gt;Juan D. Tard&amp;#xf3;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montiel_J/0/1/0/all/0/1&quot;&gt;J.M.M. Montiel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10229">
<title>How does Contrastive Learning Organize Images?. (arXiv:2305.10229v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10229</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning, a dominant self-supervised technique, emphasizes
similarity in representations between augmentations of the same input and
dissimilarity for different ones. Although low contrastive loss often
correlates with high classification accuracy, recent studies challenge this
direct relationship, spotlighting the crucial role of inductive biases. We
delve into these biases from a clustering viewpoint, noting that contrastive
learning creates locally dense clusters, contrasting the globally dense
clusters from supervised learning. To capture this discrepancy, we introduce
the &quot;RLD (Relative Local Density)&quot; metric. While this cluster property can
hinder linear classification accuracy, leveraging a Graph Convolutional Network
(GCN) based classifier mitigates this, boosting accuracy and reducing parameter
requirements. The code is available
\href{https://github.com/xsgxlz/How-does-Contrastive-Learning-Organize-Images/tree/main}{here}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunzhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Q/0/1/0/all/0/1&quot;&gt;Qi Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11618">
<title>DAP: A Dynamic Adversarial Patch for Evading Person Detectors. (arXiv:2305.11618v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11618</link>
<description rdf:parseType="Literal">&lt;p&gt;Patch-based adversarial attacks were proven to compromise the robustness and
reliability of computer vision systems. However, their conspicuous and easily
detectable nature challenge their practicality in real-world setting. To
address this, recent work has proposed using Generative Adversarial Networks
(GANs) to generate naturalistic patches that may not attract human attention.
However, such approaches suffer from a limited latent space making it
challenging to produce a patch that is efficient, stealthy, and robust to
multiple real-world transformations. This paper introduces a novel approach
that produces a Dynamic Adversarial Patch (DAP) designed to overcome these
limitations. DAP maintains a naturalistic appearance while optimizing attack
efficiency and robustness to real-world transformations. The approach involves
redefining the optimization problem and introducing a novel objective function
that incorporates a similarity metric to guide the patch&apos;s creation. Unlike
GAN-based techniques, the DAP directly modifies pixel values within the patch,
providing increased flexibility and adaptability to multiple transformations.
Furthermore, most clothing-based physical attacks assume static objects and
ignore the possible transformations caused by non-rigid deformation due to
changes in a person&apos;s pose. To address this limitation, a &apos;Creases
Transformation&apos; (CT) block is introduced, enhancing the patch&apos;s resilience to a
variety of real-world distortions. Experimental results demonstrate that the
proposed approach outperforms state-of-the-art attacks, achieving a success
rate of up to 82.28% in the digital world when targeting the YOLOv7 detector
and 65% in the physical world when targeting YOLOv3tiny detector deployed in
edge-based smart cameras.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guesmi_A/0/1/0/all/0/1&quot;&gt;Amira Guesmi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Ruitian Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanif_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdullah Hanif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alouani_I/0/1/0/all/0/1&quot;&gt;Ihsen Alouani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafique_M/0/1/0/all/0/1&quot;&gt;Muhammad Shafique&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11818">
<title>MaGIC: Multi-modality Guided Image Completion. (arXiv:2305.11818v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11818</link>
<description rdf:parseType="Literal">&lt;p&gt;Vanilla image completion approaches exhibit sensitivity to large missing
regions, attributed to the limited availability of reference information for
plausible generation. To mitigate this, existing methods incorporate the extra
cue as a guidance for image completion. Despite improvements, these approaches
are often restricted to employing a single modality (e.g., segmentation or
sketch maps), which lacks scalability in leveraging multi-modality for more
plausible completion. In this paper, we propose a novel, simple yet effective
method for Multi-modal Guided Image Completion, dubbed MaGIC, which not only
supports a wide range of single modality as the guidance (e.g., text, canny
edge, sketch, segmentation, depth, and pose), but also adapts to arbitrarily
customized combination of these modalities (i.e., arbitrary multi-modality) for
image completion. For building MaGIC, we first introduce a modality-specific
conditional U-Net (MCU-Net) that injects single-modal signal into a U-Net
denoiser for single-modal guided image completion. Then, we devise a consistent
modality blending (CMB) method to leverage modality signals encoded in multiple
learned MCU-Nets through gradient guidance in latent space. Our CMB is
training-free, thereby avoids the cumbersome joint re-training of different
modalities, which is the secret of MaGIC to achieve exceptional flexibility in
accommodating new modalities for completion. Experiments show the superiority
of MaGIC over state-of-the-art methods and its generalization to various
completion tasks. Our project with code and models is available at
yeates.github.io/MaGIC-Page/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yongsheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tiejian Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Libo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12827">
<title>Task Arithmetic in the Tangent Space: Improved Editing of Pre-Trained Models. (arXiv:2305.12827v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12827</link>
<description rdf:parseType="Literal">&lt;p&gt;Task arithmetic has recently emerged as a cost-effective and scalable
approach to edit pre-trained models directly in weight space: By adding the
fine-tuned weights of different tasks, the model&apos;s performance can be improved
on these tasks, while negating them leads to task forgetting. Yet, our
understanding of the effectiveness of task arithmetic and its underlying
principles remains limited. We present a comprehensive study of task arithmetic
in vision-language models and show that weight disentanglement is the crucial
factor that makes it effective. This property arises during pre-training and
manifests when distinct directions in weight space govern separate, localized
regions in function space associated with the tasks. Notably, we show that
fine-tuning models in their tangent space by linearizing them amplifies weight
disentanglement. This leads to substantial performance improvements across
multiple task arithmetic benchmarks and diverse models. Building on these
findings, we provide theoretical and empirical analyses of the neural tangent
kernel (NTK) of these models and establish a compelling link between task
arithmetic and the spatial localization of the NTK eigenfunctions. Overall, our
work uncovers novel insights into the fundamental mechanisms of task arithmetic
and offers a more reliable and effective approach to edit pre-trained models
through the NTK linearization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortiz_Jimenez_G/0/1/0/all/0/1&quot;&gt;Guillermo Ortiz-Jimenez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favero_A/0/1/0/all/0/1&quot;&gt;Alessandro Favero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frossard_P/0/1/0/all/0/1&quot;&gt;Pascal Frossard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18183">
<title>On Counterfactual Data Augmentation Under Confounding. (arXiv:2305.18183v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18183</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual data augmentation has recently emerged as a method to mitigate
confounding biases in the training data. These biases, such as spurious
correlations, arise due to various observed and unobserved confounding
variables in the data generation process. In this paper, we formally analyze
how confounding biases impact downstream classifiers and present a causal
viewpoint to the solutions based on counterfactual data augmentation. We
explore how removing confounding biases serves as a means to learn invariant
features, ultimately aiding in generalization beyond the observed data
distribution. Additionally, we present a straightforward yet powerful algorithm
for generating counterfactual images, which effectively mitigates the influence
of confounding effects on downstream classifiers. Through experiments on MNIST
variants and the CelebA datasets, we demonstrate how our simple augmentation
method helps existing state-of-the-art methods achieve good results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_A/0/1/0/all/0/1&quot;&gt;Abbavaram Gowtham Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachu_S/0/1/0/all/0/1&quot;&gt;Saketh Bachu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dash_S/0/1/0/all/0/1&quot;&gt;Saloni Dash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_C/0/1/0/all/0/1&quot;&gt;Charchit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Amit Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00917">
<title>Vocabulary-free Image Classification. (arXiv:2306.00917v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00917</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large vision-language models have revolutionized the image
classification paradigm. Despite showing impressive zero-shot capabilities, a
pre-defined set of categories, a.k.a. the vocabulary, is assumed at test time
for composing the textual prompts. However, such assumption can be impractical
when the semantic context is unknown and evolving. We thus formalize a novel
task, termed as Vocabulary-free Image Classification (VIC), where we aim to
assign to an input image a class that resides in an unconstrained
language-induced semantic space, without the prerequisite of a known
vocabulary. VIC is a challenging task as the semantic space is extremely large,
containing millions of concepts, with hard-to-discriminate fine-grained
categories. In this work, we first empirically verify that representing this
semantic space by means of an external vision-language database is the most
effective way to obtain semantically relevant content for classifying the
image. We then propose Category Search from External Databases (CaSED), a
method that exploits a pre-trained vision-language model and an external
vision-language database to address VIC in a training-free manner. CaSED first
extracts a set of candidate categories from captions retrieved from the
database based on their semantic similarity to the image, and then assigns to
the image the best matching candidate category according to the same
vision-language model. Experiments on benchmark datasets validate that CaSED
outperforms other complex vision-language frameworks, while being efficient
with much fewer parameters, paving the way for future research in this
direction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conti_A/0/1/0/all/0/1&quot;&gt;Alessandro Conti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fini_E/0/1/0/all/0/1&quot;&gt;Enrico Fini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mancini_M/0/1/0/all/0/1&quot;&gt;Massimiliano Mancini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rota_P/0/1/0/all/0/1&quot;&gt;Paolo Rota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02728">
<title>Overcoming Weak Visual-Textual Alignment for Video Moment Retrieval. (arXiv:2306.02728v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02728</link>
<description rdf:parseType="Literal">&lt;p&gt;Video moment retrieval (VMR) identifies a specific moment in an untrimmed
video for a given natural language query. This task is prone to suffer the weak
visual-textual alignment problem innate in video datasets. Due to the
ambiguity, a query does not fully cover the relevant details of the
corresponding moment, or the moment may contain misaligned and irrelevant
frames, potentially limiting further performance gains. To tackle this problem,
we propose a background-aware moment detection transformer (BM-DETR). Our model
adopts a contrastive approach, carefully utilizing the negative queries matched
to other moments in the video. Specifically, our model learns to predict the
target moment from the joint probability of each frame given the positive query
and the complement of negative queries. This leads to effective use of the
surrounding background, improving moment sensitivity and enhancing overall
alignments in videos. Extensive experiments on four benchmarks demonstrate the
effectiveness of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_M/0/1/0/all/0/1&quot;&gt;Minjoon Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Youwon Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_S/0/1/0/all/0/1&quot;&gt;Seongho Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Joochan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin-Hwa Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06010">
<title>A Large-Scale Analysis on Self-Supervised Video Representation Learning. (arXiv:2306.06010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06010</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning is an effective way for label-free model
pre-training, especially in the video domain where labeling is expensive.
Existing self-supervised works in the video domain use varying experimental
setups to demonstrate their effectiveness and comparison across approaches
becomes challenging with no standard benchmark. In this work, we first provide
a benchmark that enables a comparison of existing approaches on the same
ground. Next, we study five different aspects of self-supervised learning
important for videos; 1) dataset size, 2) complexity, 3) data distribution, 4)
data noise, and, 5)feature analysis. To facilitate this study, we focus on
seven different methods along with seven different network architectures and
perform an extensive set of experiments on 5 different datasets with an
evaluation of two different downstream tasks. We present several interesting
insights from this study which span across different properties of pretraining
and target datasets, pretext-tasks, and model architectures among others. We
further put some of these insights to the real test and propose an approach
that requires a limited amount of training data and outperforms existing
state-of-the-art approaches which use 10x pretraining data. We believe this
work will pave the way for researchers to a better understanding of
self-supervised pretext tasks in video representation learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Akash Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ashlesha Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vineet_V/0/1/0/all/0/1&quot;&gt;Vibhav Vineet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1&quot;&gt;Yogesh Singh Rawat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12685">
<title>Rethinking the Backward Propagation for Adversarial Transferability. (arXiv:2306.12685v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12685</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer-based attacks generate adversarial examples on the surrogate model,
which can mislead other black-box models without access, making it promising to
attack real-world applications. Recently, several works have been proposed to
boost adversarial transferability, in which the surrogate model is usually
overlooked. In this work, we identify that non-linear layers (e.g., ReLU,
max-pooling, etc.) truncate the gradient during backward propagation, making
the gradient w.r.t. input image imprecise to the loss function. We hypothesize
and empirically validate that such truncation undermines the transferability of
adversarial examples. Based on these findings, we propose a novel method called
Backward Propagation Attack (BPA) to increase the relevance between the
gradient w.r.t. input image and loss function so as to generate adversarial
examples with higher transferability. Specifically, BPA adopts a non-monotonic
function as the derivative of ReLU and incorporates softmax with temperature to
smooth the derivative of max-pooling, thereby mitigating the information loss
during the backward propagation of gradients. Empirical results on the ImageNet
dataset demonstrate that not only does our method substantially boost the
adversarial transferability, but it is also general to existing transfer-based
attacks. Code is available at https://github.com/Trustworthy-AI-Group/RPA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaosen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_K/0/1/0/all/0/1&quot;&gt;Kangheng Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13074">
<title>Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports. (arXiv:2306.13074v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13074</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based object detectors have driven notable progress in
multi-object tracking algorithms. Yet, current tracking methods mainly focus on
simple, regular motion patterns in pedestrians or vehicles. This leaves a gap
in tracking algorithms for targets with nonlinear, irregular motion, like
athletes. Additionally, relying on the Kalman filter in recent tracking
algorithms falls short when object motion defies its linear assumption. To
overcome these issues, we propose a novel online and robust multi-object
tracking approach named deep ExpansionIoU (Deep-EIoU), which focuses on
multi-object tracking for sports scenarios. Unlike conventional methods, we
abandon the use of the Kalman filter and leverage the iterative scale-up
ExpansionIoU and deep features for robust tracking in sports scenarios. This
approach achieves superior tracking performance without adopting a more robust
detector, all while keeping the tracking process in an online fashion. Our
proposed method demonstrates remarkable effectiveness in tracking irregular
motion objects, achieving a score of 77.2% HOTA on the SportsMOT dataset and
85.4% HOTA on the SoccerNet-Tracking dataset. It outperforms all previous
state-of-the-art trackers on various large-scale multi-object tracking
benchmarks, covering various kinds of sports scenarios. The code and models are
available at https://github.com/hsiangwei0903/Deep-EIoU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_P/0/1/0/all/0/1&quot;&gt;Pyong-Kun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang-Ju Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyoungoh Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chung-I Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16772">
<title>Learning from Synthetic Human Group Activities. (arXiv:2306.16772v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of complex human interactions and group activities has become a
focal point in human-centric computer vision. However, progress in related
tasks is often hindered by the challenges of obtaining large-scale labeled
datasets from real-world scenarios. To address the limitation, we introduce
M3Act, a synthetic data generator for multi-view multi-group multi-person human
atomic actions and group activities. Powered by the Unity engine, M3Act
features multiple semantic groups, highly diverse and photorealistic images,
and a comprehensive set of annotations, which facilitates the learning of
human-centered tasks across single-person, multi-person, and multi-group
conditions. We demonstrate the advantages of M3Act across three core
experiments using various input modalities. First, adding our synthetic data
significantly improves the performance of MOTRv2 on DanceTrack, leading to a
hop on the leaderboard from 10th to 2nd place. With M3Act, we achieve tracking
results on par with MOTRv2*, which is trained with 62.5% more real-world data.
Second, M3Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on
group activity and atomic action accuracy respectively. Moreover, M3Act opens
new research for controllable 3D group activity generation. We define multiple
metrics and propose a competitive baseline for the novel task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Che-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Danrui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Deep Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1&quot;&gt;Parth Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honglu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Samuel S. Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sejong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1&quot;&gt;Mubbasir Kapadia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09112">
<title>NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF. (arXiv:2307.09112v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09112</link>
<description rdf:parseType="Literal">&lt;p&gt;Remarkable progress has been made in 3D reconstruction from single-view RGB-D
inputs. MCC is the current state-of-the-art method in this field, which
achieves unprecedented success by combining vision Transformers with
large-scale training. However, we identified two key limitations of MCC: 1) The
Transformer decoder is inefficient in handling large number of query points; 2)
The 3D representation struggles to recover high-fidelity details. In this
paper, we propose a new approach called NU-MCC that addresses these
limitations. NU-MCC includes two key innovations: a Neighborhood decoder and a
Repulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood
decoder introduces center points as an efficient proxy of input visual
features, allowing each query point to only attend to a small neighborhood.
This design not only results in much faster inference speed but also enables
the exploitation of finer-scale visual features for improved recovery of 3D
textures. Second, our Repulsive UDF is a novel alternative to the occupancy
field used in MCC, significantly improving the quality of 3D object
reconstruction. Compared to standard UDFs that suffer from holes in results,
our proposed Repulsive UDF can achieve more complete surface reconstruction.
Experimental results demonstrate that NU-MCC is able to learn a strong 3D
representation, significantly advancing the state of the art in single-view 3D
reconstruction. Particularly, it outperforms MCC by 9.7% in terms of the
F1-score on the CO3D-v2 dataset with more than 5x faster running speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lionar_S/0/1/0/all/0/1&quot;&gt;Stefan Lionar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gim Hee Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09815">
<title>LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network. (arXiv:2307.09815v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09815</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent
blur is a challenging task.~Existing blur map-based deblurring methods have
demonstrated promising results. In this paper, we propose, to the best of our
knowledge, the first framework that introduces the contrastive language-image
pre-training framework (CLIP) to accurately estimate the blur map from a DP
pair unsupervisedly. To achieve this, we first carefully design text prompts to
enable CLIP to understand blur-related geometric prior knowledge from the DP
pair. Then, we propose a format to input a stereo DP pair to CLIP without any
fine-tuning, despite the fact that CLIP is pre-trained on monocular images.
Given the estimated blur map, we introduce a blur-prior attention block, a
blur-weighting loss, and a blur-aware loss to recover the all-in-focus image.
Our method achieves state-of-the-art performance in extensive experiments (see
Fig.~\ref{fig:teaser}).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miaomiao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01981">
<title>CartiMorph: a framework for automated knee articular cartilage morphometrics. (arXiv:2308.01981v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01981</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce CartiMorph, a framework for automated knee articular cartilage
morphometrics. It takes an image as input and generates quantitative metrics
for cartilage subregions, including the percentage of full-thickness cartilage
loss (FCL), mean thickness, surface area, and volume. CartiMorph leverages the
power of deep learning models for hierarchical image feature representation.
Deep learning models were trained and validated for tissue segmentation,
template construction, and template-to-image registration. We established
methods for surface-normal-based cartilage thickness mapping, FCL estimation,
and rule-based cartilage parcellation. Our cartilage thickness map showed less
error in thin and peripheral regions. We evaluated the effectiveness of the
adopted segmentation model by comparing the quantitative metrics obtained from
model segmentation and those from manual segmentation. The root-mean-squared
deviation of the FCL measurements was less than 8%, and strong correlations
were observed for the mean thickness (Pearson&apos;s correlation coefficient $\rho
\in [0.82,0.97]$), surface area ($\rho \in [0.82,0.98]$) and volume ($\rho \in
[0.89,0.98]$) measurements. We compared our FCL measurements with those from a
previous study and found that our measurements deviated less from the ground
truths. We observed superior performance of the proposed rule-based cartilage
parcellation method compared with the atlas-based approach. CartiMorph has the
potential to promote imaging biomarkers discovery for knee osteoarthritis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yongcheng Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Junru Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Sheheryar Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12320">
<title>Understanding Dark Scenes by Contrasting Multi-Modal Observations. (arXiv:2308.12320v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12320</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding dark scenes based on multi-modal image data is challenging, as
both the visible and auxiliary modalities provide limited semantic information
for the task. Previous methods focus on fusing the two modalities but neglect
the correlations among semantic classes when minimizing losses to align pixels
with labels, resulting in inaccurate class predictions. To address these
issues, we introduce a supervised multi-modal contrastive learning approach to
increase the semantic discriminability of the learned multi-modal feature
spaces by jointly performing cross-modal and intra-modal contrast under the
supervision of the class correlations. The cross-modal contrast encourages
same-class embeddings from across the two modalities to be closer and pushes
different-class ones apart. The intra-modal contrast forces same-class or
different-class embeddings within each modality to be together or apart. We
validate our approach on a variety of tasks that cover diverse light conditions
and image modalities. Experiments show that our approach can effectively
enhance dark scene understanding based on multi-modal images with limited
semantics by shaping semantic-discriminative feature spaces. Comparisons with
previous methods demonstrate our state-of-the-art performance. Code and
pretrained models are available at https://github.com/palmdong/SMMCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokoya_N/0/1/0/all/0/1&quot;&gt;Naoto Yokoya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16819">
<title>BTSeg: Barlow Twins Regularization for Domain Adaptation in Semantic Segmentation. (arXiv:2308.16819v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16819</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic image segmentation is particularly vital for the advancement of
autonomous vehicle technologies. However, this domain faces substantial
challenges under adverse conditions like rain or darkness, which remain
underrepresented in most datasets. The generation of additional training data
for these scenarios is not only costly but also fraught with potential
inaccuracies, largely attributable to the aleatoric uncertainty inherent in
such conditions.
&lt;/p&gt;
&lt;p&gt;We introduce BTSeg, an innovative, semi-supervised training approach
enhancing semantic segmentation models in order to effectively handle a range
of adverse conditions without requiring the creation of extensive new datasets.
BTSeg employs a novel application of the Barlow Twins loss, a concept borrowed
from unsupervised learning. The original Barlow Twins approach uses stochastic
augmentations in order to learn useful representations from unlabeled data
without the need for external labels. In our approach, we regard images
captured at identical locations but under varying adverse conditions as
manifold representation of the same scene (which could be interpreted as
&quot;natural augmentations&quot;), thereby enabling the model to conceptualize its
understanding of the environment.
&lt;/p&gt;
&lt;p&gt;We evaluate our approach on the ACDC dataset, where it performs favorably
when compared to the current state-of-the-art methods, while also being simpler
to implement and train. For the new challenging ACG benchmark it shows
cutting-edge performance, demonstrating its robustness and generalization
capabilities. We will make the code publicly available post-acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunzel_J/0/1/0/all/0/1&quot;&gt;Johannes K&amp;#xfc;nzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00018">
<title>Unsupervised discovery of Interpretable Visual Concepts. (arXiv:2309.00018v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00018</link>
<description rdf:parseType="Literal">&lt;p&gt;Providing interpretability of deep-learning models to non-experts, while
fundamental for a responsible real-world usage, is challenging. Attribution
maps from xAI techniques, such as Integrated Gradients, are a typical example
of a visualization technique containing a high level of information, but with
difficult interpretation. In this paper, we propose two methods, Maximum
Activation Groups Extraction (MAGE) and Multiscale Interpretable Visualization
(Ms-IV), to explain the model&apos;s decision, enhancing global interpretability.
MAGE finds, for a given CNN, combinations of features which, globally, form a
semantic meaning, that we call concepts. We group these similar feature
patterns by clustering in ``concepts&apos;&apos;, that we visualize through Ms-IV. This
last method is inspired by Occlusion and Sensitivity analysis (incorporating
causality), and uses a novel metric, called Class-aware Order Correlation
(CaOC), to globally evaluate the most important image regions according to the
model&apos;s decision space. We compare our approach to xAI methods such as LIME and
Integrated Gradients. Experimental results evince the Ms-IV higher localization
and faithfulness values. Finally, qualitative evaluation of combined MAGE and
Ms-IV demonstrates humans&apos; ability to agree, based on the visualization, with
the decision of clusters&apos; concepts; and, to detect, among a given set of
networks, the existence of bias.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodrigues_C/0/1/0/all/0/1&quot;&gt;Caroline Mazini Rodrigues&lt;/a&gt; (LIGM, LRDE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutry_N/0/1/0/all/0/1&quot;&gt;Nicolas Boutry&lt;/a&gt; (LRDE), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najman_L/0/1/0/all/0/1&quot;&gt;Laurent Najman&lt;/a&gt; (LIGM)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01446">
<title>Open Sesame! Universal Black Box Jailbreaking of Large Language Models. (arXiv:2309.01446v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01446</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), designed to provide helpful and safe responses,
often rely on alignment techniques to align with user intent and social
guidelines. Unfortunately, this alignment can be exploited by malicious actors
seeking to manipulate an LLM&apos;s outputs for unintended purposes. In this paper
we introduce a novel approach that employs a genetic algorithm (GA) to
manipulate LLMs when model architecture and parameters are inaccessible. The GA
attack works by optimizing a universal adversarial prompt that -- when combined
with a user&apos;s query -- disrupts the attacked model&apos;s alignment, resulting in
unintended and potentially harmful outputs. Our novel approach systematically
reveals a model&apos;s limitations and vulnerabilities by uncovering instances where
its responses deviate from expected behavior. Through extensive experiments we
demonstrate the efficacy of our technique, thus contributing to the ongoing
discussion on responsible AI development by providing a diagnostic tool for
evaluating and enhancing alignment of LLMs with human intent. To our knowledge
this is the first automated universal black box jailbreak attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapid_R/0/1/0/all/0/1&quot;&gt;Raz Lapid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Langberg_R/0/1/0/all/0/1&quot;&gt;Ron Langberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1&quot;&gt;Moshe Sipper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.04342">
<title>Revealing the preference for correcting separated aberrations in joint optic-image design. (arXiv:2309.04342v3 [physics.optics] UPDATED)</title>
<link>http://arxiv.org/abs/2309.04342</link>
<description rdf:parseType="Literal">&lt;p&gt;The joint design of the optical system and the downstream algorithm is a
challenging and promising task. Due to the demand for balancing the global
optimal of imaging systems and the computational cost of physical simulation,
existing methods cannot achieve efficient joint design of complex systems such
as smartphones and drones. In this work, starting from the perspective of the
optical design, we characterize the optics with separated aberrations.
Additionally, to bridge the hardware and software without gradients, an image
simulation system is presented to reproduce the genuine imaging procedure of
lenses with large field-of-views. As for aberration correction, we propose a
network to perceive and correct the spatially varying aberrations and validate
its superiority over state-of-the-art methods. Comprehensive experiments reveal
that the preference for correcting separated aberrations in joint design is as
follows: longitudinal chromatic aberration, lateral chromatic aberration,
spherical aberration, field curvature, and coma, with astigmatism coming last.
Drawing from the preference, a 10% reduction in the total track length of the
consumer-level mobile phone lens module is accomplished. Moreover, this
procedure spares more space for manufacturing deviations, realizing
extreme-quality enhancement of computational photography. The optimization
paradigm provides innovative insight into the practical joint design of
sophisticated optical systems and post-processing algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingwen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ren_Z/0/1/0/all/0/1&quot;&gt;Zheng Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenguan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiapu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Huajun Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yueting Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06255">
<title>Enhancing Multi-modal Cooperation via Fine-grained Modality Valuation. (arXiv:2309.06255v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06255</link>
<description rdf:parseType="Literal">&lt;p&gt;One primary topic of multi-modal learning is to jointly incorporate
heterogeneous information from different modalities. However, most models often
suffer from unsatisfactory multi-modal cooperation, which could not jointly
utilize all modalities well. Some methods are proposed to identify and enhance
the worse learnt modality, but are often hard to provide the fine-grained
observation of multi-modal cooperation at sample-level with theoretical
support. Hence, it is essential to reasonably observe and improve the
fine-grained cooperation between modalities, especially when facing realistic
scenarios where the modality discrepancy could vary across different samples.
To this end, we introduce a fine-grained modality valuation metric to evaluate
the contribution of each modality at sample-level. Via modality valuation, we
regretfully observe that the multi-modal model tends to rely on one specific
modality, resulting in other modalities being low-contributing. We further
analyze this issue and improve cooperation between modalities by enhancing the
discriminative ability of low-contributing modalities in a targeted manner.
Overall, our methods reasonably observe the fine-grained uni-modal contribution
at sample-level and achieve considerable improvement on different multi-modal
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yake Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruoxuan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Di Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12969">
<title>Detect Every Thing with Few Examples. (arXiv:2309.12969v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12969</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-set object detection aims at detecting arbitrary categories beyond those
seen during training. Most recent advancements have adopted the open-vocabulary
paradigm, utilizing vision-language backbones to represent categories with
language. In this paper, we introduce DE-ViT, an open-set object detector that
employs vision-only DINOv2 backbones and learns new categories through example
images instead of language. To improve general detection ability, we transform
multi-classification tasks into binary classification tasks while bypassing
per-class inference, and propose a novel region propagation technique for
localization. We evaluate DE-ViT on open-vocabulary, few-shot, and one-shot
object detection benchmark with COCO and LVIS. For COCO, DE-ViT outperforms the
open-vocabulary SoTA by 6.9 AP50 and achieves 50 AP50 in novel classes. DE-ViT
surpasses the few-shot SoTA by 15 mAP on 10-shot and 7.2 mAP on 30-shot and
one-shot SoTA by 2.8 AP50. For LVIS, DE-ViT outperforms the open-vocabulary
SoTA by 2.2 mask AP and reaches 34.3 mask APr. Code is available at
https://github.com/mlzxy/devit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuting Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boularias_A/0/1/0/all/0/1&quot;&gt;Abdeslam Boularias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13101">
<title>Deformable 3D Gaussians for High-Fidelity Monocular Dynamic Scene Reconstruction. (arXiv:2309.13101v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13101</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural representation has paved the way for new approaches to
dynamic scene reconstruction and rendering. Nonetheless, cutting-edge dynamic
neural rendering methods rely heavily on these implicit representations, which
frequently struggle to capture the intricate details of objects in the scene.
Furthermore, implicit methods have difficulty achieving real-time rendering in
general dynamic scenes, limiting their use in a variety of tasks. To address
the issues, we propose a deformable 3D Gaussians Splatting method that
reconstructs scenes using 3D Gaussians and learns them in canonical space with
a deformation field to model monocular dynamic scenes. We also introduce an
annealing smoothing training mechanism with no extra overhead, which can
mitigate the impact of inaccurate poses on the smoothness of time interpolation
tasks in real-world datasets. Through a differential Gaussian rasterizer, the
deformable 3D Gaussians not only achieve higher rendering quality but also
real-time rendering speed. Experiments show that our method outperforms
existing methods significantly in terms of both rendering quality and speed,
making it well-suited for tasks such as novel-view synthesis, time
interpolation, and real-time rendering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_S/0/1/0/all/0/1&quot;&gt;Shaohui Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00582">
<title>Pink: Unveiling the Power of Referential Comprehension for Multi-modal LLMs. (arXiv:2310.00582v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00582</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal Large Language Models (MLLMs) have shown remarkable capabilities
in various multi-modal tasks. Nevertheless, their performance in fine-grained
image understanding tasks is still limited. To address this issue, this paper
proposes a new framework to enhance the fine-grained image understanding
abilities of MLLMs. Specifically, we present a new method for constructing the
instruction tuning dataset at a low cost by leveraging annotations in existing
datasets. A self-consistent bootstrapping method is also introduced to extend
existing dense object annotations into high-quality
referring-expression-bounding-box pairs. These methods enable the generation of
high-quality instruction data which includes a wide range of fundamental
abilities essential for fine-grained image perception. Moreover, we argue that
the visual encoder should be tuned during instruction tuning to mitigate the
gap between full image perception and fine-grained image perception.
Experimental results demonstrate the superior performance of our method. For
instance, our model exhibits a 5.2% accuracy improvement over Qwen-VL on GQA
and surpasses the accuracy of Kosmos-2 by 24.7% on RefCOCO_val. We also attain
the top rank on the leaderboard of MMBench. This promising performance is
achieved by training on only publicly available data, making it easily
reproducible. The models, datasets, and codes are publicly available at
https://github.com/SY-Xuan/Pink.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_S/0/1/0/all/0/1&quot;&gt;Shiyu Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qingpei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiliang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02129">
<title>Unveiling the Pitfalls of Knowledge Editing for Large Language Models. (arXiv:2310.02129v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02129</link>
<description rdf:parseType="Literal">&lt;p&gt;As the cost associated with fine-tuning Large Language Models (LLMs)
continues to rise, recent research efforts have pivoted towards developing
methodologies to edit implicit knowledge embedded within LLMs. Yet, there&apos;s
still a dark cloud lingering overhead -- will knowledge editing trigger
butterfly effect? since it is still unclear whether knowledge editing might
introduce side effects that pose potential risks or not. This paper pioneers
the investigation into the potential pitfalls associated with knowledge editing
for LLMs. To achieve this, we introduce new benchmark datasets and propose
innovative evaluation metrics. Our results underline two pivotal concerns: (1)
Knowledge Conflict: Editing groups of facts that logically clash can magnify
the inherent inconsistencies in LLMs-a facet neglected by previous methods. (2)
Knowledge Distortion: Altering parameters with the aim of editing factual
knowledge can irrevocably warp the innate knowledge structure of LLMs.
Experimental results vividly demonstrate that knowledge editing might
inadvertently cast a shadow of unintended consequences on LLMs, which warrant
attention and efforts for future works. Code is available at
https://github.com/zjunlp/PitfallsKnowledgeEditing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhoubo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08897">
<title>Self supervised convolutional kernel based handcrafted feature harmonization: Enhanced left ventricle hypertension disease phenotyping on echocardiography. (arXiv:2310.08897v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08897</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiomics, a medical imaging technique, extracts quantitative handcrafted
features from images to predict diseases. Harmonization in those features
ensures consistent feature extraction across various imaging devices and
protocols. Methods for harmonization include standardized imaging protocols,
statistical adjustments, and evaluating feature robustness. Myocardial diseases
such as Left Ventricular Hypertrophy (LVH) and Hypertensive Heart Disease (HHD)
are diagnosed via echocardiography, but variable imaging settings pose
challenges. Harmonization techniques are crucial for applying handcrafted
features in disease diagnosis in such scenario. Self-supervised learning (SSL)
enhances data understanding within limited datasets and adapts to diverse data
settings. ConvNeXt-V2 integrates convolutional layers into SSL, displaying
superior performance in various tasks. This study focuses on convolutional
filters within SSL, using them as preprocessing to convert images into feature
maps for handcrafted feature harmonization. Our proposed method excelled in
harmonization evaluation and exhibited superior LVH classification performance
compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jina Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Youngtaek Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_D/0/1/0/all/0/1&quot;&gt;Dawun Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Yeonggul Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jeong_S/0/1/0/all/0/1&quot;&gt;Sihyeon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jung_T/0/1/0/all/0/1&quot;&gt;Taekgeun Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yoon_Y/0/1/0/all/0/1&quot;&gt;Yeonyee E. Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moon_I/0/1/0/all/0/1&quot;&gt;Inki Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung-Ah Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyuk-Jae Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13019">
<title>Tailoring Adversarial Attacks on Deep Neural Networks for Targeted Class Manipulation Using DeepFool Algorithm. (arXiv:2310.13019v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13019</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have significantly advanced various domains, but
their vulnerability to adversarial attacks poses serious concerns.
Understanding these vulnerabilities and developing effective defense mechanisms
is crucial. DeepFool, an algorithm proposed by Moosavi-Dezfooli et al. (2016),
finds minimal perturbations to misclassify input images. However, DeepFool
lacks a targeted approach, making it less effective in specific attack
scenarios. Also, in previous related works, researchers primarily focus on
success, not considering how much an image is getting distorted; the integrity
of the image quality, and the confidence level to misclassifying. So, in this
paper, we propose Enhanced Targeted DeepFool, an augmented version of DeepFool
that allows targeting specific classes for misclassification and also introduce
a minimum confidence score requirement hyperparameter to enhance flexibility.
Our experiments demonstrate the effectiveness and efficiency of the proposed
method across different deep neural network architectures while preserving
image integrity as much and perturbation rate as less as possible. By using our
approach, the behavior of models can be manipulated arbitrarily using the
perturbed images, as we can specify both the target class and the associated
confidence score, unlike other DeepFool-derivative works, such as Targeted
DeepFool by Gajjar et al. (2022). Results show that one of the deep
convolutional neural network architectures, AlexNet, and one of the
state-of-the-art model Vision Transformer exhibit high robustness to getting
fooled. This approach can have larger implication, as our tuning of confidence
level can expose the robustness of image recognition models. Our code will be
made public upon acceptance of the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Labib_S/0/1/0/all/0/1&quot;&gt;S. M. Fazle Rabby Labib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1&quot;&gt;Joyanta Jyoti Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manab_M/0/1/0/all/0/1&quot;&gt;Meem Arafat Manab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13030">
<title>SIRe-IR: Inverse Rendering for BRDF Reconstruction with Shadow and Illumination Removal in High-Illuminance Scenes. (arXiv:2310.13030v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13030</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural representation has opened up new possibilities for inverse
rendering. However, existing implicit neural inverse rendering methods struggle
to handle strongly illuminated scenes with significant shadows and indirect
illumination. The existence of shadows and reflections can lead to an
inaccurate understanding of scene geometry, making precise factorization
difficult. To this end, we present SIRe-IR, an implicit neural inverse
rendering approach that uses non-linear mapping and regularized visibility
estimation to decompose the scene into environment map, albedo, and roughness.
By accurately modeling the indirect radiance field, normal, visibility, and
direct light simultaneously, we are able to remove both shadows and indirect
illumination in materials without imposing strict constraints on the scene.
Even in the presence of intense illumination, our method recovers high-quality
albedo and roughness with no shadow interference. SIRe-IR outperforms existing
methods in both quantitative and qualitative evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanzhen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yazhen Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xiaogang Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17949">
<title>Instance Segmentation under Occlusions via Location-aware Copy-Paste Data Augmentation. (arXiv:2310.17949v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17949</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion is a long-standing problem in computer vision, particularly in
instance segmentation. ACM MMSports 2023 DeepSportRadar has introduced a
dataset that focuses on segmenting human subjects within a basketball context
and a specialized evaluation metric for occlusion scenarios. Given the modest
size of the dataset and the highly deformable nature of the objects to be
segmented, this challenge demands the application of robust data augmentation
techniques and wisely-chosen deep learning architectures. Our work (ranked 1st
in the competition) first proposes a novel data augmentation technique, capable
of generating more training samples with wider distribution. Then, we adopt a
new architecture - Hybrid Task Cascade (HTC) framework with CBNetV2 as backbone
and MaskIoU head to improve segmentation performance. Furthermore, we employ a
Stochastic Weight Averaging (SWA) training strategy to improve the model&apos;s
generalization. As a result, we achieve a remarkable occlusion score (OM) of
0.533 on the challenge dataset, securing the top-1 position on the leaderboard.
Source code is available at this
https://github.com/nguyendinhson-kaist/MMSports23-Seg-AutoID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1&quot;&gt;Son Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lainsa_M/0/1/0/all/0/1&quot;&gt;Mikel Lainsa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_H/0/1/0/all/0/1&quot;&gt;Hung Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daeyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_G/0/1/0/all/0/1&quot;&gt;Giang Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18639">
<title>Towards Plastic and Stable Exemplar-Free Incremental Learning: A Dual-Learner Framework with Cumulative Parameter Averaging. (arXiv:2310.18639v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18639</link>
<description rdf:parseType="Literal">&lt;p&gt;The dilemma between plasticity and stability presents a significant challenge
in Incremental Learning (IL), especially in the exemplar-free scenario where
accessing old-task samples is strictly prohibited during the learning of a new
task. A straightforward solution to this issue is learning and storing an
independent model for each task, known as Single Task Learning (STL). Despite
the linear growth in model storage with the number of tasks in STL, we
empirically discover that averaging these model parameters can potentially
preserve knowledge across all tasks. Inspired by this observation, we propose a
Dual-Learner framework with Cumulative Parameter Averaging (DLCPA). DLCPA
employs a dual-learner design: a plastic learner focused on acquiring new-task
knowledge and a stable learner responsible for accumulating all learned
knowledge. The knowledge from the plastic learner is transferred to the stable
learner via cumulative parameter averaging. Additionally, several task-specific
classifiers work in cooperation with the stable learner to yield the final
prediction. Specifically, when learning a new task, these modules are updated
in a cyclic manner: i) the plastic learner is initially optimized using a
self-supervised loss besides the supervised loss to enhance the feature
extraction robustness; ii) the stable learner is then updated with respect to
the plastic learner in a cumulative parameter averaging manner to maintain its
task-wise generalization; iii) the task-specific classifier is accordingly
optimized to align with the stable learner. Experimental results on CIFAR-100
and Tiny-ImageNet show that DLCPA outperforms several state-of-the-art
exemplar-free baselines in both Task-IL and Class-IL settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wenju Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yangli-ao Geng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19641">
<title>DistNet2D: Leveraging long-range temporal information for efficient segmentation and tracking. (arXiv:2310.19641v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19641</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting long tracks and lineages from videomicroscopy requires an
extremely low error rate, which is challenging on complex datasets of dense or
deforming cells. Leveraging temporal context is key to overcoming this
challenge. We propose DistNet2D, a new deep neural network (DNN) architecture
for 2D cell segmentation and tracking that leverages both mid- and long-term
temporal information. DistNet2D considers seven frames at the input and uses a
post-processing procedure that exploits information from the entire video to
correct segmentation errors. DistNet2D outperforms two recent methods on two
experimental datasets, one containing densely packed bacterial cells and the
other containing eukaryotic cells. It is integrated into an ImageJ-based
graphical user interface for 2D data visualization, curation, and training.
Finally, we demonstrate the performance of DistNet2D on correlating the size
and shape of cells with their transport properties over large statistics, for
both bacterial and eukaryotic cells.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ollion_J/0/1/0/all/0/1&quot;&gt;Jean Ollion&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maliet_M/0/1/0/all/0/1&quot;&gt;Martin Maliet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giuglaris_C/0/1/0/all/0/1&quot;&gt;Caroline Giuglaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vacher_E/0/1/0/all/0/1&quot;&gt;Elise Vacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deforet_M/0/1/0/all/0/1&quot;&gt;Maxime Deforet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00187">
<title>Decodable and Sample Invariant Continuous Object Encoder. (arXiv:2311.00187v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00187</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Hyper-Dimensional Function Encoding (HDFE). Given samples of a
continuous object (e.g. a function), HDFE produces an explicit vector
representation of the given object, invariant to the sample distribution and
density. Sample distribution and density invariance enables HDFE to
consistently encode continuous objects regardless of their sampling, and
therefore allows neural networks to receive continuous objects as inputs for
machine learning tasks, such as classification and regression. Besides, HDFE
does not require any training and is proved to map the object into an organized
embedding space, which facilitates the training of the downstream tasks. In
addition, the encoding is decodable, which enables neural networks to regress
continuous objects by regressing their encodings. Therefore, HDFE serves as an
interface for processing continuous objects.
&lt;/p&gt;
&lt;p&gt;We apply HDFE to function-to-function mapping, where vanilla HDFE achieves
competitive performance as the state-of-the-art algorithm. We apply HDFE to
point cloud surface normal estimation, where a simple replacement from PointNet
to HDFE leads to immediate 12% and 15% error reductions in two benchmarks. In
addition, by integrating HDFE into the PointNet-based SOTA network, we improve
the SOTA baseline by 2.5% and 1.7% in the same benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_D/0/1/0/all/0/1&quot;&gt;Dehao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1&quot;&gt;Cornelia Ferm&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aloimonos_Y/0/1/0/all/0/1&quot;&gt;Yiannis Aloimonos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02332">
<title>Multimodal Machine Learning in Image-Based and Clinical Biomedicine: Survey and Prospects. (arXiv:2311.02332v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02332</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning (ML) applications in medical artificial intelligence (AI)
systems have shifted from traditional and statistical methods to increasing
application of deep learning models. This survey navigates the current
landscape of multimodal ML, focusing on its profound impact on medical image
analysis and clinical decision support systems. Emphasizing challenges and
innovations in addressing multimodal representation, fusion, translation,
alignment, and co-learning, the paper explores the transformative potential of
multimodal models for clinical predictions. It also questions practical
implementation of such models, bringing attention to the dynamics between
decision support systems and healthcare providers. Despite advancements,
challenges such as data biases and the scarcity of &quot;big data&quot; in many
biomedical domains persist. We conclude with a discussion on effective
innovation and collaborative efforts to further the miss
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warner_E/0/1/0/all/0/1&quot;&gt;Elisa Warner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Joonsang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_W/0/1/0/all/0/1&quot;&gt;William Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Syeda_Mahmood_T/0/1/0/all/0/1&quot;&gt;Tanveer Syeda-Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahn_C/0/1/0/all/0/1&quot;&gt;Charles Kahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gevaert_O/0/1/0/all/0/1&quot;&gt;Olivier Gevaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Arvind Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04071">
<title>Energy-Calibrated VAE with Test Time Free Lunch. (arXiv:2311.04071v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04071</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel generative model that utilizes a
conditional Energy-Based Model (EBM) for enhancing Variational Autoencoder
(VAE), termed Energy-Calibrated VAE (EC-VAE). Specifically, VAEs often suffer
from blurry generated samples due to the lack of a tailored training on the
samples generated in the generative direction. On the other hand, EBMs can
generate high-quality samples but require expensive Markov Chain Monte Carlo
(MCMC) sampling. To address these issues, we introduce a conditional EBM for
calibrating the generative direction of VAE during training, without requiring
it for the generation at test time. In particular, we train EC-VAE upon both
the input data and the calibrated samples with adaptive weight to enhance
efficacy while avoiding MCMC sampling at test time. Furthermore, we extend the
calibration idea of EC-VAE to variational learning and normalizing flows, and
apply EC-VAE to an additional application of zero-shot image restoration via
neural transport prior and range-null theory. We evaluate the proposed method
with two applications, including image generation and zero-shot image
restoration, and the experimental results show that our method achieves the
state-of-the-art performance over single-step non-adversarial generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yihong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_S/0/1/0/all/0/1&quot;&gt;Siya Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xingjian Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yujun Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jing Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05858">
<title>Layer-wise Auto-Weighting for Non-Stationary Test-Time Adaptation. (arXiv:2311.05858v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05858</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the inevitability of domain shifts during inference in real-world
applications, test-time adaptation (TTA) is essential for model adaptation
after deployment. However, the real-world scenario of continuously changing
target distributions presents challenges including catastrophic forgetting and
error accumulation. Existing TTA methods for non-stationary domain shifts,
while effective, incur excessive computational load, making them impractical
for on-device settings. In this paper, we introduce a layer-wise auto-weighting
algorithm for continual and gradual TTA that autonomously identifies layers for
preservation or concentrated adaptation. By leveraging the Fisher Information
Matrix (FIM), we first design the learning weight to selectively focus on
layers associated with log-likelihood changes while preserving unrelated ones.
Then, we further propose an exponential min-max scaler to make certain layers
nearly frozen while mitigating outliers. This minimizes forgetting and error
accumulation, leading to efficient adaptation to non-stationary target
distribution. Experiments on CIFAR-10C, CIFAR-100C, and ImageNet-C show our
method outperforms conventional continual and gradual TTA approaches while
significantly reducing computational load, highlighting the importance of
FIM-based learning weight in adapting to continuously or gradually shifting
target domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Hyeongjun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_I/0/1/0/all/0/1&quot;&gt;Ilhoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_K/0/1/0/all/0/1&quot;&gt;Kwanghoon Sohn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06031">
<title>Diagonal Hierarchical Consistency Learning for Semi-supervised Medical Image Segmentation. (arXiv:2311.06031v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06031</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image segmentation, which is essential for many clinical
applications, has achieved almost human-level performance via data-driven deep
learning techniques. Nevertheless, its performance is predicated upon the
costly process of manually annotating a vast amount of medical images. To this
end, we propose a novel framework for robust semi-supervised medical image
segmentation using diagonal hierarchical consistency learning (DiHC-Net).
First, it is composed of multiple sub-models with identical multi-scale
architecture but with distinct sub-layers, such as up-sampling and
normalisation layers. Second, along with mutual consistency, a novel diagonal
hierarchical consistency is enforced between one model&apos;s intermediate and final
prediction and other models&apos; soft pseudo labels in a diagonal hierarchical
fashion. Experimental results verify the efficacy of our simple framework,
outperforming all previous approaches on public Left Atrium (LA) dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_H/0/1/0/all/0/1&quot;&gt;Heejoon Koo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06185">
<title>An Automated Pipeline for Tumour-Infiltrating Lymphocyte Scoring in Breast Cancer. (arXiv:2311.06185v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06185</link>
<description rdf:parseType="Literal">&lt;p&gt;Tumour-infiltrating lymphocytes (TILs) are considered as a valuable
prognostic markers in both triple-negative and human epidermal growth factor
receptor 2 (HER2) positive breast cancer. In this study, we introduce an
innovative deep learning pipeline based on the Efficient-UNet architecture to
predict the TILs score for breast cancer whole-slide images (WSIs). We first
segment tumour and stromal regions in order to compute a tumour bulk mask. We
then detect TILs within the tumour-associated stroma, generating a TILs score
by closely mirroring the pathologist&apos;s workflow. Our method exhibits
state-of-the-art performance in segmenting tumour/stroma areas and TILs
detection, as demonstrated by internal cross-validation on the TiGER Challenge
training dataset and evaluation on the final leaderboards. Additionally, our
TILs score proves competitive in predicting survival outcomes within the same
challenge, underscoring the clinical relevance and potential of our automated
TILs scoring pipeline as a breast cancer prognostic tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shephard_A/0/1/0/all/0/1&quot;&gt;Adam J Shephard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jahanifar_M/0/1/0/all/0/1&quot;&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dawood_M/0/1/0/all/0/1&quot;&gt;Muhammad Dawood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Graham_S/0/1/0/all/0/1&quot;&gt;Simon Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sidlauskas_K/0/1/0/all/0/1&quot;&gt;Kastytis Sidlauskas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khurram_S/0/1/0/all/0/1&quot;&gt;Syed Ali Khurram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rajpoot_N/0/1/0/all/0/1&quot;&gt;Nasir M Rajpoot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raza_S/0/1/0/all/0/1&quot;&gt;Shan E Ahmed Raza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06322">
<title>Post-training Quantization with Progressive Calibration and Activation Relaxing for Text-to-Image Diffusion Models. (arXiv:2311.06322v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06322</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved great success due to their remarkable
generation ability. However, their high computational overhead is still a
troublesome problem. Recent studies have leveraged post-training quantization
(PTQ) to compress diffusion models. However, most of them only focus on
unconditional models, leaving the quantization of widely used large pretrained
text-to-image models, e.g., Stable Diffusion, largely unexplored. In this
paper, we propose a novel post-training quantization method PCR (Progressive
Calibration and Relaxing) for text-to-image diffusion models, which consists of
a progressive calibration strategy that considers the accumulated quantization
error across timesteps, and an activation relaxing strategy that improves the
performance with negligible cost. Additionally, we demonstrate the previous
metrics for text-to-image diffusion model quantization are not accurate due to
the distribution gap. To tackle the problem, we propose a novel QDiffBench
benchmark, which utilizes data in the same domain for more accurate evaluation.
Besides, QDiffBench also considers the generalization performance of the
quantized model outside the calibration dataset. Extensive experiments on
Stable Diffusion and Stable Diffusion XL demonstrate the superiority of our
method and benchmark. Moreover, we are the first to achieve quantization for
Stable Diffusion XL while maintaining the performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_C/0/1/0/all/0/1&quot;&gt;Chaoyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zewen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yansong Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wenwu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06504">
<title>SCL-VI: Self-supervised Context Learning for Visual Inspection of Industrial Defects. (arXiv:2311.06504v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06504</link>
<description rdf:parseType="Literal">&lt;p&gt;The unsupervised visual inspection of defects in industrial products poses a
significant challenge due to substantial variations in product surfaces.
Current unsupervised models struggle to strike a balance between detecting
texture and object defects, lacking the capacity to discern latent
representations and intricate features. In this paper, we present a novel
self-supervised learning algorithm designed to derive an optimal encoder by
tackling the renowned jigsaw puzzle. Our approach involves dividing the target
image into nine patches, tasking the encoder with predicting the relative
position relationships between any two patches to extract rich semantics.
Subsequently, we introduce an affinity-augmentation method to accentuate
differences between normal and abnormal latent representations. Leveraging the
classic support vector data description algorithm yields final detection
results. Experimental outcomes demonstrate that our proposed method achieves
outstanding detection and segmentation performance on the widely used MVTec AD
dataset, with rates of 95.8% and 96.8%, respectively, establishing a
state-of-the-art benchmark for both texture and object defects. Comprehensive
experimentation underscores the effectiveness of our approach in diverse
industrial applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Haiming Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenyong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07784">
<title>A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks. (arXiv:2311.07784v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07784</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models often suffer from forgetting previously learned
information when trained on new data. This problem is exacerbated in federated
learning (FL), where the data is distributed and can change independently for
each user. Many solutions are proposed to resolve this catastrophic forgetting
in a centralized setting. However, they do not apply directly to FL because of
its unique complexities, such as privacy concerns and resource limitations. To
overcome these challenges, this paper presents a framework for
$\textbf{federated class incremental learning}$ that utilizes a generative
model to synthesize samples from past distributions. This data can be later
exploited alongside the training data to mitigate catastrophic forgetting. To
preserve privacy, the generative model is trained on the server using data-free
methods at the end of each task without requesting data from clients. Moreover,
our solution does not demand the users to store old data or models, which gives
them the freedom to join/leave the training at any time. Additionally, we
introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically
tailored for federated continual learning. We demonstrate significant
improvements compared to existing baselines through extensive experiments on
multiple datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babakniya_S/0/1/0/all/0/1&quot;&gt;Sara Babakniya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabian_Z/0/1/0/all/0/1&quot;&gt;Zalan Fabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chaoyang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soltanolkotabi_M/0/1/0/all/0/1&quot;&gt;Mahdi Soltanolkotabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avestimehr_S/0/1/0/all/0/1&quot;&gt;Salman Avestimehr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08577">
<title>Finding AI-Generated Faces in the Wild. (arXiv:2311.08577v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08577</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-based image generation has continued to rapidly improve, producing
increasingly more realistic images with fewer obvious visual flaws.
AI-generated images are being used to create fake online profiles which in turn
are being used for spam, fraud, and disinformation campaigns. As the general
problem of detecting any type of manipulated or synthesized content is
receiving increasing attention, here we focus on a more narrow task of
distinguishing a real face from an AI-generated face. This is particularly
applicable when tackling inauthentic online accounts with a fake user profile
photo. We show that by focusing on only faces, a more resilient and
general-purpose artifact can be detected that allows for the detection of
AI-generated faces from a variety of GAN- and diffusion-based synthesis
engines, and across image resolutions (as low as 128 x 128 pixels) and
qualities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porcile_G/0/1/0/all/0/1&quot;&gt;Gonzalo J. Aniano Porcile&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gindi_J/0/1/0/all/0/1&quot;&gt;Jack Gindi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundra_S/0/1/0/all/0/1&quot;&gt;Shivansh Mundra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbus_J/0/1/0/all/0/1&quot;&gt;James R. Verbus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farid_H/0/1/0/all/0/1&quot;&gt;Hany Farid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10122">
<title>Video-LLaVA: Learning United Visual Representation by Alignment Before Projection. (arXiv:2311.10122v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10122</link>
<description rdf:parseType="Literal">&lt;p&gt;The Large Vision-Language Model (LVLM) has enhanced the performance of
various downstream tasks in visual-language understanding. Most existing
approaches encode images and videos into separate feature spaces, which are
then fed as inputs to large language models. However, due to the lack of
unified tokenization for images and videos, namely misalignment before
projection, it becomes challenging for a Large Language Model (LLM) to learn
multi-modal interactions from several poor projection layers. In this work, we
unify visual representation into the language feature space to advance the
foundational LLM towards a unified LVLM. As a result, we establish a simple but
robust LVLM baseline, Video-LLaVA, which learns from a mixed dataset of images
and videos, mutually enhancing each other. Video-LLaVA achieves superior
performances on a broad range of 9 image benchmarks across 5 image
question-answering datasets and 4 image benchmark toolkits. Additionally, our
Video-LLaVA also outperforms Video-ChatGPT by 5.8%, 9.9%, 18.6%, and 10.1% on
MSRVTT, MSVD, TGIF, and ActivityNet, respectively. Notably, extensive
experiments demonstrate that Video-LLaVA mutually benefits images and videos
within a unified visual representation, outperforming models designed
specifically for images or videos. We aim for this work to provide modest
insights into the multi-modal inputs for the LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaxi Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10899">
<title>Extraction and Summarization of Explicit Video Content using Multi-Modal Deep Learning. (arXiv:2311.10899v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10899</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increase in video-sharing platforms across the internet, it is
difficult for humans to moderate the data for explicit content. Hence, an
automated pipeline to scan through video data for explicit content has become
the need of the hour. We propose a novel pipeline that uses multi-modal deep
learning to first extract the explicit segments of input videos and then
summarize their content using text to determine its age appropriateness and age
rating. We also evaluate our pipeline&apos;s effectiveness in the end using standard
metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Shaunak Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaggar_R/0/1/0/all/0/1&quot;&gt;Raghav Gaggar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11013">
<title>Implicit Event-RGBD Neural SLAM. (arXiv:2311.11013v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11013</link>
<description rdf:parseType="Literal">&lt;p&gt;Implicit neural SLAM has achieved remarkable progress recently. Nevertheless,
existing methods face significant challenges in non-ideal scenarios, such as
motion blur or lighting variation, which often leads to issues like convergence
failures, localization drifts, and distorted mapping. To address these
challenges, we propose $\textbf{EN-SLAM}$, the first event-RGBD implicit neural
SLAM framework, which effectively leverages the high rate and high dynamic
range advantages of event data for tracking and mapping. Specifically, EN-SLAM
proposes a differentiable CRF (Camera Response Function) rendering technique to
generate distinct RGB and event camera data via a shared radiance field, which
is optimized by learning a unified implicit representation with the captured
event and RGBD supervision. Moreover, based on the temporal difference property
of events, we propose a temporal aggregating optimization strategy for the
event joint tracking and global bundle adjustment, capitalizing on the
consecutive difference constraints of events, significantly enhancing tracking
accuracy and robustness. Finally, we construct the simulated dataset
$\textbf{DEV-Indoors}$ and real captured dataset $\textbf{DEV-Reals}$
containing 6 scenes, 17 sequences with practical motion blur and lighting
changes for evaluations. Experimental results show that our method outperforms
the SOTA methods in both tracking ATE and mapping ACC with a real-time $17$ FPS
in various challenging environments. The code and dataset will be released
soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1&quot;&gt;Delin Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jie Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11306">
<title>UMAAF: Unveiling Aesthetics via Multifarious Attributes of Images. (arXiv:2311.11306v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11306</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing prevalence of smartphones and websites, Image Aesthetic
Assessment (IAA) has become increasingly crucial. While the significance of
attributes in IAA is widely recognized, many attribute-based methods lack
consideration for the selection and utilization of aesthetic attributes. Our
initial step involves the acquisition of aesthetic attributes from both intra-
and inter-perspectives. Within the intra-perspective, we extract the direct
visual attributes of images, constituting the absolute attribute. In the
inter-perspective, our focus lies in modeling the relative score relationships
between images within the same sequence, forming the relative attribute. Then,
to better utilize image attributes in aesthetic assessment, we propose the
Unified Multi-attribute Aesthetic Assessment Framework (UMAAF) to model both
absolute and relative attributes of images. For absolute attributes, we
leverage multiple absolute-attribute perception modules and an
absolute-attribute interacting network. The absolute-attribute perception
modules are first pre-trained on several absolute-attribute learning tasks and
then used to extract corresponding absolute attribute features. The
absolute-attribute interacting network adaptively learns the weight of diverse
absolute-attribute features, effectively integrating them with generic
aesthetic features from various absolute-attribute perspectives and generating
the aesthetic prediction. To model the relative attribute of images, we
consider the relative ranking and relative distance relationships between
images in a Relative-Relation Loss function, which boosts the robustness of the
UMAAF. Furthermore, UMAAF achieves state-of-the-art performance on TAD66K and
AVA datasets, and multiple experiments demonstrate the effectiveness of each
module and the model&apos;s alignment with human preference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weijie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yitian Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xingjiao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Junjie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11354">
<title>Scale-aware competition network for palmprint recognition. (arXiv:2311.11354v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11354</link>
<description rdf:parseType="Literal">&lt;p&gt;Palmprint biometrics garner heightened attention in palm-scanning payment and
social security due to their distinctive attributes. However, prevailing
methodologies singularly prioritize texture orientation, neglecting the
significant texture scale dimension. We design an innovative network for
concurrently extracting intra-scale and inter-scale features to redress this
limitation. This paper proposes a scale-aware competitive network (SAC-Net),
which includes the Inner-Scale Competition Module (ISCM) and the Across-Scale
Competition Module (ASCM) to capture texture characteristics related to
orientation and scale. ISCM efficiently integrates learnable Gabor filters and
a self-attention mechanism to extract rich orientation data and discern
textures with long-range discriminative properties. Subsequently, ASCM
leverages a competitive strategy across various scales to effectively
encapsulate the competitive texture scale elements. By synergizing ISCM and
ASCM, our method adeptly characterizes palmprint features. Rigorous
experimentation across three benchmark datasets unequivocally demonstrates our
proposed approach&apos;s exceptional recognition performance and resilience relative
to state-of-the-art alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chengrui Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Ziyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Min Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teoh_A/0/1/0/all/0/1&quot;&gt;Andrew Beng Jin Teoh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11439">
<title>Improved Defect Detection and Classification Method for Advanced IC Nodes by Using Slicing Aided Hyper Inference with Refinement Strategy. (arXiv:2311.11439v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11439</link>
<description rdf:parseType="Literal">&lt;p&gt;In semiconductor manufacturing, lithography has often been the manufacturing
step defining the smallest possible pattern dimensions. In recent years,
progress has been made towards high-NA (Numerical Aperture) EUVL
(Extreme-Ultraviolet-Lithography) paradigm, which promises to advance pattern
shrinking (2 nm node and beyond). However, a significant increase in stochastic
defects and the complexity of defect detection becomes more pronounced with
high-NA. Present defect inspection techniques (both non-machine learning and
machine learning based), fail to achieve satisfactory performance at high-NA
dimensions. In this work, we investigate the use of the Slicing Aided Hyper
Inference (SAHI) framework for improving upon current techniques. Using SAHI,
inference is performed on size-increased slices of the SEM images. This leads
to the object detector&apos;s receptive field being more effective in capturing
small defect instances. First, the performance on previously investigated
semiconductor datasets is benchmarked across various configurations, and the
SAHI approach is demonstrated to substantially enhance the detection of small
defects, by approx. 2x. Afterwards, we also demonstrated application of SAHI
leads to flawless detection rates on a new test dataset, with scenarios not
encountered during training, whereas previous trained models failed. Finally,
we formulate an extension of SAHI that does not significantly reduce
true-positive predictions while eliminating false-positive predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ridder_V/0/1/0/all/0/1&quot;&gt;Vic De Ridder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_B/0/1/0/all/0/1&quot;&gt;Bappaditya Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanco_V/0/1/0/all/0/1&quot;&gt;Victor Blanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halder_S/0/1/0/all/0/1&quot;&gt;Sandip Halder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waeyenberge_B/0/1/0/all/0/1&quot;&gt;Bartel Van Waeyenberge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11700">
<title>GS-SLAM: Dense Visual SLAM with 3D Gaussian Splatting. (arXiv:2311.11700v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11700</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce $\textbf{GS-SLAM}$ that first utilizes 3D
Gaussian representation in the Simultaneous Localization and Mapping (SLAM)
system. It facilitates a better balance between efficiency and accuracy.
Compared to recent SLAM methods employing neural implicit representations, our
method utilizes a real-time differentiable splatting rendering pipeline that
offers significant speedup to map optimization and RGB-D re-rendering.
Specifically, we propose an adaptive expansion strategy that adds new or
deletes noisy 3D Gaussian in order to efficiently reconstruct new observed
scene geometry and improve the mapping of previously observed areas. This
strategy is essential to extend 3D Gaussian representation to reconstruct the
whole scene rather than synthesize a static object in existing methods.
Moreover, in the pose tracking process, an effective coarse-to-fine technique
is designed to select reliable 3D Gaussian representations to optimize camera
pose, resulting in runtime reduction and robust estimation. Our method achieves
competitive performance compared with existing state-of-the-art real-time
methods on the Replica, TUM-RGBD datasets. The source code will be released
soon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_C/0/1/0/all/0/1&quot;&gt;Chi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_D/0/1/0/all/0/1&quot;&gt;Delin Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11808">
<title>Robot Hand-Eye Calibration using Structure-from-Motion. (arXiv:2311.11808v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11808</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we propose a new flexible method for hand-eye calibration. The
vast majority of existing hand-eye calibration techniques requires a
calibration rig which is used in conjunction with camera pose estimation
methods. Instead, we combine structure-from-motion with known robot motions and
we show that the solution can be obtained in linear form. The latter solves for
both the hand-eye parameters and for the unknown scale factor inherent with
structure-from-motion methods. The algebraic analysis that is made possible
with such a linear formulation allows to investigate not only the well known
case of general screw motions but also such singular motions as pure
translations, pure rotations, and planar motions. In essence, the robot-mounted
camera looks to an unknown rigid layout, tracks points over an image sequence
and estimates the camera-to-robot relationship. Such a self calibration process
is relevant for unmanned vehicles, robots working in remote places, and so
forth. We conduct a large number of experiments which validate the quality of
the method by comparing it with existing ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreff_N/0/1/0/all/0/1&quot;&gt;Nicolas Andreff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horaud_R/0/1/0/all/0/1&quot;&gt;Radu Horaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espiau_B/0/1/0/all/0/1&quot;&gt;Bernard Espiau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11908">
<title>Continual Learning: Applications and the Road Forward. (arXiv:2311.11908v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11908</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning is a sub-field of machine learning, which aims to allow
machine learning models to continuously learn on new data, by accumulating
knowledge without forgetting what was learned in the past. In this work, we
take a step back, and ask: &quot;Why should one care about continual learning in the
first place?&quot;. We set the stage by surveying recent continual learning papers
published at three major machine learning conferences, and show that
memory-constrained settings dominate the field. Then, we discuss five open
problems in machine learning, and even though they seem unrelated to continual
learning at first sight, we show that continual learning will inevitably be
part of their solution. These problems are model-editing, personalization,
on-device learning, faster (re-)training and reinforcement learning. Finally,
by comparing the desiderata from these unsolved problems and the current
assumptions in continual learning, we highlight and discuss four future
directions for continual learning research. We hope that this work offers an
interesting perspective on the future of continual learning, while displaying
its potential value and the paths we have to pursue in order to make it
successful. This work is the result of the many discussions the authors had at
the Dagstuhl seminar on Deep Continual Learning, in March 2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verwimp_E/0/1/0/all/0/1&quot;&gt;Eli Verwimp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljundi_R/0/1/0/all/0/1&quot;&gt;Rahaf Aljundi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_David_S/0/1/0/all/0/1&quot;&gt;Shai Ben-David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethge_M/0/1/0/all/0/1&quot;&gt;Matthias Bethge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cossu_A/0/1/0/all/0/1&quot;&gt;Andrea Cossu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gepperth_A/0/1/0/all/0/1&quot;&gt;Alexander Gepperth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_T/0/1/0/all/0/1&quot;&gt;Tyler L. Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hullermeier_E/0/1/0/all/0/1&quot;&gt;Eyke H&amp;#xfc;llermeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanan_C/0/1/0/all/0/1&quot;&gt;Christopher Kanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kudithipudi_D/0/1/0/all/0/1&quot;&gt;Dhireesha Kudithipudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lampert_C/0/1/0/all/0/1&quot;&gt;Christoph H. Lampert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundt_M/0/1/0/all/0/1&quot;&gt;Martin Mundt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popescu_A/0/1/0/all/0/1&quot;&gt;Adrian Popescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tolias_A/0/1/0/all/0/1&quot;&gt;Andreas S. Tolias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost van de Weijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lomonaco_V/0/1/0/all/0/1&quot;&gt;Vincenzo Lomonaco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuytelaars_T/0/1/0/all/0/1&quot;&gt;Tinne Tuytelaars&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ven_G/0/1/0/all/0/1&quot;&gt;Gido M. van de Ven&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>