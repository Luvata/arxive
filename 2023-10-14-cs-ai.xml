<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-10-12T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07747" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07771" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07802" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07867" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07885" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08085" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08276" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08292" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.09348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.07580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.12389" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.02809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00457" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17319" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13869" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2310.07724">
<title>Visual Forecasting as a Mid-level Representation for Avoidance. (arXiv:2310.07724v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.07724</link>
<description rdf:parseType="Literal">&lt;p&gt;The challenge of navigation in environments with dynamic objects continues to
be a central issue in the study of autonomous agents. While predictive methods
hold promise, their reliance on precise state information makes them less
practical for real-world implementation. This study presents visual forecasting
as an innovative alternative. By introducing intuitive visual cues, this
approach projects the future trajectories of dynamic objects to improve agent
perception and enable anticipatory actions. Our research explores two distinct
strategies for conveying predictive information through visual forecasting: (1)
sequences of bounding boxes, and (2) augmented paths. To validate the proposed
visual forecasting strategies, we initiate evaluations in simulated
environments using the Unity engine and then extend these evaluations to
real-world scenarios to assess both practicality and effectiveness. The results
confirm the viability of visual forecasting as a promising solution for
navigation and obstacle avoidance in dynamic environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hsuan-Kung Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_T/0/1/0/all/0/1&quot;&gt;Tsung-Chih Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting-Ru Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chun-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jou-Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chun-Yi Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07726">
<title>Towards the Vulnerability of Watermarking Artificial Intelligence Generated Content. (arXiv:2310.07726v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07726</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence Generated Content (AIGC) is gaining great popularity
in social media, with many commercial services available. These services
leverage advanced generative models, such as latent diffusion models and large
language models, to generate creative content (e.g., realistic images, fluent
sentences) for users. The usage of such generated content needs to be highly
regulated, as the service providers need to ensure the users do not violate the
usage policies (e.g., abuse for commercialization, generating and distributing
unsafe content).
&lt;/p&gt;
&lt;p&gt;Numerous watermarking approaches have been proposed recently. However, in
this paper, we show that an adversary can easily break these watermarking
mechanisms. Specifically, we consider two possible attacks. (1) Watermark
removal: the adversary can easily erase the embedded watermark from the
generated content and then use it freely without the regulation of the service
provider. (2) Watermark forge: the adversary can create illegal content with
forged watermarks from another user, causing the service provider to make wrong
attributions. We propose WMaGi, a unified framework to achieve both attacks in
a holistic way. The key idea is to leverage a pre-trained diffusion model for
content processing, and a generative adversarial network for watermark removing
or forging. We evaluate WMaGi on different datasets and embedding setups. The
results prove that it can achieve high success rates while maintaining the
quality of the generated content. Compared with existing diffusion model-based
attacks, WMaGi is 5,050$\sim$11,000$\times$ faster.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Shangwei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianwei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07728">
<title>AI Algorithm for the Generation of Three-Dimensional Accessibility Ramps in Grasshopper / Rhinoceros 7. (arXiv:2310.07728v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2310.07728</link>
<description rdf:parseType="Literal">&lt;p&gt;Often overlooked as a component of urban development, accessibility
infrastructure is undeniably crucial in daily life. Accessibility ramps are one
of the most common types of accessibility infrastructure, and serve to benefit
not only people with mobile impairments but also able-bodied third parties.
While the necessity of accessibility ramps is acknowledged, actual
implementation fails in light of the limits of manpower required for the design
stage. In response, we present an algorithm capable of the automatic generation
of a feasible accessibility ramp based on a 3D model of the relevant
environment. Through the manual specification of initial and terminal points
within a 3D model, the algorithm uses AI search algorithms to determine the
optimal pathway connecting these points. Essential components in devising a
wheelchair-accessible ramp are encoded within the process, as evaluated by the
algorithm, including but not limited to elevation differentials, spatial
constraints, and gradient specifications. From this, the algorithm then
generates the pathway to be expanded into a full-scale, usable model of a ramp,
which then can be easily exported and transformed through inter-software
exchanges. Though some human input is still required following the generation
stage, the minimising of human resources provides significant boosts of
efficiency in the design process thus lowering the threshold for the
incorporation of accessibility features in future urban design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Antonio Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Leila Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1&quot;&gt;Brandon Yeo Pei Hui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07747">
<title>Accountability in Offline Reinforcement Learning: Explaining Decisions with a Corpus of Examples. (arXiv:2310.07747v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07747</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning transparent, interpretable controllers with offline data in
decision-making systems is an essential area of research due to its potential
to reduce the risk of applications in real-world systems. However, in
responsibility-sensitive settings such as healthcare, decision accountability
is of paramount importance, yet has not been adequately addressed by the
literature. This paper introduces the Accountable Offline Controller (AOC) that
employs the offline dataset as the Decision Corpus and performs accountable
control based on a tailored selection of examples, referred to as the Corpus
Subset. ABC operates effectively in low-data scenarios, can be extended to the
strictly offline imitation setting, and displays qualities of both conservation
and adaptability. We assess ABC&apos;s performance in both simulated and real-world
healthcare scenarios, emphasizing its capability to manage offline control
tasks with high levels of performance while maintaining accountability.
&lt;/p&gt;
&lt;p&gt;Keywords: Interpretable Reinforcement Learning, Explainable Reinforcement
Learning, Reinforcement Learning Transparency, Offline Reinforcement Learning,
Batched Control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huyuk_A/0/1/0/all/0/1&quot;&gt;Alihan H&amp;#xfc;y&amp;#xfc;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jarrett_D/0/1/0/all/0/1&quot;&gt;Daniel Jarrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07771">
<title>DrivingDiffusion: Layout-Guided multi-view driving scene video generation with latent diffusion model. (arXiv:2310.07771v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07771</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing popularity of autonomous driving based on the powerful
and unified bird&apos;s-eye-view (BEV) representation, a demand for high-quality and
large-scale multi-view video data with accurate annotation is urgently
required. However, such large-scale multi-view data is hard to obtain due to
expensive collection and annotation costs. To alleviate the problem, we propose
a spatial-temporal consistent diffusion framework DrivingDiffusion, to generate
realistic multi-view videos controlled by 3D layout. There are three challenges
when synthesizing multi-view videos given a 3D layout: How to keep 1)
cross-view consistency and 2) cross-frame consistency? 3) How to guarantee the
quality of the generated instances? Our DrivingDiffusion solves the problem by
cascading the multi-view single-frame image generation step, the single-view
video generation step shared by multiple cameras, and post-processing that can
handle long video generation. In the multi-view model, the consistency of
multi-view images is ensured by information exchange between adjacent cameras.
In the temporal model, we mainly query the information that needs attention in
subsequent frame generation from the multi-view images of the first frame. We
also introduce the local prompt to effectively improve the quality of generated
instances. In post-processing, we further enhance the cross-view consistency of
subsequent frames and extend the video length by employing temporal sliding
window algorithm. Without any extra cost, our model can generate large-scale
realistic multi-camera driving videos in complex urban scenes, fueling the
downstream driving tasks. The code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaofan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yifu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07793">
<title>GenTKG: Generative Forecasting on Temporal Knowledge Graph. (arXiv:2310.07793v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07793</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advancements in large language models (LLMs) have ignited interest
in the temporal knowledge graph (tKG) domain, where conventional carefully
designed embedding-based and rule-based models dominate. The question remains
open of whether pre-trained LLMs can understand structured temporal relational
data and replace them as the foundation model for temporal relational
forecasting. Therefore, we bring temporal knowledge forecasting into the
generative setting. However, challenges occur in the huge chasms between
complex temporal graph data structure and sequential natural expressions LLMs
can handle, and between the enormous data sizes of tKGs and heavy computation
costs of finetuning LLMs. To address these challenges, we propose a novel
retrieval augmented generation framework that performs generative forecasting
on tKGs named GenTKG, which combines a temporal logical rule-based retrieval
strategy and lightweight parameter-efficient instruction tuning. Extensive
experiments have shown that GenTKG outperforms conventional methods of temporal
relational forecasting under low computation resources. GenTKG also highlights
remarkable transferability with exceeding performance on unseen datasets
without re-training. Our work reveals the huge potential of LLMs in the tKG
domain and opens a new frontier for generative forecasting on tKGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_R/0/1/0/all/0/1&quot;&gt;Ruotong Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xu Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunpu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tresp_V/0/1/0/all/0/1&quot;&gt;Volker Tresp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07799">
<title>A Transfer-Learning-Based Prognosis Prediction Paradigm that Bridges Data Distribution Shift across EMR Datasets. (arXiv:2310.07799v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07799</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the limited information about emerging diseases, symptoms are hard to
be noticed and recognized, so that the window for clinical intervention could
be ignored. An effective prognostic model is expected to assist doctors in
making right diagnosis and designing personalized treatment plan, so to
promptly prevent unfavorable outcomes. However, in the early stage of a
disease, limited data collection and clinical experiences, plus the concern out
of privacy and ethics, may result in restricted data availability for
reference, to the extent that even data labels are difficult to mark correctly.
In addition, Electronic Medical Record (EMR) data of different diseases or of
different sources of the same disease can prove to be having serious
cross-dataset feature misalignment problems, greatly mutilating the efficiency
of deep learning models. This article introduces a transfer learning method to
build a transition model from source dataset to target dataset. By way of
constraining the distribution shift of features generated in disparate domains,
domain-invariant features that are exclusively relative to downstream tasks are
captured, so to cultivate a unified domain-invariant encoder across various
task domains to achieve better feature representation. Experimental results of
several target tasks demonstrate that our proposed model outperforms competing
baseline methods and has higher rate of training convergence, especially in
dealing with limited data amount. A multitude of experiences have proven the
efficacy of our method to provide more accurate predictions concerning newly
emergent pandemics and other diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaohe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liantao Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07800">
<title>Explainable Attention for Few-shot Learning and Beyond. (arXiv:2310.07800v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07800</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention mechanisms have exhibited promising potential in enhancing learning
models by identifying salient portions of input data. This is particularly
valuable in scenarios where limited training samples are accessible due to
challenges in data collection and labeling. Drawing inspiration from human
recognition processes, we posit that an AI baseline&apos;s performance could be more
accurate and dependable if it is exposed to essential segments of raw data
rather than the entire input dataset, akin to human perception. However, the
task of selecting these informative data segments, referred to as hard
attention finding, presents a formidable challenge. In situations with few
training samples, existing studies struggle to locate such informative regions
due to the large number of training parameters that cannot be effectively
learned from the available limited samples. In this study, we introduce a novel
and practical framework for achieving explainable hard attention finding,
specifically tailored for few-shot learning scenarios, called FewXAT. Our
approach employs deep reinforcement learning to implement the concept of hard
attention, directly impacting raw input data and thus rendering the process
interpretable for human understanding. Through extensive experimentation across
various benchmark datasets, we demonstrate the efficacy of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikpour_B/0/1/0/all/0/1&quot;&gt;Bahareh Nikpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07801">
<title>Trajectory-aware Principal Manifold Framework for Data Augmentation and Image Generation. (arXiv:2310.07801v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07801</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation for deep learning benefits model training, image
transformation, medical imaging analysis and many other fields. Many existing
methods generate new samples from a parametric distribution, like the Gaussian,
with little attention to generate samples along the data manifold in either the
input or feature space. In this paper, we verify that there are theoretical and
practical advantages of using the principal manifold hidden in the feature
space than the Gaussian distribution. We then propose a novel trajectory-aware
principal manifold framework to restore the manifold backbone and generate
samples along a specific trajectory. On top of the autoencoder architecture, we
further introduce an intrinsic dimension regularization term to make the
manifold more compact and enable few-shot image generation. Experimental
results show that the novel framework is able to extract more compact manifold
representation, improve classification accuracy and generate smooth
transformation among few samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_E/0/1/0/all/0/1&quot;&gt;Elvis Han Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_W/0/1/0/all/0/1&quot;&gt;Weng Kee Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Donghui Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07802">
<title>An Information Bottleneck Characterization of the Understanding-Workload Tradeoff. (arXiv:2310.07802v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07802</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in artificial intelligence (AI) have underscored the need for
explainable AI (XAI) to support human understanding of AI systems.
Consideration of human factors that impact explanation efficacy, such as mental
workload and human understanding, is central to effective XAI design. Existing
work in XAI has demonstrated a tradeoff between understanding and workload
induced by different types of explanations. Explaining complex concepts through
abstractions (hand-crafted groupings of related problem features) has been
shown to effectively address and balance this workload-understanding tradeoff.
In this work, we characterize the workload-understanding balance via the
Information Bottleneck method: an information-theoretic approach which
automatically generates abstractions that maximize informativeness and minimize
complexity. In particular, we establish empirical connections between workload
and complexity and between understanding and informativeness through
human-subject experiments. This empirical link between human factors and
information-theoretic concepts provides an important mathematical
characterization of the workload-understanding tradeoff which enables
user-tailored XAI design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanneman_L/0/1/0/all/0/1&quot;&gt;Lindsay Sanneman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_M/0/1/0/all/0/1&quot;&gt;Mycal Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_J/0/1/0/all/0/1&quot;&gt;Julie Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07803">
<title>A general mechanism of humor: reformulating the semantic overlap. (arXiv:2310.07803v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07803</link>
<description rdf:parseType="Literal">&lt;p&gt;This article proposes a cognitive mechanism of humour of general
applicability, not restricted to verbal communication. It is indebted to
Raskin&apos;s concept of script overlap, and conforms to the incongruity-resolution
theoretical framework, but it is built on the notion of constraint, an abstract
correspondence between sets of data. Under this view, script overlap is an
outcome of a more abstractly described phenomenon, constraint overlap. The
important concept of the overlooked argument is introduced to characterise the
two overlapping constraints -- overt and covert. Their inputs and outputs are
not directly encoded in utterances, but implicated by them, and their overlap
results in another overlap at the level of the communicated utterances, that
the incongruity reveals. Our hypothesis assumes as a given that the evocation
of such constraints is a cognitive effect of the inferential process by which a
hearer interprets utterances. We base this assumption on Hofstadter&apos;s theory of
analogy-making as the essence of human thought. By substituting &quot;stimuli&quot; of
any kind for &quot;utterances&quot; in this model, we obtain a mechanism as easily
applicable to non-verbal communication -- slapstick, cartoons -- and we propose
it describes the necessary and sufficient conditions for a communicative act in
any modality to carry humour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_J/0/1/0/all/0/1&quot;&gt;Javier Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07805">
<title>Generative Modeling with Phase Stochastic Bridges. (arXiv:2310.07805v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07805</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models (DMs) represent state-of-the-art generative models for
continuous inputs. DMs work by constructing a Stochastic Differential Equation
(SDE) in the input space (ie, position space), and using a neural network to
reverse it. In this work, we introduce a novel generative modeling framework
grounded in \textbf{phase space dynamics}, where a phase space is defined as
{an augmented space encompassing both position and velocity.} Leveraging
insights from Stochastic Optimal Control, we construct a path measure in the
phase space that enables efficient sampling. {In contrast to DMs, our framework
demonstrates the capability to generate realistic data points at an early stage
of dynamics propagation.} This early prediction sets the stage for efficient
data generation by leveraging additional velocity information along the
trajectory. On standard image generation benchmarks, our model yields favorable
performance over baselines in the regime of small Number of Function
Evaluations (NFEs). Furthermore, our approach rivals the performance of
diffusion models equipped with efficient sampling techniques, underscoring its
potential as a new tool generative modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianrong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiatao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_L/0/1/0/all/0/1&quot;&gt;Laurent Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theodorou_E/0/1/0/all/0/1&quot;&gt;Evangelos A. Theodorou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Susskind_J/0/1/0/all/0/1&quot;&gt;Josh Susskind&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_S/0/1/0/all/0/1&quot;&gt;Shuangfei Zhai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07812">
<title>Automatic Identification of Stone-Handling Behaviour in Japanese Macaques Using LabGym Artificial Intelligence. (arXiv:2310.07812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07812</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest advancements in artificial intelligence technology have opened
doors to the analysis of intricate behaviours. In light of this, ethologists
are actively exploring the potential of these innovations to streamline the
time-intensive process of behavioural analysis using video data. In the realm
of primatology, several tools have been developed for this purpose.
Nonetheless, each of these tools grapples with technical constraints that we
aim to surmount. To address these limitations, we have established a
comprehensive protocol designed to harness the capabilities of a cutting-edge
tool, LabGym. Our primary objective was to evaluate LabGym&apos;s suitability for
the analysis of primate behaviour, with a focus on Japanese macaques as our
model subjects. We have successfully developed a model that demonstrates a high
degree of accuracy in detecting Japanese macaques stone-handling behaviour. Our
behavioural analysis model was completed as per our initial expectations and
LabGym succeed to recognise stone-handling behaviour on videos. However, it is
important to note that our study&apos;s ability to draw definitive conclusions
regarding the quality of the behavioural analysis is hampered by the absence of
quantitative data within the specified timeframe. Nevertheless, our model
represents the pioneering endeavour, as far as our knowledge extends, in
leveraging LabGym for the analysis of primate behaviours. It lays the
groundwork for potential future research in this promising field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ardoin_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;o Ardoin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sueur_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe9;dric Sueur&lt;/a&gt; (IPHC, ANTHROPO LAB, IUF)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07818">
<title>Exploring the Relationship between Analogy Identification and Sentence Structure Encoding in Large Language Models. (arXiv:2310.07818v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07818</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying analogies plays a pivotal role in human cognition and language
proficiency. In the last decade, there has been extensive research on word
analogies in the form of ``A is to B as C is to D.&apos;&apos; However, there is a
growing interest in analogies that involve longer text, such as sentences and
collections of sentences, which convey analogous meanings. While the current
NLP research community evaluates the ability of Large Language Models (LLMs) to
identify such analogies, the underlying reasons behind these abilities warrant
deeper investigation. Furthermore, the capability of LLMs to encode both
syntactic and semantic structures of language within their embeddings has
garnered significant attention with the surge in their utilization. In this
work, we examine the relationship between the abilities of multiple LLMs to
identify sentence analogies, and their capacity to encode syntactic and
semantic structures. Through our analysis, we find that analogy identification
ability of LLMs is positively correlated with their ability to encode syntactic
and semantic structures of sentences. Specifically, we find that the LLMs which
capture syntactic structures better, also have higher abilities in identifying
sentence analogies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijesiriwardene_T/0/1/0/all/0/1&quot;&gt;Thilini Wijesiriwardene&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wickramarachchi_R/0/1/0/all/0/1&quot;&gt;Ruwan Wickramarachchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reganti_A/0/1/0/all/0/1&quot;&gt;Aishwarya Naresh Reganti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vinija Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheth_A/0/1/0/all/0/1&quot;&gt;Amit Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Amitava Das&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07830">
<title>Does Synthetic Data Make Large Language Models More Efficient?. (arXiv:2310.07830v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07830</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural Language Processing (NLP) has undergone transformative changes with
the advent of deep learning methodologies. One challenge persistently
confronting researchers is the scarcity of high-quality, annotated datasets
that drive these models. This paper explores the nuances of synthetic data
generation in NLP, with a focal point on template-based question generation. By
assessing its advantages, including data augmentation potential and the
introduction of structured variety, we juxtapose these benefits against
inherent limitations, such as the risk of overfitting and the constraints posed
by pre-defined templates. Drawing from empirical evaluations, we demonstrate
the impact of template-based synthetic data on the performance of modern
transformer models. We conclude by emphasizing the delicate balance required
between synthetic and real-world data, and the future trajectories of
integrating synthetic data in model training pipelines. The findings aim to
guide NLP practitioners in harnessing synthetic data&apos;s potential, ensuring
optimal model performance in diverse applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_S/0/1/0/all/0/1&quot;&gt;Sia Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Omar_M/0/1/0/all/0/1&quot;&gt;Marwan Omar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07831">
<title>When, Why and How Much? Adaptive Learning Rate Scheduling by Refinement. (arXiv:2310.07831v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07831</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning rate schedules used in practice bear little resemblance to those
recommended by theory. We close much of this theory/practice gap, and as a
consequence are able to derive new problem-adaptive learning rate schedules.
Our key technical contribution is a refined analysis of learning rate schedules
for a wide class of optimization algorithms (including SGD). In contrast to
most prior works that study the convergence of the average iterate, we study
the last iterate, which is what most people use in practice. When considering
only worst-case analysis, our theory predicts that the best choice is the
linear decay schedule: a popular choice in practice that sets the stepsize
proportionally to $1 - t/T$, where $t$ is the current iteration and $T$ is the
total number of steps. To go beyond this worst-case analysis, we use the
observed gradient norms to derive schedules refined for any particular task.
These refined schedules exhibit learning rate warm-up and rapid learning rate
annealing near the end of training. Ours is the first systematic approach to
automatically yield both of these properties. We perform the most comprehensive
evaluation of learning rate schedules to date, evaluating across 10 diverse
deep learning problems, a series of LLMs, and a suite of logistic regression
problems. We validate that overall, the linear-decay schedule matches or
outperforms all commonly used default schedules including cosine annealing, and
that our schedule refinement method gives further improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Defazio_A/0/1/0/all/0/1&quot;&gt;Aaron Defazio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cutkosky_A/0/1/0/all/0/1&quot;&gt;Ashok Cutkosky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_H/0/1/0/all/0/1&quot;&gt;Harsh Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishchenko_K/0/1/0/all/0/1&quot;&gt;Konstantin Mishchenko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07838">
<title>Towards the Fundamental Limits of Knowledge Transfer over Finite Domains. (arXiv:2310.07838v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07838</link>
<description rdf:parseType="Literal">&lt;p&gt;We characterize the statistical efficiency of knowledge transfer through $n$
samples from a teacher to a probabilistic student classifier with input space
$\mathcal S$ over labels $\mathcal A$. We show that privileged information at
three progressive levels accelerates the transfer. At the first level, only
samples with hard labels are known, via which the maximum likelihood estimator
attains the minimax rate $\sqrt{{|{\mathcal S}||{\mathcal A}|}/{n}}$. The
second level has the teacher probabilities of sampled labels available in
addition, which turns out to boost the convergence rate lower bound to
${{|{\mathcal S}||{\mathcal A}|}/{n}}$. However, under this second data
acquisition protocol, minimizing a naive adaptation of the cross-entropy loss
results in an asymptotically biased student. We overcome this limitation and
achieve the fundamental limit by using a novel empirical variant of the squared
error logit loss. The third level further equips the student with the soft
labels (complete logits) on ${\mathcal A}$ given every sampled input, thereby
provably enables the student to enjoy a rate ${|{\mathcal S}|}/{n}$ free of
$|{\mathcal A}|$. We find any Kullback-Leibler divergence minimizer to be
optimal in the last case. Numerical simulations distinguish the four learners
and corroborate our theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qingyue Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07849">
<title>Synthetic Data Generation with Large Language Models for Text Classification: Potential and Limitations. (arXiv:2310.07849v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07849</link>
<description rdf:parseType="Literal">&lt;p&gt;The collection and curation of high-quality training data is crucial for
developing text classification models with superior performance, but it is
often associated with significant costs and time investment. Researchers have
recently explored using large language models (LLMs) to generate synthetic
datasets as an alternative approach. However, the effectiveness of the
LLM-generated synthetic data in supporting model training is inconsistent
across different classification tasks. To better understand factors that
moderate the effectiveness of the LLM-generated synthetic data, in this study,
we look into how the performance of models trained on these synthetic data may
vary with the subjectivity of classification. Our results indicate that
subjectivity, at both the task level and instance level, is negatively
associated with the performance of the model trained on synthetic data. We
conclude by discussing the implications of our work on the potential and
limitations of leveraging LLM for synthetic data generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuoyan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hangxiao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Ming Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07867">
<title>Cheap Talking Algorithms. (arXiv:2310.07867v1 [econ.TH])</title>
<link>http://arxiv.org/abs/2310.07867</link>
<description rdf:parseType="Literal">&lt;p&gt;We simulate behaviour of independent reinforcement learning algorithms
playing the Crawford and Sobel (1982) game of strategic information
transmission. We show that a sender and a receiver training together converge
to strategies close to the exante optimal equilibrium of the game. Hence,
communication takes place to the largest extent predicted by Nash equilibrium
given the degree of conflict of interest between agents. The conclusion is
shown to be robust to alternative specifications of the hyperparameters and of
the game. We discuss implications for theories of equilibrium selection in
information transmission games, for work on emerging communication among
algorithms in computer science and for the economics of collusions in markets
populated by artificially intelligent agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Condorelli_D/0/1/0/all/0/1&quot;&gt;Daniele Condorelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Furlan_M/0/1/0/all/0/1&quot;&gt;Massimiliano Furlan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07871">
<title>Hierarchical Pretraining on Multimodal Electronic Health Records. (arXiv:2310.07871v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07871</link>
<description rdf:parseType="Literal">&lt;p&gt;Pretraining has proven to be a powerful technique in natural language
processing (NLP), exhibiting remarkable success in various NLP downstream
tasks. However, in the medical domain, existing pretrained models on electronic
health records (EHR) fail to capture the hierarchical nature of EHR data,
limiting their generalization capability across diverse downstream tasks using
a single pretrained model. To tackle this challenge, this paper introduces a
novel, general, and unified pretraining framework called MEDHMP, specifically
designed for hierarchically multimodal EHR data. The effectiveness of the
proposed MEDHMP is demonstrated through experimental results on eight
downstream tasks spanning three levels. Comparisons against eighteen baselines
further highlight the efficacy of our approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaochen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Junyu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_S/0/1/0/all/0/1&quot;&gt;Suhan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yuan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_F/0/1/0/all/0/1&quot;&gt;Fenglong Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07875">
<title>TabLib: A Dataset of 627M Tables with Context. (arXiv:2310.07875v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07875</link>
<description rdf:parseType="Literal">&lt;p&gt;It is well-established that large, diverse datasets play a pivotal role in
the performance of modern AI systems for text and image modalities. However,
there are no datasets for tabular data of comparable size and diversity to
those available for text and images. Thus we present &quot;TabLib&apos;&apos;, a compilation
of 627 million tables totaling 69 TiB, along with 867B tokens of context.
TabLib was extracted from numerous file formats, including CSV, HTML, SQLite,
PDF, Excel, and others, sourced from GitHub and Common Crawl. The size and
diversity of TabLib offer considerable promise in the table modality,
reminiscent of the original promise of foundational datasets for text and
images, such as The Pile and LAION.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eggert_G/0/1/0/all/0/1&quot;&gt;Gus Eggert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_K/0/1/0/all/0/1&quot;&gt;Kevin Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biven_M/0/1/0/all/0/1&quot;&gt;Mike Biven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waugh_J/0/1/0/all/0/1&quot;&gt;Justin Waugh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07881">
<title>DeePref: Deep Reinforcement Learning For Video Prefetching In Content Delivery Networks. (arXiv:2310.07881v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2310.07881</link>
<description rdf:parseType="Literal">&lt;p&gt;Content Delivery Networks carry the majority of Internet traffic, and the
increasing demand for video content as a major IP traffic across the Internet
highlights the importance of caching and prefetching optimization algorithms.
Prefetching aims to make data available in the cache before the requester
places its request to reduce access time and improve the Quality of Experience
on the user side. Prefetching is well investigated in operating systems,
compiler instructions, in-memory cache, local storage systems, high-speed
networks, and cloud systems. Traditional prefetching techniques are well
adapted to a particular access pattern, but fail to adapt to sudden variations
or randomization in workloads. This paper explores the use of reinforcement
learning to tackle the changes in user access patterns and automatically adapt
over time. To this end, we propose, DeePref, a Deep Reinforcement Learning
agent for online video content prefetching in Content Delivery Networks.
DeePref is a prefetcher implemented on edge networks and is agnostic to
hardware design, operating systems, and applications. Our results show that
DeePref DRQN, using a real-world dataset, achieves a 17% increase in
prefetching accuracy and a 28% increase in prefetching coverage on average
compared to baseline approaches that use video content popularity as a building
block to statically or dynamically make prefetching decisions. We also study
the possibility of transfer learning of statistical models from one edge
network into another, where unseen user requests from unknown distribution are
observed. In terms of transfer learning, the increase in prefetching accuracy
and prefetching coverage are [$30%$, $10%$], respectively. Our source code will
be available on Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alkassab_N/0/1/0/all/0/1&quot;&gt;Nawras Alkassab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chin-Tser Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botran_T/0/1/0/all/0/1&quot;&gt;Tania Lorido Botran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07882">
<title>The Thousand Faces of Explainable AI Along the Machine Learning Life Cycle: Industrial Reality and Current State of Research. (arXiv:2310.07882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07882</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the practical relevance of explainable
artificial intelligence (XAI) with a special focus on the producing industries
and relate them to the current state of academic XAI research. Our findings are
based on an extensive series of interviews regarding the role and applicability
of XAI along the Machine Learning (ML) lifecycle in current industrial practice
and its expected relevance in the future. The interviews were conducted among a
great variety of roles and key stakeholders from different industry sectors. On
top of that, we outline the state of XAI research by providing a concise review
of the relevant literature. This enables us to provide an encompassing overview
covering the opinions of the surveyed persons as well as the current state of
academic research. By comparing our interview results with the current research
approaches we reveal several discrepancies. While a multitude of different XAI
approaches exists, most of them are centered around the model evaluation phase
and data scientists. Their versatile capabilities for other stages are
currently either not sufficiently explored or not popular among practitioners.
In line with existing work, our findings also confirm that more efforts are
needed to enable also non-expert users&apos; interpretation and understanding of
opaque AI models with existing methods and frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Decker_T/0/1/0/all/0/1&quot;&gt;Thomas Decker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_R/0/1/0/all/0/1&quot;&gt;Ralf Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koebler_A/0/1/0/all/0/1&quot;&gt;Alexander Koebler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lebacher_M/0/1/0/all/0/1&quot;&gt;Michael Lebacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnitzer_R/0/1/0/all/0/1&quot;&gt;Ronald Schnitzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_S/0/1/0/all/0/1&quot;&gt;Stefan H. Weber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07885">
<title>Leader-Follower Neural Networks with Local Error Signals Inspired by Complex Collectives. (arXiv:2310.07885v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07885</link>
<description rdf:parseType="Literal">&lt;p&gt;The collective behavior of a network with heterogeneous, resource-limited
information processing units (e.g., group of fish, flock of birds, or network
of neurons) demonstrates high self-organization and complexity. These emergent
properties arise from simple interaction rules where certain individuals can
exhibit leadership-like behavior and influence the collective activity of the
group. Motivated by the intricacy of these collectives, we propose a neural
network (NN) architecture inspired by the rules observed in nature&apos;s collective
ensembles. This NN structure contains workers that encompass one or more
information processing units (e.g., neurons, filters, layers, or blocks of
layers). Workers are either leaders or followers, and we train a
leader-follower neural network (LFNN) by leveraging local error signals and
optionally incorporating backpropagation (BP) and global loss. We investigate
worker behavior and evaluate LFNNs through extensive experimentation. Our LFNNs
trained with local error signals achieve significantly lower error rates than
previous BP-free algorithms on MNIST and CIFAR-10 and even surpass BP-enabled
baselines. In the case of ImageNet, our LFNN-l demonstrates superior
scalability and outperforms previous BP-free algorithms by a significant
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Chenzhong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_M/0/1/0/all/0/1&quot;&gt;Mingxi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiongye Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazarian_S/0/1/0/all/0/1&quot;&gt;Shahin Nazarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irimia_A/0/1/0/all/0/1&quot;&gt;Andrei Irimia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1&quot;&gt;Paul Bogdan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07889">
<title>LangNav: Language as a Perceptual Representation for Navigation. (arXiv:2310.07889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07889</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the use of language as a perceptual representation for
vision-and-language navigation. Our approach uses off-the-shelf vision systems
(for image captioning and object detection) to convert an agent&apos;s egocentric
panoramic view at each time step into natural language descriptions. We then
finetune a pretrained language model to select an action, based on the current
view and the trajectory history, that would best fulfill the navigation
instructions. In contrast to the standard setup which adapts a pretrained
language model to work directly with continuous visual features from pretrained
vision models, our approach instead uses (discrete) language as the perceptual
representation. We explore two use cases of our language-based navigation
(LangNav) approach on the R2R vision-and-language navigation benchmark:
generating synthetic trajectories from a prompted large language model (GPT-4)
with which to finetune a smaller language model; and sim-to-real transfer where
we transfer a policy learned on a simulated environment (ALFRED) to a
real-world environment (R2R). Our approach is found to improve upon strong
baselines that rely on visual features in settings where only a few gold
trajectories (10-100) are available, demonstrating the potential of using
language as a perceptual representation for navigation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1&quot;&gt;Bowen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panda_R/0/1/0/all/0/1&quot;&gt;Rameswar Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_S/0/1/0/all/0/1&quot;&gt;SouYoung Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feris_R/0/1/0/all/0/1&quot;&gt;Rogerio Feris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliva_A/0/1/0/all/0/1&quot;&gt;Aude Oliva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isola_P/0/1/0/all/0/1&quot;&gt;Phillip Isola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07894">
<title>Efficient Integrators for Diffusion Generative Models. (arXiv:2310.07894v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07894</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models suffer from slow sample generation at inference time.
Therefore, developing a principled framework for fast deterministic/stochastic
sampling for a broader class of diffusion models is a promising direction. We
propose two complementary frameworks for accelerating sample generation in
pre-trained models: Conjugate Integrators and Splitting Integrators. Conjugate
integrators generalize DDIM, mapping the reverse diffusion dynamics to a more
amenable space for sampling. In contrast, splitting-based integrators, commonly
used in molecular dynamics, reduce the numerical simulation error by cleverly
alternating between numerical updates involving the data and auxiliary
variables. After extensively studying these methods empirically and
theoretically, we present a hybrid method that leads to the best-reported
performance for diffusion models in augmented spaces. Applied to Phase Space
Langevin Diffusion [Pandey &amp;amp; Mandt, 2023] on CIFAR-10, our deterministic and
stochastic samplers achieve FID scores of 2.11 and 2.36 in only 100 network
function evaluations (NFE) as compared to 2.57 and 2.63 for the best-performing
baselines, respectively. Our code and model checkpoints will be made publicly
available at \url{https://github.com/mandt-lab/PSLD}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandey_K/0/1/0/all/0/1&quot;&gt;Kushagra Pandey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudolph_M/0/1/0/all/0/1&quot;&gt;Maja Rudolph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07899">
<title>RoboCLIP: One Demonstration is Enough to Learn Robot Policies. (arXiv:2310.07899v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07899</link>
<description rdf:parseType="Literal">&lt;p&gt;Reward specification is a notoriously difficult problem in reinforcement
learning, requiring extensive expert supervision to design robust reward
functions. Imitation learning (IL) methods attempt to circumvent these problems
by utilizing expert demonstrations but typically require a large number of
in-domain expert demonstrations. Inspired by advances in the field of
Video-and-Language Models (VLMs), we present RoboCLIP, an online imitation
learning method that uses a single demonstration (overcoming the large data
requirement) in the form of a video demonstration or a textual description of
the task to generate rewards without manual reward function design.
Additionally, RoboCLIP can also utilize out-of-domain demonstrations, like
videos of humans solving the task for reward generation, circumventing the need
to have the same demonstration and deployment domains. RoboCLIP utilizes
pretrained VLMs without any finetuning for reward generation. Reinforcement
learning agents trained with RoboCLIP rewards demonstrate 2-3 times higher
zero-shot performance than competing imitation learning methods on downstream
robot manipulation tasks, doing so using only one video/text demonstration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1&quot;&gt;Sumedh A Sontakke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jesse Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnold_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien M. R. Arnold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1&quot;&gt;Karl Pertsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biyik_E/0/1/0/all/0/1&quot;&gt;Erdem B&amp;#x131;y&amp;#x131;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1&quot;&gt;Laurent Itti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07908">
<title>Recurrent networks recognize patterns with low-dimensional oscillations. (arXiv:2310.07908v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2310.07908</link>
<description rdf:parseType="Literal">&lt;p&gt;This study proposes a novel dynamical mechanism for pattern recognition
discovered by interpreting a recurrent neural network (RNN) trained on a simple
task inspired by the SET card game. We interpreted the trained RNN as
recognizing patterns via phase shifts in a low-dimensional limit cycle in a
manner analogous to transitions in a finite state automaton (FSA). We further
validated this interpretation by handcrafting a simple oscillatory model that
reproduces the dynamics of the trained RNN. Our findings not only suggest of a
potential dynamical mechanism capable of pattern recognition, but also suggest
of a potential neural implementation of FSA. Above all, this work contributes
to the growing discourse on deep learning model interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Murray_K/0/1/0/all/0/1&quot;&gt;Keith T. Murray&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07917">
<title>A Review of Machine Learning Techniques in Imbalanced Data and Future Trends. (arXiv:2310.07917v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07917</link>
<description rdf:parseType="Literal">&lt;p&gt;For over two decades, detecting rare events has been a challenging task among
researchers in the data mining and machine learning domain. Real-life problems
inspire researchers to navigate and further improve data processing and
algorithmic approaches to achieve effective and computationally efficient
methods for imbalanced learning. In this paper, we have collected and reviewed
258 peer-reviewed papers from archival journals and conference papers in an
attempt to provide an in-depth review of various approaches in imbalanced
learning from technical and application perspectives. This work aims to provide
a structured review of methods used to address the problem of imbalanced data
in various domains and create a general guideline for researchers in academia
or industry who want to dive into the broad field of machine learning using
large-scale imbalanced data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jafarigol_E/0/1/0/all/0/1&quot;&gt;Elaheh Jafarigol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trafalis_T/0/1/0/all/0/1&quot;&gt;Theodore Trafalis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07918">
<title>Contextualized Policy Recovery: Modeling and Interpreting Medical Decisions with Adaptive Imitation Learning. (arXiv:2310.07918v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07918</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable policy learning seeks to estimate intelligible decision
policies from observed actions; however, existing models fall short by forcing
a tradeoff between accuracy and interpretability. This tradeoff limits
data-driven interpretations of human decision-making process. e.g. to audit
medical decisions for biases and suboptimal practices, we require models of
decision processes which provide concise descriptions of complex behaviors.
Fundamentally, existing approaches are burdened by this tradeoff because they
represent the underlying decision process as a universal policy, when in fact
human decisions are dynamic and can change drastically with contextual
information. Thus, we propose Contextualized Policy Recovery (CPR), which
re-frames the problem of modeling complex decision processes as a multi-task
learning problem in which complex decision policies are comprised of
context-specific policies. CPR models each context-specific policy as a linear
observation-to-action mapping, and generates new decision models
$\textit{on-demand}$ as contexts are updated with new observations. CPR is
compatible with fully offline and partially observable decision environments,
and can be tailored to incorporate any recurrent black-box model or
interpretable decision model. We assess CPR through studies on simulated and
real data, achieving state-of-the-art performance on the canonical tasks of
predicting antibiotic prescription in intensive care units ($+22\%$ AUROC vs.
previous SOTA) and predicting MRI prescription for Alzheimer&apos;s patients
($+7.7\%$ AUROC vs. previous SOTA). With this improvement in predictive
performance, CPR closes the accuracy gap between interpretable and black-box
methods for policy learning, allowing high-resolution exploration and analysis
of context-specific decision models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deuschel_J/0/1/0/all/0/1&quot;&gt;Jannik Deuschel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellington_C/0/1/0/all/0/1&quot;&gt;Caleb N. Ellington&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lengerich_B/0/1/0/all/0/1&quot;&gt;Benjamin J. Lengerich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yingtao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friederich_P/0/1/0/all/0/1&quot;&gt;Pascal Friederich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07931">
<title>D2 Pruning: Message Passing for Balancing Diversity and Difficulty in Data Pruning. (arXiv:2310.07931v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07931</link>
<description rdf:parseType="Literal">&lt;p&gt;Analytical theories suggest that higher-quality data can lead to lower test
errors in models trained on a fixed data budget. Moreover, a model can be
trained on a lower compute budget without compromising performance if a dataset
can be stripped of its redundancies. Coreset selection (or data pruning) seeks
to select a subset of the training data so as to maximize the performance of
models trained on this subset, also referred to as coreset. There are two
dominant approaches: (1) geometry-based data selection for maximizing data
diversity in the coreset, and (2) functions that assign difficulty scores to
samples based on training dynamics. Optimizing for data diversity leads to a
coreset that is biased towards easier samples, whereas, selection by difficulty
ranking omits easy samples that are necessary for the training of deep learning
models. This demonstrates that data diversity and importance scores are two
complementary factors that need to be jointly considered during coreset
selection. We represent a dataset as an undirected graph and propose a novel
pruning algorithm, D2 Pruning, that uses forward and reverse message passing
over this dataset graph for coreset selection. D2 Pruning updates the
difficulty scores of each example by incorporating the difficulty of its
neighboring examples in the dataset graph. Then, these updated difficulty
scores direct a graph-based sampling method to select a coreset that
encapsulates both diverse and difficult regions of the dataset space. We
evaluate supervised and self-supervised versions of our method on various
vision and language datasets. Results show that D2 Pruning improves coreset
selection over previous state-of-the-art methods for up to 70% pruning rates.
Additionally, we find that using D2 Pruning for filtering large multimodal
datasets leads to increased diversity in the dataset and improved
generalization of pretrained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maharana_A/0/1/0/all/0/1&quot;&gt;Adyasha Maharana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_P/0/1/0/all/0/1&quot;&gt;Prateek Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07932">
<title>What Matters to You? Towards Visual Representation Alignment for Robot Learning. (arXiv:2310.07932v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.07932</link>
<description rdf:parseType="Literal">&lt;p&gt;When operating in service of people, robots need to optimize rewards aligned
with end-user preferences. Since robots will rely on raw perceptual inputs like
RGB images, their rewards will inevitably use visual representations. Recently
there has been excitement in using representations from pre-trained visual
models, but key to making these work in robotics is fine-tuning, which is
typically done via proxy tasks like dynamics prediction or enforcing temporal
cycle-consistency. However, all these proxy tasks bypass the human&apos;s input on
what matters to them, exacerbating spurious correlations and ultimately leading
to robot behaviors that are misaligned with user preferences. In this work, we
propose that robots should leverage human feedback to align their visual
representations with the end-user and disentangle what matters for the task. We
propose Representation-Aligned Preference-based Learning (RAPL), a method for
solving the visual representation alignment problem and visual reward learning
problem through the lens of preference-based learning and optimal transport.
Across experiments in X-MAGICAL and in robotic manipulation, we find that
RAPL&apos;s reward consistently generates preferred robot behaviors with high sample
efficiency, and shows strong zero-shot generalization when the visual
representation is learned from a different embodiment than the robot&apos;s.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_R/0/1/0/all/0/1&quot;&gt;Ran Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenfeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomizuka_M/0/1/0/all/0/1&quot;&gt;Masayoshi Tomizuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajcsy_A/0/1/0/all/0/1&quot;&gt;Andrea Bajcsy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07937">
<title>Co-NavGPT: Multi-Robot Cooperative Visual Semantic Navigation using Large Language Models. (arXiv:2310.07937v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.07937</link>
<description rdf:parseType="Literal">&lt;p&gt;In advanced human-robot interaction tasks, visual target navigation is
crucial for autonomous robots navigating unknown environments. While numerous
approaches have been developed in the past, most are designed for single-robot
operations, which often suffer from reduced efficiency and robustness due to
environmental complexities. Furthermore, learning policies for multi-robot
collaboration are resource-intensive. To address these challenges, we propose
Co-NavGPT, an innovative framework that integrates Large Language Models (LLMs)
as a global planner for multi-robot cooperative visual target navigation.
Co-NavGPT encodes the explored environment data into prompts, enhancing LLMs&apos;
scene comprehension. It then assigns exploration frontiers to each robot for
efficient target search. Experimental results on Habitat-Matterport 3D (HM3D)
demonstrate that Co-NavGPT surpasses existing models in success rates and
efficiency without any learning process, demonstrating the vast potential of
LLMs in multi-robot collaboration domains. The supplementary video, prompts,
and code can be accessed via the following link:
\href{https://sites.google.com/view/co-navgpt}{https://sites.google.com/view/co-navgpt}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bangguo Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_H/0/1/0/all/0/1&quot;&gt;Hamidreza Kasaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Ming Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07944">
<title>AutoRepo: A general framework for multi-modal LLM-based automated construction reporting. (arXiv:2310.07944v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07944</link>
<description rdf:parseType="Literal">&lt;p&gt;Ensuring the safety, quality, and timely completion of construction projects
is paramount, with construction inspections serving as a vital instrument
towards these goals. Nevertheless, the predominantly manual approach of
present-day inspections frequently results in inefficiencies and inadequate
information management. Such methods often fall short of providing holistic,
exhaustive assessments, consequently engendering regulatory oversights and
potential safety hazards. To address this issue, this paper presents a novel
framework named AutoRepo for automated generation of construction inspection
reports. The unmanned vehicles efficiently perform construction inspections and
collect scene information, while the multimodal large language models (LLMs)
are leveraged to automatically generate the inspection reports. The framework
was applied and tested on a real-world construction site, demonstrating its
potential to expedite the inspection process, significantly reduce resource
allocation, and produce high-quality, regulatory standard-compliant inspection
reports. This research thus underscores the immense potential of multimodal
large language models in revolutionizing construction inspection practices,
signaling a significant leap forward towards a more efficient and safer
construction management paradigm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_H/0/1/0/all/0/1&quot;&gt;Hongxu Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xincong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_R/0/1/0/all/0/1&quot;&gt;Runhao Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Heng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07957">
<title>A New Approach Towards Autoformalization. (arXiv:2310.07957v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.07957</link>
<description rdf:parseType="Literal">&lt;p&gt;Verifying mathematical proofs is difficult, but can be automated with the
assistance of a computer. Autoformalization is the task of automatically
translating natural language mathematics into a formal language that can be
verified by a program. This is a challenging task, and especially for
higher-level mathematics found in research papers. Research paper mathematics
requires large amounts of background and context. In this paper, we propose an
avenue towards tackling autoformalization for research-level mathematics, by
breaking the task into easier and more approachable subtasks: unlinked
formalization (formalization with unlinked definitions and theorems), entity
linking (linking to the proper theorems and definitions), and finally adjusting
types so it passes the type checker. In addition, we present arXiv2Formal, a
benchmark dataset for unlinked formalization consisting of 50 theorems
formalized for the Lean theorem prover sampled from papers on arXiv.org. We
welcome any contributions from the community to future versions of this
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_N/0/1/0/all/0/1&quot;&gt;Nilay Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flanigan_J/0/1/0/all/0/1&quot;&gt;Jeffrey Flanigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_R/0/1/0/all/0/1&quot;&gt;Rahul Saha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07972">
<title>Interpretable Diffusion via Information Decomposition. (arXiv:2310.07972v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.07972</link>
<description rdf:parseType="Literal">&lt;p&gt;Denoising diffusion models enable conditional generation and density modeling
of complex relationships like images and text. However, the nature of the
learned relationships is opaque making it difficult to understand precisely
what relationships between words and parts of an image are captured, or to
predict the effect of an intervention. We illuminate the fine-grained
relationships learned by diffusion models by noticing a precise relationship
between diffusion and information decomposition. Exact expressions for mutual
information and conditional mutual information can be written in terms of the
denoising model. Furthermore, pointwise estimates can be easily estimated as
well, allowing us to ask questions about the relationships between specific
images and captions. Decomposing information even further to understand which
variables in a high-dimensional space carry information is a long-standing
problem. For diffusion models, we show that a natural non-negative
decomposition of mutual information emerges, allowing us to quantify
informative relationships between words and pixels in an image. We exploit
these new relations to measure the compositional understanding of diffusion
models, to do unsupervised localization of objects in images, and to measure
effects when selectively editing images through prompt interventions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xianghao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_O/0/1/0/all/0/1&quot;&gt;Ollie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yogatama_D/0/1/0/all/0/1&quot;&gt;Dani Yogatama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steeg_G/0/1/0/all/0/1&quot;&gt;Greg Ver Steeg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07975">
<title>Self-supervised visual learning for analyzing firearms trafficking activities on the Web. (arXiv:2310.07975v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07975</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated visual firearms classification from RGB images is an important
real-world task with applications in public space security, intelligence
gathering and law enforcement investigations. When applied to images massively
crawled from the World Wide Web (including social media and dark Web sites), it
can serve as an important component of systems that attempt to identify
criminal firearms trafficking networks, by analyzing Big Data from open-source
intelligence. Deep Neural Networks (DNN) are the state-of-the-art methodology
for achieving this, with Convolutional Neural Networks (CNN) being typically
employed. The common transfer learning approach consists of pretraining on a
large-scale, generic annotated dataset for whole-image classification, such as
ImageNet-1k, and then finetuning the DNN on a smaller, annotated,
task-specific, downstream dataset for visual firearms classification. Neither
Visual Transformer (ViT) neural architectures nor Self-Supervised Learning
(SSL) approaches have been so far evaluated on this critical task. SSL
essentially consists of replacing the traditional supervised pretraining
objective with an unsupervised pretext task that does not require ground-truth
labels..
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konstantakos_S/0/1/0/all/0/1&quot;&gt;Sotirios Konstantakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chalkiadaki_D/0/1/0/all/0/1&quot;&gt;Despina Ioanna Chalkiadaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mademlis_I/0/1/0/all/0/1&quot;&gt;Ioannis Mademlis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrysochoou_A/0/1/0/all/0/1&quot;&gt;Adamantia Anna Rebolledo Chrysochoou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_G/0/1/0/all/0/1&quot;&gt;Georgios Th. Papadopoulos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07984">
<title>Large Language Models for Scientific Synthesis, Inference and Explanation. (arXiv:2310.07984v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07984</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are a form of artificial intelligence systems whose
primary knowledge consists of the statistical patterns, semantic relationships,
and syntactical structures of language1. Despite their limited forms of
&quot;knowledge&quot;, these systems are adept at numerous complex tasks including
creative writing, storytelling, translation, question-answering, summarization,
and computer code generation. However, they have yet to demonstrate advanced
applications in natural science. Here we show how large language models can
perform scientific synthesis, inference, and explanation. We present a method
for using general-purpose large language models to make inferences from
scientific datasets of the form usually associated with special-purpose machine
learning algorithms. We show that the large language model can augment this
&quot;knowledge&quot; by synthesizing from the scientific literature. When a conventional
machine learning system is augmented with this synthesized and inferred
knowledge it can outperform the current state of the art across a range of
benchmark tasks for predicting molecular properties. This approach has the
further advantage that the large language model can explain the machine
learning system&apos;s predictions. We anticipate that our framework will open new
avenues for AI to accelerate the pace of scientific discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yizhen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1&quot;&gt;Huan Yee Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_J/0/1/0/all/0/1&quot;&gt;Jiaxin Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh T.N. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+May_L/0/1/0/all/0/1&quot;&gt;Lauren T. May&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_G/0/1/0/all/0/1&quot;&gt;Geoffrey I. Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07995">
<title>HeightFormer: A Multilevel Interaction and Image-adaptive Classification-regression Network for Monocular Height Estimation with Aerial Images. (arXiv:2310.07995v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07995</link>
<description rdf:parseType="Literal">&lt;p&gt;Height estimation has long been a pivotal topic within measurement and remote
sensing disciplines, proving critical for endeavours such as 3D urban
modelling, MR and autonomous driving. Traditional methods utilise stereo
matching or multisensor fusion, both well-established techniques that typically
necessitate multiple images from varying perspectives and adjunct sensors like
SAR, leading to substantial deployment costs. Single image height estimation
has emerged as an attractive alternative, boasting a larger data source variety
and simpler deployment. However, current methods suffer from limitations such
as fixed receptive fields, a lack of global information interaction, leading to
noticeable instance-level height deviations. The inherent complexity of height
prediction can result in a blurry estimation of object edge depth when using
mainstream regression methods based on fixed height division. This paper
presents a comprehensive solution for monocular height estimation in remote
sensing, termed HeightFormer, combining multilevel interactions and
image-adaptive classification-regression. It features the Multilevel
Interaction Backbone (MIB) and Image-adaptive Classification-regression Height
Generator (ICG). MIB supplements the fixed sample grid in CNN of the
conventional backbone network with tokens of different interaction ranges. It
is complemented by a pixel-, patch-, and feature map-level hierarchical
interaction mechanism, designed to relay spatial geometry information across
different scales and introducing a global receptive field to enhance the
quality of instance-level height estimation. The ICG dynamically generates
height partition for each image and reframes the traditional regression task,
using a refinement from coarse to fine classification-regression that
significantly mitigates the innate ill-posedness issue and drastically improves
edge sharpness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yidan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiyu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_L/0/1/0/all/0/1&quot;&gt;Lulu Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hui Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yunping Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07997">
<title>Point-NeuS: Point-Guided Neural Implicit Surface Reconstruction by Volume Rendering. (arXiv:2310.07997v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.07997</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learning neural implicit surface by volume rendering has been a
promising way for multi-view reconstruction. However, limited accuracy and
excessive time complexity remain bottlenecks that current methods urgently need
to overcome. To address these challenges, we propose a new method called
Point-NeuS, utilizing point-guided mechanisms to achieve accurate and efficient
reconstruction. Point modeling is organically embedded into the volume
rendering to enhance and regularize the representation of implicit surface.
Specifically, to achieve precise point guidance and noise robustness, aleatoric
uncertainty of the point cloud is modeled to capture the distribution of noise
and estimate the reliability of points. Additionally, a Neural Projection
module connecting points and images is introduced to add geometric constraints
to the Signed Distance Function (SDF). To better compensate for geometric bias
between volume rendering and point modeling, high-fidelity points are filtered
into an Implicit Displacement Network to improve the representation of SDF.
Benefiting from our effective point guidance, lightweight networks are employed
to achieve an impressive 11x speedup compared to NeuS. Extensive experiments
show that our method yields high-quality surfaces, especially for fine-grained
details and smooth regions. Moreover, it exhibits strong robustness to both
noisy and sparse data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Wanjuan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wenbing Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07998">
<title>A Novel Statistical Measure for Out-of-Distribution Detection in Data Quality Assurance. (arXiv:2310.07998v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.07998</link>
<description rdf:parseType="Literal">&lt;p&gt;Data outside the problem domain poses significant threats to the security of
AI-based intelligent systems. Aiming to investigate the data domain and
out-of-distribution (OOD) data in AI quality management (AIQM) study, this
paper proposes to use deep learning techniques for feature representation and
develop a novel statistical measure for OOD detection. First, to extract
low-dimensional representative features distinguishing normal and OOD data, the
proposed research combines the deep auto-encoder (AE) architecture and neuron
activation status for feature engineering. Then, using local conditional
probability (LCP) in data reconstruction, a novel and superior statistical
measure is developed to calculate the score of OOD detection. Experiments and
evaluations are conducted on image benchmark datasets and an industrial
dataset. Through comparative analysis with other common statistical measures in
OOD detection, the proposed research is validated as feasible and effective in
OOD and AIQM studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_T/0/1/0/all/0/1&quot;&gt;Tinghui Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echizen_I/0/1/0/all/0/1&quot;&gt;Isao Echizen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_Y/0/1/0/all/0/1&quot;&gt;Yoshiki Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08008">
<title>Effects of Human Adversarial and Affable Samples on BERT Generalizability. (arXiv:2310.08008v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08008</link>
<description rdf:parseType="Literal">&lt;p&gt;BERT-based models have had strong performance on leaderboards, yet have been
demonstrably worse in real-world settings requiring generalization. Limited
quantities of training data is considered a key impediment to achieving
generalizability in machine learning. In this paper, we examine the impact of
training \textit{data quality}, not quantity, on a model&apos;s generalizability. We
consider two characteristics of training data: the portion of human-adversarial
(h-adversarial), i.e., sample pairs with seemingly minor differences but
different ground-truth labels, and human-affable (h-affable) training samples,
i.e., sample pairs with minor differences but the same ground-truth label. We
find that for a fixed size of training samples, as a rule of thumb, having
10-30\% h-adversarial instances improves the precision, and therefore F1, by up
to 20 points in the tasks of text classification and relation extraction.
Increasing h-adversarials beyond this range can result in performance plateaus
or even degradation.In contrast, h-affables may not contribute to a model&apos;s
generalizability and may even degrade generalization performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elangovan_A/0/1/0/all/0/1&quot;&gt;Aparna Elangovan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiayuan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verspoor_K/0/1/0/all/0/1&quot;&gt;Karin Verspoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08026">
<title>Beyond Sharing Weights in Decoupling Feature Learning Network for UAV RGB-Infrared Vehicle Re-Identification. (arXiv:2310.08026v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08026</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to the capacity of performing full-time target search, cross-modality
vehicle re-identification (Re-ID) based on unmanned aerial vehicle (UAV) is
gaining more attention in both video surveillance and public security. However,
this promising and innovative research has not been studied sufficiently due to
the data inadequacy issue. Meanwhile, the cross-modality discrepancy and
orientation discrepancy challenges further aggravate the difficulty of this
task. To this end, we pioneer a cross-modality vehicle Re-ID benchmark named
UAV Cross-Modality Vehicle Re-ID (UCM-VeID), containing 753 identities with
16015 RGB and 13913 infrared images. Moreover, to meet cross-modality
discrepancy and orientation discrepancy challenges, we present a hybrid weights
decoupling network (HWDNet) to learn the shared discriminative
orientation-invariant features. For the first challenge, we proposed a hybrid
weights siamese network with a well-designed weight restrainer and its
corresponding objective function to learn both modality-specific and modality
shared information. In terms of the second challenge, three effective
decoupling structures with two pretext tasks are investigated to learn
orientation-invariant feature. Comprehensive experiments are carried out to
validate the effectiveness of the proposed method. The dataset and codes will
be released at https://github.com/moonstarL/UAV-CM-VeID.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xingyue Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jiahao Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_K/0/1/0/all/0/1&quot;&gt;Kangcheng Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_P/0/1/0/all/0/1&quot;&gt;Ping Zhong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08032">
<title>Incorporating Domain Knowledge Graph into Multimodal Movie Genre Classification with Self-Supervised Attention and Contrastive Learning. (arXiv:2310.08032v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08032</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal movie genre classification has always been regarded as a demanding
multi-label classification task due to the diversity of multimodal data such as
posters, plot summaries, trailers and metadata. Although existing works have
made great progress in modeling and combining each modality, they still face
three issues: 1) unutilized group relations in metadata, 2) unreliable
attention allocation, and 3) indiscriminative fused features. Given that the
knowledge graph has been proven to contain rich information, we present a novel
framework that exploits the knowledge graph from various perspectives to
address the above problems. As a preparation, the metadata is processed into a
domain knowledge graph. A translate model for knowledge graph embedding is
adopted to capture the relations between entities. Firstly we retrieve the
relevant embedding from the knowledge graph by utilizing group relations in
metadata and then integrate it with other modalities. Next, we introduce an
Attention Teacher module for reliable attention allocation based on
self-supervised learning. It learns the distribution of the knowledge graph and
produces rational attention weights. Finally, a Genre-Centroid Anchored
Contrastive Learning module is proposed to strengthen the discriminative
ability of fused features. The embedding space of anchors is initialized from
the genre entities in the knowledge graph. To verify the effectiveness of our
framework, we collect a larger and more challenging dataset named MM-IMDb 2.0
compared with the MM-IMDb dataset. The experimental results on two datasets
demonstrate that our model is superior to the state-of-the-art methods. We will
release the code in the near future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guilin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuanyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongrui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yiming Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chenlong Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Ye Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08034">
<title>Receive, Reason, and React: Drive as You Say with Large Language Models in Autonomous Vehicles. (arXiv:2310.08034v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2310.08034</link>
<description rdf:parseType="Literal">&lt;p&gt;The fusion of human-centric design and artificial intelligence (AI)
capabilities has opened up new possibilities for next-generation autonomous
vehicles that go beyond transportation. These vehicles can dynamically interact
with passengers and adapt to their preferences. This paper proposes a novel
framework that leverages Large Language Models (LLMs) to enhance the
decision-making process in autonomous vehicles. By utilizing LLMs&apos; linguistic
and contextual understanding abilities with specialized tools, we aim to
integrate the language and reasoning capabilities of LLMs into autonomous
vehicles. Our research includes experiments in HighwayEnv, a collection of
environments for autonomous driving and tactical decision-making tasks, to
explore LLMs&apos; interpretation, interaction, and reasoning in various scenarios.
We also examine real-time personalization, demonstrating how LLMs can influence
driving behaviors based on verbal commands. Our empirical results highlight the
substantial advantages of utilizing chain-of-thought prompting, leading to
improved driving decisions, and showing the potential for LLMs to enhance
personalized driving experiences through ongoing verbal feedback. The proposed
framework aims to transform autonomous vehicle operations, offering
personalized support, transparent decision-making, and continuous learning to
enhance safety and effectiveness. We achieve user-centric, transparent, and
adaptive autonomous driving ecosystems supported by the integration of LLMs
into autonomous vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_C/0/1/0/all/0/1&quot;&gt;Can Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yunsheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wenqian Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziran Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08039">
<title>Rethinking Large-scale Pre-ranking System: Entire-chain Cross-domain Models. (arXiv:2310.08039v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2310.08039</link>
<description rdf:parseType="Literal">&lt;p&gt;Industrial systems such as recommender systems and online advertising, have
been widely equipped with multi-stage architectures, which are divided into
several cascaded modules, including matching, pre-ranking, ranking and
re-ranking. As a critical bridge between matching and ranking, existing
pre-ranking approaches mainly endure sample selection bias (SSB) problem owing
to ignoring the entire-chain data dependence, resulting in sub-optimal
performances. In this paper, we rethink pre-ranking system from the perspective
of the entire sample space, and propose Entire-chain Cross-domain Models (ECM),
which leverage samples from the whole cascaded stages to effectively alleviate
SSB problem. Besides, we design a fine-grained neural structure named ECMM to
further improve the pre-ranking accuracy. Specifically, we propose a
cross-domain multi-tower neural network to comprehensively predict for each
stage result, and introduce the sub-networking routing strategy with $L0$
regularization to reduce computational costs. Evaluations on real-world
large-scale traffic logs demonstrate that our pre-ranking models outperform
SOTA methods while time consumption is maintained within an acceptable level,
which achieves better trade-off between efficiency and effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jinbo Song&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruoran Huang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyang Wang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mingming Chen&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yafei Yao&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chaosheng Fan&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_C/0/1/0/all/0/1&quot;&gt;Changping Peng&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhangang Lin&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jinghe Hu&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jingping Shao&lt;/a&gt; (1) ((1) Marketing and Commercialization Center, JD.com)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08041">
<title>QLLM: Accurate and Efficient Low-Bitwidth Quantization for Large Language Models. (arXiv:2310.08041v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08041</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) excel in NLP, but their demands hinder their
widespread deployment. While Quantization-Aware Training (QAT) offers a
solution, its extensive training costs make Post-Training Quantization (PTQ) a
more practical approach for LLMs. In existing studies, activation outliers in
particular channels are identified as the bottleneck to PTQ accuracy. They
propose to transform the magnitudes from activations to weights, which however
offers limited alleviation or suffers from unstable gradients, resulting in a
severe performance drop at low-bitwidth. In this paper, we propose QLLM, an
accurate and efficient low-bitwidth PTQ method designed for LLMs. QLLM
introduces an adaptive channel reassembly technique that reallocates the
magnitude of outliers to other channels, thereby mitigating their impact on the
quantization range. This is achieved by channel disassembly and channel
assembly, which first breaks down the outlier channels into several
sub-channels to ensure a more balanced distribution of activation magnitudes.
Then similar channels are merged to maintain the original channel number for
efficiency. Additionally, an adaptive strategy is designed to autonomously
determine the optimal number of sub-channels for channel disassembly. To
further compensate for the performance loss caused by quantization, we propose
an efficient tuning method that only learns a small number of low-rank weights
while freezing the pre-trained quantized model. After training, these low-rank
parameters can be fused into the frozen weights without affecting inference.
Extensive experiments on LLaMA-1 and LLaMA-2 show that QLLM can obtain accurate
quantized models efficiently. For example, QLLM quantizes the 4-bit LLaMA-2-70B
within 10 hours on a single A100-80G GPU, outperforming the previous
state-of-the-art method by 7.89% on the average accuracy across five zero-shot
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1&quot;&gt;Ruihao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiuying Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08043">
<title>Understanding and Controlling a Maze-Solving Policy Network. (arXiv:2310.08043v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08043</link>
<description rdf:parseType="Literal">&lt;p&gt;To understand the goals and goal representations of AI systems, we carefully
study a pretrained reinforcement learning policy that solves mazes by
navigating to a range of target squares. We find this network pursues multiple
context-dependent goals, and we further identify circuits within the network
that correspond to one of these goals. In particular, we identified eleven
channels that track the location of the goal. By modifying these channels,
either with hand-designed interventions or by combining forward passes, we can
partially control the policy. We show that this network contains redundant,
distributed, and retargetable goal representations, shedding light on the
nature of goal-direction in trained policy networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mini_U/0/1/0/all/0/1&quot;&gt;Ulisse Mini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grietzer_P/0/1/0/all/0/1&quot;&gt;Peli Grietzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Mrinank Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meek_A/0/1/0/all/0/1&quot;&gt;Austin Meek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1&quot;&gt;Monte MacDiarmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_A/0/1/0/all/0/1&quot;&gt;Alexander Matt Turner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08056">
<title>Learning from Label Proportions: Bootstrapping Supervised Learners via Belief Propagation. (arXiv:2310.08056v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08056</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from Label Proportions (LLP) is a learning problem where only
aggregate level labels are available for groups of instances, called bags,
during training, and the aim is to get the best performance at the
instance-level on the test data. This setting arises in domains like
advertising and medicine due to privacy considerations. We propose a novel
algorithmic framework for this problem that iteratively performs two main
steps. For the first step (Pseudo Labeling) in every iteration, we define a
Gibbs distribution over binary instance labels that incorporates a) covariate
information through the constraint that instances with similar covariates
should have similar labels and b) the bag level aggregated label. We then use
Belief Propagation (BP) to marginalize the Gibbs distribution to obtain pseudo
labels. In the second step (Embedding Refinement), we use the pseudo labels to
provide supervision for a learner that yields a better embedding. Further, we
iterate on the two steps again by using the second step&apos;s embeddings as new
covariates for the next iteration. In the final iteration, a classifier is
trained using the pseudo labels. Our algorithm displays strong gains against
several SOTA baselines (up to 15%) for the LLP Binary Classification problem on
various dataset types - tabular and Image. We achieve these improvements with
minimal computational overhead above standard supervised learning due to Belief
Propagation, for large bag sizes, even for a million samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Havaldar_S/0/1/0/all/0/1&quot;&gt;Shreyas Havaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_N/0/1/0/all/0/1&quot;&gt;Navodita Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sareen_S/0/1/0/all/0/1&quot;&gt;Shubhi Sareen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanmugam_K/0/1/0/all/0/1&quot;&gt;Karthikeyan Shanmugam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuveer_A/0/1/0/all/0/1&quot;&gt;Aravindan Raghuveer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08066">
<title>The Search-and-Mix Paradigm in Approximate Nash Equilibrium Algorithms. (arXiv:2310.08066v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2310.08066</link>
<description rdf:parseType="Literal">&lt;p&gt;AI in Math deals with mathematics in a constructive manner so that reasoning
becomes automated, less laborious, and less error-prone. For algorithms, the
question becomes how to automate analyses for specific problems. For the first
time, this work provides an automatic method for approximation analysis on a
well-studied problem in theoretical computer science: computing approximate
Nash equilibria in two-player games. We observe that such algorithms can be
reformulated into a search-and-mix paradigm, which involves a search phase
followed by a mixing phase. By doing so, we are able to fully automate the
procedure of designing and analyzing the mixing phase. For example, we
illustrate how to perform our method with a program to analyze the
approximation bounds of all the algorithms in the literature. Same
approximation bounds are computed without any hand-written proof. Our automatic
method heavily relies on the LP-relaxation structure in approximate Nash
equilibria. Since many approximation algorithms and online algorithms adopt the
LP relaxation, our approach may be extended to automate the analysis of other
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiaotie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hanyu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08067">
<title>GameGPT: Multi-agent Collaborative Framework for Game Development. (arXiv:2310.08067v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08067</link>
<description rdf:parseType="Literal">&lt;p&gt;The large language model (LLM) based agents have demonstrated their capacity
to automate and expedite software development processes. In this paper, we
focus on game development and propose a multi-agent collaborative framework,
dubbed GameGPT, to automate game development. While many studies have
pinpointed hallucination as a primary roadblock for deploying LLMs in
production, we identify another concern: redundancy. Our framework presents a
series of methods to mitigate both concerns. These methods include dual
collaboration and layered approaches with several in-house lexicons, to
mitigate the hallucination and redundancy in the planning, task identification,
and implementation phases. Furthermore, a decoupling approach is also
introduced to achieve code generation with better precision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dake Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huo_Y/0/1/0/all/0/1&quot;&gt;Yunhao Huo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuzhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08085">
<title>Low-Resource Clickbait Spoiling for Indonesian via Question Answering. (arXiv:2310.08085v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08085</link>
<description rdf:parseType="Literal">&lt;p&gt;Clickbait spoiling aims to generate a short text to satisfy the curiosity
induced by a clickbait post. As it is a newly introduced task, the dataset is
only available in English so far. Our contributions include the construction of
manually labeled clickbait spoiling corpus in Indonesian and an evaluation on
using cross-lingual zero-shot question answering-based models to tackle
clikcbait spoiling for low-resource language like Indonesian. We utilize
selection of multilingual language models. The experimental results suggest
that XLM-RoBERTa (large) model outperforms other models for phrase and passage
spoilers, meanwhile, mDeBERTa (base) model outperforms other models for
multipart spoilers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maharani_N/0/1/0/all/0/1&quot;&gt;Ni Putu Intan Maharani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purwarianti_A/0/1/0/all/0/1&quot;&gt;Ayu Purwarianti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1&quot;&gt;Alham Fikri Aji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08091">
<title>Discerning Temporal Difference Learning. (arXiv:2310.08091v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08091</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal difference learning (TD) is a foundational concept in reinforcement
learning (RL), aimed at efficiently assessing a policy&apos;s value function.
TD($\lambda$), a potent variant, incorporates a memory trace to distribute the
prediction error into the historical context. However, this approach often
neglects the significance of historical states and the relative importance of
propagating the TD error, influenced by challenges such as visitation imbalance
or outcome noise. To address this, we propose a novel TD algorithm named
discerning TD learning (DTD), which allows flexible emphasis
functions$-$predetermined or adapted during training$-$to allocate efforts
effectively across states. We establish the convergence properties of our
method within a specific class of emphasis functions and showcase its promising
potential for adaptation to deep RL contexts. Empirical results underscore that
employing a judicious emphasis function not only improves value estimation but
also expedites learning across diverse scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianfei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08097">
<title>Sentinel: An Aggregation Function to Secure Decentralized Federated Learning. (arXiv:2310.08097v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2310.08097</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid integration of Federated Learning (FL) into networking encompasses
various aspects such as network management, quality of service, and
cybersecurity while preserving data privacy. In this context, Decentralized
Federated Learning (DFL) emerges as an innovative paradigm to train
collaborative models, addressing the single point of failure limitation.
However, the security and trustworthiness of FL and DFL are compromised by
poisoning attacks, negatively impacting its performance. Existing defense
mechanisms have been designed for centralized FL and they do not adequately
exploit the particularities of DFL. Thus, this work introduces Sentinel, a
defense strategy to counteract poisoning attacks in DFL. Sentinel leverages the
accessibility of local data and defines a three-step aggregation protocol
consisting of similarity filtering, bootstrap validation, and normalization to
safeguard against malicious model updates. Sentinel has been evaluated with
diverse datasets and various poisoning attack types and threat levels,
improving the state-of-the-art performance against both untargeted and targeted
poisoning attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celdran_A/0/1/0/all/0/1&quot;&gt;Alberto Huertas Celdran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baltensperger_J/0/1/0/all/0/1&quot;&gt;Janosch Baltensperger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertran_E/0/1/0/all/0/1&quot;&gt;Enrique Tomas Mat&amp;#x131;nez Bertran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovet_G/0/1/0/all/0/1&quot;&gt;Gerome Bovet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stiller_B/0/1/0/all/0/1&quot;&gt;Burkhard Stiller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08101">
<title>Promptor: A Conversational and Autonomous Prompt Generation Agent for Intelligent Text Entry Techniques. (arXiv:2310.08101v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08101</link>
<description rdf:parseType="Literal">&lt;p&gt;Text entry is an essential task in our day-to-day digital interactions.
Numerous intelligent features have been developed to streamline this process,
making text entry more effective, efficient, and fluid. These improvements
include sentence prediction and user personalization. However, as deep
learning-based language models become the norm for these advanced features, the
necessity for data collection and model fine-tuning increases. These challenges
can be mitigated by harnessing the in-context learning capability of large
language models such as GPT-3.5. This unique feature allows the language model
to acquire new skills through prompts, eliminating the need for data collection
and fine-tuning. Consequently, large language models can learn various text
prediction techniques. We initially showed that, for a sentence prediction
task, merely prompting GPT-3.5 surpassed a GPT-2 backed system and is
comparable with a fine-tuned GPT-3.5 model, with the latter two methods
requiring costly data collection, fine-tuning and post-processing. However, the
task of prompting large language models to specialize in specific text
prediction tasks can be challenging, particularly for designers without
expertise in prompt engineering. To address this, we introduce Promptor, a
conversational prompt generation agent designed to engage proactively with
designers. Promptor can automatically generate complex prompts tailored to meet
specific needs, thus offering a solution to this challenge. We conducted a user
study involving 24 participants creating prompts for three intelligent text
entry tasks, half of the participants used Promptor while the other half
designed prompts themselves. The results show that Promptor-designed prompts
result in a 35% increase in similarity and 22% in coherence over those by
designers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Junxiao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dudley_J/0/1/0/all/0/1&quot;&gt;John J. Dudley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jingyao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_B/0/1/0/all/0/1&quot;&gt;Bill Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristensson_P/0/1/0/all/0/1&quot;&gt;Per Ola Kristensson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08117">
<title>DUSA: Decoupled Unsupervised Sim2Real Adaptation for Vehicle-to-Everything Collaborative Perception. (arXiv:2310.08117v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08117</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle-to-Everything (V2X) collaborative perception is crucial for
autonomous driving. However, achieving high-precision V2X perception requires a
significant amount of annotated real-world data, which can always be expensive
and hard to acquire. Simulated data have raised much attention since they can
be massively produced at an extremely low cost. Nevertheless, the significant
domain gap between simulated and real-world data, including differences in
sensor type, reflectance patterns, and road surroundings, often leads to poor
performance of models trained on simulated data when evaluated on real-world
data. In addition, there remains a domain gap between real-world collaborative
agents, e.g. different types of sensors may be installed on autonomous vehicles
and roadside infrastructures with different extrinsics, further increasing the
difficulty of sim2real generalization. To take full advantage of simulated
data, we present a new unsupervised sim2real domain adaptation method for V2X
collaborative detection named Decoupled Unsupervised Sim2Real Adaptation
(DUSA). Our new method decouples the V2X collaborative sim2real domain
adaptation problem into two sub-problems: sim2real adaptation and inter-agent
adaptation. For sim2real adaptation, we design a Location-adaptive Sim2Real
Adapter (LSA) module to adaptively aggregate features from critical locations
of the feature map and align the features between simulated data and real-world
data via a sim/real discriminator on the aggregated global feature. For
inter-agent adaptation, we further devise a Confidence-aware Inter-agent
Adapter (CIA) module to align the fine-grained features from heterogeneous
agents under the guidance of agent-wise confidence maps. Experiments
demonstrate the effectiveness of the proposed DUSA approach on unsupervised
sim2real adaptation from the simulated V2XSet dataset to the real-world
DAIR-V2X-C dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xianghao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wentao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jinrang Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yifeng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Si Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08118">
<title>Can Large Language Models Really Improve by Self-critiquing Their Own Plans?. (arXiv:2310.08118v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08118</link>
<description rdf:parseType="Literal">&lt;p&gt;There have been widespread claims about Large Language Models (LLMs) being
able to successfully verify or self-critique their candidate solutions in
reasoning problems in an iterative mode. Intrigued by those claims, in this
paper we set out to investigate the verification/self-critiquing abilities of
large language models in the context of planning. We evaluate a planning system
that employs LLMs for both plan generation and verification. We assess the
verifier LLM&apos;s performance against ground-truth verification, the impact of
self-critiquing on plan generation, and the influence of varying feedback
levels on system performance. Using GPT-4, a state-of-the-art LLM, for both
generation and verification, our findings reveal that self-critiquing appears
to diminish plan generation performance, especially when compared to systems
with external, sound verifiers and the LLM verifiers in that system produce a
notable number of false positives, compromising the system&apos;s reliability.
Additionally, the nature of feedback, whether binary or detailed, showed
minimal impact on plan generation. Collectively, our results cast doubt on the
effectiveness of LLMs in a self-critiquing, iterative framework for planning
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valmeekam_K/0/1/0/all/0/1&quot;&gt;Karthik Valmeekam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marquez_M/0/1/0/all/0/1&quot;&gt;Matthew Marquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08138">
<title>Multi-Scale Spatial-Temporal Recurrent Networks for Traffic Flow Prediction. (arXiv:2310.08138v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08138</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic flow prediction is one of the most fundamental tasks of intelligent
transportation systems. The complex and dynamic spatial-temporal dependencies
make the traffic flow prediction quite challenging. Although existing
spatial-temporal graph neural networks hold prominent, they often encounter
challenges such as (1) ignoring the fixed graph that limits the predictive
performance of the model, (2) insufficiently capturing complex spatial-temporal
dependencies simultaneously, and (3) lacking attention to spatial-temporal
information at different time lengths. In this paper, we propose a Multi-Scale
Spatial-Temporal Recurrent Network for traffic flow prediction, namely MSSTRN,
which consists of two different recurrent neural networks: the single-step gate
recurrent unit and the multi-step gate recurrent unit to fully capture the
complex spatial-temporal information in the traffic data under different time
windows. Moreover, we propose a spatial-temporal synchronous attention
mechanism that integrates adaptive position graph convolutions into the
self-attention mechanism to achieve synchronous capture of spatial-temporal
dependencies. We conducted extensive experiments on four real traffic datasets
and demonstrated that our model achieves the best prediction accuracy with
non-trivial margins compared to all the twenty baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haiyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chunjiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Detian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08184">
<title>Learn From Model Beyond Fine-Tuning: A Survey. (arXiv:2310.08184v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08184</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models (FM) have demonstrated remarkable performance across a wide
range of tasks (especially in the fields of natural language processing and
computer vision), primarily attributed to their ability to comprehend
instructions and access extensive, high-quality data. This not only showcases
their current effectiveness but also sets a promising trajectory towards the
development of artificial general intelligence. Unfortunately, due to multiple
constraints, the raw data of the model used for large model training are often
inaccessible, so the use of end-to-end models for downstream tasks has become a
new research trend, which we call Learn From Model (LFM) in this article. LFM
focuses on the research, modification, and design of FM based on the model
interface, so as to better understand the model structure and weights (in a
black box environment), and to generalize the model to downstream tasks. The
study of LFM techniques can be broadly categorized into five major areas: model
tuning, model distillation, model reuse, meta learning and model editing. Each
category encompasses a repertoire of methods and strategies that aim to enhance
the capabilities and performance of FM. This paper gives a comprehensive review
of the current methods based on FM from the perspective of LFM, in order to
help readers better understand the current research status and ideas. To
conclude, we summarize the survey by highlighting several critical areas for
future exploration and addressing open issues that require further attention
from the research community. The relevant papers we investigated in this
article can be accessed at
&amp;lt;https://github.com/ruthless-man/Awesome-Learn-from-Model&amp;gt;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hongling Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_A/0/1/0/all/0/1&quot;&gt;Anke Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08185">
<title>EIPE-text: Evaluation-Guided Iterative Plan Extraction for Long-Form Narrative Text Generation. (arXiv:2310.08185v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08185</link>
<description rdf:parseType="Literal">&lt;p&gt;Plan-and-Write is a common hierarchical approach in long-form narrative text
generation, which first creates a plan to guide the narrative writing.
Following this approach, several studies rely on simply prompting large
language models for planning, which often yields suboptimal results. In this
paper, we propose a new framework called Evaluation-guided Iterative Plan
Extraction for long-form narrative text generation (EIPE-text), which extracts
plans from the corpus of narratives and utilizes the extracted plans to
construct a better planner. EIPE-text has three stages: plan extraction,
learning, and inference. In the plan extraction stage, it iteratively extracts
and improves plans from the narrative corpus and constructs a plan corpus. We
propose a question answer (QA) based evaluation mechanism to automatically
evaluate the plans and generate detailed plan refinement instructions to guide
the iterative improvement. In the learning stage, we build a better planner by
fine-tuning with the plan corpus or in-context learning with examples in the
plan corpus. Finally, we leverage a hierarchical approach to generate long-form
narratives. We evaluate the effectiveness of EIPE-text in the domains of novels
and storytelling. Both GPT-4-based evaluations and human evaluations
demonstrate that our method can generate more coherent and relevant long-form
narratives. Our code will be released in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_W/0/1/0/all/0/1&quot;&gt;Wang You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenshan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yaobo Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shaoguang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chenfei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Maosong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiduo Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08198">
<title>Beyond Traditional DoE: Deep Reinforcement Learning for Optimizing Experiments in Model Identification of Battery Dynamics. (arXiv:2310.08198v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08198</link>
<description rdf:parseType="Literal">&lt;p&gt;Model identification of battery dynamics is a central problem in energy
research; many energy management systems and design processes rely on accurate
battery models for efficiency optimization. The standard methodology for
battery modelling is traditional design of experiments (DoE), where the battery
dynamics are excited with many different current profiles and the measured
outputs are used to estimate the system dynamics. However, although it is
possible to obtain useful models with the traditional approach, the process is
time consuming and expensive because of the need to sweep many different
current-profile configurations. In the present work, a novel DoE approach is
developed based on deep reinforcement learning, which alters the configuration
of the experiments on the fly based on the statistics of past experiments.
Instead of sticking to a library of predefined current profiles, the proposed
approach modifies the current profiles dynamically by updating the output space
covered by past measurements, hence only the current profiles that are
informative for future experiments are applied. Simulations and real
experiments are used to show that the proposed approach gives models that are
as accurate as those obtained with traditional DoE but by using 85\% less
resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budan_G/0/1/0/all/0/1&quot;&gt;Gokhan Budan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damiani_F/0/1/0/all/0/1&quot;&gt;Francesca Damiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurtulus_C/0/1/0/all/0/1&quot;&gt;Can Kurtulus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ure_N/0/1/0/all/0/1&quot;&gt;N. Kemal Ure&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08206">
<title>Long-Tailed Classification Based on Coarse-Grained Leading Forest and Multi-Center Loss. (arXiv:2310.08206v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08206</link>
<description rdf:parseType="Literal">&lt;p&gt;Long-tailed(LT) classification is an unavoidable and challenging problem in
the real world. Most of the existing long-tailed classification methods focus
only on solving the inter-class imbalance in which there are more samples in
the head class than in the tail class, while ignoring the intra-lass imbalance
in which the number of samples of the head attribute within the same class is
much larger than the number of samples of the tail attribute. The deviation in
the model is caused by both of these factors, and due to the fact that
attributes are implicit in most datasets and the combination of attributes is
very complex, the intra-class imbalance is more difficult to handle. For this
purpose, we proposed a long-tailed classification framework, known as
\textbf{\textsc{Cognisance}}, which is founded on Coarse-Grained Leading Forest
(CLF) and Multi-Center Loss (MCL), aiming to build a multi-granularity joint
solution model by means of invariant feature learning. In this method, we
designed an unsupervised learning method, i.e., CLF, to better characterize the
distribution of attributes within a class. Depending on the distribution of
attributes, we can flexibly construct sampling strategies suitable for
different environments. In addition, we introduce a new metric learning loss
(MCL), which aims to gradually eliminate confusing attributes during the
feature learning process. More importantly, this approach does not depend on a
specific model structure and can be integrated with existing LT methods as an
independent component. We have conducted extensive experiments and our approach
has state-of-the-art performance in both existing benchmarks ImageNet-GLT and
MSCOCO-GLT, and can improve the performance of existing LT methods. Our codes
are available on GitHub: \url{https://github.com/jinyery/cognisance}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinye Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Ji Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08215">
<title>Trustworthy Machine Learning. (arXiv:2310.08215v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08215</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning technology gets applied to actual products and solutions,
new challenges have emerged. Models unexpectedly fail to generalize to small
changes in the distribution, tend to be confident on novel data they have never
seen, or cannot communicate the rationale behind their decisions effectively
with the end users. Collectively, we face a trustworthiness issue with the
current machine learning technology. This textbook on Trustworthy Machine
Learning (TML) covers a theoretical and technical background of four key topics
in TML: Out-of-Distribution Generalization, Explainability, Uncertainty
Quantification, and Evaluation of Trustworthiness. We discuss important
classical and contemporary research papers of the aforementioned fields and
uncover and connect their underlying intuitions. The book evolved from the
homonymous course at the University of T\&quot;ubingen, first offered in the Winter
Semester of 2022/23. It is meant to be a stand-alone product accompanied by
code snippets and various pointers to further sources on topics of TML. The
dedicated website of the book is https://trustworthyml.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mucsanyi_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe1;lint Mucs&amp;#xe1;nyi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirchhof_M/0/1/0/all/0/1&quot;&gt;Michael Kirchhof&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_E/0/1/0/all/0/1&quot;&gt;Elisa Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubinstein_A/0/1/0/all/0/1&quot;&gt;Alexander Rubinstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seong Joon Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08217">
<title>TriRE: A Multi-Mechanism Learning Paradigm for Continual Knowledge Retention and Promotion. (arXiv:2310.08217v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08217</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning (CL) has remained a persistent challenge for deep neural
networks due to catastrophic forgetting (CF) of previously learned tasks.
Several techniques such as weight regularization, experience rehearsal, and
parameter isolation have been proposed to alleviate CF. Despite their relative
success, these research directions have predominantly remained orthogonal and
suffer from several shortcomings, while missing out on the advantages of
competing strategies. On the contrary, the brain continually learns,
accommodates, and transfers knowledge across tasks by simultaneously leveraging
several neurophysiological processes, including neurogenesis, active
forgetting, neuromodulation, metaplasticity, experience rehearsal, and
context-dependent gating, rarely resulting in CF. Inspired by how the brain
exploits multiple mechanisms concurrently, we propose TriRE, a novel CL
paradigm that encompasses retaining the most prominent neurons for each task,
revising and solidifying the extracted knowledge of current and past tasks, and
actively promoting less active neurons for subsequent tasks through rewinding
and relearning. Across CL settings, TriRE significantly reduces task
interference and surpasses different CL approaches considered in isolation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vijayan_P/0/1/0/all/0/1&quot;&gt;Preetha Vijayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_P/0/1/0/all/0/1&quot;&gt;Prashant Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1&quot;&gt;Elahe Arani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1&quot;&gt;Bahram Zonooz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08221">
<title>SimCKP: Simple Contrastive Learning of Keyphrase Representations. (arXiv:2310.08221v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08221</link>
<description rdf:parseType="Literal">&lt;p&gt;Keyphrase generation (KG) aims to generate a set of summarizing words or
phrases given a source document, while keyphrase extraction (KE) aims to
identify them from the text. Because the search space is much smaller in KE, it
is often combined with KG to predict keyphrases that may or may not exist in
the corresponding document. However, current unified approaches adopt sequence
labeling and maximization-based generation that primarily operate at a token
level, falling short in observing and scoring keyphrases as a whole. In this
work, we propose SimCKP, a simple contrastive learning framework that consists
of two stages: 1) An extractor-generator that extracts keyphrases by learning
context-aware phrase-level representations in a contrastive manner while also
generating keyphrases that do not appear in the document; 2) A reranker that
adapts scores for each generated phrase by likewise aligning their
representations with the corresponding document. Experimental results on
multiple benchmark datasets demonstrate the effectiveness of our proposed
approach, which outperforms the state-of-the-art models by a significant
margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gwak_C/0/1/0/all/0/1&quot;&gt;Chaeheon Gwak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Si Hyeong Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08233">
<title>The Impact of Time Step Frequency on the Realism of Robotic Manipulation Simulation for Objects of Different Scales. (arXiv:2310.08233v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2310.08233</link>
<description rdf:parseType="Literal">&lt;p&gt;This work evaluates the impact of time step frequency and component scale on
robotic manipulation simulation accuracy. Increasing the time step frequency
for small-scale objects is shown to improve simulation accuracy. This
simulation, demonstrating pre-assembly part picking for two object geometries,
serves as a starting point for discussing how to improve Sim2Real transfer in
robotic assembly processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ta_M/0/1/0/all/0/1&quot;&gt;Minh Q. Ta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinkel_H/0/1/0/all/0/1&quot;&gt;Holly Dinkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdul_Rashid_H/0/1/0/all/0/1&quot;&gt;Hameed Abdul-Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yangfei Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_J/0/1/0/all/0/1&quot;&gt;Jessica Myers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_J/0/1/0/all/0/1&quot;&gt;Junyi Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bretl_T/0/1/0/all/0/1&quot;&gt;Timothy Bretl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08235">
<title>GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08235</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. Code and video can be
found on the website https://craftjarvis-groot.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Shaofei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yitao Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08252">
<title>MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning. (arXiv:2310.08252v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08252</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Meta-Black-Box Optimization with Reinforcement Learning
(MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to
mitigate manual fine-tuning of low-level black-box optimizers. However, this
field is hindered by the lack of a unified benchmark. To fill this gap, we
introduce MetaBox, the first benchmark platform expressly tailored for
developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible
algorithmic template that allows users to effortlessly implement their unique
designs within the platform. Moreover, it provides a broad spectrum of over 300
problem instances, collected from synthetic to realistic scenarios, and an
extensive library of 19 baseline methods, including both traditional black-box
optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three
standardized performance metrics, enabling a more thorough assessment of the
methods. In a bid to illustrate the utility of MetaBox for facilitating
rigorous evaluation and in-depth analysis, we carry out a wide-ranging
benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source
and accessible at: https://github.com/GMC-DRL/MetaBox.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zeyuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hongshu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiacheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenrui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_G/0/1/0/all/0/1&quot;&gt;Guojun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yue-Jiao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yining Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiguang Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08256">
<title>Impact of Co-occurrence on Factual Knowledge of Large Language Models. (arXiv:2310.08256v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08256</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) often make factually incorrect responses despite
their success in various applications. In this paper, we hypothesize that
relying heavily on simple co-occurrence statistics of the pre-training corpora
is one of the main factors that cause factual errors. Our results reveal that
LLMs are vulnerable to the co-occurrence bias, defined as preferring frequently
co-occurred words over the correct answer. Consequently, LLMs struggle to
recall facts whose subject and object rarely co-occur in the pre-training
dataset although they are seen during finetuning. We show that co-occurrence
bias remains despite scaling up model sizes or finetuning. Therefore, we
suggest finetuning on a debiased dataset to mitigate the bias by filtering out
biased samples whose subject-object co-occurrence count is high. Although
debiased finetuning allows LLMs to memorize rare facts in the training set, it
is not effective in recalling rare facts unseen during finetuning. Further
research in mitigation will help build reliable language models by preventing
potential errors. The code is available at
\url{https://github.com/CheongWoong/impact_of_cooccurrence}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_C/0/1/0/all/0/1&quot;&gt;Cheongwoong Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaesik Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08276">
<title>Direction-Oriented Visual-semantic Embedding Model for Remote Sensing Image-text Retrieval. (arXiv:2310.08276v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08276</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-text retrieval has developed rapidly in recent years. However, it is
still a challenge in remote sensing due to visual-semantic imbalance, which
leads to incorrect matching of non-semantic visual and textual features. To
solve this problem, we propose a novel Direction-Oriented Visual-semantic
Embedding Model (DOVE) to mine the relationship between vision and language.
Concretely, a Regional-Oriented Attention Module (ROAM) adaptively adjusts the
distance between the final visual and textual embeddings in the latent semantic
space, oriented by regional visual features. Meanwhile, a lightweight Digging
Text Genome Assistant (DTGA) is designed to expand the range of tractable
textual representation and enhance global word-level semantic connections using
less attention operations. Ultimately, we exploit a global visual-semantic
constraint to reduce single visual dependency and serve as an external
constraint for the final visual and textual representations. The effectiveness
and superiority of our method are verified by extensive experiments including
parameter evaluation, quantitative comparison, ablation studies and visual
analysis, on two benchmark datasets, RSICD and RSITMD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Q/0/1/0/all/0/1&quot;&gt;Qing Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiancheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_C/0/1/0/all/0/1&quot;&gt;Cong Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08278">
<title>Lag-Llama: Towards Foundation Models for Time Series Forecasting. (arXiv:2310.08278v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2310.08278</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming to build foundation models for time-series forecasting and study their
scaling behavior, we present here our work-in-progress on Lag-Llama, a
general-purpose univariate probabilistic time-series forecasting model trained
on a large collection of time-series data. The model shows good zero-shot
prediction capabilities on unseen &quot;out-of-distribution&quot; time-series datasets,
outperforming supervised baselines. We use smoothly broken power-laws to fit
and predict model scaling behavior. The open source code is made available at
https://github.com/kashif/pytorch-transformer-ts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasul_K/0/1/0/all/0/1&quot;&gt;Kashif Rasul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashok_A/0/1/0/all/0/1&quot;&gt;Arjun Ashok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_A/0/1/0/all/0/1&quot;&gt;Andrew Robert Williams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorasani_A/0/1/0/all/0/1&quot;&gt;Arian Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adamopoulos_G/0/1/0/all/0/1&quot;&gt;George Adamopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagwatkar_R/0/1/0/all/0/1&quot;&gt;Rishika Bhagwatkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilos_M/0/1/0/all/0/1&quot;&gt;Marin Bilo&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghonia_H/0/1/0/all/0/1&quot;&gt;Hena Ghonia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassen_N/0/1/0/all/0/1&quot;&gt;Nadhir Vincent Hassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_A/0/1/0/all/0/1&quot;&gt;Anderson Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_S/0/1/0/all/0/1&quot;&gt;Sahil Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Drouin_A/0/1/0/all/0/1&quot;&gt;Alexandre Drouin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chapados_N/0/1/0/all/0/1&quot;&gt;Nicolas Chapados&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevmyvaka_Y/0/1/0/all/0/1&quot;&gt;Yuriy Nevmyvaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rish_I/0/1/0/all/0/1&quot;&gt;Irina Rish&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08279">
<title>CP-KGC: Constrained-Prompt Knowledge Graph Completion with Large Language Models. (arXiv:2310.08279v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08279</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph completion (KGC) aims to utilize existing knowledge to deduce
and infer missing connections within knowledge graphs. Text-based approaches,
like SimKGC, have outperformed graph embedding methods, showcasing the promise
of inductive KGC. However, the efficacy of text-based methods hinges on the
quality of entity textual descriptions. In this paper, we identify the key
issue of whether large language models (LLMs) can generate effective text. To
mitigate hallucination in LLM-generated text in this paper, we introduce a
constraint-based prompt that utilizes the entity and its textual description as
contextual constraints to enhance data quality. Our Constrained-Prompt
Knowledge Graph Completion (CP-KGC) method demonstrates effective inference
under low resource computing conditions and surpasses prior results on the
WN18RR and FB15K237 datasets. This showcases the integration of LLMs in KGC
tasks and provides new directions for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Rui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Li Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08291">
<title>Expanding the Vocabulary of BERT for Knowledge Base Construction. (arXiv:2310.08291v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2310.08291</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge base construction entails acquiring structured information to
create a knowledge base of factual and relational data, facilitating question
answering, information retrieval, and semantic understanding. The challenge
called &quot;Knowledge Base Construction from Pretrained Language Models&quot; at
International Semantic Web Conference 2023 defines tasks focused on
constructing knowledge base using language model. Our focus was on Track 1 of
the challenge, where the parameters are constrained to a maximum of 1 billion,
and the inclusion of entity descriptions within the prompt is prohibited.
&lt;/p&gt;
&lt;p&gt;Although the masked language model offers sufficient flexibility to extend
its vocabulary, it is not inherently designed for multi-token prediction. To
address this, we present Vocabulary Expandable BERT for knowledge base
construction, which expand the language model&apos;s vocabulary while preserving
semantic embeddings for newly added words. We adopt task-specific
re-pre-training on masked language model to further enhance the language model.
&lt;/p&gt;
&lt;p&gt;Through experimentation, the results show the effectiveness of our
approaches. Our framework achieves F1 score of 0.323 on the hidden test set and
0.362 on the validation set, both data set is provided by the challenge.
Notably, our framework adopts a lightweight language model (BERT-base, 0.13
billion parameters) and surpasses the model using prompts directly on large
language model (Chatgpt-3, 175 billion parameters). Besides, Token-Recode
achieves comparable performances as Re-pretrain. This research advances
language understanding models by enabling the direct embedding of multi-token
entities, signifying a substantial step forward in link prediction task in
knowledge graph and metadata completion in data management.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celebi_R/0/1/0/all/0/1&quot;&gt;Remzi Celebi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08292">
<title>Concealed Electronic Countermeasures of Radar Signal with Adversarial Examples. (arXiv:2310.08292v1 [eess.SP])</title>
<link>http://arxiv.org/abs/2310.08292</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic countermeasures involving radar signals are an important aspect of
modern warfare. Traditional electronic countermeasures techniques typically add
large-scale interference signals to ensure interference effects, which can lead
to attacks being too obvious. In recent years, AI-based attack methods have
emerged that can effectively solve this problem, but the attack scenarios are
currently limited to time domain radar signal classification. In this paper, we
focus on the time-frequency images classification scenario of radar signals. We
first propose an attack pipeline under the time-frequency images scenario and
DITIMI-FGSM attack algorithm with high transferability. Then, we propose
STFT-based time domain signal attack(STDS) algorithm to solve the problem of
non-invertibility in time-frequency analysis, thus obtaining the time-domain
representation of the interference signal. A large number of experiments show
that our attack pipeline is feasible and the proposed attack method has a high
success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_R/0/1/0/all/0/1&quot;&gt;Ruinan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Canjie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mingfeng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yu-an Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruibin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tao_R/0/1/0/all/0/1&quot;&gt;Ran Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08295">
<title>If our aim is to build morality into an artificial agent, how might we begin to go about doing so?. (arXiv:2310.08295v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08295</link>
<description rdf:parseType="Literal">&lt;p&gt;As Artificial Intelligence (AI) becomes pervasive in most fields, from
healthcare to autonomous driving, it is essential that we find successful ways
of building morality into our machines, especially for decision-making.
However, the question of what it means to be moral is still debated,
particularly in the context of AI. In this paper, we highlight the different
aspects that should be considered when building moral agents, including the
most relevant moral paradigms and challenges. We also discuss the top-down and
bottom-up approaches to design and the role of emotion and sentience in
morality. We then propose solutions including a hybrid approach to design and a
hierarchical approach to combining moral paradigms. We emphasize how governance
and policy are becoming ever more critical in AI Ethics and in ensuring that
the tasks we set for moral agents are attainable, that ethical behavior is
achieved, and that we obtain good AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seeamber_R/0/1/0/all/0/1&quot;&gt;Reneira Seeamber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badea_C/0/1/0/all/0/1&quot;&gt;Cosmin Badea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08304">
<title>CHIP: Contrastive Hierarchical Image Pretraining. (arXiv:2310.08304v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2310.08304</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot object classification is the task of classifying objects in an image
with limited number of examples as supervision. We propose a one-shot/few-shot
classification model that can classify an object of any unseen class into a
relatively general category in an hierarchically based classification. Our
model uses a three-level hierarchical contrastive loss based ResNet152
classifier for classifying an object based on its features extracted from Image
embedding, not used during the training phase. For our experimentation, we have
used a subset of the ImageNet (ILSVRC-12) dataset that contains only the animal
classes for training our model and created our own dataset of unseen classes
for evaluating our trained model. Our model provides satisfactory results in
classifying the unknown objects into a generic category which has been later
discussed in greater detail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_A/0/1/0/all/0/1&quot;&gt;Arpit Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhaveri_H/0/1/0/all/0/1&quot;&gt;Harshil Jhaveri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallick_S/0/1/0/all/0/1&quot;&gt;Swapnil Mallick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajmera_A/0/1/0/all/0/1&quot;&gt;Abhishek Ajmera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08328">
<title>Transport-Hub-Aware Spatial-Temporal Adaptive Graph Transformer for Traffic Flow Prediction. (arXiv:2310.08328v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2310.08328</link>
<description rdf:parseType="Literal">&lt;p&gt;As a core technology of Intelligent Transportation System (ITS), traffic flow
prediction has a wide range of applications. Traffic flow data are
spatial-temporal, which are not only correlated to spatial locations in road
networks, but also vary with temporal time indices. Existing methods have
solved the challenges in traffic flow prediction partly, focusing on modeling
spatial-temporal dependencies effectively, while not all intrinsic properties
of traffic flow data are utilized fully. Besides, there are very few attempts
at incremental learning of spatial-temporal data mining, and few previous works
can be easily transferred to the traffic flow prediction task. Motivated by the
challenge of incremental learning methods for traffic flow prediction and the
underutilization of intrinsic properties of road networks, we propose a
Transport-Hub-aware Spatial-Temporal adaptive graph transFormer (H-STFormer)
for traffic flow prediction. Specifically, we first design a novel spatial
self-attention module to capture the dynamic spatial dependencies. Three graph
masking matrices are integrated into spatial self-attentions to highlight both
short- and long-term dependences. Additionally, we employ a temporal
self-attention module to detect dynamic temporal patterns in the traffic flow
data. Finally, we design an extra spatial-temporal knowledge distillation
module for incremental learning of traffic flow prediction tasks. Through
extensive experiments, we show the effectiveness of H-STFormer in normal and
incremental traffic flow prediction tasks. The code is available at
https://github.com/Fantasy-Shaw/H-STFormer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bailong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhizhen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuefei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08335">
<title>2SFGL: A Simple And Robust Protocol For Graph-Based Fraud Detection. (arXiv:2310.08335v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2310.08335</link>
<description rdf:parseType="Literal">&lt;p&gt;Financial crime detection using graph learning improves financial safety and
efficiency. However, criminals may commit financial crimes across different
institutions to avoid detection, which increases the difficulty of detection
for financial institutions which use local data for graph learning. As most
financial institutions are subject to strict regulations in regards to data
privacy protection, the training data is often isolated and conventional
learning technology cannot handle the problem. Federated learning (FL) allows
multiple institutions to train a model without revealing their datasets to each
other, hence ensuring data privacy protection. In this paper, we proposes a
novel two-stage approach to federated graph learning (2SFGL): The first stage
of 2SFGL involves the virtual fusion of multiparty graphs, and the second
involves model training and inference on the virtual graph. We evaluate our
framework on a conventional fraud detection task based on the
FraudAmazonDataset and FraudYelpDataset. Experimental results show that
integrating and applying a GCN (Graph Convolutional Network) with our 2SFGL
framework to the same task results in a 17.6\%-30.2\% increase in performance
on several typical metrics compared to the case only using FedAvg, while
integrating GraphSAGE with 2SFGL results in a 6\%-16.2\% increase in
performance compared to the case only using FedAvg. We conclude that our
proposed framework is a robust and simple protocol which can be simply
integrated to pre-existing graph-based fraud detection methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhirui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guangzhong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhaoning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1&quot;&gt;Yang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Lai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03469">
<title>Federated Learning from Small Datasets. (arXiv:2110.03469v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03469</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning allows multiple parties to collaboratively train a joint
model without sharing local data. This enables applications of machine learning
in settings of inherently distributed, undisclosable data such as in the
medical domain. In practice, joint training is usually achieved by aggregating
local models, for which local training objectives have to be in expectation
similar to the joint (global) objective. Often, however, local datasets are so
small that local objectives differ greatly from the global objective, resulting
in federated learning to fail. We propose a novel approach that intertwines
model aggregations with permutations of local models. The permutations expose
each local model to a daisy chain of local datasets resulting in more efficient
training in data-sparse domains. This enables training on extremely small local
datasets, such as patient data across hospitals, while retaining the training
efficiency and privacy benefits of federated learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamp_M/0/1/0/all/0/1&quot;&gt;Michael Kamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_J/0/1/0/all/0/1&quot;&gt;Jonas Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vreeken_J/0/1/0/all/0/1&quot;&gt;Jilles Vreeken&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.09348">
<title>A Machine Learning Paradigm for Studying Pictorial Realism: Are Constable&apos;s Clouds More Real than His Contemporaries?. (arXiv:2202.09348v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.09348</link>
<description rdf:parseType="Literal">&lt;p&gt;The British landscape painter John Constable is considered foundational for
the Realist movement in 19th-century European painting. Constable&apos;s painted
skies, in particular, were seen as remarkably accurate by his contemporaries,
an impression shared by many viewers today. Yet, assessing the accuracy of
realist paintings like Constable&apos;s is subjective or intuitive, even for
professional art historians, making it difficult to say with certainty what set
Constable&apos;s skies apart from those of his contemporaries. Our goal is to
contribute to a more objective understanding of Constable&apos;s realism. We propose
a new machine-learning-based paradigm for studying pictorial realism in an
explainable way. Our framework assesses realism by measuring the similarity
between clouds painted by artists noted for their skies, like Constable, and
photographs of clouds. The experimental results of cloud classification show
that Constable approximates more consistently than his contemporaries the
formal features of actual clouds in his paintings. The study, as a novel
interdisciplinary approach that combines computer vision and machine learning,
meteorology, and art history, is a springboard for broader and deeper analyses
of pictorial realism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuomin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansfield_E/0/1/0/all/0/1&quot;&gt;Elizabeth C. Mansfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_J/0/1/0/all/0/1&quot;&gt;John Russell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_G/0/1/0/all/0/1&quot;&gt;George S. Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adams_C/0/1/0/all/0/1&quot;&gt;Catherine Adams&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;James Z. Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.07580">
<title>mGPT: Few-Shot Learners Go Multilingual. (arXiv:2204.07580v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2204.07580</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shliazhko_O/0/1/0/all/0/1&quot;&gt;Oleh Shliazhko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fenogenova_A/0/1/0/all/0/1&quot;&gt;Alena Fenogenova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tikhonova_M/0/1/0/all/0/1&quot;&gt;Maria Tikhonova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikhailov_V/0/1/0/all/0/1&quot;&gt;Vladislav Mikhailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozlova_A/0/1/0/all/0/1&quot;&gt;Anastasia Kozlova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shavrina_T/0/1/0/all/0/1&quot;&gt;Tatiana Shavrina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.12389">
<title>MemSAC: Memory Augmented Sample Consistency for Large Scale Unsupervised Domain Adaptation. (arXiv:2207.12389v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.12389</link>
<description rdf:parseType="Literal">&lt;p&gt;Practical real world datasets with plentiful categories introduce new
challenges for unsupervised domain adaptation like small inter-class
discriminability, that existing approaches relying on domain invariance alone
cannot handle sufficiently well. In this work we propose MemSAC, which exploits
sample level similarity across source and target domains to achieve
discriminative transfer, along with architectures that scale to a large number
of categories. For this purpose, we first introduce a memory augmented approach
to efficiently extract pairwise similarity relations between labeled source and
unlabeled target domain instances, suited to handle an arbitrary number of
classes. Next, we propose and theoretically justify a novel variant of the
contrastive loss to promote local consistency among within-class cross domain
samples while enforcing separation between classes, thus preserving
discriminative transfer from source to target. We validate the advantages of
MemSAC with significant improvements over previous state-of-the-art on multiple
challenging transfer tasks designed for large-scale adaptation, such as
DomainNet with 345 classes and fine-grained adaptation on Caltech-UCSD birds
dataset with 200 classes. We also provide in-depth analysis and insights into
the effectiveness of MemSAC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalluri_T/0/1/0/all/0/1&quot;&gt;Tarun Kalluri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Astuti Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandraker_M/0/1/0/all/0/1&quot;&gt;Manmohan Chandraker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.02809">
<title>The Role of Morphological Variation in Evolutionary Robotics: Maximizing Performance and Robustness. (arXiv:2208.02809v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2208.02809</link>
<description rdf:parseType="Literal">&lt;p&gt;Exposing an Evolutionary Algorithm that is used to evolve robot controllers
to variable conditions is necessary to obtain solutions which are robust and
can cross the reality gap. However, we do not yet have methods for analyzing
and understanding the impact of the varying morphological conditions which
impact the evolutionary process, and therefore for choosing suitable variation
ranges. By morphological conditions, we refer to the starting state of the
robot, and to variations in its sensor readings during operation due to noise.
In this article, we introduce a method that permits us to measure the impact of
these morphological variations and we analyze the relation between the
amplitude of variations, the modality with which they are introduced, and the
performance and robustness of evolving agents. Our results demonstrate that (i)
the evolutionary algorithm can tolerate morphological variations which have a
very high impact, (ii) variations affecting the actions of the agent are
tolerated much better than variations affecting the initial state of the agent
or of the environment, and (iii) improving the accuracy of the fitness measure
through multiple evaluations is not always useful. Moreover, our results show
that morphological variations permit generating solutions which perform better
both in varying and non-varying conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_J/0/1/0/all/0/1&quot;&gt;Jonata Tyska Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nolfi_S/0/1/0/all/0/1&quot;&gt;Stefano Nolfi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15889">
<title>Towards Data-and Knowledge-Driven Artificial Intelligence: A Survey on Neuro-Symbolic Computing. (arXiv:2210.15889v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15889</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural-symbolic computing (NeSy), which pursues the integration of the
symbolic and statistical paradigms of cognition, has been an active research
area of Artificial Intelligence (AI) for many years. As NeSy shows promise of
reconciling the advantages of reasoning and interpretability of symbolic
representation and robust learning in neural networks, it may serve as a
catalyst for the next generation of AI. In the present paper, we provide a
systematic overview of the recent developments and important contributions of
NeSy research. Firstly, we introduce study history of this area, covering early
work and foundations. We further discuss background concepts and identify key
driving factors behind the development of NeSy. Afterward, we categorize recent
landmark approaches along several main characteristics that underline this
research paradigm, including neural-symbolic integration, knowledge
representation, knowledge embedding, and functionality. Next, we briefly
discuss the successful application of modern NeSy approaches in several
domains. Then, we benchmark several NeSy methods on three representative
application tasks. Finally, we identify the open problems together with
potential future research directions. This survey is expected to help new
researchers enter this rapidly evolving field and accelerate the progress
towards data-and knowledge-driven AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10886">
<title>Automatic Intrinsic Reward Shaping for Exploration in Deep Reinforcement Learning. (arXiv:2301.10886v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;We present AIRS: Automatic Intrinsic Reward Shaping that intelligently and
adaptively provides high-quality intrinsic rewards to enhance exploration in
reinforcement learning (RL). More specifically, AIRS selects shaping function
from a predefined set based on the estimated task return in real-time,
providing reliable exploration incentives and alleviating the biased objective
problem. Moreover, we develop an intrinsic reward toolkit to provide efficient
and reliable implementations of diverse intrinsic reward approaches. We test
AIRS on various tasks of MiniGrid, Procgen, and DeepMind Control Suite.
Extensive simulation demonstrates that AIRS can outperform the benchmarking
schemes and achieve superior performance with simple architecture.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingqi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wenjun Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13326">
<title>A Framework for Adapting Offline Algorithms to Solve Combinatorial Multi-Armed Bandit Problems with Bandit Feedback. (arXiv:2301.13326v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13326</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of stochastic, combinatorial multi-armed bandits
where the learner only has access to bandit feedback and the reward function
can be non-linear. We provide a general framework for adapting discrete offline
approximation algorithms into sublinear $\alpha$-regret methods that only
require bandit feedback, achieving
$\mathcal{O}\left(T^\frac{2}{3}\log(T)^\frac{1}{3}\right)$ expected cumulative
$\alpha$-regret dependence on the horizon $T$. The framework only requires the
offline algorithms to be robust to small errors in function evaluation. The
adaptation procedure does not even require explicit knowledge of the offline
approximation algorithm -- the offline algorithm can be used as a black box
subroutine. To demonstrate the utility of the proposed framework, the proposed
framework is applied to diverse applications in submodular maximization. The
new CMAB algorithms for submodular maximization with knapsack constraints
outperform a full-bandit method developed for the adversarial setting in
experiments with real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_G/0/1/0/all/0/1&quot;&gt;Guanyu Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nadew_Y/0/1/0/all/0/1&quot;&gt;Yididiya Y Nadew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yanhui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_V/0/1/0/all/0/1&quot;&gt;Vaneet Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quinn_C/0/1/0/all/0/1&quot;&gt;Christopher John Quinn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13359">
<title>IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13359</link>
<description rdf:parseType="Literal">&lt;p&gt;Image anomaly detection (IAD) is an urgent issue that needs to be addressed
in modern industrial manufacturing (IM). Recently, many advanced algorithms
have been released, but their performance varies greatly due to non-uniformed
settings. That is, researchers find it difficult to analyze because they are
designed for different or specific cases in IM. To eliminate this problem, we
first propose a uniform IAD setting to systematically assess the effectiveness
of these algorithms, mainly considering three aspects of supervision level
(unsupervised, fully supervised), learning paradigm (few-shot, continual, noisy
label), and efficiency (memory usage, inference speed). Then, we skillfully
construct a comprehensive image anomaly detection benchmark (IM-IAD), which
includes 19 algorithms on 7 major datasets with the same setting. Our extensive
experiments (17,017 total) provide new insights into the redesign or selection
of the IAD algorithm under uniform conditions. Importantly, the proposed IM-IAD
presents feasible challenges and future directions for further work. We believe
that this work can have a significant impact on the IAD field. To foster
reproducibility and accessibility, the source code of IM-IAD is uploaded on the
website, https://github.com/M-3LAB/IM-IAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiayi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03770">
<title>Provably Efficient Offline Goal-Conditioned Reinforcement Learning with General Function Approximation and Single-Policy Concentrability. (arXiv:2302.03770v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03770</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal-conditioned reinforcement learning (GCRL) refers to learning
general-purpose skills that aim to reach diverse goals. In particular, offline
GCRL only requires purely pre-collected datasets to perform training tasks
without additional interactions with the environment. Although offline GCRL has
become increasingly prevalent and many previous works have demonstrated its
empirical success, the theoretical understanding of efficient offline GCRL
algorithms is not well established, especially when the state space is huge and
the offline dataset only covers the policy we aim to learn. In this paper, we
provide a rigorous theoretical analysis of an existing empirically successful
offline GCRL algorithm. We prove that under slight modification, this algorithm
enjoys an $\widetilde{O}(\text{poly}(1/\epsilon))$ sample complexity (where
$\epsilon$ is the desired suboptimality of the learned policy) with general
function approximation thanks to the property of (semi-)strong convexity of the
objective functions. We only require nearly minimal assumptions on the dataset
(single-policy concentrability) and the function class (realizability).
Moreover, this algorithm consists of two uninterleaved optimization steps,
which we refer to as $V$-learning and policy learning, and is computationally
stable since it does not involve minimax optimization. We also empirically
validate our theory by showing that the modified algorithm outperforms the
previous algorithm in various real-world environments. To the best of our
knowledge, this is the first algorithm that is both provably efficient with
general function approximation and single-policy concentrability, and
empirically successful without requiring solving minimax optimization problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hanlin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11713">
<title>Can Pre-trained Vision and Language Models Answer Visual Information-Seeking Questions?. (arXiv:2302.11713v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11713</link>
<description rdf:parseType="Literal">&lt;p&gt;Pre-trained vision and language models have demonstrated state-of-the-art
capabilities over existing tasks involving images and texts, including visual
question answering. However, it remains unclear whether these models possess
the capability to answer questions that are not only querying visual content
but knowledge-intensive and information-seeking. In this study, we introduce
InfoSeek, a visual question answering dataset tailored for information-seeking
questions that cannot be answered with only common sense knowledge. Using
InfoSeek, we analyze various pre-trained visual question answering models and
gain insights into their characteristics. Our findings reveal that
state-of-the-art pre-trained multi-modal models (e.g., PaLI-X, BLIP2, etc.)
face challenges in answering visual information-seeking questions, but
fine-tuning on the InfoSeek dataset elicits models to use fine-grained
knowledge that was learned during their pre-training. Furthermore, we show that
accurate visual entity recognition can be used to improve performance on
InfoSeek by retrieving relevant documents, showing a significant space for
improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hexiang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luan_Y/0/1/0/all/0/1&quot;&gt;Yi Luan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Haitian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Changpinyo_S/0/1/0/all/0/1&quot;&gt;Soravit Changpinyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ritter_A/0/1/0/all/0/1&quot;&gt;Alan Ritter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1&quot;&gt;Ming-Wei Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12461">
<title>Analyzing And Editing Inner Mechanisms Of Backdoored Language Models. (arXiv:2302.12461v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12461</link>
<description rdf:parseType="Literal">&lt;p&gt;Poisoning of data sets is a potential security threat to large language
models that can lead to backdoored models. A description of the internal
mechanisms of backdoored language models and how they process trigger inputs,
e.g., when switching to toxic language, has yet to be found. In this work, we
study the internal representations of transformer-based backdoored language
models and determine early-layer MLP modules as most important for the backdoor
mechanism in combination with the initial embedding projection. We use this
knowledge to remove, insert, and modify backdoor mechanisms with engineered
replacements that reduce the MLP module outputs to essentials for the backdoor
mechanism. To this end, we introduce PCP ablation, where we replace transformer
modules with low-rank matrices based on the principal components of their
activations. We demonstrate our results on backdoored toy, backdoored large,
and non-backdoored open-source models. We show that we can improve the backdoor
robustness of large language models by locally constraining individual modules
during fine-tuning on potentially poisonous data sets.
&lt;/p&gt;
&lt;p&gt;Trigger warning: Offensive language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamparth_M/0/1/0/all/0/1&quot;&gt;Max Lamparth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reuel_A/0/1/0/all/0/1&quot;&gt;Anka Reuel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00457">
<title>LLMMaps -- A Visual Metaphor for Stratified Evaluation of Large Language Models. (arXiv:2304.00457v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00457</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized natural language processing
and demonstrated impressive capabilities in various tasks. Unfortunately, they
are prone to hallucinations, where the model exposes incorrect or false
information in its responses, which renders diligent evaluation approaches
mandatory. While LLM performance in specific knowledge fields is often
evaluated based on question and answer (Q&amp;amp;A) datasets, such evaluations usually
report only a single accuracy number for the dataset, which often covers an
entire field. This field-based evaluation, is problematic with respect to
transparency and model improvement. A stratified evaluation could instead
reveal subfields, where hallucinations are more likely to occur and thus help
to better assess LLMs&apos; risks and guide their further development. To support
such stratified evaluations, we propose LLMMaps as a novel visualization
technique that enables users to evaluate LLMs&apos; performance with respect to Q&amp;amp;A
datasets. LLMMaps provide detailed insights into LLMs&apos; knowledge capabilities
in different subfields, by transforming Q&amp;amp;A datasets as well as LLM responses
into an internal knowledge structure. An extension for comparative
visualization furthermore, allows for the detailed comparison of multiple LLMs.
To assess LLMMaps we use them to conduct a comparative analysis of several
state-of-the-art LLMs, such as BLOOM, GPT-2, GPT-3, ChatGPT and LLaMa-13B, as
well as two qualitative user evaluations. All necessary source code and data
for generating LLMMaps to be used in scientific publications and elsewhere is
available on GitHub: https://github.com/viscom-ulm/LLMMaps
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puchert_P/0/1/0/all/0/1&quot;&gt;Patrik Puchert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poonam_P/0/1/0/all/0/1&quot;&gt;Poonam Poonam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onzenoodt_C/0/1/0/all/0/1&quot;&gt;Christian van Onzenoodt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ropinski_T/0/1/0/all/0/1&quot;&gt;Timo Ropinski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11657">
<title>Enhancing Chain-of-Thoughts Prompting with Iterative Bootstrapping in Large Language Models. (arXiv:2304.11657v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11657</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can achieve highly effective performance on
various reasoning tasks by incorporating step-by-step chain-of-thought (CoT)
prompting as demonstrations. However, the reasoning chains of demonstrations
generated by LLMs are prone to errors, which can subsequently lead to incorrect
reasoning during inference. Furthermore, inappropriate exemplars (overly
simplistic or complex), can affect overall performance among varying levels of
difficulty. We introduce Iter-CoT (Iterative bootstrapping in Chain-of-Thoughts
Prompting), an iterative bootstrapping approach for selecting exemplars and
generating reasoning chains. By utilizing iterative bootstrapping, our approach
enables LLMs to autonomously rectify errors, resulting in more precise and
comprehensive reasoning chains. Simultaneously, our approach selects
challenging yet answerable questions accompanied by reasoning chains as
exemplars with a moderate level of difficulty, which enhances the LLMs&apos;
generalizability across varying levels of difficulty. Experimental results
indicate that Iter-CoT exhibits superiority, achieving competitive performance
across three distinct reasoning tasks on ten datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiashuo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yeyun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yelong Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jian Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12233">
<title>Diffusion-based Generative AI for Exploring Transition States from 2D Molecular Graphs. (arXiv:2304.12233v3 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12233</link>
<description rdf:parseType="Literal">&lt;p&gt;The exploration of transition state (TS) geometries is crucial for
elucidating chemical reaction mechanisms and modeling their kinetics. Recently,
machine learning (ML) models have shown remarkable performance for prediction
of TS geometries. However, they require 3D conformations of reactants and
products often with their appropriate orientations as input, which demands
substantial efforts and computational cost. Here, we propose a generative
approach based on the stochastic diffusion method, namely TSDiff, for
prediction of TS geometries just from 2D molecular graphs. TSDiff outperformed
the existing ML models with 3D geometries in terms of both accuracy and
efficiency. Moreover, it enables to sample various TS conformations, because it
learned the distribution of TS geometries for diverse reactions in training.
Thus, TSDiff was able to find more favorable reaction pathways with lower
barrier heights than those in the reference database. These results demonstrate
that TSDiff shows promising potential for an efficient and reliable TS
exploration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seonghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Woo_J/0/1/0/all/0/1&quot;&gt;Jeheon Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kim_W/0/1/0/all/0/1&quot;&gt;Woo Youn Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07375">
<title>Is ChatGPT a Good Causal Reasoner? A Comprehensive Evaluation. (arXiv:2305.07375v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07375</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal reasoning ability is crucial for numerous NLP applications. Despite
the impressive emerging ability of ChatGPT in various NLP tasks, it is unclear
how well ChatGPT performs in causal reasoning. In this paper, we conduct the
first comprehensive evaluation of the ChatGPT&apos;s causal reasoning capabilities.
Experiments show that ChatGPT is not a good causal reasoner, but a good causal
explainer. Besides, ChatGPT has a serious hallucination on causal reasoning,
possibly due to the reporting biases between causal and non-causal
relationships in natural language, as well as ChatGPT&apos;s upgrading processes,
such as RLHF. The In-Context Learning (ICL) and Chain-of-Thought (CoT)
techniques can further exacerbate such causal hallucination. Additionally, the
causal reasoning ability of ChatGPT is sensitive to the words used to express
the causal concept in prompts, and close-ended prompts perform better than
open-ended prompts. For events in sentences, ChatGPT excels at capturing
explicit causality rather than implicit causality, and performs better in
sentences with lower event density and smaller lexical distance between events.
The code is available on https://github.com/ArrogantL/ChatGPT4CausalReasoning .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jinglong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Ting Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09656">
<title>SatLM: Satisfiability-Aided Language Models Using Declarative Prompting. (arXiv:2305.09656v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09656</link>
<description rdf:parseType="Literal">&lt;p&gt;Prior work has combined chain-of-thought prompting in large language models
(LLMs) with programmatic representations to perform effective and transparent
reasoning. While such an approach works well for tasks that only require
forward reasoning (e.g., straightforward arithmetic), it is less effective for
constraint solving problems that require more sophisticated planning and
search. In this paper, we propose a new satisfiability-aided language modeling
(SatLM) approach for improving the reasoning capabilities of LLMs. We use an
LLM to generate a declarative task specification rather than an imperative
program and leverage an off-the-shelf automated theorem prover to derive the
final answer. This approach has two key advantages. The declarative
specification is closer to the problem description than the reasoning steps
are, so the LLM can parse it out of the description more accurately.
Furthermore, by offloading the actual reasoning task to an automated theorem
prover, our approach can guarantee the correctness of the answer with respect
to the parsed specification and avoid planning errors in the solving process.
We evaluate SATLM on 8 different datasets and show that it consistently
outperforms program-aided LMs in the imperative paradigm. In particular, SATLM
outperforms program-aided LMs by 23% on a challenging subset of the GSM
arithmetic reasoning dataset; SATLM also achieves a new SoTA on LSAT and
BoardgameQA, surpassing previous models that are trained on the respective
training sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiaochu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dillig_I/0/1/0/all/0/1&quot;&gt;Isil Dillig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durrett_G/0/1/0/all/0/1&quot;&gt;Greg Durrett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14259">
<title>Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14259</link>
<description rdf:parseType="Literal">&lt;p&gt;Literature-Based Discovery (LBD) aims to discover new scientific knowledge by
mining papers and generating hypotheses. Standard LBD is limited to predicting
pairwise relations between discrete concepts (e.g., drug-disease links), and
ignores critical contexts like experimental settings (e.g., a specific patient
population where a drug is evaluated) and background motivations (e.g., to find
drugs without specific side effects). We address these limitations with a novel
formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in
natural language, while grounding them in a context that controls the
hypothesis search space. We present a modeling framework using retrieval of
``inspirations&apos;&apos; from past scientific papers. Our evaluations reveal that GPT-4
tends to generate ideas with overall low technical depth and novelty, while our
inspiration prompting approaches partially mitigate this issue. Our work
represents a first step toward building language models that generate new ideas
derived from scientific literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1&quot;&gt;Doug Downey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1&quot;&gt;Tom Hope&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00721">
<title>UnDiff: Unsupervised Voice Restoration with Unconditional Diffusion Model. (arXiv:2306.00721v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00721</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces UnDiff, a diffusion probabilistic model capable of
solving various speech inverse tasks. Being once trained for speech waveform
generation in an unconditional manner, it can be adapted to different tasks
including degradation inversion, neural vocoding, and source separation. In
this paper, we, first, tackle the challenging problem of unconditional waveform
generation by comparing different neural architectures and preconditioning
domains. After that, we demonstrate how the trained unconditional diffusion
could be adapted to different tasks of speech processing by the means of recent
developments in post-training conditioning of diffusion models. Finally, we
demonstrate the performance of the proposed technique on the tasks of bandwidth
extension, declipping, vocoding, and speech source separation and compare it to
the baselines. The codes are publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iashchenko_A/0/1/0/all/0/1&quot;&gt;Anastasiia Iashchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreev_P/0/1/0/all/0/1&quot;&gt;Pavel Andreev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shchekotov_I/0/1/0/all/0/1&quot;&gt;Ivan Shchekotov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaev_N/0/1/0/all/0/1&quot;&gt;Nicholas Babaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetrov_D/0/1/0/all/0/1&quot;&gt;Dmitry Vetrov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08762">
<title>Theoretical Hardness and Tractability of POMDPs in RL with Partial Online State Information. (arXiv:2306.08762v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08762</link>
<description rdf:parseType="Literal">&lt;p&gt;Partially observable Markov decision processes (POMDPs) have been widely
applied to capture many real-world applications. However, existing theoretical
results have shown that learning in general POMDPs could be intractable, where
the main challenge lies in the lack of latent state information. A key
fundamental question here is how much online state information (OSI) is
sufficient to achieve tractability. In this paper, we establish a lower bound
that reveals a surprising hardness result: unless we have full OSI, we need an
exponentially scaling sample complexity to obtain an $\epsilon$-optimal policy
solution for POMDPs. Nonetheless, inspired by the key insights in our lower
bound design, we find that there exist important tractable classes of POMDPs
even with only partial OSI. In particular, for two novel classes of POMDPs with
partial OSI, we provide new algorithms that are proved to be near-optimal by
establishing new regret upper and lower bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Ming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yingbin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shroff_N/0/1/0/all/0/1&quot;&gt;Ness Shroff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11670">
<title>GIO: Gradient Information Optimization for Training Dataset Selection. (arXiv:2306.11670v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11670</link>
<description rdf:parseType="Literal">&lt;p&gt;It is often advantageous to train models on a subset of the available train
examples, because the examples are of variable quality or because one would
like to train with fewer examples, without sacrificing performance. We present
Gradient Information Optimization (GIO), a scalable, task-agnostic approach to
this data selection problem that requires only a small set of (unlabeled)
examples representing a target distribution. GIO begins from a natural,
information-theoretic objective that is intractable in practice. Our
contribution is in showing that it can be made highly scalable through a simple
relaxation of the objective and a highly efficient implementation. In
experiments with machine translation, spelling correction, and image
recognition, we show that GIO delivers outstanding results with very small
train sets. These findings are robust to different representation models and
hyperparameters for GIO itself. GIO is task- and domain-agnostic and can be
applied out-of-the-box to new datasets and domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Everaert_D/0/1/0/all/0/1&quot;&gt;Dante Everaert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02484">
<title>Elastic Decision Transformer. (arXiv:2307.02484v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02484</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Elastic Decision Transformer (EDT), a significant
advancement over the existing Decision Transformer (DT) and its variants.
Although DT purports to generate an optimal trajectory, empirical evidence
suggests it struggles with trajectory stitching, a process involving the
generation of an optimal or near-optimal trajectory from the best parts of a
set of sub-optimal trajectories. The proposed EDT differentiates itself by
facilitating trajectory stitching during action inference at test time,
achieved by adjusting the history length maintained in DT. Further, the EDT
optimizes the trajectory by retaining a longer history when the previous
trajectory is optimal and a shorter one when it is sub-optimal, enabling it to
&quot;stitch&quot; with a more optimal trajectory. Extensive experimentation demonstrates
EDT&apos;s ability to bridge the performance gap between DT-based and Q
Learning-based approaches. In particular, the EDT outperforms Q Learning-based
methods in a multi-task regime on the D4RL locomotion benchmark and Atari
games. Videos are available at: https://kristery.github.io/edt/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yueh-Hua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamaya_M/0/1/0/all/0/1&quot;&gt;Masashi Hamaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03135">
<title>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03135</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student&apos;s OOD
generalization: (1) by better imitating teacher&apos;s visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher&apos;s language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Poster:
https://xuanlinli17.github.io/pdfs/iccv23_large_vlm_distillation_poster.pdf
Code: https://github.com/xuanlinli17/large_vlm_distillation_ood
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03486">
<title>Discovering Hierarchical Achievements in Reinforcement Learning via Contrastive Learning. (arXiv:2307.03486v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03486</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering achievements with a hierarchical structure in procedurally
generated environments presents a significant challenge. This requires an agent
to possess a broad range of abilities, including generalization and long-term
reasoning. Many prior methods have been built upon model-based or hierarchical
approaches, with the belief that an explicit module for long-term planning
would be advantageous for learning hierarchical dependencies. However, these
methods demand an excessive number of environment interactions or large model
sizes, limiting their practicality. In this work, we demonstrate that proximal
policy optimization (PPO), a simple yet versatile model-free algorithm,
outperforms previous methods when optimized with recent implementation
practices. Moreover, we find that the PPO agent can predict the next
achievement to be unlocked to some extent, albeit with limited confidence.
Based on this observation, we introduce a novel contrastive learning method,
called achievement distillation, which strengthens the agent&apos;s ability to
predict the next achievement. Our method exhibits a strong capacity for
discovering hierarchical achievements and shows state-of-the-art performance on
the challenging Crafter environment in a sample-efficient manner while
utilizing fewer model parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seungyong Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeom_J/0/1/0/all/0/1&quot;&gt;Junyoung Yeom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_B/0/1/0/all/0/1&quot;&gt;Bumsoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hyun Oh Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05035">
<title>Expert load matters: operating networks at high accuracy and low manual effort. (arXiv:2308.05035v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05035</link>
<description rdf:parseType="Literal">&lt;p&gt;In human-AI collaboration systems for critical applications, in order to
ensure minimal error, users should set an operating point based on model
confidence to determine when the decision should be delegated to human experts.
Samples for which model confidence is lower than the operating point would be
manually analysed by experts to avoid mistakes. Such systems can become truly
useful only if they consider two aspects: models should be confident only for
samples for which they are accurate, and the number of samples delegated to
experts should be minimized. The latter aspect is especially crucial for
applications where available expert time is limited and expensive, such as
healthcare. The trade-off between the model accuracy and the number of samples
delegated to experts can be represented by a curve that is similar to an ROC
curve, which we refer to as confidence operating characteristic (COC) curve. In
this paper, we argue that deep neural networks should be trained by taking into
account both accuracy and expert load and, to that end, propose a new
complementary loss function for classification that maximizes the area under
this COC curve. This promotes simultaneously the increase in network accuracy
and the reduction in number of samples delegated to humans. We perform
experiments on multiple computer vision and medical image datasets for
classification. Our results demonstrate that the proposed loss improves
classification accuracy and delegates less number of decisions to experts,
achieves better out-of-distribution samples detection and on par calibration
performance compared to existing loss functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sangalli_S/0/1/0/all/0/1&quot;&gt;Sara Sangalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdil_E/0/1/0/all/0/1&quot;&gt;Ertunc Erdil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konukoglu_E/0/1/0/all/0/1&quot;&gt;Ender Konukoglu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11534">
<title>PlatoLM: Teaching LLMs via a Socratic Questioning User Simulator. (arXiv:2308.11534v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11534</link>
<description rdf:parseType="Literal">&lt;p&gt;The unparalleled performance of closed-sourced ChatGPT has sparked efforts
towards its democratization, with notable strides made by leveraging real user
and ChatGPT conversations, as evidenced by Vicuna. However, due to challenges
in gathering conversations involving human participation, current endeavors
like Baize and UltraChat aim to automatically generate conversational data.
They primarily rely on ChatGPT conducting roleplay to simulate human behaviors
based on instructions rather than genuine learning from humans, resulting in
limited scope, diminished diversity, and an absence of genuine multi-round
conversational dynamics. To address the above issues, we target human questions
extracted from genuine human-machine conversations as a learning goal and train
a user simulator called `Socratic&apos; to produce a high-quality human-centric
synthetic conversation dataset. Subsequently, this dataset was used to train
our assistant model, named `PlatoLM&apos;. Experimentally, PlatoLM outpaces baseline
models in both Vicuna-Bench and MT-Bench by pairwise comparison when
considering equivalent training set sizes, and manual evaluation also shows
that our model is highly competitive. Impressively, when fine-tuned with the
latest LLaMA 2 model, PlatoLM achieves the SOTA performance among 7B models
(including LLaMA-2-7B-chat and Vicuna-7B) in MT-Bench benchmark and in
Alpaca-Eval benchmark, it ranks second among 7B models, even beating some
larger scale models (including LLaMA-2-13B-chat and GPT-3.5). Further in-depth
analysis demonstrates the scalability and transferability of our approach. The
code is available at https://github.com/FreedomIntelligence/PlatoLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_C/0/1/0/all/0/1&quot;&gt;Chuyi Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yaxin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Feng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benyou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12243">
<title>Multi-Objective Optimization for Sparse Deep Neural Network Training. (arXiv:2308.12243v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12243</link>
<description rdf:parseType="Literal">&lt;p&gt;Different conflicting optimization criteria arise naturally in various Deep
Learning scenarios. These can address different main tasks (i.e., in the
setting of Multi-Task Learning), but also main and secondary tasks such as loss
minimization versus sparsity. The usual approach is a simple weighting of the
criteria, which formally only works in the convex setting. In this paper, we
present a Multi-Objective Optimization algorithm using a modified Weighted
Chebyshev scalarization for training Deep Neural Networks (DNNs) with respect
to several tasks. By employing this scalarization technique, the algorithm can
identify all optimal solutions of the original problem while reducing its
complexity to a sequence of single-objective problems. The simplified problems
are then solved using an Augmented Lagrangian method, enabling the use of
popular optimization techniques such as Adam and Stochastic Gradient Descent,
while efficaciously handling constraints. Our work aims to address the
(economical and also ecological) sustainability issue of DNN models, with a
particular focus on Deep Multi-Task models, which are typically designed with a
very large number of weights to perform equally well on multiple tasks. Through
experiments conducted on two Machine Learning datasets, we demonstrate the
possibility of adaptively sparsifying the model during training without
significantly impacting its performance, if we are willing to apply
task-specific adaptations to the network weights. Code is available at
https://github.com/salomonhotegni/MDMTN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hotegni_S/0/1/0/all/0/1&quot;&gt;S. S. Hotegni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peitz_S/0/1/0/all/0/1&quot;&gt;S. Peitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkemeier_M/0/1/0/all/0/1&quot;&gt;M. Berkemeier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13422">
<title>QKSAN: A Quantum Kernel Self-Attention Network. (arXiv:2308.13422v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13422</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-Attention Mechanism (SAM) excels at distilling important information
from the interior of data to improve the computational efficiency of models.
Nevertheless, many Quantum Machine Learning (QML) models lack the ability to
distinguish the intrinsic connections of information like SAM, which limits
their effectiveness on massive high-dimensional quantum data. To tackle the
above issue, a Quantum Kernel Self-Attention Mechanism (QKSAM) is introduced to
combine the data representation merit of Quantum Kernel Methods (QKM) with the
efficient information extraction capability of SAM. Further, a Quantum Kernel
Self-Attention Network (QKSAN) framework is proposed based on QKSAM, which
ingeniously incorporates the Deferred Measurement Principle (DMP) and
conditional measurement techniques to release half of quantum resources by
mid-circuit measurement, thereby bolstering both feasibility and adaptability.
Simultaneously, the Quantum Kernel Self-Attention Score (QKSAS) with an
exponentially large characterization space is spawned to accommodate more
information and determine the measurement conditions. Eventually, four QKSAN
sub-models are deployed on PennyLane and IBM Qiskit platforms to perform binary
classification on MNIST and Fashion MNIST, where the QKSAS tests and
correlation assessments between noise immunity and learning ability are
executed on the best-performing sub-model. The paramount experimental finding
is that a potential learning advantage is revealed in partial QKSAN subclasses
that acquire an impressive more than 98.05% high accuracy with very few
parameters that are much less in aggregate than classical machine learning
models. Predictably, QKSAN lays the foundation for future quantum computers to
perform machine learning on massive amounts of data while driving advances in
areas such as quantum computer vision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ren-Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jinjing Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14311">
<title>Spread Control Method on Unknown Networks Based on Hierarchical Reinforcement Learning. (arXiv:2308.14311v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14311</link>
<description rdf:parseType="Literal">&lt;p&gt;Epidemics such as COVID-19 pose serious threats to public health and our
society, and it is critical to investigate effective methods to control the
spread of epidemics over networks. Prior works on epidemic control often assume
complete knowledge of network structures, a presumption seldom valid in
real-world situations. In this paper, we study epidemic control on networks
with unknown structures, and propose a hierarchical reinforcement learning
framework for joint network structure exploration and epidemic control. To
reduce the action space and achieve computation tractability, our proposed
framework contains three modules: the Policy Selection Module, which determines
whether to explore the structure or remove nodes to control the epidemic; the
Explore Module, responsible for selecting nodes to explore; and the Remove
Module, which decides which nodes to remove to stop the epidemic spread.
Simulation results show that our proposed method outperforms baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Wenxiang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhanjiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;H.Vicky Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16198">
<title>Learning Collaborative Information Dissemination with Graph-based Multi-Agent Reinforcement Learning. (arXiv:2308.16198v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16198</link>
<description rdf:parseType="Literal">&lt;p&gt;In modern communication systems, efficient and reliable information
dissemination is crucial for supporting critical operations across domains like
disaster response, autonomous vehicles, and sensor networks. This paper
introduces a Multi-Agent Reinforcement Learning (MARL) approach as a
significant step forward in achieving more decentralized, efficient, and
collaborative solutions. We propose a Partially Observable Stochastic Game
(POSG) formulation for information dissemination empowering each agent to
decide on message forwarding independently, based on their one-hop
neighborhood. This constitutes a significant paradigm shift from traditional
heuristics based on Multi-Point Relay (MPR) selection. Our approach harnesses
Graph Convolutional Reinforcement Learning, employing Graph Attention Networks
(GAT) with dynamic attention to capture essential network features. We propose
two approaches, L-DGN and HL-DGN, which differ in the information that is
exchanged among agents. We evaluate the performance of our decentralized
approaches, by comparing them with a widely-used MPR heuristic, and we show
that our trained policies are able to efficiently cover the network while
bypassing the MPR set selection process. Our approach is a first step toward
supporting the resilience of real-world broadcast communication infrastructures
via learned, collaborative information dissemination.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galliera_R/0/1/0/all/0/1&quot;&gt;Raffaele Galliera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venable_K/0/1/0/all/0/1&quot;&gt;Kristen Brent Venable&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bassani_M/0/1/0/all/0/1&quot;&gt;Matteo Bassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suri_N/0/1/0/all/0/1&quot;&gt;Niranjan Suri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00480">
<title>Learning-based NLOS Detection and Uncertainty Prediction of GNSS Observations with Transformer-Enhanced LSTM Network. (arXiv:2309.00480v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00480</link>
<description rdf:parseType="Literal">&lt;p&gt;The global navigation satellite systems (GNSS) play a vital role in transport
systems for accurate and consistent vehicle localization. However, GNSS
observations can be distorted due to multipath effects and non-line-of-sight
(NLOS) receptions in challenging environments such as urban canyons. In such
cases, traditional methods to classify and exclude faulty GNSS observations may
fail, leading to unreliable state estimation and unsafe system operations. This
work proposes a deep-learning-based method to detect NLOS receptions and
predict GNSS pseudorange errors by analyzing GNSS observations as a
spatio-temporal modeling problem. Compared to previous works, we construct a
transformer-like attention mechanism to enhance the long short-term memory
(LSTM) networks, improving model performance and generalization. For the
training and evaluation of the proposed network, we used labeled datasets from
the cities of Hong Kong and Aachen. We also introduce a dataset generation
process to label the GNSS observations using lidar maps. In experimental
studies, we compare the proposed network with a deep-learning-based model and
classical machine-learning models. Furthermore, we conduct ablation studies of
our network components and integrate the NLOS detection with data
out-of-distribution in a state estimator. As a result, our network presents
improved precision and recall ratios compared to other models. Additionally, we
show that the proposed method avoids trajectory divergence in real-world
vehicle localization by classifying and excluding NLOS observations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhanxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vallery_H/0/1/0/all/0/1&quot;&gt;Heike Vallery&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03084">
<title>Pure Monte Carlo Counterfactual Regret Minimization. (arXiv:2309.03084v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03084</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactual Regret Minimization (CFR) and its variants are the best
algorithms so far for solving large-scale incomplete information games.
However, we believe that there are two problems with CFR: First, matrix
multiplication is required in CFR iteration, and the time complexity of one
iteration is too high; Secondly, the game characteristics in the real world are
different. Just using one CFR algorithm will not be perfectly suitable for all
game problems.
&lt;/p&gt;
&lt;p&gt;For these two problems, this paper proposes a new algorithm called Pure CFR
(PCFR) based on CFR. PCFR can be seen as a combination of CFR and Fictitious
Play (FP), inheriting the concept of counterfactual regret (value) from CFR,
and using the best response strategy instead of the regret matching strategy
for the next iteration. This algorithm has three advantages. First, PCFR can be
combined with any CFR variant. The resulting Pure MCCFR (PMCCFR) can
significantly reduce the time and space complexity of one iteration. Secondly,
our experiments show that the convergence speed of the PMCCFR is 2$\sim$3 times
that of the MCCFR. Finally, there is a type of game that is very suitable for
PCFR, we call this type of game clear-game, which is characterized by a high
proportion of dominated strategies. Experiments show that in clear-game, the
convergence rate of PMCCFR is two orders of magnitude higher than that of
MCCFR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Ju Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_T/0/1/0/all/0/1&quot;&gt;Ting Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hei_F/0/1/0/all/0/1&quot;&gt;Falun Hei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Z/0/1/0/all/0/1&quot;&gt;Zhemei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yunfeng Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05173">
<title>DePT: Decomposed Prompt Tuning for Parameter-Efficient Fine-tuning. (arXiv:2309.05173v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Prompt tuning (PT), where a small amount of trainable soft (continuous)
prompt vectors is affixed to the input of language models (LM), has shown
promising results across various tasks and models for parameter-efficient
fine-tuning (PEFT). PT stands out from other PEFT approaches because it
maintains competitive performance with fewer trainable parameters and does not
drastically scale up its parameters as the model size expands. However, PT
introduces additional soft prompt tokens, leading to longer input sequences,
which significantly impacts training and inference time and memory usage due to
the Transformer&apos;s quadratic complexity. Particularly concerning for Large
Language Models (LLMs) that face heavy daily querying. To address this issue,
we propose Decomposed Prompt Tuning (DePT), which decomposes the soft prompt
into a shorter soft prompt and a pair of low-rank matrices that are then
optimised with two different learning rates. This allows DePT to achieve better
performance while saving over 20% memory and time costs compared to vanilla PT
and its variants, without changing trainable parameter sizes. Through extensive
experiments on 23 natural language processing (NLP) and vision-language (VL)
tasks, we demonstrate that DePT outperforms state-of-the-art PEFT approaches,
including the full fine-tuning baseline in some scenarios. Additionally, we
empirically show that DEPT grows more efficient as the model size increases.
Our further study reveals that DePT integrates seamlessly with
parameter-efficient transfer learning in the few-shot learning setting and
highlights its adaptability to various model architectures and sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhengxiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipani_A/0/1/0/all/0/1&quot;&gt;Aldo Lipani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05925">
<title>On Regularized Sparse Logistic Regression. (arXiv:2309.05925v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05925</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse logistic regression is for classification and feature selection
simultaneously. Although many studies have been done to solve
$\ell_1$-regularized logistic regression, there is no equivalently abundant
work on solving sparse logistic regression with nonconvex regularization term.
In this paper, we propose a unified framework to solve $\ell_1$-regularized
logistic regression, which can be naturally extended to nonconvex
regularization term, as long as certain requirement is satisfied. In addition,
we also utilize a different line search criteria to guarantee monotone
convergence for various regularization terms. Empirical experiments on binary
classification tasks with real-world datasets demonstrate our proposed
algorithms are capable of performing classification and feature selection
effectively at a lower computational cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kai Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10691">
<title>MINT: Evaluating LLMs in Multi-turn Interaction with Tools and Language Feedback. (arXiv:2309.10691v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10691</link>
<description rdf:parseType="Literal">&lt;p&gt;To solve complex tasks, large language models (LLMs) often require multiple
rounds of interactions with the user, sometimes assisted by external tools.
However, current evaluation protocols often emphasize benchmark performance
with single-turn exchanges, neglecting the nuanced interactions among the user,
LLMs, and external tools, while also underestimating the importance of natural
language feedback from users. These oversights contribute to discrepancies
between research benchmark evaluations and real-world use cases. We introduce
MINT, a benchmark that evaluates LLMs&apos; ability to solve tasks with multi-turn
interactions by (1) using tools and (2) leveraging natural language feedback.
To ensure reproducibility, we provide an evaluation framework where LLMs can
access tools by executing Python code and receive users&apos; natural language
feedback simulated by GPT-4. We repurpose a diverse set of established
evaluation datasets focusing on reasoning, coding, and decision-making and
carefully curate them into a compact subset for efficient evaluation. Our
analysis of 20 open- and closed-source LLMs offers intriguing findings. (a)
LLMs generally benefit from tools and language feedback, with performance gains
(absolute, same below) of 1-8% for each turn of tool use and 2-17% with natural
language feedback. (b) Better single-turn performance does not guarantee better
multi-turn performance. (c) Surprisingly, on the LLMs evaluated, supervised
instruction-finetuning (SIFT) and reinforcement learning from human feedback
(RLHF) generally hurt multi-turn capabilities. We expect MINT can help measure
progress and incentivize research in improving LLMs&apos; capabilities in multi-turn
interactions, especially for open-source communities where multi-turn human
evaluation can be less accessible compared to commercial LLMs with a larger
user base.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiateng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yangyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Lifan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17319">
<title>Building Privacy-Preserving and Secure Geospatial Artificial Intelligence Foundation Models. (arXiv:2309.17319v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17319</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years we have seen substantial advances in foundation models for
artificial intelligence, including language, vision, and multimodal models.
Recent studies have highlighted the potential of using foundation models in
geospatial artificial intelligence, known as GeoAI Foundation Models, for
geographic question answering, remote sensing image understanding, map
generation, and location-based services, among others. However, the development
and application of GeoAI foundation models can pose serious privacy and
security risks, which have not been fully discussed or addressed to date. This
paper introduces the potential privacy and security risks throughout the
lifecycle of GeoAI foundation models and proposes a comprehensive blueprint for
research directions and preventative and control strategies. Through this
vision paper, we hope to draw the attention of researchers and policymakers in
geospatial domains to these privacy and security risks inherent in GeoAI
foundation models and advocate for the development of privacy-preserving and
secure GeoAI foundation models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1&quot;&gt;Jinmeng Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1&quot;&gt;Gengchen Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1&quot;&gt;Krzysztof Janowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00656">
<title>LEGO-Prover: Neural Theorem Proving with Growing Libraries. (arXiv:2310.00656v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00656</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of large language models (LLMs), the task of theorem
proving still remains one of the hardest reasoning tasks that is far from being
fully solved. Prior methods using language models have demonstrated promising
results, but they still struggle to prove even middle school level theorems.
One common limitation of these methods is that they assume a fixed theorem
library during the whole theorem proving process. However, as we all know,
creating new useful theorems or even new theories is not only helpful but
crucial and necessary for advancing mathematics and proving harder and deeper
results. In this work, we present LEGO-Prover, which employs a growing skill
library containing verified lemmas as skills to augment the capability of LLMs
used in theorem proving. By constructing the proof modularly, LEGO-Prover
enables LLMs to utilize existing skills retrieved from the library and to
create new skills during the proving process. These skills are further evolved
(by prompting an LLM) to enrich the library on another scale. Modular and
reusable skills are constantly added to the library to enable tackling
increasingly intricate mathematical problems. Moreover, the learned library
further bridges the gap between human proofs and formal proofs by making it
easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass
rate on miniF2F-valid (48.0% to 57.0%) and miniF2F-test (45.5% to 47.1%).
During the proving process, LEGO-Prover also manages to generate over 20,000
skills (theorems/lemmas) and adds them to the growing library. Our ablation
study indicates that these newly added skills are indeed helpful for proving
theorems, resulting in an improvement from a success rate of 47.1% to 50.4%. We
also release our code and all the generated skills.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_H/0/1/0/all/0/1&quot;&gt;Huajian Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Chuanyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengying Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qingxing Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yinya Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Jing Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Han Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jian Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenguo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Heng Liao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00737">
<title>GenAI Against Humanity: Nefarious Applications of Generative Artificial Intelligence and Large Language Models. (arXiv:2310.00737v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00737</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Artificial Intelligence (GenAI) and Large Language Models (LLMs)
are marvels of technology; celebrated for their prowess in natural language
processing and multimodal content generation, they promise a transformative
future. But as with all powerful tools, they come with their shadows. Picture
living in a world where deepfakes are indistinguishable from reality, where
synthetic identities orchestrate malicious campaigns, and where targeted
misinformation or scams are crafted with unparalleled precision. Welcome to the
darker side of GenAI applications. This article is not just a journey through
the meanders of potential misuse of GenAI and LLMs, but also a call to
recognize the urgency of the challenges ahead. As we navigate the seas of
misinformation campaigns, malicious content generation, and the eerie creation
of sophisticated malware, we&apos;ll uncover the societal implications that ripple
through the GenAI revolution we are witnessing. From AI-powered botnets on
social media platforms to the unnerving potential of AI to generate fabricated
identities, or alibis made of synthetic realities, the stakes have never been
higher. The lines between the virtual and the real worlds are blurring, and the
consequences of potential GenAI&apos;s nefarious applications impact us all. This
article serves both as a synthesis of rigorous research presented on the risks
of GenAI and misuse of LLMs and as a thought-provoking vision of the different
types of harmful GenAI applications we might encounter in the near future, and
some ways we can prepare for them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrara_E/0/1/0/all/0/1&quot;&gt;Emilio Ferrara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01649">
<title>On Training Derivative-Constrained Neural Networks. (arXiv:2310.01649v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01649</link>
<description rdf:parseType="Literal">&lt;p&gt;We refer to the setting where the (partial) derivatives of a neural network&apos;s
(NN&apos;s) predictions with respect to its inputs are used as additional training
signal as a derivative-constrained (DC) NN. This situation is common in
physics-informed settings in the natural sciences. We propose an integrated
RELU (IReLU) activation function to improve training of DC NNs. We also
investigate denormalization and label rescaling to help stabilize DC training.
We evaluate our methods on physics-informed settings including quantum
chemistry and Scientific Machine Learning (SciML) tasks. We demonstrate that
existing architectures with IReLU activations combined with denormalization and
label rescaling better incorporate training signal provided by derivative
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_K/0/1/0/all/0/1&quot;&gt;KaiChieh Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Daniel Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02772">
<title>Spike Accumulation Forwarding for Effective Training of Spiking Neural Networks. (arXiv:2310.02772v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02772</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we propose a new paradigm for training spiking neural
networks (SNNs), spike accumulation forwarding (SAF). It is known that SNNs are
energy-efficient but difficult to train. Consequently, many researchers have
proposed various methods to solve this problem, among which online training
through time (OTTT) is a method that allows inferring at each time step while
suppressing the memory cost. However, to compute efficiently on GPUs, OTTT
requires operations with spike trains and weighted summation of spike trains
during forwarding. In addition, OTTT has shown a relationship with the Spike
Representation, an alternative training method, though theoretical agreement
with Spike Representation has yet to be proven. Our proposed method can solve
these problems; namely, SAF can halve the number of operations during the
forward process, and it can be theoretically proven that SAF is consistent with
the Spike Representation and OTTT, respectively. Furthermore, we confirmed the
above contents through experiments and showed that it is possible to reduce
memory and training time while maintaining accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saiin_R/0/1/0/all/0/1&quot;&gt;Ryuji Saiin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shirakawa_T/0/1/0/all/0/1&quot;&gt;Tomoya Shirakawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshihara_S/0/1/0/all/0/1&quot;&gt;Sota Yoshihara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawada_Y/0/1/0/all/0/1&quot;&gt;Yoshihide Sawada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kusumoto_H/0/1/0/all/0/1&quot;&gt;Hiroyuki Kusumoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03131">
<title>Axiomatic Aggregations of Abductive Explanations. (arXiv:2310.03131v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03131</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent criticisms of the robustness of post hoc model approximation
explanation methods (like LIME and SHAP) have led to the rise of model-precise
abductive explanations. For each data point, abductive explanations provide a
minimal subset of features that are sufficient to generate the outcome. While
theoretically sound and rigorous, abductive explanations suffer from a major
issue -- there can be several valid abductive explanations for the same data
point. In such cases, providing a single abductive explanation can be
insufficient; on the other hand, providing all valid abductive explanations can
be incomprehensible due to their size. In this work, we solve this issue by
aggregating the many possible abductive explanations into feature importance
scores. We propose three aggregation methods: two based on power indices from
cooperative game theory and a third based on a well-known measure of causal
strength. We characterize these three methods axiomatically, showing that each
of them uniquely satisfies a set of desirable properties. We also evaluate them
on multiple datasets and show that these explanations are robust to the attacks
that fool SHAP and LIME.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biradar_G/0/1/0/all/0/1&quot;&gt;Gagan Biradar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Izza_Y/0/1/0/all/0/1&quot;&gt;Yacine Izza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobo_E/0/1/0/all/0/1&quot;&gt;Elita Lobo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viswanathan_V/0/1/0/all/0/1&quot;&gt;Vignesh Viswanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zick_Y/0/1/0/all/0/1&quot;&gt;Yair Zick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04413">
<title>Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets. (arXiv:2310.04413v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04413</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline policy learning is aimed at learning decision-making policies using
existing datasets of trajectories without collecting additional data. The
primary motivation for using reinforcement learning (RL) instead of supervised
learning techniques such as behavior cloning is to find a policy that achieves
a higher average return than the trajectories constituting the dataset.
However, we empirically find that when a dataset is dominated by suboptimal
trajectories, state-of-the-art offline RL algorithms do not substantially
improve over the average return of trajectories in the dataset. We argue this
is due to an assumption made by current offline RL algorithms of staying close
to the trajectories in the dataset. If the dataset primarily consists of
sub-optimal trajectories, this assumption forces the policy to mimic the
suboptimal actions. We overcome this issue by proposing a sampling strategy
that enables the policy to only be constrained to ``good data&quot; rather than all
actions in the dataset (i.e., uniform sampling). We present a realization of
the sampling strategy and an algorithm that can be used as a plug-and-play
module in standard offline RL algorithms. Our evaluation demonstrates
significant performance gains in 72 imbalanced datasets, D4RL dataset, and
across three different offline RL algorithms. Code is available at
https://github.com/Improbable-AI/dw-offline-rl.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Z/0/1/0/all/0/1&quot;&gt;Zhang-Wei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Aviral Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnik_S/0/1/0/all/0/1&quot;&gt;Sathwik Karnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandwaldar_A/0/1/0/all/0/1&quot;&gt;Abhishek Bhandwaldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Akash Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laroche_R/0/1/0/all/0/1&quot;&gt;Romain Laroche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_P/0/1/0/all/0/1&quot;&gt;Pulkit Agrawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04472">
<title>Effective Slogan Generation with Noise Perturbation. (arXiv:2310.04472v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04472</link>
<description rdf:parseType="Literal">&lt;p&gt;Slogans play a crucial role in building the brand&apos;s identity of the firm. A
slogan is expected to reflect firm&apos;s vision and brand&apos;s value propositions in
memorable and likeable ways. Automating the generation of slogans with such
characteristics is challenging. Previous studies developted and tested slogan
generation with syntactic control and summarization models which are not
capable of generating distinctive slogans. We introduce a a novel apporach that
leverages pre-trained transformer T5 model with noise perturbation on newly
proposed 1:N matching pair dataset. This approach serves as a contributing
fator in generting distinctive and coherent slogans. Turthermore, the proposed
approach incorporates descriptions about the firm and brand into the generation
of slogans. We evaluate generated slogans based on ROUGE1, ROUGEL and Cosine
Similarity metrics and also assess them with human subjects in terms of
slogan&apos;s distinctiveness, coherence, and fluency. The results demonstrate that
our approach yields better performance than baseline models and other
transformer-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jongeun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;MinChung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehwan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04610">
<title>DeepSpeed4Science Initiative: Enabling Large-Scale Scientific Discovery through Sophisticated AI System Technologies. (arXiv:2310.04610v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04610</link>
<description rdf:parseType="Literal">&lt;p&gt;In the upcoming decade, deep learning may revolutionize the natural sciences,
enhancing our capacity to model and predict natural occurrences. This could
herald a new era of scientific exploration, bringing significant advancements
across sectors from drug development to renewable energy. To answer this call,
we present DeepSpeed4Science initiative (deepspeed4science.ai) which aims to
build unique capabilities through AI system technology innovations to help
domain experts to unlock today&apos;s biggest science mysteries. By leveraging
DeepSpeed&apos;s current technology pillars (training, inference and compression) as
base technology enablers, DeepSpeed4Science will create a new set of AI system
technologies tailored for accelerating scientific discoveries by addressing
their unique complexity beyond the common technical approaches used for
accelerating generic large language models (LLMs). In this paper, we showcase
the early progress we made with DeepSpeed4Science in addressing two of the
critical system challenges in structural biology research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuaiwen Leon Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruft_B/0/1/0/all/0/1&quot;&gt;Bonnie Kruft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Conglong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1&quot;&gt;Masahiro Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoxia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasley_J/0/1/0/all/0/1&quot;&gt;Jeff Rasley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awan_A/0/1/0/all/0/1&quot;&gt;Ammar Ahmad Awan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holmes_C/0/1/0/all/0/1&quot;&gt;Connor Holmes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_M/0/1/0/all/0/1&quot;&gt;Martin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghanem_A/0/1/0/all/0/1&quot;&gt;Adam Ghanem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhongzhu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yuxiong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luferenko_P/0/1/0/all/0/1&quot;&gt;Pete Luferenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_D/0/1/0/all/0/1&quot;&gt;Divya Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyn_J/0/1/0/all/0/1&quot;&gt;Jonathan Weyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruixiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klocek_S/0/1/0/all/0/1&quot;&gt;Sylwester Klocek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vragov_V/0/1/0/all/0/1&quot;&gt;Volodymyr Vragov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlQuraishi_M/0/1/0/all/0/1&quot;&gt;Mohammed AlQuraishi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahdritz_G/0/1/0/all/0/1&quot;&gt;Gustaf Ahdritz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Floristean_C/0/1/0/all/0/1&quot;&gt;Christina Floristean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negri_C/0/1/0/all/0/1&quot;&gt;Cristina Negri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotamarthi_R/0/1/0/all/0/1&quot;&gt;Rao Kotamarthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwanath_V/0/1/0/all/0/1&quot;&gt;Venkatram Vishwanath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanathan_A/0/1/0/all/0/1&quot;&gt;Arvind Ramanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foreman_S/0/1/0/all/0/1&quot;&gt;Sam Foreman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hippe_K/0/1/0/all/0/1&quot;&gt;Kyle Hippe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcomano_T/0/1/0/all/0/1&quot;&gt;Troy Arcomano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maulik_R/0/1/0/all/0/1&quot;&gt;Romit Maulik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zvyagin_M/0/1/0/all/0/1&quot;&gt;Maxim Zvyagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brace_A/0/1/0/all/0/1&quot;&gt;Alexander Brace&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohorquez_C/0/1/0/all/0/1&quot;&gt;Cindy Orozco Bohorquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clyde_A/0/1/0/all/0/1&quot;&gt;Austin Clyde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_B/0/1/0/all/0/1&quot;&gt;Bharat Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Rivera_D/0/1/0/all/0/1&quot;&gt;Danilo Perez-Rivera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Heng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mann_C/0/1/0/all/0/1&quot;&gt;Carla M. Mann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irvin_M/0/1/0/all/0/1&quot;&gt;Michael Irvin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pauloski_J/0/1/0/all/0/1&quot;&gt;J. Gregory Pauloski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ward_L/0/1/0/all/0/1&quot;&gt;Logan Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayot_V/0/1/0/all/0/1&quot;&gt;Valerie Hayot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emani_M/0/1/0/all/0/1&quot;&gt;Murali Emani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhen Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Diangen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukla_M/0/1/0/all/0/1&quot;&gt;Maulik Shukla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_I/0/1/0/all/0/1&quot;&gt;Ian Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;James J. Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papka_M/0/1/0/all/0/1&quot;&gt;Michael E. Papka&lt;/a&gt;, et al. (40 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04755">
<title>Pairwise GUI Dataset Construction Between Android Phones and Tablets. (arXiv:2310.04755v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04755</link>
<description rdf:parseType="Literal">&lt;p&gt;In the current landscape of pervasive smartphones and tablets, apps
frequently exist across both platforms. Although apps share most graphic user
interfaces (GUIs) and functionalities across phones and tablets, developers
often rebuild from scratch for tablet versions, escalating costs and
squandering existing design resources. Researchers are attempting to collect
data and employ deep learning in automated GUIs development to enhance
developers&apos; productivity. There are currently several publicly accessible GUI
page datasets for phones, but none for pairwise GUIs between phones and
tablets. This poses a significant barrier to the employment of deep learning in
automated GUI development. In this paper, we introduce the Papt dataset, a
pioneering pairwise GUI dataset tailored for Android phones and tablets,
encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app
pairs. We propose novel pairwise GUI collection approaches for constructing
this dataset and delineate its advantages over currently prevailing datasets in
the field. Through preliminary experiments on this dataset, we analyze the
present challenges of utilizing deep learning in automated GUI development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Haolan Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yujin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Di Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04874">
<title>AirIMU: Learning Uncertainty Propagation for Inertial Odometry. (arXiv:2310.04874v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04874</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate uncertainty estimation for inertial odometry is the foundation to
achieve optimal fusion in multi-sensor systems, such as visual or LiDAR
inertial odometry. Prior studies often simplify the assumptions regarding the
uncertainty of inertial measurements, presuming fixed covariance parameters and
empirical IMU sensor models. However, the inherent physical limitations and
non-linear characteristics of sensors are difficult to capture. Moreover,
uncertainty may fluctuate based on sensor rates and motion modalities, leading
to variations across different IMUs. To address these challenges, we formulate
a learning-based method that not only encapsulate the non-linearities inherent
to IMUs but also ensure the accurate propagation of covariance in a data-driven
manner. We extend the PyPose library to enable differentiable batched IMU
integration with covariance propagation on manifolds, leading to significant
runtime speedup. To demonstrate our method&apos;s adaptability, we evaluate it on
several benchmarks as well as a large-scale helicopter dataset spanning over
262 kilometers. The drift rate of the inertial odometry on these datasets is
reduced by a factor of between 2.2 and 4 times. Our method lays the groundwork
for advanced developments in inertial odometry.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yuheng Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xunfei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Youjie Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04986">
<title>A new economic and financial theory of money. (arXiv:2310.04986v2 [econ.TH] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04986</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper fundamentally reformulates economic and financial theory to
include electronic currencies. The valuation of the electronic currencies will
be based on macroeconomic theory and the fundamental equation of monetary
policy, not the microeconomic theory of discounted cash flows. The view of
electronic currency as a transactional equity associated with tangible assets
of a sub-economy will be developed, in contrast to the view of stock as an
equity associated mostly with intangible assets of a sub-economy. The view will
be developed of the electronic currency management firm as an entity
responsible for coordinated monetary (electronic currency supply and value
stabilization) and fiscal (investment and operational) policies of a
substantial (for liquidity of the electronic currency) sub-economy. The risk
model used in the valuations and the decision-making will not be the
ubiquitous, yet inappropriate, exponential risk model that leads to discount
rates, but will be multi time scale models that capture the true risk. The
decision-making will be approached from the perspective of true systems control
based on a system response function given by the multi scale risk model and
system controllers that utilize the Deep Reinforcement Learning, Generative
Pretrained Transformers, and other methods of Artificial Intelligence
(DRL/GPT/AI). Finally, the sub-economy will be viewed as a nonlinear complex
physical system with both stable equilibriums that are associated with
short-term exploitation, and unstable equilibriums that need to be stabilized
with active nonlinear control based on the multi scale system response
functions and DRL/GPT/AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Glinsky_M/0/1/0/all/0/1&quot;&gt;Michael E. Glinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/econ/1/au:+Sievert_S/0/1/0/all/0/1&quot;&gt;Sharon Sievert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05680">
<title>Automated Argument Generation from Legal Facts. (arXiv:2310.05680v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05680</link>
<description rdf:parseType="Literal">&lt;p&gt;The count of pending cases has shown an exponential rise across nations
(e.g., with more than 10 million pending cases in India alone). The main issue
lies in the fact that the number of cases submitted to the law system is far
greater than the available number of legal professionals present in a country.
Given this worldwide context, the utilization of AI technology has gained
paramount importance to enhance the efficiency and speed of legal procedures.
In this study we partcularly focus on helping legal professionals in the
process of analyzing a legal case. Our specific investigation delves into
harnessing the generative capabilities of open-sourced large language models to
create arguments derived from the facts present in legal cases. Experimental
results show that the generated arguments from the best performing method have
on average 63% overlap with the benchmark set gold standard annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tuvey_O/0/1/0/all/0/1&quot;&gt;Oscar Tuvey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_P/0/1/0/all/0/1&quot;&gt;Procheta Sen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05898">
<title>Lion Secretly Solves Constrained Optimization: As Lyapunov Predicts. (arXiv:2310.05898v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05898</link>
<description rdf:parseType="Literal">&lt;p&gt;Lion (Evolved Sign Momentum), a new optimizer discovered through program
search, has shown promising results in training large AI models. It performs
comparably or favorably to AdamW but with greater memory efficiency. As we can
expect from the results of a random search program, Lion incorporates elements
from several existing algorithms, including signed momentum, decoupled weight
decay, Polak, and Nesterov momentum, but does not fit into any existing
category of theoretically grounded optimizers. Thus, even though Lion appears
to perform well as a general-purpose optimizer for a wide range of tasks, its
theoretical basis remains uncertain. This lack of theoretical clarity limits
opportunities to further enhance and expand Lion&apos;s efficacy.
&lt;/p&gt;
&lt;p&gt;This work aims to demystify Lion. Based on both continuous-time and
discrete-time analysis, we demonstrate that Lion is a theoretically novel and
principled approach for minimizing a general loss function $f(x)$ while
enforcing a bound constraint $\|x\|_\infty \leq 1/\lambda$. Lion achieves this
through the incorporation of decoupled weight decay, where $\lambda$ represents
the weight decay coefficient. Our analysis is made possible by the development
of a new Lyapunov function for the Lion updates. It applies to a broader family
of Lion-$\kappa$ algorithms, where the $\text{sign}(\cdot)$ operator in Lion is
replaced by the subgradient of a convex function $\kappa$, leading to the
solution of a general composite optimization problem of $\min_x f(x) +
\kappa^*(x)$. Our findings provide valuable insights into the dynamics of Lion
and pave the way for further improvements and extensions of Lion-related
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lizhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kaizhao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qiang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05969">
<title>Automated Chest X-Ray Report Generator Using Multi-Model Deep Learning Approach. (arXiv:2310.05969v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05969</link>
<description rdf:parseType="Literal">&lt;p&gt;Reading and interpreting chest X-ray images is one of the most radiologist&apos;s
routines. However, it still can be challenging, even for the most experienced
ones. Therefore, we proposed a multi-model deep learning-based automated chest
X-ray report generator system designed to assist radiologists in their work.
The basic idea of the proposed system is by utilizing multi
binary-classification models for detecting multi abnormalities, with each model
responsible for detecting one abnormality, in a single image. In this study, we
limited the radiology abnormalities detection to only cardiomegaly, lung
effusion, and consolidation. The system generates a radiology report by
performing the following three steps: image pre-processing, utilizing deep
learning models to detect abnormalities, and producing a report. The aim of the
image pre-processing step is to standardize the input by scaling it to 128x128
pixels and slicing it into three segments, which covers the upper, lower, and
middle parts of the lung. After pre-processing, each corresponding model
classifies the image, resulting in a 0 (zero) for no abnormality detected and a
1 (one) for the presence of an abnormality. The prediction outputs of each
model are then concatenated to form a &apos;result code&apos;. The &apos;result code&apos; is used
to construct a report by selecting the appropriate pre-determined sentence for
each detected abnormality in the report generation step. The proposed system is
expected to reduce the workload of radiologists and increase the accuracy of
chest X-ray diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muharram_A/0/1/0/all/0/1&quot;&gt;Arief Purnama Muharram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haryono_H/0/1/0/all/0/1&quot;&gt;Hollyana Puteri Haryono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Juma_A/0/1/0/all/0/1&quot;&gt;Abassi Haji Juma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Puspasari_I/0/1/0/all/0/1&quot;&gt;Ira Puspasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Utama_N/0/1/0/all/0/1&quot;&gt;Nugraha Priya Utama&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06225">
<title>GPT-4 as an Agronomist Assistant? Answering Agriculture Exams Using Large Language Models. (arXiv:2310.06225v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06225</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated remarkable capabilities in
natural language understanding across various domains, including healthcare and
finance. For some tasks, LLMs achieve similar or better performance than
trained human beings, therefore it is reasonable to employ human exams (e.g.,
certification tests) to assess the performance of LLMs. We present a
comprehensive evaluation of popular LLMs, such as Llama 2 and GPT, on their
ability to answer agriculture-related questions. In our evaluation, we also
employ RAG (Retrieval-Augmented Generation) and ER (Ensemble Refinement)
techniques, which combine information retrieval, generation capabilities, and
prompting strategies to improve the LLMs&apos; performance. To demonstrate the
capabilities of LLMs, we selected agriculture exams and benchmark datasets from
three of the largest agriculture producer countries: Brazil, India, and the
USA. Our analysis highlights GPT-4&apos;s ability to achieve a passing score on
exams to earn credits for renewing agronomist certifications, answering 93% of
the questions correctly and outperforming earlier general-purpose models, which
achieved 88% accuracy. On one of our experiments, GPT-4 obtained the highest
performance when compared to human subjects. This performance suggests that
GPT-4 could potentially pass on major graduate education admission tests or
even earn credits for renewing agronomy certificates. We also explore the
models&apos; capacity to address general agriculture-related questions and generate
crop management guidelines for Brazilian and Indian farmers, utilizing robust
datasets from the Brazilian Agency of Agriculture (Embrapa) and graduate
program exams from India. The results suggest that GPT-4, ER, and RAG can
contribute meaningfully to agricultural education, assessment, and crop
management practice, offering valuable insights to farmers and agricultural
professionals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Bruno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_L/0/1/0/all/0/1&quot;&gt;Leonardo Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Estevao_R/0/1/0/all/0/1&quot;&gt;Roberto Estev&amp;#xe3;o&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aski_V/0/1/0/all/0/1&quot;&gt;Vijay Aski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1&quot;&gt;Ranveer Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06763">
<title>FABind: Fast and Accurate Protein-Ligand Binding. (arXiv:2310.06763v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06763</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling the interaction between proteins and ligands and accurately
predicting their binding structures is a critical yet challenging task in drug
discovery. Recent advancements in deep learning have shown promise in
addressing this challenge, with sampling-based and regression-based methods
emerging as two prominent approaches. However, these methods have notable
limitations. Sampling-based methods often suffer from low efficiency due to the
need for generating multiple candidate structures for selection. On the other
hand, regression-based methods offer fast predictions but may experience
decreased accuracy. Additionally, the variation in protein sizes often requires
external modules for selecting suitable binding pockets, further impacting
efficiency. In this work, we propose $\mathbf{FABind}$, an end-to-end model
that combines pocket prediction and docking to achieve accurate and fast
protein-ligand binding. $\mathbf{FABind}$ incorporates a unique ligand-informed
pocket prediction module, which is also leveraged for docking pose estimation.
The model further enhances the docking process by incrementally integrating the
predicted pocket to optimize protein-ligand binding, reducing discrepancies
between training and inference. Through extensive experiments on benchmark
datasets, our proposed $\mathbf{FABind}$ demonstrates strong advantages in
terms of effectiveness and efficiency compared to existing methods. Our code is
available at $\href{https://github.com/QizhiPei/FABind}{Github}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_Q/0/1/0/all/0/1&quot;&gt;Qizhi Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_K/0/1/0/all/0/1&quot;&gt;Kaiyuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lijun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jinhua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_Y/0/1/0/all/0/1&quot;&gt;Yingce Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shufang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tao Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_K/0/1/0/all/0/1&quot;&gt;Kun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tie-Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_R/0/1/0/all/0/1&quot;&gt;Rui Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06823">
<title>NECO: NEural Collapse Based Out-of-distribution detection. (arXiv:2310.06823v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06823</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting out-of-distribution (OOD) data is a critical challenge in machine
learning due to model overconfidence, often without awareness of their
epistemological limits. We hypothesize that ``neural collapse&apos;&apos;, a phenomenon
affecting in-distribution data for models trained beyond loss convergence, also
influences OOD data. To benefit from this interplay, we introduce NECO, a novel
post-hoc method for OOD detection, which leverages the geometric properties of
``neural collapse&apos;&apos; and of principal component spaces to identify OOD data. Our
extensive experiments demonstrate that NECO achieves state-of-the-art results
on both small and large-scale OOD detection tasks while exhibiting strong
generalization capabilities across different network architectures.
Furthermore, we provide a theoretical explanation for the effectiveness of our
method in OOD detection. We plan to release the code after the anonymity
period.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ammar_M/0/1/0/all/0/1&quot;&gt;Mou&amp;#xef;n Ben Ammar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belkhir_N/0/1/0/all/0/1&quot;&gt;Nacim Belkhir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Popescu_S/0/1/0/all/0/1&quot;&gt;Sebastian Popescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Manzanera_A/0/1/0/all/0/1&quot;&gt;Antoine Manzanera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Franchi_G/0/1/0/all/0/1&quot;&gt;Gianni Franchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07282">
<title>An Analysis on Large Language Models in Healthcare: A Case Study of BioBERT. (arXiv:2310.07282v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07282</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper conducts a comprehensive investigation into applying large
language models, particularly on BioBERT, in healthcare. It begins with
thoroughly examining previous natural language processing (NLP) approaches in
healthcare, shedding light on the limitations and challenges these methods
face. Following that, this research explores the path that led to the
incorporation of BioBERT into healthcare applications, highlighting its
suitability for addressing the specific requirements of tasks related to
biomedical text mining. The analysis outlines a systematic methodology for
fine-tuning BioBERT to meet the unique needs of the healthcare domain. This
approach includes various components, including the gathering of data from a
wide range of healthcare sources, data annotation for tasks like identifying
medical entities and categorizing them, and the application of specialized
preprocessing techniques tailored to handle the complexities found in
biomedical texts. Additionally, the paper covers aspects related to model
evaluation, with a focus on healthcare benchmarks and functions like processing
of natural language in biomedical, question-answering, clinical document
classification, and medical entity recognition. It explores techniques to
improve the model&apos;s interpretability and validates its performance compared to
existing healthcare-focused language models. The paper thoroughly examines
ethical considerations, particularly patient privacy and data security. It
highlights the benefits of incorporating BioBERT into healthcare contexts,
including enhanced clinical decision support and more efficient information
retrieval. Nevertheless, it acknowledges the impediments and complexities of
this integration, encompassing concerns regarding data privacy, transparency,
resource-intensive requirements, and the necessity for model customization to
align with diverse healthcare domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharaf_S/0/1/0/all/0/1&quot;&gt;Shyni Sharaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anoop_V/0/1/0/all/0/1&quot;&gt;V. S. Anoop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07312">
<title>WiGenAI: The Symphony of Wireless and Generative AI via Diffusion Models. (arXiv:2310.07312v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07312</link>
<description rdf:parseType="Literal">&lt;p&gt;Innovative foundation models, such as GPT-3 and stable diffusion models, have
made a paradigm shift in the realm of artificial intelligence (AI) towards
generative AI-based systems. In unison, from data communication and networking
perspective, AI and machine learning (AI/ML) algorithms are envisioned to be
pervasively incorporated into the future generations of wireless communications
systems, highlighting the need for novel AI-native solutions for the emergent
communication scenarios. In this article, we outline the applications of
generative AI in wireless communication systems to lay the foundations for
research in this field. Diffusion-based generative models, as the new
state-of-the-art paradigm of generative models, are introduced, and their
applications in wireless communication systems are discussed. Two case studies
are also presented to showcase how diffusion models can be exploited for the
development of resilient AI-native communication systems. Specifically, we
propose denoising diffusion probabilistic models (DDPM) for a wireless
communication scheme with non-ideal transceivers, where 30% improvement is
achieved in terms of bit error rate. As the second application, DDPMs are
employed at the transmitter to shape the constellation symbols, highlighting a
robust out-of-distribution performance. Finally, future directions and open
issues for the development of generative AI-based wireless systems are
discussed to promote future research endeavors towards wireless generative AI
(WiGenAI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letafati_M/0/1/0/all/0/1&quot;&gt;Mehdi Letafati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_S/0/1/0/all/0/1&quot;&gt;Samad Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latva_aho_M/0/1/0/all/0/1&quot;&gt;Matti Latva-aho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07402">
<title>NuTime: Numerically Multi-Scaled Embedding for Large-Scale Time Series Pretraining. (arXiv:2310.07402v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07402</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent research on time-series self-supervised models shows great promise in
learning semantic representations. However, it has been limited to small-scale
datasets, e.g., thousands of temporal sequences. In this work, we make key
technical contributions that are tailored to the numerical properties of
time-series data and allow the model to scale to large datasets, e.g., millions
of temporal sequences. We adopt the Transformer architecture by first
partitioning the input into non-overlapping windows. Each window is then
characterized by its normalized shape and two scalar values denoting the mean
and standard deviation within each window. To embed scalar values that may
possess arbitrary numerical scales to high-dimensional vectors, we propose a
numerically multi-scaled embedding module enumerating all possible scales for
the scalar values. The model undergoes pretraining using the proposed
numerically multi-scaled embedding with a simple contrastive objective on a
large-scale dataset containing over a million sequences. We study its transfer
performance on a number of univariate and multivariate classification
benchmarks. Our method exhibits remarkable improvement against previous
representation learning approaches and establishes the new state of the art,
even compared with domain-specific non-learning-based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenguo Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xumeng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_W/0/1/0/all/0/1&quot;&gt;Wei Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Congrui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_J/0/1/0/all/0/1&quot;&gt;Jiang Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Stephen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhirong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07433">
<title>Imitation Learning from Observation with Automatic Discount Scheduling. (arXiv:2310.07433v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07433</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans often acquire new skills through observation and imitation. For
robotic agents, learning from the plethora of unlabeled video demonstration
data available on the Internet necessitates imitating the expert without access
to its action, presenting a challenge known as Imitation Learning from
Observations (ILfO). A common approach to tackle ILfO problems is to convert
them into inverse reinforcement learning problems, utilizing a proxy reward
computed from the agent&apos;s and the expert&apos;s observations. Nonetheless, we
identify that tasks characterized by a progress dependency property pose
significant challenges for such approaches; in these tasks, the agent needs to
initially learn the expert&apos;s preceding behaviors before mastering the
subsequent ones. Our investigation reveals that the main cause is that the
reward signals assigned to later steps hinder the learning of initial
behaviors. To address this challenge, we present a novel ILfO framework that
enables the agent to master earlier behaviors before advancing to later ones.
We introduce an Automatic Discount Scheduling (ADS) mechanism that adaptively
alters the discount factor in reinforcement learning during the training phase,
prioritizing earlier rewards initially and gradually engaging later rewards
only when the earlier behaviors have been mastered. Our experiments, conducted
on nine Meta-World tasks, demonstrate that our method significantly outperforms
state-of-the-art methods across all tasks, including those that are unsolvable
by them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_W/0/1/0/all/0/1&quot;&gt;Weijun Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yingdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Chuan Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Zhao-Heng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongjie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07478">
<title>Multimodal Graph Learning for Generative Tasks. (arXiv:2310.07478v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07478</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning combines multiple data modalities, broadening the types
and complexity of data our models can utilize: for example, from plain text to
image-caption pairs. Most multimodal learning algorithms focus on modeling
simple one-to-one pairs of data from two modalities, such as image-caption
pairs, or audio-text pairs. However, in most real-world settings, entities of
different modalities interact with each other in more complex and multifaceted
ways, going beyond one-to-one mappings. We propose to represent these complex
relationships as graphs, allowing us to capture data with any number of
modalities, and with complex relationships between modalities that can flexibly
vary from one sample to another. Toward this goal, we propose Multimodal Graph
Learning (MMGL), a general and systematic framework for capturing information
from multiple multimodal neighbors with relational structures among them. In
particular, we focus on MMGL for generative tasks, building upon pretrained
Language Models (LMs), aiming to augment their text generation with multimodal
neighbor contexts. We study three research questions raised by MMGL: (1) how
can we infuse multiple neighbor information into the pretrained LMs, while
avoiding scalability issues? (2) how can we infuse the graph structure
information among multimodal neighbors into the LMs? and (3) how can we
finetune the pretrained LMs to learn from the neighbor context in a
parameter-efficient manner? We conduct extensive experiments to answer these
three questions on MMGL and analyze the empirical results to pave the way for
future MMGL research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_M/0/1/0/all/0/1&quot;&gt;Minji Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_J/0/1/0/all/0/1&quot;&gt;Jing Yu Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hooi_B/0/1/0/all/0/1&quot;&gt;Bryan Hooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07579">
<title>In-Context Unlearning: Language Models as Few Shot Unlearners. (arXiv:2310.07579v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07579</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine unlearning, the study of efficiently removing the impact of specific
training points on the trained model, has garnered increased attention of late,
driven by the need to comply with privacy regulations like the Right to be
Forgotten. Although unlearning is particularly relevant for LLMs in light of
the copyright issues they raise, achieving precise unlearning is
computationally infeasible for very large models. To this end, recent work has
proposed several algorithms which approximate the removal of training data
without retraining the model. These algorithms crucially rely on access to the
model parameters in order to update them, an assumption that may not hold in
practice due to computational constraints or when the LLM is accessed via API.
In this work, we propose a new class of unlearning methods for LLMs we call
&apos;&apos;In-Context Unlearning&apos;&apos;, providing inputs in context and without having to
update model parameters. To unlearn a particular training instance, we provide
the instance alongside a flipped label and additional correctly labelled
instances which are prepended as inputs to the LLM at inference time. Our
experimental results demonstrate that these contexts effectively remove
specific information from the training set while maintaining performance levels
that are competitive with (or in some cases exceed) state-of-the-art unlearning
methods that require access to the LLM parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pawelczyk_M/0/1/0/all/0/1&quot;&gt;Martin Pawelczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neel_S/0/1/0/all/0/1&quot;&gt;Seth Neel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_H/0/1/0/all/0/1&quot;&gt;Himabindu Lakkaraju&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07637">
<title>OpsEval: A Comprehensive Task-Oriented AIOps Benchmark for Large Language Models. (arXiv:2310.07637v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07637</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have exhibited remarkable capabilities in
NLP-related tasks such as translation, summarizing, and generation. The
application of LLMs in specific areas, notably AIOps (Artificial Intelligence
for IT Operations), holds great potential due to their advanced abilities in
information summarizing, report analyzing, and ability of API calling.
Nevertheless, the performance of current LLMs in AIOps tasks is yet to be
determined. Furthermore, a comprehensive benchmark is required to steer the
optimization of LLMs tailored for AIOps. Compared with existing benchmarks that
focus on evaluating specific fields like network configuration, in this paper,
we present \textbf{OpsEval}, a comprehensive task-oriented AIOps benchmark
designed for LLMs. For the first time, OpsEval assesses LLMs&apos; proficiency in
three crucial scenarios (Wired Network Operation, 5G Communication Operation,
and Database Operation) at various ability levels (knowledge recall, analytical
thinking, and practical application). The benchmark includes 7,200 questions in
both multiple-choice and question-answer (QA) formats, available in English and
Chinese. With quantitative and qualitative results, we show how various LLM
tricks can affect the performance of AIOps, including zero-shot,
chain-of-thought, and few-shot in-context learning. We find that GPT4-score is
more consistent with experts than widely used Bleu and Rouge, which can be used
to replace automatic metrics for large-scale qualitative evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuhe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_C/0/1/0/all/0/1&quot;&gt;Changhua Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Longlong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bohan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Mingze Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yongqian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shenglin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianhui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Gaogang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_X/0/1/0/all/0/1&quot;&gt;Xidao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_D/0/1/0/all/0/1&quot;&gt;Dan Pei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07644">
<title>Rethinking the BERT-like Pretraining for DNA Sequences. (arXiv:2310.07644v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07644</link>
<description rdf:parseType="Literal">&lt;p&gt;With the success of large-scale pretraining in NLP, there is an increasing
trend of applying it to the domain of life sciences. In particular, pretraining
methods based on DNA sequences have garnered growing attention due to their
potential to capture generic information about genes. However, existing
pretraining methods for DNA sequences largely rely on direct adoptions of BERT
pretraining from NLP, lacking a comprehensive understanding and a specifically
tailored approach. To address this research gap, we first conducted a series of
exploratory experiments and gained several insightful observations: 1) In the
fine-tuning phase of downstream tasks, when using K-mer overlapping
tokenization instead of K-mer non-overlapping tokenization, both overlapping
and non-overlapping pretraining weights show consistent performance
improvement.2) During the pre-training process, using K-mer overlapping
tokenization quickly produces clear K-mer embeddings and reduces the loss to a
very low level, while using K-mer non-overlapping tokenization results in less
distinct embeddings and continuously decreases the loss. 3) Using overlapping
tokenization causes the self-attention in the intermediate layers of
pre-trained models to tend to overly focus on certain tokens, reflecting that
these layers are not adequately optimized. In summary, overlapping tokenization
can benefit the fine-tuning of downstream tasks but leads to inadequate
pretraining with fast convergence. To unleash the pretraining potential, we
introduce a novel approach called RandomMask, which gradually increases the
task difficulty of BERT-like pretraining by continuously expanding its mask
boundary, forcing the model to learn more knowledge. RandomMask is simple but
effective, achieving top-tier performance across 26 datasets of 28 datasets
spanning 7 downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chaoqi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Weiqiang Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Lifeng Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuchen Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jianle Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_P/0/1/0/all/0/1&quot;&gt;Peng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hongliang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinzhu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wanli Ouyang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13869">
<title>PRiSM: Enhancing Low-Resource Document-Level Relation Extraction with Relation-Aware Score Calibration. (arXiv:2309.13869v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2309.13869</link>
<description rdf:parseType="Literal">&lt;p&gt;Document-level relation extraction (DocRE) aims to extract relations of all
entity pairs in a document. A key challenge in DocRE is the cost of annotating
such data which requires intensive human effort. Thus, we investigate the case
of DocRE in a low-resource setting, and we find that existing models trained on
low data overestimate the NA (&quot;no relation&quot;) label, causing limited
performance. In this work, we approach the problem from a calibration
perspective and propose PRiSM, which learns to adapt logits based on relation
semantic information. We evaluate our method on three DocRE datasets and
demonstrate that integrating existing models with PRiSM improves performance by
as much as 26.38 F1 score, while the calibration error drops as much as 36
times when trained with about 3% of data. The code is publicly available at
https://github.com/brightjade/PRiSM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minseok Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hyesu Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>