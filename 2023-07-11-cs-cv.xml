<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03871" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03872" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03918" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04054" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04087" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04106" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04113" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04149" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04223" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04296" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1908.08016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.12556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.13136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.11089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.00627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.01996" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.12306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02370" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.14645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.01735" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.07354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07308" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.04743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13399" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00612" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06388" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01465" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03073" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.03757">
<title>A Fully Automated and Explainable Algorithm for the Prediction of Malignant Transformation in Oral Epithelial Dysplasia. (arXiv:2307.03757v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2307.03757</link>
<description rdf:parseType="Literal">&lt;p&gt;Oral epithelial dysplasia (OED) is a premalignant histopathological diagnosis
given to lesions of the oral cavity. Its grading suffers from significant
inter-/intra- observer variability, and does not reliably predict malignancy
progression, potentially leading to suboptimal treatment decisions. To address
this, we developed a novel artificial intelligence algorithm that can assign an
Oral Malignant Transformation (OMT) risk score, based on histological patterns
in the in Haematoxylin and Eosin stained whole slide images, to quantify the
risk of OED progression. The algorithm is based on the detection and
segmentation of nuclei within (and around) the epithelium using an in-house
segmentation model. We then employed a shallow neural network fed with
interpretable morphological/spatial features, emulating histological markers.
We conducted internal cross-validation on our development cohort (Sheffield; n
= 193 cases) followed by independent validation on two external cohorts
(Birmingham and Belfast; n = 92 cases). The proposed OMTscore yields an AUROC =
0.74 in predicting whether an OED progresses to malignancy or not. Survival
analyses showed the prognostic value of our OMTscore for predicting malignancy
transformation, when compared to the manually-assigned WHO and binary grades.
Analysis of the correctly predicted cases elucidated the presence of
peri-epithelial and epithelium-infiltrating lymphocytes in the most predictive
patches of cases that transformed (p &amp;lt; 0.0001). This is the first study to
propose a completely automated algorithm for predicting OED transformation
based on interpretable nuclear features, whilst being validated on external
datasets. The algorithm shows better-than-human-level performance for
prediction of OED malignant transformation and offers a promising solution to
the challenges of grading OED in routine clinical practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Shephard_A/0/1/0/all/0/1&quot;&gt;Adam J Shephard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bashir_R/0/1/0/all/0/1&quot;&gt;Raja Muhammad Saad Bashir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mahmood_H/0/1/0/all/0/1&quot;&gt;Hanya Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Jahanifar_M/0/1/0/all/0/1&quot;&gt;Mostafa Jahanifar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Minhas_F/0/1/0/all/0/1&quot;&gt;Fayyaz Minhas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Raza_S/0/1/0/all/0/1&quot;&gt;Shan E Ahmed Raza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+McCombe_K/0/1/0/all/0/1&quot;&gt;Kris D McCombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Craig_S/0/1/0/all/0/1&quot;&gt;Stephanie G Craig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+James_J/0/1/0/all/0/1&quot;&gt;Jacqueline James&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Brooks_J/0/1/0/all/0/1&quot;&gt;Jill Brooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nankivell_P/0/1/0/all/0/1&quot;&gt;Paul Nankivell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mehanna_H/0/1/0/all/0/1&quot;&gt;Hisham Mehanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Khurram_S/0/1/0/all/0/1&quot;&gt;Syed Ali Khurram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Rajpoot_N/0/1/0/all/0/1&quot;&gt;Nasir M Rajpoot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03777">
<title>Unsupervised 3D out-of-distribution detection with latent diffusion models. (arXiv:2307.03777v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03777</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods for out-of-distribution (OOD) detection that scale to 3D data are
crucial components of any real-world clinical deep learning system. Classic
denoising diffusion probabilistic models (DDPMs) have been recently proposed as
a robust way to perform reconstruction-based OOD detection on 2D datasets, but
do not trivially scale to 3D data. In this work, we propose to use Latent
Diffusion Models (LDMs), which enable the scaling of DDPMs to high-resolution
3D medical data. We validate the proposed approach on near- and far-OOD
datasets and compare it to a recently proposed, 3D-enabled approach using
Latent Transformer Models (LTMs). Not only does the proposed LDM-based approach
achieve statistically significant better performance, it also shows less
sensitivity to the underlying latent representation, more favourable memory
scaling, and produces better spatial anomaly maps. Code is available at
https://github.com/marksgraham/ddpm-ood
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graham_M/0/1/0/all/0/1&quot;&gt;Mark S. Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinaya_W/0/1/0/all/0/1&quot;&gt;Walter Hugo Lopez Pinaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_P/0/1/0/all/0/1&quot;&gt;Paul Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tudosiu_P/0/1/0/all/0/1&quot;&gt;Petru-Daniel Tudosiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mah_Y/0/1/0/all/0/1&quot;&gt;Yee H. Mah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teo_J/0/1/0/all/0/1&quot;&gt;James T. Teo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jager_H/0/1/0/all/0/1&quot;&gt;H. Rolf J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werring_D/0/1/0/all/0/1&quot;&gt;David Werring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1&quot;&gt;Parashkev Nachev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_M/0/1/0/all/0/1&quot;&gt;M. Jorge Cardoso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03786">
<title>Context-aware Pedestrian Trajectory Prediction with Multimodal Transformer. (arXiv:2307.03786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03786</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel solution for predicting future trajectories of
pedestrians. Our method uses a multimodal encoder-decoder transformer
architecture, which takes as input both pedestrian locations and ego-vehicle
speeds. Notably, our decoder predicts the entire future trajectory in a
single-pass and does not perform one-step-ahead prediction, which makes the
method effective for embedded edge deployment. We perform detailed experiments
and evaluate our method on two popular datasets, PIE and JAAD. Quantitative
results demonstrate the superiority of our proposed model over the current
state-of-the-art, which consistently achieves the lowest error for 3 time
horizons of 0.5, 1.0 and 1.5 seconds. Moreover, the proposed method is
significantly faster than the state-of-the-art for the two datasets of PIE and
JAAD. Lastly, ablation experiments demonstrate the impact of the key multimodal
configuration of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damirchi_H/0/1/0/all/0/1&quot;&gt;Haleh Damirchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenspan_M/0/1/0/all/0/1&quot;&gt;Michael Greenspan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etemad_A/0/1/0/all/0/1&quot;&gt;Ali Etemad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03789">
<title>Synthesizing Forestry Images Conditioned on Plant Phenotype Using a Generative Adversarial Network. (arXiv:2307.03789v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03789</link>
<description rdf:parseType="Literal">&lt;p&gt;Plant phenology and phenotype prediction using remote sensing data is
increasingly gaining the attention of the plant science community to improve
agricultural productivity. In this work, we generate synthetic forestry images
that satisfy certain phenotypic attributes, viz. canopy greenness. The
greenness index of plants describes a particular vegetation type in a mixed
forest. Our objective is to develop a Generative Adversarial Network (GAN) to
synthesize forestry images conditioned on this continuous attribute, i.e.,
greenness of vegetation, over a specific region of interest. The training data
is based on the automated digital camera imagery provided by the National
Ecological Observatory Network (NEON) and processed by the PhenoCam Network.
The synthetic images generated by our method are also used to predict another
phenotypic attribute, viz., redness of plants. The Structural SIMilarity (SSIM)
index is utilized to assess the quality of the synthetic images. The greenness
and redness indices of the generated synthetic images are compared against that
of the original images using Root Mean Squared Error (RMSE) in order to
evaluate their accuracy and integrity. Moreover, the generalizability and
scalability of our proposed GAN model is determined by effectively transforming
it to generate synthetic images for other forest sites and vegetation types.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_D/0/1/0/all/0/1&quot;&gt;Debasmita Pal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ross_A/0/1/0/all/0/1&quot;&gt;Arun Ross&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03798">
<title>CLIPMasterPrints: Fooling Contrastive Language-Image Pre-training Using Latent Variable Evolution. (arXiv:2307.03798v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03798</link>
<description rdf:parseType="Literal">&lt;p&gt;Models leveraging both visual and textual data such as Contrastive
Language-Image Pre-training (CLIP), are increasingly gaining importance. In
this work, we show that despite their versatility, such models are vulnerable
to what we refer to as fooling master images. Fooling master images are capable
of maximizing the confidence score of a CLIP model for a significant number of
widely varying prompts, while being unrecognizable for humans. We demonstrate
how fooling master images can be mined by searching the latent space of
generative models by means of an evolution strategy or stochastic gradient
descent. We investigate the properties of the mined fooling master images, and
find that images trained on a small number of image captions potentially
generalize to a much larger number of semantically related captions. Further,
we evaluate two possible mitigation strategies and find that vulnerability to
fooling master examples is closely related to a modality gap in contrastive
pre-trained multi-modal networks. From the perspective of vulnerability to
off-manifold attacks, we therefore argue for the mitigation of modality gaps in
CLIP and related multi-modal approaches. Source code and mined CLIPMasterPrints
are available at https://github.com/matfrei/CLIPMasterPrints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freiberger_M/0/1/0/all/0/1&quot;&gt;Matthias Freiberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kun_P/0/1/0/all/0/1&quot;&gt;Peter Kun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovlie_A/0/1/0/all/0/1&quot;&gt;Anders Sundnes L&amp;#xf8;vlie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risi_S/0/1/0/all/0/1&quot;&gt;Sebastian Risi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03800">
<title>Thoracic Cartilage Ultrasound-CT Registration using Dense Skeleton Graph. (arXiv:2307.03800v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03800</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous ultrasound (US) imaging has gained increased interest recently,
and it has been seen as a potential solution to overcome the limitations of
free-hand US examinations, such as inter-operator variations. However, it is
still challenging to accurately map planned paths from a generic atlas to
individual patients, particularly for thoracic applications with high
acoustic-impedance bone structures under the skin. To address this challenge, a
graph-based non-rigid registration is proposed to enable transferring planned
paths from the atlas to the current setup by explicitly considering
subcutaneous bone surface features instead of the skin surface. To this end,
the sternum and cartilage branches are segmented using a template matching to
assist coarse alignment of US and CT point clouds. Afterward, a directed graph
is generated based on the CT template. Then, the self-organizing map using
geographical distance is successively performed twice to extract the optimal
graph representations for CT and US point clouds, individually. To evaluate the
proposed approach, five cartilage point clouds from distinct patients are
employed. The results demonstrate that the proposed graph-based registration
can effectively map trajectories from CT to the current setup for displaying US
views through limited intercostal space. The non-rigid registration results in
terms of Hausdorff distance (Mean$\pm$SD) is 9.48$\pm$0.27 mm and the path
transferring error in terms of Euclidean distance is 2.21$\pm$1.11 mm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongliang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuesong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03827">
<title>Effect of Intensity Standardization on Deep Learning for WML Segmentation in Multi-Centre FLAIR MRI. (arXiv:2307.03827v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03827</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) methods for white matter lesion (WML) segmentation in MRI
suffer a reduction in performance when applied on data from a scanner or centre
that is out-of-distribution (OOD) from the training data. This is critical for
translation and widescale adoption, since current models cannot be readily
applied to data from new institutions. In this work, we evaluate several
intensity standardization methods for MRI as a preprocessing step for WML
segmentation in multi-centre Fluid-Attenuated Inversion Recovery (FLAIR) MRI.
We evaluate a method specifically developed for FLAIR MRI called IAMLAB along
with other popular normalization techniques such as White-strip, Nyul and
Z-score. We proposed an Ensemble model that combines predictions from each of
these models. A skip-connection UNet (SC UNet) was trained on the standardized
images, as well as the original data and segmentation performance was evaluated
over several dimensions. The training (in-distribution) data consists of a
single study, of 60 volumes, and the test (OOD) data is 128 unseen volumes from
three clinical cohorts. Results show IAMLAB and Ensemble provide higher WML
segmentation performance compared to models from original data or other
normalization methods. IAMLAB &amp;amp; Ensemble have the highest dice similarity
coefficient (DSC) on the in-distribution data (0.78 &amp;amp; 0.80) and on clinical OOD
data. DSC was significantly higher for IAMLAB compared to the original data
(p&amp;lt;0.05) for all lesion categories (LL&amp;gt;25mL: 0.77 vs. 0.71; 10mL&amp;lt;= LL&amp;lt;25mL:
0.66 vs. 0.61; LL&amp;lt;10mL: 0.53 vs. 0.52). The IAMLAB and Ensemble normalization
methods are mitigating MRI domain shift and are optimal for DL-based WML
segmentation in unseen FLAIR data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ghazvanchahi_A/0/1/0/all/0/1&quot;&gt;Abdollah Ghazvanchahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maralani_P/0/1/0/all/0/1&quot;&gt;Pejman Jahbedar Maralani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moody_A/0/1/0/all/0/1&quot;&gt;Alan R. Moody&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khademi_A/0/1/0/all/0/1&quot;&gt;April Khademi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03833">
<title>Back to Optimization: Diffusion-based Zero-Shot 3D Human Pose Estimation. (arXiv:2307.03833v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03833</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based methods have dominated the 3D human pose estimation (HPE)
tasks with significantly better performance in most benchmarks than traditional
optimization-based methods. Nonetheless, 3D HPE in the wild is still the
biggest challenge of learning-based models, whether with 2D-3D lifting,
image-to-3D, or diffusion-based methods, since the trained networks implicitly
learn camera intrinsic parameters and domain-based 3D human pose distributions
and estimate poses by statistical average. On the other hand, the
optimization-based methods estimate results case-by-case, which can predict
more diverse and sophisticated human poses in the wild. By combining the
advantages of optimization-based and learning-based methods, we propose the
Zero-shot Diffusion-based Optimization (ZeDO) pipeline for 3D HPE to solve the
problem of cross-domain and in-the-wild 3D HPE. Our multi-hypothesis ZeDO
achieves state-of-the-art (SOTA) performance on Human3.6M as minMPJPE $51.4$mm
without training with any 2D-3D or image-3D pairs. Moreover, our
single-hypothesis ZeDO achieves SOTA performance on 3DPW dataset with PA-MPJPE
$42.6$mm on cross-dataset evaluation, which even outperforms learning-based
methods trained on 3DPW.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_W/0/1/0/all/0/1&quot;&gt;Wenhao Chai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03847">
<title>Blocks2World: Controlling Realistic Scenes with Editable Primitives. (arXiv:2307.03847v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03847</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Blocks2World, a novel method for 3D scene rendering and editing
that leverages a two-step process: convex decomposition of images and
conditioned synthesis. Our technique begins by extracting 3D parallelepipeds
from various objects in a given scene using convex decomposition, thus
obtaining a primitive representation of the scene. These primitives are then
utilized to generate paired data through simple ray-traced depth maps. The next
stage involves training a conditioned model that learns to generate images from
the 2D-rendered convex primitives. This step establishes a direct mapping
between the 3D model and its 2D representation, effectively learning the
transition from a 3D model to an image. Once the model is fully trained, it
offers remarkable control over the synthesis of novel and edited scenes. This
is achieved by manipulating the primitives at test time, including translating
or adding them, thereby enabling a highly customizable scene rendering process.
Our method provides a fresh perspective on 3D scene rendering and editing,
offering control and flexibility. It opens up new avenues for research and
applications in the field, including authoring and data augmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1&quot;&gt;Vaibhav Vavilala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Seemandhar Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasanth_R/0/1/0/all/0/1&quot;&gt;Rahul Vasanth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1&quot;&gt;Anand Bhattad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;David Forsyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03856">
<title>Novel Categories Discovery from probability matrix perspective. (arXiv:2307.03856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03856</link>
<description rdf:parseType="Literal">&lt;p&gt;Novel Categories Discovery (NCD) tackles the open-world problem of
classifying known and clustering novel categories based on the class semantics
using partial class space annotated data. Unlike traditional pseudo-label and
retraining, we investigate NCD from the novel data probability matrix
perspective. We leverage the connection between NCD novel data sampling with
provided novel class Multinoulli (categorical) distribution and hypothesize to
implicitly achieve semantic-based novel data clustering by learning their class
distribution. We propose novel constraints on first-order (mean) and
second-order (covariance) statistics of probability matrix features while
applying instance-wise information constraints. In particular, we align the
neuron distribution (activation patterns) under a large batch of Monte-Carlo
novel data sampling by matching their empirical features mean and covariance
with the provided Multinoulli-distribution. Simultaneously, we minimize entropy
and enforce prediction consistency for each instance. Our simple approach
successfully realizes semantic-based novel data clustering provided the
semantic similarity between label-unlabeled classes. We demonstrate the
discriminative capacity of our approaches in image and video modalities.
Moreover, we perform extensive ablation studies regarding data, networks, and
our framework components to provide better insights. Our approach maintains
~94%, ~93%, and ~85%, classification accuracy in labeled data while achieving
~90%, ~84%, and ~72% clustering accuracy for novel categories for Cifar10,
UCF101, and MPSC-ARL datasets that matches state-of-the-art approaches without
any external clustering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_Z/0/1/0/all/0/1&quot;&gt;Zahid Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faridee_A/0/1/0/all/0/1&quot;&gt;Abu Zaher Md Faridee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_M/0/1/0/all/0/1&quot;&gt;Masud Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purushotham_S/0/1/0/all/0/1&quot;&gt;Sanjay Purushotham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_H/0/1/0/all/0/1&quot;&gt;Heesung Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungtae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_N/0/1/0/all/0/1&quot;&gt;Nirmalya Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03869">
<title>Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation. (arXiv:2307.03869v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03869</link>
<description rdf:parseType="Literal">&lt;p&gt;Significant progress has recently been made in creative applications of large
pre-trained models for downstream tasks in 3D vision, such as text-to-shape
generation. This motivates our investigation of how these pre-trained models
can be used effectively to generate 3D shapes from sketches, which has largely
remained an open challenge due to the limited sketch-shape paired datasets and
the varying level of abstraction in the sketches. We discover that conditioning
a 3D generative model on the features (obtained from a frozen large pre-trained
vision model) of synthetic renderings during training enables us to effectively
generate 3D shapes from sketches at inference time. This suggests that the
large pre-trained vision model features carry semantic signals that are
resilient to domain shifts, i.e., allowing us to use only RGB renderings, but
generalizing to sketches at inference time. We conduct a comprehensive set of
experiments investigating different design factors and demonstrate the
effectiveness of our straightforward approach for generation of multiple 3D
shapes per each input sketch regardless of their level of abstraction without
requiring any paired datasets during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanghi_A/0/1/0/all/0/1&quot;&gt;Aditya Sanghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaraman_P/0/1/0/all/0/1&quot;&gt;Pradeep Kumar Jayaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rampini_A/0/1/0/all/0/1&quot;&gt;Arianna Rampini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambourne_J/0/1/0/all/0/1&quot;&gt;Joseph Lambourne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shayani_H/0/1/0/all/0/1&quot;&gt;Hooman Shayani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atherton_E/0/1/0/all/0/1&quot;&gt;Evan Atherton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taghanaki_S/0/1/0/all/0/1&quot;&gt;Saeid Asgari Taghanaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03871">
<title>HUMS2023 Data Challenge Result Submission. (arXiv:2307.03871v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03871</link>
<description rdf:parseType="Literal">&lt;p&gt;We implemented a simple method for early detection in this research. The
implemented methods are plotting the given mat files and analyzing scalogram
images generated by performing Continuous Wavelet Transform (CWT) on the
samples. Also, finding the mean, standard deviation (STD), and peak-to-peak
(P2P) values from each signal also helped detect faulty signs. We have
implemented the autoregressive integrated moving average (ARIMA) method to
track the progression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neupane_D/0/1/0/all/0/1&quot;&gt;Dhiraj Neupane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamang_L/0/1/0/all/0/1&quot;&gt;Lakpa Dorje Tamang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_N/0/1/0/all/0/1&quot;&gt;Ngoc Dung Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouadjenek_M/0/1/0/all/0/1&quot;&gt;Mohamed Reda Bouadjenek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aryal_S/0/1/0/all/0/1&quot;&gt;Sunil Aryal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03872">
<title>Domain Adaptation using Silver Standard Labels for Ki-67 Scoring in Digital Pathology: A Step Closer to Widescale Deployment. (arXiv:2307.03872v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03872</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning systems have been proposed to improve the objectivity and
efficiency of Ki- 67 PI scoring. The challenge is that while very accurate,
deep learning techniques suffer from reduced performance when applied to
out-of-domain data. This is a critical challenge for clinical translation, as
models are typically trained using data available to the vendor, which is not
from the target domain. To address this challenge, this study proposes a domain
adaptation pipeline that employs an unsupervised framework to generate silver
standard (pseudo) labels in the target domain, which is used to augment the
gold standard (GS) source domain data. Five training regimes were tested on two
validated Ki-67 scoring architectures (UV-Net and piNET), (1) SS Only: trained
on target silver standard (SS) labels, (2) GS Only: trained on source GS
labels, (3) Mixed: trained on target SS and source GS labels, (4) GS+SS:
trained on source GS labels and fine-tuned on target SS labels, and our
proposed method (5) SS+GS: trained on source SS labels and fine-tuned on source
GS labels. The SS+GS method yielded significantly (p &amp;lt; 0.05) higher PI accuracy
(95.9%) and more consistent results compared to the GS Only model on target
data. Analysis of t-SNE plots showed features learned by the SS+GS models are
more aligned for source and target data, resulting in improved generalization.
The proposed pipeline provides an efficient method for learning the target
distribution without manual annotations, which are time-consuming and costly to
generate for medical images. This framework can be applied to any target site
as a per-laboratory calibration method, for widescale deployment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dy_A/0/1/0/all/0/1&quot;&gt;Amanda Dy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_N/0/1/0/all/0/1&quot;&gt;Ngoc-Nhu Jennifer Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mirjahanmardi_S/0/1/0/all/0/1&quot;&gt;Seyed Hossein Mirjahanmardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dawe_M/0/1/0/all/0/1&quot;&gt;Melanie Dawe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fyles_A/0/1/0/all/0/1&quot;&gt;Anthony Fyles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shi_W/0/1/0/all/0/1&quot;&gt;Wei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fei-Fei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Androutsos_D/0/1/0/all/0/1&quot;&gt;Dimitrios Androutsos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Done_S/0/1/0/all/0/1&quot;&gt;Susan Done&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khademi_A/0/1/0/all/0/1&quot;&gt;April Khademi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03887">
<title>Improving Prototypical Part Networks with Reward Reweighing, Reselection, and Retraining. (arXiv:2307.03887v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.03887</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, work has gone into developing deep interpretable methods for
image classification that clearly attributes a model&apos;s output to specific
features of the data. One such of these methods is the prototypical part
network (ProtoPNet), which attempts to classify images based on meaningful
parts of the input. While this method results in interpretable classifications,
this method often learns to classify from spurious or inconsistent parts of the
image. Hoping to remedy this, we take inspiration from the recent developments
in Reinforcement Learning with Human Feedback (RLHF) to fine-tune these
prototypes. By collecting human annotations of prototypes quality via a 1-5
scale on the CUB-200-2011 dataset, we construct a reward model that learns to
identify non-spurious prototypes. In place of a full RL update, we propose the
reweighted, reselected, and retrained prototypical part network (R3-ProtoPNet),
which adds an additional three steps to the ProtoPNet training loop. The first
two steps are reward-based reweighting and reselection, which align prototypes
with human feedback. The final step is retraining to realign the model&apos;s
features with the updated prototypes. We find that R3-ProtoPNet improves the
overall consistency and meaningfulness of the prototypes, but lower the test
predictive accuracy when used independently. When multiple R3-ProtoPNets are
incorporated into an ensemble, we find an increase in test predictive
performance while maintaining interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Netzorg_R/0/1/0/all/0/1&quot;&gt;Robin Netzorg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaxun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bin Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03898">
<title>StyleGAN3: Generative Networks for Improving the Equivariance of Translation and Rotation. (arXiv:2307.03898v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03898</link>
<description rdf:parseType="Literal">&lt;p&gt;StyleGAN can use style to affect facial posture and identity features, and
noise to affect hair, wrinkles, skin color and other details. Among these, the
outcomes of the picture processing will vary slightly between different
versions of styleGAN. As a result, the comparison of performance differences
between styleGAN2 and the two modified versions of styleGAN3 will be the main
focus of this study. We used the FFHQ dataset as the dataset and FID, EQ-T, and
EQ-R were used to be the assessment of the model. In the end, we discovered
that Stylegan3 version is a better generative network to improve the
equivariance. Our findings have a positive impact on the creation of animation
and videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tianlei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_R/0/1/0/all/0/1&quot;&gt;Renzhe Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_G/0/1/0/all/0/1&quot;&gt;Gaurav Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03903">
<title>Adversarial Self-Attack Defense and Spatial-Temporal Relation Mining for Visible-Infrared Video Person Re-Identification. (arXiv:2307.03903v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03903</link>
<description rdf:parseType="Literal">&lt;p&gt;In visible-infrared video person re-identification (re-ID), extracting
features not affected by complex scenes (such as modality, camera views,
pedestrian pose, background, etc.) changes, and mining and utilizing motion
information are the keys to solving cross-modal pedestrian identity matching.
To this end, the paper proposes a new visible-infrared video person re-ID
method from a novel perspective, i.e., adversarial self-attack defense and
spatial-temporal relation mining. In this work, the changes of views, posture,
background and modal discrepancy are considered as the main factors that cause
the perturbations of person identity features. Such interference information
contained in the training samples is used as an adversarial perturbation. It
performs adversarial attacks on the re-ID model during the training to make the
model more robust to these unfavorable factors. The attack from the adversarial
perturbation is introduced by activating the interference information contained
in the input samples without generating adversarial samples, and it can be thus
called adversarial self-attack. This design allows adversarial attack and
defense to be integrated into one framework. This paper further proposes a
spatial-temporal information-guided feature representation network to use the
information in video sequences. The network cannot only extract the information
contained in the video-frame sequences but also use the relation of the local
information in space to guide the network to extract more robust features. The
proposed method exhibits compelling performance on large-scale cross-modality
video datasets. The source code of the proposed method will be released at
https://github.com/lhf12278/xxx.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huafeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Le Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yafei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dapeng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03918">
<title>VS-TransGRU: A Novel Transformer-GRU-based Framework Enhanced by Visual-Semantic Fusion for Egocentric Action Anticipation. (arXiv:2307.03918v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03918</link>
<description rdf:parseType="Literal">&lt;p&gt;Egocentric action anticipation is a challenging task that aims to make
advanced predictions of future actions from current and historical observations
in the first-person view. Most existing methods focus on improving the model
architecture and loss function based on the visual input and recurrent neural
network to boost the anticipation performance. However, these methods, which
merely consider visual information and rely on a single network architecture,
gradually reach a performance plateau. In order to fully understand what has
been observed and capture the dependencies between current observations and
future actions well enough, we propose a novel visual-semantic fusion enhanced
and Transformer GRU-based action anticipation framework in this paper. Firstly,
high-level semantic information is introduced to improve the performance of
action anticipation for the first time. We propose to use the semantic features
generated based on the class labels or directly from the visual observations to
augment the original visual features. Secondly, an effective visual-semantic
fusion module is proposed to make up for the semantic gap and fully utilize the
complementarity of different modalities. Thirdly, to take advantage of both the
parallel and autoregressive models, we design a Transformer based encoder for
long-term sequential modeling and a GRU-based decoder for flexible iteration
decoding. Extensive experiments on two large-scale first-person view datasets,
i.e., EPIC-Kitchens and EGTEA Gaze+, validate the effectiveness of our proposed
method, which achieves new state-of-the-art performance, outperforming previous
approaches by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Congqi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Ze Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1&quot;&gt;Qinyi Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_L/0/1/0/all/0/1&quot;&gt;Lingtong Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yanning Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03932">
<title>Edge-Aware Mirror Network for Camouflaged Object Detection. (arXiv:2307.03932v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03932</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing edge-aware camouflaged object detection (COD) methods normally
output the edge prediction in the early stage. However, edges are important and
fundamental factors in the following segmentation task. Due to the high visual
similarity between camouflaged targets and the surroundings, edge prior
predicted in early stage usually introduces erroneous foreground-background and
contaminates features for segmentation. To tackle this problem, we propose a
novel Edge-aware Mirror Network (EAMNet), which models edge detection and
camouflaged object segmentation as a cross refinement process. More
specifically, EAMNet has a two-branch architecture, where a
segmentation-induced edge aggregation module and an edge-induced integrity
aggregation module are designed to cross-guide the segmentation branch and edge
detection branch. A guided-residual channel attention module which leverages
the residual connection and gated convolution finally better extracts
structural details from low-level features. Quantitative and qualitative
experiment results show that EAMNet outperforms existing cutting-edge baselines
on three widely used COD datasets. Codes are available at
https://github.com/sdy1999/EAMNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Dongyue Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shiyao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lin Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03942">
<title>Ariadne&apos;s Thread:Using Text Prompts to Improve Segmentation of Infected Areas from Chest X-ray images. (arXiv:2307.03942v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.03942</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmentation of the infected areas of the lung is essential for quantifying
the severity of lung disease like pulmonary infections. Existing medical image
segmentation methods are almost uni-modal methods based on image. However,
these image-only methods tend to produce inaccurate results unless trained with
large amounts of annotated data. To overcome this challenge, we propose a
language-driven segmentation method that uses text prompt to improve to the
segmentation result. Experiments on the QaTa-COV19 dataset indicate that our
method improves the Dice score by 6.09% at least compared to the uni-modal
methods. Besides, our extended study reveals the flexibility of multi-modal
methods in terms of the information granularity of text and demonstrates that
multi-modal methods have a significant advantage over image-only methods in
terms of the size of training data required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yi Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mengqiu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kongming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Ming Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03943">
<title>Camouflaged Object Detection with Feature Grafting and Distractor Aware. (arXiv:2307.03943v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03943</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Camouflaged Object Detection (COD) aims to accurately segment
camouflaged objects that integrated into the environment, which is more
challenging than ordinary detection as the texture between the target and
background is visually indistinguishable. In this paper, we proposed a novel
Feature Grafting and Distractor Aware network (FDNet) to handle the COD task.
Specifically, we use CNN and Transformer to encode multi-scale images in
parallel. In order to better explore the advantages of the two encoders, we
design a cross-attention-based Feature Grafting Module to graft features
extracted from Transformer branch into CNN branch, after which the features are
aggregated in the Feature Fusion Module. A Distractor Aware Module is designed
to explicitly model the two possible distractors in the COD task to refine the
coarse camouflage map. We also proposed the largest artificial camouflaged
object dataset which contains 2000 images with annotations, named ACOD2K. We
conducted extensive experiments on four widely used benchmark datasets and the
ACOD2K dataset. The results show that our method significantly outperforms
other state-of-the-art methods. The code and the ACOD2K will be available at
https://github.com/syxvision/FDNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lin Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03948">
<title>Reading Between the Lanes: Text VideoQA on the Road. (arXiv:2307.03948v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03948</link>
<description rdf:parseType="Literal">&lt;p&gt;Text and signs around roads provide crucial information for drivers, vital
for safe navigation and situational awareness. Scene text recognition in motion
is a challenging problem, while textual cues typically appear for a short time
span, and early detection at a distance is necessary. Systems that exploit such
information to assist the driver should not only extract and incorporate visual
and textual cues from the video stream but also reason over time. To address
this issue, we introduce RoadTextVQA, a new dataset for the task of video
question answering (VideoQA) in the context of driver assistance. RoadTextVQA
consists of $3,222$ driving videos collected from multiple countries, annotated
with $10,500$ questions, all based on text or road signs present in the driving
videos. We assess the performance of state-of-the-art video question answering
models on our RoadTextVQA dataset, highlighting the significant potential for
improvement in this domain and the usefulness of the dataset in advancing
research on in-vehicle support systems and text-aware multimodal question
answering. The dataset is available at
&lt;a href=&quot;http://cvit.iiit.ac.in/research/projects/cvit-projects/roadtextvqa&quot;&gt;this http URL&lt;/a&gt;
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tom_G/0/1/0/all/0/1&quot;&gt;George Tom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mathew_M/0/1/0/all/0/1&quot;&gt;Minesh Mathew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_S/0/1/0/all/0/1&quot;&gt;Sergi Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karatzas_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Karatzas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jawahar_C/0/1/0/all/0/1&quot;&gt;C.V. Jawahar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03967">
<title>End-to-End Supervised Multilabel Contrastive Learning. (arXiv:2307.03967v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03967</link>
<description rdf:parseType="Literal">&lt;p&gt;Multilabel representation learning is recognized as a challenging problem
that can be associated with either label dependencies between object categories
or data-related issues such as the inherent imbalance of positive/negative
samples. Recent advances address these challenges from model- and data-centric
viewpoints. In model-centric, the label correlation is obtained by an external
model designs (e.g., graph CNN) to incorporate an inductive bias for training.
However, they fail to design an end-to-end training framework, leading to high
computational complexity. On the contrary, in data-centric, the realistic
nature of the dataset is considered for improving the classification while
ignoring the label dependencies. In this paper, we propose a new end-to-end
training framework -- dubbed KMCL (Kernel-based Mutlilabel Contrastive
Learning) -- to address the shortcomings of both model- and data-centric
designs. The KMCL first transforms the embedded features into a mixture of
exponential kernels in Gaussian RKHS. It is then followed by encoding an
objective loss that is comprised of (a) reconstruction loss to reconstruct
kernel representation, (b) asymmetric classification loss to address the
inherent imbalance problem, and (c) contrastive loss to capture label
correlation. The KMCL models the uncertainty of the feature encoder while
maintaining a low computational footprint. Extensive experiments are conducted
on image classification tasks to showcase the consistent improvements of KMCL
over the SOTA methods. PyTorch implementation is provided in
\url{https://github.com/mahdihosseini/KMCL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sajedi_A/0/1/0/all/0/1&quot;&gt;Ahmad Sajedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1&quot;&gt;Samir Khaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plataniotis_K/0/1/0/all/0/1&quot;&gt;Konstantinos N. Plataniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hosseini_M/0/1/0/all/0/1&quot;&gt;Mahdi S. Hosseini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03980">
<title>Building and Road Segmentation Using EffUNet and Transfer Learning Approach. (arXiv:2307.03980v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03980</link>
<description rdf:parseType="Literal">&lt;p&gt;In city, information about urban objects such as water supply, railway lines,
power lines, buildings, roads, etc., is necessary for city planning. In
particular, information about the spread of these objects, locations and
capacity is needed for the policymakers to make impactful decisions. This
thesis aims to segment the building and roads from the aerial image captured by
the satellites and UAVs. Many different architectures have been proposed for
the semantic segmentation task and UNet being one of them. In this thesis, we
propose a novel architecture based on Google&apos;s newly proposed EfficientNetV2 as
an encoder for feature extraction with UNet decoder for constructing the
segmentation map. Using this approach we achieved a benchmark score for the
Massachusetts Building and Road dataset with an mIOU of 0.8365 and 0.9153
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gangurde_S/0/1/0/all/0/1&quot;&gt;Sahil Gangurde&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03982">
<title>TractGeoNet: A geometric deep learning framework for pointwise analysis of tract microstructure to predict language assessment performance. (arXiv:2307.03982v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03982</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a geometric deep-learning-based framework, TractGeoNet, for
performing regression using diffusion magnetic resonance imaging (dMRI)
tractography and associated pointwise tissue microstructure measurements. By
employing a point cloud representation, TractGeoNet can directly utilize
pointwise tissue microstructure and positional information from all points
within a fiber tract. To improve regression performance, we propose a novel
loss function, the Paired-Siamese Regression loss, which encourages the model
to focus on accurately predicting the relative differences between regression
label scores rather than just their absolute values. In addition, we propose a
Critical Region Localization algorithm to identify highly predictive anatomical
regions within the white matter fiber tracts for the regression task. We
evaluate the effectiveness of the proposed method by predicting individual
performance on two neuropsychological assessments of language using a dataset
of 20 association white matter fiber tracts from 806 subjects from the Human
Connectome Project. The results demonstrate superior prediction performance of
TractGeoNet compared to several popular regression models. Of the twenty tracts
studied, we find that the left arcuate fasciculus tract is the most highly
predictive of the two studied language performance assessments. The localized
critical regions are widespread and distributed across both hemispheres and all
cerebral lobes, including areas of the brain considered important for language
function such as superior and anterior temporal regions, pars opercularis, and
precentral gyrus. Overall, TractGeoNet demonstrates the potential of geometric
deep learning to enhance the study of the brain&apos;s white matter fiber tracts and
to relate their structure to human traits such as language performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zekelman_L/0/1/0/all/0/1&quot;&gt;Leo R. Zekelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tengfei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1&quot;&gt;Nikos Makris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1&quot;&gt;Yogesh Rathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golby_A/0/1/0/all/0/1&quot;&gt;Alexandra J. Golby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1&quot;&gt;Lauren J. O&amp;#x27;Donnell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03990">
<title>FTFDNet: Learning to Detect Talking Face Video Manipulation with Tri-Modality Interaction. (arXiv:2307.03990v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03990</link>
<description rdf:parseType="Literal">&lt;p&gt;DeepFake based digital facial forgery is threatening public media security,
especially when lip manipulation has been used in talking face generation, and
the difficulty of fake video detection is further improved. By only changing
lip shape to match the given speech, the facial features of identity are hard
to be discriminated in such fake talking face videos. Together with the lack of
attention on audio stream as the prior knowledge, the detection failure of fake
talking face videos also becomes inevitable. It&apos;s found that the optical flow
of the fake talking face video is disordered especially in the lip region while
the optical flow of the real video changes regularly, which means the motion
feature from optical flow is useful to capture manipulation cues. In this
study, a fake talking face detection network (FTFDNet) is proposed by
incorporating visual, audio and motion features using an efficient cross-modal
fusion (CMF) module. Furthermore, a novel audio-visual attention mechanism
(AVAM) is proposed to discover more informative features, which can be
seamlessly integrated into any audio-visual CNN architecture by modularization.
With the additional AVAM, the proposed FTFDNet is able to achieve a better
detection performance than other state-of-the-art DeepFake video detection
methods not only on the established fake talking face detection dataset (FTFDD)
but also on the DeepFake video detection datasets (DFDC and DF-TIMIT).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Ganglai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_J/0/1/0/all/0/1&quot;&gt;Junwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Feihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Y/0/1/0/all/0/1&quot;&gt;Yufei Zha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03992">
<title>Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling. (arXiv:2307.03992v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03992</link>
<description rdf:parseType="Literal">&lt;p&gt;Image denoising is a fundamental problem in computational photography, where
achieving high-quality perceptual performance with low distortion is highly
demanding. Current methods either struggle with perceptual performance or
suffer from significant distortion. Recently, the emerging diffusion model
achieves state-of-the-art performance in various tasks, and its denoising
mechanism demonstrates great potential for image denoising. However,
stimulating diffusion models for image denoising is not straightforward and
requires solving several critical problems. On the one hand, the input
inconsistency hinders the connection of diffusion models and image denoising.
On the other hand, the content inconsistency between the generated image and
the desired denoised image introduces additional distortion. To tackle these
problems, we present a novel strategy called Diffusion Model for Image
Denoising (DMID) by understanding and rethinking the diffusion model from a
denoising perspective. Our DMID strategy includes an adaptive embedding method
that embeds the noisy image into a pre-trained diffusion model, and an adaptive
ensembling method that reduces distortion in the denoised image. Our DMID
strategy achieves state-of-the-art performance on all distortion-based and
perceptual metrics, for both Gaussian and real-world image denoising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hansen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03998">
<title>Lightweight Improved Residual Network for Efficient Inverse Tone Mapping. (arXiv:2307.03998v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.03998</link>
<description rdf:parseType="Literal">&lt;p&gt;The display devices like HDR10 televisions are increasingly prevalent in our
daily life for visualizing high dynamic range (HDR) images. But the majority of
media images on the internet remain in 8-bit standard dynamic range (SDR)
format. Therefore, converting SDR images to HDR ones by inverse tone mapping
(ITM) is crucial to unlock the full potential of abundant media images.
However, existing ITM methods are usually developed with complex network
architectures requiring huge computational costs. In this paper, we propose a
lightweight Improved Residual Network (IRNet) by enhancing the power of popular
residual block for efficient ITM. Specifically, we propose a new Improved
Residual Block (IRB) to extract and fuse multi-layer features for fine-grained
HDR image reconstruction. Experiments on three benchmark datasets demonstrate
that our IRNet achieves state-of-the-art performance on both the ITM and joint
SR-ITM tasks. The code, models and data will be publicly available at
https://github.com/ThisisVikki/ITM-baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_L/0/1/0/all/0/1&quot;&gt;Liqi Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tianyi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yongbao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_X/0/1/0/all/0/1&quot;&gt;Xiantong Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04013">
<title>BPNet: B\&apos;ezier Primitive Segmentation on 3D Point Clouds. (arXiv:2307.04013v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04013</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes BPNet, a novel end-to-end deep learning framework to
learn B\&apos;ezier primitive segmentation on 3D point clouds. The existing works
treat different primitive types separately, thus limiting them to finite shape
categories. To address this issue, we seek a generalized primitive segmentation
on point clouds. Taking inspiration from B\&apos;ezier decomposition on NURBS
models, we transfer it to guide point cloud segmentation casting off primitive
types. A joint optimization framework is proposed to learn B\&apos;ezier primitive
segmentation and geometric fitting simultaneously on a cascaded architecture.
Specifically, we introduce a soft voting regularizer to improve primitive
segmentation and propose an auto-weight embedding module to cluster point
features, making the network more robust and generic. We also introduce a
reconstruction module where we successfully process multiple CAD models with
different primitives simultaneously. We conducted extensive experiments on the
synthetic ABC dataset and real-scan datasets to validate and compare our
approach with different baseline methods. Experiments show superior performance
over previous work in terms of segmentation, with a substantially faster
inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Rao Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_C/0/1/0/all/0/1&quot;&gt;Cheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xiao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alliez_P/0/1/0/all/0/1&quot;&gt;Pierre Alliez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04014">
<title>Novel Pipeline for Diagnosing Acute Lymphoblastic Sensitive to Related Biomarkers. (arXiv:2307.04014v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04014</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Lymphoblastic Leukemia (ALL) is one of the most common types of
childhood blood cancer. The quick start of the treatment process is critical to
saving the patient&apos;s life, and for this reason, early diagnosis of this disease
is essential. Examining the blood smear images of these patients is one of the
methods used by expert doctors to diagnose this disease. Deep learning-based
methods have numerous applications in medical fields, as they have
significantly advanced in recent years. ALL diagnosis is not an exception in
this field, and several machine learning-based methods for this problem have
been proposed. In previous methods, high diagnostic accuracy was reported, but
our work showed that this alone is not sufficient, as it can lead to models
taking shortcuts and not making meaningful decisions. This issue arises due to
the small size of medical training datasets. To address this, we constrained
our model to follow a pipeline inspired by experts&apos; work. We also demonstrated
that, since a judgement based on only one image is insufficient, redefining the
problem as a multiple-instance learning problem is necessary for achieving a
practical result. Our model is the first to provide a solution to this problem
in a multiple-instance learning setup. We introduced a novel pipeline for
diagnosing ALL that approximates the process used by hematologists, is
sensitive to disease biomarkers, and achieves an accuracy of 96.15%, an
F1-score of 94.24%, a sensitivity of 97.56%, and a specificity of 90.91% on ALL
IDB 1. Our method was further evaluated on an out-of-distribution dataset,
which posed a challenging test and had acceptable performance. Notably, our
model was trained on a relatively small dataset, highlighting the potential for
our approach to be applied to other medical datasets with limited data
availability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askari_Farsangi_A/0/1/0/all/0/1&quot;&gt;Amirhossein Askari-Farsangi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharifi_Zarchi_A/0/1/0/all/0/1&quot;&gt;Ali Sharifi-Zarchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rohban_M/0/1/0/all/0/1&quot;&gt;Mohammad Hossein Rohban&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04028">
<title>Measuring the Success of Diffusion Models at Imitating Human Artists. (arXiv:2307.04028v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04028</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern diffusion models have set the state-of-the-art in AI image generation.
Their success is due, in part, to training on Internet-scale data which often
includes copyrighted work. This prompts questions about the extent to which
these models learn from, imitate, or copy the work of human artists. This work
suggests that tying copyright liability to the capabilities of the model may be
useful given the evolving ecosystem of generative models. Specifically, much of
the legal analysis of copyright and generative systems focuses on the use of
protected data for training. As a result, the connections between data,
training, and the system are often obscured. In our approach, we consider
simple image classification techniques to measure a model&apos;s ability to imitate
specific artists. Specifically, we use Contrastive Language-Image Pretrained
(CLIP) encoders to classify images in a zero-shot fashion. Our process first
prompts a model to imitate a specific artist. Then, we test whether CLIP can be
used to reclassify the artist (or the artist&apos;s work) from the imitation. If
these tests match the imitation back to the original artist, this suggests the
model can imitate that artist&apos;s expression. Our approach is simple and
quantitative. Furthermore, it uses standard techniques and does not require
additional training. We demonstrate our approach with an audit of Stable
Diffusion&apos;s capacity to imitate 70 professional digital artists with
copyrighted work online. When Stable Diffusion is prompted to imitate an artist
from this set, we find that the artist can be identified from the imitation
with an average accuracy of 81.0%. Finally, we also show that a sample of the
artist&apos;s work can be matched to these imitation images with a high degree of
statistical reliability. Overall, these results suggest that Stable Diffusion
is broadly successful at imitating individual human artists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1&quot;&gt;Stephen Casper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zifan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mogulothu_S/0/1/0/all/0/1&quot;&gt;Shreya Mogulothu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinov_Z/0/1/0/all/0/1&quot;&gt;Zachary Marinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_C/0/1/0/all/0/1&quot;&gt;Chinmay Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yew_R/0/1/0/all/0/1&quot;&gt;Rui-Jie Yew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zheng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04036">
<title>Designing a Direct Feedback Loop between Humans and Convolutional Neural Networks through Local Explanations. (arXiv:2307.04036v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.04036</link>
<description rdf:parseType="Literal">&lt;p&gt;The local explanation provides heatmaps on images to explain how
Convolutional Neural Networks (CNNs) derive their output. Due to its visual
straightforwardness, the method has been one of the most popular explainable AI
(XAI) methods for diagnosing CNNs. Through our formative study (S1), however,
we captured ML engineers&apos; ambivalent perspective about the local explanation as
a valuable and indispensable envision in building CNNs versus the process that
exhausts them due to the heuristic nature of detecting vulnerability. Moreover,
steering the CNNs based on the vulnerability learned from the diagnosis seemed
highly challenging. To mitigate the gap, we designed DeepFuse, the first
interactive design that realizes the direct feedback loop between a user and
CNNs in diagnosing and revising CNN&apos;s vulnerability using local explanations.
DeepFuse helps CNN engineers to systemically search &quot;unreasonable&quot; local
explanations and annotate the new boundaries for those identified as
unreasonable in a labor-efficient manner. Next, it steers the model based on
the given annotation such that the model doesn&apos;t introduce similar mistakes. We
conducted a two-day study (S2) with 12 experienced CNN engineers. Using
DeepFuse, participants made a more accurate and &quot;reasonable&quot; model than the
current state-of-the-art. Also, participants found the way DeepFuse guides
case-based reasoning can practically improve their current practice. We provide
implications for design that explain how future HCI-driven design can move our
practice forward to make XAI-driven insights more actionable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tong Steven Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaladkar_S/0/1/0/all/0/1&quot;&gt;Shubham Khaladkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young-Ho Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Sungsoo Ray Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04047">
<title>Calibration-Aware Margin Loss: Pushing the Accuracy-Calibration Consistency Pareto Frontier for Deep Metric Learning. (arXiv:2307.04047v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04047</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to use the same distance threshold across different test classes
/ distributions is highly desired for a frictionless deployment of commercial
image retrieval systems. However, state-of-the-art deep metric learning losses
often result in highly varied intra-class and inter-class embedding structures,
making threshold calibration a non-trivial process in practice. In this paper,
we propose a novel metric named Operating-Point-Incosistency-Score (OPIS) that
measures the variance in the operating characteristics across different classes
in a target calibration range, and demonstrate that high accuracy of a metric
learning embedding model does not guarantee calibration consistency for both
seen and unseen classes. We find that, in the high-accuracy regime, there
exists a Pareto frontier where accuracy improvement comes at the cost of
calibration consistency. To address this, we develop a novel regularization,
named Calibration-Aware Margin (CAM) loss, to encourage uniformity in the
representation structures across classes during training. Extensive experiments
demonstrate CAM&apos;s effectiveness in improving calibration-consistency while
retaining or even enhancing accuracy, outperforming state-of-the-art deep
metric learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linghan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qingming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jun Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Ying Nian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tighe_J/0/1/0/all/0/1&quot;&gt;Joe Tighe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Y/0/1/0/all/0/1&quot;&gt;Yifan Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04054">
<title>Deep Unsupervised Learning Using Spike-Timing-Dependent Plasticity. (arXiv:2307.04054v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04054</link>
<description rdf:parseType="Literal">&lt;p&gt;Spike-Timing-Dependent Plasticity (STDP) is an unsupervised learning
mechanism for Spiking Neural Networks (SNNs) that has received significant
attention from the neuromorphic hardware community. However, scaling such local
learning techniques to deeper networks and large-scale tasks has remained
elusive. In this work, we investigate a Deep-STDP framework where a
convolutional network is trained in tandem with pseudo-labels generated by the
STDP clustering process on the network outputs. We achieve $24.56\%$ higher
accuracy and $3.5\times$ faster convergence speed at iso-accuracy on a 10-class
subset of the Tiny ImageNet dataset in contrast to a $k$-means clustering
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Sen Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sengupta_A/0/1/0/all/0/1&quot;&gt;Abhronil Sengupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04066">
<title>Random Position Adversarial Patch for Vision Transformers. (arXiv:2307.04066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04066</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous studies have shown the vulnerability of vision transformers to
adversarial patches, but these studies all rely on a critical assumption: the
attack patches must be perfectly aligned with the patches used for linear
projection in vision transformers. Due to this stringent requirement, deploying
adversarial patches for vision transformers in the physical world becomes
impractical, unlike their effectiveness on CNNs. This paper proposes a novel
method for generating an adversarial patch (G-Patch) that overcomes the
alignment constraint, allowing the patch to launch a targeted attack at any
position within the field of view. Specifically, instead of directly optimizing
the patch using gradients, we employ a GAN-like structure to generate the
adversarial patch. Our experiments show the effectiveness of the adversarial
patch in achieving universal attacks on vision transformers, both in digital
and physical-world scenarios. Additionally, further analysis reveals that the
generated adversarial patch exhibits robustness to brightness restriction,
color transfer, and random noise. Real-world attack experiments validate the
effectiveness of the G-Patch to launch robust attacks even under some very
challenging conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_M/0/1/0/all/0/1&quot;&gt;Mingzhen Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04081">
<title>Score-based Conditional Generation with Fewer Labeled Data by Self-calibrating Classifier Guidance. (arXiv:2307.04081v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04081</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based Generative Models (SGMs) are a popular family of deep generative
models that achieves leading image generation quality. Earlier studies have
extended SGMs to tackle class-conditional generation by coupling an
unconditional SGM with the guidance of a trained classifier. Nevertheless, such
classifier-guided SGMs do not always achieve accurate conditional generation,
especially when trained with fewer labeled data. We argue that the issue is
rooted in unreliable gradients of the classifier and the inability to fully
utilize unlabeled data during training. We then propose to improve
classifier-guided SGMs by letting the classifier calibrate itself. Our key idea
is to use principles from energy-based models to convert the classifier as
another view of the unconditional SGM. Then, existing loss for the
unconditional SGM can be adopted to calibrate the classifier using both labeled
and unlabeled data. Empirical results validate that the proposed approach
significantly improves the conditional generation quality across different
percentages of labeled data. The improved performance makes the proposed
approach consistently superior to other conditional SGMs when using fewer
labeled data. The results confirm the potential of the proposed approach for
generative modeling with limited labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_P/0/1/0/all/0/1&quot;&gt;Paul Kuo-Ming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Si-An Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Hsuan-Tien Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04087">
<title>SVIT: Scaling up Visual Instruction Tuning. (arXiv:2307.04087v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04087</link>
<description rdf:parseType="Literal">&lt;p&gt;Thanks to the emerging of foundation models, the large language and vision
models are integrated to acquire the multimodal ability of visual captioning,
dialogue, question answering, etc. Although existing multimodal models present
impressive performance of visual understanding and reasoning, their limits are
still largely under-explored due to the scarcity of high-quality instruction
tuning data. To push the limits of multimodal capability, we Sale up Visual
Instruction Tuning (SVIT) by constructing a dataset of 3.2 million visual
instruction tuning data including 1.6M conversation question-answer (QA) pairs
and 1.6M complex reasoning QA pairs and 106K detailed image descriptions.
Besides the volume, the proposed dataset is also featured by the high quality
and rich diversity, which is generated by prompting GPT-4 with the abundant
manual annotations of images. We empirically verify that training multimodal
models on SVIT can significantly improve the multimodal performance in terms of
visual perception, reasoning and planing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Boya Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04091">
<title>CMDFusion: Bidirectional Fusion Network with Cross-modality Knowledge Distillation for LIDAR Semantic Segmentation. (arXiv:2307.04091v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04091</link>
<description rdf:parseType="Literal">&lt;p&gt;2D RGB images and 3D LIDAR point clouds provide complementary knowledge for
the perception system of autonomous vehicles. Several 2D and 3D fusion methods
have been explored for the LIDAR semantic segmentation task, but they suffer
from different problems. 2D-to-3D fusion methods require strictly paired data
during inference, which may not be available in real-world scenarios, while
3D-to-2D fusion methods cannot explicitly make full use of the 2D information.
Therefore, we propose a Bidirectional Fusion Network with Cross-Modality
Knowledge Distillation (CMDFusion) in this work. Our method has two
contributions. First, our bidirectional fusion scheme explicitly and implicitly
enhances the 3D feature via 2D-to-3D fusion and 3D-to-2D fusion, respectively,
which surpasses either one of the single fusion schemes. Second, we distillate
the 2D knowledge from a 2D network (Camera branch) to a 3D network (2D
knowledge branch) so that the 3D network can generate 2D information even for
those points not in the FOV (field of view) of the camera. In this way, RGB
images are not required during inference anymore since the 2D knowledge branch
provides 2D information according to the 3D LIDAR input. We show that our
CMDFusion achieves the best performance among all fusion-based methods on
SemanticKITTI and nuScenes datasets. The code will be released at
https://github.com/Jun-CEN/CMDFusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cen_J/0/1/0/all/0/1&quot;&gt;Jun Cen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shiwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_Y/0/1/0/all/0/1&quot;&gt;Yixuan Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Maochun Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04099">
<title>GNP Attack: Transferable Adversarial Examples via Gradient Norm Penalty. (arXiv:2307.04099v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04099</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AE) with good transferability enable practical
black-box attacks on diverse target models, where insider knowledge about the
target models is not required. Previous methods often generate AE with no or
very limited transferability; that is, they easily overfit to the particular
architecture and feature representation of the source, white-box model and the
generated AE barely work for target, black-box models. In this paper, we
propose a novel approach to enhance AE transferability using Gradient Norm
Penalty (GNP). It drives the loss function optimization procedure to converge
to a flat region of local optima in the loss landscape. By attacking 11
state-of-the-art (SOTA) deep learning models and 6 advanced defense methods, we
empirically show that GNP is very effective in generating AE with high
transferability. We also demonstrate that it is very flexible in that it can be
easily integrated with other gradient based methods for stronger transfer-based
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_T/0/1/0/all/0/1&quot;&gt;Tie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wunsch_D/0/1/0/all/0/1&quot;&gt;Donald C. Wunsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04100">
<title>Visible and infrared self-supervised fusion trained on a single example. (arXiv:2307.04100v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04100</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of visible (RGB) to Near-Infrared (NIR)
image fusion. Multispectral imaging is an important task relevant to image
processing and computer vision, even more, since the development of the RGBT
sensor. While the visible image sees color and suffers from noise, haze, and
clouds, the NIR channel captures a clearer picture and it is significantly
required by applications such as dehazing or object detection. The proposed
approach fuses these two aligned channels by training a
Convolutional-Neural-Network (CNN) by a Self-Supervised-Learning (SSL) on a
single example. For each such pair, RGB and IR, the network is trained for
seconds to deduce the final fusion. The SSL is based on Sturcture-of-Similarity
(SSIM) loss combined with Edge-Preservation (EP) loss. The labels for the SSL
are the input channels themselves. This fusion preserves the relevant detail of
each spectral channel while not based on a heavy training process. In the
experiments section, the proposed approach achieves better qualitative and
quantitative multispectral fusion results with respect to other recent methods,
that are not based on large dataset training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ofir_N/0/1/0/all/0/1&quot;&gt;Nati Ofir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04101">
<title>Enhancing Building Semantic Segmentation Accuracy with Super Resolution and Deep Learning: Investigating the Impact of Spatial Resolution on Various Datasets. (arXiv:2307.04101v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04101</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of remote sensing and deep learning techniques has enabled
building semantic segmentation with high accuracy and efficiency. Despite their
success in different tasks, the discussions on the impact of spatial resolution
on deep learning based building semantic segmentation are quite inadequate,
which makes choosing a higher cost-effective data source a big challenge. To
address the issue mentioned above, in this study, we create remote sensing
images among three study areas into multiple spatial resolutions by
super-resolution and down-sampling. After that, two representative deep
learning architectures: UNet and FPN, are selected for model training and
testing. The experimental results obtained from three cities with two deep
learning models indicate that the spatial resolution greatly influences
building segmentation results, and with a better cost-effectiveness around
0.3m, which we believe will be an important insight for data selection and
preparation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhiling Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaodan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dou Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiaoya Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jinyue Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shibasaki_R/0/1/0/all/0/1&quot;&gt;Ryosuke Shibasaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04103">
<title>CA-CentripetalNet: A novel anchor-free deep learning framework for hardhat wearing detection. (arXiv:2307.04103v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04103</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic hardhat wearing detection can strengthen the safety management in
construction sites, which is still challenging due to complicated video
surveillance scenes. To deal with the poor generalization of previous deep
learning based methods, a novel anchor-free deep learning framework called
CA-CentripetalNet is proposed for hardhat wearing detection. Two novel schemes
are proposed to improve the feature extraction and utilization ability of
CA-CentripetalNet, which are vertical-horizontal corner pooling and bounding
constrained center attention. The former is designed to realize the
comprehensive utilization of marginal features and internal features. The
latter is designed to enforce the backbone to pay attention to internal
features, which is only used during the training rather than during the
detection. Experimental results indicate that the CA-CentripetalNet achieves
better performance with the 86.63% mAP (mean Average Precision) with less
memory consumption at a reasonable speed than the existing deep learning based
methods, especially in case of small-scale hardhats and non-worn-hardhats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_N/0/1/0/all/0/1&quot;&gt;Nian Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_W/0/1/0/all/0/1&quot;&gt;Wensheng Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengbin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_N/0/1/0/all/0/1&quot;&gt;Nili Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Han Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04106">
<title>Parametric Depth Based Feature Representation Learning for Object Detection and Segmentation in Bird&apos;s Eye View. (arXiv:2307.04106v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04106</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent vision-only perception models for autonomous driving achieved
promising results by encoding multi-view image features into Bird&apos;s-Eye-View
(BEV) space. A critical step and the main bottleneck of these methods is
transforming image features into the BEV coordinate frame. This paper focuses
on leveraging geometry information, such as depth, to model such feature
transformation. Existing works rely on non-parametric depth distribution
modeling leading to significant memory consumption, or ignore the geometry
information to address this problem. In contrast, we propose to use parametric
depth distribution modeling for feature transformation. We first lift the 2D
image features to the 3D space defined for the ego vehicle via a predicted
parametric depth distribution for each pixel in each view. Then, we aggregate
the 3D feature volume based on the 3D space occupancy derived from depth to the
BEV frame. Finally, we use the transformed features for downstream tasks such
as object detection and semantic segmentation. Existing semantic segmentation
methods do also suffer from an hallucination problem as they do not take
visibility information into account. This hallucination can be particularly
problematic for subsequent modules such as control and planning. To mitigate
the issue, our method provides depth uncertainty and reliable visibility-aware
estimations. We further leverage our parametric depth modeling to present a
novel visibility-aware evaluation metric that, when taken into account, can
mitigate the hallucination problem. Extensive experiments on object detection
and semantic segmentation on the nuScenes datasets demonstrate that our method
outperforms existing methods on both tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiayu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_E/0/1/0/all/0/1&quot;&gt;Enze Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_J/0/1/0/all/0/1&quot;&gt;Jose M. Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miaomiao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04113">
<title>Mitosis Detection from Partial Annotation by Dataset Generation via Frame-Order Flipping. (arXiv:2307.04113v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04113</link>
<description rdf:parseType="Literal">&lt;p&gt;Detection of mitosis events plays an important role in biomedical research.
Deep-learning-based mitosis detection methods have achieved outstanding
performance with a certain amount of labeled data. However, these methods
require annotations for each imaging condition. Collecting labeled data
involves time-consuming human labor. In this paper, we propose a mitosis
detection method that can be trained with partially annotated sequences. The
base idea is to generate a fully labeled dataset from the partial labels and
train a mitosis detection model with the generated dataset. First, we generate
an image pair not containing mitosis events by frame-order flipping. Then, we
paste mitosis events to the image pair by alpha-blending pasting and generate a
fully labeled dataset. We demonstrate the performance of our method on four
datasets, and we confirm that our method outperforms other comparisons which
use partially labeled sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_K/0/1/0/all/0/1&quot;&gt;Kazuya Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katanaya_A/0/1/0/all/0/1&quot;&gt;Ami Katanaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuma_S/0/1/0/all/0/1&quot;&gt;Shinichiro Chuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bise_R/0/1/0/all/0/1&quot;&gt;Ryoma Bise&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04114">
<title>FILM: How can Few-Shot Image Classification Benefit from Pre-Trained Language Models?. (arXiv:2307.04114v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.04114</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot learning aims to train models that can be generalized to novel
classes with only a few samples. Recently, a line of works are proposed to
enhance few-shot learning with accessible semantic information from class
names. However, these works focus on improving existing modules such as visual
prototypes and feature extractors of the standard few-shot learning framework.
This limits the full potential use of semantic information. In this paper, we
propose a novel few-shot learning framework that uses pre-trained language
models based on contrastive learning. To address the challenge of alignment
between visual features and textual embeddings obtained from text-based
pre-trained language model, we carefully design the textual branch of our
framework and introduce a metric module to generalize the cosine similarity.
For better transferability, we let the metric module adapt to different
few-shot tasks and adopt MAML to train the model via bi-level optimization.
Moreover, we conduct extensive experiments on multiple benchmarks to
demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zihao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1&quot;&gt;Yunkai Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_D/0/1/0/all/0/1&quot;&gt;Dong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huishuai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Weiran Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04122">
<title>Enhancing Low-Light Images Using Infrared-Encoded Images. (arXiv:2307.04122v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04122</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light image enhancement task is essential yet challenging as it is
ill-posed intrinsically. Previous arts mainly focus on the low-light images
captured in the visible spectrum using pixel-wise loss, which limits the
capacity of recovering the brightness, contrast, and texture details due to the
small number of income photons. In this work, we propose a novel approach to
increase the visibility of images captured under low-light environments by
removing the in-camera infrared (IR) cut-off filter, which allows for the
capture of more photons and results in improved signal-to-noise ratio due to
the inclusion of information from the IR spectrum. To verify the proposed
strategy, we collect a paired dataset of low-light images captured without the
IR cut-off filter, with corresponding long-exposure reference images with an
external filter. The experimental results on the proposed dataset demonstrate
the effectiveness of the proposed method, showing better performance
quantitatively and qualitatively. The dataset and code are publicly available
at https://wyf0912.github.io/ELIEI/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_S/0/1/0/all/0/1&quot;&gt;Shulin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yufei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_R/0/1/0/all/0/1&quot;&gt;Renjie Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kot_A/0/1/0/all/0/1&quot;&gt;Alex C. Kot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_B/0/1/0/all/0/1&quot;&gt;Bihan Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04128">
<title>Marine Debris Detection in Satellite Surveillance using Attention Mechanisms. (arXiv:2307.04128v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04128</link>
<description rdf:parseType="Literal">&lt;p&gt;Marine debris is an important issue for environmental protection, but current
methods for locating marine debris are yet limited. In order to achieve higher
efficiency and wider applicability in the localization of Marine debris, this
study tries to combine the instance segmentation of YOLOv7 with different
attention mechanisms and explores the best model. By utilizing a labelled
dataset consisting of satellite images containing ocean debris, we examined
three attentional models including lightweight coordinate attention, CBAM
(combining spatial and channel focus), and bottleneck transformer (based on
self-attention). Box detection assessment revealed that CBAM achieved the best
outcome (F1 score of 77%) compared to coordinate attention (F1 score of 71%)
and YOLOv7/bottleneck transformer (both F1 scores around 66%). Mask evaluation
showed CBAM again leading with an F1 score of 73%, whereas coordinate attention
and YOLOv7 had comparable performances (around F1 score of 68%/69%) and
bottleneck transformer lagged behind at F1 score of 56%. These findings suggest
that CBAM offers optimal suitability for detecting marine debris. However, it
should be noted that the bottleneck transformer detected some areas missed by
manual annotation and displayed better mask precision for larger debris pieces,
signifying potentially superior practical performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_A/0/1/0/all/0/1&quot;&gt;Ao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yijie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Richard Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04129">
<title>Cross-modal Orthogonal High-rank Augmentation for RGB-Event Transformer-trackers. (arXiv:2307.04129v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04129</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the problem of cross-modal object tracking from RGB
videos and event data. Rather than constructing a complex cross-modal fusion
network, we explore the great potential of a pre-trained vision Transformer
(ViT). Particularly, we delicately investigate plug-and-play training
augmentations that encourage the ViT to bridge the vast distribution gap
between the two modalities, enabling comprehensive cross-modal information
interaction and thus enhancing its ability. Specifically, we propose a mask
modeling strategy that randomly masks a specific modality of some tokens to
enforce the interaction between tokens from different modalities interacting
proactively. To mitigate network oscillations resulting from the masking
strategy and further amplify its positive effect, we then theoretically propose
an orthogonal high-rank loss to regularize the attention matrix. Extensive
experiments demonstrate that our plug-and-play training augmentation techniques
can significantly boost state-of-the-art one-stream and twostream trackers to a
large extent in terms of both tracking precision and success rate. Our new
perspective and findings will potentially bring insights to the field of
leveraging powerful pre-trained ViTs to model cross-modal data. The code will
be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Dapeng Oliver Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04132">
<title>Reasoning over the Behaviour of Objects in Video-Clips for Adverb-Type Recognition. (arXiv:2307.04132v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04132</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, following the intuition that adverbs describing scene-sequences
are best identified by reasoning over high-level concepts of object-behavior,
we propose the design of a new framework that reasons over object-behaviours
extracted from raw-video-clips to recognize the clip&apos;s corresponding
adverb-types. Importantly, while previous works for general scene
adverb-recognition assume knowledge of the clips underlying action-types, our
method is directly applicable in the more general problem setting where the
action-type of a video-clip is unknown. Specifically, we propose a novel
pipeline that extracts human-interpretable object-behaviour-facts from raw
video clips and propose novel symbolic and transformer based reasoning methods
that operate over these extracted facts to identify adverb-types. Experiment
results demonstrate that our proposed methods perform favourably against the
previous state-of-the-art. Additionally, to support efforts in symbolic
video-processing, we release two new datasets of object-behaviour-facts
extracted from raw video clips - the MSR-VTT-ASP and ActivityNet-ASP datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seshadri_A/0/1/0/all/0/1&quot;&gt;Amrit Diggavi Seshadri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_A/0/1/0/all/0/1&quot;&gt;Alessandra Russo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04133">
<title>Ultrasonic Image&apos;s Annotation Removal: A Self-supervised Noise2Noise Approach. (arXiv:2307.04133v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.04133</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurately annotated ultrasonic images are vital components of a high-quality
medical report. Hospitals often have strict guidelines on the types of
annotations that should appear on imaging results. However, manually inspecting
these images can be a cumbersome task. While a neural network could potentially
automate the process, training such a model typically requires a dataset of
paired input and target images, which in turn involves significant human
labour. This study introduces an automated approach for detecting annotations
in images. This is achieved by treating the annotations as noise, creating a
self-supervised pretext task and using a model trained under the Noise2Noise
scheme to restore the image to a clean state. We tested a variety of model
structures on the denoising task against different types of annotation,
including body marker annotation, radial line annotation, etc. Our results
demonstrate that most models trained under the Noise2Noise scheme outperformed
their counterparts trained with noisy-clean data pairs. The costumed U-Net
yielded the most optimal outcome on the body marker annotation dataset, with
high scores on segmentation precision and reconstruction similarity. We
released our code at https://github.com/GrandArth/UltrasonicImage-N2N-Approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhaoheng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Junying Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Teng_Y/0/1/0/all/0/1&quot;&gt;Yueyang Teng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04136">
<title>ECL: Class-Enhancement Contrastive Learning for Long-tailed Skin Lesion Classification. (arXiv:2307.04136v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04136</link>
<description rdf:parseType="Literal">&lt;p&gt;Skin image datasets often suffer from imbalanced data distribution,
exacerbating the difficulty of computer-aided skin disease diagnosis. Some
recent works exploit supervised contrastive learning (SCL) for this long-tailed
challenge. Despite achieving significant performance, these SCL-based methods
focus more on head classes, yet ignoring the utilization of information in tail
classes. In this paper, we propose class-Enhancement Contrastive Learning
(ECL), which enriches the information of minority classes and treats different
classes equally. For information enhancement, we design a hybrid-proxy model to
generate class-dependent proxies and propose a cycle update strategy for
parameters optimization. A balanced-hybrid-proxy loss is designed to exploit
relations between samples and proxies with different classes treated equally.
Taking both &quot;imbalanced data&quot; and &quot;imbalanced diagnosis difficulty&quot; into
account, we further present a balanced-weighted cross-entropy loss following
curriculum learning schedule. Experimental results on the classification of
imbalanced skin lesion data have demonstrated the superiority and effectiveness
of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yilan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Ke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_F/0/1/0/all/0/1&quot;&gt;Fengying Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04137">
<title>A Novel Explainable Artificial Intelligence Model in Image Classification problem. (arXiv:2307.04137v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04137</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, artificial intelligence is increasingly being applied widely
in many different fields and has a profound and direct impact on human life.
Following this is the need to understand the principles of the model making
predictions. Since most of the current high-precision models are black boxes,
neither the AI scientist nor the end-user deeply understands what&apos;s going on
inside these models. Therefore, many algorithms are studied for the purpose of
explaining AI models, especially those in the problem of image classification
in the field of computer vision such as LIME, CAM, GradCAM. However, these
algorithms still have limitations such as LIME&apos;s long execution time and CAM&apos;s
confusing interpretation of concreteness and clarity. Therefore, in this paper,
we propose a new method called Segmentation - Class Activation Mapping (SeCAM)
that combines the advantages of these algorithms above, while at the same time
overcoming their disadvantages. We tested this algorithm with various models,
including ResNet50, Inception-v3, VGG16 from ImageNet Large Scale Visual
Recognition Challenge (ILSVRC) data set. Outstanding results when the algorithm
has met all the requirements for a specific explanation in a remarkably concise
time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Quoc Hung Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong Thanh Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Vo Thanh Khang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_X/0/1/0/all/0/1&quot;&gt;Xuan Phong Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04147">
<title>A Survey and Approach to Chart Classification. (arXiv:2307.04147v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04147</link>
<description rdf:parseType="Literal">&lt;p&gt;Charts represent an essential source of visual information in documents and
facilitate a deep understanding and interpretation of information typically
conveyed numerically. In the scientific literature, there are many charts, each
with its stylistic differences. Recently the document understanding community
has begun to address the problem of automatic chart understanding, which begins
with chart classification. In this paper, we present a survey of the current
state-of-the-art techniques for chart classification and discuss the available
datasets and their supported chart types. We broadly classify these
contributions as traditional approaches based on ML, CNN, and Transformers.
Furthermore, we carry out an extensive comparative performance analysis of
CNN-based and transformer-based approaches on the recently published CHARTINFO
UB-UNITECH PMC dataset for the CHART-Infographics competition at ICPR 2022. The
data set includes 15 different chart categories, including 22,923 training
images and 13,260 test images. We have implemented a vision-based transformer
model that produces state-of-the-art results in chart classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhote_A/0/1/0/all/0/1&quot;&gt;Anurag Dhote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_M/0/1/0/all/0/1&quot;&gt;Mohammed Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doermann_D/0/1/0/all/0/1&quot;&gt;David S Doermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04149">
<title>Latent Graph Attention for Enhanced Spatial Context. (arXiv:2307.04149v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04149</link>
<description rdf:parseType="Literal">&lt;p&gt;Global contexts in images are quite valuable in image-to-image translation
problems. Conventional attention-based and graph-based models capture the
global context to a large extent, however, these are computationally expensive.
Moreover, the existing approaches are limited to only learning the pairwise
semantic relation between any two points on the image. In this paper, we
present Latent Graph Attention (LGA) a computationally inexpensive (linear to
the number of nodes) and stable, modular framework for incorporating the global
context in the existing architectures, especially empowering small-scale
architectures to give performance closer to large size architectures, thus
making the light-weight architectures more useful for edge devices with lower
compute power and lower energy needs. LGA propagates information spatially
using a network of locally connected graphs, thereby facilitating to construct
a semantically coherent relation between any two spatially distant points that
also takes into account the influence of the intermediate pixels. Moreover, the
depth of the graph network can be used to adapt the extent of contextual spread
to the target dataset, thereby being able to explicitly control the added
computational cost. To enhance the learning mechanism of LGA, we also introduce
a novel contrastive loss term that helps our LGA module to couple well with the
original architecture at the expense of minimal additional computational load.
We show that incorporating LGA improves the performance on three challenging
applications, namely transparent object segmentation, image restoration for
dehazing and optical flow estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Ayush Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhambhu_Y/0/1/0/all/0/1&quot;&gt;Yash Bhambhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckchash_H/0/1/0/all/0/1&quot;&gt;Himanshu Buckchash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_D/0/1/0/all/0/1&quot;&gt;Deepak K. Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prasad_D/0/1/0/all/0/1&quot;&gt;Dilip K. Prasad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04157">
<title>DIFF-NST: Diffusion Interleaving For deFormable Neural Style Transfer. (arXiv:2307.04157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04157</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Style Transfer (NST) is the field of study applying neural techniques
to modify the artistic appearance of a content image to match the style of a
reference style image. Traditionally, NST methods have focused on texture-based
image edits, affecting mostly low level information and keeping most image
structures the same. However, style-based deformation of the content is
desirable for some styles, especially in cases where the style is abstract or
the primary concept of the style is in its deformed rendition of some content.
With the recent introduction of diffusion models, such as Stable Diffusion, we
can access far more powerful image generation techniques, enabling new
possibilities. In our work, we propose using this new class of models to
perform style transfer while enabling deformable style transfer, an elusive
capability in previous models. We show how leveraging the priors of these
models can expose new artistic controls at inference time, and we document our
findings in exploring this new direction for the field of style transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruta_D/0/1/0/all/0/1&quot;&gt;Dan Ruta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarres_G/0/1/0/all/0/1&quot;&gt;Gemma Canet Tarr&amp;#xe9;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Andrew Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1&quot;&gt;Nicholas Kolkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collomosse_J/0/1/0/all/0/1&quot;&gt;John Collomosse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04159">
<title>Reducing False Alarms in Video Surveillance by Deep Feature Statistical Modeling. (arXiv:2307.04159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04159</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting relevant changes is a fundamental problem of video surveillance.
Because of the high variability of data and the difficulty of properly
annotating changes, unsupervised methods dominate the field. Arguably one of
the most critical issues to make them practical is to reduce their false alarm
rate. In this work, we develop a method-agnostic weakly supervised a-contrario
validation process, based on high dimensional statistical modeling of deep
features, to reduce the number of false alarms of any change detection
algorithm. We also raise the insufficiency of the conventionally used
pixel-wise evaluation, as it fails to precisely capture the performance needs
of most real applications. For this reason, we complement pixel-wise metrics
with object-wise metrics and evaluate the impact of our approach at both pixel
and object levels, on six methods and several sequences from different
datasets. Experimental results reveal that the proposed a-contrario validation
is able to largely reduce the number of false alarms at both pixel and object
levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bou_X/0/1/0/all/0/1&quot;&gt;Xavier Bou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artola_A/0/1/0/all/0/1&quot;&gt;Aitor Artola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehret_T/0/1/0/all/0/1&quot;&gt;Thibaud Ehret&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1&quot;&gt;Gabriele Facciolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morel_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Morel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gioi_R/0/1/0/all/0/1&quot;&gt;Rafael Grompone von Gioi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04187">
<title>Predictive Coding For Animation-Based Video Compression. (arXiv:2307.04187v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04187</link>
<description rdf:parseType="Literal">&lt;p&gt;We address the problem of efficiently compressing video for conferencing-type
applications. We build on recent approaches based on image animation, which can
achieve good reconstruction quality at very low bitrate by representing face
motions with a compact set of sparse keypoints. However, these methods encode
video in a frame-by-frame fashion, i.e. each frame is reconstructed from a
reference frame, which limits the reconstruction quality when the bandwidth is
larger. Instead, we propose a predictive coding scheme which uses image
animation as a predictor, and codes the residual with respect to the actual
target frame. The residuals can be in turn coded in a predictive manner, thus
removing efficiently temporal dependencies. Our experiments indicate a
significant bitrate gain, in excess of 70% compared to the HEVC video standard
and over 30% compared to VVC, on a datasetof talking-head videos
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Konuko_G/0/1/0/all/0/1&quot;&gt;Goluck Konuko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valenzise_G/0/1/0/all/0/1&quot;&gt;Giuseppe Valenzise&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04189">
<title>Histopathology Whole Slide Image Analysis with Heterogeneous Graph Representation Learning. (arXiv:2307.04189v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04189</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-based methods have been extensively applied to whole-slide
histopathology image (WSI) analysis due to the advantage of modeling the
spatial relationships among different entities. However, most of the existing
methods focus on modeling WSIs with homogeneous graphs (e.g., with homogeneous
node type). Despite their successes, these works are incapable of mining the
complex structural relations between biological entities (e.g., the diverse
interaction among different cell types) in the WSI. We propose a novel
heterogeneous graph-based framework to leverage the inter-relationships among
different types of nuclei for WSI analysis. Specifically, we formulate the WSI
as a heterogeneous graph with &quot;nucleus-type&quot; attribute to each node and a
semantic similarity attribute to each edge. We then present a new
heterogeneous-graph edge attribute transformer (HEAT) to take advantage of the
edge and node heterogeneity during massage aggregating. Further, we design a
new pseudo-label-based semantic-consistent pooling mechanism to obtain
graph-level features, which can mitigate the over-parameterization issue of
conventional cluster-based pooling. Additionally, observing the limitations of
existing association-based localization methods, we propose a causal-driven
approach attributing the contribution of each node to improve the
interpretability of our framework. Extensive experiments on three public TCGA
benchmark datasets demonstrate that our framework outperforms the
state-of-the-art methods with considerable margins on various tasks. Our codes
are available at https://github.com/HKU-MedAI/WSI-HGNN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_T/0/1/0/all/0/1&quot;&gt;Tsai Hor Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cendra_F/0/1/0/all/0/1&quot;&gt;Fernando Julio Cendra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1&quot;&gt;Guosheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lequan Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04192">
<title>SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04192</link>
<description rdf:parseType="Literal">&lt;p&gt;Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1&quot;&gt;Min-Yen Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1&quot;&gt;Soujanya Poria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04223">
<title>Real-time Human Detection in Fire Scenarios using Infrared and Thermal Imaging Fusion. (arXiv:2307.04223v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04223</link>
<description rdf:parseType="Literal">&lt;p&gt;Fire is considered one of the most serious threats to human lives which
results in a high probability of fatalities. Those severe consequences stem
from the heavy smoke emitted from a fire that mostly restricts the visibility
of escaping victims and rescuing squad. In such hazardous circumstances, the
use of a vision-based human detection system is able to improve the ability to
save more lives. To this end, a thermal and infrared imaging fusion strategy
based on multiple cameras for human detection in low-visibility scenarios
caused by smoke is proposed in this paper. By processing with multiple cameras,
vital information can be gathered to generate more useful features for human
detection. Firstly, the cameras are calibrated using a Light Heating
Chessboard. Afterward, the features extracted from the input images are merged
prior to being passed through a lightweight deep neural network to perform the
human detection task. The experiments conducted on an NVIDIA Jetson Nano
computer demonstrated that the proposed method can process with reasonable
speed and can achieve favorable performance with a mAP@0.5 of 95%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_T/0/1/0/all/0/1&quot;&gt;Truong-Dong Do&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_N/0/1/0/all/0/1&quot;&gt;Nghe-Nhan Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;My-Ha Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04231">
<title>Mx2M: Masked Cross-Modality Modeling in Domain Adaptation for 3D Semantic Segmentation. (arXiv:2307.04231v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04231</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing methods of cross-modal domain adaptation for 3D semantic
segmentation predict results only via 2D-3D complementarity that is obtained by
cross-modal feature matching. However, as lacking supervision in the target
domain, the complementarity is not always reliable. The results are not ideal
when the domain gap is large. To solve the problem of lacking supervision, we
introduce masked modeling into this task and propose a method Mx2M, which
utilizes masked cross-modality modeling to reduce the large domain gap. Our
Mx2M contains two components. One is the core solution, cross-modal removal and
prediction (xMRP), which makes the Mx2M adapt to various scenarios and provides
cross-modal self-supervision. The other is a new way of cross-modal feature
matching, the dynamic cross-modal filter (DxMF) that ensures the whole method
dynamically uses more suitable 2D-3D complementarity. Evaluation of the Mx2M on
three DA scenarios, including Day/Night, USA/Singapore, and A2D2/SemanticKITTI,
brings large improvements over previous methods on many metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Boxiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zunran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Y/0/1/0/all/0/1&quot;&gt;Yonggen Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shenghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhui Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04245">
<title>A Novel Pipeline for Improving Optical Character Recognition through Post-processing Using Natural Language Processing. (arXiv:2307.04245v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04245</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical Character Recognition (OCR) technology finds applications in
digitizing books and unstructured documents, along with applications in other
domains such as mobility statistics, law enforcement, traffic, security
systems, etc. The state-of-the-art methods work well with the OCR with printed
text on license plates, shop names, etc. However, applications such as printed
textbooks and handwritten texts have limited accuracy with existing techniques.
The reason may be attributed to similar-looking characters and variations in
handwritten characters. Since these issues are challenging to address with OCR
technologies exclusively, we propose a post-processing approach using Natural
Language Processing (NLP) tools. This work presents an end-to-end pipeline that
first performs OCR on the handwritten or printed text and then improves its
accuracy using NLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakshit_A/0/1/0/all/0/1&quot;&gt;Aishik Rakshit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Samyak Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1&quot;&gt;Anirban Dasgupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04246">
<title>Convex Decomposition of Indoor Scenes. (arXiv:2307.04246v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.04246</link>
<description rdf:parseType="Literal">&lt;p&gt;We describe a method to parse a complex, cluttered indoor scene into
primitives which offer a parsimonious abstraction of scene structure. Our
primitives are simple convexes. Our method uses a learned regression procedure
to parse a scene into a fixed number of convexes from RGBD input, and can
optionally accept segmentations to improve the decomposition. The result is
then polished with a descent method which adjusts the convexes to produce a
very good fit, and greedily removes superfluous primitives. Because the entire
scene is parsed, we can evaluate using traditional depth, normal, and
segmentation error metrics. Our evaluation procedure demonstrates that the
error from our primitive representation is comparable to that of predicting
depth from a single image.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vavilala_V/0/1/0/all/0/1&quot;&gt;Vaibhav Vavilala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;David Forsyth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04296">
<title>K-Space-Aware Cross-Modality Score for Synthesized Neuroimage Quality Assessment. (arXiv:2307.04296v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.04296</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of how to assess cross-modality medical image synthesis has been
largely unexplored. The most used measures like PSNR and SSIM focus on
analyzing the structural features but neglect the crucial lesion location and
fundamental k-space speciality of medical images. To overcome this problem, we
propose a new metric K-CROSS to spur progress on this challenging problem.
Specifically, K-CROSS uses a pre-trained multi-modality segmentation network to
predict the lesion location, together with a tumor encoder for representing
features, such as texture details and brightness intensities. To further
reflect the frequency-specific information from the magnetic resonance imaging
principles, both k-space features and vision features are obtained and employed
in our comprehensive encoders with a frequency reconstruction penalty. The
structure-shared encoders are designed and constrained with a similarity loss
to capture the intrinsic common structural information for both modalities. As
a consequence, the features learned from lesion regions, k-space, and
anatomical structures are all captured, which serve as our quality evaluators.
We evaluate the performance by constructing a large-scale cross-modality
neuroimaging perceptual similarity (NIRPS) dataset with 6,000 radiologist
judgments. Extensive experiments demonstrate that the proposed method
outperforms other metrics, especially in comparison with the radiologists on
NIRPS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yawen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiayi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1908.08016">
<title>Testing Robustness Against Unforeseen Adversaries. (arXiv:1908.08016v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1908.08016</link>
<description rdf:parseType="Literal">&lt;p&gt;When considering real-world adversarial settings, defenders are unlikely to
have access to the full range of deployment-time adversaries during training,
and adversaries are likely to use realistic adversarial distortions that will
not be limited to small L_p-constrained perturbations. To narrow in on this
discrepancy between research and reality we introduce eighteen novel
adversarial attacks, which we use to create ImageNet-UA, a new benchmark for
evaluating model robustness against a wide range of unforeseen adversaries. We
make use of our benchmark to identify a range of defense strategies which can
help overcome this generalization gap, finding a rich space of techniques which
can improve unforeseen robustness. We hope the greater variety and realism of
ImageNet-UA will make it a useful tool for those working on real-world
worst-case robustness, enabling development of more robust defenses which can
generalize beyond attacks seen during training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaufmann_M/0/1/0/all/0/1&quot;&gt;Max Kaufmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_D/0/1/0/all/0/1&quot;&gt;Daniel Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basart_S/0/1/0/all/0/1&quot;&gt;Steven Basart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xuwang Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazeika_M/0/1/0/all/0/1&quot;&gt;Mantas Mazeika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1&quot;&gt;Akul Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1&quot;&gt;Adam Dziedzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boenisch_F/0/1/0/all/0/1&quot;&gt;Franziska Boenisch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1&quot;&gt;Tom Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinhardt_J/0/1/0/all/0/1&quot;&gt;Jacob Steinhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendrycks_D/0/1/0/all/0/1&quot;&gt;Dan Hendrycks&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.12556">
<title>A Survey on Visual Transformer. (arXiv:2012.12556v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2012.12556</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer, first applied to the field of natural language processing, is a
type of deep neural network mainly based on the self-attention mechanism.
Thanks to its strong representation capabilities, researchers are looking at
ways to apply transformer to computer vision tasks. In a variety of visual
benchmarks, transformer-based models perform similar to or better than other
types of networks such as convolutional and recurrent neural networks. Given
its high performance and less need for vision-specific inductive bias,
transformer is receiving more and more attention from the computer vision
community. In this paper, we review these vision transformer models by
categorizing them in different tasks and analyzing their advantages and
disadvantages. The main categories we explore include the backbone network,
high/mid-level vision, low-level vision, and video processing. We also include
efficient transformer methods for pushing transformer into real device-based
applications. Furthermore, we also take a brief look at the self-attention
mechanism in computer vision, as it is the base component in transformer.
Toward the end of this paper, we discuss the challenges and provide several
further research directions for vision transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kai Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinghao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jianyuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yehui Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_A/0/1/0/all/0/1&quot;&gt;An Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chunjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yixing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhaohui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiman Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.13136">
<title>Insights from Generative Modeling for Neural Video Compression. (arXiv:2107.13136v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2107.13136</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent machine learning research has revealed connections between deep
generative models such as VAEs and rate-distortion losses used in learned
compression, most of this work has focused on images. In a similar spirit, we
view recently proposed neural video coding algorithms through the lens of deep
autoregressive and latent variable modeling. We present these codecs as
instances of a generalized stochastic temporal autoregressive transform, and
propose new avenues for further improvements inspired by normalizing flows and
structured priors. We propose several architectures that yield state-of-the-art
video compression performance on high-resolution video and discuss their
tradeoffs and ablations. In particular, we propose (i) improved temporal
autoregressive transforms, (ii) improved entropy models with structured and
temporal dependencies, and (iii) variable bitrate versions of our algorithms.
Since our improvements are compatible with a large class of existing models, we
provide further evidence that the generative modeling viewpoint can advance the
neural video coding field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_R/0/1/0/all/0/1&quot;&gt;Ruihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yibo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marino_J/0/1/0/all/0/1&quot;&gt;Joseph Marino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mandt_S/0/1/0/all/0/1&quot;&gt;Stephan Mandt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03360">
<title>Sparse MoEs meet Efficient Ensembles. (arXiv:2110.03360v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models based on the aggregated outputs of submodels, either
at the activation or prediction levels, often exhibit strong performance
compared to individual models. We study the interplay of two popular classes of
such models: ensembles of neural networks and sparse mixture of experts (sparse
MoEs). First, we show that the two approaches have complementary features whose
combination is beneficial. This includes a comprehensive evaluation of sparse
MoEs in uncertainty related benchmarks. Then, we present Efficient Ensemble of
Experts (E$^3$), a scalable and simple ensemble of sparse MoEs that takes the
best of both classes of models, while using up to 45% fewer FLOPs than a deep
ensemble. Extensive experiments demonstrate the accuracy, log-likelihood,
few-shot learning, robustness, and uncertainty improvements of E$^3$ over
several challenging vision Transformer-based baselines. E$^3$ not only
preserves its efficiency while scaling to models with up to 2.7B parameters,
but also provides better predictive performance and uncertainty estimates for
larger models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allingham_J/0/1/0/all/0/1&quot;&gt;James Urquhart Allingham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1&quot;&gt;Florian Wenzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mariet_Z/0/1/0/all/0/1&quot;&gt;Zelda E Mariet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1&quot;&gt;Basil Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puigcerver_J/0/1/0/all/0/1&quot;&gt;Joan Puigcerver&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houlsby_N/0/1/0/all/0/1&quot;&gt;Neil Houlsby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jerfel_G/0/1/0/all/0/1&quot;&gt;Ghassen Jerfel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fortuin_V/0/1/0/all/0/1&quot;&gt;Vincent Fortuin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakshminarayanan_B/0/1/0/all/0/1&quot;&gt;Balaji Lakshminarayanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_J/0/1/0/all/0/1&quot;&gt;Jasper Snoek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_D/0/1/0/all/0/1&quot;&gt;Dustin Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruiz_C/0/1/0/all/0/1&quot;&gt;Carlos Riquelme Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jenatton_R/0/1/0/all/0/1&quot;&gt;Rodolphe Jenatton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.11089">
<title>Monocular Road Planar Parallax Estimation. (arXiv:2111.11089v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.11089</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating the 3D structure of the drivable surface and surrounding
environment is a crucial task for assisted and autonomous driving. It is
commonly solved either by using 3D sensors such as LiDAR or directly predicting
the depth of points via deep learning. However, the former is expensive, and
the latter lacks the use of geometry information for the scene. In this paper,
instead of following existing methodologies, we propose Road Planar Parallax
Attention Network (RPANet), a new deep neural network for 3D sensing from
monocular image sequences based on planar parallax, which takes full advantage
of the omnipresent road plane geometry in driving scenes. RPANet takes a pair
of images aligned by the homography of the road plane as input and outputs a
$\gamma$ map (the ratio of height to depth) for 3D reconstruction. The $\gamma$
map has the potential to construct a two-dimensional transformation between two
consecutive frames. It implies planar parallax and can be combined with the
road plane serving as a reference to estimate the 3D structure by warping the
consecutive frames. Furthermore, we introduce a novel cross-attention module to
make the network better perceive the displacements caused by planar parallax.
To verify the effectiveness of our method, we sample data from the Waymo Open
Dataset and construct annotations related to planar parallax. Comprehensive
experiments are conducted on the sampled dataset to demonstrate the 3D
reconstruction accuracy of our approach in challenging scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Haobo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Teng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_W/0/1/0/all/0/1&quot;&gt;Wei Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiafeng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lefei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.00627">
<title>Deep fiber clustering: Anatomically informed fiber clustering with self-supervised deep learning for fast and effective tractography parcellation. (arXiv:2205.00627v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.00627</link>
<description rdf:parseType="Literal">&lt;p&gt;White matter fiber clustering is an important strategy for white matter
parcellation, which enables quantitative analysis of brain connections in
health and disease. In combination with expert neuroanatomical labeling,
data-driven white matter fiber clustering is a powerful tool for creating
atlases that can model white matter anatomy across individuals. While widely
used fiber clustering approaches have shown good performance using classical
unsupervised machine learning techniques, recent advances in deep learning
reveal a promising direction toward fast and effective fiber clustering. In
this work, we propose a novel deep learning framework for white matter fiber
clustering, Deep Fiber Clustering (DFC), which solves the unsupervised
clustering problem as a self-supervised learning task with a domain-specific
pretext task to predict pairwise fiber distances. This process learns a
high-dimensional embedding feature representation for each fiber, regardless of
the order of fiber points reconstructed during tractography. We design a novel
network architecture that represents input fibers as point clouds and allows
the incorporation of additional sources of input information from gray matter
parcellation to improve anatomical coherence of clusters. In addition, DFC
conducts outlier removal naturally by rejecting fibers with low cluster
assignment probability. We evaluate DFC on three independently acquired
cohorts, including data from 220 individuals across genders, ages (young and
elderly adults), and different health conditions (healthy control and multiple
neuropsychiatric disorders). We compare DFC to several state-of-the-art white
matter fiber clustering algorithms. Experimental results demonstrate superior
performance of DFC in terms of cluster compactness, generalization ability,
anatomical coherence, and computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuqian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chaoyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tengfei Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makris_N/0/1/0/all/0/1&quot;&gt;Nikos Makris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathi_Y/0/1/0/all/0/1&quot;&gt;Yogesh Rathi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ODonnell_L/0/1/0/all/0/1&quot;&gt;Lauren J. O&amp;#x27;Donnell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04530">
<title>DORA: Exploring Outlier Representations in Deep Neural Networks. (arXiv:2206.04530v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04530</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) excel at learning complex abstractions within
their internal representations. However, the concepts they learn remain opaque,
a problem that becomes particularly acute when models unintentionally learn
spurious correlations. In this work, we present DORA (Data-agnOstic
Representation Analysis), the first data-agnostic framework for analyzing the
representational space of DNNs. Central to our framework is the proposed
Extreme-Activation (EA) distance measure, which assesses similarities between
representations by analyzing their activation patterns on data points that
cause the highest level of activation. As spurious correlations often manifest
in features of data that are anomalous to the desired task, such as watermarks
or artifacts, we demonstrate that internal representations capable of detecting
such artifactual concepts can be found by analyzing relationships within neural
representations. We validate the EA metric quantitatively, demonstrating its
effectiveness both in controlled scenarios and real-world applications.
Finally, we provide practical examples from popular Computer Vision models to
illustrate that representations identified as outliers using the EA metric
often correspond to undesired and spurious concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1&quot;&gt;Kirill Bykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deb_M/0/1/0/all/0/1&quot;&gt;Mayukh Deb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grinwald_D/0/1/0/all/0/1&quot;&gt;Dennis Grinwald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1&quot;&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.01996">
<title>Adaptive Domain Generalization via Online Disagreement Minimization. (arXiv:2208.01996v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.01996</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks suffer from significant performance deterioration when
there exists distribution shift between deployment and training. Domain
Generalization (DG) aims to safely transfer a model to unseen target domains by
only relying on a set of source domains. Although various DG approaches have
been proposed, a recent study named DomainBed, reveals that most of them do not
beat the simple Empirical Risk Minimization (ERM). To this end, we propose a
general framework that is orthogonal to existing DG algorithms and could
improve their performance consistently. Unlike previous DG works that stake on
a static source model to be hopefully a universal one, our proposed AdaODM
adaptively modifies the source model at test time for different target domains.
Specifically, we create multiple domain-specific classifiers upon a shared
domain-generic feature extractor. The feature extractor and classifiers are
trained in an adversarial way, where the feature extractor embeds the input
samples into a domain-invariant space, and the multiple classifiers capture the
distinct decision boundaries that each of them relates to a specific source
domain. During testing, distribution differences between target and source
domains could be effectively measured by leveraging prediction disagreement
among source classifiers. By fine-tuning source models to minimize the
disagreement at test time, target domain features are well aligned to the
invariant feature space. We verify AdaODM on two popular DG methods, namely ERM
and CORAL, and four DG benchmarks, namely VLCS, PACS, OfficeHome, and
TerraIncognita. The results show AdaODM stably improves the generalization
capacity on unseen domains and achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-Cong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10967">
<title>The Value of Out-of-Distribution Data. (arXiv:2208.10967v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10967</link>
<description rdf:parseType="Literal">&lt;p&gt;We expect the generalization error to improve with more samples from a
similar task, and to deteriorate with more samples from an out-of-distribution
(OOD) task. In this work, we show a counter-intuitive phenomenon: the
generalization error of a task can be a non-monotonic function of the number of
OOD samples. As the number of OOD samples increases, the generalization error
on the target task improves before deteriorating beyond a threshold. In other
words, there is value in training on small amounts of OOD data. We use Fisher&apos;s
Linear Discriminant on synthetic datasets and deep networks on computer vision
benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate
and analyze this phenomenon. In the idealistic setting where we know which
samples are OOD, we show that these non-monotonic trends can be exploited using
an appropriately weighted objective of the target and OOD empirical risk. While
its practical utility is limited, this does suggest that if we can detect OOD
samples, then there may be ways to benefit from them. When we do not know which
samples are OOD, we show how a number of go-to strategies such as
data-augmentation, hyper-parameter optimization, and pre-training are not
enough to ensure that the target generalization error does not deteriorate with
the number of OOD samples in the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Ashwin De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1&quot;&gt;Rahul Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.12306">
<title>Multimedia Generative Script Learning for Task Planning. (arXiv:2208.12306v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2208.12306</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal-oriented generative script learning aims to generate subsequent steps to
reach a particular goal, which is an essential task to assist robots or humans
in performing stereotypical activities. An important aspect of this process is
the ability to capture historical states visually, which provides detailed
information that is not covered by text and will guide subsequent steps.
Therefore, we propose a new task, Multimedia Generative Script Learning, to
generate subsequent steps by tracking historical states in both text and vision
modalities, as well as presenting the first benchmark containing 5,652 tasks
and 79,089 multimedia steps. This task is challenging in three aspects: the
multimedia challenge of capturing the visual states in images, the induction
challenge of performing unseen tasks, and the diversity challenge of covering
different information in individual steps. We propose to encode visual state
changes through a selective multimedia encoder to address the multimedia
challenge, transfer knowledge from previously observed tasks using a
retrieval-augmented decoder to overcome the induction challenge, and further
present distinct information at each step by optimizing a diversity-oriented
contrastive learning objective. We define metrics to evaluate both generation
and inductive quality. Experiment results demonstrate that our approach
significantly outperforms strong baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Manling Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Hou Pong Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lifu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hockenmaier_J/0/1/0/all/0/1&quot;&gt;Julia Hockenmaier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhary_G/0/1/0/all/0/1&quot;&gt;Girish Chowdhary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02370">
<title>Continual Learning, Fast and Slow. (arXiv:2209.02370v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02370</link>
<description rdf:parseType="Literal">&lt;p&gt;According to the Complementary Learning Systems (CLS)
theory~\cite{mcclelland1995there} in neuroscience, humans do effective
\emph{continual learning} through two complementary systems: a fast learning
system centered on the hippocampus for rapid learning of the specifics,
individual experiences; and a slow learning system located in the neocortex for
the gradual acquisition of structured knowledge about the environment.
Motivated by this theory, we propose \emph{DualNets} (for Dual Networks), a
general continual learning framework comprising a fast learning system for
supervised learning of pattern-separated representation from specific tasks and
a slow learning system for representation learning of task-agnostic general
representation via Self-Supervised Learning (SSL). DualNets can seamlessly
incorporate both representation types into a holistic framework to facilitate
better continual learning in deep neural networks. Via extensive experiments,
we demonstrate the promising results of DualNets on a wide range of continual
learning protocols, ranging from the standard offline, task-aware setting to
the challenging online, task-free scenario. Notably, on the
CTrL~\cite{veniat2020efficient} benchmark that has unrelated tasks with vastly
different visual images, DualNets can achieve competitive performance with
existing state-of-the-art dynamic architecture
strategies~\cite{ostapenko2021continual}. Furthermore, we conduct comprehensive
ablation studies to validate DualNets efficacy, robustness, and scalability.
Code will be made available at \url{https://github.com/phquang/DualNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1&quot;&gt;Quang Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoi_S/0/1/0/all/0/1&quot;&gt;Steven C. H. Hoi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02905">
<title>A Data-dependent Approach for High Dimensional (Robust) Wasserstein Alignment. (arXiv:2209.02905v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02905</link>
<description rdf:parseType="Literal">&lt;p&gt;Many real-world problems can be formulated as the alignment between two
geometric patterns. Previously, a great amount of research focus on the
alignment of 2D or 3D patterns in the field of computer vision. Recently, the
alignment problem in high dimensions finds several novel applications in
practice. However, the research is still rather limited in the algorithmic
aspect. To the best of our knowledge, most existing approaches are just simple
extensions of their counterparts for 2D and 3D cases, and often suffer from the
issues such as high computational complexities. In this paper, we propose an
effective framework to compress the high dimensional geometric patterns. Any
existing alignment method can be applied to the compressed geometric patterns
and the time complexity can be significantly reduced. Our idea is inspired by
the observation that high dimensional data often has a low intrinsic dimension.
Our framework is a ``data-dependent&apos;&apos; approach that has the complexity
depending on the intrinsic dimension of the input data. Our experimental
results reveal that running the alignment algorithm on compressed patterns can
achieve similar qualities, comparing with the results on the original patterns,
but the runtimes (including the times cost for compression) are substantially
lower.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_M/0/1/0/all/0/1&quot;&gt;Mingquan Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.14645">
<title>Super-Resolution Based Patch-Free 3D Image Segmentation with High-Frequency Guidance. (arXiv:2210.14645v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.14645</link>
<description rdf:parseType="Literal">&lt;p&gt;High resolution (HR) 3D images are widely used nowadays, such as medical
images like Magnetic Resonance Imaging (MRI) and Computed Tomography (CT).
However, segmentation of these 3D images remains a challenge due to their high
spatial resolution and dimensionality in contrast to currently limited GPU
memory. Therefore, most existing 3D image segmentation methods use patch-based
models, which have low inference efficiency and ignore global contextual
information. To address these problems, we propose a super-resolution (SR)
based patch-free 3D image segmentation framework that can realize HR
segmentation from a global-wise low-resolution (LR) input. The framework
contains two sub-tasks, of which semantic segmentation is the main task and
super resolution is an auxiliary task aiding in rebuilding the high frequency
information from the LR input. To furthermore balance the information loss with
the LR input, we propose a High-Frequency Guidance Module (HGM), and design an
efficient selective cropping algorithm to crop an HR patch from the original
image as restoration guidance for it. In addition, we also propose a
Task-Fusion Module (TFM) to exploit the inter connections between segmentation
and SR task, realizing joint optimization of the two tasks. When predicting,
only the main segmentation task is needed, while other modules can be removed
for acceleration. The experimental results on two different datasets show that
our framework has a four times higher inference speed compared to traditional
patch-based methods, while its performance also surpasses other patch-based and
patch-free models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lanfen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hongjie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qingqing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iwamoto_Y/0/1/0/all/0/1&quot;&gt;Yutaro Iwamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xian-Hua Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tong_R/0/1/0/all/0/1&quot;&gt;Ruofeng Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.01735">
<title>Neural Fourier Filter Bank. (arXiv:2212.01735v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.01735</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method to provide efficient and highly detailed
reconstructions. Inspired by wavelets, we learn a neural field that decompose
the signal both spatially and frequency-wise. We follow the recent grid-based
paradigm for spatial decomposition, but unlike existing work, encourage
specific frequencies to be stored in each grid via Fourier features encodings.
We then apply a multi-layer perceptron with sine activations, taking these
Fourier encoded features in at appropriate layers so that higher-frequency
components are accumulated on top of lower-frequency components sequentially,
which we sum up to form the final output. We demonstrate that our method
outperforms the state of the art regarding model compactness and convergence
speed on multiple tasks: 2D image fitting, 3D shape reconstruction, and neural
radiance fields. Our code is available at https://github.com/ubc-vision/NFFB.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhijie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yuhe Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_K/0/1/0/all/0/1&quot;&gt;Kwang Moo Yi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04108">
<title>Shadow Removal by High-Quality Shadow Synthesis. (arXiv:2212.04108v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04108</link>
<description rdf:parseType="Literal">&lt;p&gt;Most shadow removal methods rely on the invasion of training images
associated with laborious and lavish shadow region annotations, leading to the
increasing popularity of shadow image synthesis. However, the poor performance
also stems from these synthesized images since they are often
shadow-inauthentic and details-impaired. In this paper, we present a novel
generation framework, referred to as HQSS, for high-quality pseudo shadow image
synthesis. The given image is first decoupled into a shadow region identity and
a non-shadow region identity. HQSS employs a shadow feature encoder and a
generator to synthesize pseudo images. Specifically, the encoder extracts the
shadow feature of a region identity which is then paired with another region
identity to serve as the generator input to synthesize a pseudo image. The
pseudo image is expected to have the shadow feature as its input shadow feature
and as well as a real-like image detail as its input region identity. To
fulfill this goal, we design three learning objectives. When the shadow feature
and input region identity are from the same region identity, we propose a
self-reconstruction loss that guides the generator to reconstruct an identical
pseudo image as its input. When the shadow feature and input region identity
are from different identities, we introduce an inter-reconstruction loss and a
cycle-reconstruction loss to make sure that shadow characteristics and detail
information can be well retained in the synthesized images. Our HQSS is
observed to outperform the state-of-the-art methods on ISTD dataset, Video
Shadow Removal dataset, and SRD dataset. The code is available at
https://github.com/zysxmu/HQSS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yunshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_L/0/1/0/all/0/1&quot;&gt;Lizhou You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1&quot;&gt;Fei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08334">
<title>Lightweight integration of 3D features to improve 2D image segmentation. (arXiv:2212.08334v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08334</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene understanding has made tremendous progress over the past few years, as
data acquisition systems are now providing an increasing amount of data of
various modalities (point cloud, depth, RGB...). However, this improvement
comes at a large cost on computation resources and data annotation
requirements. To analyze geometric information and images jointly, many
approaches rely on both a 2D loss and 3D loss, requiring not only 2D per
pixel-labels but also 3D per-point labels. However, obtaining a 3D groundtruth
is challenging, time-consuming and error-prone. In this paper, we show that
image segmentation can benefit from 3D geometric information without requiring
a 3D groundtruth, by training the geometric feature extraction and the 2D
segmentation network jointly, in an end-to-end fashion, using only the 2D
segmentation loss. Our method starts by extracting a map of 3D features
directly from a provided point cloud by using a lightweight 3D neural network.
The 3D feature map, merged with the RGB image, is then used as an input to a
classical image segmentation network. Our method can be applied to many 2D
segmentation networks, improving significantly their performance with only a
marginal network weight increase and light input dataset requirements, since no
3D groundtruth is required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pradelle_O/0/1/0/all/0/1&quot;&gt;Olivier Pradelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaine_R/0/1/0/all/0/1&quot;&gt;Raphaelle Chaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wendland_D/0/1/0/all/0/1&quot;&gt;David Wendland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Digne_J/0/1/0/all/0/1&quot;&gt;Julie Digne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.07354">
<title>MADAv2: Advanced Multi-Anchor Based Active Domain Adaptation Segmentation. (arXiv:2301.07354v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.07354</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaption has been widely adopted in tasks with scarce
annotated data. Unfortunately, mapping the target-domain distribution to the
source-domain unconditionally may distort the essential structural information
of the target-domain data, leading to inferior performance. To address this
issue, we firstly propose to introduce active sample selection to assist domain
adaptation regarding the semantic segmentation task. By innovatively adopting
multiple anchors instead of a single centroid, both source and target domains
can be better characterized as multimodal distributions, in which way more
complementary and informative samples are selected from the target domain. With
only a little workload to manually annotate these active samples, the
distortion of the target-domain distribution can be effectively alleviated,
achieving a large performance gain. In addition, a powerful semi-supervised
domain adaptation strategy is proposed to alleviate the long-tail distribution
problem and further improve the segmentation performance. Extensive experiments
are conducted on public datasets, and the results demonstrate that the proposed
approach outperforms state-of-the-art methods by large margins and achieves
similar performance to the fully-supervised upperbound, i.e., 71.4% mIoU on
GTA5 and 71.8% mIoU on SYNTHIA. The effectiveness of each component is also
verified by thorough ablation studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1&quot;&gt;Donghuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yujia Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dongdong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_D/0/1/0/all/0/1&quot;&gt;Dong Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yefeng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13359">
<title>IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13359</link>
<description rdf:parseType="Literal">&lt;p&gt;Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently many advanced algorithms have been
published, but their performance deviates greatly. We realize that the lack of
actual IM settings most probably hinders the development and usage of these
methods in real-world applications. As far as we know, IAD methods are not
evaluated systematically. As a result, this makes it difficult for researchers
to analyze them because they are designed for different or special cases. To
solve this problem, we first propose a uniform IM setting to assess how well
these algorithms perform, which includes several aspects, i.e., various levels
of supervision (unsupervised vs. semi-supervised), few-shot learning, continual
learning, noisy labels, memory usage, and inference speed. Moreover, we
skillfully build a comprehensive image anomaly detection benchmark (IM-IAD)
that includes 16 algorithms on 7 mainstream datasets with uniform settings. Our
extensive experiments (17,017 in total) provide in-depth insights for IAD
algorithm redesign or selection under the IM setting. Next, the proposed
benchmark IM-IAD gives challenges as well as directions for the future. To
foster reproducibility and accessibility, the source code of IM-IAD is uploaded
on the website, https://github.com/M-3LAB/IM-IAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiayi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10473">
<title>Oriented Object Detection in Optical Remote Sensing Images using Deep Learning: A Survey. (arXiv:2302.10473v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10473</link>
<description rdf:parseType="Literal">&lt;p&gt;Oriented object detection is one of the most fundamental and challenging
tasks in remote sensing, aiming at locating the oriented objects of numerous
predefined object categories. Recently, deep learning based methods have
achieved remarkable performance in detecting oriented objects in optical remote
sensing imagery. However, a thorough review of the literature in remote sensing
has not yet emerged. Therefore, we give a comprehensive survey of recent
advances and cover many aspects of oriented object detection, including problem
definition, commonly used datasets, evaluation protocols, detection frameworks,
oriented object representations, and feature representations. Besides, the
state-of-the-art methods are analyzed and discussed. We finally discuss future
research directions to put forward some useful research guidance. We believe
that this survey shall be valuable to researchers across academia and industry
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_A/0/1/0/all/0/1&quot;&gt;Ang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_X/0/1/0/all/0/1&quot;&gt;Xichao Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qifeng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02401">
<title>Open-Vocabulary Affordance Detection in 3D Point Clouds. (arXiv:2303.02401v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02401</link>
<description rdf:parseType="Literal">&lt;p&gt;Affordance detection is a challenging problem with a wide variety of robotic
applications. Traditional affordance detection methods are limited to a
predefined set of affordance labels, hence potentially restricting the
adaptability of intelligent robots in complex and dynamic environments. In this
paper, we present the Open-Vocabulary Affordance Detection (OpenAD) method,
which is capable of detecting an unbounded number of affordances in 3D point
clouds. By simultaneously learning the affordance text and the point feature,
OpenAD successfully exploits the semantic relationships between affordances.
Therefore, our proposed method enables zero-shot detection and can be able to
detect previously unseen affordances without a single annotation example.
Intensive experimental results show that OpenAD works effectively on a wide
range of affordance detection setups and outperforms other baselines by a large
margin. Additionally, we demonstrate the practicality of the proposed OpenAD in
real-world robotic applications with a fast inference speed (~100ms). Our
project is available at https://openad2023.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Toan Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_M/0/1/0/all/0/1&quot;&gt;Minh Nhat Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_A/0/1/0/all/0/1&quot;&gt;An Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dzung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vo_T/0/1/0/all/0/1&quot;&gt;Thieu Vo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Ngan Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_A/0/1/0/all/0/1&quot;&gt;Anh Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07308">
<title>NeuSE: Neural SE(3)-Equivariant Embedding for Consistent Spatial Understanding with Objects. (arXiv:2303.07308v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07308</link>
<description rdf:parseType="Literal">&lt;p&gt;We present NeuSE, a novel Neural SE(3)-Equivariant Embedding for objects, and
illustrate how it supports object SLAM for consistent spatial understanding
with long-term scene changes. NeuSE is a set of latent object embeddings
created from partial object observations. It serves as a compact point cloud
surrogate for complete object models, encoding full shape information while
transforming SE(3)-equivariantly in tandem with the object in the physical
world. With NeuSE, relative frame transforms can be directly derived from
inferred latent codes. Our proposed SLAM paradigm, using NeuSE for object shape
and pose characterization, can operate independently or in conjunction with
typical SLAM systems. It directly infers SE(3) camera pose constraints that are
compatible with general SLAM pose graph optimization, while also maintaining a
lightweight object-centric map that adapts to real-world changes. Our approach
is evaluated on synthetic and real-world sequences featuring changed objects
and shows improved localization accuracy and change-aware mapping capability,
when working either standalone or jointly with a common SLAM pipeline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jiahui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Kurran Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonard_J/0/1/0/all/0/1&quot;&gt;John J. Leonard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08061">
<title>Point Cloud Diffusion Models for Automatic Implant Generation. (arXiv:2303.08061v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08061</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in 3D printing of biocompatible materials make patient-specific
implants increasingly popular. The design of these implants is, however, still
a tedious and largely manual process. Existing approaches to automate implant
generation are mainly based on 3D U-Net architectures on downsampled or
patch-wise data, which can result in a loss of detail or contextual
information. Following the recent success of Diffusion Probabilistic Models, we
propose a novel approach for implant generation based on a combination of 3D
point cloud diffusion models and voxelization networks. Due to the stochastic
sampling process in our diffusion model, we can propose an ensemble of
different implants per defect, from which the physicians can choose the most
suitable one. We evaluate our method on the SkullBreak and SkullFix datasets,
generating high-quality implants and achieving competitive evaluation scores.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Friedrich_P/0/1/0/all/0/1&quot;&gt;Paul Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wolleb_J/0/1/0/all/0/1&quot;&gt;Julia Wolleb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bieder_F/0/1/0/all/0/1&quot;&gt;Florentin Bieder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thieringer_F/0/1/0/all/0/1&quot;&gt;Florian M. Thieringer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cattin_P/0/1/0/all/0/1&quot;&gt;Philippe C. Cattin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10834">
<title>Object-Centric Slot Diffusion. (arXiv:2303.10834v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10834</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent success of transformer-based image generative models in
object-centric learning highlights the importance of powerful image generators
for handling complex scenes. However, despite the high expressiveness of
diffusion models in image generation, their integration into object-centric
learning remains largely unexplored in this domain. In this paper, we explore
the feasibility and potential of integrating diffusion models into
object-centric learning and investigate the pros and cons of this approach. We
introduce Latent Slot Diffusion (LSD), a novel model that serves dual purposes:
it is the first object-centric learning model to replace conventional slot
decoders with a latent diffusion model conditioned on object slots, and it is
also the first unsupervised compositional conditional diffusion model that
operates without the need for supervised annotations like text. Through
experiments on various object-centric tasks, including the first application of
the FFHQ dataset in this field, we demonstrate that LSD significantly
outperforms state-of-the-art transformer-based decoders, particularly in more
complex scenes, and exhibits superior unsupervised compositional generation
quality. Project page is available at
$\href{https://latentslotdiffusion.github.io}{here}$
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jindong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_F/0/1/0/all/0/1&quot;&gt;Fei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_G/0/1/0/all/0/1&quot;&gt;Gautam Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Sungjin Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14773">
<title>BlackVIP: Black-Box Visual Prompting for Robust Transfer Learning. (arXiv:2303.14773v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14773</link>
<description rdf:parseType="Literal">&lt;p&gt;With the surge of large-scale pre-trained models (PTMs), fine-tuning these
models to numerous downstream tasks becomes a crucial problem. Consequently,
parameter efficient transfer learning (PETL) of large models has grasped huge
attention. While recent PETL methods showcase impressive performance, they rely
on optimistic assumptions: 1) the entire parameter set of a PTM is available,
and 2) a sufficiently large memory capacity for the fine-tuning is equipped.
However, in most real-world applications, PTMs are served as a black-box API or
proprietary software without explicit parameter accessibility. Besides, it is
hard to meet a large memory requirement for modern PTMs. In this work, we
propose black-box visual prompting (BlackVIP), which efficiently adapts the
PTMs without knowledge about model architectures and parameters. BlackVIP has
two components; 1) Coordinator and 2) simultaneous perturbation stochastic
approximation with gradient correction (SPSA-GC). The Coordinator designs
input-dependent image-shaped visual prompts, which improves few-shot adaptation
and robustness on distribution/location shift. SPSA-GC efficiently estimates
the gradient of a target model to update Coordinator. Extensive experiments on
16 datasets demonstrate that BlackVIP enables robust adaptation to diverse
domains without accessing PTMs&apos; parameters, with minimal memory requirements.
Code: \url{https://github.com/changdaeoh/BlackVIP}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_C/0/1/0/all/0/1&quot;&gt;Changdae Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Hyeji Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hee-young Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_Y/0/1/0/all/0/1&quot;&gt;YongTaek Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_G/0/1/0/all/0/1&quot;&gt;Geunyoung Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_J/0/1/0/all/0/1&quot;&gt;Jiyoung Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hosik Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_K/0/1/0/all/0/1&quot;&gt;Kyungwoo Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16897">
<title>Physics-Driven Diffusion Models for Impact Sound Synthesis from Videos. (arXiv:2303.16897v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16897</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling sounds emitted from physical object interactions is critical for
immersive perceptual experiences in real and virtual worlds. Traditional
methods of impact sound synthesis use physics simulation to obtain a set of
physics parameters that could represent and synthesize the sound. However, they
require fine details of both the object geometries and impact locations, which
are rarely available in the real world and can not be applied to synthesize
impact sounds from common videos. On the other hand, existing video-driven deep
learning-based approaches could only capture the weak correspondence between
visual content and impact sounds since they lack of physics knowledge. In this
work, we propose a physics-driven diffusion model that can synthesize
high-fidelity impact sound for a silent video clip. In addition to the video
content, we propose to use additional physics priors to guide the impact sound
synthesis procedure. The physics priors include both physics parameters that
are directly estimated from noisy real-world impact sound examples without
sophisticated setup and learned residual parameters that interpret the sound
environment via neural networks. We further implement a novel diffusion model
with specific training and inference strategies to combine physics priors and
visual information for impact sound synthesis. Experimental results show that
our model outperforms several existing systems in generating realistic impact
sounds. More importantly, the physics-based representations are fully
interpretable and transparent, thus enabling us to perform sound editing
flexibly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kun Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_K/0/1/0/all/0/1&quot;&gt;Kaizhi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlizerman_E/0/1/0/all/0/1&quot;&gt;Eli Shlizerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_C/0/1/0/all/0/1&quot;&gt;Chuang Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01354">
<title>Functional Knowledge Transfer with Self-supervised Representation Learning. (arXiv:2304.01354v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01354</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the unexplored usability of self-supervised
representation learning in the direction of functional knowledge transfer. In
this work, functional knowledge transfer is achieved by joint optimization of
self-supervised learning pseudo task and supervised learning task, improving
supervised learning task performance. Recent progress in self-supervised
learning uses a large volume of data, which becomes a constraint for its
applications on small-scale datasets. This work shares a simple yet effective
joint training framework that reinforces human-supervised task learning by
learning self-supervised representations just-in-time and vice versa.
Experiments on three public datasets from different visual domains, Intel
Image, CIFAR, and APTOS, reveal a consistent track of performance improvements
on classification tasks during joint optimization. Qualitative analysis also
supports the robustness of learnt representations. Source code and trained
models are available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chhipa_P/0/1/0/all/0/1&quot;&gt;Prakash Chandra Chhipa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chopra_M/0/1/0/all/0/1&quot;&gt;Muskaan Chopra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mengi_G/0/1/0/all/0/1&quot;&gt;Gopal Mengi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_V/0/1/0/all/0/1&quot;&gt;Varun Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_R/0/1/0/all/0/1&quot;&gt;Richa Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chippa_M/0/1/0/all/0/1&quot;&gt;Meenakshi Subhash Chippa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_K/0/1/0/all/0/1&quot;&gt;Kanjar De&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saini_R/0/1/0/all/0/1&quot;&gt;Rajkumar Saini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uchida_S/0/1/0/all/0/1&quot;&gt;Seiichi Uchida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liwicki_M/0/1/0/all/0/1&quot;&gt;Marcus Liwicki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04116">
<title>Marginal Thresholding in Noisy Image Segmentation. (arXiv:2304.04116v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04116</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents a study on label noise in medical image segmentation by
considering a noise model based on Gaussian field deformations. Such noise is
of interest because it yields realistic looking segmentations and because it is
unbiased in the sense that the expected deformation is the identity mapping.
Efficient methods for sampling and closed form solutions for the marginal
probabilities are provided. Moreover, theoretically optimal solutions to the
loss functions cross-entropy and soft-Dice are studied and it is shown how they
diverge as the level of noise increases. Based on recent work on loss function
characterization, it is shown that optimal solutions to soft-Dice can be
recovered by thresholding solutions to cross-entropy with a particular a priori
unknown threshold that efficiently can be computed. This raises the question
whether the decrease in performance seen when using cross-entropy as compared
to soft-Dice is caused by using the wrong threshold. The hypothesis is
validated in 5-fold studies on three organ segmentation problems from the
TotalSegmentor data set, using 4 different strengths of noise. The results show
that changing the threshold leads the performance of cross-entropy to go from
systematically worse than soft-Dice to similar or better results than
soft-Dice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nordstrom_M/0/1/0/all/0/1&quot;&gt;Marcus Nordstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hult_H/0/1/0/all/0/1&quot;&gt;Henrik Hult&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maki_A/0/1/0/all/0/1&quot;&gt;Atsuto Maki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04589">
<title>Hyperspectral Image Super-Resolution via Dual-domain Network Based on Hybrid Convolution. (arXiv:2304.04589v8 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04589</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the number of incident energies is limited, it is difficult to directly
acquire hyperspectral images (HSI) with high spatial resolution. Considering
the high dimensionality and correlation of HSI, super-resolution (SR) of HSI
remains a challenge in the absence of auxiliary high-resolution images.
Furthermore, it is very important to extract the spatial features effectively
and make full use of the spectral information. This paper proposes a novel HSI
super-resolution algorithm, termed dual-domain network based on hybrid
convolution (SRDNet). Specifically, a dual-domain network is designed to fully
exploit the spatial-spectral and frequency information among the hyper-spectral
data. To capture inter-spectral self-similarity, a self-attention learning
mechanism (HSL) is devised in the spatial domain. Meanwhile the pyramid
structure is applied to increase the acceptance field of attention, which
further reinforces the feature representation ability of the network. Moreover,
to further improve the perceptual quality of HSI, a frequency loss(HFL) is
introduced to optimize the model in the frequency domain. The dynamic weighting
mechanism drives the network to gradually refine the generated frequency and
excessive smoothing caused by spatial loss. Finally, In order to better fully
obtain the mapping relationship between high-resolution space and
low-resolution space, a hybrid module of 2D and 3D units with progressive
upsampling strategy is utilized in our method. Experiments on a widely used
benchmark dataset illustrate that the proposed SRDNet method enhances the
texture information of HSI and is superior to state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tingting Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuncheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liyin_Y/0/1/0/all/0/1&quot;&gt;Yuan Liyin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_X/0/1/0/all/0/1&quot;&gt;Xiubao Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qian Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08978">
<title>Visual-LiDAR Odometry and Mapping with Monocular Scale Correction and Visual Bootstrapping. (arXiv:2304.08978v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08978</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel visual-LiDAR odometry and mapping method with
low-drift characteristics. The proposed method is based on two popular
approaches, ORB-SLAM and A-LOAM, with monocular scale correction and
visual-bootstrapped LiDAR poses initialization modifications. The scale
corrector calculates the proportion between the depth of image keypoints
recovered by triangulation and that provided by LiDAR, using an outlier
rejection process for accuracy improvement. Concerning LiDAR poses
initialization, the visual odometry approach gives the initial guesses of LiDAR
motions for better performance. This methodology is not only applicable to
high-resolution LiDAR but can also adapt to low-resolution LiDAR. To evaluate
the proposed SLAM system&apos;s robustness and accuracy, we conducted experiments on
the KITTI Odometry and S3E datasets. Experimental results illustrate that our
method significantly outperforms standalone ORB-SLAM2 and A-LOAM. Furthermore,
regarding the accuracy of visual odometry with scale correction, our method
performs similarly to the stereo-mode ORB-SLAM2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hanyu Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_N/0/1/0/all/0/1&quot;&gt;Ni Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junzheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11857">
<title>Accurate and Efficient Event-based Semantic Segmentation Using Adaptive Spiking Encoder-Decoder Network. (arXiv:2304.11857v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11857</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging the low-power, event-driven computation and the inherent temporal
dynamics, spiking neural networks (SNNs) are potentially ideal solutions for
processing dynamic and asynchronous signals from event-based sensors. However,
due to the challenges in training and the restrictions in architectural design,
there are limited examples of competitive SNNs in the realm of event-based
dense prediction when compared to artificial neural networks (ANNs). In this
paper, we present an efficient spiking encoder-decoder network designed for
large-scale event-based semantic segmentation tasks. This is achieved by
optimizing the encoder using a hierarchical search method. To enhance learning
from dynamic event streams, we harness the inherent adaptive threshold of
spiking neurons to modulate network activation. Moreover, we introduce a
dual-path Spiking Spatially-Adaptive Modulation (SSAM) block, specifically
designed to enhance the representation of sparse events, thereby considerably
improving network performance. Our proposed network achieves a 72.57% mean
intersection over union (MIoU) on the DDD17 dataset and a 57.22% MIoU on the
recently introduced, larger DSEC-Semantic dataset. This performance surpasses
the current state-of-the-art ANNs by 4%, whilst consuming significantly less
computational resources. To the best of our knowledge, this is the first study
demonstrating SNNs outperforming ANNs in demanding event-based semantic
segmentation tasks, thereby establishing the vast potential of SNNs in the
field of event-based vision. Our source code will be made publicly accessible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leng_L/0/1/0/all/0/1&quot;&gt;Luziwei Leng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_K/0/1/0/all/0/1&quot;&gt;Kaiwei Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jie Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qinghai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_J/0/1/0/all/0/1&quot;&gt;Jiangxing Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_R/0/1/0/all/0/1&quot;&gt;Ran Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12652">
<title>Hybrid Neural Rendering for Large-Scale Scenes with Motion Blur. (arXiv:2304.12652v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12652</link>
<description rdf:parseType="Literal">&lt;p&gt;Rendering novel view images is highly desirable for many applications.
Despite recent progress, it remains challenging to render high-fidelity and
view-consistent novel views of large-scale scenes from in-the-wild images with
inevitable artifacts (e.g., motion blur). To this end, we develop a hybrid
neural rendering model that makes image-based representation and neural 3D
representation join forces to render high-quality, view-consistent images.
Besides, images captured in the wild inevitably contain artifacts, such as
motion blur, which deteriorates the quality of rendered images. Accordingly, we
propose strategies to simulate blur effects on the rendered images to mitigate
the negative influence of blurriness images and reduce their importance during
training based on precomputed quality-aware weights. Extensive experiments on
real and synthetic data demonstrate our model surpasses state-of-the-art
point-based methods for novel view synthesis. The code is available at
https://daipengwa.github.io/Hybrid-Rendering-ProjectPage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_P/0/1/0/all/0/1&quot;&gt;Peng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinda Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00594">
<title>The MCC approaches the geometric mean of precision and recall as true negatives approach infinity. (arXiv:2305.00594v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.00594</link>
<description rdf:parseType="Literal">&lt;p&gt;The performance of a binary classifier is described by a confusion matrix
with four entries: the number of true positives (TP), true negatives (TN),
false positives (FP), and false negatives (FN).
&lt;/p&gt;
&lt;p&gt;The Matthew&apos;s Correlation Coefficient (MCC), F1, and Fowlkes--Mallows (FM)
scores are scalars that summarize a confusion matrix. Both the F1 and FM scores
are based on only three of the four entries in the confusion matrix (they
ignore TN). In contrast, the MCC takes into account all four entries of the
confusion matrix and thus can be seen as providing a more representative
picture.
&lt;/p&gt;
&lt;p&gt;However, in object detection problems, measuring the number of true negatives
is so large it is often intractable. Thus we ask, what happens to the MCC as
the number of true negatives approaches infinity? This paper provides insight
into the relationship between the MCC and FM score by proving that the
FM-measure is equal to the limit of the MCC as the number of true negatives
approaches infinity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crall_J/0/1/0/all/0/1&quot;&gt;Jon Crall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02693">
<title>Semi-supervised Domain Adaptation via Prototype-based Multi-level Learning. (arXiv:2305.02693v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02693</link>
<description rdf:parseType="Literal">&lt;p&gt;In semi-supervised domain adaptation (SSDA), a few labeled target samples of
each class help the model to transfer knowledge representation from the fully
labeled source domain to the target domain. Many existing methods ignore the
benefits of making full use of the labeled target samples from multi-level. To
make better use of this additional data, we propose a novel Prototype-based
Multi-level Learning (ProML) framework to better tap the potential of labeled
target samples. To achieve intra-domain adaptation, we first introduce a
pseudo-label aggregation based on the intra-domain optimal transport to help
the model align the feature distribution of unlabeled target samples and the
prototype. At the inter-domain level, we propose a cross-domain alignment loss
to help the model use the target prototype for cross-domain knowledge transfer.
We further propose a dual consistency based on prototype similarity and linear
classifier to promote discriminative learning of compact target feature
representation at the batch level. Extensive experiments on three datasets,
including DomainNet, VisDA2017, and Office-Home demonstrate that our proposed
method achieves state-of-the-art performance in SSDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xinyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chuang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenkai Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04526">
<title>CrAFT: Compression-Aware Fine-Tuning for Efficient Visual Task Adaptation. (arXiv:2305.04526v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04526</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning has become a popular task adaptation method in the era of
foundation models. However, many foundation models require large storage and
computing resources, which makes off-the-shelf deployment impractical.
Post-training compression techniques such as pruning and quantization can help
lower deployment costs. Unfortunately, the resulting performance degradation
limits the usability and benefits of such techniques. To close this performance
gap, we propose CrAFT, a simple fine-tuning framework that enables effective
post-training network compression. In CrAFT, users simply employ the default
fine-tuning schedule along with sharpness minimization objective,
simultaneously facilitating task adaptation and compression-friendliness.
Contrary to the conventional sharpness minimization techniques, which are
applied during pretraining, the CrAFT approach adds negligible training
overhead as fine-tuning is done in under a couple of minutes or hours with a
single GPU. The effectiveness of CrAFT, which is a general-purpose tool that
can significantly boost one-shot pruning and post-training quantization, is
demonstrated on both convolution-based and attention-based vision foundation
models on a variety of target tasks. The code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_J/0/1/0/all/0/1&quot;&gt;Jung Hwan Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1&quot;&gt;Seyedarmin Azizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayyazi_A/0/1/0/all/0/1&quot;&gt;Arash Fayyazi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedram_M/0/1/0/all/0/1&quot;&gt;Massoud Pedram&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.04743">
<title>MARS: Mask Attention Refinement with Sequential Quadtree Nodes for Car Damage Instance Segmentation. (arXiv:2305.04743v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.04743</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating car damages from misfortune is critical to the car insurance
industry. However, the accuracy is still insufficient for real-world
applications since the deep learning network is not designed for car damage
images as inputs, and its segmented masks are still very coarse. This paper
presents MARS (Mask Attention Refinement with Sequential quadtree nodes) for
car damage instance segmentation. Our MARS represents self-attention mechanisms
to draw global dependencies between the sequential quadtree nodes layer and
quadtree transformer to recalibrate channel weights and predict highly accurate
instance masks. Our extensive experiments demonstrate that MARS outperforms
state-of-the-art (SOTA) instance segmentation methods on three popular
benchmarks such as Mask R-CNN [9], PointRend [13], and Mask Transfiner [12], by
a large margin of +1.3 maskAP-based R50-FPN backbone and +2.3 maskAP-based
R101-FPN backbone on Thai car-damage dataset. Our demos are available at
https://github.com/kaopanboonyuen/MARS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panboonyuen_T/0/1/0/all/0/1&quot;&gt;Teerapong Panboonyuen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nithisopa_N/0/1/0/all/0/1&quot;&gt;Naphat Nithisopa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pienroj_P/0/1/0/all/0/1&quot;&gt;Panin Pienroj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jirachuphun_L/0/1/0/all/0/1&quot;&gt;Laphonchai Jirachuphun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watthanasirikrit_C/0/1/0/all/0/1&quot;&gt;Chaiwasut Watthanasirikrit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pornwiriyakul_N/0/1/0/all/0/1&quot;&gt;Naruepon Pornwiriyakul&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13399">
<title>Efficient Large-Scale Visual Representation Learning. (arXiv:2305.13399v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13399</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we present our approach to single-modality visual
representation learning. Understanding visual representations of product
content is vital for recommendations, search, and advertising applications in
e-commerce. We detail and contrast techniques used to fine-tune large-scale
visual representation learning models in an efficient manner under low-resource
settings, including several pretrained backbone architectures, both in the
convolutional neural network as well as the vision transformer family. We
highlight the challenges for e-commerce applications at-scale and highlight the
efforts to more efficiently train, evaluate, and serve visual representations.
We present ablation studies evaluating the representation offline performance
for several downstream tasks, including our visually similar ad
recommendations. To this end, we present a novel text-to-image generative
offline evaluation method for visually similar recommendation systems. Finally,
we include online results from deployed machine learning systems in production
at Etsy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolev_E/0/1/0/all/0/1&quot;&gt;Eden Dolev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awad_A/0/1/0/all/0/1&quot;&gt;Alaa Awad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_D/0/1/0/all/0/1&quot;&gt;Denisa Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebrahimzadeh_Z/0/1/0/all/0/1&quot;&gt;Zahra Ebrahimzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mejran_M/0/1/0/all/0/1&quot;&gt;Marcin Mejran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malpani_V/0/1/0/all/0/1&quot;&gt;Vaibhav Malpani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_M/0/1/0/all/0/1&quot;&gt;Mahir Yavuz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17033">
<title>The Brain Tumor Segmentation (BraTS) Challenge 2023: Focus on Pediatrics (CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs). (arXiv:2305.17033v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17033</link>
<description rdf:parseType="Literal">&lt;p&gt;Pediatric tumors of the central nervous system are the most common cause of
cancer-related death in children. The five-year survival rate for high-grade
gliomas in children is less than 20\%. Due to their rarity, the diagnosis of
these entities is often delayed, their treatment is mainly based on historic
treatment concepts, and clinical trials require multi-institutional
collaborations. The MICCAI Brain Tumor Segmentation (BraTS) Challenge is a
landmark community benchmark event with a successful history of 12 years of
resource creation for the segmentation and analysis of adult glioma. Here we
present the CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023 challenge, which
represents the first BraTS challenge focused on pediatric brain tumors with
data acquired across multiple international consortia dedicated to pediatric
neuro-oncology and clinical trials. The BraTS-PEDs 2023 challenge focuses on
benchmarking the development of volumentric segmentation algorithms for
pediatric brain glioma through standardized quantitative performance evaluation
metrics utilized across the BraTS 2023 cluster of challenges. Models gaining
knowledge from the BraTS-PEDs multi-parametric structural MRI (mpMRI) training
data will be evaluated on separate validation and unseen test mpMRI dataof
high-grade pediatric glioma. The CBTN-CONNECT-DIPGR-ASNR-MICCAI BraTS-PEDs 2023
challenge brings together clinicians and AI/imaging scientists to lead to
faster development of automated segmentation techniques that could benefit
clinical trials, and ultimately the care of children with brain tumors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kazerooni_A/0/1/0/all/0/1&quot;&gt;Anahita Fathi Kazerooni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khalili_N/0/1/0/all/0/1&quot;&gt;Nastaran Khalili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haldar_D/0/1/0/all/0/1&quot;&gt;Debanjan Haldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhifan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anwar_S/0/1/0/all/0/1&quot;&gt;Syed Muhammed Anwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Albrecht_J/0/1/0/all/0/1&quot;&gt;Jake Albrecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adewole_M/0/1/0/all/0/1&quot;&gt;Maruf Adewole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anazodo_U/0/1/0/all/0/1&quot;&gt;Udunna Anazodo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Anderson_H/0/1/0/all/0/1&quot;&gt;Hannah Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagheri_S/0/1/0/all/0/1&quot;&gt;Sina Bagheri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baid_U/0/1/0/all/0/1&quot;&gt;Ujjwal Baid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bergquist_T/0/1/0/all/0/1&quot;&gt;Timothy Bergquist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Borja_A/0/1/0/all/0/1&quot;&gt;Austin J. Borja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Calabrese_E/0/1/0/all/0/1&quot;&gt;Evan Calabrese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_V/0/1/0/all/0/1&quot;&gt;Verena Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Conte_G/0/1/0/all/0/1&quot;&gt;Gian-Marco Conte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dako_F/0/1/0/all/0/1&quot;&gt;Farouk Dako&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eddy_J/0/1/0/all/0/1&quot;&gt;James Eddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ezhov_I/0/1/0/all/0/1&quot;&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Familiar_A/0/1/0/all/0/1&quot;&gt;Ariana Familiar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Farahani_K/0/1/0/all/0/1&quot;&gt;Keyvan Farahani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Haldar_S/0/1/0/all/0/1&quot;&gt;Shuvanjan Haldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Iglesias_J/0/1/0/all/0/1&quot;&gt;Juan Eugenio Iglesias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Janas_A/0/1/0/all/0/1&quot;&gt;Anastasia Janas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Johansen_E/0/1/0/all/0/1&quot;&gt;Elaine Johansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jones_B/0/1/0/all/0/1&quot;&gt;Blaise V Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kofler_F/0/1/0/all/0/1&quot;&gt;Florian Kofler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+LaBella_D/0/1/0/all/0/1&quot;&gt;Dominic LaBella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lai_H/0/1/0/all/0/1&quot;&gt;Hollie Anne Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Leemput_K/0/1/0/all/0/1&quot;&gt;Koen Van Leemput&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongwei Bran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maleki_N/0/1/0/all/0/1&quot;&gt;Nazanin Maleki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McAllister_A/0/1/0/all/0/1&quot;&gt;Aaron S McAllister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meier_Z/0/1/0/all/0/1&quot;&gt;Zeke Meier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Moawad_A/0/1/0/all/0/1&quot;&gt;Ahmed W Moawad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nandolia_K/0/1/0/all/0/1&quot;&gt;Khanak K Nandolia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pavaine_J/0/1/0/all/0/1&quot;&gt;Julija Pavaine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piraud_M/0/1/0/all/0/1&quot;&gt;Marie Piraud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poussaint_T/0/1/0/all/0/1&quot;&gt;Tina Poussaint&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prabhu_S/0/1/0/all/0/1&quot;&gt;Sanjay P Prabhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Reitman_Z/0/1/0/all/0/1&quot;&gt;Zachary Reitman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Andres Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rudie_J/0/1/0/all/0/1&quot;&gt;Jeffrey D Rudie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shaikh_I/0/1/0/all/0/1&quot;&gt;Ibraheem Salman Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_L/0/1/0/all/0/1&quot;&gt;Lubdha M. Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sheth_N/0/1/0/all/0/1&quot;&gt;Nakul Sheth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shinohara_R/0/1/0/all/0/1&quot;&gt;Russel Taki Shinohara&lt;/a&gt;, et al. (23 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00612">
<title>AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset. (arXiv:2306.00612v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00612</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a long-term vision for Autonomous Driving (AD) community that the
perception models can learn from a large-scale point cloud dataset, to obtain
unified representations that can achieve promising results on different tasks
or benchmarks. Previous works mainly focus on the self-supervised pre-training
pipeline, meaning that they perform the pre-training and fine-tuning on the
same benchmark, which is difficult to attain the performance scalability and
cross-dataset application for the pre-training checkpoint. In this paper, for
the first time, we are committed to building a large-scale pre-training
point-cloud dataset with diverse data distribution, and meanwhile learning
generalizable representations from such a diverse pre-training dataset. We
formulate the point-cloud pre-training task as a semi-supervised problem, which
leverages the few-shot labeled and massive unlabeled point-cloud data to
generate the unified backbone representations that can be directly applied to
many baseline models and benchmarks, decoupling the AD-related pre-training
process and downstream fine-tuning task. During the period of backbone
pre-training, by enhancing the scene- and instance-level distribution diversity
and exploiting the backbone&apos;s ability to learn from unknown instances, we
achieve significant performance gains on a series of downstream perception
benchmarks including Waymo, nuScenes, and KITTI, under different baseline
models like PV-RCNN++, SECOND, CenterPoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jiakang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xiangchao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yikang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06388">
<title>From NeRFLiX to NeRFLiX++: A General NeRF-Agnostic Restorer Paradigm. (arXiv:2306.06388v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06388</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRF) have shown great success in novel view
synthesis. However, recovering high-quality details from real-world scenes is
still challenging for the existing NeRF-based approaches, due to the potential
imperfect calibration information and scene representation inaccuracy. Even
with high-quality training frames, the synthetic novel views produced by NeRF
models still suffer from notable rendering artifacts, such as noise and blur.
To address this, we propose NeRFLiX, a general NeRF-agnostic restorer paradigm
that learns a degradation-driven inter-viewpoint mixer. Specially, we design a
NeRF-style degradation modeling approach and construct large-scale training
data, enabling the possibility of effectively removing NeRF-native rendering
artifacts for deep neural networks. Moreover, beyond the degradation removal,
we propose an inter-viewpoint aggregation framework that fuses highly related
high-quality training images, pushing the performance of cutting-edge NeRF
models to entirely new levels and producing highly photo-realistic synthetic
views. Based on this paradigm, we further present NeRFLiX++ with a stronger
two-stage NeRF degradation simulator and a faster inter-viewpoint mixer,
achieving superior performance with significantly improved computational
efficiency. Notably, NeRFLiX++ is capable of restoring photo-realistic
ultra-high-resolution outputs from noisy low-resolution NeRF-rendered views.
Extensive experiments demonstrate the excellent restoration ability of
NeRFLiX++ on various novel view synthesis benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenbo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nianjuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiangbo Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07615">
<title>UOD: Universal One-shot Detection of Anatomical Landmarks. (arXiv:2306.07615v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07615</link>
<description rdf:parseType="Literal">&lt;p&gt;One-shot medical landmark detection gains much attention and achieves great
success for its label-efficient training process. However, existing one-shot
learning methods are highly specialized in a single domain and suffer domain
preference heavily in the situation of multi-domain unlabeled data. Moreover,
one-shot learning is not robust that it faces performance drop when annotating
a sub-optimal image. To tackle these issues, we resort to developing a
domain-adaptive one-shot landmark detection framework for handling multi-domain
medical images, named Universal One-shot Detection (UOD). UOD consists of two
stages and two corresponding universal models which are designed as
combinations of domain-specific modules and domain-shared modules. In the first
stage, a domain-adaptive convolution model is self-supervised learned to
generate pseudo landmark labels. In the second stage, we design a
domain-adaptive transformer to eliminate domain preference and build the global
context for multi-domain data. Even though only one annotated sample from each
domain is available for training, the domain-shared modules help UOD aggregate
all one-shot samples to detect more robust and accurate landmarks. We
investigated both qualitatively and quantitatively the proposed UOD on three
widely-used public X-ray datasets in different anatomical domains (i.e., head,
hand, chest) and obtained state-of-the-art performances in each domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Heqin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quan_Q/0/1/0/all/0/1&quot;&gt;Quan Quan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Qingsong Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zaiyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08861">
<title>Motion Capture Dataset for Practical Use of AI-based Motion Editing and Stylization. (arXiv:2306.08861v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08861</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we proposed a new style-diverse dataset for the domain of
motion style transfer. The motion dataset uses an industrial-standard human
bone structure and thus is industry-ready to be plugged into 3D characters for
many projects. We claim the challenges in motion style transfer and encourage
future work in this domain by releasing the proposed motion dataset both to the
public and the market. We conduct a comprehensive study on motion style
transfer in the experiment using the state-of-the-art method, and the results
show the proposed dataset&apos;s validity for the motion style transfer task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_M/0/1/0/all/0/1&quot;&gt;Makito Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_C/0/1/0/all/0/1&quot;&gt;Chen-Chieh Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inoue_K/0/1/0/all/0/1&quot;&gt;Keito Inoue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yojima_S/0/1/0/all/0/1&quot;&gt;Sentaro Yojima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_M/0/1/0/all/0/1&quot;&gt;Masafumi Takahashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09417">
<title>Diff-TTSG: Denoising probabilistic integrated speech and gesture synthesis. (arXiv:2306.09417v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09417</link>
<description rdf:parseType="Literal">&lt;p&gt;With read-aloud speech synthesis achieving high naturalness scores, there is
a growing research interest in synthesising spontaneous speech. However, human
spontaneous face-to-face conversation has both spoken and non-verbal aspects
(here, co-speech gestures). Only recently has research begun to explore the
benefits of jointly synthesising these two modalities in a single system. The
previous state of the art used non-probabilistic methods, which fail to capture
the variability of human speech and motion, and risk producing oversmoothing
artefacts and sub-optimal synthesis quality. We present the first
diffusion-based probabilistic model, called Diff-TTSG, that jointly learns to
synthesise speech and gestures together. Our method can be trained on small
datasets from scratch. Furthermore, we describe a set of careful uni- and
multi-modal subjective tests for evaluating integrated speech and gesture
synthesis systems, and use them to validate our proposed approach. For
synthesised examples please see https://shivammehta25.github.io/Diff-TTSG
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Shivam Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alexanderson_S/0/1/0/all/0/1&quot;&gt;Simon Alexanderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Beskow_J/0/1/0/all/0/1&quot;&gt;Jonas Beskow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Szekely_E/0/1/0/all/0/1&quot;&gt;&amp;#xc9;va Sz&amp;#xe9;kely&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Henter_G/0/1/0/all/0/1&quot;&gt;Gustav Eje Henter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10006">
<title>Unsupervised Learning of Style-Aware Facial Animation from Real Acting Performances. (arXiv:2306.10006v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10006</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach for text/speech-driven animation of a
photo-realistic head model based on blend-shape geometry, dynamic textures, and
neural rendering. Training a VAE for geometry and texture yields a parametric
model for accurate capturing and realistic synthesis of facial expressions from
a latent feature vector. Our animation method is based on a conditional CNN
that transforms text or speech into a sequence of animation parameters. In
contrast to previous approaches, our animation model learns
disentangling/synthesizing different acting-styles in an unsupervised manner,
requiring only phonetic labels that describe the content of training sequences.
For realistic real-time rendering, we train a U-Net that refines
rasterization-based renderings by computing improved pixel colors and a
foreground matte. We compare our framework qualitatively/quantitatively against
recent methods for head modeling as well as facial animation and evaluate the
perceived rendering/animation quality in a user-study, which indicates large
improvements compared to state-of-the-art approaches
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paier_W/0/1/0/all/0/1&quot;&gt;Wolfgang Paier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilsmann_A/0/1/0/all/0/1&quot;&gt;Anna Hilsmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisert_P/0/1/0/all/0/1&quot;&gt;Peter Eisert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11925">
<title>LVM-Med: Learning Large-Scale Self-Supervised Vision Models for Medical Imaging via Second-order Graph Matching. (arXiv:2306.11925v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11925</link>
<description rdf:parseType="Literal">&lt;p&gt;Obtaining large pre-trained models that can be fine-tuned to new tasks with
limited annotated samples has remained an open challenge for medical imaging
data. While pre-trained deep networks on ImageNet and vision-language
foundation models trained on web-scale data are prevailing approaches, their
effectiveness on medical tasks is limited due to the significant domain shift
between natural and medical images. To bridge this gap, we introduce LVM-Med,
the first family of deep networks trained on large-scale medical datasets. We
have collected approximately 1.3 million medical images from 55 publicly
available datasets, covering a large number of organs and modalities such as
CT, MRI, X-ray, and Ultrasound. We benchmark several state-of-the-art
self-supervised algorithms on this dataset and propose a novel self-supervised
contrastive learning algorithm using a graph-matching formulation. The proposed
approach makes three contributions: (i) it integrates prior pair-wise image
similarity metrics based on local and global information; (ii) it captures the
structural constraints of feature embeddings through a loss function
constructed via a combinatorial graph-matching objective; and (iii) it can be
trained efficiently end-to-end using modern gradient-estimation techniques for
black-box solvers. We thoroughly evaluate the proposed LVM-Med on 15 downstream
medical tasks ranging from segmentation and classification to object detection,
and both for the in and out-of-distribution settings. LVM-Med empirically
outperforms a number of state-of-the-art supervised, self-supervised, and
foundation models. For challenging tasks such as Brain Tumor Classification or
Diabetic Retinopathy Grading, LVM-Med improves previous vision-language models
trained on 1 billion masks by 6-7% while using only a ResNet-50.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duy M. H. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hoang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diep_N/0/1/0/all/0/1&quot;&gt;Nghiem T. Diep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_T/0/1/0/all/0/1&quot;&gt;Tan N. Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Tri Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_B/0/1/0/all/0/1&quot;&gt;Binh T. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swoboda_P/0/1/0/all/0/1&quot;&gt;Paul Swoboda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_N/0/1/0/all/0/1&quot;&gt;Nhat Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albarqouni_S/0/1/0/all/0/1&quot;&gt;Shadi Albarqouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengtao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonntag_D/0/1/0/all/0/1&quot;&gt;Daniel Sonntag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niepert_M/0/1/0/all/0/1&quot;&gt;Mathias Niepert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14435">
<title>DragDiffusion: Harnessing Diffusion Models for Interactive Point-based Image Editing. (arXiv:2306.14435v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14435</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise and controllable image editing is a challenging task that has
attracted significant attention. Recently, DragGAN enables an interactive
point-based image editing framework and achieves impressive editing results
with pixel-level precision. However, since this method is based on generative
adversarial networks (GAN), its generality is upper-bounded by the capacity of
the pre-trained GAN models. In this work, we extend such an editing framework
to diffusion models and propose DragDiffusion. By leveraging large-scale
pretrained diffusion models, we greatly improve the applicability of
interactive point-based editing in real world scenarios. While most existing
diffusion-based image editing methods work on text embeddings, DragDiffusion
optimizes the diffusion latent to achieve precise spatial control. Although
diffusion models generate images in an iterative manner, we empirically show
that optimizing diffusion latent at one single step suffices to generate
coherent results, enabling DragDiffusion to complete high-quality editing
efficiently. Extensive experiments across a wide range of challenging cases
(e.g., multi-objects, diverse object categories, various styles, etc.)
demonstrate the versatility and generality of DragDiffusion. Code:
https://github.com/Yujun-Shi/DragDiffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujun Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_C/0/1/0/all/0/1&quot;&gt;Chuhui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Song Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16718">
<title>Metric-aligned Sample Selection and Critical Feature Sampling for Oriented Object Detection. (arXiv:2306.16718v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16718</link>
<description rdf:parseType="Literal">&lt;p&gt;Arbitrary-oriented object detection is a relatively emerging but challenging
task. Although remarkable progress has been made, there still remain many
unsolved issues due to the large diversity of patterns in orientation, scale,
aspect ratio, and visual appearance of objects in aerial images. Most of the
existing methods adopt a coarse-grained fixed label assignment strategy and
suffer from the inconsistency between the classification score and localization
accuracy. First, to align the metric inconsistency between sample selection and
regression loss calculation caused by fixed IoU strategy, we introduce affine
transformation to evaluate the quality of samples and propose a distance-based
label assignment strategy. The proposed metric-aligned selection (MAS) strategy
can dynamically select samples according to the shape and rotation
characteristic of objects. Second, to further address the inconsistency between
classification and localization, we propose a critical feature sampling (CFS)
module, which performs localization refinement on the sampling location for
classification task to extract critical features accurately. Third, we present
a scale-controlled smooth $L_1$ loss (SC-Loss) to adaptively select high
quality samples by changing the form of regression loss function based on the
statistics of proposals during training. Extensive experiments are conducted on
four challenging rotated object detection datasets DOTA, FAIR1M-1.0, HRSC2016,
and UCAS-AOD. The results show the state-of-the-art accuracy of the proposed
detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yongbin Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenqi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wanying Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_S/0/1/0/all/0/1&quot;&gt;Shengjian Bai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00097">
<title>Prompting classes: Exploring the Power of Prompt Class Learning in Weakly Supervised Semantic Segmentation. (arXiv:2307.00097v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00097</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, CLIP-based approaches have exhibited remarkable performance on
generalization and few-shot learning tasks, fueled by the power of contrastive
language-vision pre-training. In particular, prompt tuning has emerged as an
effective strategy to adapt the pre-trained language-vision models to
downstream tasks by employing task-related textual tokens. Motivated by this
progress, in this work we question whether other fundamental problems, such as
weakly supervised semantic segmentation (WSSS), can benefit from prompt tuning.
Our findings reveal two interesting observations that shed light on the impact
of prompt tuning on WSSS. First, modifying only the class token of the text
prompt results in a greater impact on the Class Activation Map (CAM), compared
to arguably more complex strategies that optimize the context. And second, the
class token associated with the image ground truth does not necessarily
correspond to the category that yields the best CAM. Motivated by these
observations, we introduce a novel approach based on a PrOmpt cLass lEarning
(POLE) strategy. Through extensive experiments we demonstrate that our simple,
yet efficient approach achieves SOTA performance in a well-known WSSS
benchmark. These results highlight not only the benefits of language-vision
models in WSSS but also the potential of prompt learning for this problem. The
code is available at https://github.com/rB080/WSS_POLE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murugesan_B/0/1/0/all/0/1&quot;&gt;Balamurali Murugesan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hussain_R/0/1/0/all/0/1&quot;&gt;Rukhshanda Hussain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1&quot;&gt;Rajarshi Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dolz_J/0/1/0/all/0/1&quot;&gt;Jose Dolz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01462">
<title>Practical Collaborative Perception: A Framework for Asynchronous and Multi-Agent 3D Object Detection. (arXiv:2307.01462v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01462</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion is a major challenge for LiDAR-based object detection methods. This
challenge becomes safety-critical in urban traffic where the ego vehicle must
have reliable object detection to avoid collision while its field of view is
severely reduced due to the obstruction posed by a large number of road users.
Collaborative perception via Vehicle-to-Everything (V2X) communication, which
leverages the diverse perspective thanks to the presence at multiple locations
of connected agents to form a complete scene representation, is an appealing
solution. State-of-the-art V2X methods resolve the performance-bandwidth
tradeoff using a mid-collaboration approach where the Bird-Eye View images of
point clouds are exchanged so that the bandwidth consumption is lower than
communicating point clouds as in early collaboration, and the detection
performance is higher than late collaboration, which fuses agents&apos; output,
thanks to a deeper interaction among connected agents. While achieving strong
performance, the real-world deployment of most mid-collaboration approaches is
hindered by their overly complicated architectures, involving learnable
collaboration graphs and autoencoder-based compressor/ decompressor, and
unrealistic assumptions about inter-agent synchronization. In this work, we
devise a simple yet effective collaboration method that achieves a better
bandwidth-performance tradeoff than prior state-of-the-art methods while
minimizing changes made to the single-vehicle detection models and relaxing
unrealistic assumptions on inter-agent synchronization. Experiments on the
V2X-Sim dataset show that our collaboration method achieves 98\% of the
performance of an early-collaboration method, while only consuming the
equivalent bandwidth of a late-collaboration method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dao_M/0/1/0/all/0/1&quot;&gt;Minh-Quan Dao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrio_J/0/1/0/all/0/1&quot;&gt;Julie Stephany Berrio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fremont_V/0/1/0/all/0/1&quot;&gt;Vincent Fr&amp;#xe9;mont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_M/0/1/0/all/0/1&quot;&gt;Mao Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hery_E/0/1/0/all/0/1&quot;&gt;Elwan H&amp;#xe9;ry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Worrall_S/0/1/0/all/0/1&quot;&gt;Stewart Worrall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01465">
<title>AdAM: Few-Shot Image Generation via Adaptation-Aware Kernel Modulation. (arXiv:2307.01465v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01465</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot image generation (FSIG) aims to learn to generate new and diverse
images given few (e.g., 10) training samples. Recent work has addressed FSIG by
leveraging a GAN pre-trained on a large-scale source domain and adapting it to
the target domain with few target samples. Central to recent FSIG methods are
knowledge preservation criteria, which select and preserve a subset of source
knowledge to the adapted model. However, a major limitation of existing methods
is that their knowledge preserving criteria consider only source domain/task
and fail to consider target domain/adaptation in selecting source knowledge,
casting doubt on their suitability for setups of different proximity between
source and target domain. Our work makes two contributions. Firstly, we revisit
recent FSIG works and their experiments. We reveal that under setups which
assumption of close proximity between source and target domains is relaxed,
many existing state-of-the-art (SOTA) methods which consider only source domain
in knowledge preserving perform no better than a baseline method. As our second
contribution, we propose Adaptation-Aware kernel Modulation (AdAM) for general
FSIG of different source-target domain proximity. Extensive experiments show
that AdAM consistently achieves SOTA performance in FSIG, including challenging
setups where source and target domains are more apart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunqing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrasegaran_K/0/1/0/all/0/1&quot;&gt;Keshigeyan Chandrasegaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milad_A/0/1/0/all/0/1&quot;&gt;Abdollahzadeh Milad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruoteng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Henghui Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_N/0/1/0/all/0/1&quot;&gt;Ngai-Man Cheung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01473">
<title>Mitigating Bias: Enhancing Image Classification by Improving Model Explanations. (arXiv:2307.01473v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01473</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have demonstrated remarkable capabilities in learning
complex patterns and concepts from training data. However, recent findings
indicate that these models tend to rely heavily on simple and easily
discernible features present in the background of images rather than the main
concepts or objects they are intended to classify. This phenomenon poses a
challenge to image classifiers as the crucial elements of interest in images
may be overshadowed. In this paper, we propose a novel approach to address this
issue and improve the learning of main concepts by image classifiers. Our
central idea revolves around concurrently guiding the model&apos;s attention toward
the foreground during the classification task. By emphasizing the foreground,
which encapsulates the primary objects of interest, we aim to shift the focus
of the model away from the dominant influence of the background. To accomplish
this, we introduce a mechanism that encourages the model to allocate sufficient
attention to the foreground. We investigate various strategies, including
modifying the loss function or incorporating additional architectural
components, to enable the classifier to effectively capture the primary concept
within an image. Additionally, we explore the impact of different foreground
attention mechanisms on model performance and provide insights into their
effectiveness. Through extensive experimentation on benchmark datasets, we
demonstrate the efficacy of our proposed approach in improving the
classification accuracy of image classifiers. Our findings highlight the
importance of foreground attention in enhancing model understanding and
representation of the main concepts within images. The results of this study
contribute to advancing the field of image classification and provide valuable
insights for developing more robust and accurate deep-learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_R/0/1/0/all/0/1&quot;&gt;Raha Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabi_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Rajabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalooie_M/0/1/0/all/0/1&quot;&gt;Mohammad Khalooie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabokrou_M/0/1/0/all/0/1&quot;&gt;Mohammad Sabokrou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01750">
<title>SRCD: Semantic Reasoning with Compound Domains for Single-Domain Generalized Object Detection. (arXiv:2307.01750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01750</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a novel framework for single-domain generalized object
detection (i.e., Single-DGOD), where we are interested in learning and
maintaining the semantic structures of self-augmented compound cross-domain
samples to enhance the model&apos;s generalization ability. Different from DGOD
trained on multiple source domains, Single-DGOD is far more challenging to
generalize well to multiple target domains with only one single source domain.
Existing methods mostly adopt a similar treatment from DGOD to learn
domain-invariant features by decoupling or compressing the semantic space.
However, there may have two potential limitations: 1) pseudo attribute-label
correlation, due to extremely scarce single-domain data; and 2) the semantic
structural information is usually ignored, i.e., we found the affinities of
instance-level semantic relations in samples are crucial to model
generalization. In this paper, we introduce Semantic Reasoning with Compound
Domains (SRCD) for Single-DGOD. Specifically, our SRCD contains two main
components, namely, the texture-based self-augmentation (TBSA) module, and the
local-global semantic reasoning (LGSR) module. TBSA aims to eliminate the
effects of irrelevant attributes associated with labels, such as light, shadow,
color, etc., at the image level by a light-yet-efficient self-augmentation.
Moreover, LGSR is used to further model the semantic relationships on instance
features to uncover and maintain the intrinsic semantic structures. Extensive
experiments on multiple benchmarks demonstrate the effectiveness of the
proposed SRCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Z/0/1/0/all/0/1&quot;&gt;Zhijie Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Luyao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xinghao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01844">
<title>Advancing Wound Filling Extraction on 3D Faces: A Auto-Segmentation and Wound Face Regeneration Approach. (arXiv:2307.01844v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01844</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial wound segmentation plays a crucial role in preoperative planning and
optimizing patient outcomes in various medical applications. In this paper, we
propose an efficient approach for automating 3D facial wound segmentation using
a two-stream graph convolutional network. Our method leverages the Cir3D-FaIR
dataset and addresses the challenge of data imbalance through extensive
experimentation with different loss functions. To achieve accurate
segmentation, we conducted thorough experiments and selected a high-performing
model from the trained models. The selected model demonstrates exceptional
segmentation performance for complex 3D facial wounds. Furthermore, based on
the segmentation model, we propose an improved approach for extracting 3D
facial wound fillers and compare it to the results of the previous study. Our
method achieved a remarkable accuracy of 0.9999986\% on the test suite,
surpassing the performance of the previous method. From this result, we use 3D
printing technology to illustrate the shape of the wound filling. The outcomes
of this study have significant implications for physicians involved in
preoperative planning and intervention design. By automating facial wound
segmentation and improving the accuracy of wound-filling extraction, our
approach can assist in carefully assessing and optimizing interventions,
leading to enhanced patient outcomes. Additionally, it contributes to advancing
facial reconstruction techniques by utilizing machine learning and 3D
bioprinting for printing skin tissue implants. Our source code is available at
\url{https://github.com/SIMOGroup/WoundFilling3D}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong Q. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thinh D. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_P/0/1/0/all/0/1&quot;&gt;Phuong D. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Nga T.K. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Xuan_H/0/1/0/all/0/1&quot;&gt;H. Nguyen-Xuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02010">
<title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: Semi-Supervised Video Object Segmentation. (arXiv:2307.02010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02010</link>
<description rdf:parseType="Literal">&lt;p&gt;The Associating Objects with Transformers (AOT) framework has exhibited
exceptional performance in a wide range of complex scenarios for video object
segmentation. In this study, we introduce MSDeAOT, a variant of the AOT series
that incorporates transformers at multiple feature scales. Leveraging the
hierarchical Gated Propagation Module (GPM), MSDeAOT efficiently propagates
object masks from previous frames to the current frame using a feature scale
with a stride of 16. Additionally, we employ GPM in a more refined feature
scale with a stride of 8, leading to improved accuracy in detecting and
tracking small objects. Through the implementation of test-time augmentations
and model ensemble techniques, we achieve the top-ranking position in the
EPIC-KITCHEN VISOR Semi-supervised Video Object Segmentation Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanyou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02203">
<title>Neural Fields for Interactive Visualization of Statistical Dependencies in 3D Simulation Ensembles. (arXiv:2307.02203v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02203</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first neural network that has learned to compactly represent
and can efficiently reconstruct the statistical dependencies between the values
of physical variables at different spatial locations in large 3D simulation
ensembles. Going beyond linear dependencies, we consider mutual information as
a measure of non-linear dependence. We demonstrate learning and reconstruction
with a large weather forecast ensemble comprising 1000 members, each storing
multiple physical variables at a 250 x 352 x 20 simulation grid. By
circumventing compute-intensive statistical estimators at runtime, we
demonstrate significantly reduced memory and computation requirements for
reconstructing the major dependence structures. This enables embedding the
estimator into a GPU-accelerated direct volume renderer and interactively
visualizing all mutual dependencies for a selected domain point.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farokhmanesh_F/0/1/0/all/0/1&quot;&gt;Fatemeh Farokhmanesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohlein_K/0/1/0/all/0/1&quot;&gt;Kevin H&amp;#xf6;hlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuhauser_C/0/1/0/all/0/1&quot;&gt;Christoph Neuhauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westermann_R/0/1/0/all/0/1&quot;&gt;R&amp;#xfc;diger Westermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02245">
<title>Set Learning for Accurate and Calibrated Models. (arXiv:2307.02245v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02245</link>
<description rdf:parseType="Literal">&lt;p&gt;Model overconfidence and poor calibration are common in machine learning and
difficult to account for when applying standard empirical risk minimization. In
this work, we propose a novel method to alleviate these problems that we call
odd-$k$-out learning (OKO), which minimizes the cross-entropy error for sets
rather than for single examples. This naturally allows the model to capture
correlations across data examples and achieves both better accuracy and
calibration, especially in limited training data and class-imbalanced regimes.
Perhaps surprisingly, OKO often yields better calibration even when training
with hard labels and dropping any additional calibration parameter tuning, such
as temperature scaling. We provide theoretical justification, establishing that
OKO naturally yields better calibration, and provide extensive experimental
analyses that corroborate our theoretical findings. We emphasize that OKO is a
general framework that can be easily adapted to many settings and the trained
model can be applied to single examples at inference time, without introducing
significant run-time overhead or architecture changes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muttenthaler_L/0/1/0/all/0/1&quot;&gt;Lukas Muttenthaler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vandermeulen_R/0/1/0/all/0/1&quot;&gt;Robert A. Vandermeulen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiuyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Unterthiner_T/0/1/0/all/0/1&quot;&gt;Thomas Unterthiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02334">
<title>Dual Arbitrary Scale Super-Resolution for Multi-Contrast MRI. (arXiv:2307.02334v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02334</link>
<description rdf:parseType="Literal">&lt;p&gt;Limited by imaging systems, the reconstruction of Magnetic Resonance Imaging
(MRI) images from partial measurement is essential to medical imaging research.
Benefiting from the diverse and complementary information of multi-contrast MR
images in different imaging modalities, multi-contrast Super-Resolution (SR)
reconstruction is promising to yield SR images with higher quality. In the
medical scenario, to fully visualize the lesion, radiologists are accustomed to
zooming the MR images at arbitrary scales rather than using a fixed scale, as
used by most MRI SR methods. In addition, existing multi-contrast MRI SR
methods often require a fixed resolution for the reference image, which makes
acquiring reference images difficult and imposes limitations on arbitrary scale
SR tasks. To address these issues, we proposed an implicit neural
representations based dual-arbitrary multi-contrast MRI super-resolution
method, called Dual-ArbNet. First, we decouple the resolution of the target and
reference images by a feature encoder, enabling the network to input target and
reference images at arbitrary scales. Then, an implicit fusion decoder fuses
the multi-contrast features and uses an Implicit Decoding Function~(IDF) to
obtain the final MRI SR results. Furthermore, we introduce a curriculum
learning strategy to train our network, which improves the generalization and
performance of our Dual-ArbNet. Extensive experiments in two public MRI
datasets demonstrate that our method outperforms state-of-the-art approaches
under different scale factors and has great potential in clinical practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiamiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yichen Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jun Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yapeng Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02508">
<title>ZJU ReLER Submission for EPIC-KITCHEN Challenge 2023: TREK-150 Single Object Tracking. (arXiv:2307.02508v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02508</link>
<description rdf:parseType="Literal">&lt;p&gt;The Associating Objects with Transformers (AOT) framework has exhibited
exceptional performance in a wide range of complex scenarios for video object
tracking and segmentation. In this study, we convert the bounding boxes to
masks in reference frames with the help of the Segment Anything Model (SAM) and
Alpha-Refine, and then propagate the masks to the current frame, transforming
the task from Video Object Tracking (VOT) to video object segmentation (VOS).
Furthermore, we introduce MSDeAOT, a variant of the AOT series that
incorporates transformers at multiple feature scales. MSDeAOT efficiently
propagates object masks from previous frames to the current frame using two
feature scales of 16 and 8. As a testament to the effectiveness of our design,
we achieved the 1st place in the EPIC-KITCHENS TREK-150 Object Tracking
Challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuanyou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiahao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yueting Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02595">
<title>GNEP Based Dynamic Segmentation and Motion Estimation for Neuromorphic Imaging. (arXiv:2307.02595v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02595</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the application of event-based cameras in the domains of
image segmentation and motion estimation. These cameras offer a groundbreaking
technology by capturing visual information as a continuous stream of
asynchronous events, departing from the conventional frame-based image
acquisition. We introduce a Generalized Nash Equilibrium based framework that
leverages the temporal and spatial information derived from the event stream to
carry out segmentation and velocity estimation. To establish the theoretical
foundations, we derive an existence criteria and propose a multi-level
optimization method for calculating equilibrium. The efficacy of this approach
is shown through a series of experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antil_H/0/1/0/all/0/1&quot;&gt;Harbir Antil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayre_D/0/1/0/all/0/1&quot;&gt;David Sayre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02881">
<title>Probabilistic and Semantic Descriptions of Image Manifolds and Their Applications. (arXiv:2307.02881v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02881</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper begins with a description of methods for estimating probability
density functions for images that reflects the observation that such data is
usually constrained to lie in restricted regions of the high-dimensional image
space - not every pattern of pixels is an image. It is common to say that
images lie on a lower-dimensional manifold in the high-dimensional space.
However, although images may lie on such lower-dimensional manifolds, it is not
the case that all points on the manifold have an equal probability of being
images. Images are unevenly distributed on the manifold, and our task is to
devise ways to model this distribution as a probability distribution. In
pursuing this goal, we consider generative models that are popular in AI and
computer vision community. For our purposes, generative/probabilistic models
should have the properties of 1) sample generation: it should be possible to
sample from this distribution according to the modelled density function, and
2) probability computation: given a previously unseen sample from the dataset
of interest, one should be able to compute the probability of the sample, at
least up to a normalising constant. To this end, we investigate the use of
methods such as normalising flow and diffusion models. We then show that such
probabilistic descriptions can be used to construct defences against
adversarial attacks. In addition to describing the manifold in terms of
density, we also consider how semantic interpretations can be used to describe
points on the manifold. To this end, we consider an emergent language framework
which makes use of variational encoders to produce a disentangled
representation of points that reside on a given manifold. Trajectories between
points on a manifold can then be described in terms of evolving semantic
descriptions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_P/0/1/0/all/0/1&quot;&gt;Peter Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhaoyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hartley_R/0/1/0/all/0/1&quot;&gt;Richard Hartley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1&quot;&gt;Dylan Campbell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03017">
<title>EffLiFe: Efficient Light Field Generation via Hierarchical Sparse Gradient Descent. (arXiv:2307.03017v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03017</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of Extended Reality (XR) technology, there is a growing need
for real-time light field generation from sparse view inputs. Existing methods
can be classified into offline techniques, which can generate high-quality
novel views but at the cost of long inference/training time, and online
methods, which either lack generalizability or produce unsatisfactory results.
However, we have observed that the intrinsic sparse manifold of Multi-plane
Images (MPI) enables a significant acceleration of light field generation while
maintaining rendering quality. Based on this insight, we introduce EffLiFe, a
novel light field optimization method, which leverages the proposed
Hierarchical Sparse Gradient Descent (HSGD) to produce high-quality light
fields from sparse view images in real time. Technically, the coarse MPI of a
scene is first generated using a 3D CNN, and it is further sparsely optimized
by focusing only on important MPI gradients in a few iterations. Nevertheless,
relying solely on optimization can lead to artifacts at occlusion boundaries.
Therefore, we propose an occlusion-aware iterative refinement module that
removes visual artifacts in occluded regions by iteratively filtering the
input. Extensive experiments demonstrate that our method achieves comparable
visual quality while being 100x faster on average than state-of-the-art offline
methods and delivering better performance (about 2 dB higher in PSNR) compared
to other online approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yijie Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianpeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Lu Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03039">
<title>Art Authentication with Vision Transformers. (arXiv:2307.03039v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03039</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Transformers, initially developed for language, have been
successfully applied to visual tasks. Vision Transformers have been shown to
push the state-of-the-art in a wide range of tasks, including image
classification, object detection, and semantic segmentation. While ample
research has shown promising results in art attribution and art authentication
tasks using Convolutional Neural Networks, this paper examines if the
superiority of Vision Transformers extends to art authentication, improving,
thus, the reliability of computer-based authentication of artworks. Using a
carefully compiled dataset of authentic paintings by Vincent van Gogh and two
contrast datasets, we compare the art authentication performances of Swin
Transformers with those of EfficientNet. Using a standard contrast set
containing imitations and proxies (works by painters with styles closely
related to van Gogh), we find that EfficientNet achieves the best performance
overall. With a contrast set that only consists of imitations, we find the Swin
Transformer to be superior to EfficientNet by achieving an authentication
accuracy of over 85%. These results lead us to conclude that Vision
Transformers represent a strong and promising contender in art authentication,
particularly in enhancing the computer-based ability to detect artistic
imitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaerf_L/0/1/0/all/0/1&quot;&gt;Ludovica Schaerf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Popovici_C/0/1/0/all/0/1&quot;&gt;Carina Popovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Postma_E/0/1/0/all/0/1&quot;&gt;Eric Postma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03073">
<title>Proto-CLIP: Vision-Language Prototypical Network for Few-Shot Learning. (arXiv:2307.03073v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03073</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel framework for few-shot learning by leveraging large-scale
vision-language models such as CLIP. Motivated by the unimodal prototypical
networks for few-shot learning, we introduce PROTO-CLIP that utilizes image
prototypes and text prototypes for few-shot learning. Specifically, PROTO-CLIP
adapts the image encoder and text encoder in CLIP in a joint fashion using
few-shot examples. The two encoders are used to compute prototypes of image
classes for classification. During adaptation, we propose aligning the image
and text prototypes of corresponding classes. Such a proposed alignment is
beneficial for few-shot classification due to the contributions from both types
of prototypes. We demonstrate the effectiveness of our method by conducting
experiments on benchmark datasets for few-shot learning as well as in the real
world for robot perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+P_J/0/1/0/all/0/1&quot;&gt;Jishnu Jaykumar P&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palanisamy_K/0/1/0/all/0/1&quot;&gt;Kamalesh Palanisamy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_Y/0/1/0/all/0/1&quot;&gt;Yu-Wei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinya Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>