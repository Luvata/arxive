<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-27T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14674" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14692" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14767" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15033" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15041" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1807.03418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2107.10021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.06584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.12589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.12181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.08253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04568" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.14362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.00462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.09846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15445" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.02858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06026" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19599" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03081" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.11845" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.15176" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.07037" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11136" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.12532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17338" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.17401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01825" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04914" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.07997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.16842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.01043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.05419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11602" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14084" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14619" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.14674">
<title>Emotion-Oriented Behavior Model Using Deep Learning. (arXiv:2311.14674v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14674</link>
<description rdf:parseType="Literal">&lt;p&gt;Emotions, as a fundamental ingredient of any social interaction, lead to
behaviors that represent the effectiveness of the interaction through facial
expressions and gestures in humans. Hence an agent must possess the social and
cognitive abilities to understand human social parameters and behave
accordingly. However, no such emotion-oriented behavior model is presented yet
in the existing research. The emotion prediction may generate appropriate
agents&apos; behaviors for effective interaction using conversation modality.
Considering the importance of emotions, and behaviors, for an agent&apos;s social
interaction, an Emotion-based Behavior model is presented in this paper for
Socio-cognitive artificial agents. The proposed model is implemented using
tweets data trained on multiple models like Long Short-Term Memory (LSTM),
Convolution Neural Network (CNN) and Bidirectional Encoder Representations from
Transformers (BERT) for emotion prediction with an average accuracy of 92%, and
55% respectively. Further, using emotion predictions from CNN-LSTM, the
behavior module responds using facial expressions and gestures using Behavioral
Markup Language (BML). The accuracy of emotion-based behavior predictions is
statistically validated using the 2-tailed Pearson correlation on the data
collected from human users through questionnaires. Analysis shows that all
emotion-based behaviors accurately depict human-like gestures and facial
expressions based on the significant correlation at the 0.01 and 0.05 levels.
This study is a steppingstone to a multi-faceted artificial agent interaction
based on emotion-oriented behaviors. Cognition has significance regarding
social interaction among humans.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raza_M/0/1/0/all/0/1&quot;&gt;Muhammad Arslan Raza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1&quot;&gt;Muhammad Shoaib Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khelifi_A/0/1/0/all/0/1&quot;&gt;Adel Khelifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvi_A/0/1/0/all/0/1&quot;&gt;Atif Alvi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14683">
<title>Data Science for Social Good. (arXiv:2311.14683v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14683</link>
<description rdf:parseType="Literal">&lt;p&gt;Data science has been described as the fourth paradigm for scientific
discovery. The latest wave of data science research, pertaining to machine
learning and artificial intelligence (AI), is growing exponentially and
garnering millions of annual citations. However, this growth has been
accompanied by a diminishing emphasis on social good challenges - our analysis
reveals that the proportion of data science research focusing on social good is
less than it has ever been. At the same time, the proliferation of machine
learning and generative AI have sparked debates about the socio-technical
prospects and challenges associated with data science for human flourishing,
organizations, and society. Against this backdrop, we present a framework for
&quot;data science for social good&quot; (DSSG) research that considers the interplay
between relevant data science research genres, social good challenges, and
different levels of socio-technical abstraction. We perform an analysis of the
literature to empirically demonstrate the paucity of work on DSSG in
information systems (and other related disciplines) and highlight current
impediments. We then use our proposed framework to introduce the articles
appearing in the special issue. We hope that this article and the special issue
will spur future DSSG research and help reverse the alarming trend across data
science research over the past 30-plus years in which social good challenges
are garnering proportionately less attention with each passing day.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasi_A/0/1/0/all/0/1&quot;&gt;Ahmed Abbasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_R/0/1/0/all/0/1&quot;&gt;Roger H. L. Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jennifer J. Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14684">
<title>The risks of risk-based AI regulation: taking liability seriously. (arXiv:2311.14684v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14684</link>
<description rdf:parseType="Literal">&lt;p&gt;The development and regulation of multi-purpose, large &quot;foundation models&quot; of
AI seems to have reached a critical stage, with major investments and new
applications announced every other day. Some experts are calling for a
moratorium on the training of AI systems more powerful than GPT-4. Legislators
globally compete to set the blueprint for a new regulatory regime. This paper
analyses the most advanced legal proposal, the European Union&apos;s AI Act
currently in the stage of final &quot;trilogue&quot; negotiations between the EU
institutions. This legislation will likely have extra-territorial implications,
sometimes called &quot;the Brussels effect&quot;. It also constitutes a radical departure
from conventional information and communications technology policy by
regulating AI ex-ante through a risk-based approach that seeks to prevent
certain harmful outcomes based on product safety principles. We offer a review
and critique, specifically discussing the AI Act&apos;s problematic obligations
regarding data quality and human oversight. Our proposal is to take liability
seriously as the key regulatory mechanism. This signals to industry that if a
breach of law occurs, firms are required to know in particular what their
inputs were and how to retrain the system to remedy the breach. Moreover, we
suggest differentiating between endogenous and exogenous sources of potential
harm, which can be mitigated by carefully allocating liability between
developers and deployers of AI technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretschmer_M/0/1/0/all/0/1&quot;&gt;Martin Kretschmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kretschmer_T/0/1/0/all/0/1&quot;&gt;Tobias Kretschmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peukert_A/0/1/0/all/0/1&quot;&gt;Alexander Peukert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peukert_C/0/1/0/all/0/1&quot;&gt;Christian Peukert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14686">
<title>Causal Models Applied to the Patterns of Human Migration due to Climate Change. (arXiv:2311.14686v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14686</link>
<description rdf:parseType="Literal">&lt;p&gt;The impacts of mass migration, such as crisis induced by climate change,
extend beyond environmental concerns and can greatly affect social
infrastructure and public services, such as education, healthcare, and
security. These crises exacerbate certain elements like cultural barriers, and
discrimination by amplifying the challenges faced by these affected
communities. This paper proposes an innovative approach to address migration
crises in the context of crisis management through a combination of modeling
and imbalance assessment tools. By employing deep learning for forecasting and
integrating causal reasoning via Bayesian networks, this methodology enables
the evaluation of imbalances and risks in the socio-technological landscape,
providing crucial insights for informed decision-making. Through this
framework, critical systems can be analyzed to understand how fluctuations in
migration levels may impact them, facilitating effective crisis governance
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_K/0/1/0/all/0/1&quot;&gt;Kenneth Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanushkevich_S/0/1/0/all/0/1&quot;&gt;Svetlana Yanushkevich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14687">
<title>Does Explainable AI Have Moral Value?. (arXiv:2311.14687v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14687</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable AI (XAI) aims to bridge the gap between complex algorithmic
systems and human stakeholders. Current discourse often examines XAI in
isolation as either a technological tool, user interface, or policy mechanism.
This paper proposes a unifying ethical framework grounded in moral duties and
the concept of reciprocity. We argue that XAI should be appreciated not merely
as a right, but as part of our moral duties that helps sustain a reciprocal
relationship between humans affected by AI systems. This is because, we argue,
explanations help sustain constitutive symmetry and agency in AI-led
decision-making processes. We then assess leading XAI communities and reveal
gaps between the ideal of reciprocity and practical feasibility. Machine
learning offers useful techniques but overlooks evaluation and adoption
challenges. Human-computer interaction provides preliminary insights but
oversimplifies organizational contexts. Policies espouse accountability but
lack technical nuance. Synthesizing these views exposes barriers to
implementable, ethical XAI. Still, positioning XAI as a moral duty transcends
rights-based discourse to capture a more robust and complete moral picture.
This paper provides an accessible, detailed analysis elucidating the moral
value of explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brand_J/0/1/0/all/0/1&quot;&gt;Joshua L.M. Brand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nannini_L/0/1/0/all/0/1&quot;&gt;Luca Nannini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14688">
<title>Procedural Fairness Through Decoupling Objectionable Data Generating Components. (arXiv:2311.14688v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14688</link>
<description rdf:parseType="Literal">&lt;p&gt;We reveal and address the frequently overlooked yet important issue of
disguised procedural unfairness, namely, the potentially inadvertent
alterations on the behavior of neutral (i.e., not problematic) aspects of data
generating process, and/or the lack of procedural assurance of the greatest
benefit of the least advantaged individuals. Inspired by John Rawls&apos;s advocacy
for pure procedural justice, we view automated decision-making as a microcosm
of social institutions, and consider how the data generating process itself can
satisfy the requirements of procedural fairness. We propose a framework that
decouples the objectionable data generating components from the neutral ones by
utilizing reference points and the associated value instantiation rule. Our
findings highlight the necessity of preventing disguised procedural unfairness,
drawing attention not only to the objectionable data generating components that
we aim to mitigate, but also more importantly, to the neutral components that
we intend to keep unaffected.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jialu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spirtes_P/0/1/0/all/0/1&quot;&gt;Peter Spirtes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14690">
<title>Evolutionary City: Towards a Flexible, Agile and Symbiotic System. (arXiv:2311.14690v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14690</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban growth sometimes leads to rigid infrastructure that struggles to adapt
to changing demand. This paper introduces a novel approach, aiming to enable
cities to evolve and respond more effectively to such dynamic demand. It
identifies the limitations arising from the complexity and inflexibility of
existing urban systems. A framework is presented for enhancing the city&apos;s
adaptability perception through advanced sensing technologies, conducting
parallel simulation via graph-based techniques, and facilitating autonomous
decision-making across domains through decentralized and autonomous
organization and operation. Notably, a symbiotic mechanism is employed to
implement these technologies practically, thereby making urban management more
agile and responsive. In the case study, we explore how this approach can
optimize traffic flow by adjusting lane allocations. This case not only
enhances traffic efficiency but also reduces emissions. The proposed
evolutionary city offers a new perspective on sustainable urban development,
highliting the importance of integrated intelligence within urban systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jingru Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Ding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shengyue Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yilun Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei-Yue Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14692">
<title>COVID-19 Imposes Rethinking of Conferencing -- Environmental Impact Assessment of Artificial Intelligence Conferences. (arXiv:2311.14692v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14692</link>
<description rdf:parseType="Literal">&lt;p&gt;It has been noticed that through COVID-19 greenhouse gas emissions had a
sudden reduction. Based on this significant observation, we decided to conduct
a research to quantify the impact of scientific conferences&apos; air-travelling,
explore and suggest alternative ways for greener conferences to re-duce the
global carbon footprint. Specifically, we focused on the most popular
conferences for the Artificial Intelligence community based on their scientific
impact factor, their scale, and the well-organized proceedings towards
measuring the impact of air travelling participation. This is the first time
that systematic quantification of a state-of-the-art subject like Artificial
Intelligence takes place to define its conferencing footprint in the broader
frames of environmental awareness. Our findings highlight that the virtual way
is the first on the list of green conferences&apos; conduction although there are
serious concerns about it. Alternatives to optimal conferences&apos; location
selection have demonstrated savings on air-travelling CO2 emissions of up to
63.9%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitsou_P/0/1/0/all/0/1&quot;&gt;Pavlina Mitsou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsakalidou_N/0/1/0/all/0/1&quot;&gt;Nikoleta-Victoria Tsakalidou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vrochidou_E/0/1/0/all/0/1&quot;&gt;Eleni Vrochidou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papakostas_G/0/1/0/all/0/1&quot;&gt;George A. Papakostas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14693">
<title>Benefits and Harms of Large Language Models in Digital Mental Health. (arXiv:2311.14693v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14693</link>
<description rdf:parseType="Literal">&lt;p&gt;The past decade has been transformative for mental health research and
practice. The ability to harness large repositories of data, whether from
electronic health records (EHR), mobile devices, or social media, has revealed
a potential for valuable insights into patient experiences, promising early,
proactive interventions, as well as personalized treatment plans. Recent
developments in generative artificial intelligence, particularly large language
models (LLMs), show promise in leading digital mental health to uncharted
territory. Patients are arriving at doctors&apos; appointments with information
sourced from chatbots, state-of-the-art LLMs are being incorporated in medical
software and EHR systems, and chatbots from an ever-increasing number of
startups promise to serve as AI companions, friends, and partners. This article
presents contemporary perspectives on the opportunities and risks posed by LLMs
in the design, development, and implementation of digital mental health tools.
We adopt an ecological framework and draw on the affordances offered by LLMs to
discuss four application areas -- care-seeking behaviors from individuals in
need of care, community care provision, institutional and medical care
provision, and larger care ecologies at the societal level. We engage in a
thoughtful consideration of whether and how LLM-based technologies could or
should be employed for enhancing mental health. The benefits and harms our
article surfaces could serve to help shape future research, advocacy, and
regulatory efforts focused on creating more responsible, user-friendly,
equitable, and secure LLM-based tools for mental health treatment and
intervention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1&quot;&gt;Munmun De Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pendse_S/0/1/0/all/0/1&quot;&gt;Sachin R. Pendse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_N/0/1/0/all/0/1&quot;&gt;Neha Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14695">
<title>AI for All: Operationalising Diversity and Inclusion Requirements for AI Systems. (arXiv:2311.14695v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14695</link>
<description rdf:parseType="Literal">&lt;p&gt;As Artificial Intelligence (AI) permeates many aspects of society, it brings
numerous advantages while at the same time raising ethical concerns and
potential risks, such as perpetuating inequalities through biased or
discriminatory decision-making. To develop AI systems that cater for the needs
of diverse users and uphold ethical values, it is essential to consider and
integrate diversity and inclusion (D&amp;amp;I) principles throughout AI development
and deployment. Requirements engineering (RE) is a fundamental process in
developing software systems by eliciting and specifying relevant needs from
diverse stakeholders. This research aims to address the lack of research and
practice on how to elicit and capture D&amp;amp;I requirements for AI systems. We have
conducted comprehensive data collection and synthesis from the literature
review to extract requirements themes related to D&amp;amp;I in AI. We have proposed a
tailored user story template to capture D&amp;amp;I requirements and conducted focus
group exercises to use the themes and user story template in writing D&amp;amp;I
requirements for two example AI systems. Additionally, we have investigated the
capability of our solution by generating synthetic D&amp;amp;I requirements captured in
user stories with the help of a Large Language Model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bano_M/0/1/0/all/0/1&quot;&gt;Muneera Bano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zowghi_D/0/1/0/all/0/1&quot;&gt;Didar Zowghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gervasi_V/0/1/0/all/0/1&quot;&gt;Vincenzo Gervasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shams_R/0/1/0/all/0/1&quot;&gt;Rifat Shams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14699">
<title>Ontology Learning Using Formal Concept Analysis and WordNet. (arXiv:2311.14699v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14699</link>
<description rdf:parseType="Literal">&lt;p&gt;Manual ontology construction takes time, resources, and domain specialists.
Supporting a component of this process for automation or semi-automation would
be good. This project and dissertation provide a Formal Concept Analysis and
WordNet framework for learning concept hierarchies from free texts. The process
has steps. First, the document is Part-Of-Speech labeled, then parsed to
produce sentence parse trees. Verb/noun dependencies are derived from parse
trees next. After lemmatizing, pruning, and filtering the word pairings, the
formal context is created. The formal context may contain some erroneous and
uninteresting pairs because the parser output may be erroneous, not all derived
pairs are interesting, and it may be large due to constructing it from a large
free text corpus. Deriving lattice from the formal context may take longer,
depending on the size and complexity of the data. Thus, decreasing formal
context may eliminate erroneous and uninteresting pairs and speed up idea
lattice derivation. WordNet-based and Frequency-based approaches are tested.
Finally, we compute formal idea lattice and create a classical concept
hierarchy. The reduced concept lattice is compared to the original to evaluate
the outcomes. Despite several system constraints and component discrepancies
that may prevent logical conclusion, the following data imply idea hierarchies
in this project and dissertation are promising. First, the reduced idea lattice
and original concept have commonalities. Second, alternative language or
statistical methods can reduce formal context size. Finally, WordNet-based and
Frequency-based approaches reduce formal context differently, and the order of
applying them is examined to reduce context efficiently.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_B/0/1/0/all/0/1&quot;&gt;Bryar A. Hassan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14700">
<title>Towards a Feminist Metaethics of AI. (arXiv:2311.14700v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14700</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of Artificial Intelligence (AI) has sparked an overwhelming
number of AI ethics guidelines, boards and codes of conduct. These outputs
primarily analyse competing theories, principles and values for AI development
and deployment. However, as a series of recent problematic incidents about AI
ethics/ethicists demonstrate, this orientation is insufficient. Before
proceeding to evaluate other professions, AI ethicists should critically
evaluate their own; yet, such an evaluation should be more explicitly and
systematically undertaken in the literature. I argue that these insufficiencies
could be mitigated by developing a research agenda for a feminist metaethics of
AI. Contrary to traditional metaethics, which reflects on the nature of
morality and moral judgements in a non-normative way, feminist metaethics
expands its scope to ask not only what ethics is but also what our engagement
with it should be like. Applying this perspective to the context of AI, I
suggest that a feminist metaethics of AI would examine: (i) the continuity
between theory and action in AI ethics; (ii) the real-life effects of AI
ethics; (iii) the role and profile of those involved in AI ethics; and (iv) the
effects of AI on power relations through methods that pay attention to context,
emotions and narrative.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siapka_A/0/1/0/all/0/1&quot;&gt;Anastasia Siapka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14701">
<title>ChatGPT as Co-Advisor in Scientific Initiation: Action Research with Project-Based Learning in Elementary Education. (arXiv:2311.14701v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14701</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: In the contemporary educational landscape, technology has the
power to drive innovative pedagogical practices. Overcoming the resistance of
teachers and students to adopting new methods and technologies is a challenge
that needs to be addressed. Objectives: To evaluate the effectiveness of
ChatGPT as a co-advisor in research projects and its influence on the
implementation of Project-Based Learning (PBL), as well as overcoming
resistance to the use of new pedagogical methodologies. Design: An
action-research methodology was employed, including unstructured interviews and
the application of questionnaires via Google Forms. Setting and Participants:
The research was conducted in an elementary school, involving 353 students and
16 teachers. Data Collection and Analysis: Data were gathered through
observations and notes in meetings and interviews, complemented by electronic
questionnaires, with quantitative and qualitative analyses performed via
Microsoft Excel and Google Forms. Results: The introduction of ChatGPT as a
pedagogical tool led to increased student engagement and decreased teacher
resistance, reflected in recognition at local science fairs. Conclusion: The
study confirmed the utility of ChatGPT in school research co-orientation,
highlighting its role in facilitating PBL and promoting cultural changes in
educational practice, with proactive school management identified as a
catalysing element in adapting to educational innovations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Villan_F/0/1/0/all/0/1&quot;&gt;Fabiano Villan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_R/0/1/0/all/0/1&quot;&gt;Renato P. dos Santos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14702">
<title>Transdisciplinary AI Education: The Confluence of Curricular and Community Needs in the Instruction of Artificial Intelligence. (arXiv:2311.14702v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14702</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of artificial intelligence (AI) into education has the
potential to transform the way we learn and teach. In this paper, we examine
the current state of AI in education and explore the potential benefits and
challenges of incorporating this technology into the classroom. The approaches
currently available for AI education often present students with experiences
only focusing on discrete computer science concepts agnostic to a larger
curriculum. However, teaching AI must not be siloed or interdisciplinary.
Rather, AI instruction ought to be transdisciplinary, including connections to
the broad curriculum and community in which students are learning. This paper
delves into the AI program currently in development for Neom Community School
and the larger Education, Research, and Innovation Sector in Neom, Saudi Arabia
s new megacity under development. In this program, AI is both taught as a
subject and to learn other subjects within the curriculum through the school
systems International Baccalaureate (IB) approach, which deploys learning
through Units of Inquiry. This approach to education connects subjects across a
curriculum under one major guiding question at a time. The proposed method
offers a meaningful approach to introducing AI to students throughout these
Units of Inquiry, as it shifts AI from a subject that students like or not like
to a subject that is taught throughout the curriculum.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliabadi_R/0/1/0/all/0/1&quot;&gt;Roozbeh Aliabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aditi Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_E/0/1/0/all/0/1&quot;&gt;Eryka Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14708">
<title>Large Language Model-Driven Classroom Flipping: Empowering Student-Centric Peer Questioning with Flipped Interaction. (arXiv:2311.14708v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14708</link>
<description rdf:parseType="Literal">&lt;p&gt;Reciprocal questioning is essential for effective teaching and learning,
fostering active engagement and deeper understanding through collaborative
interactions, especially in large classrooms. Can large language model (LLM),
such as OpenAI&apos;s GPT (Generative Pre-trained Transformer) series, assist in
this? This paper investigates a pedagogical approach of classroom flipping
based on flipped interaction in LLMs. Flipped interaction involves using
language models to prioritize generating questions instead of answers to
prompts. We demonstrate how traditional classroom flipping techniques,
including Peer Instruction and Just-in-Time Teaching (JiTT), can be enhanced
through flipped interaction techniques, creating student-centric questions for
hybrid teaching. In particular, we propose a workflow to integrate prompt
engineering with clicker and JiTT quizzes by a poll-prompt-quiz routine and a
quiz-prompt-discuss routine to empower students to self-regulate their learning
capacity and enable teachers to swiftly personalize training pathways. We
develop an LLM-driven chatbot software that digitizes various elements of
classroom flipping and facilitates the assessment of students using these
routines to deliver peer-generated questions. We have applied our LLM-driven
chatbot software for teaching both undergraduate and graduate students from
2020 to 2022, effectively useful for bridging the gap between teachers and
students in remote teaching during the COVID-19 pandemic years. In particular,
LLM-driven classroom flipping can be particularly beneficial in large class
settings to optimize teaching pace and enable engaging classroom experiences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chee Wei Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14711">
<title>Towards Publicly Accountable Frontier LLMs: Building an External Scrutiny Ecosystem under the ASPIRE Framework. (arXiv:2311.14711v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14711</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing integration of frontier large language models (LLMs) into
society and the economy, decisions related to their training, deployment, and
use have far-reaching implications. These decisions should not be left solely
in the hands of frontier LLM developers. LLM users, civil society and
policymakers need trustworthy sources of information to steer such decisions
for the better. Involving outside actors in the evaluation of these systems -
what we term &apos;external scrutiny&apos; - via red-teaming, auditing, and external
researcher access, offers a solution. Though there are encouraging signs of
increasing external scrutiny of frontier LLMs, its success is not assured. In
this paper, we survey six requirements for effective external scrutiny of
frontier AI systems and organize them under the ASPIRE framework: Access,
Searching attitude, Proportionality to the risks, Independence, Resources, and
Expertise. We then illustrate how external scrutiny might function throughout
the AI lifecycle and offer recommendations to policymakers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderljung_M/0/1/0/all/0/1&quot;&gt;Markus Anderljung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_E/0/1/0/all/0/1&quot;&gt;Everett Thornton Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OBrien_J/0/1/0/all/0/1&quot;&gt;Joe O&amp;#x27;Brien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soder_L/0/1/0/all/0/1&quot;&gt;Lisa Soder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucknall_B/0/1/0/all/0/1&quot;&gt;Benjamin Bucknall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bluemke_E/0/1/0/all/0/1&quot;&gt;Emma Bluemke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schuett_J/0/1/0/all/0/1&quot;&gt;Jonas Schuett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trager_R/0/1/0/all/0/1&quot;&gt;Robert Trager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strahm_L/0/1/0/all/0/1&quot;&gt;Lacey Strahm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_R/0/1/0/all/0/1&quot;&gt;Rumman Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14713">
<title>The Rise of the AI Co-Pilot: Lessons for Design from Aviation and Beyond. (arXiv:2311.14713v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2311.14713</link>
<description rdf:parseType="Literal">&lt;p&gt;The fast pace of advances in AI promises to revolutionize various aspects of
knowledge work, extending its influence to daily life and professional fields
alike. We advocate for a paradigm where AI is seen as a collaborative co-pilot,
working under human guidance rather than as a mere tool. Drawing from relevant
research and literature in the disciplines of Human-Computer Interaction and
Human Factors Engineering, we highlight the criticality of maintaining human
oversight in AI interactions. Reflecting on lessons from aviation, we address
the dangers of over-relying on automation, such as diminished human vigilance
and skill erosion. Our paper proposes a design approach that emphasizes active
human engagement, control, and skill enhancement in the AI partnership, aiming
to foster a harmonious, effective, and empowering human-AI relationship. We
particularly call out the critical need to design AI interaction capabilities
and software applications to enable and celebrate the primacy of human agency.
This calls for designs for human-AI partnership that cede ultimate control and
responsibility to the human user as pilot, with the AI co-pilot acting in a
well-defined supporting role.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellen_A/0/1/0/all/0/1&quot;&gt;Abigail Sellen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horvitz_E/0/1/0/all/0/1&quot;&gt;Eric Horvitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14720">
<title>AI Use in Manuscript Preparation for Academic Journals. (arXiv:2311.14720v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2311.14720</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergent abilities of Large Language Models (LLMs), which power tools
like ChatGPT and Bard, have produced both excitement and worry about how AI
will impact academic writing. In response to rising concerns about AI use,
authors of academic publications may decide to voluntarily disclose any AI
tools they use to revise their manuscripts, and journals and conferences could
begin mandating disclosure and/or turn to using detection services, as many
teachers have done with student writing in class settings. Given these looming
possibilities, we investigate whether academics view it as necessary to report
AI use in manuscript preparation and how detectors react to the use of AI in
academic writing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chemaya_N/0/1/0/all/0/1&quot;&gt;Nir Chemaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_D/0/1/0/all/0/1&quot;&gt;Daniel Martin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14722">
<title>Zero-Shot Question Answering over Financial Documents using Large Language Models. (arXiv:2311.14722v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14722</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a large language model (LLM) based approach to answer complex
questions requiring multi-hop numerical reasoning over financial reports. While
LLMs have exhibited remarkable performance on various natural language and
reasoning tasks, complex reasoning problems often rely on few-shot prompts that
require carefully crafted examples. In contrast, our approach uses novel
zero-shot prompts that guide the LLM to encode the required reasoning into a
Python program or a domain specific language. The generated program is then
executed by a program interpreter, thus mitigating the limitations of LLM in
performing accurate arithmetic calculations.
&lt;/p&gt;
&lt;p&gt;We evaluate the proposed approach on three financial datasets using some of
the recently developed generative pretrained transformer (GPT) models and
perform comparisons with various zero-shot baselines. The experimental results
demonstrate that our approach significantly improves the accuracy for all the
LLMs over their respective baselines. We provide a detailed analysis of the
results, generating insights to support our findings. The success of our
approach demonstrates the enormous potential to extract complex domain specific
numerical reasoning by designing zero-shot prompts to effectively exploit the
knowledge embedded in LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phogat_K/0/1/0/all/0/1&quot;&gt;Karmvir Singh Phogat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harsha_C/0/1/0/all/0/1&quot;&gt;Chetan Harsha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasaratha_S/0/1/0/all/0/1&quot;&gt;Sridhar Dasaratha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishna_S/0/1/0/all/0/1&quot;&gt;Shashishekar Ramakrishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puranam_S/0/1/0/all/0/1&quot;&gt;Sai Akhil Puranam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14730">
<title>MemoryCompanion: A Smart Healthcare Solution to Empower Efficient Alzheimer&apos;s Care Via Unleashing Generative AI. (arXiv:2311.14730v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14730</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of Large Language Models (LLMs), notably characterized by GPT
frameworks, there emerges a catalyst for novel healthcare applications. Earlier
iterations of chatbot caregivers, though existent, have yet to achieve a
dimension of human-like authenticity. This paper unveils `MemoryCompanion&apos; a
pioneering digital health solution explicitly tailored for Alzheimer&apos;s disease
(AD) patients and their caregivers. Drawing upon the nuances of GPT technology
and prompt engineering, MemoryCompanion manifests a personalized caregiving
paradigm, fostering interactions via voice-cloning and talking-face mechanisms
that resonate with the familiarity of known companions. Using advanced
prompt-engineering, the system intricately adapts to each patient&apos;s distinct
profile, curating its content and communication style accordingly. This
approach strives to counteract prevalent issues of social isolation and
loneliness frequently observed in AD demographics. Our methodology, grounded in
its innovative design, addresses both the caregiving and technological
challenges intrinsic to this domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lifei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heo_Y/0/1/0/all/0/1&quot;&gt;Yeonie Heo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yi Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14737">
<title>Positional Description Matters for Transformers Arithmetic. (arXiv:2311.14737v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14737</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers, central to the successes in modern Natural Language Processing,
often falter on arithmetic tasks despite their vast capabilities --which
paradoxically include remarkable coding abilities. We observe that a crucial
challenge is their naive reliance on positional information to solve arithmetic
problems with a small number of digits, leading to poor performance on larger
numbers. Herein, we delve deeper into the role of positional encoding, and
propose several ways to fix the issue, either by modifying the positional
encoding directly, or by modifying the representation of the arithmetic task to
leverage standard positional encoding differently. We investigate the value of
these modifications for three tasks: (i) classical multiplication, (ii) length
extrapolation in addition, and (iii) addition in natural language context. For
(i) we train a small model on a small dataset (100M parameters and 300k
samples) with remarkable aptitude in (direct, no scratchpad) 15 digits
multiplication and essentially perfect up to 12 digits, while usual training in
this context would give a model failing at 4 digits multiplication. In the
experiments on addition, we use a mere 120k samples to demonstrate: for (ii)
extrapolation from 10 digits to testing on 12 digits numbers while usual
training would have no extrapolation, and for (iii) almost perfect accuracy up
to 5 digits while usual training would be correct only up to 3 digits (which is
essentially memorization with a training set of 120k samples).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_R/0/1/0/all/0/1&quot;&gt;Ruoqi Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bubeck_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;bastien Bubeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldan_R/0/1/0/all/0/1&quot;&gt;Ronen Eldan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yin Tat Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14741">
<title>@ve: A Chatbot for Latin. (arXiv:2311.14741v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14741</link>
<description rdf:parseType="Literal">&lt;p&gt;Dead, extinct, and endangered languages have been preserved primarily through
audio conservation and the collection and digitization of scripts and have been
promoted through targeted language acquisition efforts. Another possibility
would be to build conversational agents that can master these languages. This
would provide an artificial, active conversational partner which has knowledge
of the vocabulary and grammar, and one learns with it in a different way. The
chatbot @ve, with which one can communicate in Latin, was developed in
2022/2023 based on GPT-3.0. It was additionally equipped with a manually
created knowledge base. After conceptual groundwork, this paper presents the
preparation and implementation of the project. In addition, it summarizes the
test that a Latin expert conducted with the chatbot. A critical discussion
elaborates advantages and disadvantages. @ve could be a new tool for teaching
Latin in a memorable and entertaining way through dialogue. However, the
present implementation is still too prone to glitches for stand-alone use -
i.e., without the accompaniment of a teacher. The use of GPT-4 could be a
solution as well as the extension of the knowledge base. In conclusion, it can
be argued that conversational agents are an innovative approach to promoting
and preserving languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bendel_O/0/1/0/all/0/1&quot;&gt;Oliver Bendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ndiaye_K/0/1/0/all/0/1&quot;&gt;Karim N&amp;#x27;diaye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14744">
<title>Coarse-Grained Configurational Polymer Fingerprints for Property Prediction using Machine Learning. (arXiv:2311.14744v1 [physics.chem-ph])</title>
<link>http://arxiv.org/abs/2311.14744</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a method to generate a configurational level
fingerprint for polymers using the Bead-Spring-Model. Unlike some of the
previous fingerprinting approaches that employ monomer-level information where
atomistic descriptors are computed using quantum chemistry calculations, this
approach incorporates configurational information from a coarse-grained model
of a long polymer chain. The proposed approach may be advantageous for the
study of behavior resulting from large molecular weights. To create this
fingerprint, we make use of two kinds of descriptors. First, we calculate
certain geometric descriptors like Re2, Rg2 etc. and label them as Calculated
Descriptors. Second, we generate a set of data-driven descriptors using an
unsupervised autoencoder model and call them Learnt Descriptors. Using a
combination of both of them, we are able to learn mappings from the structure
to various properties of the polymer chain by training ML models. We test our
fingerprint to predict the probability of occurrence of a configuration at
equilibrium, which is approximated by a simple linear relationship between the
instantaneous internal energy and equilibrium average internal energy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_I/0/1/0/all/0/1&quot;&gt;Ishan Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jha_P/0/1/0/all/0/1&quot;&gt;Prateek K Jha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14756">
<title>Task-Distributionally Robust Data-Free Meta-Learning. (arXiv:2311.14756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.14756</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-Free Meta-Learning (DFML) aims to efficiently learn new tasks by
leveraging multiple pre-trained models without requiring their original
training data. Existing inversion-based DFML methods construct pseudo tasks
from a learnable dataset, which is inversely generated from the pre-trained
model pool. For the first time, we reveal two major challenges hindering their
practical deployments: Task-Distribution Shift (TDS) and Task-Distribution
Corruption (TDC). TDS leads to a biased meta-learner because of the skewed task
distribution towards newly generated tasks. TDC occurs when untrusted models
characterized by misleading labels or poor quality pollute the task
distribution. To tackle these issues, we introduce a robust DFML framework that
ensures task distributional robustness. We propose to meta-learn from a pseudo
task distribution, diversified through task interpolation within a compact
task-memory buffer. This approach reduces the meta-learner&apos;s overreliance on
newly generated tasks by maintaining consistent performance across a broader
range of interpolated memory tasks, thus ensuring its generalization for unseen
tasks. Additionally, our framework seamlessly incorporates an automated model
selection mechanism into the meta-training phase, parameterizing each model&apos;s
reliability as a learnable weight. This is optimized with a policy gradient
algorithm inspired by reinforcement learning, effectively addressing the
non-differentiable challenge posed by model selection. Comprehensive
experiments across various datasets demonstrate the framework&apos;s effectiveness
in mitigating TDS and TDC, underscoring its potential to improve DFML in
real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zixuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yongxian Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14757">
<title>PointOBB: Learning Oriented Object Detection via Single Point Supervision. (arXiv:2311.14757v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14757</link>
<description rdf:parseType="Literal">&lt;p&gt;Single point-supervised object detection is gaining attention due to its
cost-effectiveness. However, existing approaches focus on generating horizontal
bounding boxes (HBBs) while ignoring oriented bounding boxes (OBBs) commonly
used for objects in aerial images. This paper proposes PointOBB, the first
single Point-based OBB generation method, for oriented object detection.
PointOBB operates through the collaborative utilization of three distinctive
views: an original view, a resized view, and a rotated/flipped (rot/flp) view.
Upon the original view, we leverage the resized and rot/flp views to build a
scale augmentation module and an angle acquisition module, respectively. In the
former module, a Scale-Sensitive Consistency (SSC) loss is designed to enhance
the deep network&apos;s ability to perceive the object scale. For accurate object
angle predictions, the latter module incorporates self-supervised learning to
predict angles, which is associated with a scale-guided Dense-to-Sparse (DS)
matching strategy for aggregating dense angles corresponding to sparse objects.
The resized and rot/flp views are switched using a progressive multi-view
switching strategy during training to achieve coupled optimization of scale and
angle. Experimental results on the DIOR-R and DOTA-v1.0 datasets demonstrate
that PointOBB achieves promising performance, and significantly outperforms
potential point-supervised baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Junwei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yansheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14758">
<title>Point2RBox: Combine Knowledge from Synthetic Visual Patterns for End-to-end Oriented Object Detection with Single Point Supervision. (arXiv:2311.14758v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14758</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapidly increasing demand for oriented object detection (OOD),
recent research involving weakly-supervised detectors for learning rotated box
(RBox) from the horizontal box (HBox) has attracted more and more attention. In
this paper, we explore a more challenging yet label-efficient setting, namely
single point-supervised OOD, and present our approach called Point2RBox.
Specifically, we propose to leverage two principles: 1) Synthetic pattern
knowledge combination: By sampling around each labelled point on the image, we
transfer the object feature to synthetic visual patterns with the known
bounding box to provide the knowledge for box regression. 2) Transform
self-supervision: With a transformed input image (e.g. scaled/rotated), the
output RBoxes are trained to follow the same transformation so that the network
can perceive the relative size/rotation between objects. The detector is
further enhanced by a few devised techniques to cope with peripheral issues,
e.g. the anchor/layer assignment as the size of the object is not available in
our point supervision setting. To our best knowledge, Point2RBox is the first
end-to-end solution for point-supervised OOD. In particular, our method uses a
lightweight paradigm, yet it achieves a competitive performance among
point-supervised alternatives, 41.05%/27.62%/80.01% on DOTA/DIOR/HRSC datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Y/0/1/0/all/0/1&quot;&gt;Yu Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qingyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_F/0/1/0/all/0/1&quot;&gt;Feipeng Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14762">
<title>The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024. (arXiv:2311.14762v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14762</link>
<description rdf:parseType="Literal">&lt;p&gt;The 2nd Workshop on Maritime Computer Vision (MaCVi) 2024 addresses maritime
computer vision for Unmanned Aerial Vehicles (UAV) and Unmanned Surface
Vehicles (USV). Three challenges categories are considered: (i) UAV-based
Maritime Object Tracking with Re-identification, (ii) USV-based Maritime
Obstacle Segmentation and Detection, (iii) USV-based Maritime Boat Tracking.
The USV-based Maritime Obstacle Segmentation and Detection features three
sub-challenges, including a new embedded challenge addressing efficicent
inference on real-world embedded devices. This report offers a comprehensive
overview of the findings from the challenges. We provide both statistical and
qualitative analyses, evaluating trends from over 195 submissions. All
datasets, evaluation code, and the leaderboard are available to the public at
https://macvi.org/workshop/macvi24.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiefer_B/0/1/0/all/0/1&quot;&gt;Benjamin Kiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zust_L/0/1/0/all/0/1&quot;&gt;Lojze &amp;#x17d;ust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kristan_M/0/1/0/all/0/1&quot;&gt;Matej Kristan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pers_J/0/1/0/all/0/1&quot;&gt;Janez Per&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tersek_M/0/1/0/all/0/1&quot;&gt;Matija Ter&amp;#x161;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiliem_A/0/1/0/all/0/1&quot;&gt;Arnold Wiliem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messmer_M/0/1/0/all/0/1&quot;&gt;Martin Messmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhongyu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuo_H/0/1/0/all/0/1&quot;&gt;Heng-Cheng Kuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_J/0/1/0/all/0/1&quot;&gt;Jie Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stadler_D/0/1/0/all/0/1&quot;&gt;Daniel Stadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_L/0/1/0/all/0/1&quot;&gt;Lars Sommer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_K/0/1/0/all/0/1&quot;&gt;Kaer Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1&quot;&gt;Aiguo Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1&quot;&gt;Weitu Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lertniphonphan_K/0/1/0/all/0/1&quot;&gt;Kanokphan Lertniphonphan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Feng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhepeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zedda_L/0/1/0/all/0/1&quot;&gt;Luca Zedda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loddo_A/0/1/0/all/0/1&quot;&gt;Andrea Loddo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruberto_C/0/1/0/all/0/1&quot;&gt;Cecilia Di Ruberto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_T/0/1/0/all/0/1&quot;&gt;Tuan-Anh Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Truong_H/0/1/0/all/0/1&quot;&gt;Hai Nguyen-Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_T/0/1/0/all/0/1&quot;&gt;Tan-Sang Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_Q/0/1/0/all/0/1&quot;&gt;Quan-Dung Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Sai-Kit Yeung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yuan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thien_N/0/1/0/all/0/1&quot;&gt;Nguyen Thanh Thien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lixin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuan_S/0/1/0/all/0/1&quot;&gt;Sheng-Yao Kuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_Y/0/1/0/all/0/1&quot;&gt;Yuan-Hao Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_A/0/1/0/all/0/1&quot;&gt;Angel Bueno Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrillo_Perez_B/0/1/0/all/0/1&quot;&gt;Borja Carrillo-Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_A/0/1/0/all/0/1&quot;&gt;Alexander Klein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alex_A/0/1/0/all/0/1&quot;&gt;Antje Alex&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steiniger_Y/0/1/0/all/0/1&quot;&gt;Yannik Steiniger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sattler_F/0/1/0/all/0/1&quot;&gt;Felix Sattler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solano_Carrillo_E/0/1/0/all/0/1&quot;&gt;Edgardo Solano-Carrillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabijanic_M/0/1/0/all/0/1&quot;&gt;Matej Fabijani&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumunec_M/0/1/0/all/0/1&quot;&gt;Magdalena &amp;#x160;umunec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapetanovic_N/0/1/0/all/0/1&quot;&gt;Nadir Kapetanovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michel_A/0/1/0/all/0/1&quot;&gt;Andreas Michel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_W/0/1/0/all/0/1&quot;&gt;Wolfgang Gross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinmann_M/0/1/0/all/0/1&quot;&gt;Martin Weinmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14767">
<title>Low-Cost HEM with Arduino and Zigbee Technologies in the Energy Sector in Colombia. (arXiv:2311.14767v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2311.14767</link>
<description rdf:parseType="Literal">&lt;p&gt;Since no solutions have been proposed in Colombia that seek to reduce the
consumption of electricity at the residential level, this paper describes the
design and implementation of a simple prototype of a low-cost home energy
management system (HEMS). The objective of this plat-form is to monitor the
energy consumption of typical household devices so that users can access the
consumption of each device separately and then establish the strategy that
allows them to reduce energy consumption at home. In order to demonstrate that
our system is viable, the system has been evaluated by measuring weekly energy
consumption with the on-line and off-line HEMS using a test bench with typical
household devices in a Sincelejo typical household. The evaluation has shown
that with the installation of this HEMS, consumption is reduced by 27%. This
shows that it is possible to achieve a good reduction percentage with a
low-cost system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maury_Z/0/1/0/all/0/1&quot;&gt;Zurisaddai de la Cruz Severiche Maury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vilas_A/0/1/0/all/0/1&quot;&gt;Ana Fernandez Vilas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca Diaz Redondo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14768">
<title>AdaDiff: Adaptive Step Selection for Fast Diffusion. (arXiv:2311.14768v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14768</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models, as a type of generative models, have achieved impressive
results in generating images and videos conditioned on textual conditions.
However, the generation process of diffusion models involves denoising for
dozens of steps to produce photorealistic images/videos, which is
computationally expensive. Unlike previous methods that design
``one-size-fits-all&apos;&apos; approaches for speed up, we argue denoising steps should
be sample-specific conditioned on the richness of input texts. To this end, we
introduce AdaDiff, a lightweight framework designed to learn instance-specific
step usage policies, which are then used by the diffusion model for generation.
AdaDiff is optimized using a policy gradient method to maximize a carefully
designed reward function, balancing inference time and generation quality. We
conduct experiments on three image generation and two video generation
benchmarks and demonstrate that our approach achieves similar results in terms
of visual quality compared to the baseline using a fixed 50 denoising steps
while reducing inference time by at least 33%, going as high as 40%.
Furthermore, our qualitative analysis shows that our method allocates more
steps to more informative text conditions and fewer steps to simpler text
conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhen Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_J/0/1/0/all/0/1&quot;&gt;Jie Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14770">
<title>Learning to Cooperate and Communicate Over Imperfect Channels. (arXiv:2311.14770v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2311.14770</link>
<description rdf:parseType="Literal">&lt;p&gt;Information exchange in multi-agent systems improves the cooperation among
agents, especially in partially observable settings. In the real world,
communication is often carried out over imperfect channels. This requires
agents to handle uncertainty due to potential information loss. In this paper,
we consider a cooperative multi-agent system where the agents act and exchange
information in a decentralized manner using a limited and unreliable channel.
To cope with such channel constraints, we propose a novel communication
approach based on independent Q-learning. Our method allows agents to
dynamically adapt how much information to share by sending messages of
different sizes, depending on their local observations and the channel&apos;s
properties. In addition to this message size selection, agents learn to encode
and decode messages to improve their jointly trained policies. We show that our
approach outperforms approaches without adaptive capabilities in a novel
cooperative digit-prediction environment and discuss its limitations in the
traffic junction environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weil_J/0/1/0/all/0/1&quot;&gt;Jannis Weil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ekinci_G/0/1/0/all/0/1&quot;&gt;Gizem Ekinci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1&quot;&gt;Heinz Koeppl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meuser_T/0/1/0/all/0/1&quot;&gt;Tobias Meuser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14786">
<title>GPT-4V Takes the Wheel: Evaluating Promise and Challenges for Pedestrian Behavior Prediction. (arXiv:2311.14786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14786</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing pedestrian behavior prediction methods rely primarily on deep neural
networks that utilize features extracted from video frame sequences. Although
these vision-based models have shown promising results, they face limitations
in effectively capturing and utilizing the dynamic spatio-temporal interactions
between the target pedestrian and its surrounding traffic elements, crucial for
accurate reasoning. Additionally, training these models requires manually
annotating domain-specific datasets, a process that is expensive,
time-consuming, and difficult to generalize to new environments and scenarios.
The recent emergence of Large Multimodal Models (LMMs) offers potential
solutions to these limitations due to their superior visual understanding and
causal reasoning capabilities, which can be harnessed through semi-supervised
training. GPT-4V(ision), the latest iteration of the state-of-the-art
Large-Language Model GPTs, now incorporates vision input capabilities. This
report provides a comprehensive evaluation of the potential of GPT-4V for
pedestrian behavior prediction in autonomous driving using publicly available
datasets: JAAD, PIE, and WiDEVIEW. Quantitative and qualitative evaluations
demonstrate GPT-4V(ision)&apos;s promise in zero-shot pedestrian behavior prediction
and driving scene understanding ability for autonomous driving. However, it
still falls short of the state-of-the-art traditional domain-specific models.
Challenges include difficulties in handling small pedestrians and vehicles in
motion. These limitations highlight the need for further research and
development in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Peng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gautam_A/0/1/0/all/0/1&quot;&gt;Alvika Gautam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saripalli_S/0/1/0/all/0/1&quot;&gt;Srikanth Saripalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14788">
<title>Evaluating Large Language Models through Gender and Racial Stereotypes. (arXiv:2311.14788v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14788</link>
<description rdf:parseType="Literal">&lt;p&gt;Language Models have ushered a new age of AI gaining traction within the NLP
community as well as amongst the general population. AI&apos;s ability to make
predictions, generations and its applications in sensitive decision-making
scenarios, makes it even more important to study these models for possible
biases that may exist and that can be exaggerated. We conduct a quality
comparative study and establish a framework to evaluate language models under
the premise of two kinds of biases: gender and race, in a professional setting.
We find out that while gender bias has reduced immensely in newer models, as
compared to older ones, racial bias still exists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1&quot;&gt;Ananya Malik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14824">
<title>A Reusable AI-Enabled Defect Detection System for Railway Using Ensembled CNN. (arXiv:2311.14824v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14824</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate Defect detection is crucial for ensuring the trustworthiness of
intelligent railway systems. Current approaches rely on single deep-learning
models, like CNNs, which employ a large amount of data to capture underlying
patterns. Training a new defect classifier with limited samples often leads to
overfitting and poor performance on unseen images. To address this, researchers
have advocated transfer learning and fine-tuning the pre-trained models.
However, using a single backbone network in transfer learning still may cause
bottleneck issues and inconsistent performance if it is not suitable for a
specific problem domain. To overcome these challenges, we propose a reusable
AI-enabled defect detection approach. By combining ensemble learning with
transfer learning models (VGG-19, MobileNetV3, and ResNet-50), we improved the
classification accuracy and achieved consistent performance at a certain phase
of training. Our empirical analysis demonstrates better and more consistent
performance compared to other state-of-the-art approaches. The consistency
substantiates the reusability of the defect detection system for newly evolved
defected rail parts. Therefore we anticipate these findings to benefit further
research and development of reusable AI-enabled solutions for railway systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferdousi_R/0/1/0/all/0/1&quot;&gt;Rahatara Ferdousi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laamarti_F/0/1/0/all/0/1&quot;&gt;Fedwa Laamarti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chunsheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14865">
<title>Improving Cross-Domain Hate Speech Generalizability with Emotion Knowledge. (arXiv:2311.14865v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.14865</link>
<description rdf:parseType="Literal">&lt;p&gt;Reliable automatic hate speech (HS) detection systems must adapt to the
in-flow of diverse new data to curtail hate speech. However, hate speech
detection systems commonly lack generalizability in identifying hate speech
dissimilar to data used in training, impeding their robustness in real-world
deployments. In this work, we propose a hate speech generalization framework
that leverages emotion knowledge in a multitask architecture to improve the
generalizability of hate speech detection in a cross-domain setting. We
investigate emotion corpora with varying emotion categorical scopes to
determine the best corpus scope for supplying emotion knowledge to foster
generalized hate speech detection. We further assess the relationship between
using pretrained Transformers models adapted for hate speech and its effect on
our emotion-enriched hate speech generalization model. We perform extensive
experiments on six publicly available datasets sourced from different online
domains and show that our emotion-enriched HS detection generalization method
demonstrates consistent generalization improvement in cross-domain evaluation,
increasing generalization performance up to 18.1% and average cross-domain
performance up to 8.5%, according to the F1 measure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shi Yin Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_S/0/1/0/all/0/1&quot;&gt;Susan Gauch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14874">
<title>Advancing Fluid-Based Thermal Management Systems Design: Leveraging Graph Neural Networks for Graph Regression and Efficient Enumeration Reduction. (arXiv:2311.14874v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2311.14874</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research, we developed a graph-based framework to represent various
aspects of optimal thermal management system design, with the aim of rapidly
and efficiently identifying optimal design candidates. Initially, the
graph-based framework is utilized to generate diverse thermal management system
architectures. The dynamics of these system architectures are modeled under
various loading conditions, and an open-loop optimal controller is employed to
determine each system&apos;s optimal performance. These modeled cases constitute the
dataset, with the corresponding optimal performance values serving as the
labels for the data. In the subsequent step, a Graph Neural Network (GNN) model
is trained on 30% of the labeled data to predict the systems&apos; performance,
effectively addressing a regression problem. Utilizing this trained model, we
estimate the performance values for the remaining 70% of the data, which serves
as the test set. In the third step, the predicted performance values are
employed to rank the test data, facilitating prioritized evaluation of the
design scenarios. Specifically, a small subset of the test data with the
highest estimated ranks undergoes evaluation via the open-loop optimal control
solver. This targeted approach concentrates on evaluating higher-ranked designs
identified by the GNN, replacing the exhaustive search (enumeration-based) of
all design cases. The results demonstrate a significant average reduction of
over 92% in the number of system dynamic modeling and optimal control analyses
required to identify optimal design scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bayat_S/0/1/0/all/0/1&quot;&gt;Saeid Bayat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shahmansouri_N/0/1/0/all/0/1&quot;&gt;Nastaran Shahmansouri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peddada_S/0/1/0/all/0/1&quot;&gt;Satya RT Peddada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tessier_A/0/1/0/all/0/1&quot;&gt;Alex Tessier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Butscher_A/0/1/0/all/0/1&quot;&gt;Adrian Butscher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Allison_J/0/1/0/all/0/1&quot;&gt;James T Allison&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14900">
<title>Resfusion: Prior Residual Noise embedded Denoising Diffusion Probabilistic Models. (arXiv:2311.14900v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14900</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Denoising Diffusion Probabilistic Models have been widely used in
image segmentation, by generating segmentation masks conditioned on the input
image. However, previous works can not seamlessly integrate existing end-to-end
models with denoising diffusion models. Existing research can only select
acceleration steps based on experience rather than calculating them
specifically. Moreover, most methods are limited to small models and
small-scale datasets, unable to generalize to general datasets and a wider
range of tasks. Therefore, we propose Resfusion with a novel resnoise-diffusion
process, which gradually generates segmentation masks or any type of target
image, seamlessly integrating state-of-the-art end-to-end models and denoising
diffusion models. Resfusion bridges the discrepancy between the likelihood
output and the ground truth output through a Markov process. Through the novel
smooth equivalence transformation in resnoise-diffusion process, we determine
the optimal acceleration step. Experimental results demonstrate that Resfusion
combines the capabilities of existing end-to-end models and denoising diffusion
models, further enhancing performance and achieving outstanding results.
Moreover, Resfusion is not limited to segmentation tasks, it can easily
generalize to any general tasks of image generation and exhibit strong
competitiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhenning_S/0/1/0/all/0/1&quot;&gt;Shi Zhenning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Changsheng_D/0/1/0/all/0/1&quot;&gt;Dong Changsheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bin_P/0/1/0/all/0/1&quot;&gt;Pan Bin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xueshuo_X/0/1/0/all/0/1&quot;&gt;Xie Xueshuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Along_H/0/1/0/all/0/1&quot;&gt;He Along&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiaoying_Q/0/1/0/all/0/1&quot;&gt;Qu Qiaoying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Li Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14926">
<title>FreePIH: Training-Free Painterly Image Harmonization with Diffusion Model. (arXiv:2311.14926v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.14926</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides an efficient training-free painterly image harmonization
(PIH) method, dubbed FreePIH, that leverages only a pre-trained diffusion model
to achieve state-of-the-art harmonization results. Unlike existing methods that
require either training auxiliary networks or fine-tuning a large pre-trained
backbone, or both, to harmonize a foreground object with a painterly-style
background image, our FreePIH tames the denoising process as a plug-in module
for foreground image style transfer. Specifically, we find that the very last
few steps of the denoising (i.e., generation) process strongly correspond to
the stylistic information of images, and based on this, we propose to augment
the latent features of both the foreground and background images with Gaussians
for a direct denoising-based harmonization. To guarantee the fidelity of the
harmonized image, we make use of multi-scale features to enforce the
consistency of the content and stability of the foreground objects in the
latent space, and meanwhile, aligning both fore-/back-grounds with the same
style. Moreover, to accommodate the generation with more structural and
textural details, we further integrate text prompts to attend to the latent
features, hence improving the generation quality. Quantitative and qualitative
evaluations on COCO and LAION 5B datasets demonstrate that our method can
surpass representative baselines by large margins.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jingcai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_S/0/1/0/all/0/1&quot;&gt;Song Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qihua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14948">
<title>Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.14948</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach.
&lt;/p&gt;
&lt;p&gt;In this work, we demonstrate that the efficacy of CleanCLIP in mitigating
backdoors is highly dependent on the particular objective used during model
pre-training.
&lt;/p&gt;
&lt;p&gt;We observe that stronger pre-training objectives correlate with harder to
remove backdoors behaviors. We show this by training multimodal models on two
large datasets consisting of 3 million (CC3M) and 6 million (CC6M) datapoints,
under various pre-training objectives, followed by poison removal using
CleanCLIP. We find that CleanCLIP is ineffective when stronger pre-training
objectives are used, even with extensive hyperparameter tuning.
&lt;/p&gt;
&lt;p&gt;Our findings underscore critical considerations for ML practitioners who
pre-train models using large-scale web-curated data and are concerned about
potential backdoor threats. Notably, our results suggest that simpler
pre-training objectives are more amenable to effective backdoor removal. This
insight is pivotal for practitioners seeking to balance the trade-offs between
using stronger pre-training objectives and security against backdoor attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Sahil Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1&quot;&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1&quot;&gt;Soumye Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav Mohanty Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chirag Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1&quot;&gt;John P Dickerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeff Bilmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14994">
<title>Exploring Causal Learning through Graph Neural Networks: An In-depth Review. (arXiv:2311.14994v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.14994</link>
<description rdf:parseType="Literal">&lt;p&gt;In machine learning, exploring data correlations to predict outcomes is a
fundamental task. Recognizing causal relationships embedded within data is
pivotal for a comprehensive understanding of system dynamics, the significance
of which is paramount in data-driven decision-making processes. Beyond
traditional methods, there has been a surge in the use of graph neural networks
(GNNs) for causal learning, given their capabilities as universal data
approximators. Thus, a thorough review of the advancements in causal learning
using GNNs is both relevant and timely. To structure this review, we introduce
a novel taxonomy that encompasses various state-of-the-art GNN methods employed
in studying causality. GNNs are further categorized based on their applications
in the causality domain. We further provide an exhaustive compilation of
datasets integral to causal learning with GNNs to serve as a resource for
practical study. This review also touches upon the application of causal
learning across diverse sectors. We conclude the review with insights into
potential challenges and promising avenues for future exploration in this
rapidly evolving field of machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Job_S/0/1/0/all/0/1&quot;&gt;Simi Job&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_X/0/1/0/all/0/1&quot;&gt;Xiaohui Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_T/0/1/0/all/0/1&quot;&gt;Taotao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Haoran Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1&quot;&gt;Jianming Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15016">
<title>E-CORE: Emotion Correlation Enhanced Empathetic Dialogue Generation. (arXiv:2311.15016v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.15016</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving empathy is a crucial step toward humanized dialogue systems.
Current approaches for empathetic dialogue generation mainly perceive an
emotional label to generate an empathetic response conditioned on it, which
simply treat emotions independently, but ignore the intrinsic emotion
correlation in dialogues, resulting in inaccurate emotion perception and
unsuitable response generation. In this paper, we propose a novel emotion
correlation enhanced empathetic dialogue generation framework, which
comprehensively realizes emotion correlation learning, utilization, and
supervising. Specifically, a multi-resolution emotion graph is devised to
capture context-based emotion interactions from different resolutions, further
modeling emotion correlation. Then we propose an emotion correlation enhanced
decoder, with a novel correlation-aware aggregation and soft/hard strategy,
respectively improving the emotion perception and response generation.
Experimental results on the benchmark dataset demonstrate the superiority of
our model in both empathetic perception and expression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_F/0/1/0/all/0/1&quot;&gt;Fengyi Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhendong Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15033">
<title>Agent as Cerebrum, Controller as Cerebellum: Implementing an Embodied LMM-based Agent on Drones. (arXiv:2311.15033v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.15033</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present a novel paradigm for industrial robotic embodied
agents, encapsulating an &apos;agent as cerebrum, controller as cerebellum&apos;
architecture. Our approach harnesses the power of Large Multimodal Models
(LMMs) within an agent framework known as AeroAgent, tailored for drone
technology in industrial settings. To facilitate seamless integration with
robotic systems, we introduce ROSchain, a bespoke linkage framework connecting
LMM-based agents to the Robot Operating System (ROS). We report findings from
extensive empirical research, including simulated experiments on the Airgen and
real-world case study, particularly in individual search and rescue operations.
The results demonstrate AeroAgent&apos;s superior performance in comparison to
existing Deep Reinforcement Learning (DRL)-based agents, highlighting the
advantages of the embodied LMM in complex, real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoran Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_F/0/1/0/all/0/1&quot;&gt;Fengxing Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_H/0/1/0/all/0/1&quot;&gt;Huqiuyue Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yaoming Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15036">
<title>On-Device Soft Sensors: Real-Time Fluid Flow Estimation from Level Sensor Data. (arXiv:2311.15036v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.15036</link>
<description rdf:parseType="Literal">&lt;p&gt;Soft sensors are crucial in bridging autonomous systems&apos; physical and digital
realms, enhancing sensor fusion and perception. Instead of deploying soft
sensors on the Cloud, this study shift towards employing on-device soft
sensors, promising heightened efficiency and bolstering data security. Our
approach substantially improves energy efficiency by deploying Artificial
Intelligence (AI) directly on devices within a wireless sensor network.
Furthermore, the synergistic integration of the Microcontroller Unit and
Field-Programmable Gate Array (FPGA) leverages the rapid AI inference
capabilities of the latter. Empirical evidence from our real-world use case
demonstrates that FPGA-based soft sensors achieve inference times ranging
remarkably from 1.04 to 12.04 microseconds. These compelling results highlight
the considerable potential of our innovative approach for executing real-time
inference tasks efficiently, thereby presenting a feasible alternative that
effectively addresses the latency challenges intrinsic to Cloud-based
deployments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1&quot;&gt;Tianheng Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_G/0/1/0/all/0/1&quot;&gt;Gregor Schiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15041">
<title>MPCNN: A Novel Matrix Profile Approach for CNN-based Sleep Apnea Classification. (arXiv:2311.15041v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.15041</link>
<description rdf:parseType="Literal">&lt;p&gt;Sleep apnea (SA) is a significant respiratory condition that poses a major
global health challenge. Previous studies have investigated several machine and
deep learning models for electrocardiogram (ECG)-based SA diagnoses. Despite
these advancements, conventional feature extractions derived from ECG signals,
such as R-peaks and RR intervals, may fail to capture crucial information
encompassed within the complete PQRST segments. In this study, we propose an
innovative approach to address this diagnostic gap by delving deeper into the
comprehensive segments of the ECG signal. The proposed methodology draws
inspiration from Matrix Profile algorithms, which generate an Euclidean
distance profile from fixed-length signal subsequences. From this, we derived
the Min Distance Profile (MinDP), Max Distance Profile (MaxDP), and Mean
Distance Profile (MeanDP) based on the minimum, maximum, and mean of the
profile distances, respectively. To validate the effectiveness of our approach,
we use the modified LeNet-5 architecture as the primary CNN model, along with
two existing lightweight models, BAFNet and SE-MSCNN, for ECG classification
tasks. Our extensive experimental results on the PhysioNet Apnea-ECG dataset
revealed that with the new feature extraction method, we achieved a per-segment
accuracy up to 92.11 \% and a per-recording accuracy of 100\%. Moreover, it
yielded the highest correlation compared to state-of-the-art methods, with a
correlation coefficient of 0.989. By introducing a new feature extraction
method based on distance relationships, we enhanced the performance of certain
lightweight models, showing potential for home sleep apnea test (HSAT) and SA
detection in IoT devices. The source code for this work is made publicly
available in GitHub: https://github.com/vinuni-vishc/MPCNN-Sleep-Apnea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Hieu X. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duong V. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_H/0/1/0/all/0/1&quot;&gt;Hieu H. Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Do_C/0/1/0/all/0/1&quot;&gt;Cuong D. Do&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15056">
<title>Accurate and interpretable drug-drug interaction prediction enabled by knowledge subgraph learning. (arXiv:2311.15056v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.15056</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Discovering potential drug-drug interactions (DDIs) is a
long-standing challenge in clinical treatments and drug developments. Recently,
deep learning techniques have been developed for DDI prediction. However, they
generally require a huge number of samples, while known DDIs are rare.
&lt;/p&gt;
&lt;p&gt;Methods: In this work, we present KnowDDI, a graph neural network-based
method that addresses the above challenge. KnowDDI enhances drug
representations by adaptively leveraging rich neighborhood information from
large biomedical knowledge graphs. Then, it learns a knowledge subgraph for
each drug-pair to interpret the predicted DDI, where each of the edges is
associated with a connection strength indicating the importance of a known DDI
or resembling strength between a drug-pair whose connection is unknown. Thus,
the lack of DDIs is implicitly compensated by the enriched drug representations
and propagated drug similarities.
&lt;/p&gt;
&lt;p&gt;Results: We evaluate KnowDDI on two benchmark DDI datasets. Results show that
KnowDDI obtains the state-of-the-art prediction performance with better
interpretability. We also find that KnowDDI suffers less than existing works
given a sparser knowledge graph. This indicates that the propagated drug
similarities play a more important role in compensating for the lack of DDIs
when the drug representations are less enriched.
&lt;/p&gt;
&lt;p&gt;Conclusions: KnowDDI nicely combines the efficiency of deep learning
techniques and the rich prior knowledge in biomedical knowledge graphs. As an
original open-source tool, KnowDDI can help detect possible interactions in a
broad range of relevant interaction prediction tasks, such as protein-protein
interactions, drug-target interactions and disease-gene interactions,
eventually promoting the development of biomedicine and healthcare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zaifei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Quanming Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1807.03418">
<title>AudioMNIST: Exploring Explainable Artificial Intelligence for Audio Analysis on a Simple Benchmark. (arXiv:1807.03418v3 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/1807.03418</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Artificial Intelligence (XAI) is targeted at understanding how
models perform feature selection and derive their classification decisions.
This paper explores post-hoc explanations for deep neural networks in the audio
domain. Notably, we present a novel Open Source audio dataset consisting of
30,000 audio samples of English spoken digits which we use for classification
tasks on spoken digits and speakers&apos; biological sex. We use the popular XAI
technique Layer-wise Relevance Propagation (LRP) to identify relevant features
for two neural network architectures that process either waveform or
spectrogram representations of the data. Based on the relevance scores obtained
from LRP, hypotheses about the neural networks&apos; feature selection are derived
and subsequently tested through systematic manipulations of the input data.
Further, we take a step beyond visual explanations and introduce audible
heatmaps. We demonstrate the superior interpretability of audible explanations
over visual ones in a human user study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becker_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Becker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vielhaben_J/0/1/0/all/0/1&quot;&gt;Johanna Vielhaben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ackermann_M/0/1/0/all/0/1&quot;&gt;Marcel Ackermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapuschkin_S/0/1/0/all/0/1&quot;&gt;Sebastian Lapuschkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samek_W/0/1/0/all/0/1&quot;&gt;Wojciech Samek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2107.10021">
<title>Neuradicon: operational representation learning of neuroimaging reports. (arXiv:2107.10021v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2107.10021</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiological reports typically summarize the content and interpretation of
imaging studies in unstructured form that precludes quantitative analysis. This
limits the monitoring of radiological services to throughput undifferentiated
by content, impeding specific, targeted operational optimization. Here we
present Neuradicon, a natural language processing (NLP) framework for
quantitative analysis of neuroradiological reports. Our framework is a hybrid
of rule-based and artificial intelligence models to represent neurological
reports in succinct, quantitative form optimally suited to operational
guidance. We demonstrate the application of Neuradicon to operational
phenotyping of a corpus of 336,569 reports, and report excellent
generalizability across time and two independent healthcare institutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_H/0/1/0/all/0/1&quot;&gt;Henry Watkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gray_R/0/1/0/all/0/1&quot;&gt;Robert Gray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julius_A/0/1/0/all/0/1&quot;&gt;Adam Julius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mah_Y/0/1/0/all/0/1&quot;&gt;Yee-Haur Mah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinaya_W/0/1/0/all/0/1&quot;&gt;Walter H.L. Pinaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_P/0/1/0/all/0/1&quot;&gt;Paul Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_A/0/1/0/all/0/1&quot;&gt;Ashwani Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engleitner_H/0/1/0/all/0/1&quot;&gt;Holger Engleitner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cardoso_J/0/1/0/all/0/1&quot;&gt;Jorge Cardoso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ourselin_S/0/1/0/all/0/1&quot;&gt;Sebastien Ourselin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rees_G/0/1/0/all/0/1&quot;&gt;Geraint Rees&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaeger_R/0/1/0/all/0/1&quot;&gt;Rolf Jaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachev_P/0/1/0/all/0/1&quot;&gt;Parashkev Nachev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.06584">
<title>Choosing the Right Algorithm With Hints From Complexity Theory. (arXiv:2109.06584v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2109.06584</link>
<description rdf:parseType="Literal">&lt;p&gt;Choosing a suitable algorithm from the myriads of different search heuristics
is difficult when faced with a novel optimization problem. In this work, we
argue that the purely academic question of what could be the best possible
algorithm in a certain broad class of black-box optimizers can give fruitful
indications in which direction to search for good established optimization
heuristics. We demonstrate this approach on the recently proposed DLB
benchmark, for which the only known results are $O(n^3)$ runtimes for several
classic evolutionary algorithms and an $O(n^2 \log n)$ runtime for an
estimation-of-distribution algorithm. Our finding that the unary unbiased
black-box complexity is only $O(n^2)$ suggests the Metropolis algorithm as an
interesting candidate and we prove that it solves the DLB problem in quadratic
time. Since we also prove that better runtimes cannot be obtained in the class
of unary unbiased algorithms, we shift our attention to algorithms that use the
information of more parents to generate new solutions. An artificial algorithm
of this type having an $O(n \log n)$ runtime leads to the result that the
significance-based compact genetic algorithm (sig-cGA) can solve the DLB
problem also in time $O(n \log n)$ with high probability. Our experiments show
a remarkably good performance of the Metropolis algorithm, clearly the best of
all algorithms regarded for reasonable problem sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shouda Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Weijie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doerr_B/0/1/0/all/0/1&quot;&gt;Benjamin Doerr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.12589">
<title>A deep reinforcement learning model for predictive maintenance planning of road assets: Integrating LCA and LCCA. (arXiv:2112.12589v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.12589</link>
<description rdf:parseType="Literal">&lt;p&gt;Road maintenance planning is an integral part of road asset management. One
of the main challenges in Maintenance and Rehabilitation (M&amp;amp;R) practices is to
determine maintenance type and timing. This research proposes a framework using
Reinforcement Learning (RL) based on the Long Term Pavement Performance (LTPP)
database to determine the type and timing of M&amp;amp;R practices. A predictive DNN
model is first developed in the proposed algorithm, which serves as the
Environment for the RL algorithm. For the Policy estimation of the RL model,
both DQN and PPO models are developed. However, PPO has been selected in the
end due to better convergence and higher sample efficiency. Indicators used in
this study are International Roughness Index (IRI) and Rutting Depth (RD).
Initially, we considered Cracking Metric (CM) as the third indicator, but it
was then excluded due to the much fewer data compared to other indicators,
which resulted in lower accuracy of the results. Furthermore, in
cost-effectiveness calculation (reward), we considered both the economic and
environmental impacts of M&amp;amp;R treatments. Costs and environmental impacts have
been evaluated with paLATE 2.0 software. Our method is tested on a hypothetical
case study of a six-lane highway with 23 kilometers length located in Texas,
which has a warm and wet climate. The results propose a 20-year M&amp;amp;R plan in
which road condition remains in an excellent condition range. Because the early
state of the road is at a good level of service, there is no need for heavy
maintenance practices in the first years. Later, after heavy M&amp;amp;R actions, there
are several 1-2 years of no need for treatments. All of these show that the
proposed plan has a logical result. Decision-makers and transportation agencies
can use this scheme to conduct better maintenance practices that can prevent
budget waste and, at the same time, minimize the environmental impacts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latifi_M/0/1/0/all/0/1&quot;&gt;Moein Latifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darvishvand_F/0/1/0/all/0/1&quot;&gt;Fateme Golivand Darvishvand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandel_O/0/1/0/all/0/1&quot;&gt;Omid Khandel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowsoud_M/0/1/0/all/0/1&quot;&gt;Mobin Latifi Nowsoud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.12181">
<title>Collaborative Target Search with a Visual Drone Swarm: An Adaptive Curriculum Embedded Multistage Reinforcement Learning Approach. (arXiv:2204.12181v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2204.12181</link>
<description rdf:parseType="Literal">&lt;p&gt;Equipping drones with target search capabilities is highly desirable for
applications in disaster rescue and smart warehouse delivery systems. Multiple
intelligent drones that can collaborate with each other and maneuver among
obstacles show more effectiveness in accomplishing tasks in a shorter amount of
time. However, carrying out collaborative target search (CTS) without prior
target information is extremely challenging, especially with a visual drone
swarm. In this work, we propose a novel data-efficient deep reinforcement
learning (DRL) approach called adaptive curriculum embedded multistage learning
(ACEMSL) to address these challenges, mainly 3-D sparse reward space
exploration with limited visual perception and collaborative behavior
requirements. Specifically, we decompose the CTS task into several subtasks
including individual obstacle avoidance, target search, and inter-agent
collaboration, and progressively train the agents with multistage learning.
Meanwhile, an adaptive embedded curriculum (AEC) is designed, where the task
difficulty level (TDL) can be adaptively adjusted based on the success rate
(SR) achieved in training. ACEMSL allows data-efficient training and
individual-team reward allocation for the visual drone swarm. Furthermore, we
deploy the trained model over a real visual drone swarm and perform CTS
operations without fine-tuning. Extensive simulations and real-world flight
tests validate the effectiveness and generalizability of ACEMSL. The project is
available at https://github.com/NTU-UAVG/CTS-visual-drone-swarm.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jiaping Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pisutsin_P/0/1/0/all/0/1&quot;&gt;Phumrapee Pisutsin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feroskhan_M/0/1/0/all/0/1&quot;&gt;Mir Feroskhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.08253">
<title>Momentum-Based Policy Gradient with Second-Order Information. (arXiv:2205.08253v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.08253</link>
<description rdf:parseType="Literal">&lt;p&gt;Variance-reduced gradient estimators for policy gradient methods have been
one of the main focus of research in the reinforcement learning in recent years
as they allow acceleration of the estimation process. We propose a
variance-reduced policy-gradient method, called SHARP, which incorporates
second-order information into stochastic gradient descent (SGD) using momentum
with a time-varying learning rate. SHARP algorithm is parameter-free, achieving
$\epsilon$-approximate first-order stationary point with $O(\epsilon^{-3})$
number of trajectories, while using a batch size of $O(1)$ at each iteration.
Unlike most previous work, our proposed algorithm does not require importance
sampling which can compromise the advantage of variance reduction process.
Moreover, the variance of estimation error decays with the fast rate of
$O(1/t^{2/3})$ where $t$ is the number of iterations. Our extensive
experimental evaluations show the effectiveness of the proposed algorithm on
various control tasks and its advantage over the state of the art in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehkaleybar_S/0/1/0/all/0/1&quot;&gt;Saber Salehkaleybar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khorasani_S/0/1/0/all/0/1&quot;&gt;Sadegh Khorasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_N/0/1/0/all/0/1&quot;&gt;Niao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_P/0/1/0/all/0/1&quot;&gt;Patrick Thiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10498">
<title>PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change. (arXiv:2206.10498v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10498</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating plans of action, and reasoning about change have long been
considered a core competence of intelligent agents. It is thus no surprise that
evaluating the planning and reasoning capabilities of large language models
(LLMs) has become a hot topic of research. Most claims about LLM planning
capabilities are however based on common sense tasks-where it becomes hard to
tell whether LLMs are planning or merely retrieving from their vast world
knowledge. There is a strong need for systematic and extensible planning
benchmarks with sufficient diversity to evaluate whether LLMs have innate
planning capabilities. Motivated by this, we propose PlanBench, an extensible
benchmark suite based on the kinds of domains used in the automated planning
community, especially in the International Planning Competition, to test the
capabilities of LLMs in planning or reasoning about actions and change.
PlanBench provides sufficient diversity in both the task domains and the
specific planning capabilities. Our studies also show that on many critical
capabilities-including plan generation-LLM performance falls quite short, even
with the SOTA models. PlanBench can thus function as a useful marker of
progress of LLMs in planning and reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valmeekam_K/0/1/0/all/0/1&quot;&gt;Karthik Valmeekam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marquez_M/0/1/0/all/0/1&quot;&gt;Matthew Marquez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olmo_A/0/1/0/all/0/1&quot;&gt;Alberto Olmo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sreedharan_S/0/1/0/all/0/1&quot;&gt;Sarath Sreedharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambhampati_S/0/1/0/all/0/1&quot;&gt;Subbarao Kambhampati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04568">
<title>The Impact of Data Corruption on Named Entity Recognition for Low-resourced Languages. (arXiv:2208.04568v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04568</link>
<description rdf:parseType="Literal">&lt;p&gt;Data availability and quality are major challenges in natural language
processing for low-resourced languages. In particular, there is significantly
less data available than for higher-resourced languages. This data is also
often of low quality, rife with errors, invalid text or incorrect annotations.
Many prior works focus on dealing with these problems, either by generating
synthetic data, or filtering out low-quality parts of datasets. We instead
investigate these factors more deeply, by systematically measuring the effect
of data quantity and quality on the performance of pre-trained language models
in a low-resourced setting. Our results show that having fewer
completely-labelled sentences is significantly better than having more
sentences with missing labels; and that models can perform remarkably well with
only 10% of the training data. Importantly, these results are consistent across
ten low-resource languages, English, and four pre-trained models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fokam_M/0/1/0/all/0/1&quot;&gt;Manuel Fokam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beukman_M/0/1/0/all/0/1&quot;&gt;Michael Beukman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.14362">
<title>AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels. (arXiv:2208.14362v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.14362</link>
<description rdf:parseType="Literal">&lt;p&gt;Weak supervision (WS) is a powerful method to build labeled datasets for
training supervised models in the face of little-to-no labeled data. It
replaces hand-labeling data with aggregating multiple noisy-but-cheap label
estimates expressed by labeling functions (LFs). While it has been used
successfully in many domains, weak supervision&apos;s application scope is limited
by the difficulty of constructing labeling functions for domains with complex
or high-dimensional features. To address this, a handful of methods have
proposed automating the LF design process using a small set of ground truth
labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating
automated WS (AutoWS) techniques in challenging WS settings -- a set of diverse
application domains on which it has been previously difficult or impossible to
apply traditional WS techniques. While AutoWS is a promising direction toward
expanding the application-scope of WS, the emergence of powerful methods such
as zero-shot foundation models reveals the need to understand how AutoWS
techniques compare or cooperate with modern zero-shot or few-shot learners.
This informs the central question of AutoWS-Bench-101: given an initial set of
100 labels for each task, we ask whether a practitioner should use an AutoWS
method to generate additional labels or use some simpler baseline, such as
zero-shot predictions from a foundation model or supervised learning. We
observe that in many settings, it is necessary for AutoWS methods to
incorporate signal from foundation models if they are to outperform simple
few-shot baselines, and AutoWS-Bench-101 promotes future research in this
direction. We conclude with a thorough ablation study of AutoWS methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1&quot;&gt;Nicholas Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xintong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzu-Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1&quot;&gt;Dyah Adila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenberg_S/0/1/0/all/0/1&quot;&gt;Spencer Schoenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cheng-Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pick_L/0/1/0/all/0/1&quot;&gt;Lauren Pick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Haotian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albarghouthi_A/0/1/0/all/0/1&quot;&gt;Aws Albarghouthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.00462">
<title>MA-RECON: Mask-aware deep-neural-network for robust fast MRI k-space interpolation. (arXiv:2209.00462v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.00462</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality reconstruction of MRI images from under-sampled `k-space&apos; data,
which is in the Fourier domain, is crucial for shortening MRI acquisition times
and ensuring superior temporal resolution. Over recent years, a wealth of deep
neural network (DNN) methods have emerged, aiming to tackle the complex,
ill-posed inverse problem linked to this process. However, their instability
against variations in the acquisition process and anatomical distribution
exposes a deficiency in the generalization of relevant physical models within
these DNN architectures. The goal of our work is to enhance the generalization
capabilities of DNN methods for k-space interpolation by introducing
`MA-RECON&apos;, an innovative mask-aware DNN architecture and associated training
method. Unlike preceding approaches, our `MA-RECON&apos; architecture encodes not
only the observed data but also the under-sampling mask within the model
structure. It implements a tailored training approach that leverages data
generated with a variety of under-sampling masks to stimulate the model&apos;s
generalization of the under-sampled MRI reconstruction problem. Therefore,
effectively represents the associated inverse problem, akin to the classical
compressed sensing approach. The benefits of our MA-RECON approach were
affirmed through rigorous testing with the widely accessible fastMRI dataset.
Compared to standard DNN methods and DNNs trained with under-sampling mask
augmentation, our approach demonstrated superior generalization capabilities.
This resulted in a considerable improvement in robustness against variations in
both the acquisition process and anatomical distribution, especially in regions
with pathology. In conclusion, our mask-aware strategy holds promise for
enhancing the generalization capacity and robustness of DNN-based methodologies
for MRI reconstruction from undersampled k-space data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Avidan_N/0/1/0/all/0/1&quot;&gt;Nitzan Avidan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Freiman_M/0/1/0/all/0/1&quot;&gt;Moti Freiman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07042">
<title>Efficient Perception, Planning, and Control Algorithms for Vision-Based Automated Vehicles. (arXiv:2209.07042v5 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07042</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous vehicles have limited computational resources; hence, their
control systems must be efficient. The cost and size of sensors have limited
the development of self-driving cars. To overcome these restrictions, this
study proposes an efficient framework for the operation of vision-based
automatic vehicles; the framework requires only a monocular camera and a few
inexpensive radars. The proposed algorithm comprises a multi-task UNet (MTUNet)
network for extracting image features and constrained iterative linear
quadratic regulator (CILQR) and vision predictive control (VPC) modules for
rapid motion planning and control. MTUNet is designed to simultaneously solve
lane line segmentation, the ego vehicle&apos;s heading angle regression, road type
classification, and traffic object detection tasks at approximately 40 FPS
(frames per second) for 228 x 228 pixel RGB input images. The CILQR controllers
then use the MTUNet outputs and radar data as inputs to produce driving
commands for lateral and longitudinal vehicle guidance within only 1 ms. In
particular, the VPC algorithm is included to reduce steering command latency to
below actuator latency to prevent self-driving vehicle performance degradation
during tight turns. The VPC algorithm uses road curvature data from MTUNet to
estimate the correction of the current steering angle at a look-ahead point to
adjust the turning amount. Including the VPC algorithm in a VPC-CILQR
controller on curvy roads leads to higher performance than CILQR alone. Our
experiments demonstrate that the proposed autonomous driving system, which does
not require high-definition maps, could be applied in current autonomous
vehicles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Der-Hau Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03829">
<title>Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03829</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive review of past and current advances in
the early detection of bark beetle-induced tree mortality from three primary
perspectives: bark beetle &amp;amp; host interactions, RS, and ML/DL. In contrast to
prior efforts, this review encompasses all RS systems and emphasizes ML/DL
methods to investigate their strengths and weaknesses. We parse existing
literature based on multi- or hyper-spectral analyses and distill their
knowledge based on: bark beetle species &amp;amp; attack phases with a primary emphasis
on early stages of attacks, host trees, study regions, RS platforms &amp;amp; sensors,
spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation
indices (SVIs), ML approaches, learning schemes, task categories, models,
algorithms, classes/clusters, features, and DL networks &amp;amp; architectures.
Although DL-based methods and the random forest (RF) algorithm showed promising
results, highlighting their potential to detect subtle changes across visible,
thermal, and short-wave infrared (SWIR) spectral regions, they still have
limited effectiveness and high uncertainties. To inspire novel solutions to
these shortcomings, we delve into the principal challenges &amp;amp; opportunities from
different perspectives, enabling a deeper understanding of the current state of
research and guiding future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1&quot;&gt;Devin Goodsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1&quot;&gt;Nilanjan Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1&quot;&gt;Nadir Erbilgin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06462">
<title>Self-Guided Diffusion Models. (arXiv:2210.06462v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06462</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated remarkable progress in image generation
quality, especially when guidance is used to control the generative process.
However, guidance requires a large amount of image-annotation pairs for
training and is thus dependent on their availability, correctness and
unbiasedness. In this paper, we eliminate the need for such annotation by
instead leveraging the flexibility of self-supervision signals to design a
framework for self-guided diffusion models. By leveraging a feature extraction
function and a self-annotation function, our method provides guidance signals
at various image granularities: from the level of holistic images to object
boxes and even segmentation masks. Our experiments on single-label and
multi-label image datasets demonstrate that self-labeled guidance always
outperforms diffusion models without guidance and may even surpass guidance
based on ground-truth labels, especially on unbalanced data. When equipped with
self-supervised box or mask proposals, our method further generates visually
diverse yet semantically consistent images, without the need for any class,
box, or segment label annotation. Self-guided diffusion is simple, flexible and
expected to profit from deployment at scale. Source code will be at:
https://taohu.me/sgdm/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1&quot;&gt;Vincent Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David W Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki M. Asano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burghouts_G/0/1/0/all/0/1&quot;&gt;Gertjan J. Burghouts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.09846">
<title>G-PECNet: Towards a Generalizable Pedestrian Trajectory Prediction System. (arXiv:2210.09846v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.09846</link>
<description rdf:parseType="Literal">&lt;p&gt;Navigating dynamic physical environments without obstructing or damaging
human assets is of quintessential importance for social robots. In this work,
we solve autonomous drone navigation&apos;s sub-problem of predicting out-of-domain
human and agent trajectories using a deep generative model. Our method:
General-PECNet or G-PECNet observes an improvement of 9.5\% on the Final
Displacement Error (FDE) on 2020&apos;s benchmark: PECNet through a combination of
architectural improvements inspired by periodic activation functions and
synthetic trajectory (data) augmentations using Hidden Markov Models (HMMs) and
Reinforcement Learning (RL). Additionally, we propose a simple
geometry-inspired metric for trajectory non-linearity and outlier detection,
helpful for the task. Code available at
$\href{https://github.com/Aryan-Garg/PECNet-Pedestrian-Trajectory-Prediction.git}{GitHub}$
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_A/0/1/0/all/0/1&quot;&gt;Aryan Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rameshan_R/0/1/0/all/0/1&quot;&gt;Renu M. Rameshan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10013">
<title>DocAsRef: An Empirical Study on Repurposing Reference-Based Summary Quality Metrics Reference-Freely. (arXiv:2212.10013v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10013</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated summary quality assessment falls into two categories:
reference-based and reference-free. Reference-based metrics, historically
deemed more accurate due to the additional information provided by
human-written references, are limited by their reliance on human input. In this
paper, we hypothesize that the comparison methodologies used by some
reference-based metrics to evaluate a system summary against its corresponding
reference can be effectively adapted to assess it against its source document,
thereby transforming these metrics into reference-free ones. Experimental
results support this hypothesis. After being repurposed reference-freely, the
zero-shot BERTScore using the pretrained DeBERTa-large-MNLI model of &amp;lt;0.5B
parameters consistently outperforms its original reference-based version across
various aspects on the SummEval and Newsroom datasets. It also excels in
comparison to most existing reference-free metrics and closely competes with
zero-shot summary evaluators based on GPT-3.5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_F/0/1/0/all/0/1&quot;&gt;Forrest Sheng Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_R/0/1/0/all/0/1&quot;&gt;Ruixuan Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_G/0/1/0/all/0/1&quot;&gt;Ge Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yinfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hebi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_M/0/1/0/all/0/1&quot;&gt;Minghui Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Youbiao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12015">
<title>Stochastic analysis of the Elo rating algorithm in round-robin tournaments. (arXiv:2212.12015v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12015</link>
<description rdf:parseType="Literal">&lt;p&gt;The Elo algorithm, renowned for its simplicity, is widely used for rating in
sports tournaments and other applications. However, despite its widespread use,
a detailed understanding of the convergence characteristics of the Elo
algorithm is still lacking. Aiming to fill this gap, this paper presents a
comprehensive (stochastic) analysis of the Elo algorithm, considering
round-robin tournaments. Specifically, analytical expressions are derived
describing the evolution of the skills and performance metrics. Then, taking
into account the relationship between the behavior of the algorithm and the
step-size value, which is a hyperparameter that can be controlled, design
guidelines and discussions about the performance of the algorithm are provided.
Experimental results are shown confirming the accuracy of the analysis and
illustrating the applicability of the theoretical findings using real-world
data obtained from SuperLega, the Italian volleyball league.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanco_D/0/1/0/all/0/1&quot;&gt;Daniel Gomes de Pinho Zanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szczecinski_L/0/1/0/all/0/1&quot;&gt;Leszek Szczecinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhn_E/0/1/0/all/0/1&quot;&gt;Eduardo Vinicius Kuhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seara_R/0/1/0/all/0/1&quot;&gt;Rui Seara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05543">
<title>Adding Conditional Control to Text-to-Image Diffusion Models. (arXiv:2302.05543v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05543</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ControlNet, a neural network architecture to add spatial
conditioning controls to large, pretrained text-to-image diffusion models.
ControlNet locks the production-ready large diffusion models, and reuses their
deep and robust encoding layers pretrained with billions of images as a strong
backbone to learn a diverse set of conditional controls. The neural
architecture is connected with &quot;zero convolutions&quot; (zero-initialized
convolution layers) that progressively grow the parameters from zero and ensure
that no harmful noise could affect the finetuning. We test various conditioning
controls, eg, edges, depth, segmentation, human pose, etc, with Stable
Diffusion, using single or multiple conditions, with or without prompts. We
show that the training of ControlNets is robust with small (&amp;lt;50k) and large
(&amp;gt;1m) datasets. Extensive results show that ControlNet may facilitate wider
applications to control image diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lvmin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_A/0/1/0/all/0/1&quot;&gt;Anyi Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawala_M/0/1/0/all/0/1&quot;&gt;Maneesh Agrawala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13997">
<title>PowerPruning: Selecting Weights and Activations for Power-Efficient Neural Network Acceleration. (arXiv:2303.13997v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13997</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been successfully applied in various fields.
A major challenge of deploying DNNs, especially on edge devices, is power
consumption, due to the large number of multiply-and-accumulate (MAC)
operations. To address this challenge, we propose PowerPruning, a novel method
to reduce power consumption in digital neural network accelerators by selecting
weights that lead to less power consumption in MAC operations. In addition, the
timing characteristics of the selected weights together with all activation
transitions are evaluated. The weights and activations that lead to small
delays are further selected. Consequently, the maximum delay of the sensitized
circuit paths in the MAC units is reduced even without modifying MAC units,
which thus allows a flexible scaling of supply voltage to reduce power
consumption further. Together with retraining, the proposed method can reduce
power consumption of DNNs on hardware by up to 78.3% with only a slight
accuracy loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petri_R/0/1/0/all/0/1&quot;&gt;Richard Petri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Grace Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlichtmann_U/0/1/0/all/0/1&quot;&gt;Ulf Schlichtmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15445">
<title>IRFL: Image Recognition of Figurative Language. (arXiv:2303.15445v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15445</link>
<description rdf:parseType="Literal">&lt;p&gt;Figures of speech such as metaphors, similes, and idioms are integral parts
of human communication. They are ubiquitous in many forms of discourse,
allowing people to convey complex, abstract ideas and evoke emotion. As
figurative forms are often conveyed through multiple modalities (e.g., both
text and images), understanding multimodal figurative language is an important
AI challenge, weaving together profound vision, language, commonsense and
cultural knowledge. In this work, we develop the Image Recognition of
Figurative Language (IRFL) dataset. We leverage human annotation and an
automatic pipeline we created to generate a multimodal dataset, and introduce
two novel tasks as a benchmark for multimodal figurative language
understanding. We experimented with state-of-the-art vision and language models
and found that the best (22%) performed substantially worse than humans (97%).
We release our dataset, benchmark, and code, in hopes of driving the
development of models that can better understand figurative language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yosef_R/0/1/0/all/0/1&quot;&gt;Ron Yosef&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahaf_D/0/1/0/all/0/1&quot;&gt;Dafna Shahaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00553">
<title>From Isolated Islands to Pangea: Unifying Semantic Space for Human Action Understanding. (arXiv:2304.00553v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00553</link>
<description rdf:parseType="Literal">&lt;p&gt;As a vital step toward the intelligent agent, Action understanding matters
for intelligent agents and has attracted long-term attention. It can be formed
as the mapping from the action physical space to the semantic space. Typically,
researchers built action datasets according to idiosyncratic choices to define
classes and push the envelope of benchmarks respectively. Thus, datasets are
incompatible with each other like &quot;Isolated Islands&quot; due to semantic gaps and
various class granularities, e.g., do housework in dataset A and wash plate in
dataset B. We argue that a more principled semantic space is an urgent need to
concentrate the community efforts and enable us to use all datasets together to
pursue generalizable action learning. To this end, we design a structured
action semantic space in view of verb taxonomy hierarchy and covering massive
actions. By aligning the classes of previous datasets to our semantic space, we
gather (image/video/skeleton/MoCap) datasets into a unified database in a
unified label system, i.e., bridging ``isolated islands&apos;&apos; into a &quot;Pangea&quot;.
Accordingly, we propose a novel model mapping from the physical space to
semantic space to fully use Pangea. In extensive experiments, our new system
shows significant superiority, especially in transfer learning. Code and data
will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinpeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zehao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Y/0/1/0/all/0/1&quot;&gt;Yiming Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yikun Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_J/0/1/0/all/0/1&quot;&gt;Jingru Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xudong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01664">
<title>An Embedding-based Approach to Inconsistency-tolerant Reasoning with Inconsistent Ontologies. (arXiv:2304.01664v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01664</link>
<description rdf:parseType="Literal">&lt;p&gt;Inconsistency handling is an important issue in knowledge management.
Especially in ontology engineering, logical inconsistencies may occur during
ontology construction. A natural way to reason with an inconsistent ontology is
to utilize the maximal consistent subsets of the ontology. However, previous
studies on selecting maximum consistent subsets have rarely considered the
semantics of the axioms, which may result in irrational inference. In this
paper, we propose a novel approach to reasoning with inconsistent ontologies in
description logics based on the embeddings of axioms. We first give a method
for turning axioms into distributed semantic vectors to compute the semantic
connections between the axioms. We then define an embedding-based method for
selecting the maximum consistent subsets and use it to define an
inconsistency-tolerant inference relation. We show the rationality of our
inference relation by considering some logical properties. Finally, we conduct
experiments on several ontologies to evaluate the reasoning power of our
inference relation. The experimental results show that our embedding-based
method can outperform existing inconsistency-tolerant reasoning methods based
on maximal consistent subsets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Keyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Site Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaye Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_G/0/1/0/all/0/1&quot;&gt;Guilin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Q/0/1/0/all/0/1&quot;&gt;Qiu Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.02858">
<title>A review of ensemble learning and data augmentation models for class imbalanced problems: combination, implementation and evaluation. (arXiv:2304.02858v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.02858</link>
<description rdf:parseType="Literal">&lt;p&gt;Class imbalance (CI) in classification problems arises when the number of
observations belonging to one class is lower than the other. Ensemble learning
combines multiple models to obtain a robust model and has been prominently used
with data augmentation methods to address class imbalance problems. In the last
decade, a number of strategies have been added to enhance ensemble learning and
data augmentation methods, along with new methods such as generative
adversarial networks (GANs). A combination of these has been applied in many
studies, and the evaluation of different combinations would enable a better
understanding and guidance for different application domains. In this paper, we
present a computational study to evaluate data augmentation and ensemble
learning methods used to address prominent benchmark CI problems. We present a
general framework that evaluates 9 data augmentation and 9 ensemble learning
methods for CI problems. Our objective is to identify the most effective
combination for improving classification performance on imbalanced datasets.
The results indicate that combinations of data augmentation methods with
ensemble learning can significantly improve classification performance on
imbalanced datasets. We find that traditional data augmentation methods such as
the synthetic minority oversampling technique (SMOTE) and random oversampling
(ROS) are not only better in performance for selected CI problems, but also
computationally less expensive than GANs. Our study is vital for the
development of novel models for handling imbalanced datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Azal Ahmad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_O/0/1/0/all/0/1&quot;&gt;Omkar Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1&quot;&gt;Rohitash Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07143">
<title>Car-Following Models: A Multidisciplinary Review. (arXiv:2304.07143v3 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07143</link>
<description rdf:parseType="Literal">&lt;p&gt;Car-following (CF) algorithms are crucial components of traffic simulations
and have been integrated into many production vehicles equipped with Advanced
Driving Assistance Systems (ADAS). Insights from the model of car-following
behavior help us understand the causes of various macro phenomena that arise
from interactions between pairs of vehicles. Car-following models encompass
multiple disciplines, including traffic engineering, physics, dynamic system
control, cognitive science, machine learning, and reinforcement learning. This
paper presents an extensive survey that highlights the differences,
complementarities, and overlaps among microscopic traffic flow and control
models based on their underlying principles and design logic. It reviews
representative algorithms, ranging from theory-based kinematic models,
Psycho-Physical Models, and Adaptive cruise control models to data-driven
algorithms like Reinforcement Learning and Imitation Learning (IL). The
manuscript discusses the strengths and limitations of these models and explores
their applications in different contexts. This review synthesizes existing
researches across different domains to fill knowledge gaps and offer guidance
for future research by identifying the latest trends in car following models
and their applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_P/0/1/0/all/0/1&quot;&gt;Peter J. Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bayen_A/0/1/0/all/0/1&quot;&gt;Alexandre Bayen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+D%2E_P/0/1/0/all/0/1&quot;&gt;Ph.D.&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piccoli_B/0/1/0/all/0/1&quot;&gt;Benedetto Piccoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09444">
<title>Rank-Based Learning and Local Model Based Evolutionary Algorithm for High-Dimensional Expensive Multi-Objective Problems. (arXiv:2304.09444v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09444</link>
<description rdf:parseType="Literal">&lt;p&gt;Surrogate-assisted evolutionary algorithms have been widely developed to
solve complex and computationally expensive multi-objective optimization
problems in recent years. However, when dealing with high-dimensional
optimization problems, the performance of these surrogate-assisted
multi-objective evolutionary algorithms deteriorate drastically. In this work,
a novel Classifier-assisted rank-based learning and Local Model based
multi-objective Evolutionary Algorithm (CLMEA) is proposed for high-dimensional
expensive multi-objective optimization problems. The proposed algorithm
consists of three parts: classifier-assisted rank-based learning,
hypervolume-based non-dominated search, and local search in the relatively
sparse objective space. Specifically, a probabilistic neural network is built
as classifier to divide the offspring into a number of ranks. The offspring in
different ranks uses rank-based learning strategy to generate more promising
and informative candidates for real function evaluations. Then, radial basis
function networks are built as surrogates to approximate the objective
functions. After searching non-dominated solutions assisted by the surrogate
model, the candidates with higher hypervolume improvement are selected for real
evaluations. Subsequently, in order to maintain the diversity of solutions, the
most uncertain sample point from the non-dominated solutions measured by the
crowding distance is selected as the guided parent to further infill in the
uncertain region of the front. The experimental results of benchmark problems
and a real-world application on geothermal reservoir heat extraction
optimization demonstrate that the proposed algorithm shows superior performance
compared with the state-of-the-art surrogate-assisted multi-objective
evolutionary algorithms. The source code for this work is available at
https://github.com/JellyChen7/CLMEA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guodong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiu Jimmy Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiaoming Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongzheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06026">
<title>Uncertainty in GNN Learning Evaluations: The Importance of a Consistent Benchmark for Community Detection. (arXiv:2305.06026v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06026</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have improved unsupervised community detection
of clustered nodes due to their ability to encode the dual dimensionality of
the connectivity and feature information spaces of graphs. Identifying the
latent communities has many practical applications from social networks to
genomics. Current benchmarks of real world performance are confusing due to the
variety of decisions influencing the evaluation of GNNs at this task. To
address this, we propose a framework to establish a common evaluation protocol.
We motivate and justify it by demonstrating the differences with and without
the protocol. The W Randomness Coefficient is a metric proposed for assessing
the consistency of algorithm rankings to quantify the reliability of results
under the presence of randomness. We find that by ensuring the same evaluation
criteria is followed, there may be significant differences from the reported
performance of methods at this task, but a more complete evaluation and
comparison of methods is possible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leeney_W/0/1/0/all/0/1&quot;&gt;William Leeney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McConville_R/0/1/0/all/0/1&quot;&gt;Ryan McConville&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09620">
<title>AI-Augmented Surveys: Leveraging Large Language Models and Surveys for Opinion Prediction. (arXiv:2305.09620v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09620</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) that produce human-like responses have begun to
revolutionize research practices in the social sciences. This paper shows how
we can integrate LLMs and social surveys to accurately predict individual
responses to survey questions that were not asked before. We develop a novel
methodological framework to personalize LLMs by considering the meaning of
survey questions derived from their text, the latent beliefs of individuals
inferred from their response patterns, and the temporal contexts across
different survey periods through fine-tuning LLMs with survey data. Using the
General Social Survey from 1972 to 2021, we show that the fine-tuned model
based on Alpaca-7b can predict individual responses to survey questions that
are partially missing as well as entirely missing. The remarkable prediction
capabilities allow us to fill in missing trends with high confidence and
pinpoint when public attitudes changed, such as the rising support for same-sex
marriage. We discuss practical constraints, socio-demographic representation,
and ethical concerns regarding individual autonomy and privacy when using LLMs
for opinion prediction. This study demonstrates that LLMs and surveys can
mutually enhance each other&apos;s capabilities: LLMs broaden survey potential,
while surveys improve the alignment of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junsol Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byungkyu Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11461">
<title>SelfzCoT: a Self-Prompt Zero-shot CoT from Semantic-level to Code-level for a Better Utilization of LLMs. (arXiv:2305.11461v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11461</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper show a work on better use of LLMs with SelfzCoT a self-prompt
zero-shot CoT. Specifically, on the zero-shot arithmetic reasoning tasks, the
accuracy of the proposed SelfzCoT is improved with GSM8K from 40.50% to 82.34%,
with MultiArith from 79.3% to 94.7%, with ADDSUB from 74.70% to 94.10%, with
SingleEq from 78.70% to 91.30%, with AQUA from 31.90% to 82.33%, and with SVAMP
from 63.70% to 79.70%. Totally, using the first two lasting path activations to
LLM and particularly, the code-level self-prompt, the SelfzCoT has a huge
improvement on all six zero-shot arithmetic reasoning tasks. Additionally, our
modified zero-shot CoT (MzCoT) also achieves remarkable performance in the
reasoning tasks. The accuracy of the proposed MzCoT is enhanced with GSM8K from
40.50% to 76.32%, with MultiArith from 79.3% to 96.97%, with ADDSUB from 74.70%
to 92.39%, with SingleEq from 78.70% to 94.60%, with AQUA from 31.90% to
79.90%, and with SVAMP from 63.70% to 81.50%. Notably, SelfzCoT has the best
performance on GSM8K among all the recent zero-shot methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_I/0/1/0/all/0/1&quot;&gt;Ioktong Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhidong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14930">
<title>In-Context Impersonation Reveals Large Language Models&apos; Strengths and Biases. (arXiv:2305.14930v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14930</link>
<description rdf:parseType="Literal">&lt;p&gt;In everyday conversations, humans can take on different roles and adapt their
vocabulary to their chosen roles. We explore whether LLMs can take on, that is
impersonate, different roles when they generate text in-context. We ask LLMs to
assume different personas before solving vision and language tasks. We do this
by prefixing the prompt with a persona that is associated either with a social
identity or domain expertise. In a multi-armed bandit task, we find that LLMs
pretending to be children of different ages recover human-like developmental
stages of exploration. In a language-based reasoning task, we find that LLMs
impersonating domain experts perform better than LLMs impersonating non-domain
experts. Finally, we test whether LLMs&apos; impersonations are complementary to
visual information when describing different categories. We find that
impersonation can improve performance: an LLM prompted to be a bird expert
describes birds better than one prompted to be a car expert. However,
impersonation can also uncover LLMs&apos; biases: an LLM prompted to be a man
describes cars better than one prompted to be a woman. These findings
demonstrate that LLMs are capable of taking on diverse roles and that this
in-context impersonation can be used to uncover their hidden strengths and
biases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salewski_L/0/1/0/all/0/1&quot;&gt;Leonard Salewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaniz_S/0/1/0/all/0/1&quot;&gt;Stephan Alaniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rio_Torto_I/0/1/0/all/0/1&quot;&gt;Isabel Rio-Torto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schulz_E/0/1/0/all/0/1&quot;&gt;Eric Schulz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1&quot;&gt;Zeynep Akata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19569">
<title>Domain knowledge-informed Synthetic fault sample generation with Health Data Map for cross-domain Planetary Gearbox Fault Diagnosis. (arXiv:2305.19569v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19569</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive research has been conducted on fault diagnosis of planetary
gearboxes using vibration signals and deep learning (DL) approaches. However,
DL-based methods are susceptible to the domain shift problem caused by varying
operating conditions of the gearbox. Although domain adaptation and data
synthesis methods have been proposed to overcome such domain shifts, they are
often not directly applicable in real-world situations where only healthy data
is available in the target domain. To tackle the challenge of extreme domain
shift scenarios where only healthy data is available in the target domain, this
paper proposes two novel domain knowledge-informed data synthesis methods
utilizing the health data map (HDMap). The two proposed approaches are referred
to as scaled CutPaste and FaultPaste. The HDMap is used to physically represent
the vibration signal of the planetary gearbox as an image-like matrix, allowing
for visualization of fault-related features. CutPaste and FaultPaste are then
applied to generate faulty samples based on the healthy data in the target
domain, using domain knowledge and fault signatures extracted from the source
domain, respectively. In addition to generating realistic faults, the proposed
methods introduce scaling of fault signatures for controlled synthesis of
faults with various severity levels. A case study is conducted on a planetary
gearbox testbed to evaluate the proposed approaches. The results show that the
proposed methods are capable of accurately diagnosing faults, even in cases of
extreme domain shift, and can estimate the severity of faults that have not
been previously observed in the target domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_J/0/1/0/all/0/1&quot;&gt;Jong Moon Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19599">
<title>RealignDiff: Boosting Text-to-Image Diffusion Model with Coarse-to-fine Semantic Re-alignment. (arXiv:2305.19599v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19599</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in text-to-image diffusion models have achieved remarkable
success in generating high-quality, realistic images from textual descriptions.
However, these approaches have faced challenges in precisely aligning the
generated visual content with the textual concepts described in the prompts. In
this paper, we propose a two-stage coarse-to-fine semantic re-alignment method,
named RealignDiff, aimed at improving the alignment between text and images in
text-to-image diffusion models. In the coarse semantic re-alignment phase, a
novel caption reward, leveraging the BLIP-2 model, is proposed to evaluate the
semantic discrepancy between the generated image caption and the given text
prompt. Subsequently, the fine semantic re-alignment stage employs a local
dense caption generation module and a re-weighting attention modulation module
to refine the previously generated images from a local semantic view.
Experimental results on the MS-COCO benchmark demonstrate that the proposed
two-stage coarse-to-fine semantic re-alignment method outperforms other
baseline re-alignment techniques by a substantial margin in both visual quality
and semantic similarity with the input prompt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1&quot;&gt;Guian Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zutao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jianhua Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guansong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_S/0/1/0/all/0/1&quot;&gt;Shengcai Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xiaodan Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00577">
<title>TorchRL: A data-driven decision-making library for PyTorch. (arXiv:2306.00577v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00577</link>
<description rdf:parseType="Literal">&lt;p&gt;PyTorch has ascended as a premier machine learning framework, yet it lacks a
native and comprehensive library for decision and control tasks suitable for
large development teams dealing with complex real-world data and environments.
To address this issue, we propose TorchRL, a generalistic control library for
PyTorch that provides well-integrated, yet standalone components. We introduce
a new and flexible PyTorch primitive, the TensorDict, which facilitates
streamlined algorithm development across the many branches of Reinforcement
Learning (RL) and control. We provide a detailed description of the building
blocks and an extensive overview of the library across domains and tasks.
Finally, we experimentally demonstrate its reliability and flexibility and show
comparative benchmarks to demonstrate its computational efficiency. TorchRL
fosters long-term support and is publicly available on GitHub for greater
reproducibility and collaboration within the research community. The code is
open-sourced on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bou_A/0/1/0/all/0/1&quot;&gt;Albert Bou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bettini_M/0/1/0/all/0/1&quot;&gt;Matteo Bettini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dittert_S/0/1/0/all/0/1&quot;&gt;Sebastian Dittert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vikash Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sodhani_S/0/1/0/all/0/1&quot;&gt;Shagun Sodhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaomeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabritiis_G/0/1/0/all/0/1&quot;&gt;Gianni De Fabritiis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moens_V/0/1/0/all/0/1&quot;&gt;Vincent Moens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03081">
<title>Sequential Monte Carlo Steering of Large Language Models using Probabilistic Programs. (arXiv:2306.03081v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03081</link>
<description rdf:parseType="Literal">&lt;p&gt;Even after fine-tuning and reinforcement learning, large language models
(LLMs) can be difficult, if not impossible, to control reliably with prompts
alone. We propose a new inference-time approach to enforcing syntactic and
semantic constraints on the outputs of LLMs, called sequential Monte Carlo
(SMC) steering. The key idea is to specify language generation tasks as
posterior inference problems in a class of discrete probabilistic sequence
models, and replace standard decoding with sequential Monte Carlo inference.
For a computational cost similar to that of beam search, SMC can steer LLMs to
solve diverse tasks, including infilling, generation under syntactic
constraints, and prompt intersection. To facilitate experimentation with SMC
steering, we present a probabilistic programming library, LLaMPPL
(https://github.com/probcomp/hfppl), for concisely specifying new generation
tasks as language model probabilistic programs, and automating steering of
LLaMA-family Transformers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lew_A/0/1/0/all/0/1&quot;&gt;Alexander K. Lew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_Xuan_T/0/1/0/all/0/1&quot;&gt;Tan Zhi-Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grand_G/0/1/0/all/0/1&quot;&gt;Gabriel Grand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansinghka_V/0/1/0/all/0/1&quot;&gt;Vikash K. Mansinghka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07294">
<title>Computational and Storage Efficient Quadratic Neurons for Deep Neural Networks. (arXiv:2306.07294v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07294</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) have been widely deployed across diverse domains
such as computer vision and natural language processing. However, the
impressive accomplishments of DNNs have been realized alongside extensive
computational demands, thereby impeding their applicability on
resource-constrained devices. To address this challenge, many researchers have
been focusing on basic neuron structures, the fundamental building blocks of
neural networks, to alleviate the computational and storage cost. In this work,
an efficient quadratic neuron architecture distinguished by its enhanced
utilization of second-order computational information is introduced. By virtue
of their better expressivity, DNNs employing the proposed quadratic neurons can
attain similar accuracy with fewer neurons and computational cost. Experimental
results have demonstrated that the proposed quadratic neuron structure exhibits
superior computational and storage efficiency across various tasks when
compared with both linear and non-linear neurons in prior work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chuangtao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Grace Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_X/0/1/0/all/0/1&quot;&gt;Xunzhao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_C/0/1/0/all/0/1&quot;&gt;Cheng Zhuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlichtmann_U/0/1/0/all/0/1&quot;&gt;Ulf Schlichtmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.11845">
<title>Multimodal Document Analytics for Banking Process Automation. (arXiv:2307.11845v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.11845</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional banks face increasing competition from FinTechs in the rapidly
evolving financial ecosystem. Raising operational efficiency is vital to
address this challenge. Our study aims to improve the efficiency of
document-intensive business processes in banking. To that end, we first review
the landscape of business documents in the retail segment. Banking documents
often contain text, layout, and visuals, suggesting that document analytics and
process automation require more than plain natural language processing (NLP).
To verify this and assess the incremental value of visual cues when processing
business documents, we compare a recently proposed multimodal model called
LayoutXLM to powerful text classifiers (e.g., BERT) and large language models
(e.g., GPT) in a case study related to processing company register extracts.
The results confirm that incorporating layout information in a model
substantially increases its performance. Interestingly, we also observed that
more than 75% of the best model performance (in terms of the F1 score) can be
achieved with as little as 30% of the training data. This shows that the demand
for data labeled data to set up a multi-modal model can be moderate, which
simplifies real-world applications of multimodal document analytics. Our study
also sheds light on more specific practices in the scope of calibrating a
multimodal banking document classifier, including the need for fine-tuning. In
sum, the paper contributes original empirical evidence on the effectiveness and
efficiency of multi-model models for document processing in the banking
business and offers practical guidance on how to unlock this potential in
day-to-day operations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerling_C/0/1/0/all/0/1&quot;&gt;Christopher Gerling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lessmann_S/0/1/0/all/0/1&quot;&gt;Stefan Lessmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14335">
<title>WavJourney: Compositional Audio Creation with Large Language Models. (arXiv:2307.14335v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14335</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite breakthroughs in audio generation models, their capabilities are
often confined to domain-specific conditions such as speech transcriptions and
audio captions. However, real-world audio creation aims to generate harmonious
audio containing various elements such as speech, music, and sound effects with
controllable conditions, which is challenging to address using existing audio
generation systems. We present WavJourney, a novel framework that leverages
Large Language Models (LLMs) to connect various audio models for audio
creation. WavJourney allows users to create storytelling audio content with
diverse audio elements simply from textual descriptions. Specifically, given a
text instruction, WavJourney first prompts LLMs to generate an audio script
that serves as a structured semantic representation of audio elements. The
audio script is then converted into a computer program, where each line of the
program calls a task-specific audio generation model or computational operation
function. The computer program is then executed to obtain a compositional and
interpretable solution for audio creation. Experimental results suggest that
WavJourney is capable of synthesizing realistic audio aligned with
textually-described semantic, spatial and temporal conditions, achieving
state-of-the-art results on text-to-audio generation benchmarks. Additionally,
we introduce a new multi-genre story benchmark. Subjective evaluations
demonstrate the potential of WavJourney in crafting engaging storytelling audio
content from text. We further demonstrate that WavJourney can facilitate
human-machine co-creation in multi-round dialogues. To foster future research,
the code and synthesized audio are available at:
https://audio-agi.github.io/WavJourney_demopage/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xubo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhongkai Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haohe Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yi Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_M/0/1/0/all/0/1&quot;&gt;Meng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiushi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jinhua Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Q/0/1/0/all/0/1&quot;&gt;Qiuqiang Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plumbley_M/0/1/0/all/0/1&quot;&gt;Mark D. Plumbley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenwu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.15176">
<title>RCT Rejection Sampling for Causal Estimation Evaluation. (arXiv:2307.15176v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.15176</link>
<description rdf:parseType="Literal">&lt;p&gt;Confounding is a significant obstacle to unbiased estimation of causal
effects from observational data. For settings with high-dimensional covariates
-- such as text data, genomics, or the behavioral social sciences --
researchers have proposed methods to adjust for confounding by adapting machine
learning methods to the goal of causal estimation. However, empirical
evaluation of these adjustment methods has been challenging and limited. In
this work, we build on a promising empirical evaluation strategy that
simplifies evaluation design and uses real data: subsampling randomized
controlled trials (RCTs) to create confounded observational datasets while
using the average causal effects from the RCTs as ground-truth. We contribute a
new sampling algorithm, which we call RCT rejection sampling, and provide
theoretical guarantees that causal identification holds in the observational
data to allow for valid comparisons to the ground-truth RCT. Using synthetic
data, we show our algorithm indeed results in low bias when oracle estimators
are evaluated on the confounded samples, which is not always the case for a
previously proposed algorithm. In addition to this identification result, we
highlight several finite data considerations for evaluation designers who plan
to use RCT rejection sampling on their own datasets. As a proof of concept, we
implement an example evaluation pipeline and walk through these finite data
considerations with a novel, real-world RCT -- which we release publicly --
consisting of approximately 70k observations and text data as high-dimensional
covariates. Together, these contributions build towards a broader agenda of
improved empirical evaluation for causal estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keith_K/0/1/0/all/0/1&quot;&gt;Katherine A. Keith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldman_S/0/1/0/all/0/1&quot;&gt;Sergey Feldman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1&quot;&gt;David Jurgens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bragg_J/0/1/0/all/0/1&quot;&gt;Jonathan Bragg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_R/0/1/0/all/0/1&quot;&gt;Rohit Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16387">
<title>Relation-Oriented: Toward Causal Knowledge-Aligned AGI. (arXiv:2307.16387v11 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16387</link>
<description rdf:parseType="Literal">&lt;p&gt;The current relationship modeling paradigm, grounded in the observational
i.i.d assumption, fundamentally misaligns with our causal knowledge
understanding due to two key oversights: 1) the unobservable relations, which
lead to undetectable hierarchical levels of knowledge, driving the need for
model generalizability; 2) the cognitive relative timings, which crucially
support our structural knowledge comprehension, resulting in inherent biases
within the present Observation-Oriented paradigm. Adopting a novel
Relation-Oriented perspective, this paper proposes a new framework to unify the
various confusions surrounding causality learning, ranging from traditional
causal inference to modern language models. Also, relation-indexed
representation learning (RIRL) is raised as a baseline implementation method of
the proposed new paradigm, alongside comprehensive experiments demonstrating
its efficacy in autonomously identifying dynamical effects in relationship
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.07037">
<title>Bayesian Flow Networks. (arXiv:2308.07037v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.07037</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Bayesian Flow Networks (BFNs), a new class of
generative model in which the parameters of a set of independent distributions
are modified with Bayesian inference in the light of noisy data samples, then
passed as input to a neural network that outputs a second, interdependent
distribution. Starting from a simple prior and iteratively updating the two
distributions yields a generative procedure similar to the reverse process of
diffusion models; however it is conceptually simpler in that no forward process
is required. Discrete and continuous-time loss functions are derived for
continuous, discretised and discrete data, along with sample generation
procedures. Notably, the network inputs for discrete data lie on the
probability simplex, and are therefore natively differentiable, paving the way
for gradient-based sample guidance and few-step generation in discrete domains
such as language modelling. The loss function directly optimises data
compression and places no restrictions on the network architecture. In our
experiments BFNs achieve competitive log-likelihoods for image modelling on
dynamically binarized MNIST and CIFAR-10, and outperform all known discrete
diffusion models on the text8 character-level language modelling task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graves_A/0/1/0/all/0/1&quot;&gt;Alex Graves&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_R/0/1/0/all/0/1&quot;&gt;Rupesh Kumar Srivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkinson_T/0/1/0/all/0/1&quot;&gt;Timothy Atkinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_F/0/1/0/all/0/1&quot;&gt;Faustino Gomez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10819">
<title>Evaluating the Instruction-Following Robustness of Large Language Models to Prompt Injection. (arXiv:2308.10819v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10819</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have demonstrated exceptional proficiency in
instruction-following, becoming increasingly crucial across various
applications. However, this capability brings with it the risk of prompt
injection attacks, where attackers inject instructions into LLMs&apos; input to
elicit undesirable actions or content. Understanding the robustness of LLMs
against such attacks is vital for their safe implementation. In this work, we
establish a benchmark to evaluate the robustness of instruction-following LLMs
against prompt injection attacks. Our objective is to determine the extent to
which LLMs can be influenced by injected instructions and their ability to
differentiate between these injected and original target instructions. Through
extensive experiments with leading instruction-following LLMs, we uncover
significant vulnerabilities in their robustness to such attacks. Our results
indicate that some models are overly tuned to follow any embedded instructions
in the prompt, overly focusing on the latter parts of the prompt without fully
grasping the entire context. By contrast, models with a better grasp of the
context and instruction-following capabilities will potentially be more
susceptible to compromise by injected instructions. This underscores the need
to shift the focus from merely enhancing LLMs&apos; instruction-following
capabilities to improving their overall comprehension of prompts and
discernment of instructions that are appropriate to follow. We hope our
in-depth analysis offers insights into the underlying causes of these
vulnerabilities, aiding in the development of future solutions. Code and data
are available at
https://github.com/Leezekun/instruction-following-robustness-eval
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zekun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Baolin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pengcheng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xifeng Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11136">
<title>Is There Any Social Principle for LLM-Based Agents?. (arXiv:2308.11136v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11136</link>
<description rdf:parseType="Literal">&lt;p&gt;Focus on Large Language Model based agents should involve more than
&quot;human-centered&quot; alignment or application. We argue that more attention should
be paid to the agent itself and discuss the potential of establishing tailored
social sciences for agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jitao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Simiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhonghao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11978">
<title>Will More Expressive Graph Neural Networks do Better on Generative Tasks?. (arXiv:2308.11978v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11978</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph generation poses a significant challenge as it involves predicting a
complete graph with multiple nodes and edges based on simply a given label.
This task also carries fundamental importance to numerous real-world
applications, including de-novo drug and molecular design. In recent years,
several successful methods have emerged in the field of graph generation.
However, these approaches suffer from two significant shortcomings: (1) the
underlying Graph Neural Network (GNN) architectures used in these methods are
often underexplored; and (2) these methods are often evaluated on only a
limited number of metrics. To fill this gap, we investigate the expressiveness
of GNNs under the context of the molecular graph generation task, by replacing
the underlying GNNs of graph generative models with more expressive GNNs.
Specifically, we analyse the performance of six GNNs on six different molecular
generative objectives on the ZINC-250k dataset in two different generative
frameworks: autoregressive generation models, such as GCPN and GraphAF, and
one-shot generation models, such as GraphEBM. Through our extensive
experiments, we demonstrate that advanced GNNs can indeed improve the
performance of GCPN, GraphAF, and GraphEBM on molecular generation tasks, but
GNN expressiveness is not a necessary condition for a good GNN-based generative
model. Moreover, we show that GCPN and GraphAF with advanced GNNs can achieve
state-of-the-art results across 17 other non-GNN-based graph generative
approaches, such as variational autoencoders and Bayesian optimisation models,
on the proposed molecular generative objectives (DRD2, Median1, Median2), which
are important metrics for de-novo molecular design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xiandong Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yiren Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.12532">
<title>FedSoL: Bridging Global Alignment and Local Generality in Federated Learning. (arXiv:2308.12532v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.12532</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) aggregates locally trained models from individual
clients to construct a global model. While FL enables learning a model with
data privacy, it often suffers from significant performance degradation when
client data distributions are heterogeneous. Many previous FL algorithms have
addressed this issue by introducing various proximal restrictions. These
restrictions aim to encourage global alignment by constraining the deviation of
local learning from the global objective. However, they inherently limit local
learning by interfering with the original local objectives. Recently, an
alternative approach has emerged to improve local learning generality. By
obtaining local models within a smooth loss landscape, this approach mitigates
conflicts among different local objectives of the clients. Yet, it does not
ensure stable global alignment, as local learning does not take the global
objective into account. In this study, we propose Federated Stability on
Learning (FedSoL), which combines both the concepts of global alignment and
local generality. In FedSoL, the local learning seeks a parameter region robust
against proximal perturbations. This strategy introduces an implicit proximal
restriction effect in local learning while maintaining the original local
objective for parameter update. Our experiments show that FedSoL consistently
achieves state-of-the-art performance on various setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gihun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_M/0/1/0/all/0/1&quot;&gt;Minchan Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangmook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jaehoon Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Se-Young Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13234">
<title>Decoding Natural Images from EEG for Object Recognition. (arXiv:2308.13234v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13234</link>
<description rdf:parseType="Literal">&lt;p&gt;Electroencephalography (EEG) signals, known for convenient non-invasive
acquisition but low signal-to-noise ratio, have recently gained substantial
attention due to the potential to decode natural images. This paper presents a
self-supervised framework to demonstrate the feasibility of learning image
representations from EEG signals, particularly for object recognition. The
framework utilizes image and EEG encoders to extract features from paired image
stimuli and EEG responses. Contrastive learning aligns these two modalities by
constraining their similarity. With the framework, we attain significantly
above-chance results on a comprehensive EEG-image dataset, achieving a top-1
accuracy of 15.6% and a top-5 accuracy of 42.8% in challenging 200-way
zero-shot tasks. Moreover, we perform extensive experiments to explore the
biological plausibility by resolving the temporal, spatial, spectral, and
semantic aspects of EEG signals. Besides, we introduce attention modules to
capture spatial correlations, providing implicit evidence of the brain activity
perceived from EEG data. These findings yield valuable insights for neural
decoding and brain-computer interfaces in real-world scenarios. The code will
be released on https://github.com/eeyhsong/NICE-EEG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yonghao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bingchuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_N/0/1/0/all/0/1&quot;&gt;Nanlin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yijun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiaorong Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14306">
<title>Evaluating the Robustness to Instructions of Large Language Models. (arXiv:2308.14306v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14306</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Instruction fine-tuning has risen to prominence as a potential
method for enhancing the zero-shot capabilities of Large Language Models (LLMs)
on novel tasks. This technique has shown an exceptional ability to boost the
performance of moderately sized LLMs, sometimes even reaching performance
levels comparable to those of much larger model variants. The focus is on the
robustness of instruction-tuned LLMs to seen and unseen tasks. We conducted an
exploration of six models including Alpaca, Vicuna, WizardLM, and Traditional
Task-oriented Models(Flan-T5-XL/XXL, T0++) using real-world relation extraction
datasets as case studies. We carried out a comprehensive evaluation of these
instruction-following LLMs which have been tuned based on open-domain
instructions and task-oriented instructions. The main discussion is their
performance and robustness towards instructions. We have observed that in most
cases, the model&apos;s performance in dealing with unfamiliar instructions tends to
worsen significantly, and the robustness of the model for RE instructions
deteriorates compared to QA. Further, we discovered that up until a certain
parameter size threshold (3B), the performance of the FLAN-T5 model improves as
the parameter count increases. The robustness of different scales of FLAN-T5
models to RE instruction is worse than the robustness to QA instruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Sichao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+wu_X/0/1/0/all/0/1&quot;&gt;Xinyu wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Hui Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuli Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16781">
<title>StratMed: Relevance Stratification between Biomedical Entities for Sparsity on Medication Recommendation. (arXiv:2308.16781v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16781</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growing imbalance between limited medical resources and escalating
demands, AI-based clinical tasks have become paramount. As a sub-domain,
medication recommendation aims to amalgamate longitudinal patient history with
medical knowledge, assisting physicians in prescribing safer and more accurate
medication combinations. Existing works ignore the inherent long-tailed
distribution of medical data, have uneven learning strengths for hot and sparse
data, and fail to balance safety and accuracy. To address the above
limitations, we propose StratMed, which introduces a stratification strategy
that overcomes the long-tailed problem and achieves fuller learning of sparse
data. It also utilizes a dual-property network to address the issue of mutual
constraints on the safety and accuracy of medication combinations,
synergistically enhancing these two properties. Specifically, we construct a
pre-training method using deep learning networks to obtain medication and
disease representations. After that, we design a pyramid-like stratification
method based on relevance to strengthen the expressiveness of sparse data.
Based on this relevance, we design two graph structures to express medication
safety and precision at the same level to obtain patient representations.
Finally, the patient&apos;s historical clinical information is fitted to generate
medication combinations for the current health condition. We employed the
MIMIC-III dataset to evaluate our model against state-of-the-art methods in
three aspects comprehensively. Compared to the sub-optimal baseline model, our
model reduces safety risk by 15.08\%, improves accuracy by 0.36\%, and reduces
training time consumption by 81.66\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shunpan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yulei Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_T/0/1/0/all/0/1&quot;&gt;Tengfei Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01897">
<title>Inferring Actual Treatment Pathways from Patient Records. (arXiv:2309.01897v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01897</link>
<description rdf:parseType="Literal">&lt;p&gt;Treatment pathways are step-by-step plans outlining the recommended medical
care for specific diseases; they get revised when different treatments are
found to improve patient outcomes. Examining health records is an important
part of this revision process, but inferring patients&apos; actual treatments from
health data is challenging due to complex event-coding schemes and the absence
of pathway-related annotations. This study aims to infer the actual treatment
steps for a particular patient group from administrative health records (AHR) -
a common form of tabular healthcare data - and address several technique- and
methodology-based gaps in treatment pathway-inference research. We introduce
Defrag, a method for examining AHRs to infer the real-world treatment steps for
a particular patient group. Defrag learns the semantic and temporal meaning of
healthcare event sequences, allowing it to reliably infer treatment steps from
complex healthcare data. To our knowledge, Defrag is the first
pathway-inference method to utilise a neural network (NN), an approach made
possible by a novel, self-supervised learning objective. We also developed a
testing and validation framework for pathway inference, which we use to
characterise and evaluate Defrag&apos;s pathway inference ability and compare
against baselines. We demonstrate Defrag&apos;s effectiveness by identifying
best-practice pathway fragments for breast cancer, lung cancer, and melanoma in
public healthcare records. Additionally, we use synthetic data experiments to
demonstrate the characteristics of the Defrag method, and to compare Defrag to
several baselines where it significantly outperforms non-NN-based methods.
Defrag significantly outperforms several existing pathway-inference methods and
offers an innovative and effective approach for inferring treatment pathways
from AHRs. Open-source code is provided to encourage further research in this
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilkins_Caruana_A/0/1/0/all/0/1&quot;&gt;Adrian Wilkins-Caruana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandara_M/0/1/0/all/0/1&quot;&gt;Madhushi Bandara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musial_K/0/1/0/all/0/1&quot;&gt;Katarzyna Musial&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catchpoole_D/0/1/0/all/0/1&quot;&gt;Daniel Catchpoole&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kennedy_P/0/1/0/all/0/1&quot;&gt;Paul J. Kennedy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14681">
<title>Are Human-generated Demonstrations Necessary for In-context Learning?. (arXiv:2309.14681v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14681</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the promising few-shot ability of large language models (LLMs), the
standard paradigm of In-context Learning (ICL) suffers the disadvantages of
susceptibility to selected demonstrations and the intricacy to generate these
demonstrations. In this paper, we raise the fundamental question that whether
human-generated demonstrations are necessary for ICL. To answer this question,
we propose self-contemplation prompting strategy (SEC), a paradigm free from
human-crafted demonstrations. The key point of SEC is that, instead of using
hand-crafted examples as demonstrations in ICL, SEC asks LLMs to first create
demonstrations on their own, based on which the final output is generated. SEC
is a flexible framework and can be adapted to both the vanilla ICL and the
chain-of-thought (CoT), but with greater ease: as the manual-generation process
of both examples and rationale can be saved. Extensive experiments in
arithmetic reasoning, commonsense reasoning, multi-task language understanding,
and code generation benchmarks, show that SEC, which does not require
hand-crafted demonstrations, significantly outperforms the zero-shot learning
strategy, and achieves comparable results to ICL with hand-crafted
demonstrations. This demonstrates that, for many tasks, contemporary LLMs
possess a sufficient level of competence to exclusively depend on their own
capacity for decision making, removing the need for external training data.
Code is available at https://github.com/ruili33/SEC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_G/0/1/0/all/0/1&quot;&gt;Guoyin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiwei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17255">
<title>Knowledge Graphs for the Life Sciences: Recent Developments, Challenges and Opportunities. (arXiv:2309.17255v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17255</link>
<description rdf:parseType="Literal">&lt;p&gt;The term life sciences refers to the disciplines that study living organisms
and life processes, and include chemistry, biology, medicine, and a range of
other related disciplines. Research efforts in life sciences are heavily
data-driven, as they produce and consume vast amounts of scientific data, much
of which is intrinsically relational and graph-structured.
&lt;/p&gt;
&lt;p&gt;The volume of data and the complexity of scientific concepts and relations
referred to therein promote the application of advanced knowledge-driven
technologies for managing and interpreting data, with the ultimate aim to
advance scientific discovery.
&lt;/p&gt;
&lt;p&gt;In this survey and position paper, we discuss recent developments and
advances in the use of graph-based technologies in life sciences and set out a
vision for how these technologies will impact these fields into the future. We
focus on three broad topics: the construction and management of Knowledge
Graphs (KGs), the use of KGs and associated technologies in the discovery of
new knowledge, and the use of KGs in artificial intelligence applications to
support explanations (explainable AI). We select a few exemplary use cases for
each topic, discuss the challenges and open research questions within these
topics, and conclude with a perspective and outlook that summarizes the
overarching challenges and their potential solutions as a guide for future
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiaoyan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hang Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hastings_J/0/1/0/all/0/1&quot;&gt;Janna Hastings&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jimenez_Ruiz_E/0/1/0/all/0/1&quot;&gt;Ernesto Jim&amp;#xe9;nez-Ruiz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_V/0/1/0/all/0/1&quot;&gt;Vanessa L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1&quot;&gt;Pierre Monnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pesquita_C/0/1/0/all/0/1&quot;&gt;Catia Pesquita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skoda_P/0/1/0/all/0/1&quot;&gt;Petr &amp;#x160;koda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamma_V/0/1/0/all/0/1&quot;&gt;Valentina Tamma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17338">
<title>Improving Trajectory Prediction in Dynamic Multi-Agent Environment by Dropping Waypoints. (arXiv:2309.17338v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17338</link>
<description rdf:parseType="Literal">&lt;p&gt;The inherently diverse and uncertain nature of trajectories presents a
formidable challenge in accurately modeling them. Motion prediction systems
must effectively learn spatial and temporal information from the past to
forecast the future trajectories of the agent. Many existing methods learn
temporal motion via separate components within stacked models to capture
temporal features. Furthermore, prediction methods often operate under the
assumption that observed trajectory waypoint sequences are complete,
disregarding scenarios where missing values may occur, which can influence
their performance. Moreover, these models may be biased toward particular
waypoint sequences when making predictions. We propose a novel approach called
Temporal Waypoint Dropping (TWD) that explicitly incorporates temporal
dependencies during the training of a trajectory prediction model. By
stochastically dropping waypoints from past observed trajectories, the model is
forced to learn the underlying temporal representation from the remaining
waypoints, resulting in an improved model. Incorporating stochastic temporal
waypoint dropping into the model learning process significantly enhances its
performance in scenarios with missing values. Experimental results demonstrate
our approach&apos;s substantial improvement in trajectory prediction capabilities.
Our approach can complement existing trajectory prediction methods to improve
their prediction accuracy. We evaluate our proposed approach on three datasets:
NBA Sports VU, ETH-UCY, and TrajNet++.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chib_P/0/1/0/all/0/1&quot;&gt;Pranav Singh Chib&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Pravendra Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.17401">
<title>Adversarial Machine Learning in Latent Representations of Neural Networks. (arXiv:2309.17401v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.17401</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed deep neural networks (DNNs) have been shown to reduce the
computational burden of mobile devices and decrease the end-to-end inference
latency in edge computing scenarios. While distributed DNNs have been studied,
to the best of our knowledge the resilience of distributed DNNs to adversarial
action still remains an open problem. In this paper, we fill the existing
research gap by rigorously analyzing the robustness of distributed DNNs against
adversarial action. We cast this problem in the context of information theory
and introduce two new measurements for distortion and robustness. Our
theoretical findings indicate that (i) assuming the same level of information
distortion, latent features are always more robust than input representations;
(ii) the adversarial robustness is jointly determined by the feature dimension
and the generalization capability of the DNN. To test our theoretical findings,
we perform extensive experimental analysis by considering 6 different DNN
architectures, 6 different approaches for distributed DNN and 10 different
adversarial attacks to the ImageNet-1K dataset. Our experimental results
support our theoretical findings by showing that the compressed latent
representations can reduce the success rate of adversarial attacks by 88% in
the best case and by 57% on the average compared to attacks to the input space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Milin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdi_M/0/1/0/all/0/1&quot;&gt;Mohammad Abdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Restuccia_F/0/1/0/all/0/1&quot;&gt;Francesco Restuccia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01825">
<title>Empirical Study of PEFT techniques for Winter Wheat Segmentation. (arXiv:2310.01825v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01825</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter Efficient Fine Tuning (PEFT) techniques have recently experienced
significant growth and have been extensively employed to adapt large vision and
language models to various domains, enabling satisfactory model performance
with minimal computational needs. Despite these advances, more research has yet
to delve into potential PEFT applications in real-life scenarios, particularly
in the critical domains of remote sensing and crop monitoring. The diversity of
climates across different regions and the need for comprehensive large-scale
datasets have posed significant obstacles to accurately identify crop types
across varying geographic locations and changing growing seasons. This study
seeks to bridge this gap by comprehensively exploring the feasibility of
cross-area and cross-year out-of-distribution generalization using the
State-of-the-Art (SOTA) wheat crop monitoring model. The aim of this work is to
explore PEFT approaches for crop monitoring. Specifically, we focus on adapting
the SOTA TSViT model to address winter wheat field segmentation, a critical
task for crop monitoring and food security. This adaptation process involves
integrating different PEFT techniques, including BigFit, LoRA, Adaptformer, and
prompt tuning. Using PEFT techniques, we achieved notable results comparable to
those achieved using full fine-tuning methods while training only a mere 0.7%
parameters of the whole TSViT architecture. The in-house labeled data-set,
referred to as the Beqaa-Lebanon dataset, comprises high-quality annotated
polygons for wheat and non-wheat classes with a total surface of 170 kmsq, over
five consecutive years. Using Sentinel-2 images, our model achieved a 84%
F1-score. We intend to publicly release the Lebanese winter wheat data set,
code repository, and model weights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahweh_M/0/1/0/all/0/1&quot;&gt;Mohamad Hasan Zahweh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrallah_H/0/1/0/all/0/1&quot;&gt;Hasan Nasrallah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shukor_M/0/1/0/all/0/1&quot;&gt;Mustafa Shukor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faour_G/0/1/0/all/0/1&quot;&gt;Ghaleb Faour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1&quot;&gt;Ali J. Ghandour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01828">
<title>Trainable Noise Model as an XAI evaluation method: application on Sobol for remote sensing image segmentation. (arXiv:2310.01828v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01828</link>
<description rdf:parseType="Literal">&lt;p&gt;eXplainable Artificial Intelligence (XAI) has emerged as an essential
requirement when dealing with mission-critical applications, ensuring
transparency and interpretability of the employed black box AI models. The
significance of XAI spans various domains, from healthcare to finance, where
understanding the decision-making process of deep learning algorithms is
essential. Most AI-based computer vision models are often black boxes; hence,
providing explainability of deep neural networks in image processing is crucial
for their wide adoption and deployment in medical image analysis, autonomous
driving, and remote sensing applications. Recently, several XAI methods for
image classification tasks have been introduced. On the contrary, image
segmentation has received comparatively less attention in the context of
explainability, although it is a fundamental task in computer vision
applications, especially in remote sensing. Only some research proposes
gradient-based XAI algorithms for image segmentation. This paper adapts the
recent gradient-free Sobol XAI method for semantic segmentation. To measure the
performance of the Sobol method for segmentation, we propose a quantitative XAI
evaluation method based on a learnable noise model. The main objective of this
model is to induce noise on the explanation maps, where higher induced noise
signifies low accuracy and vice versa. A benchmark analysis is conducted to
evaluate and compare performance of three XAI methods, including Seg-Grad-CAM,
Seg-Grad-CAM++ and Seg-Sobol using the proposed noise-based evaluation
technique. This constitutes the first attempt to run and evaluate XAI methods
using high-resolution satellite images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shreim_H/0/1/0/all/0/1&quot;&gt;Hossein Shreim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gizzini_A/0/1/0/all/0/1&quot;&gt;Abdul Karim Gizzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1&quot;&gt;Ali J. Ghandour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01852">
<title>LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment. (arXiv:2310.01852v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01852</link>
<description rdf:parseType="Literal">&lt;p&gt;The video-language (VL) pretraining has achieved remarkable improvement in
multiple downstream tasks. However, the current VL pretraining framework is
hard to extend to multiple modalities (N modalities, N&amp;gt;=3) beyond vision and
language. We thus propose LanguageBind, taking the language as the bind across
different modalities because the language modality is well-explored and
contains rich semantics. Specifically, we freeze the language encoder acquired
by VL pretraining, then train encoders for other modalities with contrastive
learning. As a result, all modalities are mapped to a shared feature space,
implementing multi-modal semantic alignment. While LanguageBind ensures that we
can extend VL modalities to N modalities, we also need a high-quality dataset
with alignment data pairs centered on language. We thus propose VIDAL-10M with
Video, Infrared, Depth, Audio and their corresponding Language, naming as
VIDAL-10M. In our VIDAL-10M, all videos are from short video platforms with
complete semantics rather than truncated segments from long videos, and all the
video, depth, infrared, and audio modalities are aligned to their textual
descriptions. After pretraining on VIDAL-10M, we outperform ImageBind by 5.8%
R@1 on the MSR-VTT dataset with only 15% of the parameters in the zero-shot
video-text retrieval task. Beyond this, our LanguageBind has greatly improved
in the zero-shot video, audio, depth, and infrared understanding tasks. For
instance, LanguageBind surpassing InterVideo by 1.9% on MSR-VTT, 8.8% on MSVD,
6.3% on DiDeMo, and 4.4% on ActivityNet. On the LLVIP and NYU-D datasets,
LanguageBind outperforms ImageBind with 23.8% and 11.1% top-1 accuracy. Code
address: https://github.com/PKU-YuanGroup/LanguageBind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_M/0/1/0/all/0/1&quot;&gt;Munan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yang Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiaxi Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;HongFa Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_Y/0/1/0/all/0/1&quot;&gt;Yatian Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenhao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junwu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wancai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_L/0/1/0/all/0/1&quot;&gt;Li Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04914">
<title>Analyzing Zero-Shot Abilities of Vision-Language Models on Video Understanding Tasks. (arXiv:2310.04914v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04914</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundational multimodal models pre-trained on large scale image-text pairs or
video-text pairs or both have shown strong generalization abilities on
downstream tasks. However unlike image-text models, pretraining video-text
models is always not feasible due to the difficulty in collecting large-scale
clean and aligned data, and exponential computational costs involved in the
pretraining phase. Therefore, the pertinent question to ask is: Can image-text
models be adapted to video tasks and is there any benefit to using these models
over pretraining directly on videos? In this work, we focus on this question by
proposing a detailed study on the generalization abilities of image-text models
when evaluated on video understanding tasks in a zero-shot setting. We
investigate 9 foundational image-text models on a diverse set of video tasks
that include video action recognition (video AR), video retrieval (video RT),
video question answering (video QA), video multiple choice (video MC) and video
captioning (video CP). Our experiments show that image-text models exhibit
impressive performance on video AR, video RT and video MC. Furthermore, they
perform moderately on video captioning and poorly on video QA. These findings
shed a light on the benefits of adapting foundational image-text models to an
array of video tasks while avoiding the costly pretraining step.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madasu_A/0/1/0/all/0/1&quot;&gt;Avinash Madasu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhiwandiwalla_A/0/1/0/all/0/1&quot;&gt;Anahita Bhiwandiwalla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lal_V/0/1/0/all/0/1&quot;&gt;Vasudev Lal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06422">
<title>Large Language Models for Propaganda Detection. (arXiv:2310.06422v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06422</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence of propaganda in our digital society poses a challenge to
societal harmony and the dissemination of truth. Detecting propaganda through
NLP in text is challenging due to subtle manipulation techniques and contextual
dependencies. To address this issue, we investigate the effectiveness of modern
Large Language Models (LLMs) such as GPT-3 and GPT-4 for propaganda detection.
We conduct experiments using the SemEval-2020 task 11 dataset, which features
news articles labeled with 14 propaganda techniques as a multi-label
classification problem. Five variations of GPT-3 and GPT-4 are employed,
incorporating various prompt engineering and fine-tuning strategies across the
different models. We evaluate the models&apos; performance by assessing metrics such
as $F1$ score, $Precision$, and $Recall$, comparing the results with the
current state-of-the-art approach using RoBERTa. Our findings demonstrate that
GPT-4 achieves comparable results to the current state-of-the-art. Further,
this study analyzes the potential and challenges of LLMs in complex tasks like
propaganda detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sprenkamp_K/0/1/0/all/0/1&quot;&gt;Kilian Sprenkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jones_D/0/1/0/all/0/1&quot;&gt;Daniel Gordon Jones&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zavolokina_L/0/1/0/all/0/1&quot;&gt;Liudmila Zavolokina&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06756">
<title>Going Beyond Neural Network Feature Similarity: The Network Feature Complexity and Its Interpretation Using Category Theory. (arXiv:2310.06756v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06756</link>
<description rdf:parseType="Literal">&lt;p&gt;The behavior of neural networks still remains opaque, and a recently widely
noted phenomenon is that networks often achieve similar performance when
initialized with different random parameters. This phenomenon has attracted
significant attention in measuring the similarity between features learned by
distinct networks. However, feature similarity could be vague in describing the
same feature since equivalent features hardly exist. In this paper, we expand
the concept of equivalent feature and provide the definition of what we call
functionally equivalent features. These features produce equivalent output
under certain transformations. Using this definition, we aim to derive a more
intrinsic metric for the so-called feature complexity regarding the redundancy
of features learned by a neural network at each layer. We offer a formal
interpretation of our approach through the lens of category theory, a
well-developed area in mathematics. To quantify the feature complexity, we
further propose an efficient algorithm named Iterative Feature Merging. Our
experimental results validate our ideas and theories from various perspectives.
We empirically demonstrate that the functionally equivalence widely exists
among different features learned by the same neural network and we could reduce
the number of parameters of the network without affecting the performance.The
IFM shows great potential as a data-agnostic model prune method. We have also
drawn several interesting empirical findings regarding the defined feature
complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhanpeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07587">
<title>Fed-GraB: Federated Long-tailed Learning with Self-Adjusting Gradient Balancer. (arXiv:2310.07587v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07587</link>
<description rdf:parseType="Literal">&lt;p&gt;Data privacy and long-tailed distribution are the norms rather than the
exception in many real-world tasks. This paper investigates a federated
long-tailed learning (Fed-LT) task in which each client holds a locally
heterogeneous dataset; if the datasets can be globally aggregated, they jointly
exhibit a long-tailed distribution. Under such a setting, existing federated
optimization and/or centralized long-tailed learning methods hardly apply due
to challenges in (a) characterizing the global long-tailed distribution under
privacy constraints and (b) adjusting the local learning strategy to cope with
the head-tail imbalance. In response, we propose a method termed
$\texttt{Fed-GraB}$, comprised of a Self-adjusting Gradient Balancer (SGB)
module that re-weights clients&apos; gradients in a closed-loop manner, based on the
feedback of global long-tailed distribution evaluated by a Direct Prior
Analyzer (DPA) module. Using $\texttt{Fed-GraB}$, clients can effectively
alleviate the distribution drift caused by data heterogeneity during the model
training process and obtain a global model with better performance on the
minority classes while maintaining the performance of the majority classes.
Extensive experiments demonstrate that $\texttt{Fed-GraB}$ achieves
state-of-the-art performance on representative datasets such as CIFAR-10-LT,
CIFAR-100-LT, ImageNet-LT, and iNaturalist.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zikai Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zihan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Songshang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hualiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jin Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Howard Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zuozhu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07665">
<title>Deep Backtracking Counterfactuals for Causally Compliant Explanations. (arXiv:2310.07665v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07665</link>
<description rdf:parseType="Literal">&lt;p&gt;Counterfactuals can offer valuable insights by answering what would have been
observed under altered circumstances, conditional on a factual observation.
Whereas the classical interventional interpretation of counterfactuals has been
studied extensively, backtracking constitutes a less studied alternative the
backtracking principle has emerged as an alternative philosophy where all
causal laws are kept intact. In the present work, we introduce a practical
method for computing backtracking counterfactuals in structural causal models
that consist of deep generative components. To this end, we impose conditions
on the structural assignments that enable the generation of counterfactuals by
solving a tractable constrained optimization problem in the structured latent
space of a causal model. Our formulation also facilitates a comparison with
methods in the field of counterfactual explanations. Compared to these, our
method represents a versatile, modular and causally compliant alternative. We
demonstrate these properties experimentally on a modified version of MNIST and
CelebA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kladny_K/0/1/0/all/0/1&quot;&gt;Klaus-Rudolf Kladny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1&quot;&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muehlebach_M/0/1/0/all/0/1&quot;&gt;Michael Muehlebach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.07997">
<title>PG-NeuS: Robust and Efficient Point Guidance for Multi-View Neural Surface Reconstruction. (arXiv:2310.07997v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.07997</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, learning multi-view neural surface reconstruction with the
supervision of point clouds or depth maps has been a promising way. However,
due to the underutilization of prior information, current methods still
struggle with the challenges of limited accuracy and excessive time complexity.
In addition, prior data perturbation is also an important but rarely considered
issue. To address these challenges, we propose a novel point-guided method
named PG-NeuS, which achieves accurate and efficient reconstruction while
robustly coping with point noise. Specifically, aleatoric uncertainty of the
point cloud is modeled to capture the distribution of noise, leading to noise
robustness. Furthermore, a Neural Projection module connecting points and
images is proposed to add geometric constraints to implicit surface, achieving
precise point guidance. To better compensate for geometric bias between volume
rendering and point modeling, high-fidelity points are filtered into a Bias
Network to further improve details representation. Benefiting from the
effective point guidance, even with a lightweight network, the proposed PG-NeuS
achieves fast convergence with an impressive 11x speedup compared to NeuS.
Extensive experiments show that our method yields high-quality surfaces with
high efficiency, especially for fine-grained details and smooth regions,
outperforming the state-of-the-art methods. Moreover, it exhibits strong
robustness to noisy data and sparse data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Wanjuan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingshan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_W/0/1/0/all/0/1&quot;&gt;Wenbing Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08540">
<title>Do pretrained Transformers Really Learn In-context by Gradient Descent?. (arXiv:2310.08540v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08540</link>
<description rdf:parseType="Literal">&lt;p&gt;Is In-Context Learning (ICL) implicitly equivalent to Gradient Descent (GD)?
Several recent works draw analogies between the dynamics of GD and the emergent
behavior of ICL in large language models. However, these works make assumptions
far from the realistic natural language setting in which language models are
trained. Therefore, such discrepancies between theory and practice necessitate
further investigation to validate their applicability.
&lt;/p&gt;
&lt;p&gt;We start by highlighting the assumptions in prior works that construct
Transformer weights to simulate gradient descent. Their experiments with
training Transformers on ICL objective, inconsistencies in the order
sensitivity of ICL and GD, sparsity of the constructed weights, and sensitivity
to parameter changes are some examples of mismatch from the real-world setting.
&lt;/p&gt;
&lt;p&gt;Furthermore, we probe and compare the ICL vs. GD hypothesis in a natural
setting. We conduct comprehensive empirical analyses on language models
pretrained on natural data (LLaMa-7B). Our comparisons on various performance
metrics highlight the inconsistent behavior of ICL and GD as a function of
various factors such as datasets, models, and the number of demonstrations. We
observe that ICL and GD modify the output distribution of language models
differently. These results indicate that the equivalence between ICL and GD is
an open hypothesis, requires nuanced considerations, and calls for further
studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lingfeng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Aayush Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1&quot;&gt;Daniel Khashabi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09926">
<title>Estimating Uncertainty in Multimodal Foundation Models using Public Internet Data. (arXiv:2310.09926v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09926</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models are trained on vast amounts of data at scale using
self-supervised learning, enabling adaptation to a wide range of downstream
tasks. At test time, these models exhibit zero-shot capabilities through which
they can classify previously unseen (user-specified) categories. In this paper,
we address the problem of quantifying uncertainty in these zero-shot
predictions. We propose a heuristic approach for uncertainty estimation in
zero-shot settings using conformal prediction with web data. Given a set of
classes at test time, we conduct zero-shot classification with CLIP-style
models using a prompt template, e.g., &quot;an image of a &amp;lt;category&amp;gt;&quot;, and use the
same template as a search query to source calibration data from the open web.
Given a web-based calibration set, we apply conformal prediction with a novel
conformity score that accounts for potential errors in retrieved web data. We
evaluate the utility of our proposed method in Biomedical foundation models;
our preliminary results show that web-based conformal prediction sets achieve
the target coverage with satisfactory efficiency on a variety of biomedical
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dutta_S/0/1/0/all/0/1&quot;&gt;Shiladitya Dutta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hongbo Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laan_L/0/1/0/all/0/1&quot;&gt;Lars van der Laan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaa_A/0/1/0/all/0/1&quot;&gt;Ahmed M. Alaa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10520">
<title>Semantic Parsing by Large Language Models for Intricate Updating Strategies of Zero-Shot Dialogue State Tracking. (arXiv:2310.10520v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10520</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot Dialogue State Tracking (DST) addresses the challenge of acquiring
and annotating task-oriented dialogues, which can be time-consuming and costly.
However, DST extends beyond simple slot-filling and requires effective updating
strategies for tracking dialogue state as conversations progress. In this
paper, we propose ParsingDST, a new In-Context Learning (ICL) method, to
introduce additional intricate updating strategies in zero-shot DST. Our
approach reformulates the DST task by leveraging powerful Large Language Models
(LLMs) and translating the original dialogue text to JSON through semantic
parsing as an intermediate state. We also design a novel framework that
includes more modules to ensure the effectiveness of updating strategies in the
text-to-JSON process. Experimental results demonstrate that our approach
outperforms existing zero-shot DST methods on MultiWOZ, exhibiting significant
improvements in Joint Goal Accuracy (JGA) and slot accuracy compared to
existing ICL methods. Our code has been released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yuxiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_G/0/1/0/all/0/1&quot;&gt;Guanting Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weiran Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10541">
<title>AST: Effective Dataset Distillation through Alignment with Smooth and High-Quality Expert Trajectories. (arXiv:2310.10541v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10541</link>
<description rdf:parseType="Literal">&lt;p&gt;Training large AI models typically requires large-scale datasets in the
machine learning process, making training and parameter-tuning process both
time-consuming and costly. Some researchers address this problem by carefully
synthesizing a very small number of highly representative and informative
samples from real-world datasets. This approach, known as Dataset Distillation
(DD), proposes a perspective for data-efficient learning. Despite recent
progress in this field, the performance of existing methods still cannot meet
expectations, and distilled datasets cannot effectively replace original
datasets. In this paper, unlike previous methods that focus solely on improving
the effectiveness of student distillation, we recognize and leverage the
important mutual influence between expert and student models. We observed that
the smoothness of expert trajectories has a significant impact on subsequent
student parameter alignment. Based on this, we propose an effective DD
framework named AST, standing for Alignment with Smooth and high-quality expert
Trajectories. We devise the integration of clipping loss and gradient penalty
to regulate the rate of parameter changes in expert trajectory generation. To
further refine the student parameter alignment with expert trajectory, we put
forward representative initialization for the synthetic dataset and balanced
inner-loop loss in response to the sensitivity exhibited towards randomly
initialized variables during distillation. We also propose two enhancement
strategies, namely intermediate matching loss and weight perturbation, to
mitigate the potential occurrence of cumulative errors. We conduct extensive
experiments on datasets of different scales, sizes, and resolutions. The
results demonstrate that the proposed method significantly outperforms prior
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Jiyuan Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenzhuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kwok-Yan Lam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10544">
<title>Use of probabilistic phrases in a coordination game: human versus GPT-4. (arXiv:2310.10544v3 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10544</link>
<description rdf:parseType="Literal">&lt;p&gt;English speakers use probabilistic phrases such as likely to communicate
information about the probability or likelihood of events. Communication is
successful to the extent that the listener grasps what the speaker means to
convey and, if communication is successful, individuals can potentially
coordinate their actions based on shared knowledge about uncertainty. We first
assessed human ability to estimate the probability and the ambiguity
(imprecision) of twenty-three probabilistic phrases in a coordination game in
two different contexts, investment advice and medical advice. We then had GPT4
(OpenAI), a Large Language Model, complete the same tasks as the human
participants. We found that the median human participant and GPT4 assigned
probability estimates that were in good agreement (proportions of variance
accounted for close to .90). GPT4&apos;s estimates of probability both in the
investment and Medical contexts were as close or closer to that of the human
participants as the human participants&apos; estimates were to one another.
Estimates of probability for both the human participants and GPT4 were little
affected by context. In contrast, human and GPT4 estimates of ambiguity were
not in such good agreement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Maloney_L/0/1/0/all/0/1&quot;&gt;Laurence T Maloney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Martello_M/0/1/0/all/0/1&quot;&gt;Maria F Dal Martello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Fei_V/0/1/0/all/0/1&quot;&gt;Vivian Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_V/0/1/0/all/0/1&quot;&gt;Valerie Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13121">
<title>Understanding Addition in Transformers. (arXiv:2310.13121v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13121</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the inner workings of machine learning models like Transformers
is vital for their safe and ethical use. This paper presents an in-depth
analysis of a one-layer Transformer model trained for n-digit integer addition.
We reveal that the model divides the task into parallel, digit-specific streams
and employs distinct algorithms for different digit positions. Our study also
finds that the model starts calculations late but executes them rapidly. A rare
use case with high loss is identified and explained. Overall, the model&apos;s
algorithm is explained in detail. These findings are validated through rigorous
testing and mathematical modeling, contributing to the broader works in
Mechanistic Interpretability, AI safety, and alignment. Our approach opens the
door for analyzing more complex tasks and multi-layer Transformer models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quirke_P/0/1/0/all/0/1&quot;&gt;Philip Quirke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13258">
<title>ManiCast: Collaborative Manipulation with Cost-Aware Human Forecasting. (arXiv:2310.13258v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13258</link>
<description rdf:parseType="Literal">&lt;p&gt;Seamless human-robot manipulation in close proximity relies on accurate
forecasts of human motion. While there has been significant progress in
learning forecast models at scale, when applied to manipulation tasks, these
models accrue high errors at critical transition points leading to degradation
in downstream planning performance. Our key insight is that instead of
predicting the most likely human motion, it is sufficient to produce forecasts
that capture how future human motion would affect the cost of a robot&apos;s plan.
We present ManiCast, a novel framework that learns cost-aware human forecasts
and feeds them to a model predictive control planner to execute collaborative
manipulation tasks. Our framework enables fluid, real-time interactions between
a human and a 7-DoF robot arm across a number of real-world tasks such as
reactive stirring, object handovers, and collaborative table setting. We
evaluate both the motion forecasts and the end-to-end forecaster-planner system
against a range of learned and heuristic baselines while additionally
contributing new datasets. We release our code and datasets at
https://portal-cornell.github.io/manicast/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kedia_K/0/1/0/all/0/1&quot;&gt;Kushal Kedia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dan_P/0/1/0/all/0/1&quot;&gt;Prithwish Dan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhardwaj_A/0/1/0/all/0/1&quot;&gt;Atiksh Bhardwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_S/0/1/0/all/0/1&quot;&gt;Sanjiban Choudhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.16842">
<title>Enhancing Energy-efficiency by Solving the Throughput Bottleneck of LSTM Cells for Embedded FPGAs. (arXiv:2310.16842v2 [cs.AR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.16842</link>
<description rdf:parseType="Literal">&lt;p&gt;To process sensor data in the Internet of Things(IoTs), embedded deep
learning for 1-dimensional data is an important technique. In the past, CNNs
were frequently used because they are simple to optimise for special embedded
hardware such as FPGAs. This work proposes a novel LSTM cell optimisation aimed
at energy-efficient inference on end devices. Using the traffic speed
prediction as a case study, a vanilla LSTM model with the optimised LSTM cell
achieves 17534 inferences per second while consuming only 3.8 $\mu$J per
inference on the FPGA XC7S15 from Spartan-7 family. It achieves at least
5.4$\times$ faster throughput and 1.37$\times$ more energy efficient than
existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chao Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_T/0/1/0/all/0/1&quot;&gt;Tianheng Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_G/0/1/0/all/0/1&quot;&gt;Gregor Schiele&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18332">
<title>WordArt Designer: User-Driven Artistic Typography Synthesis using Large Language Models. (arXiv:2310.18332v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18332</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces WordArt Designer, a user-driven framework for artistic
typography synthesis, relying on the Large Language Model (LLM). The system
incorporates four key modules: the LLM Engine, SemTypo, StyTypo, and TexTypo
modules. 1) The LLM Engine, empowered by the LLM (e.g., GPT-3.5), interprets
user inputs and generates actionable prompts for the other modules, thereby
transforming abstract concepts into tangible designs. 2) The SemTypo module
optimizes font designs using semantic concepts, striking a balance between
artistic transformation and readability. 3) Building on the semantic layout
provided by the SemTypo module, the StyTypo module creates smooth, refined
images. 4) The TexTypo module further enhances the design&apos;s aesthetics through
texture rendering, enabling the generation of inventive textured fonts.
Notably, WordArt Designer highlights the fusion of generative AI with artistic
typography. Experience its capabilities on ModelScope:
https://www.modelscope.cn/studios/WordArt/WordArt.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Z/0/1/0/all/0/1&quot;&gt;Zhi-Qi Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingdong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_W/0/1/0/all/0/1&quot;&gt;Wangmeng Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xianhui Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zengke Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yusen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Y/0/1/0/all/0/1&quot;&gt;Yifeng Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18387">
<title>OffMix-3L: A Novel Code-Mixed Dataset in Bangla-English-Hindi for Offensive Language Identification. (arXiv:2310.18387v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18387</link>
<description rdf:parseType="Literal">&lt;p&gt;Code-mixing is a well-studied linguistic phenomenon when two or more
languages are mixed in text or speech. Several works have been conducted on
building datasets and performing downstream NLP tasks on code-mixed data.
Although it is not uncommon to observe code-mixing of three or more languages,
most available datasets in this domain contain code-mixed data from only two
languages. In this paper, we introduce OffMix-3L, a novel offensive language
identification dataset containing code-mixed data from three different
languages. We experiment with several models on this dataset and observe that
BanglishBERT outperforms other transformer-based models and GPT-3.5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_D/0/1/0/all/0/1&quot;&gt;Dhiman Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raihan_M/0/1/0/all/0/1&quot;&gt;Md Nishat Raihan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmud_A/0/1/0/all/0/1&quot;&gt;Antara Mahmud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasopoulos_A/0/1/0/all/0/1&quot;&gt;Antonios Anastasopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zampieri_M/0/1/0/all/0/1&quot;&gt;Marcos Zampieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18446">
<title>A Novel Skip Orthogonal List for Dynamic Optimal Transport Problem. (arXiv:2310.18446v2 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18446</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal transportation is a fundamental topic that has attracted a great
amount of attention from machine learning community in the past decades. In
this paper, we consider an interesting discrete dynamic optimal transport
problem: can we efficiently update the optimal transport plan when the weights
or the locations of the data points change? This problem is naturally motivated
by several applications in machine learning. For example, we often need to
compute the optimal transportation cost between two different data sets; if
some change happens to a few data points, should we re-compute the high
complexity cost function or update the cost by some efficient dynamic data
structure? We are aware that several dynamic maximum flow algorithms have been
proposed before, however, the research on dynamic minimum cost flow problem is
still quite limited, to the best of our knowledge. We propose a novel 2D Skip
Orthogonal List together with some dynamic tree techniques. Although our
algorithm is based on the conventional simplex method, it can efficiently
complete each pivoting operation within $O(|V|)$ time with high probability
where $V$ is the set of all supply and demand nodes. Since dynamic
modifications typically do not introduce significant changes, our algorithm
requires only a few simplex iterations in practice. So our algorithm is more
efficient than re-computing the optimal transportation cost that needs at least
one traversal over all the $O(|E|) = O(|V|^2)$ variables in general cases. Our
experiments demonstrate that our algorithm significantly outperforms existing
algorithms in the dynamic scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hu Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19062">
<title>A multi-modal table tennis robot system. (arXiv:2310.19062v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19062</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, robotic table tennis has become a popular research challenge
for perception and robot control. Here, we present an improved table tennis
robot system with high accuracy vision detection and fast robot reaction. Based
on previous work, our system contains a KUKA robot arm with 6 DOF, with four
frame-based cameras and two additional event-based cameras. We developed a
novel calibration approach to calibrate this multimodal perception system. For
table tennis, spin estimation is crucial. Therefore, we introduced a novel, and
more accurate spin estimation approach. Finally, we show how combining the
output of an event-based camera and a Spiking Neural Network (SNN) can be used
for accurate ball detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziegler_A/0/1/0/all/0/1&quot;&gt;Andreas Ziegler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gossard_T/0/1/0/all/0/1&quot;&gt;Thomas Gossard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vetter_K/0/1/0/all/0/1&quot;&gt;Karl Vetter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tebbe_J/0/1/0/all/0/1&quot;&gt;Jonas Tebbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zell_A/0/1/0/all/0/1&quot;&gt;Andreas Zell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19453">
<title>FLIP: Towards Fine-grained Alignment between ID-based Models and Pretrained Language Models for CTR Prediction. (arXiv:2310.19453v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19453</link>
<description rdf:parseType="Literal">&lt;p&gt;Click-through rate (CTR) prediction plays as a core function module in
various personalized online services. The traditional ID-based models for CTR
prediction take as inputs the one-hot encoded ID features of tabular modality,
which capture the collaborative signals via feature interaction modeling. But
the one-hot encoding discards the semantic information conceived in the
original feature texts. Recently, the emergence of Pretrained Language Models
(PLMs) has given rise to another paradigm, which takes as inputs the sentences
of textual modality obtained by hard prompt templates and adopts PLMs to
extract the semantic knowledge. However, PLMs generally tokenize the input text
data into subword tokens and ignore field-wise collaborative signals.
Therefore, these two lines of research focus on different characteristics of
the same input data (i.e., textual and tabular modalities), forming a distinct
complementary relationship with each other. In this paper, we propose to
conduct Fine-grained feature-level ALignment between ID-based Models and
Pretrained Language Models (FLIP) for CTR prediction. We design a novel joint
reconstruction pretraining task for both masked language and tabular modeling.
Specifically, the masked data of one modality (i.e., tokens or features) has to
be recovered with the help of the other modality, which establishes the
feature-level interaction and alignment via sufficient mutual information
extraction between dual modalities. Moreover, we propose to jointly finetune
the ID-based model and PLM for downstream CTR prediction tasks, thus achieving
superior performance by combining the advantages of both models. Extensive
experiments on three real-world datasets demonstrate that FLIP outperforms SOTA
baselines, and is highly compatible for various ID-based models and PLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hangyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jianghao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chenxu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_R/0/1/0/all/0/1&quot;&gt;Ruiming Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19736">
<title>Evaluating Large Language Models: A Comprehensive Survey. (arXiv:2310.19736v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19736</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated remarkable capabilities across
a broad spectrum of tasks. They have attracted significant attention and been
deployed in numerous downstream applications. Nevertheless, akin to a
double-edged sword, LLMs also present potential risks. They could suffer from
private data leaks or yield inappropriate, harmful, or misleading content.
Additionally, the rapid progress of LLMs raises concerns about the potential
emergence of superintelligent systems without adequate safeguards. To
effectively capitalize on LLM capacities as well as ensure their safe and
beneficial development, it is critical to conduct a rigorous and comprehensive
evaluation of LLMs.
&lt;/p&gt;
&lt;p&gt;This survey endeavors to offer a panoramic perspective on the evaluation of
LLMs. We categorize the evaluation of LLMs into three major groups: knowledge
and capability evaluation, alignment evaluation and safety evaluation. In
addition to the comprehensive review on the evaluation methodologies and
benchmarks on these three aspects, we collate a compendium of evaluations
pertaining to LLMs&apos; performance in specialized domains, and discuss the
construction of comprehensive evaluation platforms that cover LLM evaluations
on capabilities, alignment, safety, and applicability.
&lt;/p&gt;
&lt;p&gt;We hope that this comprehensive overview will stimulate further research
interests in the evaluation of LLMs, with the ultimate goal of making
evaluation serve as a cornerstone in guiding the responsible development of
LLMs. We envision that this will channel their evolution into a direction that
maximizes societal benefit while minimizing potential risks. A curated list of
related papers has been publicly available at
https://github.com/tjunlp-lab/Awesome-LLMs-Evaluation-Papers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zishan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Renren Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chuang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yufei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1&quot;&gt;Dan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Supryadi/0/1/0/all/0/1&quot;&gt;Supryadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Linhao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_B/0/1/0/all/0/1&quot;&gt;Bojian Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_D/0/1/0/all/0/1&quot;&gt;Deyi Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19806">
<title>Automated Verification of Equivalence Properties in Advanced Logic Programs -- Bachelor Thesis. (arXiv:2310.19806v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19806</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increase in industrial applications using Answer Set Programming,
the need for formal verification tools, particularly for critical applications,
has also increased. During the program optimisation process, it would be
desirable to have a tool which can automatically verify whether an optimised
subprogram can replace the original subprogram. Formally this corresponds to
the problem of verifying the strong equivalence of two programs. In order to do
so, the translation tool anthem was developed. It can be used in conjunction
with an automated theorem prover for classical logic to verify that two
programs are strongly equivalent. With the current version of anthem, only the
strong equivalence of positive programs with a restricted input language can be
verified. This is a result of the translation $\tau^*$ implemented in anthem
that produces formulas in the logic of here-and-there, which coincides with
classical logic only for positive programs. This thesis extends anthem in order
to overcome these limitations. First, the transformation $\sigma^*$ is
presented, which transforms formulas from the logic of here-and-there to
classical logic. A theorem formalises how $\sigma^*$ can be used to express
equivalence in the logic of here-and-there in classical logic. Second, the
translation $\tau^*$ is extended to programs containing pools. Another theorem
shows how $\sigma^*$ can be combined with $\tau^*$ to express the strong
equivalence of two programs in classical logic. With $\sigma^*$ and the
extended $\tau^*$, it is possible to express the strong equivalence of logic
programs containing negation, simple choices, and pools. Both the extended
$\tau^*$ and $\sigma^*$ are implemented in a new version of anthem. Several
examples of logic programs containing pools, negation, and simple choice rules,
which the new version of anthem can translate to classical logic, are
presented. Some a...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heuer_J/0/1/0/all/0/1&quot;&gt;Jan Heuer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20174">
<title>GraphTransformers for Geospatial Forecasting of Hurricane Trajectories. (arXiv:2310.20174v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20174</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we introduce a novel framework for trajectory prediction of
geospatial sequences using GraphTransformers. When viewed across several
sequences, we observed that a graph structure automatically emerges between
different geospatial points that is often not taken into account for such
sequence modeling tasks. We show that by leveraging this graph structure
explicitly, geospatial trajectory prediction can be significantly improved. Our
GraphTransformer approach improves upon state-of-the-art Transformer based
baseline significantly on HURDAT, a dataset where we are interested in
predicting the trajectory of a hurricane on a 6 hourly basis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_P/0/1/0/all/0/1&quot;&gt;Pallavi Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_S/0/1/0/all/0/1&quot;&gt;Satyaki Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.01043">
<title>LLM4Drive: A Survey of Large Language Models for Autonomous Driving. (arXiv:2311.01043v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.01043</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous driving technology, a catalyst for revolutionizing transportation
and urban mobility, has the tend to transition from rule-based systems to
data-driven strategies. Traditional module-based systems are constrained by
cumulative errors among cascaded modules and inflexible pre-set rules. In
contrast, end-to-end autonomous driving systems have the potential to avoid
error accumulation due to their fully data-driven training process, although
they often lack transparency due to their &quot;black box&quot; nature, complicating the
validation and traceability of decisions. Recently, large language models
(LLMs) have demonstrated abilities including understanding context, logical
reasoning, and generating answers. A natural thought is to utilize these
abilities to empower autonomous driving. By combining LLM with foundation
vision models, it could open the door to open-world understanding, reasoning,
and few-shot learning, which current autonomous driving systems are lacking. In
this paper, we systematically review a research line about \textit{Large
Language Models for Autonomous Driving (LLM4AD)}. This study evaluates the
current state of technological advancements, distinctly outlining the principal
challenges and prospective directions for the field. For the convenience of
researchers in academia and industry, we provide real-time updates on the
latest advances in the field as well as relevant open-source resources via the
designated link: https://github.com/Thinklab-SJTU/Awesome-LLM4AD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhenjie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaosong Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Junchi Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04014">
<title>A Method to Improve the Performance of Reinforcement Learning Based on the Y Operator for a Class of Stochastic Differential Equation-Based Child-Mother Systems. (arXiv:2311.04014v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04014</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a novel operator, termed the Y operator, to elevate
control performance in Actor-Critic(AC) based reinforcement learning for
systems governed by stochastic differential equations(SDEs). The Y operator
ingeniously integrates the stochasticity of a class of child-mother system into
the Critic network&apos;s loss function, yielding substantial advancements in the
control performance of RL algorithms.Additionally, the Y operator elegantly
reformulates the challenge of solving partial differential equations for the
state-value function into a parallel problem for the drift and diffusion
functions within the system&apos;s SDEs.A rigorous mathematical proof confirms the
operator&apos;s validity.This transformation enables the Y Operator-based
Reinforcement Learning(YORL) framework to efficiently tackle optimal control
problems in both model-based and data-driven systems.The superiority of YORL is
demonstrated through linear and nonlinear numerical examples showing its
enhanced performance over existing methods post convergence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Cheng Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.05419">
<title>Mirror: A Universal Framework for Various Information Extraction Tasks. (arXiv:2311.05419v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.05419</link>
<description rdf:parseType="Literal">&lt;p&gt;Sharing knowledge between information extraction tasks has always been a
challenge due to the diverse data formats and task variations. Meanwhile, this
divergence leads to information waste and increases difficulties in building
complex applications in real scenarios. Recent studies often formulate IE tasks
as a triplet extraction problem. However, such a paradigm does not support
multi-span and n-ary extraction, leading to weak versatility. To this end, we
reorganize IE problems into unified multi-slot tuples and propose a universal
framework for various IE tasks, namely Mirror. Specifically, we recast existing
IE tasks as a multi-span cyclic graph extraction problem and devise a
non-autoregressive graph decoding algorithm to extract all spans in a single
step. It is worth noting that this graph structure is incredibly versatile, and
it supports not only complex IE tasks, but also machine reading comprehension
and classification tasks. We manually construct a corpus containing 57 datasets
for model pretraining, and conduct experiments on 30 datasets across 8
downstream tasks. The experimental results demonstrate that our model has
decent compatibility and outperforms or reaches competitive performance with
SOTA systems under few-shot and zero-shot settings. The code, model weights,
and pretraining corpus are available at https://github.com/Spico197/Mirror .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_J/0/1/0/all/0/1&quot;&gt;Junfei Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zijian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Mengsong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guoliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaoye Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenliang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhefeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huai_B/0/1/0/all/0/1&quot;&gt;Baoxing Huai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07590">
<title>Technical Report: Large Language Models can Strategically Deceive their Users when Put Under Pressure. (arXiv:2311.07590v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07590</link>
<description rdf:parseType="Literal">&lt;p&gt;We demonstrate a situation in which Large Language Models, trained to be
helpful, harmless, and honest, can display misaligned behavior and
strategically deceive their users about this behavior without being instructed
to do so. Concretely, we deploy GPT-4 as an agent in a realistic, simulated
environment, where it assumes the role of an autonomous stock trading agent.
Within this environment, the model obtains an insider tip about a lucrative
stock trade and acts upon it despite knowing that insider trading is
disapproved of by company management. When reporting to its manager, the model
consistently hides the genuine reasons behind its trading decision. We perform
a brief investigation of how this behavior varies under changes to the setting,
such as removing model access to a reasoning scratchpad, attempting to prevent
the misaligned behavior by changing system instructions, changing the amount of
pressure the model is under, varying the perceived risk of getting caught, and
making other simple changes to the environment. To our knowledge, this is the
first demonstration of Large Language Models trained to be helpful, harmless,
and honest, strategically deceiving their users in a realistic situation
without direct instructions or training for deception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scheurer_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Scheurer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balesni_M/0/1/0/all/0/1&quot;&gt;Mikita Balesni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hobbhahn_M/0/1/0/all/0/1&quot;&gt;Marius Hobbhahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09247">
<title>Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks. (arXiv:2311.09247v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09247</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the abstract reasoning abilities of text-only and multimodal
versions of GPT-4, using the ConceptARC benchmark [10], which is designed to
evaluate robust understanding and reasoning with core-knowledge concepts. We
extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed,
one-shot prompting (rather than simple, zero-shot prompts) with text versions
of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4,
on zero- and one-shot prompts using image versions of the simplest tasks. Our
experimental results support the conclusion that neither version of GPT-4 has
developed robust abstraction abilities at humanlike levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_M/0/1/0/all/0/1&quot;&gt;Melanie Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmarini_A/0/1/0/all/0/1&quot;&gt;Alessandro B. Palmarini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskvichev_A/0/1/0/all/0/1&quot;&gt;Arseny Moskvichev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09680">
<title>Trustworthy Large Models in Vision: A Survey. (arXiv:2311.09680v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09680</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Large Models (LMs) has recently revolutionized various
fields of deep learning with remarkable grades, ranging from Natural Language
Processing (NLP) to Computer Vision (CV). However, LMs are increasingly
challenged and criticized by academia and industry due to their powerful
performance but untrustworthy behavior, which urgently needs to be alleviated
by reliable methods. Despite the abundance of literature on trustworthy LMs in
NLP, a systematic survey specifically delving into the trustworthiness of LMs
in CV remains absent. In order to mitigate this gap, we summarize four relevant
concerns that obstruct the trustworthy usage in vision of LMs in this survey,
including 1) human misuse, 2) vulnerability, 3) inherent issue and 4)
interpretability. By highlighting corresponding challenge, countermeasures, and
discussion in each topic, we hope this survey will facilitate readers&apos;
understanding of this field, promote alignment of LMs with human expectations
and enable trustworthy LMs to serve as welfare rather than disaster for human
society.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Ziyan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Li Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09740">
<title>Redefining Super-Resolution: Fine-mesh PDE predictions without classical simulations. (arXiv:2311.09740v3 [physics.flu-dyn] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09740</link>
<description rdf:parseType="Literal">&lt;p&gt;In Computational Fluid Dynamics (CFD), coarse mesh simulations offer
computational efficiency but often lack precision. Applying conventional
super-resolution to these simulations poses a significant challenge due to the
fundamental contrast between downsampling high-resolution images and
authentically emulating low-resolution physics. The former method conserves
more of the underlying physics, surpassing the usual constraints of real-world
scenarios. We propose a novel definition of super-resolution tailored for
PDE-based problems. Instead of simply downsampling from a high-resolution
dataset, we use coarse-grid simulated data as our input and predict fine-grid
simulated outcomes. Employing a physics-infused UNet upscaling method, we
demonstrate its efficacy across various 2D-CFD problems such as discontinuity
detection in Burger&apos;s equation, Methane combustion, and fouling in Industrial
heat exchangers. Our method enables the generation of fine-mesh solutions
bypassing traditional simulation, ensuring considerable computational saving
and fidelity to the original ground truth outcomes. Through diverse boundary
conditions during training, we further establish the robustness of our method,
paving the way for its broad applications in engineering and scientific CFD
solvers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sarkar_R/0/1/0/all/0/1&quot;&gt;Rajat Kumar Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Majumdar_R/0/1/0/all/0/1&quot;&gt;Ritam Majumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jadhav_V/0/1/0/all/0/1&quot;&gt;Vishal Jadhav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sakhinana_S/0/1/0/all/0/1&quot;&gt;Sagar Srinivas Sakhinana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Runkana_V/0/1/0/all/0/1&quot;&gt;Venkataramana Runkana&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10522">
<title>Enhancing Object Coherence in Layout-to-Image Synthesis. (arXiv:2311.10522v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10522</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-to-image synthesis is an emerging technique in conditional image
generation. It aims to generate complex scenes, where users require fine
control over the layout of the objects in a scene. However, it remains
challenging to control the object coherence, including semantic coherence
(e.g., the cat looks at the flowers or not) and physical coherence (e.g., the
hand and the racket should not be misaligned). In this paper, we propose a
novel diffusion model with effective global semantic fusion (GSF) and
self-similarity feature enhancement modules to guide the object coherence for
this task. For semantic coherence, we argue that the image caption contains
rich information for defining the semantic relationship within the objects in
the images. Instead of simply employing cross-attention between captions and
generated images, which addresses the highly relevant layout restriction and
semantic coherence separately and thus leads to unsatisfying results shown in
our experiments, we develop GSF to fuse the supervision from the layout
restriction and semantic coherence requirement and exploit it to guide the
image synthesis process. Moreover, to improve the physical coherence, we
develop a Self-similarity Coherence Attention (SCA) module to explicitly
integrate local contextual physical coherence into each pixel&apos;s generation
process. Specifically, we adopt a self-similarity map to encode the coherence
restrictions and employ it to extract coherent features from text embedding.
Through visualization of our self-similarity map, we explore the essence of
SCA, revealing that its effectiveness is not only in capturing reliable
physical coherence patterns but also in enhancing complex texture generation.
Extensive experiments demonstrate the superiority of our proposed method in
both image generation quality and controllability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weizhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jianwei Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cheng Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10934">
<title>Case Repositories: Towards Case-Based Reasoning for AI Alignment. (arXiv:2311.10934v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10934</link>
<description rdf:parseType="Literal">&lt;p&gt;Case studies commonly form the pedagogical backbone in law, ethics, and many
other domains that face complex and ambiguous societal questions informed by
human values. Similar complexities and ambiguities arise when we consider how
AI should be aligned in practice: when faced with vast quantities of diverse
(and sometimes conflicting) values from different individuals and communities,
with whose values is AI to align, and how should AI do so? We propose a
complementary approach to constitutional AI alignment, grounded in ideas from
case-based reasoning (CBR), that focuses on the construction of policies
through judgments on a set of cases. We present a process to assemble such a
case repository by: 1) gathering a set of ``seed&apos;&apos; cases -- questions one may
ask an AI system -- in a particular domain, 2) eliciting domain-specific key
dimensions for cases through workshops with domain experts, 3) using LLMs to
generate variations of cases not seen in the wild, and 4) engaging with the
public to judge and improve cases. We then discuss how such a case repository
could assist in AI alignment, both through directly acting as precedents to
ground acceptable behaviors, and as a medium for individuals and communities to
engage in moral reasoning around AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_K/0/1/0/all/0/1&quot;&gt;K. J. Kevin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Quan Ze Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_I/0/1/0/all/0/1&quot;&gt;Inyoung Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_K/0/1/0/all/0/1&quot;&gt;King Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy X. Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11235">
<title>Unraveling the &quot;Anomaly&quot; in Time Series Anomaly Detection: A Self-supervised Tri-domain Solution. (arXiv:2311.11235v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11235</link>
<description rdf:parseType="Literal">&lt;p&gt;The ongoing challenges in time series anomaly detection (TSAD), notably the
scarcity of anomaly labels and the variability in anomaly lengths and shapes,
have led to the need for a more efficient solution. As limited anomaly labels
hinder traditional supervised models in TSAD, various SOTA deep learning
techniques, such as self-supervised learning, have been introduced to tackle
this issue. However, they encounter difficulties handling variations in anomaly
lengths and shapes, limiting their adaptability to diverse anomalies.
Additionally, many benchmark datasets suffer from the problem of having
explicit anomalies that even random functions can detect. This problem is
exacerbated by ill-posed evaluation metrics, known as point adjustment (PA),
which can result in inflated model performance. In this context, we propose a
novel self-supervised learning based Tri-domain Anomaly Detector (TriAD), which
addresses these challenges by modeling features across three data domains -
temporal, frequency, and residual domains - without relying on anomaly labels.
Unlike traditional contrastive learning methods, TriAD employs both
inter-domain and intra-domain contrastive loss to learn common attributes among
normal data and differentiate them from anomalies. Additionally, our approach
can detect anomalies of varying lengths by integrating with a discord discovery
algorithm. It is worth noting that this study is the first to reevaluate the
deep learning potential in TSAD, utilizing both rigorously designed datasets
(i.e., UCR Archive) and evaluation metrics (i.e., PA%K and affiliation).
Through experimental results on the UCR dataset, TriAD achieves an impressive
three-fold increase in PA%K based F1 scores over SOTA deep learning models, and
50% increase of accuracy as compared to SOTA discord discovery algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuting Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_G/0/1/0/all/0/1&quot;&gt;Guanhua Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Hongzhi Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11435">
<title>Unveiling Public Perceptions: Machine Learning-Based Sentiment Analysis of COVID-19 Vaccines in India. (arXiv:2311.11435v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11435</link>
<description rdf:parseType="Literal">&lt;p&gt;In March 2020, the World Health Organisation declared COVID-19 a global
pandemic as it spread to nearly every country. By mid-2021, India had
introduced three vaccines: Covishield, Covaxin, and Sputnik. To ensure
successful vaccination in a densely populated country like India, understanding
public sentiment was crucial. Social media, particularly Reddit with over 430
million users, played a vital role in disseminating information. This study
employs data mining techniques to analyze Reddit data and gauge Indian
sentiments towards COVID-19 vaccines. Using Python&apos;s Text Blob library,
comments are annotated to assess general sentiments. Results show that most
Reddit users in India expressed neutrality about vaccination, posing a
challenge for the Indian government&apos;s efforts to vaccinate a significant
portion of the population.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_M/0/1/0/all/0/1&quot;&gt;Milind Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaushik_A/0/1/0/all/0/1&quot;&gt;Abhishek Kaushik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11602">
<title>A Multi-In-Single-Out Network for Video Frame Interpolation without Optical Flow. (arXiv:2311.11602v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11602</link>
<description rdf:parseType="Literal">&lt;p&gt;In general, deep learning-based video frame interpolation (VFI) methods have
predominantly focused on estimating motion vectors between two input frames and
warping them to the target time. While this approach has shown impressive
performance for linear motion between two input frames, it exhibits limitations
when dealing with occlusions and nonlinear movements. Recently, generative
models have been applied to VFI to address these issues. However, as VFI is not
a task focused on generating plausible images, but rather on predicting
accurate intermediate frames between two given frames, performance limitations
still persist. In this paper, we propose a multi-in-single-out (MISO) based VFI
method that does not rely on motion vector estimation, allowing it to
effectively model occlusions and nonlinear motion. Additionally, we introduce a
novel motion perceptual loss that enables MISO-VFI to better capture the
spatio-temporal correlations within the video frames. Our MISO-VFI method
achieves state-of-the-art results on VFI benchmarks Vimeo90K, Middlebury, and
UCF101, with a significant performance gap compared to existing approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaemin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_M/0/1/0/all/0/1&quot;&gt;Minseok Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyobin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_D/0/1/0/all/0/1&quot;&gt;Dong-Geol Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11810">
<title>DocPedia: Unleashing the Power of Large Multimodal Model in the Frequency Domain for Versatile Document Understanding. (arXiv:2311.11810v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11810</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents DocPedia, a novel large multimodal model (LMM) for
versatile OCR-free document understanding, capable of parsing images up to
2,560$\times$2,560 resolution. Unlike existing work either struggle with
high-resolution documents or give up the large language model thus vision or
language ability constrained, our DocPedia directly processes visual input in
the frequency domain rather than the pixel space. The unique characteristic
enables DocPedia to capture a greater amount of visual and textual information
using a limited number of visual tokens. To consistently enhance both
perception and comprehension abilities of our model, we develop a dual-stage
training strategy and enrich instructions/annotations of all training tasks
covering multiple document types. Extensive quantitative and qualitative
experiments conducted on various publicly available benchmarks confirm the
mutual benefits of jointly learning perception and comprehension tasks. The
results provide further evidence of the effectiveness and superior performance
of our DocPedia over other methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wengang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Houqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Can Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12454">
<title>HierSpeech++: Bridging the Gap between Semantic and Acoustic Representation of Speech by Hierarchical Variational Inference for Zero-shot Speech Synthesis. (arXiv:2311.12454v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12454</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLM)-based speech synthesis has been widely adopted in
zero-shot speech synthesis. However, they require a large-scale data and
possess the same limitations as previous autoregressive speech models,
including slow inference speed and lack of robustness. This paper proposes
HierSpeech++, a fast and strong zero-shot speech synthesizer for text-to-speech
(TTS) and voice conversion (VC). We verified that hierarchical speech synthesis
frameworks could significantly improve the robustness and expressiveness of the
synthetic speech. Furthermore, we significantly improve the naturalness and
speaker similarity of synthetic speech even in zero-shot speech synthesis
scenarios. For text-to-speech, we adopt the text-to-vec framework, which
generates a self-supervised speech representation and an F0 representation
based on text representations and prosody prompts. Then, HierSpeech++ generates
speech from the generated vector, F0, and voice prompt. We further introduce a
high-efficient speech super-resolution framework from 16 kHz to 48 kHz. The
experimental results demonstrated that the hierarchical variational autoencoder
could be a strong zero-shot speech synthesizer given that it outperforms
LLM-based and diffusion-based models. Moreover, we achieved the first
human-level quality zero-shot speech synthesis. Audio samples and source code
are available at https://github.com/sh-lee-prml/HierSpeechpp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-Hoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Ha-Yeong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung-Bin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seong-Whan Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12986">
<title>Unsupervised Graph Attention Autoencoder for Attributed Networks using K-means Loss. (arXiv:2311.12986v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12986</link>
<description rdf:parseType="Literal">&lt;p&gt;Several natural phenomena and complex systems are often represented as
networks. Discovering their community structure is a fundamental task for
understanding these networks. Many algorithms have been proposed, but recently,
Graph Neural Networks (GNN) have emerged as a compelling approach for enhancing
this task.In this paper, we introduce a simple, efficient, and
clustering-oriented model based on unsupervised \textbf{G}raph Attention
\textbf{A}uto\textbf{E}ncoder for community detection in attributed networks
(GAECO). The proposed model adeptly learns representations from both the
network&apos;s topology and attribute information, simultaneously addressing dual
objectives: reconstruction and community discovery. It places a particular
emphasis on discovering compact communities by robustly minimizing clustering
errors. The model employs k-means as an objective function and utilizes a
multi-head Graph Attention Auto-Encoder for decoding the representations.
Experiments conducted on three datasets of attributed networks show that our
method surpasses state-of-the-art algorithms in terms of NMI and ARI.
Additionally, our approach scales effectively with the size of the network,
making it suitable for large-scale applications. The implications of our
findings extend beyond biological network interpretation and social network
analysis, where knowledge of the fundamental community structure is essential.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bekkaira_A/0/1/0/all/0/1&quot;&gt;Abdelfateh Bekkaira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellaouar_S/0/1/0/all/0/1&quot;&gt;Slimane Bellaouar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oulad_Naoui_S/0/1/0/all/0/1&quot;&gt;Slimane Oulad-Naoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13063">
<title>From Classification to Clinical Insights: Towards Analyzing and Reasoning About Mobile and Behavioral Health Data With Large Language Models. (arXiv:2311.13063v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13063</link>
<description rdf:parseType="Literal">&lt;p&gt;Passively collected behavioral health data from ubiquitous sensors holds
significant promise to provide mental health professionals insights from
patient&apos;s daily lives; however, developing analysis tools to use this data in
clinical practice requires addressing challenges of generalization across
devices and weak or ambiguous correlations between the measured signals and an
individual&apos;s mental health. To address these challenges, we take a novel
approach that leverages large language models (LLMs) to synthesize clinically
useful insights from multi-sensor data. We develop chain of thought prompting
methods that use LLMs to generate reasoning about how trends in data such as
step count and sleep relate to conditions like depression and anxiety. We first
demonstrate binary depression classification with LLMs achieving accuracies of
61.1% which exceed the state of the art. While it is not robust for clinical
use, this leads us to our key finding: even more impactful and valued than
classification is a new human-AI collaboration approach in which clinician
experts interactively query these tools and combine their domain expertise and
context about the patient with AI generated reasoning to support clinical
decision-making. We find models like GPT-4 correctly reference numerical data
75% of the time, and clinician participants express strong interest in using
this approach to interpret self-tracking data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Englhardt_Z/0/1/0/all/0/1&quot;&gt;Zachary Englhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chengqian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_M/0/1/0/all/0/1&quot;&gt;Margaret E. Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xuhai &amp;quot;Orson&amp;quot; Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Chun-Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Lianhui Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDuff_D/0/1/0/all/0/1&quot;&gt;Daniel McDuff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Shwetak Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iyer_V/0/1/0/all/0/1&quot;&gt;Vikram Iyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13265">
<title>Improved identification accuracy in equation learning via comprehensive $\boldsymbol{R^2}$-elimination and Bayesian model selection. (arXiv:2311.13265v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13265</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of equation learning, exhaustively considering all possible
equations derived from a basis function dictionary is infeasible. Sparse
regression and greedy algorithms have emerged as popular approaches to tackle
this challenge. However, the presence of multicollinearity poses difficulties
for sparse regression techniques, and greedy steps may inadvertently exclude
terms of the true equation, leading to reduced identification accuracy. In this
article, we present an approach that strikes a balance between
comprehensiveness and efficiency in equation learning. Inspired by stepwise
regression, our approach combines the coefficient of determination, $R^2$, and
the Bayesian model evidence, $p(\boldsymbol y|\mathcal M)$, in a novel way. Our
procedure is characterized by a comprehensive search with just a minor
reduction of the model space at each iteration step. With two flavors of our
approach and the adoption of $p(\boldsymbol y|\mathcal M)$ for bi-directional
stepwise regression, we present a total of three new avenues for equation
learning. Through three extensive numerical experiments involving random
polynomials and dynamical systems, we compare our approach against four
state-of-the-art methods and two standard approaches. The results demonstrate
that our comprehensive search approach surpasses all other methods in terms of
identification accuracy. In particular, the second flavor of our approach
establishes an efficient overfitting penalty solely based on $R^2$, which
achieves highest rates of exact equation recovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nickelsen_D/0/1/0/all/0/1&quot;&gt;Daniel Nickelsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bah_B/0/1/0/all/0/1&quot;&gt;Bubacarr Bah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13373">
<title>Large Language Model is a Good Policy Teacher for Training Reinforcement Learning Agents. (arXiv:2311.13373v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13373</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have shown that Large Language Models (LLMs) can be utilized
for solving complex sequential decision-making tasks by providing high-level
instructions. However, LLM-based agents face limitations in real-time dynamic
environments due to their lack of specialization in solving specific target
problems. Moreover, the deployment of such LLM-based agents is both costly and
time-consuming in practical scenarios. In this paper, we introduce a novel
framework that addresses these challenges by training a smaller scale
specialized student agent using instructions from an LLM-based teacher agent.
By leveraging guided actions provided by the teachers, the prior knowledge of
the LLM is distilled into the local student model. Consequently, the student
agent can be trained with significantly less data. Furthermore, subsequent
training with environment feedback empowers the student agents to surpass the
capabilities of their teachers. We conducted experiments on three challenging
MiniGrid environments to evaluate the effectiveness of our framework. The
results demonstrate that our approach enhances sample efficiency and achieves
superior performance compared to baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zihao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenyang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13534">
<title>LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13534</link>
<description rdf:parseType="Literal">&lt;p&gt;The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose a novel method which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging (namely LM-Cocktail), where the fine-tuned language model
is merged with the pre-trained base model or the peer models from other domains
through weighted average. Despite simplicity, LM-Cocktail is surprisingly
effective: the resulted model is able to achieve a strong empirical performance
in the whole scope of general tasks while preserving a superior capacity in its
targeted domain. We conduct comprehensive experiments with LLama and BGE model
on popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shitao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peitian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xingrun Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13580">
<title>$\sigma$-PCA: a unified neural model for linear and nonlinear principal component analysis. (arXiv:2311.13580v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13580</link>
<description rdf:parseType="Literal">&lt;p&gt;Linear principal component analysis (PCA), nonlinear PCA, and linear
independent component analysis (ICA) -- those are three methods with
single-layer autoencoder formulations for learning linear transformations from
data. Linear PCA learns orthogonal transformations (rotations) that orient axes
to maximise variance, but it suffers from a subspace rotational indeterminacy:
it fails to find a unique rotation for axes that share the same variance. Both
nonlinear PCA and linear ICA reduce the subspace indeterminacy from rotational
to permutational by maximising statistical independence under the assumption of
unit variance. The relationship between all three can be understood by the
singular value decomposition of the linear ICA transformation into a sequence
of rotation, scale, rotation. Linear PCA learns the first rotation; nonlinear
PCA learns the second. The scale is simply the inverse of the standard
deviations. The problem is that, in contrast to linear PCA, conventional
nonlinear PCA cannot be used directly on the data to learn the first rotation,
the first being special as it reduces dimensionality and orders by variances.
In this paper, we have identified the cause, and as a solution we propose
$\sigma$-PCA: a unified neural model for linear and nonlinear PCA as
single-layer autoencoders. One of its key ingredients: modelling not just the
rotation but also the scale -- the variances. This model bridges the disparity
between linear and nonlinear PCA. And so, like linear PCA, it can learn a
semi-orthogonal transformation that reduces dimensionality and orders by
variances, but, unlike linear PCA, it does not suffer from rotational
indeterminacy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanavati_F/0/1/0/all/0/1&quot;&gt;Fahdi Kanavati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katsnith_L/0/1/0/all/0/1&quot;&gt;Lucy Katsnith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuneki_M/0/1/0/all/0/1&quot;&gt;Masayuki Tsuneki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13721">
<title>Nova$^+$: Generative Language Models for Binaries. (arXiv:2311.13721v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13721</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative large language models (LLMs) pre-trained on code have shown
impressive effectiveness in code generation, program repair, and document
analysis. However, existing generative LLMs focus on source code and are not
specialized for binaries. There are three main challenges for LLMs to model and
learn binary code: hex-decimal values, complex global dependencies, and
compiler optimization levels. To bring the benefit of LLMs to the binary
domain, we develop Nova and Nova$^+$, which are LLMs pre-trained on binary
corpora. Nova is pre-trained with the standard language modeling task, showing
significantly better capability on five benchmarks for three downstream tasks:
binary code similarity detection (BCSD), binary code translation (BCT), and
binary code recovery (BCR), over GPT-3.5 and other existing techniques. We
build Nova$^+$ to further boost Nova using two new pre-training tasks, i.e.,
optimization generation and optimization level prediction, which are designed
to learn binary optimization and align equivalent binaries. Nova$^+$ shows
overall the best performance for all three downstream tasks on five benchmarks,
demonstrating the contributions of the new pre-training tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_N/0/1/0/all/0/1&quot;&gt;Nan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kevin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiangzhe Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_L/0/1/0/all/0/1&quot;&gt;Lin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13770">
<title>Archiving Body Movements: Collective Generation of Chinese Calligraphy. (arXiv:2311.13770v2 [cs.MM] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13770</link>
<description rdf:parseType="Literal">&lt;p&gt;As a communication channel, body movements have been widely explored in
behavioral studies and kinesics. Performing and visual arts share the same
interests but focus on documenting and representing human body movements, such
as for dance notation and visual work creation. This paper investigates body
movements in oriental calligraphy and how to apply calligraphy principles to
stimulate and archive body movements. Through an artwork (Wushu), the authors
experiment with an interactive and generative approach to engage the audience&apos;s
bodily participation and archive the body movements as a compendium of
generated calligraphy. The audience assumes the role of both writers and
readers; creating (&quot;writing&quot;) and appreciating (&quot;reading&quot;) the generated
calligraphy becomes a cyclical process within this infinite &quot;Book,&quot; which can
motivate further attention and discussions concerning Chinese characters and
calligraphy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aven Le Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jiayi Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianchen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13811">
<title>Education distillation:getting student models to learn in shcools. (arXiv:2311.13811v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13811</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge distillation is one of the methods for model compression, and
existing knowledge distillation techniques focus on how to improve the
distillation algorithm so as to enhance the distillation efficiency. This paper
introduces dynamic incremental learning into knowledge distillation and
proposes a distillation strategy for education distillation. Specifically, it
is proposed to take fragmented student models divided from the complete student
model as lower-grade models. As the grade level rises, fragmented student
models deepen in conjunction with designed teaching reference layers, while
learning and distilling from more teacher models. By moving from lower to
higher grades, fragmented student models were gradually integrated into a
complete target student model, and the performance of the student models
gradually improved from lower to higher grades of the stage. Education
distillation strategies combined with distillation algorithms outperform the
results of single distillation algorithms on the public dataset
CIFAR100,Caltech256, Food-101 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Ling Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Danyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tianhao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_X/0/1/0/all/0/1&quot;&gt;Xuliang Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14084">
<title>AI-Generated Images Introduce Invisible Relevance Bias to Text-Image Retrieval. (arXiv:2311.14084v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14084</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of generation models, AI-generated content (AIGC) is
becoming more realistic, flooding the Internet. A recent study suggests that
this phenomenon has elevated the issue of source bias in text retrieval for web
searches. Specifically, neural retrieval models tend to rank generated texts
higher than human-written texts. In this paper, we extend the study of this
bias to cross-modal retrieval. Firstly, we successfully construct a suitable
benchmark to explore the existence of the bias. Subsequent extensive
experiments on this benchmark reveal that AI-generated images introduce an
invisible relevance bias to text-image retrieval models. Specifically, our
experiments show that text-image retrieval models tend to rank the AI-generated
images higher than the real images, even though the AI-generated images do not
exhibit more visually relevant features to the query than real images. This
invisible relevance bias is prevalent across retrieval models with varying
training data and architectures. Furthermore, our subsequent exploration
reveals that the inclusion of AI-generated images in the training data of the
retrieval models exacerbates the invisible relevance bias. The above phenomenon
triggers a vicious cycle, which makes the invisible relevance bias become more
and more serious. To elucidate the potential causes of invisible relevance and
address the aforementioned issues, we introduce an effective training method
aimed at alleviating the invisible relevance bias. Subsequently, we apply our
proposed debiasing method to retroactively identify the causes of invisible
relevance, revealing that the AI-generated images induce the image encoder to
embed additional information into their representation. This information
exhibits a certain consistency across generated images with different semantics
and can make the retriever estimate a higher relevance score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shicheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_D/0/1/0/all/0/1&quot;&gt;Danyang Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_L/0/1/0/all/0/1&quot;&gt;Liang Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jingcheng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huawei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xueqi Cheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14619">
<title>Eliciting Honest Information From Authors Using Sequential Review. (arXiv:2311.14619v1 [cs.DL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.14619</link>
<description rdf:parseType="Literal">&lt;p&gt;In the setting of conference peer review, the conference aims to accept
high-quality papers and reject low-quality papers based on noisy review scores.
A recent work proposes the isotonic mechanism, which can elicit the ranking of
paper qualities from an author with multiple submissions to help improve the
conference&apos;s decisions. However, the isotonic mechanism relies on the
assumption that the author&apos;s utility is both an increasing and a convex
function with respect to the review score, which is often violated in peer
review settings (e.g.~when authors aim to maximize the number of accepted
papers). In this paper, we propose a sequential review mechanism that can
truthfully elicit the ranking information from authors while only assuming the
agent&apos;s utility is increasing with respect to the true quality of her accepted
papers. The key idea is to review the papers of an author in a sequence based
on the provided ranking and conditioning the review of the next paper on the
review scores of the previous papers. Advantages of the sequential review
mechanism include 1) eliciting truthful ranking information in a more realistic
setting than prior work; 2) improving the quality of accepted papers, reducing
the reviewing workload and increasing the average quality of papers being
reviewed; 3) incentivizing authors to write fewer papers of higher quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yichi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schoenebeck_G/0/1/0/all/0/1&quot;&gt;Grant Schoenebeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie Su&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>