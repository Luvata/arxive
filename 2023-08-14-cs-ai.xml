<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-08-13T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2005.13635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2012.01917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.11011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.09201" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.04531" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.12356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.02374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04875" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.06817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.11419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.16618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.02640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07500" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17271" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14701" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12267" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04172" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.05481" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2005.13635">
<title>Towards AI Forensics: Did the Artificial Intelligence System Do It?. (arXiv:2005.13635v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2005.13635</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) makes decisions impacting our daily lives in an
increasingly autonomous manner. Their actions might cause accidents, harm, or,
more generally, violate regulations. Determining whether an AI caused a
specific event and, if so, what triggered the AI&apos;s action, are key forensic
questions. We provide a conceptualization of the problems and strategies for
forensic investigation. We focus on AI that is potentially ``malicious by
design&apos;&apos; and grey box analysis. Our evaluation using convolutional neural
networks illustrates challenges and ideas for identifying malicious AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Johannes Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breitinger_F/0/1/0/all/0/1&quot;&gt;Frank Breitinger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2012.01917">
<title>Mapping Patterns for Virtual Knowledge Graphs. (arXiv:2012.01917v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2012.01917</link>
<description rdf:parseType="Literal">&lt;p&gt;Virtual Knowledge Graphs (VKG) constitute one of the most promising paradigms
for integrating and accessing legacy data sources. A critical bottleneck in the
integration process involves the definition, validation, and maintenance of
mappings that link data sources to a domain ontology. To support the management
of mappings throughout their entire lifecycle, we propose a comprehensive
catalog of sophisticated mapping patterns that emerge when linking databases to
ontologies. To do so, we build on well-established methodologies and patterns
studied in data management, data analysis, and conceptual modeling. These are
extended and refined through the analysis of concrete VKG benchmarks and
real-world use cases, and considering the inherent impedance mismatch between
data sources and ontologies. We validate our catalog on the considered VKG
scenarios, showing that it covers the vast majority of patterns present
therein.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calvanese_D/0/1/0/all/0/1&quot;&gt;Diego Calvanese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gal_A/0/1/0/all/0/1&quot;&gt;Avigdor Gal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanti_D/0/1/0/all/0/1&quot;&gt;Davide Lanti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1&quot;&gt;Marco Montali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mosca_A/0/1/0/all/0/1&quot;&gt;Alessandro Mosca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shraga_R/0/1/0/all/0/1&quot;&gt;Roee Shraga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03894">
<title>Neural Model Reprogramming with Similarity Based Mapping for Low-Resource Spoken Command Classification. (arXiv:2110.03894v4 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03894</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we propose a novel adversarial reprogramming (AR) approach for
low-resource spoken command recognition (SCR), and build an AR-SCR system. The
AR procedure aims to modify the acoustic signals (from the target domain) to
repurpose a pretrained SCR model (from the source domain). To solve the label
mismatches between source and target domains, and further improve the stability
of AR, we propose a novel similarity-based label mapping technique to align
classes. In addition, the transfer learning (TL) technique is combined with the
original AR process to improve the model adaptation capability. We evaluate the
proposed AR-SCR system on three low-resource SCR datasets, including Arabic,
Lithuanian, and dysarthric Mandarin speech. Experimental results show that with
a pretrained AM trained on a large-scale English dataset, the proposed AR-SCR
system outperforms the current state-of-the-art results on Arabic and
Lithuanian speech commands datasets, with only a limited amount of training
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yen_H/0/1/0/all/0/1&quot;&gt;Hao Yen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ku_P/0/1/0/all/0/1&quot;&gt;Pin-Jui Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chao-Han Huck Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siniscalchi_S/0/1/0/all/0/1&quot;&gt;Sabato Marco Siniscalchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu Tsao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.11011">
<title>CDistNet: Perceiving Multi-Domain Character Distance for Robust Text Recognition. (arXiv:2111.11011v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.11011</link>
<description rdf:parseType="Literal">&lt;p&gt;The Transformer-based encoder-decoder framework is becoming popular in scene
text recognition, largely because it naturally integrates recognition clues
from both visual and semantic domains. However, recent studies show that the
two kinds of clues are not always well registered and therefore, feature and
character might be misaligned in difficult text (e.g., with a rare shape). As a
result, constraints such as character position are introduced to alleviate this
problem. Despite certain success, visual and semantic are still separately
modeled and they are merely loosely associated. In this paper, we propose a
novel module called Multi-Domain Character Distance Perception (MDCDP) to
establish a visually and semantically related position embedding. MDCDP uses
the position embedding to query both visual and semantic features following the
cross-attention mechanism. The two kinds of clues are fused into the position
branch, generating a content-aware embedding that well perceives character
spacing and orientation variants, character semantic affinities, and clues
tying the two kinds of information. They are summarized as the multi-domain
character distance. We develop CDistNet that stacks multiple MDCDPs to guide a
gradually precise distance modeling. Thus, the feature-character alignment is
well built even various recognition difficulties are presented. We verify
CDistNet on ten challenging public datasets and two series of augmented
datasets created by ourselves. The experiments demonstrate that CDistNet
performs highly competitively. It not only ranks top-tier in standard
benchmarks, but also outperforms recent popular methods by obvious margins on
real and augmented datasets presenting severe text deformation, poor linguistic
support, and rare character layouts. Code is available at
https://github.com/simplify23/CDistNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianlun Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhineng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_S/0/1/0/all/0/1&quot;&gt;Shancheng Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Hongtao Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.09201">
<title>Vision-Based UAV Self-Positioning in Low-Altitude Urban Environments. (arXiv:2201.09201v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2201.09201</link>
<description rdf:parseType="Literal">&lt;p&gt;Unmanned Aerial Vehicles (UAVs) rely on satellite systems for stable
positioning. However, due to limited satellite coverage or communication
disruptions, UAVs may lose signals from satellite-based positioning systems. In
such situations, vision-based techniques can serve as an alternative, ensuring
the self-positioning capability of UAVs. However, most of the existing datasets
are developed for the geo-localization tasks of the objects identified by UAVs,
rather than the self-positioning task of UAVs. Furthermore, the current UAV
datasets use discrete sampling on synthetic data, such as Google Maps, thereby
neglecting the crucial aspects of dense sampling and the uncertainties commonly
experienced in real-world scenarios. To address these issues, this paper
presents a new dataset, DenseUAV, which is the first publicly available dataset
designed for the UAV self-positioning task. DenseUAV adopts dense sampling on
UAV images obtained in low-altitude urban settings. In total, over 27K UAV-view
and satellite-view images of 14 university campuses are collected and
annotated, establishing a new benchmark. In terms of model development, we
first verify the superiority of Transformers over CNNs in this task. Then, we
incorporate metric learning into representation learning to enhance the
discriminative capacity of the model and to lessen the modality discrepancy.
Besides, to facilitate joint learning from both perspectives, we propose a
mutually supervised learning approach. Last, we enhance the Recall@K metric and
introduce a new measurement, SDM@K, to evaluate the performance of a trained
model from both the retrieval and localization perspectives simultaneously. As
a result, the proposed baseline method achieves a remarkable Recall@1 score of
83.05% and an SDM@1 score of 86.24% on DenseUAV. The dataset and code will be
made publicly available on https://github.com/Dmmm1997/DenseUAV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_M/0/1/0/all/0/1&quot;&gt;Ming Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_E/0/1/0/all/0/1&quot;&gt;Enhui Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jiedong Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wankou Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.04531">
<title>ECLAD: Extracting Concepts with Local Aggregated Descriptors. (arXiv:2206.04531v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.04531</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks (CNNs) are increasingly being used in critical
systems, where robustness and alignment are crucial. In this context, the field
of explainable artificial intelligence has proposed the generation of
high-level explanations of the prediction process of CNNs through concept
extraction. While these methods can detect whether or not a concept is present
in an image, they are unable to determine its location. What is more, a fair
comparison of such approaches is difficult due to a lack of proper validation
procedures. To address these issues, we propose a novel method for automatic
concept extraction and localization based on representations obtained through
pixel-wise aggregations of CNN activation maps. Further, we introduce a process
for the validation of concept-extraction techniques based on synthetic datasets
with pixel-wise annotations of their main components, reducing the need for
human intervention. Extensive experimentation on both synthetic and real-world
datasets demonstrates that our method outperforms state-of-the-art
alternatives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posada_Moreno_A/0/1/0/all/0/1&quot;&gt;Andres Felipe Posada-Moreno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surya_N/0/1/0/all/0/1&quot;&gt;Nikita Surya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trimpe_S/0/1/0/all/0/1&quot;&gt;Sebastian Trimpe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06377">
<title>Relational Action Bases: Formalization, Effective Safety Verification, and Invariants (Extended Version). (arXiv:2208.06377v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06377</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling and verification of dynamic systems operating over a relational
representation of states are increasingly investigated problems in AI, Business
Process Management, and Database Theory. To make these systems amenable to
verification, the amount of information stored in each relational state needs
to be bounded, or restrictions are imposed on the preconditions and effects of
actions. We introduce the general framework of relational action bases (RABs),
which generalizes existing models by lifting both these restrictions: unbounded
relational states can be evolved through actions that can quantify both
existentially and universally over the data, and that can exploit numerical
datatypes with arithmetic predicates. We then study parameterized safety of
RABs via (approximated) SMT-based backward search, singling out essential
meta-properties of the resulting procedure, and showing how it can be realized
by an off-the-shelf combination of existing verification modules of the
state-of-the-art MCMT model checker. We demonstrate the effectiveness of this
approach on a benchmark of data-aware business processes. Finally, we show how
universal invariants can be exploited to make this procedure fully correct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghilardi_S/0/1/0/all/0/1&quot;&gt;Silvio Ghilardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gianola_A/0/1/0/all/0/1&quot;&gt;Alessandro Gianola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1&quot;&gt;Marco Montali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivkin_A/0/1/0/all/0/1&quot;&gt;Andrey Rivkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.12356">
<title>Lib-SibGMU -- A University Library Circulation Dataset for Recommender Systems Developmen. (arXiv:2208.12356v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2208.12356</link>
<description rdf:parseType="Literal">&lt;p&gt;We opensource under CC BY 4.0 license Lib-SibGMU - a university library
circulation dataset - for a wide research community, and benchmark major
algorithms for recommender systems on this dataset. For a recommender
architecture that consists of a vectorizer that turns the history of the books
borrowed into a vector, and a neighborhood-based recommender, trained
separately, we show that using the fastText model as a vectorizer delivers
competitive results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zubchuk_E/0/1/0/all/0/1&quot;&gt;Eduard Zubchuk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arhipkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Arhipkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menshikov_D/0/1/0/all/0/1&quot;&gt;Dmitry Menshikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karaush_A/0/1/0/all/0/1&quot;&gt;Aleksandr Karaush&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikhaylovskiy_N/0/1/0/all/0/1&quot;&gt;Nikolay Mikhaylovskiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17020">
<title>A Law of Data Separation in Deep Learning. (arXiv:2210.17020v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17020</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning has enabled significant advances in many areas of
science, its black-box nature hinders architecture design for future artificial
intelligence applications and interpretation for high-stakes decision makings.
We addressed this issue by studying the fundamental question of how deep neural
networks process data in the intermediate layers. Our finding is a simple and
quantitative law that governs how deep neural networks separate data according
to class membership throughout all layers for classification. This law shows
that each layer improves data separation at a constant geometric rate, and its
emergence is observed in a collection of network architectures and datasets
during training. This law offers practical guidelines for designing
architectures, improving model robustness and out-of-sample performance, as
well as interpreting the predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hangfeng He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weijie J. Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17040">
<title>CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17040</link>
<description rdf:parseType="Literal">&lt;p&gt;Developers often perform repetitive code editing activities for various
reasons (e.g., code refactoring) during software development. Pre-trained code
editing models have achieved the state-of-the-art (SOTA) results. Pre-trained
models are first pre-trained with pre-training tasks and fine-tuned with the
code editing task. Existing pre-training tasks mainly are code infilling tasks
(e.g., masked language modeling), which are derived from the natural language
processing field and are not designed for automatic code editing.
&lt;/p&gt;
&lt;p&gt;This paper proposes a novel pre-training task specialized in code editing and
presents an effective pre-trained code editing model named CodeEditor. Our
pre-training task further improves the performance and generalization ability
of code editing models. Specifically, we collect lots of real-world code
snippets as the ground truth and use a powerful generator to rewrite them into
mutated versions. Then, we pre-train our CodeEditor to edit mutated versions
into the corresponding ground truth, to learn edit patterns. We conduct
experiments on four code editing datasets and evaluate the pre-trained
CodeEditor in three settings. (1) In the fine-tuning setting, we train the
pre-trained CodeEditor with four datasets and evaluate it on the test data.
CodeEditor outperforms the SOTA baselines by 15%, 25.5%, and 9.4% and 26.6% on
four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor
with limited data and evaluate it on the test data. CodeEditor substantially
performs better than all baselines. (3) In the zero-shot setting, CodeEditor
correctly edits 1,113 programs while the SOTA baselines can not work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Allen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kechi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17316">
<title>There is more than one kind of robustness: Fooling Whisper with adversarial examples. (arXiv:2210.17316v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17316</link>
<description rdf:parseType="Literal">&lt;p&gt;Whisper is a recent Automatic Speech Recognition (ASR) model displaying
impressive robustness to both out-of-distribution inputs and random noise. In
this work, we show that this robustness does not carry over to adversarial
noise. We show that we can degrade Whisper performance dramatically, or even
transcribe a target sentence of our choice, by generating very small input
perturbations with Signal Noise Ratio of 35-45dB. We also show that by fooling
the Whisper language detector we can very easily degrade the performance of
multilingual models. These vulnerabilities of a widely popular open-source
model have practical security implications and emphasize the need for
adversarially robust ASR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Olivier_R/0/1/0/all/0/1&quot;&gt;Raphael Olivier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00732">
<title>Kuaipedia: a Large-scale Multi-modal Short-video Encyclopedia. (arXiv:2211.00732v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00732</link>
<description rdf:parseType="Literal">&lt;p&gt;Online encyclopedias, such as Wikipedia, have been well-developed and
researched in the last two decades. One can find any attributes or other
information of a wiki item on a wiki page edited by a community of volunteers.
However, the traditional text, images and tables can hardly express some
aspects of an wiki item. For example, when we talk about ``Shiba Inu&apos;&apos;, one may
care more about ``How to feed it&apos;&apos; or ``How to train it not to protect its
food&apos;&apos;. Currently, short-video platforms have become a hallmark in the online
world. Whether you&apos;re on TikTok, Instagram, Kuaishou, or YouTube Shorts,
short-video apps have changed how we consume and create content today. Except
for producing short videos for entertainment, we can find more and more authors
sharing insightful knowledge widely across all walks of life. These short
videos, which we call knowledge videos, can easily express any aspects (e.g.
hair or how-to-feed) consumers want to know about an item (e.g. Shiba Inu), and
they can be systematically analyzed and organized like an online encyclopedia.
In this paper, we propose Kuaipedia, a large-scale multi-modal encyclopedia
consisting of items, aspects, and short videos lined to them, which was
extracted from billions of videos of Kuaishou (Kwai), a well-known short-video
platform in China. We first collected items from multiple sources and mined
user-centered aspects from millions of users&apos; queries to build an item-aspect
tree. Then we propose a new task called ``multi-modal item-aspect linking&apos;&apos; as
an expansion of ``entity linking&apos;&apos; to link short videos into item-aspect pairs
and build the whole short-video encyclopedia. Intrinsic evaluations show that
our encyclopedia is of large scale and highly accurate. We also conduct
sufficient extrinsic experiments to show how Kuaipedia can help fundamental
applications such as entity typing and entity linking.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Haojie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1&quot;&gt;Zepeng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuzhou Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruiji Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yangqiu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.02374">
<title>On the Trade-off between Over-smoothing and Over-squashing in Deep Graph Neural Networks. (arXiv:2212.02374v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.02374</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have succeeded in various computer science
applications, yet deep GNNs underperform their shallow counterparts despite
deep learning&apos;s success in other domains. Over-smoothing and over-squashing are
key challenges when stacking graph convolutional layers, hindering deep
representation learning and information propagation from distant nodes. Our
work reveals that over-smoothing and over-squashing are intrinsically related
to the spectral gap of the graph Laplacian, resulting in an inevitable
trade-off between these two issues, as they cannot be alleviated
simultaneously. To achieve a suitable compromise, we propose adding and
removing edges as a viable approach. We introduce the Stochastic Jost and Liu
Curvature Rewiring (SJLR) algorithm, which is computationally efficient and
preserves fundamental properties compared to previous curvature-based methods.
Unlike existing approaches, SJLR performs edge addition and removal during GNN
training while maintaining the graph unchanged during testing. Comprehensive
comparisons demonstrate SJLR&apos;s competitive performance in addressing
over-smoothing and over-squashing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giraldo_J/0/1/0/all/0/1&quot;&gt;Jhony H. Giraldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skianis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Skianis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouwmans_T/0/1/0/all/0/1&quot;&gt;Thierry Bouwmans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malliaros_F/0/1/0/all/0/1&quot;&gt;Fragkiskos D. Malliaros&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04875">
<title>Expeditious Saliency-guided Mix-up through Random Gradient Thresholding. (arXiv:2212.04875v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04875</link>
<description rdf:parseType="Literal">&lt;p&gt;Mix-up training approaches have proven to be effective in improving the
generalization ability of Deep Neural Networks. Over the years, the research
community expands mix-up methods into two directions, with extensive efforts to
improve saliency-guided procedures but minimal focus on the arbitrary path,
leaving the randomization domain unexplored. In this paper, inspired by the
superior qualities of each direction over one another, we introduce a novel
method that lies at the junction of the two routes. By combining the best
elements of randomness and saliency utilization, our method balances speed,
simplicity, and accuracy. We name our method R-Mix following the concept of
&quot;Random Mix-up&quot;. We demonstrate its effectiveness in generalization, weakly
supervised object localization, calibration, and robustness to adversarial
attacks. Finally, in order to address the question of whether there exists a
better decision protocol, we train a Reinforcement Learning agent that decides
the mix-up policies based on the classifier&apos;s performance, reducing dependency
on human-designed objectives and hyperparameter tuning. Extensive experiments
further show that the agent is capable of performing at the cutting-edge level,
laying the foundation for a fully automatic mix-up. Our code is released at
[https://github.com/minhlong94/Random-Mixup].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luu_M/0/1/0/all/0/1&quot;&gt;Minh-Long Luu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zeyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric P. Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haohan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.06817">
<title>RT-1: Robotics Transformer for Real-World Control at Scale. (arXiv:2212.06817v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2212.06817</link>
<description rdf:parseType="Literal">&lt;p&gt;By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project&apos;s website and videos can be found at
robotics-transformer1.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brohan_A/0/1/0/all/0/1&quot;&gt;Anthony Brohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_N/0/1/0/all/0/1&quot;&gt;Noah Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbajal_J/0/1/0/all/0/1&quot;&gt;Justice Carbajal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chebotar_Y/0/1/0/all/0/1&quot;&gt;Yevgen Chebotar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dabis_J/0/1/0/all/0/1&quot;&gt;Joseph Dabis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzog_A/0/1/0/all/0/1&quot;&gt;Alex Herzog&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_J/0/1/0/all/0/1&quot;&gt;Jasmine Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibarz_J/0/1/0/all/0/1&quot;&gt;Julian Ibarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1&quot;&gt;Alex Irpan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jackson_T/0/1/0/all/0/1&quot;&gt;Tomas Jackson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jesmonth_S/0/1/0/all/0/1&quot;&gt;Sally Jesmonth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nikhil J Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1&quot;&gt;Ryan Julian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalashnikov_D/0/1/0/all/0/1&quot;&gt;Dmitry Kalashnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_Y/0/1/0/all/0/1&quot;&gt;Yuheng Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1&quot;&gt;Isabel Leal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kuang-Huei Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malla_U/0/1/0/all/0/1&quot;&gt;Utsav Malla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manjunath_D/0/1/0/all/0/1&quot;&gt;Deeksha Manjunath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mordatch_I/0/1/0/all/0/1&quot;&gt;Igor Mordatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nachum_O/0/1/0/all/0/1&quot;&gt;Ofir Nachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parada_C/0/1/0/all/0/1&quot;&gt;Carolina Parada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peralta_J/0/1/0/all/0/1&quot;&gt;Jodilyn Peralta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Emily Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1&quot;&gt;Karl Pertsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quiambao_J/0/1/0/all/0/1&quot;&gt;Jornell Quiambao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael Ryoo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salazar_G/0/1/0/all/0/1&quot;&gt;Grecia Salazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1&quot;&gt;Pannag Sanketi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sayed_K/0/1/0/all/0/1&quot;&gt;Kevin Sayed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jaspiar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontakke_S/0/1/0/all/0/1&quot;&gt;Sumedh Sontakke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_A/0/1/0/all/0/1&quot;&gt;Austin Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Clayton Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_H/0/1/0/all/0/1&quot;&gt;Huong Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanhoucke_V/0/1/0/all/0/1&quot;&gt;Vincent Vanhoucke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vega_S/0/1/0/all/0/1&quot;&gt;Steve Vega&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Sichun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianhe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitkovich_B/0/1/0/all/0/1&quot;&gt;Brianna Zitkovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07226">
<title>An Efficient Incremental Simple Temporal Network Data Structure for Temporal Planning. (arXiv:2212.07226v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07226</link>
<description rdf:parseType="Literal">&lt;p&gt;One popular technique to solve temporal planning problems consists in
decoupling the causal decisions, demanding them to heuristic search, from
temporal decisions, demanding them to a simple temporal network (STN) solver.
In this architecture, one needs to check the consistency of a series of STNs
that are related one another, therefore having methods to incrementally re-use
previous computations and that avoid expensive memory duplication is of
paramount importance. In this paper, we describe in detail how STNs are used in
temporal planning, we identify a clear interface to support this use-case and
we present an efficient data-structure implementing this interface that is both
time- and memory-efficient. We show that our data structure, called \deltastn,
is superior to other state-of-the-art approaches on temporal planning sequences
of problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1&quot;&gt;Andrea Micheli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.11419">
<title>Imitation Is Not Enough: Robustifying Imitation with Reinforcement Learning for Challenging Driving Scenarios. (arXiv:2212.11419v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.11419</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning (IL) is a simple and powerful way to use high-quality
human driving data, which can be collected at scale, to produce human-like
behavior. However, policies based on imitation learning alone often fail to
sufficiently account for safety and reliability concerns. In this paper, we
show how imitation learning combined with reinforcement learning using simple
rewards can substantially improve the safety and reliability of driving
policies over those learned from imitation alone. In particular, we train a
policy on over 100k miles of urban driving data, and measure its effectiveness
in test scenarios grouped by different levels of collision likelihood. Our
analysis shows that while imitation can perform well in low-difficulty
scenarios that are well-covered by the demonstration data, our proposed
approach significantly improves robustness on the most challenging scenarios
(over 38% reduction in failures). To our knowledge, this is the first
application of a combined imitation and reinforcement learning approach in
autonomous driving that utilizes large amounts of real-world human driving
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yiren Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Justin Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tucker_G/0/1/0/all/0/1&quot;&gt;George Tucker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xinlei Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bronstein_E/0/1/0/all/0/1&quot;&gt;Eli Bronstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roelofs_R/0/1/0/all/0/1&quot;&gt;Rebecca Roelofs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sapp_B/0/1/0/all/0/1&quot;&gt;Benjamin Sapp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_B/0/1/0/all/0/1&quot;&gt;Brandyn White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1&quot;&gt;Aleksandra Faust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whiteson_S/0/1/0/all/0/1&quot;&gt;Shimon Whiteson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anguelov_D/0/1/0/all/0/1&quot;&gt;Dragomir Anguelov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11189">
<title>Improving Statistical Fidelity for Neural Image Compression with Implicit Local Likelihood Models. (arXiv:2301.11189v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11189</link>
<description rdf:parseType="Literal">&lt;p&gt;Lossy image compression aims to represent images in as few bits as possible
while maintaining fidelity to the original. Theoretical results indicate that
optimizing distortion metrics such as PSNR or MS-SSIM necessarily leads to a
discrepancy in the statistics of original images from those of reconstructions,
in particular at low bitrates, often manifested by the blurring of the
compressed images. Previous work has leveraged adversarial discriminators to
improve statistical fidelity. Yet these binary discriminators adopted from
generative modeling tasks may not be ideal for image compression. In this
paper, we introduce a non-binary discriminator that is conditioned on quantized
local image representations obtained via VQ-VAE autoencoders. Our evaluations
on the CLIC2020, DIV2K and Kodak datasets show that our discriminator is more
effective for jointly optimizing distortion (e.g., PSNR) and statistical
fidelity (e.g., FID) than the PatchGAN of the state-of-the-art HiFiC model. On
CLIC2020, we obtain the same FID as HiFiC with 30-40\% fewer bits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muckley_M/0/1/0/all/0/1&quot;&gt;Matthew J. Muckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+El_Nouby_A/0/1/0/all/0/1&quot;&gt;Alaaeldin El-Nouby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ullrich_K/0/1/0/all/0/1&quot;&gt;Karen Ullrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jegou_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; J&amp;#xe9;gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14057">
<title>Cross-modal Contrastive Learning for Multimodal Fake News Detection. (arXiv:2302.14057v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14057</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatic detection of multimodal fake news has gained a widespread attention
recently. Many existing approaches seek to fuse unimodal features to produce
multimodal news representations. However, the potential of powerful cross-modal
contrastive learning methods for fake news detection has not been well
exploited. Besides, how to aggregate features from different modalities to
boost the performance of the decision-making process is still an open question.
To address that, we propose COOLANT, a cross-modal contrastive learning
framework for multimodal fake news detection, aiming to achieve more accurate
image-text alignment. To further improve the alignment precision, we leverage
an auxiliary task to soften the loss term of negative samples during the
contrast process. A cross-modal fusion module is developed to learn the
cross-modality correlations. An attention mechanism with an attention guidance
module is implemented to help effectively and interpretably aggregate the
aligned unimodal representations and the cross-modality correlations. Finally,
we evaluate the COOLANT and conduct a comparative study on two widely used
datasets, Twitter and Weibo. The experimental results demonstrate that our
COOLANT outperforms previous approaches by a large margin and achieves new
state-of-the-art results on the two datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longzheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hongbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongxiu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaohan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02266">
<title>Collaborative Learning with a Drone Orchestrator. (arXiv:2303.02266v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02266</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, the problem of drone-assisted collaborative learning is
considered. In this scenario, swarm of intelligent wireless devices train a
shared neural network (NN) model with the help of a drone. Using its sensors,
each device records samples from its environment to gather a local dataset for
training. The training data is severely heterogeneous as various devices have
different amount of data and sensor noise level. The intelligent devices
iteratively train the NN on their local datasets and exchange the model
parameters with the drone for aggregation. For this system, the convergence
rate of collaborative learning is derived while considering data heterogeneity,
sensor noise levels, and communication errors, then, the drone trajectory that
maximizes the final accuracy of the trained NN is obtained. The proposed
trajectory optimization approach is aware of both the devices data
characteristics (i.e., local dataset size and noise level) and their wireless
channel conditions, and significantly improves the convergence rate and final
accuracy in comparison with baselines that only consider data characteristics
or channel conditions. Compared to state-of-the-art baselines, the proposed
approach achieves an average 3.85% and 3.54% improvement in the final accuracy
of the trained NN on benchmark datasets for image recognition and semantic
segmentation tasks, respectively. Moreover, the proposed framework achieves a
significant speedup in training, leading to an average 24% and 87% saving in
the drone hovering time, communication overhead, and battery usage,
respectively for these tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mashhadi_M/0/1/0/all/0/1&quot;&gt;Mahdi Boloursaz Mashhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavimoghadam_M/0/1/0/all/0/1&quot;&gt;Mahnoosh Mahdavimoghadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tafazolli_R/0/1/0/all/0/1&quot;&gt;Rahim Tafazolli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11219">
<title>NeTO:Neural Reconstruction of Transparent Objects with Self-Occlusion Aware Refraction-Tracing. (arXiv:2303.11219v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11219</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method, called NeTO, for capturing 3D geometry of solid
transparent objects from 2D images via volume rendering. Reconstructing
transparent objects is a very challenging task, which is ill-suited for
general-purpose reconstruction techniques due to the specular light transport
phenomena. Although existing refraction-tracing based methods, designed
specially for this task, achieve impressive results, they still suffer from
unstable optimization and loss of fine details, since the explicit surface
representation they adopted is difficult to be optimized, and the
self-occlusion problem is ignored for refraction-tracing. In this paper, we
propose to leverage implicit Signed Distance Function (SDF) as surface
representation, and optimize the SDF field via volume rendering with a
self-occlusion aware refractive ray tracing. The implicit representation
enables our method to be capable of reconstructing high-quality reconstruction
even with a limited set of images, and the self-occlusion aware strategy makes
it possible for our method to accurately reconstruct the self-occluded regions.
Experiments show that our method achieves faithful reconstruction results and
outperforms prior works by a large margin. Visit our project page at
\url{https://www.xxlong.site/NeTO/}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yusen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_T/0/1/0/all/0/1&quot;&gt;Tuo Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_F/0/1/0/all/0/1&quot;&gt;Fei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chunxia Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12364">
<title>ExBEHRT: Extended Transformer for Electronic Health Records to Predict Disease Subtypes &amp; Progressions. (arXiv:2303.12364v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12364</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we introduce ExBEHRT, an extended version of BEHRT (BERT
applied to electronic health records), and apply different algorithms to
interpret its results. While BEHRT considers only diagnoses and patient age, we
extend the feature space to several multimodal records, namely demographics,
clinical characteristics, vital signs, smoking status, diagnoses, procedures,
medications, and laboratory tests, by applying a novel method to unify the
frequencies and temporal dimensions of the different features. We show that
additional features significantly improve model performance for various
downstream tasks in different diseases. To ensure robustness, we interpret
model predictions using an adaptation of expected gradients, which has not been
previously applied to transformers with EHR data and provides more granular
interpretations than previous approaches such as feature and token importances.
Furthermore, by clustering the model representations of oncology patients, we
show that the model has an implicit understanding of the disease and is able to
classify patients with the same cancer type into different risk groups. Given
the additional features and interpretability, ExBEHRT can help make informed
decisions about disease trajectories, diagnoses, and risk factors of various
diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rupp_M/0/1/0/all/0/1&quot;&gt;Maurice Rupp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peter_O/0/1/0/all/0/1&quot;&gt;Oriane Peter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pattipaka_T/0/1/0/all/0/1&quot;&gt;Thirupathi Pattipaka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.16618">
<title>Personalised Language Modelling of Screen Characters Using Rich Metadata Annotations. (arXiv:2303.16618v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.16618</link>
<description rdf:parseType="Literal">&lt;p&gt;Language models that are sensitive to external context can more effectively
capture the speaking patterns of individuals with specific characteristics or
in particular environments. However, obtaining and leveraging such annotations
can be challenging. In this work, we show how to leverage rich character and
film annotations to personalise language models in a scalable manner. Our best
model can reduce perplexity by up to 6.5% compared to a parameter-matched
language model. Our approach performs on par with speaker-specific fine-tuning
when the fine-tuning data (i.e. past dialogue) for individual speakers is
available. On top of that, it also generalises well to a scenario with no such
data, relying on combinations of demographic characteristics expressed via
metadata. Our findings are consistent across two corpora, one of which is also
a contribution of this paper: Cornell-rich contains rich manual annotations for
863 speaking characters from the Cornell Movie Dialog Corpus, including
features such as characteristic quotes and character descriptions, along with
six automatically extracted metadata features for over 95% of the featured
films. Finally, we also present a cost-benefit analysis highlighting which
annotations are most cost-effective in reducing perplexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vincent_S/0/1/0/all/0/1&quot;&gt;Sebastian Vincent&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sumner_R/0/1/0/all/0/1&quot;&gt;Rowanne Sumner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dowek_A/0/1/0/all/0/1&quot;&gt;Alice Dowek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charlotte Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Preston_E/0/1/0/all/0/1&quot;&gt;Emily Preston&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayliss_C/0/1/0/all/0/1&quot;&gt;Chris Bayliss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oakley_C/0/1/0/all/0/1&quot;&gt;Chris Oakley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scarton_C/0/1/0/all/0/1&quot;&gt;Carolina Scarton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17780">
<title>AceCoder: Utilizing Existing Code to Enhance Code Generation. (arXiv:2303.17780v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17780</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown great success in code generation.
LLMs take as the input a prompt and output the code. A key question is how to
make prompts (i.e., Prompting Techniques). Existing prompting techniques are
designed for natural language generation and have low accuracy in code
generation.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a new prompting technique named AceCoder. Our
motivation is that code generation meets two unique challenges (i.e.,
requirement understanding and code implementation). AceCoder contains two novel
mechanisms (i.e., guided code generation and example retrieval) to solve these
challenges. (1) Guided code generation asks LLMs first to analyze requirements
and output an intermediate preliminary (e.g., test cases). The preliminary is
used to clarify requirements and tell LLMs &quot;what to write&quot;. (2) Example
retrieval selects similar programs as examples in prompts, which provide lots
of relevant content (e.g., algorithms, APIs) and teach LLMs &quot;how to write&quot;. We
apply AceCoder to three LLMs (e.g., Codex) and evaluate it on three public
benchmarks using the Pass@k. Results show that AceCoder can significantly
improve the performance of LLMs on code generation. (1) In terms of Pass@1,
AceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,
70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs with
different sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,
and JavaScript). (3) Human evaluation shows human developers prefer programs
from AceCoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Allen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunfei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongmin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.02640">
<title>Towards Causal Representation Learning and Deconfounding from Indefinite Data. (arXiv:2305.02640v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.02640</link>
<description rdf:parseType="Literal">&lt;p&gt;Owing to the cross-pollination between causal discovery and deep learning,
non-statistical data (e.g., images, text, etc.) encounters significant
conflicts in terms of properties and methods with traditional causal data. To
unify these data types of varying forms, we redefine causal data from two novel
perspectives and then propose three data paradigms. Among them, the indefinite
data (like dialogues or video sources) induce low sample utilization and
incapability of the distribution assumption, both leading to the fact that
learning causal representation from indefinite data is, as of yet, largely
unexplored. We design the causal strength variational model to settle down
these two problems. Specifically, we leverage the causal strength instead of
independent noise as the latent variable to construct evidence lower bound. By
this design ethos, The causal strengths of different structures are regarded as
a distribution and can be expressed as a 2D matrix. Moreover, considering the
latent confounders, we disentangle the causal graph G into two relation
subgraphs O and C. O contains pure relations between observed variables, while
C represents the relations from latent variables to observed variables. We
implement the above designs as a dynamic variational inference model, tailored
to learn causal representation from indefinite data under latent confounding.
Finally, we conduct comprehensive experiments on synthetic and real-world data
to demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xinyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07500">
<title>Learning representations that are closed-form Monge mapping optimal with application to domain adaptation. (arXiv:2305.07500v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07500</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal transport (OT) is a powerful geometric tool used to compare and align
probability measures following the least effort principle. Despite its
widespread use in machine learning (ML), OT problem still bears its
computational burden, while at the same time suffering from the curse of
dimensionality for measures supported on general high-dimensional spaces. In
this paper, we propose to tackle these challenges using representation
learning. In particular, we seek to learn an embedding space such that the
samples of the two input measures become alignable in it with a simple affine
mapping that can be calculated efficiently in closed-form. We then show that
such approach leads to results that are comparable to solving the original OT
problem when applied to the transfer learning task on which many OT baselines
where previously evaluated in both homogeneous and heterogeneous DA settings.
The code for our contribution is available at
\url{https://github.com/Oleffa/LaOT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struckmeier_O/0/1/0/all/0/1&quot;&gt;Oliver Struckmeier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redko_I/0/1/0/all/0/1&quot;&gt;Ievgen Redko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallasto_A/0/1/0/all/0/1&quot;&gt;Anton Mallasto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arndt_K/0/1/0/all/0/1&quot;&gt;Karol Arndt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinonen_M/0/1/0/all/0/1&quot;&gt;Markus Heinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrki_V/0/1/0/all/0/1&quot;&gt;Ville Kyrki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17271">
<title>Robust Lane Detection through Self Pre-training with Masked Sequential Autoencoders and Fine-tuning with Customized PolyLoss. (arXiv:2305.17271v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17271</link>
<description rdf:parseType="Literal">&lt;p&gt;Lane detection is crucial for vehicle localization which makes it the
foundation for automated driving and many intelligent and advanced driving
assistant systems. Available vision-based lane detection methods do not make
full use of the valuable features and aggregate contextual information,
especially the interrelationships between lane lines and other regions of the
images in continuous frames. To fill this research gap and upgrade lane
detection performance, this paper proposes a pipeline consisting of self
pre-training with masked sequential autoencoders and fine-tuning with
customized PolyLoss for the end-to-end neural network models using
multi-continuous image frames. The masked sequential autoencoders are adopted
to pre-train the neural network models with reconstructing the missing pixels
from a random masked image as the objective. Then, in the fine-tuning
segmentation phase where lane detection segmentation is performed, the
continuous image frames are served as the inputs, and the pre-trained model
weights are transferred and further updated using the backpropagation mechanism
with customized PolyLoss calculating the weighted errors between the output
lane detection results and the labeled ground truth. Extensive experiment
results demonstrate that, with the proposed pipeline, the lane detection model
performance on both normal and challenging scenes can be advanced beyond the
state-of-the-art, delivering the best testing accuracy (98.38%), precision
(0.937), and F1-measure (0.924) on the normal scene testing set, together with
the best overall accuracy (98.36%) and precision (0.844) in the challenging
scene test set, while the training time can be substantially shortened.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Ruohan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yongqi Dong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04542">
<title>On the Design Fundamentals of Diffusion Models: A Survey. (arXiv:2306.04542v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04542</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models are generative models, which gradually add and remove noise
to learn the underlying distribution of training data for data generation. The
components of diffusion models have gained significant attention with many
design choices proposed. Existing reviews have primarily focused on
higher-level solutions, thereby covering less on the design fundamentals of
components. This study seeks to address this gap by providing a comprehensive
and coherent review on component-wise design choices in diffusion models.
Specifically, we organize this review according to their three key components,
namely the forward process, the reverse process, and the sampling procedure.
This allows us to provide a fine-grained perspective of diffusion models,
benefiting future studies in the analysis of individual components, the
applicability of design choices, and the implementation of diffusion models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koulieris_G/0/1/0/all/0/1&quot;&gt;George Alex Koulieris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Hubert P. H. Shum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09927">
<title>Trained Transformers Learn Linear Models In-Context. (arXiv:2306.09927v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09927</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention-based neural networks such as transformers have demonstrated a
remarkable ability to exhibit in-context learning (ICL): Given a short prompt
sequence of tokens from an unseen task, they can formulate relevant per-token
and next-token predictions without any parameter updates. By embedding a
sequence of labeled training data and unlabeled test data as a prompt, this
allows for transformers to behave like supervised learning algorithms. Indeed,
recent work has shown that when training transformer architectures over random
instances of linear regression problems, these models&apos; predictions mimic those
of ordinary least squares.
&lt;/p&gt;
&lt;p&gt;Towards understanding the mechanisms underlying this phenomenon, we
investigate the dynamics of ICL in transformers with a single linear
self-attention layer trained by gradient flow on linear regression tasks. We
show that despite non-convexity, gradient flow with a suitable random
initialization finds a global minimum of the objective function. At this global
minimum, when given a test prompt of labeled examples from a new prediction
task, the transformer achieves prediction error competitive with the best
linear predictor over the test prompt distribution. We additionally
characterize the robustness of the trained transformer to a variety of
distribution shifts and show that although a number of shifts are tolerated,
shifts in the covariate distribution of the prompts are not. Motivated by this,
we consider a generalized ICL setting where the covariate distributions can
vary across prompts. We show that although gradient flow succeeds at finding a
global minimum in this setting, the trained transformer is still brittle under
mild covariate shifts. We complement this finding with experiments on large,
nonlinear transformer architectures which we show are more robust under
covariate shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Frei_S/0/1/0/all/0/1&quot;&gt;Spencer Frei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bartlett_P/0/1/0/all/0/1&quot;&gt;Peter L. Bartlett&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10645">
<title>Developing Effective Educational Chatbots with ChatGPT prompts: Insights from Preliminary Tests in a Case Study on Social Media Literacy (with appendix). (arXiv:2306.10645v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10645</link>
<description rdf:parseType="Literal">&lt;p&gt;Educational chatbots come with a promise of interactive and personalized
learning experiences, yet their development has been limited by the restricted
free interaction capabilities of available platforms and the difficulty of
encoding knowledge in a suitable format. Recent advances in language learning
models with zero-shot learning capabilities, such as ChatGPT, suggest a new
possibility for developing educational chatbots using a prompt-based approach.
We present a case study with a simple system that enables mixed-turn chatbot
interactions and discuss the insights and preliminary guidelines obtained from
initial tests. We examine ChatGPT&apos;s ability to pursue multiple interconnected
learning objectives, adapt the educational activity to users&apos; characteristics,
such as culture, age, and level of education, and its ability to use diverse
educational strategies and conversational styles. Although the results are
encouraging, challenges are posed by the limited history maintained for the
conversation and the highly structured form of responses by ChatGPT, as well as
their variability, which can lead to an unexpected switch of the chatbot&apos;s role
from a teacher to a therapist. We provide some initial guidelines to address
these issues and to facilitate the development of effective educational
chatbots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyuturk_C/0/1/0/all/0/1&quot;&gt;Cansu Koyuturk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavari_M/0/1/0/all/0/1&quot;&gt;Mona Yavari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theophilou_E/0/1/0/all/0/1&quot;&gt;Emily Theophilou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bursic_S/0/1/0/all/0/1&quot;&gt;Sathya Bursic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donabauer_G/0/1/0/all/0/1&quot;&gt;Gregor Donabauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telari_A/0/1/0/all/0/1&quot;&gt;Alessia Telari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Testa_A/0/1/0/all/0/1&quot;&gt;Alessia Testa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boiano_R/0/1/0/all/0/1&quot;&gt;Raffaele Boiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbiadini_A/0/1/0/all/0/1&quot;&gt;Alessandro Gabbiadini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Leo_D/0/1/0/all/0/1&quot;&gt;Davinia Hernandez-Leo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruskov_M/0/1/0/all/0/1&quot;&gt;Martin Ruskov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ognibene_D/0/1/0/all/0/1&quot;&gt;Dimitri Ognibene&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14701">
<title>Hard Sample Mining Enabled Supervised Contrastive Feature Learning for Wind Turbine Pitch System Fault Diagnosis. (arXiv:2306.14701v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14701</link>
<description rdf:parseType="Literal">&lt;p&gt;The efficient utilization of wind power by wind turbines relies on the
ability of their pitch systems to adjust blade pitch angles in response to
varying wind speeds. However, the presence of multiple health conditions in the
pitch system due to the long-term wear and tear poses challenges in accurately
classifying them, thus increasing the maintenance cost of wind turbines or even
damaging them. This paper proposes a novel method based on hard sample
mining-enabled supervised contrastive learning (HSMSCL) to address this
problem. The proposed method employs cosine similarity to identify hard samples
and subsequently, leverages supervised contrastive learning to learn more
discriminative representations by constructing hard sample pairs. Furthermore,
the hard sample mining framework in the proposed method also constructs hard
samples with learned representations to make the training process of the
multilayer perceptron (MLP) more challenging and make it a more effective
classifier. The proposed approach progressively improves the fault diagnosis
model by introducing hard samples in the SCL and MLP phases, thus enhancing its
performance in complex multi-class fault diagnosis tasks.
&lt;/p&gt;
&lt;p&gt;To evaluate the effectiveness of the proposed method, two real datasets
comprising wind turbine pitch system cog belt fracture data are utilized. The
fault diagnosis performance of the proposed method is compared against existing
methods, and the results demonstrate its superior performance. The proposed
approach exhibits significant improvements in fault diagnosis performance,
providing promising prospects for enhancing the reliability and efficiency of
wind turbine pitch system fault diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bo Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_C/0/1/0/all/0/1&quot;&gt;Chenlu Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butala_M/0/1/0/all/0/1&quot;&gt;Mark D. Butala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_P/0/1/0/all/0/1&quot;&gt;Peng Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17609">
<title>Mixed Integer Programming for Time-Optimal Multi-Robot Coverage Path Planning with Efficient Heuristics. (arXiv:2306.17609v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17609</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate time-optimal Multi-Robot Coverage Path Planning (MCPP) for
both unweighted and weighted terrains, which aims to minimize the coverage
time, defined as the maximum travel time of all robots. Specifically, we focus
on a reduction from MCPP to Min-Max Rooted Tree Cover (MMRTC). For the first
time, we propose a Mixed Integer Programming (MIP) model to optimally solve
MMRTC, resulting in an MCPP solution with a coverage time that is provably at
most four times the optimal. Moreover, we propose two suboptimal yet effective
heuristics that reduce the number of variables in the MIP model, thus improving
its efficiency for large-scale MCPP instances. We show that both heuristics
result in reduced-size MIP models that remain complete (i.e., guaranteed to
find a solution if one exists) for all MMRTC instances. Additionally, we
explore the use of model optimization warm-startup to further improve the
efficiency of both the original MIP model and the reduced-size MIP models. We
validate the effectiveness of our MIP-based MCPP planner through experiments
that compare it with two state-of-the-art MCPP planners on various instances,
demonstrating a reduction in the coverage time by an average of 27.65% and
23.24% over them, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jingtao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00464">
<title>Human-to-Human Interaction Detection. (arXiv:2307.00464v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00464</link>
<description rdf:parseType="Literal">&lt;p&gt;A comprehensive understanding of interested human-to-human interactions in
video streams, such as queuing, handshaking, fighting and chasing, is of
immense importance to the surveillance of public security in regions like
campuses, squares and parks. Different from conventional human interaction
recognition, which uses choreographed videos as inputs, neglects concurrent
interactive groups, and performs detection and recognition in separate stages,
we introduce a new task named human-to-human interaction detection (HID). HID
devotes to detecting subjects, recognizing person-wise actions, and grouping
people according to their interactive relations, in one model. First, based on
the popular AVA dataset created for action detection, we establish a new HID
benchmark, termed AVA-Interaction (AVA-I), by adding annotations on interactive
relations in a frame-by-frame manner. AVA-I consists of 85,254 frames and
86,338 interactive groups, and each image includes up to 4 concurrent
interactive groups. Second, we present a novel baseline approach SaMFormer for
HID, containing a visual feature extractor, a split stage which leverages a
Transformer-based model to decode action instances and interactive groups, and
a merging stage which reconstructs the relationship between instances and
groups. All SaMFormer components are jointly trained in an end-to-end manner.
Extensive experiments on AVA-I validate the superiority of SaMFormer over
representative methods. The dataset and code will be made public to encourage
more follow-up studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenhua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ying_K/0/1/0/all/0/1&quot;&gt;Kaining Ying&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_J/0/1/0/all/0/1&quot;&gt;Jiajun Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_J/0/1/0/all/0/1&quot;&gt;Jifeng Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03104">
<title>Efficient Domain Adaptation of Sentence Embeddings Using Adapters. (arXiv:2307.03104v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03104</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentence embeddings enable us to capture the semantic similarity of short
texts. Most sentence embedding models are trained for general semantic textual
similarity (STS) tasks. Therefore, to use sentence embeddings in a particular
domain, the model must be adapted to it in order to achieve good results.
Usually, this is done by fine-tuning the entire sentence embedding model for
the domain of interest. While this approach yields state-of-the-art results,
all of the model&apos;s weights are updated during fine-tuning, making this method
resource-intensive. Therefore, instead of fine-tuning entire sentence embedding
models for each target domain individually, we propose to train lightweight
adapters. These domain-specific adapters do not require fine-tuning all
underlying sentence embedding model parameters. Instead, we only train a small
number of additional parameters while keeping the weights of the underlying
sentence embedding model fixed. Training domain-specific adapters allows always
using the same base model and only exchanging the domain-specific adapters to
adapt sentence embeddings to a specific domain. We show that using adapters for
parameter-efficient domain adaptation of sentence embeddings yields competitive
performance within 1% of a domain-adapted, entirely fine-tuned sentence
embedding model while only training approximately 3.6% of the parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schopf_T/0/1/0/all/0/1&quot;&gt;Tim Schopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_D/0/1/0/all/0/1&quot;&gt;Dennis N. Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthes_F/0/1/0/all/0/1&quot;&gt;Florian Matthes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08177">
<title>In-IDE Generation-based Information Support with a Large Language Model. (arXiv:2307.08177v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08177</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding code is challenging, especially when working in new and complex
development environments. Code comments and documentation can help, but are
typically scarce or hard to navigate. Large language models (LLMs) are
revolutionizing the process of writing code. Can they do the same for helping
understand it? In this study, we provide a first investigation of an LLM-based
conversational UI built directly in the IDE that is geared towards code
understanding. Our IDE plugin queries OpenAI&apos;s GPT-3.5 and GPT-4 models with
four high-level requests without the user having to write explicit prompts: to
explain a highlighted section of code, provide details of API calls used in the
code, explain key domain-specific terms, and provide usage examples for an API.
The plugin also allows for open-ended prompts, which are automatically
contextualized to the LLM with the program being edited. We evaluate this
system in a user study with 32 participants, which confirms that using our
plugin can aid task completion more than web search. We additionally provide a
thorough analysis of the ways developers use, and perceive the usefulness of,
our system, among others finding that the usage and benefits differ
significantly between students and professionals. We conclude that in-IDE
prompt-less interaction with LLMs is a promising future direction for tool
builders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_D/0/1/0/all/0/1&quot;&gt;Daye Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macvean_A/0/1/0/all/0/1&quot;&gt;Andrew Macvean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellendoorn_V/0/1/0/all/0/1&quot;&gt;Vincent Hellendoorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vasilescu_B/0/1/0/all/0/1&quot;&gt;Bogdan Vasilescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myers_B/0/1/0/all/0/1&quot;&gt;Brad Myers&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12267">
<title>Towards Automatic Boundary Detection for Human-AI Collaborative Hybrid Essay in Education. (arXiv:2307.12267v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12267</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent large language models (LLMs), e.g., ChatGPT, have been able to
generate human-like and fluent responses when provided with specific
instructions. While admitting the convenience brought by technological
advancement, educators also have concerns that students might leverage LLMs to
complete their writing assignments and pass them off as their original work.
Although many AI content detection studies have been conducted as a result of
such concerns, most of these prior studies modeled AI content detection as a
classification problem, assuming that a text is either entirely human-written
or entirely AI-generated. In this study, we investigated AI content detection
in a rarely explored yet realistic setting where the text to be detected is
collaboratively written by human and generative LLMs (i.e., hybrid text). We
first formalized the detection task as identifying the transition points
between human-written content and AI-generated content from a given hybrid text
(boundary detection). Then we proposed a two-step approach where we (1)
separated AI-generated content from human-written content during the encoder
training process; and (2) calculated the distances between every two adjacent
prototypes and assumed that the boundaries exist between the two adjacent
prototypes that have the furthest distance from each other. Through extensive
experiments, we observed the following main findings: (1) the proposed approach
consistently outperformed the baseline methods across different experiment
settings; (2) the encoder training process can significantly boost the
performance of the proposed approach; (3) when detecting boundaries for
single-boundary hybrid essays, the proposed approach could be enhanced by
adopting a relatively large prototype size, leading to a 22% improvement in the
In-Domain evaluation and an 18% improvement in the Out-of-Domain evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zijie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sha_L/0/1/0/all/0/1&quot;&gt;Lele Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kaixun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gasevic_D/0/1/0/all/0/1&quot;&gt;Dragan Ga&amp;#x161;evi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guanliang Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12488">
<title>ChatGPT for Software Security: Exploring the Strengths and Limitations of ChatGPT in the Security Applications. (arXiv:2307.12488v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12488</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT, as a versatile large language model, has demonstrated remarkable
potential in addressing inquiries across various domains. Its ability to
analyze, comprehend, and synthesize information from both online sources and
user inputs has garnered significant attention. Previous research has explored
ChatGPT&apos;s competence in code generation and code reviews. In this paper, we
delve into ChatGPT&apos;s capabilities in security-oriented program analysis,
focusing on perspectives from both attackers and security analysts. We present
a case study involving several security-oriented program analysis tasks while
deliberately introducing challenges to assess ChatGPT&apos;s responses. Through an
examination of the quality of answers provided by ChatGPT, we gain a clearer
understanding of its strengths and limitations in the realm of
security-oriented program analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Peng Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16104">
<title>AI Increases Global Access to Reliable Flood Forecasts. (arXiv:2307.16104v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16104</link>
<description rdf:parseType="Literal">&lt;p&gt;Floods are one of the most common and impactful natural disasters, with a
disproportionate impact in developing countries that often lack dense
streamflow monitoring networks. Accurate and timely warnings are critical for
mitigating flood risks, but accurate hydrological simulation models typically
must be calibrated to long data records in each watershed where they are
applied. We developed an Artificial Intelligence (AI) model to predict extreme
hydrological events at timescales up to 7 days in advance. This model
significantly outperforms current state of the art global hydrology models (the
Copernicus Emergency Management Service Global Flood Awareness System) across
all continents, lead times, and return periods. AI is especially effective at
forecasting in ungauged basins, which is important because only a few percent
of the world&apos;s watersheds have stream gauges, with a disproportionate number of
ungauged basins in developing countries that are especially vulnerable to the
human impacts of flooding. We produce forecasts of extreme events in South
America and Africa that achieve reliability approaching the current state of
the art in Europe and North America, and we achieve reliability at between 4
and 6-day lead times that are similar to current state of the art nowcasts
(0-day lead time). Additionally, we achieve accuracies over 10-year return
period events that are similar to current accuracies over 2-year return period
events, meaning that AI can provide warnings earlier and over larger and more
impactful events. The model that we develop in this paper has been incorporated
into an operational early warning system that produces publicly available (free
and open) forecasts in real time in over 80 countries. This work using AI and
open data highlights a need for increasing the availability of hydrological
data to continue to improve global access to reliable flood warnings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nearing_G/0/1/0/all/0/1&quot;&gt;Grey Nearing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_D/0/1/0/all/0/1&quot;&gt;Deborah Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dube_V/0/1/0/all/0/1&quot;&gt;Vusumuzi Dube&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gauch_M/0/1/0/all/0/1&quot;&gt;Martin Gauch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilon_O/0/1/0/all/0/1&quot;&gt;Oren Gilon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrigan_S/0/1/0/all/0/1&quot;&gt;Shaun Harrigan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassidim_A/0/1/0/all/0/1&quot;&gt;Avinatan Hassidim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kratzert_F/0/1/0/all/0/1&quot;&gt;Frederik Kratzert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzger_A/0/1/0/all/0/1&quot;&gt;Asher Metzger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nevo_S/0/1/0/all/0/1&quot;&gt;Sella Nevo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappenberger_F/0/1/0/all/0/1&quot;&gt;Florian Pappenberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prudhomme_C/0/1/0/all/0/1&quot;&gt;Christel Prudhomme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalev_G/0/1/0/all/0/1&quot;&gt;Guy Shalev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenzis_S/0/1/0/all/0/1&quot;&gt;Shlomo Shenzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekalign_T/0/1/0/all/0/1&quot;&gt;Tadele Tekalign&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weitzner_D/0/1/0/all/0/1&quot;&gt;Dana Weitzner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yoss Matias&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00158">
<title>Predicting Perfect Quality Segments in MT Output with Fine-Tuned OpenAI LLM: Is it possible to capture editing distance patterns from historical data?. (arXiv:2308.00158v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00158</link>
<description rdf:parseType="Literal">&lt;p&gt;Translation Quality Estimation (TQE) is an important step before deploying
the output translation into usage. TQE is also critical in assessing machine
translation (MT) and human translation (HT) quality without seeing the
reference translations. In this work, we examine if the state-of-the-art large
language models (LLMs) can be fine-tuned for the TQE task and their capability.
We take ChatGPT as one example and approach TQE as a binary classification
task. Using English to Italian, German, French, Japanese, Dutch, Portuguese,
Turkish, and Chinese training corpora, our experimental results show that
fine-tuned ChatGPT via its API can achieve a relatively high score on
predicting translation quality, i.e. if the translation needs to be edited, but
there is definitely much space to improve the accuracy. English-Italiano
bilingual Abstract is available in the paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gladkoff_S/0/1/0/all/0/1&quot;&gt;Serge Gladkoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erofeev_G/0/1/0/all/0/1&quot;&gt;Gleb Erofeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_L/0/1/0/all/0/1&quot;&gt;Lifeng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenadic_G/0/1/0/all/0/1&quot;&gt;Goran Nenadic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01118">
<title>A Survey on Popularity Bias in Recommender Systems. (arXiv:2308.01118v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01118</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommender systems help people find relevant content in a personalized way.
One main promise of such systems is that they are able to increase the
visibility of items in the long tail, i.e., the lesser-known items in a
catalogue. Existing research, however, suggests that in many situations today&apos;s
recommendation algorithms instead exhibit a popularity bias, meaning that they
often focus on rather popular items in their recommendations. Such a bias may
not only lead to limited value of the recommendations for consumers and
providers in the short run, but it may also cause undesired reinforcement
effects over time. In this paper, we discuss the potential reasons for
popularity bias and we review existing approaches to detect, quantify and
mitigate popularity bias in recommender systems. Our survey therefore includes
both an overview of the computational metrics used in the literature as well as
a review of the main technical approaches to reduce the bias. We furthermore
critically discuss today&apos;s literature, where we observe that the research is
almost entirely based on computational experiments and on certain assumptions
regarding the practical effects of including long-tail items in the
recommendations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klimashevskaia_A/0/1/0/all/0/1&quot;&gt;Anastasiia Klimashevskaia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannach_D/0/1/0/all/0/1&quot;&gt;Dietmar Jannach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_M/0/1/0/all/0/1&quot;&gt;Mehdi Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trattner_C/0/1/0/all/0/1&quot;&gt;Christoph Trattner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02542">
<title>Collaborative filtering to capture AI user&apos;s preferences as norms. (arXiv:2308.02542v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02542</link>
<description rdf:parseType="Literal">&lt;p&gt;Customising AI technologies to each user&apos;s preferences is fundamental to them
functioning well. Unfortunately, current methods require too much user
involvement and fail to capture their true preferences. In fact, to avoid the
nuisance of manually setting preferences, users usually accept the default
settings even if these do not conform to their true preferences. Norms can be
useful to regulate behaviour and ensure it adheres to user preferences but,
while the literature has thoroughly studied norms, most proposals take a formal
perspective. Indeed, while there has been some research on constructing norms
to capture a user&apos;s privacy preferences, these methods rely on domain knowledge
which, in the case of AI technologies, is difficult to obtain and maintain. We
argue that a new perspective is required when constructing norms, which is to
exploit the large amount of preference information readily available from whole
systems of users. Inspired by recommender systems, we believe that
collaborative filtering can offer a suitable approach to identifying a user&apos;s
norm preferences without excessive user involvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serramia_M/0/1/0/all/0/1&quot;&gt;Marc Serramia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Criado_N/0/1/0/all/0/1&quot;&gt;Natalia Criado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luck_M/0/1/0/all/0/1&quot;&gt;Michael Luck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02916">
<title>Adversarial Erasing with Pruned Elements: Towards Better Graph Lottery Ticket. (arXiv:2308.02916v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02916</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Lottery Ticket (GLT), a combination of core subgraph and sparse
subnetwork, has been proposed to mitigate the computational cost of deep Graph
Neural Networks (GNNs) on large input graphs while preserving original
performance. However, the winning GLTs in exisiting studies are obtained by
applying iterative magnitude-based pruning (IMP) without re-evaluating and
re-considering the pruned information, which disregards the dynamic changes in
the significance of edges/weights during graph/model structure pruning, and
thus limits the appeal of the winning tickets. In this paper, we formulate a
conjecture, i.e., existing overlooked valuable information in the pruned graph
connections and model parameters which can be re-grouped into GLT to enhance
the final performance. Specifically, we propose an adversarial complementary
erasing (ACE) framework to explore the valuable information from the pruned
components, thereby developing a more powerful GLT, referred to as the ACE-GLT.
The main idea is to mine valuable information from pruned edges/weights after
each round of IMP, and employ the ACE technique to refine the GLT processing.
Finally, experimental results demonstrate that our ACE-GLT outperforms existing
methods for searching GLT in diverse tasks. Our code will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shunyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tongtian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_J/0/1/0/all/0/1&quot;&gt;Ji Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_M/0/1/0/all/0/1&quot;&gt;Mengjie Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yuanyu Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04172">
<title>Predicting Drug-Drug Interactions Using Knowledge Graphs. (arXiv:2308.04172v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04172</link>
<description rdf:parseType="Literal">&lt;p&gt;In the last decades, people have been consuming and combining more drugs than
before, increasing the number of Drug-Drug Interactions (DDIs). To predict
unknown DDIs, recently, studies started incorporating Knowledge Graphs (KGs)
since they are able to capture the relationships among entities providing
better drug representations than using a single drug property. In this paper,
we propose the medicX end-to-end framework that integrates several drug
features from public drug repositories into a KG and embeds the nodes in the
graph using various translation, factorisation and Neural Network (NN) based KG
Embedding (KGE) methods. Ultimately, we use a Machine Learning (ML) algorithm
that predicts unknown DDIs. Among the different translation and
factorisation-based KGE models, we found that the best performing combination
was the ComplEx embedding method with a Long Short-Term Memory (LSTM) network,
which obtained an F1-score of 95.19% on a dataset based on the DDIs found in
DrugBank version 5.1.8. This score is 5.61% better than the state-of-the-art
model DeepDDI. Additionally, we also developed a graph auto-encoder model that
uses a Graph Neural Network (GNN), which achieved an F1-score of 91.94%.
Consequently, GNNs have demonstrated a stronger ability to mine the underlying
semantics of the KG than the ComplEx model, and thus using higher dimension
embeddings within the GNN can lead to state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farrugia_L/0/1/0/all/0/1&quot;&gt;Lizzy Farrugia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azzopardi_L/0/1/0/all/0/1&quot;&gt;Lilian M. Azzopardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Debattista_J/0/1/0/all/0/1&quot;&gt;Jeremy Debattista&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abela_C/0/1/0/all/0/1&quot;&gt;Charlie Abela&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04241">
<title>AutoPCF: Efficient Product Carbon Footprint Accounting with Large Language Models. (arXiv:2308.04241v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04241</link>
<description rdf:parseType="Literal">&lt;p&gt;The product carbon footprint (PCF) is crucial for decarbonizing the supply
chain, as it measures the direct and indirect greenhouse gas emissions caused
by all activities during the product&apos;s life cycle. However, PCF accounting
often requires expert knowledge and significant time to construct life cycle
models. In this study, we test and compare the emergent ability of five large
language models (LLMs) in modeling the &apos;cradle-to-gate&apos; life cycles of products
and generating the inventory data of inputs and outputs, revealing their
limitations as a generalized PCF knowledge database. By utilizing LLMs, we
propose an automatic AI-driven PCF accounting framework, called AutoPCF, which
also applies deep learning algorithms to automatically match calculation
parameters, and ultimately calculate the PCF. The results of estimating the
carbon footprint for three case products using the AutoPCF framework
demonstrate its potential in achieving automatic modeling and estimation of PCF
with a large reduction in modeling time from days to minutes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhu Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jinjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Biao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Can Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qingrun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Lei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenwen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04522">
<title>Deep Learning for Diverse Data Types Steganalysis: A Review. (arXiv:2308.04522v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04522</link>
<description rdf:parseType="Literal">&lt;p&gt;Steganography and steganalysis are two interrelated aspects of the field of
information security. Steganography seeks to conceal communications, whereas
steganalysis is aimed to either find them or even, if possible, recover the
data they contain. Steganography and steganalysis have attracted a great deal
of interest, particularly from law enforcement. Steganography is often used by
cybercriminals and even terrorists to avoid being captured while in possession
of incriminating evidence, even encrypted, since cryptography is prohibited or
restricted in many countries. Therefore, knowledge of cutting-edge techniques
to uncover concealed information is crucial in exposing illegal acts. Over the
last few years, a number of strong and reliable steganography and steganalysis
techniques have been introduced in the literature. This review paper provides a
comprehensive overview of deep learning-based steganalysis techniques used to
detect hidden information within digital media. The paper covers all types of
cover in steganalysis, including image, audio, and video, and discusses the
most commonly used deep learning techniques. In addition, the paper explores
the use of more advanced deep learning techniques, such as deep transfer
learning (DTL) and deep reinforcement learning (DRL), to enhance the
performance of steganalysis systems. The paper provides a systematic review of
recent research in the field, including data sets and evaluation metrics used
in recent studies. It also presents a detailed analysis of DTL-based
steganalysis approaches and their performance on different data sets. The
review concludes with a discussion on the current state of deep learning-based
steganalysis, challenges, and future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kheddar_H/0/1/0/all/0/1&quot;&gt;Hamza Kheddar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemis_M/0/1/0/all/0/1&quot;&gt;Mustapha Hemis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himeur_Y/0/1/0/all/0/1&quot;&gt;Yassine Himeur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Megias_D/0/1/0/all/0/1&quot;&gt;David Meg&amp;#xed;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amira_A/0/1/0/all/0/1&quot;&gt;Abbes Amira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04586">
<title>Developmental Bootstrapping of AIs. (arXiv:2308.04586v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04586</link>
<description rdf:parseType="Literal">&lt;p&gt;Although some current AIs surpass human abilities especially in closed
artificial worlds such as board games, their abilities in the real world are
limited. They make strange mistakes and do not notice them. They cannot be
instructed easily, fail to use common sense, and lack curiosity. They do not
make good collaborators. Mainstream approaches for creating AIs are built using
the traditional manually-constructed symbolic AI approach and generative and
deep learning AI approaches including large language models (LLMs). These
systems are not well suited for creating robust and trustworthy AIs. Although
it is outside of the mainstream, the developmental bootstrapping approach has
more promise. In developmental bootstrapping, AIs develop competences like
human children do. They start with innate competences. They interact with the
environment and learn from their interactions. They incrementally extend their
innate competences with self-developed competences. They interact and learn
from people and establish perceptual, cognitive, and common grounding. They
acquire the competences that they need through an incremental bootstrapping
process. However, developmental robotics has not yet produced AIs with robust
adult-level competences. Projects have typically stopped at the Toddler Barrier
corresponding to human infant development at about two years of age, before
their speech is fluent. They also do not bridge the Reading Barrier, to
skillfully and skeptically tap into the vast socially developed recorded
information resources that power LLMs. The next competences in human cognitive
development involve intrinsic motivation, imitation learning, imagination,
coordination, and communication. This position paper lays out the logic,
prospects, gaps, and challenges for extending the practice of developmental
bootstrapping to acquire further competences and create robust and resilient
AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_R/0/1/0/all/0/1&quot;&gt;Robert Price&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05309">
<title>Homophily-enhanced Structure Learning for Graph Clustering. (arXiv:2308.05309v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05309</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph clustering is a fundamental task in graph analysis, and recent advances
in utilizing graph neural networks (GNNs) have shown impressive results.
Despite the success of existing GNN-based graph clustering methods, they often
overlook the quality of graph structure, which is inherent in real-world graphs
due to their sparse and multifarious nature, leading to subpar performance.
Graph structure learning allows refining the input graph by adding missing
links and removing spurious connections. However, previous endeavors in graph
structure learning have predominantly centered around supervised settings, and
cannot be directly applied to our specific clustering tasks due to the absence
of ground-truth labels. To bridge the gap, we propose a novel method called
\textbf{ho}mophily-enhanced structure \textbf{le}arning for graph clustering
(HoLe). Our motivation stems from the observation that subtly enhancing the
degree of homophily within the graph structure can significantly improve GNNs
and clustering outcomes. To realize this objective, we develop two
clustering-oriented structure learning modules, i.e., hierarchical correlation
estimation and cluster-aware sparsification. The former module enables a more
accurate estimation of pairwise node relationships by leveraging guidance from
latent and clustering spaces, while the latter one generates a sparsified
structure based on the similarity matrix and clustering assignments.
Additionally, we devise a joint optimization approach alternating between
training the homophily-enhanced structure learning and GNN-based clustering,
thereby enforcing their reciprocal effects. Extensive experiments on seven
benchmark datasets of various types and scales, across a range of clustering
metrics, demonstrate the superiority of HoLe against state-of-the-art
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_M/0/1/0/all/0/1&quot;&gt;Ming Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Gaoming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_N/0/1/0/all/0/1&quot;&gt;Ning Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qiaoyu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_J/0/1/0/all/0/1&quot;&gt;Jiajun Bu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.05481">
<title>LLM As DBA. (arXiv:2308.05481v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2308.05481</link>
<description rdf:parseType="Literal">&lt;p&gt;Database administrators (DBAs) play a crucial role in managing, maintaining
and optimizing a database system to ensure data availability, performance, and
reliability. However, it is hard and tedious for DBAs to manage a large number
of database instances (e.g., millions of instances on the cloud databases).
Recently large language models (LLMs) have shown great potential to understand
valuable documents and accordingly generate reasonable answers. Thus, we
propose D-Bot, a LLM-based database administrator that can continuously acquire
database maintenance experience from textual sources, and provide reasonable,
well-founded, in-time diagnosis and optimization advice for target databases.
This paper presents a revolutionary LLM-centric framework for database
maintenance, including (i) database maintenance knowledge detection from
documents and tools, (ii) tree of thought reasoning for root cause analysis,
and (iii) collaborative diagnosis among multiple LLMs. Our preliminary
experimental results that D-Bot can efficiently and effectively diagnose the
root causes and our code is available at
github.com/TsinghuaDatabaseGroup/DB-GPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuanhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>