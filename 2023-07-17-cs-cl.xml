<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-16T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06954" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06979" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06982" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07047" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07049" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07057" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07099" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07164" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07255" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07312" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07411" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07412" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07421" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08372" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.02762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.14301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.00552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.04391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13734" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05695" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.06954">
<title>ACTI at EVALITA 2023: Overview of the Conspiracy Theory Identification Task. (arXiv:2307.06954v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06954</link>
<description rdf:parseType="Literal">&lt;p&gt;Conspiracy Theory Identication task is a new shared task proposed for the
first time at the Evalita 2023. The ACTI challenge, based exclusively on
comments published on conspiratorial channels of telegram, is divided into two
subtasks: (i) Conspiratorial Content Classification: identifying conspiratorial
content and (ii) Conspiratorial Category Classification about specific
conspiracy theory classification. A total of fifteen teams participated in the
task for a total of 81 submissions. We illustrate the best performing
approaches were based on the utilization of large language models. We finally
draw conclusions about the utilization of these models for counteracting the
spreading of misinformation in online platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_G/0/1/0/all/0/1&quot;&gt;Giuseppe Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoehr_N/0/1/0/all/0/1&quot;&gt;Niklas Stoehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_M/0/1/0/all/0/1&quot;&gt;Manoel Horta Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06962">
<title>Copy Is All You Need. (arXiv:2307.06962v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06962</link>
<description rdf:parseType="Literal">&lt;p&gt;The dominant text generation models compose the output by sequentially
selecting words from a fixed vocabulary. In this paper, we formulate text
generation as progressively copying text segments (e.g., words or phrases) from
an existing text collection. We compute the contextualized representations of
meaningful text segments and index them using efficient vector search toolkits.
The task of text generation is then decomposed into a series of copy-and-paste
operations: at each time step, we seek suitable text spans from the text
collection rather than selecting from a standalone vocabulary. Experiments on
the standard language modeling benchmark (WikiText-103) show that our approach
achieves better generation quality according to both automatic and human
evaluations. Besides, its inference efficiency is comparable to token-level
autoregressive models thanks to the reduction of decoding steps. We also show
that our approach allows for effective domain adaptation by simply switching to
domain-specific text collection without extra training. Finally, we observe
that our approach attains additional performance gains by simply scaling up to
larger text collections, again without further training.\footnote{Our source
codes are publicly available at
\url{https://github.com/gmftbyGMFTBY/Copyisallyouneed}.}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_D/0/1/0/all/0/1&quot;&gt;Deng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Heyan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_X/0/1/0/all/0/1&quot;&gt;Xian-Ling Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06979">
<title>Tackling Fake News in Bengali: Unraveling the Impact of Summarization vs. Augmentation on Pre-trained Language Models. (arXiv:2307.06979v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06979</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rise of social media and online news sources, fake news has become a
significant issue globally. However, the detection of fake news in low resource
languages like Bengali has received limited attention in research. In this
paper, we propose a methodology consisting of four distinct approaches to
classify fake news articles in Bengali using summarization and augmentation
techniques with five pre-trained language models. Our approach includes
translating English news articles and using augmentation techniques to curb the
deficit of fake news articles. Our research also focused on summarizing the
news to tackle the token length limitation of BERT based models. Through
extensive experimentation and rigorous evaluation, we show the effectiveness of
summarization and augmentation in the case of Bengali fake news detection. We
evaluated our models using three separate test datasets. The BanglaBERT Base
model, when combined with augmentation techniques, achieved an impressive
accuracy of 96% on the first test dataset. On the second test dataset, the
BanglaBERT model, trained with summarized augmented news articles achieved 97%
accuracy. Lastly, the mBERT Base model achieved an accuracy of 86% on the third
test dataset which was reserved for generalization performance evaluation. The
datasets and implementations are available at
https://github.com/arman-sakif/Bengali-Fake-News-Detection
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_A/0/1/0/all/0/1&quot;&gt;Arman Sakif Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1&quot;&gt;G. M. Shahariar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aziz_A/0/1/0/all/0/1&quot;&gt;Ahammed Tarik Aziz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_S/0/1/0/all/0/1&quot;&gt;Syed Mohibul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheikh_M/0/1/0/all/0/1&quot;&gt;Md. Azad Sheikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belal_T/0/1/0/all/0/1&quot;&gt;Tanveer Ahmed Belal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06982">
<title>Revisiting the DARPA Communicator Data using Conversation Analysis. (arXiv:2307.06982v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06982</link>
<description rdf:parseType="Literal">&lt;p&gt;The state of the art in human computer conversation leaves something to be
desired and, indeed, talking to a computer can be down-right annoying. This
paper describes an approach to identifying ``opportunities for improvement&apos;&apos; in
these systems by looking for abuse in the form of swear words. The premise is
that humans swear at computers as a sanction and, as such, swear words
represent a point of failure where the system did not behave as it should.
Having identified where things went wrong, we can work backward through the
transcripts and, using conversation analysis (CA) work out how things went
wrong. Conversation analysis is a qualitative methodology and can appear quite
alien - indeed unscientific - to those of us from a quantitative background.
The paper starts with a description of Conversation analysis in its modern
form, and then goes on to apply the methodology to transcripts of frustrated
and annoyed users in the DARPA Communicator project. The conclusion is that
there is at least one species of failure caused by the inability of the
Communicator systems to handle mixed initiative at the discourse structure
level. Along the way, I hope to demonstrate that there is an alternative future
for computational linguistics that does not rely on larger and larger text
corpora.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallis_P/0/1/0/all/0/1&quot;&gt;Peter Wallis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06985">
<title>Towards Populating Generalizable Engineering Design Knowledge. (arXiv:2307.06985v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.06985</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming to populate generalizable engineering design knowledge, we propose a
method to extract facts of the form head entity :: relationship :: tail entity
from sentences found in patent documents. These facts could be combined within
and across patent documents to form knowledge graphs that serve as schemes for
representing as well as storing design knowledge. Existing methods in
engineering design literature often utilise a set of predefined relationships
to populate triples that are statistical approximations rather than facts. In
our method, we train a tagger to identify both entities and relationships from
a sentence. Given a pair of entities thus identified, we train another tagger
to identify the relationship tokens that specifically denote the relationship
between the pair. For training these taggers, we manually construct a dataset
of 44,227 sentences and corresponding facts. We also compare the performance of
the method against typically recommended approaches, wherein, we predict the
edges among tokens by pairing the tokens independently and as part of a graph.
We apply our method to sentences found in patents related to fan systems and
build a domain knowledge base. Upon providing an overview of the knowledge
base, we search for solutions relevant to some key issues prevailing in fan
systems. We organize the responses into knowledge graphs and hold a comparative
discussion against the opinions from ChatGPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siddharth_L/0/1/0/all/0/1&quot;&gt;L Siddharth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianxi Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07002">
<title>Classical Out-of-Distribution Detection Methods Benchmark in Text Classification Tasks. (arXiv:2307.07002v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07002</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art models can perform well in controlled environments, but they
often struggle when presented with out-of-distribution (OOD) examples, making
OOD detection a critical component of NLP systems. In this paper, we focus on
highlighting the limitations of existing approaches to OOD detection in NLP.
Specifically, we evaluated eight OOD detection methods that are easily
integrable into existing NLP systems and require no additional OOD data or
model modifications. One of our contributions is providing a well-structured
research environment that allows for full reproducibility of the results.
Additionally, our analysis shows that existing OOD detection methods for NLP
tasks are not yet sufficiently sensitive to capture all samples characterized
by various types of distributional shifts. Particularly challenging testing
scenarios arise in cases of background shift and randomly shuffled word order
within in domain texts. This highlights the need for future work to develop
more effective OOD detection approaches for the NLP problems, and our work
provides a well-defined foundation for further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1&quot;&gt;Mateusz Baran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baran_J/0/1/0/all/0/1&quot;&gt;Joanna Baran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1&quot;&gt;Mateusz W&amp;#xf3;jcik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zieba_M/0/1/0/all/0/1&quot;&gt;Maciej Zi&amp;#x119;ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonczarek_A/0/1/0/all/0/1&quot;&gt;Adam Gonczarek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07007">
<title>Electoral Agitation Data Set: The Use Case of the Polish Election. (arXiv:2307.07007v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07007</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of social media makes politicians use it for political
advertisement. Therefore, social media is full of electoral agitation
(electioneering), especially during the election campaigns. The election
administration cannot track the spread and quantity of messages that count as
agitation under the election code. It addresses a crucial problem, while also
uncovering a niche that has not been effectively targeted so far. Hence, we
present the first publicly open data set for detecting electoral agitation in
the Polish language. It contains 6,112 human-annotated tweets tagged with four
legally conditioned categories. We achieved a 0.66 inter-annotator agreement
(Cohen&apos;s kappa score). An additional annotator resolved the mismatches between
the first two improving the consistency and complexity of the annotation
process. The newly created data set was used to fine-tune a Polish Language
Model called HerBERT (achieving a 68% F1 score). We also present a number of
potential use cases for such data sets and models, enriching the paper with an
analysis of the Polish 2020 Presidential Election on Twitter.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baran_M/0/1/0/all/0/1&quot;&gt;Mateusz Baran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wojcik_M/0/1/0/all/0/1&quot;&gt;Mateusz W&amp;#xf3;jcik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolebski_P/0/1/0/all/0/1&quot;&gt;Piotr Kolebski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernaczyk_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Bernaczyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajda_K/0/1/0/all/0/1&quot;&gt;Krzysztof Rajda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Augustyniak_L/0/1/0/all/0/1&quot;&gt;&amp;#x141;ukasz Augustyniak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kajdanowicz_T/0/1/0/all/0/1&quot;&gt;Tomasz Kajdanowicz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07025">
<title>Data Augmentation for Machine Translation via Dependency Subtree Swapping. (arXiv:2307.07025v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07025</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a generic framework for data augmentation via dependency subtree
swapping that is applicable to machine translation. We extract corresponding
subtrees from the dependency parse trees of the source and target sentences and
swap these across bisentences to create augmented samples. We perform thorough
filtering based on graphbased similarities of the dependency trees and
additional heuristics to ensure that extracted subtrees correspond to the same
meaning. We conduct resource-constrained experiments on 4 language pairs in
both directions using the IWSLT text translation datasets and the Hunglish2
corpus. The results demonstrate consistent improvements in BLEU score over our
baseline models in 3 out of 4 language pairs. Our code is available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagy_A/0/1/0/all/0/1&quot;&gt;Attila Nagy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakatos_D/0/1/0/all/0/1&quot;&gt;Dorina Petra Lakatos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barta_B/0/1/0/all/0/1&quot;&gt;Botond Barta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanys_P/0/1/0/all/0/1&quot;&gt;Patrick Nanys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acs_J/0/1/0/all/0/1&quot;&gt;Judit &amp;#xc1;cs&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07047">
<title>DIALGEN: Collaborative Human-LM Generated Dialogues for Improved Understanding of Human-Human Conversations. (arXiv:2307.07047v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07047</link>
<description rdf:parseType="Literal">&lt;p&gt;Applications that could benefit from automatic understanding of human-human
conversations often come with challenges associated with private information in
real-world data such as call center or clinical conversations. Working with
protected data also increases costs of annotation, which limits technology
development. To address these challenges, we propose DIALGEN, a
human-in-the-loop semi-automated dialogue generation framework. DIALGEN uses a
language model (ChatGPT) that can follow schema and style specifications to
produce fluent conversational text, generating a complex conversation through
iteratively generating subdialogues and using human feedback to correct
inconsistencies or redirect the flow. In experiments on structured
summarization of agent-client information gathering calls, framed as dialogue
state tracking, we show that DIALGEN data enables significant improvement in
model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_B/0/1/0/all/0/1&quot;&gt;Bo-Ru Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haduong_N/0/1/0/all/0/1&quot;&gt;Nikita Haduong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chia-Hsuan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zeqiu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_H/0/1/0/all/0/1&quot;&gt;Hao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koester_P/0/1/0/all/0/1&quot;&gt;Paul Koester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utke_J/0/1/0/all/0/1&quot;&gt;Jean Utke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostendorf_M/0/1/0/all/0/1&quot;&gt;Mari Ostendorf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07049">
<title>MegaWika: Millions of reports and their sources across 50 diverse languages. (arXiv:2307.07049v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07049</link>
<description rdf:parseType="Literal">&lt;p&gt;To foster the development of new models for collaborative AI-assisted report
generation, we introduce MegaWika, consisting of 13 million Wikipedia articles
in 50 diverse languages, along with their 71 million referenced source
materials. We process this dataset for a myriad of applications, going beyond
the initial Wikipedia citation extraction and web scraping of content,
including translating non-English articles for cross-lingual applications and
providing FrameNet parses for automated semantic analysis. MegaWika is the
largest resource for sentence-level report generation and the only report
generation dataset that is multilingual. We manually analyze the quality of
this resource through a semantically stratified sample. Finally, we provide
baseline results and trained models for crucial steps in automated report
generation: cross-lingual question answering and citation retrieval.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barham_S/0/1/0/all/0/1&quot;&gt;Samuel Barham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weller_O/0/1/0/all/0/1&quot;&gt;Orion Weller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Michelle Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1&quot;&gt;Kenton Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarmohammadi_M/0/1/0/all/0/1&quot;&gt;Mahsa Yarmohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhengping Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vashishtha_S/0/1/0/all/0/1&quot;&gt;Siddharth Vashishtha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1&quot;&gt;Alexander Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+White_A/0/1/0/all/0/1&quot;&gt;Aaron Steven White&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boyd_Graber_J/0/1/0/all/0/1&quot;&gt;Jordan Boyd-Graber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Durme&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07051">
<title>Making the Most Out of the Limited Context Length: Predictive Power Varies with Clinical Note Type and Note Section. (arXiv:2307.07051v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07051</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in large language models have led to renewed interest in
natural language processing in healthcare using the free text of clinical
notes. One distinguishing characteristic of clinical notes is their long time
span over multiple long documents. The unique structure of clinical notes
creates a new design choice: when the context length for a language model
predictor is limited, which part of clinical notes should we choose as the
input? Existing studies either choose the inputs with domain knowledge or
simply truncate them. We propose a framework to analyze the sections with high
predictive power. Using MIMIC-III, we show that: 1) predictive power
distribution is different between nursing notes and discharge notes and 2)
combining different types of notes could improve performance when the context
length is large. Our findings suggest that a carefully selected sampling
function could enable more efficient information extraction from clinical
notes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hongyi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lavender Yao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oermann_E/0/1/0/all/0/1&quot;&gt;Eric Karl Oermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07057">
<title>Leveraging Pretrained ASR Encoders for Effective and Efficient End-to-End Speech Intent Classification and Slot Filling. (arXiv:2307.07057v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07057</link>
<description rdf:parseType="Literal">&lt;p&gt;We study speech intent classification and slot filling (SICSF) by proposing
to use an encoder pretrained on speech recognition (ASR) to initialize an
end-to-end (E2E) Conformer-Transformer model, which achieves the new
state-of-the-art results on the SLURP dataset, with 90.14% intent accuracy and
82.27% SLURP-F1. We compare our model with encoders pretrained on
self-supervised learning (SSL), and show that ASR pretraining is much more
effective than SSL for SICSF. To explore parameter efficiency, we freeze the
encoder and add Adapter modules, and show that parameter efficiency is only
achievable with an ASR-pretrained encoder, while the SSL encoder needs full
finetuning to achieve comparable results. In addition, we provide an in-depth
comparison on end-to-end models versus cascading models (ASR+NLU), and show
that E2E models are better than cascaded models unless an oracle ASR model is
provided. Last but not least, our model is the first E2E model that achieves
the same performance as cascading models with oracle ASR. Code, checkpoints and
configs are available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;He Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balam_J/0/1/0/all/0/1&quot;&gt;Jagadeesh Balam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ginsburg_B/0/1/0/all/0/1&quot;&gt;Boris Ginsburg&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07076">
<title>An Analysis of Dialogue Repair in Virtual Voice Assistants. (arXiv:2307.07076v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.07076</link>
<description rdf:parseType="Literal">&lt;p&gt;Language speakers often use what are known as repair initiators to mend
fundamental disconnects that occur between them during verbal communication.
Previous research in this field has mainly focused on the human-to-human use of
repair initiator. We proposed an examination of dialogue repair structure
wherein the dialogue initiator is human and the party that initiates or
responds to the repair is a virtual assistant. This study examined the use of
repair initiators in both English and Spanish with two popular assistants,
Google Assistant and Apple&apos;s Siri. Our aim was to codify the differences, if
any, in responses by voice assistants to dialogues in need of repair as
compared to human-human dialogues also in need of repair. Ultimately the data
demonstrated that not only were there differences between human-assistant and
human-human dialogue repair strategies, but that there were likewise
differences among the assistants and the languages studied.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galbraith_M/0/1/0/all/0/1&quot;&gt;Matthew Carson Galbraith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1&quot;&gt;Mireia G&amp;#xf3;mez i Mart&amp;#xed;nez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07099">
<title>Generating Efficient Training Data via LLM-based Attribute Manipulation. (arXiv:2307.07099v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07099</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method, Chain-of-Thoughts Attribute
Manipulation (CoTAM), to guide few-shot learning by carefully crafted data from
Large Language Models (LLMs). The main idea is to create data with changes only
in the attribute targeted by the task. Inspired by facial attribute
manipulation, our approach generates label-switched data by leveraging LLMs to
manipulate task-specific attributes and reconstruct new sentences in a
controlled manner. Instead of conventional latent representation controlling,
we implement chain-of-thoughts decomposition and reconstruction to adapt the
procedure to LLMs. Extensive results on text classification and other tasks
verify the advantage of CoTAM over other LLM-based text generation methods with
the same number of training examples. Analysis visualizes the attribute
manipulation effectiveness of CoTAM and presents the potential of LLM-guided
learning with even less supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_L/0/1/0/all/0/1&quot;&gt;Letian Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1&quot;&gt;Jingbo Shang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07135">
<title>MMSD2.0: Towards a Reliable Multi-modal Sarcasm Detection System. (arXiv:2307.07135v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07135</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-modal sarcasm detection has attracted much recent attention.
Nevertheless, the existing benchmark (MMSD) has some shortcomings that hinder
the development of reliable multi-modal sarcasm detection system: (1) There are
some spurious cues in MMSD, leading to the model bias learning; (2) The
negative samples in MMSD are not always reasonable. To solve the aforementioned
issues, we introduce MMSD2.0, a correction dataset that fixes the shortcomings
of MMSD, by removing the spurious cues and re-annotating the unreasonable
samples. Meanwhile, we present a novel framework called multi-view CLIP that is
capable of leveraging multi-grained cues from multiple perspectives (i.e.,
text, image, and text-image interaction view) for multi-modal sarcasm
detection. Extensive experiments show that MMSD2.0 is a valuable benchmark for
building reliable multi-modal sarcasm detection systems and multi-view CLIP can
significantly outperform the previous best baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_L/0/1/0/all/0/1&quot;&gt;Libo Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shijue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qiguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_C/0/1/0/all/0/1&quot;&gt;Chenran Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yudi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1&quot;&gt;Bin Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_W/0/1/0/all/0/1&quot;&gt;Wanxiang Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Ruifeng Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07160">
<title>Do not Mask Randomly: Effective Domain-adaptive Pre-training by Masking In-domain Keywords. (arXiv:2307.07160v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07160</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel task-agnostic in-domain pre-training method that sits
between generic pre-training and fine-tuning. Our approach selectively masks
in-domain keywords, i.e., words that provide a compact representation of the
target domain. We identify such keywords using KeyBERT (Grootendorst, 2020). We
evaluate our approach using six different settings: three datasets combined
with two distinct pre-trained language models (PLMs). Our results reveal that
the fine-tuned PLMs adapted using our in-domain pre-training strategy
outperform PLMs that used in-domain pre-training with random masking as well as
those that followed the common pre-train-then-fine-tune paradigm. Further, the
overhead of identifying in-domain keywords is reasonable, e.g., 7-15% of the
pre-training time (for two epochs) for BERT Large (Devlin et al., 2019).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golchin_S/0/1/0/all/0/1&quot;&gt;Shahriar Golchin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Surdeanu_M/0/1/0/all/0/1&quot;&gt;Mihai Surdeanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavabi_N/0/1/0/all/0/1&quot;&gt;Nazgol Tavabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiapour_A/0/1/0/all/0/1&quot;&gt;Ata Kiapour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07162">
<title>Drive Like a Human: Rethinking Autonomous Driving with Large Language Models. (arXiv:2307.07162v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07162</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the potential of using a large language model (LLM)
to understand the driving environment in a human-like manner and analyze its
ability to reason, interpret, and memorize when facing complex scenarios. We
argue that traditional optimization-based and modular autonomous driving (AD)
systems face inherent performance limitations when dealing with long-tail
corner cases. To address this problem, we propose that an ideal AD system
should drive like a human, accumulating experience through continuous driving
and using common sense to solve problems. To achieve this goal, we identify
three key abilities necessary for an AD system: reasoning, interpretation, and
memorization. We demonstrate the feasibility of employing an LLM in driving
scenarios by building a closed-loop system to showcase its comprehension and
environment-interaction abilities. Our extensive experiments show that the LLM
exhibits the impressive ability to reason and solve long-tailed cases,
providing valuable insights for the development of human-like autonomous
driving. The related code are available at
https://github.com/PJLab-ADG/DriveLikeAHuman .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_D/0/1/0/all/0/1&quot;&gt;Daocheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Licheng Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_M/0/1/0/all/0/1&quot;&gt;Min Dou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_P/0/1/0/all/0/1&quot;&gt;Pinlong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Botian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07164">
<title>Learning to Retrieve In-Context Examples for Large Language Models. (arXiv:2307.07164v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07164</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated their ability to learn
in-context, allowing them to perform various tasks based on a few input-output
examples. However, the effectiveness of in-context learning is heavily reliant
on the quality of the selected examples. In this paper, we propose a novel
framework to iteratively train dense retrievers that can identify high-quality
in-context examples for LLMs. Our framework initially trains a reward model
based on LLM feedback to evaluate the quality of candidate examples, followed
by knowledge distillation to train a bi-encoder based dense retriever. Our
experiments on a suite of 30 tasks demonstrate that our framework significantly
enhances in-context learning performance. Furthermore, we show the
generalization ability of our framework to unseen tasks during training. An
in-depth analysis reveals that our model improves performance by retrieving
examples with similar patterns, and the gains are consistent across LLMs of
varying sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_N/0/1/0/all/0/1&quot;&gt;Nan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07166">
<title>Switching Head-Tail Funnel UNITER for Dual Referring Expression Comprehension with Fetch-and-Carry Tasks. (arXiv:2307.07166v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.07166</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper describes a domestic service robot (DSR) that fetches everyday
objects and carries them to specified destinations according to free-form
natural language instructions. Given an instruction such as &quot;Move the bottle on
the left side of the plate to the empty chair,&quot; the DSR is expected to identify
the bottle and the chair from multiple candidates in the environment and carry
the target object to the destination. Most of the existing multimodal language
understanding methods are impractical in terms of computational complexity
because they require inferences for all combinations of target object
candidates and destination candidates. We propose Switching Head-Tail Funnel
UNITER, which solves the task by predicting the target object and the
destination individually using a single model. Our method is validated on a
newly-built dataset consisting of object manipulation instructions and semi
photo-realistic images captured in a standard Embodied AI simulator. The
results show that our method outperforms the baseline method in terms of
language comprehension accuracy. Furthermore, we conduct physical experiments
in which a DSR delivers standardized everyday objects in a standardized
domestic environment as requested by instructions with referring expressions.
The experimental results show that the object grasping and placing actions are
achieved with success rates of more than 90%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korekata_R/0/1/0/all/0/1&quot;&gt;Ryosuke Korekata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kambara_M/0/1/0/all/0/1&quot;&gt;Motonari Kambara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoshida_Y/0/1/0/all/0/1&quot;&gt;Yu Yoshida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ishikawa_S/0/1/0/all/0/1&quot;&gt;Shintaro Ishikawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawasaki_Y/0/1/0/all/0/1&quot;&gt;Yosuke Kawasaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_M/0/1/0/all/0/1&quot;&gt;Masaki Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sugiura_K/0/1/0/all/0/1&quot;&gt;Komei Sugiura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07171">
<title>Certified Robustness for Large Language Models with Self-Denoising. (arXiv:2307.07171v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07171</link>
<description rdf:parseType="Literal">&lt;p&gt;Although large language models (LLMs) have achieved great success in vast
real-world applications, their vulnerabilities towards noisy inputs have
significantly limited their uses, especially in high-stake environments. In
these contexts, it is crucial to ensure that every prediction made by large
language models is stable, i.e., LLM predictions should be consistent given
minor differences in the input. This largely falls into the study of certified
robust LLMs, i.e., all predictions of LLM are certified to be correct in a
local region around the input. Randomized smoothing has demonstrated great
potential in certifying the robustness and prediction stability of LLMs.
However, randomized smoothing requires adding noise to the input before model
prediction, and its certification performance depends largely on the model&apos;s
performance on corrupted data. As a result, its direct application to LLMs
remains challenging and often results in a small certification radius. To
address this issue, we take advantage of the multitasking nature of LLMs and
propose to denoise the corrupted inputs with LLMs in a self-denoising manner.
Different from previous works like denoised smoothing, which requires training
a separate model to robustify LLM, our method enjoys far better efficiency and
flexibility. Our experiment results show that our method outperforms the
existing certification methods under both certified robustness and empirical
robustness. The codes are available at
https://github.com/UCSB-NLP-Chang/SelfDenoise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guanhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Bairu Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Sijia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shiyu Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07255">
<title>Dialogue Agents 101: A Beginner&apos;s Guide to Critical Ingredients for Designing Effective Conversational Systems. (arXiv:2307.07255v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07255</link>
<description rdf:parseType="Literal">&lt;p&gt;Sharing ideas through communication with peers is the primary mode of human
interaction. Consequently, extensive research has been conducted in the area of
conversational AI, leading to an increase in the availability and diversity of
conversational tasks, datasets, and methods. However, with numerous tasks being
explored simultaneously, the current landscape of conversational AI becomes
fragmented. Therefore, initiating a well-thought-out model for a dialogue agent
can pose significant challenges for a practitioner. Towards highlighting the
critical ingredients needed for a practitioner to design a dialogue agent from
scratch, the current study provides a comprehensive overview of the primary
characteristics of a dialogue agent, the supporting tasks, their corresponding
open-domain datasets, and the methods used to benchmark these datasets. We
observe that different methods have been used to tackle distinct dialogue
tasks. However, building separate models for each task is costly and does not
leverage the correlation among the several tasks of a dialogue agent. As a
result, recent trends suggest a shift towards building unified foundation
models. To this end, we propose UNIT, a UNified dIalogue dataseT constructed
from conversations of existing datasets for different dialogue tasks capturing
the nuances for each of them. We also examine the evaluation strategies used to
measure the performance of dialogue agents and highlight the scope for future
research in the area of conversational AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Shivani Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_S/0/1/0/all/0/1&quot;&gt;Sumit Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aggarwal_M/0/1/0/all/0/1&quot;&gt;Milan Aggarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmoy Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07258">
<title>Improving BERT with Hybrid Pooling Network and Drop Mask. (arXiv:2307.07258v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07258</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based pre-trained language models, such as BERT, achieve great
success in various natural language understanding tasks. Prior research found
that BERT captures a rich hierarchy of linguistic information at different
layers. However, the vanilla BERT uses the same self-attention mechanism for
each layer to model the different contextual features. In this paper, we
propose a HybridBERT model which combines self-attention and pooling networks
to encode different contextual features in each layer. Additionally, we propose
a simple DropMask method to address the mismatch between pre-training and
fine-tuning caused by excessive use of special mask tokens during Masked
Language Modeling pre-training. Experiments show that HybridBERT outperforms
BERT in pre-training with lower loss, faster training speed (8% relative),
lower memory cost (13% relative), and also in transfer learning with 1.5%
relative higher accuracies on downstream tasks. Additionally, DropMask improves
accuracies of BERT on downstream tasks across various masking rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qinglin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_C/0/1/0/all/0/1&quot;&gt;Chong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yukun_M/0/1/0/all/0/1&quot;&gt;Ma Yukun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Siqi Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07262">
<title>MorphPiece : Moving away from Statistical Language Representation. (arXiv:2307.07262v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07262</link>
<description rdf:parseType="Literal">&lt;p&gt;Tokenization is a critical part of modern NLP pipelines. However,
contemporary tokenizers for Large Language Models are based on statistical
analysis of text corpora, without much consideration to the linguistic
features. We propose a linguistically motivated tokenization scheme,
MorphPiece, which is based partly on morphological segmentation of the
underlying text. A GPT-style causal language model trained on this tokenizer
(called MorphGPT) shows superior convergence compared to the same architecture
trained on a standard BPE tokenizer. Specifically we get Language Modeling
performance comparable to a 6 times larger model. Additionally, we evaluate
MorphGPT on a variety of NLP tasks in supervised and unsupervised settings and
find superior performance across the board, compared to GPT-2 model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jabbar_H/0/1/0/all/0/1&quot;&gt;Haris Jabbar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07277">
<title>Are words equally surprising in audio and audio-visual comprehension?. (arXiv:2307.07277v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07277</link>
<description rdf:parseType="Literal">&lt;p&gt;We report a controlled study investigating the effect of visual information
(i.e., seeing the speaker) on spoken language comprehension. We compare the ERP
signature (N400) associated with each word in audio-only and audio-visual
presentations of the same verbal stimuli. We assess the extent to which
surprisal measures (which quantify the predictability of words in their lexical
context) are generated on the basis of different types of language models
(specifically n-gram and Transformer models) that predict N400 responses for
each word. Our results indicate that cognitive effort differs significantly
between multimodal and unimodal settings. In addition, our findings suggest
that while Transformer-based models, which have access to a larger lexical
context, provide a better fit in the audio-only setting, 2-gram language models
are more effective in the multimodal setting. This highlights the significant
impact of local lexical context on cognitive processing in a multimodal
environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madhyastha_P/0/1/0/all/0/1&quot;&gt;Pranava Madhyastha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vigliocco_G/0/1/0/all/0/1&quot;&gt;Gabriella Vigliocco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07280">
<title>Replay to Remember: Continual Layer-Specific Fine-tuning for German Speech Recognition. (arXiv:2307.07280v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07280</link>
<description rdf:parseType="Literal">&lt;p&gt;While Automatic Speech Recognition (ASR) models have shown significant
advances with the introduction of unsupervised or self-supervised training
techniques, these improvements are still only limited to a subsection of
languages and speakers. Transfer learning enables the adaptation of large-scale
multilingual models to not only low-resource languages but also to more
specific speaker groups. However, fine-tuning on data from new domains is
usually accompanied by a decrease in performance on the original domain.
Therefore, in our experiments, we examine how well the performance of
large-scale ASR models can be approximated for smaller domains, with our own
dataset of German Senior Voice Commands (SVC-de), and how much of the general
speech recognition performance can be preserved by selectively freezing parts
of the model during training. To further increase the robustness of the ASR
model to vocabulary and speakers outside of the fine-tuned domain, we apply
Experience Replay for continual learning. By adding only a fraction of data
from the original domain, we are able to reach Word-Error-Rates (WERs) below
5\% on the new domain, while stabilizing performance for general speech
recognition at acceptable WERs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosin_T/0/1/0/all/0/1&quot;&gt;Theresa Pekarek Rosin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wermter_S/0/1/0/all/0/1&quot;&gt;Stefan Wermter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07295">
<title>Towards dialect-inclusive recognition in a low-resource language: are balanced corpora the answer?. (arXiv:2307.07295v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07295</link>
<description rdf:parseType="Literal">&lt;p&gt;ASR systems are generally built for the spoken &apos;standard&apos;, and their
performance declines for non-standard dialects/varieties. This is a problem for
a language like Irish, where there is no single spoken standard, but rather
three major dialects: Ulster (Ul), Connacht (Co) and Munster (Mu). As a
diagnostic to quantify the effect of the speaker&apos;s dialect on recognition
performance, 12 ASR systems were trained, firstly using baseline
dialect-balanced training corpora, and then using modified versions of the
baseline corpora, where dialect-specific materials were either subtracted or
added. Results indicate that dialect-balanced corpora do not yield a similar
performance across the dialects: the Ul dialect consistently underperforms,
whereas Mu yields lowest WERs. There is a close relationship between Co and Mu
dialects, but one that is not symmetrical. These results will guide future
corpus collection and system building strategies to optimise for cross-dialect
performance equity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lonergan_L/0/1/0/all/0/1&quot;&gt;Liam Lonergan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1&quot;&gt;Mengjie Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiarain_N/0/1/0/all/0/1&quot;&gt;Neasa N&amp;#xed; Chiar&amp;#xe1;in&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gobl_C/0/1/0/all/0/1&quot;&gt;Christer Gobl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chasaide_A/0/1/0/all/0/1&quot;&gt;Ailbhe N&amp;#xed; Chasaide&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07306">
<title>C3: Zero-shot Text-to-SQL with ChatGPT. (arXiv:2307.07306v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07306</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a ChatGPT-based zero-shot Text-to-SQL method, dubbed C3,
which achieves 82.3\% in terms of execution accuracy on the holdout test set of
Spider and becomes the state-of-the-art zero-shot Text-to-SQL method on the
Spider Challenge. C3 consists of three key components: Clear Prompting (CP),
Calibration with Hints (CH), and Consistent Output (CO), which are
corresponding to the model input, model bias and model output respectively. It
provides a systematic treatment for zero-shot Text-to-SQL. Extensive
experiments have been conducted to verify the effectiveness and efficiency of
our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xuemei Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuhang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yuren Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yunjun Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_l/0/1/0/all/0/1&quot;&gt;lu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jinshu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_D/0/1/0/all/0/1&quot;&gt;Dongfang Lou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07312">
<title>Using Large Language Models for Zero-Shot Natural Language Generation from Knowledge Graphs. (arXiv:2307.07312v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07312</link>
<description rdf:parseType="Literal">&lt;p&gt;In any system that uses structured knowledge graph (KG) data as its
underlying knowledge representation, KG-to-text generation is a useful tool for
turning parts of the graph data into text that can be understood by humans.
Recent work has shown that models that make use of pretraining on large amounts
of text data can perform well on the KG-to-text task even with relatively small
sets of training data on the specific graph-to-text task. In this paper, we
build on this concept by using large language models to perform zero-shot
generation based on nothing but the model&apos;s understanding of the triple
structure from what it can read. We show that ChatGPT achieves near
state-of-the-art performance on some measures of the WebNLG 2020 challenge, but
falls behind on others. Additionally, we compare factual, counter-factual and
fictional statements, and show that there is a significant connection between
what the LLM already knows about the data it is parsing and the quality of the
output text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Axelsson_A/0/1/0/all/0/1&quot;&gt;Agnes Axelsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Skantze_G/0/1/0/all/0/1&quot;&gt;Gabriel Skantze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07317">
<title>Hybrid moderation in the newsroom: Recommending featured posts to content moderators. (arXiv:2307.07317v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.07317</link>
<description rdf:parseType="Literal">&lt;p&gt;Online news outlets are grappling with the moderation of user-generated
content within their comment section. We present a recommender system based on
ranking class probabilities to support and empower the moderator in choosing
featured posts, a time-consuming task. By combining user and textual content
features we obtain an optimal classification F1-score of 0.44 on the test set.
Furthermore, we observe an optimum mean NDCG@5 of 0.87 on a large set of
validation articles. As an expert evaluation, content moderators assessed the
output of a random selection of articles by choosing comments to feature based
on the recommendations, which resulted in a NDCG score of 0.83. We conclude
that first, adding text features yields the best score and second, while
choosing featured content remains somewhat subjective, content moderators found
suitable comments in all but one evaluated recommendations. We end the paper by
analyzing our best-performing model, a step towards transparency and
explainability in hybrid content moderation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waterschoot_C/0/1/0/all/0/1&quot;&gt;Cedric Waterschoot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosch_A/0/1/0/all/0/1&quot;&gt;Antal van den Bosch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07331">
<title>How Different Is Stereotypical Bias Across Languages?. (arXiv:2307.07331v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07331</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have demonstrated how to assess the stereotypical bias in
pre-trained English language models. In this work, we extend this branch of
research in multiple different dimensions by systematically investigating (a)
mono- and multilingual models of (b) different underlying architectures with
respect to their bias in (c) multiple different languages. To that end, we make
use of the English StereoSet data set (Nadeem et al., 2021), which we
semi-automatically translate into German, French, Spanish, and Turkish. We find
that it is of major importance to conduct this type of analysis in a
multilingual setting, as our experiments show a much more nuanced picture as
well as notable differences from the English-only analysis. The main takeaways
from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical
behavior across languages, English (monolingual) models exhibit the strongest
bias, and the stereotypes reflected in the data set are least present in
Turkish models. Finally, we release our codebase alongside the translated data
sets and practical guidelines for the semi-automatic translation to encourage a
further extension of our work to other languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozturk_I/0/1/0/all/0/1&quot;&gt;Ibrahim Tolga &amp;#xd6;zt&amp;#xfc;rk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nedelchev_R/0/1/0/all/0/1&quot;&gt;Rostislav Nedelchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heumann_C/0/1/0/all/0/1&quot;&gt;Christian Heumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arias_E/0/1/0/all/0/1&quot;&gt;Esteban Garces Arias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roger_M/0/1/0/all/0/1&quot;&gt;Marius Roger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischl_B/0/1/0/all/0/1&quot;&gt;Bernd Bischl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Assenmacher_M/0/1/0/all/0/1&quot;&gt;Matthias A&amp;#xdf;enmacher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07361">
<title>Gloss Attention for Gloss-free Sign Language Translation. (arXiv:2307.07361v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07361</link>
<description rdf:parseType="Literal">&lt;p&gt;Most sign language translation (SLT) methods to date require the use of gloss
annotations to provide additional supervision information, however, the
acquisition of gloss is not easy. To solve this problem, we first perform an
analysis of existing models to confirm how gloss annotations make SLT easier.
We find that it can provide two aspects of information for the model, 1) it can
help the model implicitly learn the location of semantic boundaries in
continuous sign language videos, 2) it can help the model understand the sign
language video globally. We then propose \emph{gloss attention}, which enables
the model to keep its attention within video segments that have the same
semantics locally, just as gloss helps existing models do. Furthermore, we
transfer the knowledge of sentence-to-sentence similarity from the natural
language model to our gloss attention SLT network (GASLT) to help it understand
sign language videos at the sentence level. Experimental results on multiple
large-scale sign language datasets show that our proposed GASLT model
significantly outperforms existing methods. Our code is provided in
\url{https://github.com/YinAoXiong/GASLT}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_A/0/1/0/all/0/1&quot;&gt;Aoxiong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_T/0/1/0/all/0/1&quot;&gt;Tianyun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Li Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_W/0/1/0/all/0/1&quot;&gt;Weike Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_T/0/1/0/all/0/1&quot;&gt;Tao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhou Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07362">
<title>A scoping review on multimodal deep learning in biomedical images and texts. (arXiv:2307.07362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.07362</link>
<description rdf:parseType="Literal">&lt;p&gt;Computer-assisted diagnostic and prognostic systems of the future should be
capable of simultaneously processing multimodal data. Multimodal deep learning
(MDL), which involves the integration of multiple sources of data, such as
images and text, has the potential to revolutionize the analysis and
interpretation of biomedical data. However, it only caught researchers&apos;
attention recently. To this end, there is a critical need to conduct a
systematic review on this topic, identify the limitations of current work, and
explore future directions. In this scoping review, we aim to provide a
comprehensive overview of the current state of the field and identify key
concepts, types of studies, and research gaps with a focus on biomedical images
and texts joint learning, mainly because these two were the most commonly
available data types in MDL research. This study reviewed the current uses of
multimodal deep learning on five tasks: (1) Report generation, (2) Visual
question answering, (3) Cross-modal retrieval, (4) Computer-aided diagnosis,
and (5) Semantic segmentation. Our results highlight the diverse applications
and potential of MDL and suggest directions for future research in the field.
We hope our review will facilitate the collaboration of natural language
processing (NLP) and medical imaging communities and support the next
generation of decision-making and computer-assisted diagnostic system
development.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaoyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingquan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Q/0/1/0/all/0/1&quot;&gt;Qingqing Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qianqian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yifan Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07380">
<title>Composition-contrastive Learning for Sentence Embeddings. (arXiv:2307.07380v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07380</link>
<description rdf:parseType="Literal">&lt;p&gt;Vector representations of natural language are ubiquitous in search
applications. Recently, various methods based on contrastive learning have been
proposed to learn textual representations from unlabelled data; by maximizing
alignment between minimally-perturbed embeddings of the same text, and
encouraging a uniform distribution of embeddings across a broader corpus.
Differently, we propose maximizing alignment between texts and a composition of
their phrasal constituents. We consider several realizations of this objective
and elaborate the impact on representations in each case. Experimental results
on semantic textual similarity tasks show improvements over baselines that are
comparable with state-of-the-art approaches. Moreover, this work is the first
to do so without incurring costs in auxiliary training objectives or additional
network parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanchani_S/0/1/0/all/0/1&quot;&gt;Sachin J. Chanchani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_R/0/1/0/all/0/1&quot;&gt;Ruihong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07392">
<title>Rank Your Summaries: Enhancing Bengali Text Summarization via Ranking-based Approach. (arXiv:2307.07392v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07392</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing need for text summarization techniques that are both
efficient and accurate, it becomes crucial to explore avenues that enhance the
quality and precision of pre-trained models specifically tailored for
summarizing Bengali texts. When it comes to text summarization tasks, there are
numerous pre-trained transformer models at one&apos;s disposal. Consequently, it
becomes quite a challenge to discern the most informative and relevant summary
for a given text among the various options generated by these pre-trained
summarization models. This paper aims to identify the most accurate and
informative summary for a given text by utilizing a simple but effective
ranking-based approach that compares the output of four different pre-trained
Bengali text summarization models. The process begins by carrying out
preprocessing of the input text that involves eliminating unnecessary elements
such as special characters and punctuation marks. Next, we utilize four
pre-trained summarization models to generate summaries, followed by applying a
text ranking algorithm to identify the most suitable summary. Ultimately, the
summary with the highest ranking score is chosen as the final one. To evaluate
the effectiveness of this approach, the generated summaries are compared
against human-annotated summaries using standard NLG metrics such as BLEU,
ROUGE, BERTScore, WIL, WER, and METEOR. Experimental results suggest that by
leveraging the strengths of each pre-trained transformer model and combining
them using a ranking-based approach, our methodology significantly improves the
accuracy and effectiveness of the Bengali text summarization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahariar_G/0/1/0/all/0/1&quot;&gt;G. M. Shahariar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukder_T/0/1/0/all/0/1&quot;&gt;Tonmoy Talukder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sotez_R/0/1/0/all/0/1&quot;&gt;Rafin Alam Khan Sotez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shawon_M/0/1/0/all/0/1&quot;&gt;Md. Tanvir Rouf Shawon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07407">
<title>Phoneme-retrieval; voice recognition; vowels recognition. (arXiv:2307.07407v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07407</link>
<description rdf:parseType="Literal">&lt;p&gt;A phoneme-retrieval technique is proposed, which is due to the particular way
of the construction of the network. An initial set of neurons is given. The
number of these neurons is approximately equal to the number of typical
structures of the data. For example if the network is built for voice retrieval
then the number of neurons must be equal to the number of characteristic
phonemes of the alphabet of the language spoken by the social group to which
the particular person belongs. Usually this task is very complicated and the
network can depend critically on the samples used for the learning. If the
network is built for image retrieval then it works only if the data to be
retrieved belong to a particular set of images. If the network is built for
voice recognition it works only for some particular set of words. A typical
example is the words used for the flight of airplanes. For example a command
like the &quot;airplane should make a turn of 120 degrees towards the east&quot; can be
easily recognized by the network if a suitable learning procedure is used.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirozzi_B/0/1/0/all/0/1&quot;&gt;Brunello Tirozzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecian_O/0/1/0/all/0/1&quot;&gt;Orchidea Maria Lecian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07409">
<title>KU-DMIS-MSRA at RadSum23: Pre-trained Vision-Language Model for Radiology Report Summarization. (arXiv:2307.07409v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07409</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce CheXOFA, a new pre-trained vision-language model
(VLM) for the chest X-ray domain. Our model is initially pre-trained on various
multimodal datasets within the general domain before being transferred to the
chest X-ray domain. Following a prominent VLM, we unify various domain-specific
tasks into a simple sequence-to-sequence schema. It enables the model to
effectively learn the required knowledge and skills from limited resources in
the domain. Demonstrating superior performance on the benchmark datasets
provided by the BioNLP shared task, our model benefits from its training across
multiple tasks and domains. With subtle techniques including ensemble and
factual calibration, our system achieves first place on the RadSum23
leaderboard for the hidden test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gangwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hajung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seongsu Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_C/0/1/0/all/0/1&quot;&gt;Chanhwi Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sung_M/0/1/0/all/0/1&quot;&gt;Mujeen Sung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunjae Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_E/0/1/0/all/0/1&quot;&gt;Eric Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jaewoo Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07411">
<title>Detecting LLM-Generated Text in Computing Education: A Comparative Study for ChatGPT Cases. (arXiv:2307.07411v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07411</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the recent improvements and wide availability of Large Language Models
(LLMs), they have posed a serious threat to academic integrity in education.
Modern LLM-generated text detectors attempt to combat the problem by offering
educators with services to assess whether some text is LLM-generated. In this
work, we have collected 124 submissions from computer science students before
the creation of ChatGPT. We then generated 40 ChatGPT submissions. We used this
data to evaluate eight publicly-available LLM-generated text detectors through
the measures of accuracy, false positives, and resilience. The purpose of this
work is to inform the community of what LLM-generated text detectors work and
which do not, but also to provide insights for educators to better maintain
academic integrity in their courses. Our results find that CopyLeaks is the
most accurate LLM-generated text detector, GPTKit is the best LLM-generated
text detector to reduce false positives, and GLTR is the most resilient
LLM-generated text detector. We also express concerns over 52 false positives
(of 114 human written submissions) generated by GPTZero. Finally, we note that
all LLM-generated text detectors are less accurate with code, other languages
(aside from English), and after the use of paraphrasing tools (like QuillBot).
Modern detectors are still in need of improvements so that they can offer a
full-proof solution to help maintain academic integrity. Further, their
usability can be improved by facilitating a smooth API integration, providing
clear documentation of their features and the understandability of their
model(s), and supporting more commonly used languages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orenstrakh_M/0/1/0/all/0/1&quot;&gt;Michael Sheinman Orenstrakh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnalim_O/0/1/0/all/0/1&quot;&gt;Oscar Karnalim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suarez_C/0/1/0/all/0/1&quot;&gt;Carlos Anibal Suarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liut_M/0/1/0/all/0/1&quot;&gt;Michael Liut&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07412">
<title>HuCurl: Human-induced Curriculum Discovery. (arXiv:2307.07412v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07412</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce the problem of curriculum discovery and describe a curriculum
learning framework capable of discovering effective curricula in a curriculum
space based on prior knowledge about sample difficulty. Using annotation
entropy and loss as measures of difficulty, we show that (i): the
top-performing discovered curricula for a given model and dataset are often
non-monotonic as opposed to monotonic curricula in existing literature, (ii):
the prevailing easy-to-hard or hard-to-easy transition curricula are often at
the risk of underperforming, and (iii): the curricula discovered for smaller
datasets and models perform well on larger datasets and models respectively.
The proposed framework encompasses some of the existing curriculum learning
approaches and can discover curricula that outperform them across several NLP
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elgaar_M/0/1/0/all/0/1&quot;&gt;Mohamed Elgaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_H/0/1/0/all/0/1&quot;&gt;Hadi Amiri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07415">
<title>AutoHint: Automatic Prompt Optimization with Hint Generation. (arXiv:2307.07415v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07415</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents AutoHint, a novel framework for automatic prompt
engineering and optimization for Large Language Models (LLM). While LLMs have
demonstrated remarkable ability in achieving high-quality annotation in various
tasks, the key to applying this ability to specific tasks lies in developing
high-quality prompts. Thus we propose a framework to inherit the merits of both
in-context learning and zero-shot learning by incorporating enriched
instructions derived from input-output demonstrations to optimize original
prompt. We refer to the enrichment as the hint and propose a framework to
automatically generate the hint from labeled data. More concretely, starting
from an initial prompt, our method first instructs a LLM to deduce new hints
for selected samples from incorrect predictions, and then summarizes from
per-sample hints and adds the results back to the initial prompt to form a new,
enriched instruction. The proposed method is evaluated on the BIG-Bench
Instruction Induction dataset for both zero-shot and few-short prompts, where
experiments demonstrate our method is able to significantly boost accuracy for
multiple tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Hong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yinchuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homma_Y/0/1/0/all/0/1&quot;&gt;Youkow Homma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qi Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_M/0/1/0/all/0/1&quot;&gt;Min Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jian Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charles_D/0/1/0/all/0/1&quot;&gt;Denis Charles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07417">
<title>RoPDA: Robust Prompt-based Data Augmentation for Low-Resource Named Entity Recognition. (arXiv:2307.07417v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07417</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation has been widely used in low-resource NER tasks to tackle
the problem of data sparsity. However, previous data augmentation methods have
the disadvantages of disrupted syntactic structures, token-label mismatch, and
requirement for external knowledge or manual effort. To address these issues,
we propose \textbf{Ro}bust \textbf{P}rompt-based \textbf{D}ata
\textbf{A}ugmentation (RoPDA) for low-resource NER. Based on pre-trained
language models (PLMs) with continuous prompt, RoPDA performs entity
augmentation and context augmentation through five fundamental augmentation
operations to generate label-flipping and label-preserving examples. To
optimize the utilization of the augmented samples, we present two techniques:
Self-Consistency Filtering and mixup. The former effectively eliminates
low-quality samples, while the latter prevents performance degradation arising
from the direct utilization of label-flipping samples. Extensive experiments on
three benchmarks from different domains demonstrate that RoPDA significantly
improves upon strong baselines, and also outperforms state-of-the-art
semi-supervised learning methods when unlabeled data is included.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sihan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_F/0/1/0/all/0/1&quot;&gt;Furao Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jian Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07420">
<title>Named entity recognition using GPT for identifying comparable companies. (arXiv:2307.07420v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07420</link>
<description rdf:parseType="Literal">&lt;p&gt;For both public and private firms, comparable companies analysis is widely
used as a method for company valuation. In particular, the method is of great
value for valuation of private equity companies. The several approaches to the
comparable companies method usually rely on a qualitative approach to
identifying similar peer companies, which tends to use established industry
classification schemes and/or analyst intuition and knowledge. However, more
quantitative methods have started being used in the literature and in the
private equity industry, in particular, machine learning clustering, and
natural language processing (NLP). For NLP methods, the process consists of
extracting product entities from e.g., the company&apos;s website or company
descriptions from some financial database system and then to perform similarity
analysis. Here, using companies descriptions/summaries from publicly available
companies&apos; Wikipedia websites, we show that using large language models (LLMs),
such as GPT from openaAI, has a much higher precision and success rate than
using the standard named entity recognition (NER) which uses manual annotation.
We demonstrate quantitatively a higher precision rate, and show that,
qualitatively, it can be used to create appropriate comparable companies peer
groups which can then be used for equity valuation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Covas_E/0/1/0/all/0/1&quot;&gt;Eurico Covas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07421">
<title>Sumformer: A Linear-Complexity Alternative to Self-Attention for Speech Recognition. (arXiv:2307.07421v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07421</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern speech recognition systems rely on self-attention. Unfortunately,
token mixing with self-attention takes quadratic time in the length of the
speech utterance, slowing down inference as well as training and increasing
memory consumption. Cheaper alternatives to self-attention for ASR have been
developed, but fail to consistently reach the same level of accuracy. In
practice, however, the self-attention weights of trained speech recognizers
take the form of a global average over time. This paper, therefore, proposes a
linear-time alternative to self-attention for speech recognition. It summarises
a whole utterance with the mean over vectors for all time steps. This single
summary is then combined with time-specific information. We call this method
``Summary Mixing&apos;&apos;. Introducing Summary Mixing in state-of-the-art ASR models
makes it feasible to preserve or exceed previous speech recognition performance
while lowering the training and inference times by up to 27% and reducing the
memory budget by a factor of two.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parcollet_T/0/1/0/all/0/1&quot;&gt;Titouan Parcollet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalen_R/0/1/0/all/0/1&quot;&gt;Rogier van Dalen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shucong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharya_S/0/1/0/all/0/1&quot;&gt;Sourav Bhattacharya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07422">
<title>Can LLMs be Good Financial Advisors?: An Initial Study in Personal Decision Making for Optimized Outcomes. (arXiv:2307.07422v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07422</link>
<description rdf:parseType="Literal">&lt;p&gt;Increasingly powerful Large Language Model (LLM) based chatbots, like ChatGPT
and Bard, are becoming available to users that have the potential to
revolutionize the quality of decision-making achieved by the public. In this
context, we set out to investigate how such systems perform in the personal
finance domain, where financial inclusion has been an overarching stated aim of
banks for decades. We asked 13 questions representing banking products in
personal finance: bank account, credit card, and certificate of deposits and
their inter-product interactions, and decisions related to high-value
purchases, payment of bank dues, and investment advice, and in different
dialects and languages (English, African American Vernacular English, and
Telugu). We find that although the outputs of the chatbots are fluent and
plausible, there are still critical gaps in providing accurate and reliable
financial information using LLM-based chatbots.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lakkaraju_K/0/1/0/all/0/1&quot;&gt;Kausik Lakkaraju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuruma_S/0/1/0/all/0/1&quot;&gt;Sai Krishna Revanth Vuruma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pallagani_V/0/1/0/all/0/1&quot;&gt;Vishal Pallagani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muppasani_B/0/1/0/all/0/1&quot;&gt;Bharath Muppasani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_B/0/1/0/all/0/1&quot;&gt;Biplav Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07436">
<title>Towards spoken dialect identification of Irish. (arXiv:2307.07436v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.07436</link>
<description rdf:parseType="Literal">&lt;p&gt;The Irish language is rich in its diversity of dialects and accents. This
compounds the difficulty of creating a speech recognition system for the
low-resource language, as such a system must contend with a high degree of
variability with limited corpora. A recent study investigating dialect bias in
Irish ASR found that balanced training corpora gave rise to unequal dialect
performance, with performance for the Ulster dialect being consistently worse
than for the Connacht or Munster dialects. Motivated by this, the present
experiments investigate spoken dialect identification of Irish, with a view to
incorporating such a system into the speech recognition pipeline. Two acoustic
classification models are tested, XLS-R and ECAPA-TDNN, in conjunction with a
text-based classifier using a pretrained Irish-language BERT model. The
ECAPA-TDNN, particularly a model pretrained for language identification on the
VoxLingua107 dataset, performed best overall, with an accuracy of 73%. This was
further improved to 76% by fusing the model&apos;s outputs with the text-based
model. The Ulster dialect was most accurately identified, with an accuracy of
94%, however the model struggled to disambiguate between the Connacht and
Munster dialects, suggesting a more nuanced approach may be necessary to
robustly distinguish between the dialects of Irish.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lonergan_L/0/1/0/all/0/1&quot;&gt;Liam Lonergan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_M/0/1/0/all/0/1&quot;&gt;Mengjie Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiarain_N/0/1/0/all/0/1&quot;&gt;Neasa N&amp;#xed; Chiar&amp;#xe1;in&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gobl_C/0/1/0/all/0/1&quot;&gt;Christer Gobl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chasaide_A/0/1/0/all/0/1&quot;&gt;Ailbhe N&amp;#xed; Chasaide&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07477">
<title>Population Expansion for Training Language Models with Private Federated Learning. (arXiv:2307.07477v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.07477</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) combined with differential privacy (DP) offers
machine learning (ML) training with distributed devices and with a formal
privacy guarantee. With a large population of devices, FL with DP produces a
performant model in a timely manner. However, for applications with a smaller
population, not only does the model utility degrade as the DP noise is
inversely proportional to population, but also the training latency increases
since waiting for enough clients to become available from a smaller pool is
slower. In this work, we thus propose expanding the population based on domain
adaptation techniques to speed up the training and improves the final model
quality when training with small populations. We empirically demonstrate that
our techniques can improve the utility by 13% to 30% on real-world language
modeling datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koga_T/0/1/0/all/0/1&quot;&gt;Tatsuki Koga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Congzheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelikan_M/0/1/0/all/0/1&quot;&gt;Martin Pelikan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chitnis_M/0/1/0/all/0/1&quot;&gt;Mona Chitnis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08372">
<title>CodeQueries: A Dataset of Semantic Queries over Code. (arXiv:2209.08372v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08372</link>
<description rdf:parseType="Literal">&lt;p&gt;Developers often have questions about semantic aspects of code they are
working on, e.g., &quot;Is there a class whose parent classes declare a conflicting
attribute?&quot;. Answering them requires understanding code semantics such as
attributes and inheritance relation of classes. An answer to such a question
should identify code spans constituting the answer (e.g., the declaration of
the subclass) as well as supporting facts (e.g., the definitions of the
conflicting attributes). The existing work on question-answering over code has
considered yes/no questions or method-level context. We contribute a labeled
dataset, called CodeQueries, of semantic queries over Python code. Compared to
the existing datasets, in CodeQueries, the queries are about code semantics,
the context is file level and the answers are code spans. We curate the dataset
based on queries supported by a widely-used static analysis tool, CodeQL, and
include both positive and negative examples, and queries requiring single-hop
and multi-hop reasoning.
&lt;/p&gt;
&lt;p&gt;To assess the value of our dataset, we evaluate baseline neural approaches.
We study a large language model (GPT3.5-Turbo) in zero-shot and few-shot
settings on a subset of CodeQueries. We also evaluate a BERT style model
(CuBERT) with fine-tuning. We find that these models achieve limited success on
CodeQueries. CodeQueries is thus a challenging dataset to test the ability of
neural models, to understand code semantics, in the extractive
question-answering setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahu_S/0/1/0/all/0/1&quot;&gt;Surya Prakash Sahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_M/0/1/0/all/0/1&quot;&gt;Madhurima Mandal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bharadwaj_S/0/1/0/all/0/1&quot;&gt;Shikhar Bharadwaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanade_A/0/1/0/all/0/1&quot;&gt;Aditya Kanade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maniatis_P/0/1/0/all/0/1&quot;&gt;Petros Maniatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shevade_S/0/1/0/all/0/1&quot;&gt;Shirish Shevade&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.02762">
<title>Vision Transformer Based Model for Describing a Set of Images as a Story. (arXiv:2210.02762v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.02762</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Story-Telling is the process of forming a multi-sentence story from a
set of images. Appropriately including visual variation and contextual
information captured inside the input images is one of the most challenging
aspects of visual storytelling. Consequently, stories developed from a set of
images often lack cohesiveness, relevance, and semantic relationship. In this
paper, we propose a novel Vision Transformer Based Model for describing a set
of images as a story. The proposed method extracts the distinct features of the
input images using a Vision Transformer (ViT). Firstly, input images are
divided into 16X16 patches and bundled into a linear projection of flattened
patches. The transformation from a single image to multiple image patches
captures the visual variety of the input visual patterns. These features are
used as input to a Bidirectional-LSTM which is part of the sequence encoder.
This captures the past and future image context of all image patches. Then, an
attention mechanism is implemented and used to increase the discriminatory
capacity of the data fed into the language model, i.e. a Mogrifier-LSTM. The
performance of our proposed model is evaluated using the Visual Story-Telling
dataset (VIST), and the results show that our model outperforms the current
state of the art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malakan_Z/0/1/0/all/0/1&quot;&gt;Zainy M. Malakan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassan_G/0/1/0/all/0/1&quot;&gt;Ghulam Mubashar Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1&quot;&gt;Ajmal Mian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11584">
<title>Dialogs Re-enacted Across Languages. (arXiv:2211.11584v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11584</link>
<description rdf:parseType="Literal">&lt;p&gt;To support machine learning of cross-language prosodic mappings and other
ways to improve speech-to-speech translation, we present a protocol for
collecting closely matched pairs of utterances across languages, a description
of the resulting data collection and its public release, and some observations
and musings. This report is intended for: people using this corpus, people
extending this corpus, and people designing similar collections of bilingual
dialog data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ward_N/0/1/0/all/0/1&quot;&gt;Nigel G. Ward&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avila_J/0/1/0/all/0/1&quot;&gt;Jonathan E. Avila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivas_E/0/1/0/all/0/1&quot;&gt;Emilia Rivas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marco_D/0/1/0/all/0/1&quot;&gt;Divette Marco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.14301">
<title>On the Effect of Anticipation on Reading Times. (arXiv:2211.14301v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2211.14301</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past two decades, numerous studies have demonstrated how less
predictable (i.e., higher surprisal) words take more time to read. In general,
these studies have implicitly assumed the reading process is purely responsive:
Readers observe a new word and allocate time to process it as required. We
argue that prior results are also compatible with a reading process that is at
least partially anticipatory: Readers could make predictions about a future
word and allocate time to process it based on their expectation. In this work,
we operationalize this anticipation as a word&apos;s contextual entropy. We assess
the effect of anticipation on reading by comparing how well surprisal and
contextual entropy predict reading times on four naturalistic reading datasets:
two self-paced and two eye-tracking. Experimentally, across datasets and
analyses, we find substantial evidence for effects of contextual entropy over
surprisal on a word&apos;s reading time (RT): in fact, entropy is sometimes better
than surprisal in predicting a word&apos;s RT. Spillover effects, however, are
generally not captured by entropy, but only by surprisal. Further, we
hypothesize four cognitive mechanisms through which contextual entropy could
impact RTs -- three of which we are able to design experiments to analyze.
Overall, our results support a view of reading that is not just responsive, but
also anticipatory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1&quot;&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meister_C/0/1/0/all/0/1&quot;&gt;Clara Meister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1&quot;&gt;Ethan G. Wilcox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_R/0/1/0/all/0/1&quot;&gt;Roger Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.00552">
<title>An Effective Deployment of Contrastive Learning in Multi-label Text Classification. (arXiv:2212.00552v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.00552</link>
<description rdf:parseType="Literal">&lt;p&gt;The effectiveness of contrastive learning technology in natural language
processing tasks is yet to be explored and analyzed. How to construct positive
and negative samples correctly and reasonably is the core challenge of
contrastive learning. It is even harder to discover contrastive objects in
multi-label text classification tasks. There are very few contrastive losses
proposed previously. In this paper, we investigate the problem from a different
angle by proposing five novel contrastive losses for multi-label text
classification tasks. These are Strict Contrastive Loss (SCL), Intra-label
Contrastive Loss (ICL), Jaccard Similarity Contrastive Loss (JSCL), Jaccard
Similarity Probability Contrastive Loss (JSPCL), and Stepwise Label Contrastive
Loss (SLCL). We explore the effectiveness of contrastive learning for
multi-label text classification tasks by the employment of these novel losses
and provide a set of baseline models for deploying contrastive learning
techniques on specific tasks. We further perform an interpretable analysis of
our approach to show how different components of contrastive learning losses
play their roles. The experimental results show that our proposed contrastive
losses can bring improvement to multi-label text classification tasks. Our work
also explores how contrastive learning should be adapted for multi-label text
classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_N/0/1/0/all/0/1&quot;&gt;Nankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_G/0/1/0/all/0/1&quot;&gt;Guanqiu Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_A/0/1/0/all/0/1&quot;&gt;Aimin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Dong Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.04391">
<title>The Re-Label Method For Data-Centric Machine Learning. (arXiv:2302.04391v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.04391</link>
<description rdf:parseType="Literal">&lt;p&gt;In industry deep learning application, our manually labeled data has a
certain number of noisy data. To solve this problem and achieve more than 90
score in dev dataset, we present a simple method to find the noisy data and
re-label the noisy data by human, given the model predictions as references in
human labeling. In this paper, we illustrate our idea for a broad set of deep
learning tasks, includes classification, sequence tagging, object detection,
sequence generation, click-through rate prediction. The experimental results
and human evaluation results verify our idea.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_T/0/1/0/all/0/1&quot;&gt;Tong Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05063">
<title>ICL-D3IE: In-Context Learning with Diverse Demonstrations Updating for Document Information Extraction. (arXiv:2303.05063v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05063</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs), such as GPT-3 and ChatGPT, have demonstrated
remarkable results in various natural language processing (NLP) tasks with
in-context learning, which involves inference based on a few demonstration
examples. Despite their successes in NLP tasks, no investigation has been
conducted to assess the ability of LLMs to perform document information
extraction (DIE) using in-context learning. Applying LLMs to DIE poses two
challenges: the modality and task gap. To this end, we propose a simple but
effective in-context learning framework called ICL-D3IE, which enables LLMs to
perform DIE with different types of demonstration examples. Specifically, we
extract the most difficult and distinct segments from hard training documents
as hard demonstrations for benefiting all test instances. We design
demonstrations describing relationships that enable LLMs to understand
positional relationships. We introduce formatting demonstrations for easy
answer extraction. Additionally, the framework improves diverse demonstrations
by updating them iteratively. Our experiments on three widely used benchmark
datasets demonstrate that the ICL-D3IE framework enables GPT-3/ChatGPT to
achieve superior performance when compared to previous pre-trained methods
fine-tuned with full training in both the in-distribution (ID) setting and in
the out-of-distribution (OOD) setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiabang He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ning Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Heng Tao Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07274">
<title>Breaking Common Sense: WHOOPS! A Vision-and-Language Benchmark of Synthetic and Compositional Images. (arXiv:2303.07274v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07274</link>
<description rdf:parseType="Literal">&lt;p&gt;Weird, unusual, and uncanny images pique the curiosity of observers because
they challenge commonsense. For example, an image released during the 2022
world cup depicts the famous soccer stars Lionel Messi and Cristiano Ronaldo
playing chess, which playfully violates our expectation that their competition
should occur on the football field. Humans can easily recognize and interpret
these unconventional images, but can AI models do the same? We introduce
WHOOPS!, a new dataset and benchmark for visual commonsense. The dataset is
comprised of purposefully commonsense-defying images created by designers using
publicly-available image generation tools like Midjourney. We consider several
tasks posed over the dataset. In addition to image captioning, cross-modal
matching, and visual question answering, we introduce a difficult explanation
generation task, where models must identify and explain why a given image is
unusual. Our results show that state-of-the-art models such as GPT3 and BLIP2
still lag behind human performance on WHOOPS!. We hope our dataset will inspire
the development of AI models with stronger visual commonsense reasoning
abilities. Data, models and code are available at the project website:
whoops-benchmark.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Guetta_N/0/1/0/all/0/1&quot;&gt;Nitzan Bitton-Guetta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bitton_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bitton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_L/0/1/0/all/0/1&quot;&gt;Ludwig Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elovici_Y/0/1/0/all/0/1&quot;&gt;Yuval Elovici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanovsky_G/0/1/0/all/0/1&quot;&gt;Gabriel Stanovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_R/0/1/0/all/0/1&quot;&gt;Roy Schwartz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08763">
<title>A Survey for Biomedical Text Summarization: From Pre-trained to Large Language Models. (arXiv:2304.08763v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08763</link>
<description rdf:parseType="Literal">&lt;p&gt;The exponential growth of biomedical texts such as biomedical literature and
electronic health records (EHRs), poses a significant challenge for clinicians
and researchers to access clinical information efficiently. To tackle this
challenge, biomedical text summarization (BTS) has been proposed as a solution
to support clinical information retrieval and management. BTS aims at
generating concise summaries that distill key information from single or
multiple biomedical documents. In recent years, the rapid advancement of
fundamental natural language processing (NLP) techniques, from pre-trained
language models (PLMs) to large language models (LLMs), has greatly facilitated
the progress of BTS. This growth has led to numerous proposed summarization
methods, datasets, and evaluation metrics, raising the need for a comprehensive
and up-to-date survey for BTS. In this paper, we present a systematic review of
recent advancements in BTS, leveraging cutting-edge NLP techniques from PLMs to
LLMs, to help understand the latest progress, challenges, and future
directions. We begin by introducing the foundational concepts of BTS, PLMs and
LLMs, followed by an in-depth review of available datasets, recent approaches,
and evaluation metrics in BTS. We finally discuss existing challenges and
promising future directions in the era of LLMs. To facilitate the research
community, we line up open resources including available datasets, recent
approaches, codes, evaluation metrics, and the leaderboard in a public project:
https://github.com/KenZLuo/Biomedical-Text-Summarization-Survey/tree/master. We
believe that this survey will be a useful resource to researchers, allowing
them to quickly track recent advancements and provide guidelines for future BTS
research within the research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Qianqian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zheheng Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Benyou Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ananiadou_S/0/1/0/all/0/1&quot;&gt;Sophia Ananiadou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03642">
<title>Jointly Extracting Interventions, Outcomes, and Findings from RCT Reports with LLMs. (arXiv:2305.03642v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03642</link>
<description rdf:parseType="Literal">&lt;p&gt;Results from Randomized Controlled Trials (RCTs) establish the comparative
effectiveness of interventions, and are in turn critical inputs for
evidence-based care. However, results from RCTs are presented in (often
unstructured) natural language articles describing the design, execution, and
outcomes of trials; clinicians must manually extract findings pertaining to
interventions and outcomes of interest from such articles. This onerous manual
process has motivated work on (semi-)automating extraction of structured
evidence from trial reports. In this work we propose and evaluate a
text-to-text model built on instruction-tuned Large Language Models (LLMs) to
jointly extract Interventions, Outcomes, and Comparators (ICO elements) from
clinical abstracts, and infer the associated results reported. Manual (expert)
and automated evaluations indicate that framing evidence extraction as a
conditional generation task and fine-tuning LLMs for this purpose realizes
considerable ($\sim$20 point absolute F1 score) gains over the previous SOTA.
We perform ablations and error analyses to assess aspects that contribute to
model performance, and to highlight potential directions for further
improvements. We apply our model to a collection of published RCTs through
mid-2022, and release a searchable database of structured findings:
bit.ly/joint-relations-extraction-mlhc
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wadhwa_S/0/1/0/all/0/1&quot;&gt;Somin Wadhwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DeYoung_J/0/1/0/all/0/1&quot;&gt;Jay DeYoung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nye_B/0/1/0/all/0/1&quot;&gt;Benjamin Nye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amir_S/0/1/0/all/0/1&quot;&gt;Silvio Amir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wallace_B/0/1/0/all/0/1&quot;&gt;Byron C. Wallace&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14724">
<title>I Spy a Metaphor: Large Language Models and Diffusion Models Co-Create Visual Metaphors. (arXiv:2305.14724v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14724</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual metaphors are powerful rhetorical devices used to persuade or
communicate creative ideas through images. Similar to linguistic metaphors,
they convey meaning implicitly through symbolism and juxtaposition of the
symbols. We propose a new task of generating visual metaphors from linguistic
metaphors. This is a challenging task for diffusion-based text-to-image models,
such as DALL$\cdot$E 2, since it requires the ability to model implicit meaning
and compositionality. We propose to solve the task through the collaboration
between Large Language Models (LLMs) and Diffusion Models: Instruct GPT-3
(davinci-002) with Chain-of-Thought prompting generates text that represents a
visual elaboration of the linguistic metaphor containing the implicit meaning
and relevant objects, which is then used as input to the diffusion-based
text-to-image models.Using a human-AI collaboration framework, where humans
interact both with the LLM and the top-performing diffusion model, we create a
high-quality dataset containing 6,476 visual metaphors for 1,540 linguistic
metaphors and their associated visual elaborations. Evaluation by professional
illustrators shows the promise of LLM-Diffusion Model collaboration for this
task . To evaluate the utility of our Human-AI collaboration framework and the
quality of our dataset, we perform both an intrinsic human-based evaluation and
an extrinsic evaluation using visual entailment as a downstream task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakrabarty_T/0/1/0/all/0/1&quot;&gt;Tuhin Chakrabarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saakyan_A/0/1/0/all/0/1&quot;&gt;Arkadiy Saakyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winn_O/0/1/0/all/0/1&quot;&gt;Olivia Winn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panagopoulou_A/0/1/0/all/0/1&quot;&gt;Artemis Panagopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yue Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apidianaki_M/0/1/0/all/0/1&quot;&gt;Marianna Apidianaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muresan_S/0/1/0/all/0/1&quot;&gt;Smaranda Muresan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16265">
<title>UNITE: A Unified Benchmark for Text-to-SQL Evaluation. (arXiv:2305.16265v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16265</link>
<description rdf:parseType="Literal">&lt;p&gt;A practical text-to-SQL system should generalize well on a wide variety of
natural language questions, unseen database schemas, and novel SQL query
structures. To comprehensively evaluate text-to-SQL systems, we introduce a
UNIfied benchmark for Text-to-SQL Evaluation (UNITE). It is composed of
publicly available text-to-SQL datasets, containing natural language questions
from more than 12 domains, SQL queries from more than 3.9K patterns, and 29K
databases. Compared to the widely used Spider benchmark, we introduce
$\sim$120K additional examples and a threefold increase in SQL patterns, such
as comparative and boolean questions. We conduct a systematic study of six
state-of-the-art (SOTA) text-to-SQL parsers on our new benchmark and show that:
1) Codex performs surprisingly well on out-of-domain datasets; 2) specially
designed decoding methods (e.g. constrained beam search) can improve
performance for both in-domain and out-of-domain settings; 3) explicitly
modeling the relationship between questions and schemas further improves the
Seq2Seq models. More importantly, our benchmark presents key challenges towards
compositional generalization and robustness issues -- which these SOTA models
cannot address well. Our code and data processing script are available at
https://github.com/awslabs/unified-text2sql-benchmark
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_W/0/1/0/all/0/1&quot;&gt;Wuwei Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiguo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chauhan_A/0/1/0/all/0/1&quot;&gt;Anuj Chauhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Henghui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Alexander Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jiang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hang_C/0/1/0/all/0/1&quot;&gt;Chung-Wei Hang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lilien_J/0/1/0/all/0/1&quot;&gt;Joseph Lilien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yiqun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Lin Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_M/0/1/0/all/0/1&quot;&gt;Mingwen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiarong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ash_S/0/1/0/all/0/1&quot;&gt;Stephen Ash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castelli_V/0/1/0/all/0/1&quot;&gt;Vittorio Castelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_P/0/1/0/all/0/1&quot;&gt;Patrick Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_B/0/1/0/all/0/1&quot;&gt;Bing Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01789">
<title>Edit Distance based RL for RNNT decoding. (arXiv:2306.01789v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01789</link>
<description rdf:parseType="Literal">&lt;p&gt;RNN-T is currently considered the industry standard in ASR due to its
exceptional WERs in various benchmark tests and its ability to support seamless
streaming and longform transcription. However, its biggest drawback lies in the
significant discrepancy between its training and inference objectives. During
training, RNN-T maximizes all alignment probabilities by teacher forcing, while
during inference, it uses beam search which may not necessarily find the
maximum probable alignment. Additionally, RNN-T&apos;s inability to experience
mistakes during teacher forcing training makes it more problematic when a
mistake occurs in inference. To address this issue, this paper proposes a
Reinforcement Learning method that minimizes the gap between training and
inference time. Our Edit Distance based RL (EDRL) approach computes rewards
based on the edit distance, and trains the network at every action level. The
proposed approach yielded SoTA WERs on LibriSpeech for the 600M Conformer RNN-T
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1&quot;&gt;Dongseong Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_C/0/1/0/all/0/1&quot;&gt;Changwan Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sim_K/0/1/0/all/0/1&quot;&gt;Khe Chai Sim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12916">
<title>Cross-lingual Cross-temporal Summarization: Dataset, Models, Evaluation. (arXiv:2306.12916v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12916</link>
<description rdf:parseType="Literal">&lt;p&gt;While summarization has been extensively researched in natural language
processing (NLP), cross-lingual cross-temporal summarization (CLCTS) is a
largely unexplored area that has the potential to improve cross-cultural
accessibility and understanding. This paper comprehensively addresses the CLCTS
task, including dataset creation, modeling, and evaluation. We build the first
CLCTS corpus, leveraging historical fictive texts and Wikipedia summaries in
English and German, and examine the effectiveness of popular transformer
end-to-end models with different intermediate finetuning tasks. Additionally,
we explore the potential of ChatGPT for CLCTS as a summarizer and an evaluator.
Overall, we report evaluations from humans, ChatGPT, and several recent
automatic evaluation metrics where we find that our intermediate task finetuned
end-to-end models generate bad to moderate quality summaries; ChatGPT as a
summarizer (without any finetuning) provides moderate to good quality outputs
and as an evaluator correlates moderately with human evaluations but is prone
to giving lower scores. ChatGPT also seems very adept at normalizing historical
text and outperforms context-unaware spelling normalization tools such as
Norma. We finally test ChatGPT in a scenario with adversarially attacked and
unseen source documents and find that ChatGPT profits from its prior knowledge
to a certain degree, with better performances for omission and entity swap than
negation against its prior knowledge. This benefit inflates its assessed
quality as ChatGPT performs slightly worse for unseen source documents compared
to seen documents. We additionally introspect our models&apos; performances to find
that longer, older and more complex source texts (all of which are more
characteristic for historical language variants) are harder to summarize for
all models, indicating the difficulty of the CLCTS task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouni_J/0/1/0/all/0/1&quot;&gt;Jihed Ouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eger_S/0/1/0/all/0/1&quot;&gt;Steffen Eger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13734">
<title>The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios. (arXiv:2306.13734v2 [eess.AS] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13734</link>
<description rdf:parseType="Literal">&lt;p&gt;The CHiME challenges have played a significant role in the development and
evaluation of robust automatic speech recognition (ASR) systems. We introduce
the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task
comprises joint ASR and diarization in far-field settings with multiple, and
possibly heterogeneous, recording devices. Different from previous challenges,
we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The
goal is for participants to devise a single system that can generalize across
different array geometries and use cases with no a-priori information. Another
departure from earlier CHiME iterations is that participants are allowed to use
open-source pre-trained models and datasets. In this paper, we describe the
challenge design, motivation, and fundamental research questions in detail. We
also present the baseline system, which is fully array-topology agnostic and
features multi-channel diarization, channel selection, guided source separation
and a robust ASR model that leverages self-supervised speech representations
(SSLR).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cornell_S/0/1/0/all/0/1&quot;&gt;Samuele Cornell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiesner_M/0/1/0/all/0/1&quot;&gt;Matthew Wiesner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Watanabe_S/0/1/0/all/0/1&quot;&gt;Shinji Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raj_D/0/1/0/all/0/1&quot;&gt;Desh Raj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_X/0/1/0/all/0/1&quot;&gt;Xuankai Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garcia_P/0/1/0/all/0/1&quot;&gt;Paola Garcia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maciejewski_M/0/1/0/all/0/1&quot;&gt;Matthew Maciejewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Masuyama_Y/0/1/0/all/0/1&quot;&gt;Yoshiki Masuyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhong-Qiu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Squartini_S/0/1/0/all/0/1&quot;&gt;Stefano Squartini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khudanpur_S/0/1/0/all/0/1&quot;&gt;Sanjeev Khudanpur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13804">
<title>Cross-Language Speech Emotion Recognition Using Multimodal Dual Attention Transformers. (arXiv:2306.13804v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13804</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent progress in speech emotion recognition (SER),
state-of-the-art systems are unable to achieve improved performance in
cross-language settings. In this paper, we propose a Multimodal Dual Attention
Transformer (MDAT) model to improve cross-language SER. Our model utilises
pre-trained models for multimodal feature extraction and is equipped with a
dual attention mechanism including graph attention and co-attention to capture
complex dependencies across different modalities and achieve improved
cross-language SER results using minimal target language data. In addition, our
model also exploits a transformer encoder layer for high-level feature
representation to improve emotion classification accuracy. In this way, MDAT
performs refinement of feature representation at various stages and provides
emotional salient features to the classification layer. This novel approach
also ensures the preservation of modality-specific emotional information while
enhancing cross-modality and cross-language interactions. We assess our model&apos;s
performance on four publicly available SER datasets and establish its superior
effectiveness compared to recent approaches and baseline models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaidi_S/0/1/0/all/0/1&quot;&gt;Syed Aun Muhammad Zaidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Latif_S/0/1/0/all/0/1&quot;&gt;Siddique Latif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qadir_J/0/1/0/all/0/1&quot;&gt;Junaid Qadir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05300">
<title>Unleashing Cognitive Synergy in Large Language Models: A Task-Solving Agent through Multi-Persona Self-Collaboration. (arXiv:2307.05300v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;Human intelligence thrives on the concept of cognitive synergy, where
collaboration and information integration among different cognitive processes
yield superior outcomes compared to individual cognitive processes in
isolation. Although Large Language Models (LLMs) have demonstrated promising
performance as general task-solving agents, they still struggle with tasks that
require intensive domain knowledge and complex reasoning. In this work, we
propose Solo Performance Prompting (SPP), which transforms a single LLM into a
cognitive synergist by engaging in multi-turn self-collaboration with multiple
personas. A cognitive synergist refers to an intelligent agent that
collaborates with multiple minds, combining their individual strengths and
knowledge, to enhance problem-solving and overall performance in complex tasks.
By dynamically identifying and simulating different personas based on task
inputs, SPP unleashes the potential of cognitive synergy in LLMs. We have
discovered that assigning multiple, fine-grained personas in LLMs elicits
better problem-solving abilities compared to using a single or fixed number of
personas. We evaluate SPP on three challenging tasks: Trivia Creative Writing,
Codenames Collaborative, and Logic Grid Puzzle, encompassing both
knowledge-intensive and reasoning-intensive types. Unlike previous works, such
as Chain-of-Thought, that solely enhance the reasoning abilities in LLMs, SPP
effectively elicits internal knowledge acquisition abilities, reduces
hallucination, and maintains strong reasoning capabilities. Code, data, and
prompts can be found at:
https://github.com/MikeWangWZHL/Solo-Performance-Prompting.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhenhailong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shaoguang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wenshan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05695">
<title>Stack More Layers Differently: High-Rank Training Through Low-Rank Updates. (arXiv:2307.05695v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05695</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the dominance and effectiveness of scaling, resulting in large
networks with hundreds of billions of parameters, the necessity to train
overparametrized models remains poorly understood, and alternative approaches
do not necessarily make it cheaper to train high-performance models. In this
paper, we explore low-rank training techniques as an alternative approach to
training large neural networks. We introduce a novel method called ReLoRA,
which utilizes low-rank updates to train high-rank networks. We apply ReLoRA to
pre-training transformer language models with up to 350M parameters and
demonstrate comparable performance to regular neural network training.
Furthermore, we observe that the efficiency of ReLoRA increases with model
size, making it a promising approach for training multi-billion-parameter
networks efficiently. Our findings shed light on the potential of low-rank
training techniques and their implications for scaling laws.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lialin_V/0/1/0/all/0/1&quot;&gt;Vladislav Lialin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivagunde_N/0/1/0/all/0/1&quot;&gt;Namrata Shivagunde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muckatira_S/0/1/0/all/0/1&quot;&gt;Sherin Muckatira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rumshisky_A/0/1/0/all/0/1&quot;&gt;Anna Rumshisky&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>