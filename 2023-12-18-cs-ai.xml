<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-14T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08375" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08383" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08538" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08571" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08616" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08658" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08677" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08726" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08782" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08793" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08968" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08975" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08977" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08995" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09007" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2104.00640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.05300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.10789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10083" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11194" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01313" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.01141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03358" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03669" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.13672" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00268" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.02299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.10158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18168" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19704" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.06497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13018" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16146" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.18259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00878" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08078" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08221" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08274" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.08374">
<title>Unsupervised Social Event Detection via Hybrid Graph Contrastive Learning and Reinforced Incremental Clustering. (arXiv:2312.08374v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2312.08374</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting events from social media data streams is gradually attracting
researchers. The innate challenge for detecting events is to extract
discriminative information from social media data thereby assigning the data
into different events. Due to the excessive diversity and high updating
frequency of social data, using supervised approaches to detect events from
social messages is hardly achieved. To this end, recent works explore learning
discriminative information from social messages by leveraging graph contrastive
learning (GCL) and embedding clustering in an unsupervised manner. However, two
intrinsic issues exist in benchmark methods: conventional GCL can only roughly
explore partial attributes, thereby insufficiently learning the discriminative
information of social messages; for benchmark methods, the learned embeddings
are clustered in the latent space by taking advantage of certain specific prior
knowledge, which conflicts with the principle of unsupervised learning
paradigm. In this paper, we propose a novel unsupervised social media event
detection method via hybrid graph contrastive learning and reinforced
incremental clustering (HCRC), which uses hybrid graph contrastive learning to
comprehensively learn semantic and structural discriminative information from
social messages and reinforced incremental clustering to perform efficient
clustering in a solidly unsupervised manner. We conduct comprehensive
experiments to evaluate HCRC on the Twitter and Maven datasets. The
experimental results demonstrate that our approach yields consistent
significant performance boosts. In traditional incremental setting,
semi-supervised incremental setting and solidly unsupervised setting, the model
performance has achieved maximum improvements of 53%, 45%, and 37%,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Z/0/1/0/all/0/1&quot;&gt;Zehua Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lixiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiangmeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08375">
<title>An Encoding of Abstract Dialectical Frameworks into Higher-Order Logic. (arXiv:2312.08375v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2312.08375</link>
<description rdf:parseType="Literal">&lt;p&gt;An approach for encoding abstract dialectical frameworks and their semantics
into classical higher-order logic is presented. Important properties and
semantic relationships are formally encoded and proven using the proof
assistant Isabelle/HOL. This approach allows for the computer-assisted analysis
of abstract dialectical frameworks using automated and interactive reasoning
tools within a uniform logic environment. Exemplary applications include the
formal analysis and verification of meta-theoretical properties, and the
generation of interpretations and extensions under specific semantic
constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martina_A/0/1/0/all/0/1&quot;&gt;Antoine Martina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steen_A/0/1/0/all/0/1&quot;&gt;Alexander Steen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08377">
<title>ALGNet: Attention Light Graph Memory Network for Medical Recommendation System. (arXiv:2312.08377v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08377</link>
<description rdf:parseType="Literal">&lt;p&gt;Medication recommendation is a vital task for improving patient care and
reducing adverse events. However, existing methods often fail to capture the
complex and dynamic relationships among patient medical records, drug efficacy
and safety, and drug-drug interactions (DDI). In this paper, we propose ALGNet,
a novel model that leverages light graph convolutional networks (LGCN) and
augmentation memory networks (AMN) to enhance medication recommendation. LGCN
can efficiently encode the patient records and the DDI graph into
low-dimensional embeddings, while AMN can augment the patient representation
with external knowledge from a memory module. We evaluate our model on the
MIMIC-III dataset and show that it outperforms several baselines in terms of
recommendation accuracy and DDI avoidance. We also conduct an ablation study to
analyze the effects of different components of our model. Our results
demonstrate that ALGNet can achieve superior performance with less computation
and more interpretability. The implementation of this paper can be found at:
https://github.com/huyquoctrinh/ALGNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_M/0/1/0/all/0/1&quot;&gt;Minh-Van Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duy-Thinh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trinh_Q/0/1/0/all/0/1&quot;&gt;Quoc-Huy Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1&quot;&gt;Bac-Hoai Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08378">
<title>Singular Value Penalization and Semantic Data Augmentation for Fully Test-Time Adaptation. (arXiv:2312.08378v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08378</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully test-time adaptation (FTTA) adapts a model that is trained on a source
domain to a target domain during the testing phase, where the two domains
follow different distributions and source data is unavailable during the
training phase. Existing methods usually adopt entropy minimization to reduce
the uncertainty of target prediction results, and improve the FTTA performance
accordingly. However, they fail to ensure the diversity in target prediction
results. Recent domain adaptation study has shown that maximizing the sum of
singular values of prediction results can simultaneously enhance their
confidence (discriminability) and diversity. However, during the training
phase, larger singular values usually take up a dominant position in loss
maximization. This results in the model being more inclined to enhance
discriminability for easily distinguishable classes, and the improvement in
diversity is insufficiently effective. Furthermore, the adaptation and
prediction in FTTA only use data from the current batch, which may lead to the
risk of overfitting. To address the aforementioned issues, we propose
maximizing the sum of singular values while minimizing their variance. This
enables the model&apos;s focus toward the smaller singular values, enhancing
discriminability between more challenging classes and effectively increasing
the diversity of prediction results. Moreover, we incorporate data from the
previous batch to realize semantic data augmentation for the current batch,
reducing the risk of overfitting. Extensive experiments on benchmark datasets
show our proposed approach outperforms some compared state-of-the-art FTTA
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Houcheng Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Daixian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengzhu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08381">
<title>An Explainable Machine Learning Framework for the Accurate Diagnosis of Ovarian Cancer. (arXiv:2312.08381v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08381</link>
<description rdf:parseType="Literal">&lt;p&gt;Ovarian cancer (OC) is one of the most prevalent types of cancer in women.
Early and accurate diagnosis is crucial for the survival of the patients.
However, the majority of women are diagnosed in advanced stages due to the lack
of effective biomarkers and accurate screening tools. While previous studies
sought a common biomarker, our study suggests different biomarkers for the
premenopausal and postmenopausal populations. This can provide a new
perspective in the search for novel predictors for the effective diagnosis of
OC. Lack of explainability is one major limitation of current AI systems. The
stochastic nature of the ML algorithms raises concerns about the reliability of
the system as it is difficult to interpret the reasons behind the decisions. To
increase the trustworthiness and accountability of the diagnostic system as
well as to provide transparency and explanations behind the predictions,
explainable AI has been incorporated into the ML framework. SHAP is employed to
quantify the contributions of the selected biomarkers and determine the most
discriminative features. A hybrid decision support system has been established
that can eliminate the bottlenecks caused by the black-box nature of the ML
algorithms providing a safe and trustworthy AI tool. The diagnostic accuracy
obtained from the proposed system outperforms the existing methods as well as
the state-of-the-art ROMA algorithm by a substantial margin which signifies its
potential to be an effective tool in the differential diagnosis of OC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newaz_A/0/1/0/all/0/1&quot;&gt;Asif Newaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taharat_A/0/1/0/all/0/1&quot;&gt;Abdullah Taharat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Sakibul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akanda_A/0/1/0/all/0/1&quot;&gt;A.G.M. Fuad Hasan Akanda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08383">
<title>Improving age prediction: Utilizing LSTM-based dynamic forecasting for data augmentation in multivariate time series analysis. (arXiv:2312.08383v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08383</link>
<description rdf:parseType="Literal">&lt;p&gt;The high dimensionality and complexity of neuroimaging data necessitate large
datasets to develop robust and high-performing deep learning models. However,
the neuroimaging field is notably hampered by the scarcity of such datasets. In
this work, we proposed a data augmentation and validation framework that
utilizes dynamic forecasting with Long Short-Term Memory (LSTM) networks to
enrich datasets. We extended multivariate time series data by predicting the
time courses of independent component networks (ICNs) in both one-step and
recursive configurations. The effectiveness of these augmented datasets was
then compared with the original data using various deep learning models
designed for chronological age prediction tasks. The results suggest that our
approach improves model performance, providing a robust solution to overcome
the challenges presented by the limited size of neuroimaging datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yutong Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_C/0/1/0/all/0/1&quot;&gt;Charles A. Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1&quot;&gt;Vince D. Calhoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miller_R/0/1/0/all/0/1&quot;&gt;Robyn L. Miller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08384">
<title>Taking it further: leveraging pseudo labels for field delineation across label-scarce smallholder regions. (arXiv:2312.08384v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08384</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning allows for resource-efficient geographic transfer of
pre-trained field delineation models. However, the scarcity of labeled data for
complex and dynamic smallholder landscapes, particularly in Sub-Saharan Africa,
remains a major bottleneck for large-area field delineation. This study
explores opportunities of using sparse field delineation pseudo labels for
fine-tuning models across geographies and sensor characteristics. We build on a
FracTAL ResUNet trained for crop field delineation in India (median field size
of 0.24 ha) and use this pre-trained model to generate pseudo labels in
Mozambique (median field size of 0.06 ha). We designed multiple pseudo label
selection strategies and compared the quantities, area properties, seasonal
distribution, and spatial agreement of the pseudo labels against
human-annotated training labels (n = 1,512). We then used the human-annotated
labels and the pseudo labels for model fine-tuning and compared predictions
against human field annotations (n = 2,199). Our results indicate i) a good
baseline performance of the pre-trained model in both field delineation and
field size estimation, and ii) the added value of regional fine-tuning with
performance improvements in nearly all experiments. Moreover, we found iii)
substantial performance increases when using only pseudo labels (up to 77% of
the IoU increases and 68% of the RMSE decreases obtained by human labels), and
iv) additional performance increases when complementing human annotations with
pseudo labels. Pseudo labels can be efficiently generated at scale and thus
facilitate domain adaptation in label-scarce settings. The workflow presented
here is a stepping stone for overcoming the persisting data gaps in
heterogeneous smallholder agriculture of Sub-Saharan Africa, where labels are
commonly scarce.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rufin_P/0/1/0/all/0/1&quot;&gt;Philippe Rufin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sherrie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisboa_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe1; Nogueira Lisboa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemmerling_J/0/1/0/all/0/1&quot;&gt;Jan Hemmerling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tulbure_M/0/1/0/all/0/1&quot;&gt;Mirela G. Tulbure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyfroidt_P/0/1/0/all/0/1&quot;&gt;Patrick Meyfroidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08385">
<title>A Structural Complexity Analysis of Synchronous Dynamical Systems. (arXiv:2312.08385v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2312.08385</link>
<description rdf:parseType="Literal">&lt;p&gt;Synchronous dynamic systems are well-established models that have been used
to capture a range of phenomena in networks, including opinion diffusion,
spread of disease and product adoption. We study the three most notable
problems in synchronous dynamic systems: whether the system will transition to
a target configuration from a starting configuration, whether the system will
reach convergence from a starting configuration, and whether the system is
guaranteed to converge from every possible starting configuration. While all
three problems were known to be intractable in the classical sense, we initiate
the study of their exact boundaries of tractability from the perspective of
structural parameters of the network by making use of the more fine-grained
parameterized complexity paradigm.
&lt;/p&gt;
&lt;p&gt;As our first result, we consider treewidth - as the most prominent and
ubiquitous structural parameter - and show that all three problems remain
intractable even on instances of constant treewidth. We complement this
negative finding with fixed-parameter algorithms for the former two problems
parameterized by treedepth, a well-studied restriction of treewidth. While it
is possible to rule out a similar algorithm for convergence guarantee under
treedepth, we conclude with a fixed-parameter algorithm for this last problem
when parameterized by treedepth and the maximum in-degree.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eiben_E/0/1/0/all/0/1&quot;&gt;Eduard Eiben&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganian_R/0/1/0/all/0/1&quot;&gt;Robert Ganian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamm_T/0/1/0/all/0/1&quot;&gt;Thekla Hamm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korchemna_V/0/1/0/all/0/1&quot;&gt;Viktoriia Korchemna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08393">
<title>Multi-criteria recommendation systems to foster online grocery. (arXiv:2312.08393v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.08393</link>
<description rdf:parseType="Literal">&lt;p&gt;With the exponential increase in information, it has become imperative to
design mechanisms that allow users to access what matters to them as quickly as
possible. The recommendation system ($RS$) with information technology
development is the solution, it is an intelligent system. Various types of data
can be collected on items of interest to users and presented as
recommendations. $RS$ also play a very important role in e-commerce. The
purpose of recommending a product is to designate the most appropriate
designation for a specific product. The major challenges when recommending
products are insufficient information about the products and the categories to
which they belong. In this paper, we transform the product data using two
methods of document representation: bag-of-words (BOW) and the neural
network-based document combination known as vector-based (Doc2Vec). We propose
three-criteria recommendation systems (product, package, and health) for each
document representation method to foster online grocery, which depends on
product characteristics such as (composition, packaging, nutrition table,
allergen, etc.). For our evaluation, we conducted a user and expert survey.
Finally, we have compared the performance of these three criteria for each
document representation method, discovering that the neural network-based
(Doc2Vec) performs better and completely alters the results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hafez_M/0/1/0/all/0/1&quot;&gt;Manar Mohamed Hafez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Redondo_R/0/1/0/all/0/1&quot;&gt;Rebeca P. D&amp;#xed;az Redondo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_Vilas_A/0/1/0/all/0/1&quot;&gt;Ana Fern&amp;#xe1;ndez-Vilas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pazo_H/0/1/0/all/0/1&quot;&gt;H&amp;#xe9;ctor Olivera Paz&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08397">
<title>Personalized Decision Supports based on Theory of Mind Modeling and Explainable Reinforcement Learning. (arXiv:2312.08397v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08397</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel personalized decision support system that
combines Theory of Mind (ToM) modeling and explainable Reinforcement Learning
(XRL) to provide effective and interpretable interventions. Our method
leverages DRL to provide expert action recommendations while incorporating ToM
modeling to understand users&apos; mental states and predict their future actions,
enabling appropriate timing for intervention. To explain interventions, we use
counterfactual explanations based on RL&apos;s feature importance and users&apos; ToM
model structure. Our proposed system generates accurate and personalized
interventions that are easily interpretable by end-users. We demonstrate the
effectiveness of our approach through a series of crowd-sourcing experiments in
a simulated team decision-making task, where our system outperforms control
baselines in terms of task performance. Our proposed approach is agnostic to
task environment and RL model structure, therefore has the potential to be
generalized to a wide range of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_K/0/1/0/all/0/1&quot;&gt;Keyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lewis_M/0/1/0/all/0/1&quot;&gt;Michael Lewis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08400">
<title>Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction. (arXiv:2312.08400v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08400</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) finetuned to follow human instruction have
recently exhibited significant capabilities in various English NLP tasks.
However, their performance in grammatical error correction (GEC), especially on
languages other than English, remains significantly unexplored. In this work,
we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a
complex task due to Arabic&apos;s rich morphology. Our findings suggest that various
prompting methods, coupled with (in-context) few-shot learning, demonstrate
considerable effectiveness, with GPT-4 achieving up to $65.49$ F$_{1}$ score
under expert prompting (approximately $5$ points higher than our established
baseline). Despite these positive results, we find that instruction finetuned
models, regardless of their size, are still outperformed by fully finetuned
ones, even if they are significantly smaller in size. This disparity highlights
substantial room for improvements for LLMs. Inspired by methods used in
low-resource machine translation, we also develop a method exploiting synthetic
data that significantly outperforms previous models on two standard Arabic
benchmarks. Our best model achieves a new SOTA on Arabic GEC, with $73.29$ and
$73.26$ F$_{1}$ on the 2014 and 2015 QALB datasets, respectively, compared to
peer-reviewed published baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_S/0/1/0/all/0/1&quot;&gt;Sang Yun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatia_G/0/1/0/all/0/1&quot;&gt;Gagan Bhatia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagoudi_E/0/1/0/all/0/1&quot;&gt;El Moatez Billah Nagoudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdul_Mageed_M/0/1/0/all/0/1&quot;&gt;Muhammad Abdul-Mageed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08402">
<title>LDM$^2$: A Large Decision Model Imitating Human Cognition with Dynamic Memory Enhancement. (arXiv:2312.08402v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08402</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid development of large language models (LLMs), it is highly
demanded that LLMs can be adopted to make decisions to enable the artificial
general intelligence. Most approaches leverage manually crafted examples to
prompt the LLMs to imitate the decision process of human. However, designing
optimal prompts is difficult and the patterned prompts can hardly be
generalized to more complex environments. In this paper, we propose a novel
model named Large Decision Model with Memory (LDM$^2$), which leverages a
dynamic memory mechanism to construct dynamic prompts, guiding the LLMs in
making proper decisions according to the faced state. LDM$^2$ consists of two
stages: memory formation and memory refinement. In the former stage, human
behaviors are decomposed into state-action tuples utilizing the powerful
summarizing ability of LLMs. Then, these tuples are stored in the memory, whose
indices are generated by the LLMs, to facilitate the retrieval of the most
relevant subset of memorized tuples based on the current state. In the latter
stage, our LDM$^2$ employs tree exploration to discover more suitable decision
processes and enrich the memory by adding valuable state-action tuples. The
dynamic circle of exploration and memory enhancement provides LDM$^2$ a better
understanding of the global environment. Extensive experiments conducted in two
interactive environments have shown that our LDM$^2$ outperforms the baselines
in terms of both score and success rate, which demonstrates its effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingjin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Daniel Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08403">
<title>Earthfarseer: Versatile Spatio-Temporal Dynamical Systems Modeling in One Model. (arXiv:2312.08403v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08403</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently modeling spatio-temporal (ST) physical processes and observations
presents a challenging problem for the deep learning community. Many recent
studies have concentrated on meticulously reconciling various advantages,
leading to designed models that are neither simple nor practical. To address
this issue, this paper presents a systematic study on existing shortcomings
faced by off-the-shelf models, including lack of local fidelity, poor
prediction performance over long time-steps,low scalability, and inefficiency.
To systematically address the aforementioned problems, we propose an
EarthFarseer, a concise framework that combines parallel local convolutions and
global Fourier-based transformer architectures, enabling dynamically capture
the local-global spatial interactions and dependencies. EarthFarseer also
incorporates a multi-scale fully convolutional and Fourier architectures to
efficiently and effectively capture the temporal evolution. Our proposal
demonstrates strong adaptability across various tasks and datasets, with fast
convergence and better local fidelity in long time-steps predictions. Extensive
experiments and visualizations over eight human society physical and natural
physical datasets demonstrates the state-of-the-art performance of
EarthFarseer. We release our code at
https://github.com/easylearningscores/EarthFarseer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shilong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08408">
<title>Explainable AI in Grassland Monitoring: Enhancing Model Performance and Domain Adaptability. (arXiv:2312.08408v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08408</link>
<description rdf:parseType="Literal">&lt;p&gt;Grasslands are known for their high biodiversity and ability to provide
multiple ecosystem services. Challenges in automating the identification of
indicator plants are key obstacles to large-scale grassland monitoring. These
challenges stem from the scarcity of extensive datasets, the distributional
shifts between generic and grassland-specific datasets, and the inherent
opacity of deep learning models. This paper delves into the latter two
challenges, with a specific focus on transfer learning and eXplainable
Artificial Intelligence (XAI) approaches to grassland monitoring, highlighting
the novelty of XAI in this domain. We analyze various transfer learning methods
to bridge the distributional gaps between generic and grassland-specific
datasets. Additionally, we showcase how explainable AI techniques can unveil
the model&apos;s domain adaptation capabilities, employing quantitative assessments
to evaluate the model&apos;s proficiency in accurately centering relevant input
features around the object of interest. This research contributes valuable
insights for enhancing model performance through transfer learning and
measuring domain adaptability with explainable AI, showing significant promise
for broader applications within the agricultural community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shanghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hedstrom_A/0/1/0/all/0/1&quot;&gt;Anna Hedstr&amp;#xf6;m&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basavegowda_D/0/1/0/all/0/1&quot;&gt;Deepak Hanike Basavegowda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weltzien_C/0/1/0/all/0/1&quot;&gt;Cornelia Weltzien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1&quot;&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08418">
<title>Automatic Bug Detection in Games using LSTM Networks. (arXiv:2312.08418v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08418</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduced a new framework to detect perceptual bugs using a Long
Short-Term Memory (LSTM) network, which detects bugs in video games as
anomalies. The detected buggy frames are then clustered to determine the
category of the occurred bug. The framework was evaluated on two First Person
Shooter (FPS) games. Results show the effectiveness of the framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizi_E/0/1/0/all/0/1&quot;&gt;Elham Azizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zaman_L/0/1/0/all/0/1&quot;&gt;Loutfouz Zaman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08459">
<title>FaceTalk: Audio-Driven Motion Diffusion for Neural Parametric Head Models. (arXiv:2312.08459v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08459</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce FaceTalk, a novel generative approach designed for synthesizing
high-fidelity 3D motion sequences of talking human heads from input audio
signal. To capture the expressive, detailed nature of human heads, including
hair, ears, and finer-scale eye movements, we propose to couple speech signal
with the latent space of neural parametric head models to create high-fidelity,
temporally coherent motion sequences. We propose a new latent diffusion model
for this task, operating in the expression space of neural parametric head
models, to synthesize audio-driven realistic head sequences. In the absence of
a dataset with corresponding NPHM expressions to audio, we optimize for these
correspondences to produce a dataset of temporally-optimized NPHM expressions
fit to audio-video recordings of people talking. To the best of our knowledge,
this is the first work to propose a generative approach for realistic and
high-quality motion synthesis of volumetric human heads, representing a
significant advancement in the field of audio-driven 3D animation. Notably, our
approach stands out in its ability to generate plausible motion sequences that
can produce high-fidelity head animation coupled with the NPHM shape space. Our
experimental results substantiate the effectiveness of FaceTalk, consistently
achieving superior and visually natural motion, encompassing diverse facial
expressions and styles, outperforming existing methods by 75% in perceptual
user study evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aneja_S/0/1/0/all/0/1&quot;&gt;Shivangi Aneja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thies_J/0/1/0/all/0/1&quot;&gt;Justus Thies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Angela Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niessner_M/0/1/0/all/0/1&quot;&gt;Matthias Nie&amp;#xdf;ner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08463">
<title>How much can change in a year? Revisiting Evaluation in Multi-Agent Reinforcement Learning. (arXiv:2312.08463v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08463</link>
<description rdf:parseType="Literal">&lt;p&gt;Establishing sound experimental standards and rigour is important in any
growing field of research. Deep Multi-Agent Reinforcement Learning (MARL) is
one such nascent field. Although exciting progress has been made, MARL has
recently come under scrutiny for replicability issues and a lack of
standardised evaluation methodology, specifically in the cooperative setting.
Although protocols have been proposed to help alleviate the issue, it remains
important to actively monitor the health of the field. In this work, we extend
the database of evaluation methodology previously published by containing
meta-data on MARL publications from top-rated conferences and compare the
findings extracted from this updated database to the trends identified in their
work. Our analysis shows that many of the worrying trends in performance
reporting remain. This includes the omission of uncertainty quantification, not
reporting all relevant evaluation details and a narrowing of algorithmic
development classes. Promisingly, we do observe a trend towards more difficult
scenarios in SMAC-v1, which if continued into SMAC-v2 will encourage novel
algorithmic development. Our data indicate that replicability needs to be
approached more proactively by the MARL community to ensure trust in the field
as we move towards exciting new frontiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Siddarth Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahjoub_O/0/1/0/all/0/1&quot;&gt;Omayma Mahjoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kock_R/0/1/0/all/0/1&quot;&gt;Ruan de Kock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khlifi_W/0/1/0/all/0/1&quot;&gt;Wiem Khlifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vall_A/0/1/0/all/0/1&quot;&gt;Abidine Vall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1&quot;&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08466">
<title>Efficiently Quantifying Individual Agent Importance in Cooperative MARL. (arXiv:2312.08466v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08466</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring the contribution of individual agents is challenging in cooperative
multi-agent reinforcement learning (MARL). In cooperative MARL, team
performance is typically inferred from a single shared global reward. Arguably,
among the best current approaches to effectively measure individual agent
contributions is to use Shapley values. However, calculating these values is
expensive as the computational complexity grows exponentially with respect to
the number of agents. In this paper, we adapt difference rewards into an
efficient method for quantifying the contribution of individual agents,
referred to as Agent Importance, offering a linear computational complexity
relative to the number of agents. We show empirically that the computed values
are strongly correlated with the true Shapley values, as well as the true
underlying individual agent rewards, used as the ground truth in environments
where these are available. We demonstrate how Agent Importance can be used to
help study MARL systems by diagnosing algorithmic failures discovered in prior
MARL benchmarking work. Our analysis illustrates Agent Importance as a valuable
explainability component for future MARL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahjoub_O/0/1/0/all/0/1&quot;&gt;Omayma Mahjoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kock_R/0/1/0/all/0/1&quot;&gt;Ruan de Kock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Siddarth Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khlifi_W/0/1/0/all/0/1&quot;&gt;Wiem Khlifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vall_A/0/1/0/all/0/1&quot;&gt;Abidine Vall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tessera_K/0/1/0/all/0/1&quot;&gt;Kale-ab Tessera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08467">
<title>Culturally Responsive Artificial Intelligence -- Problems, Challenges and Solutions. (arXiv:2312.08467v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.08467</link>
<description rdf:parseType="Literal">&lt;p&gt;In the contemporary interconnected world, the concept of cultural
responsibility occupies paramount importance. As the lines between nations
become less distinct, it is incumbent upon individuals, communities, and
institutions to assume the responsibility of safeguarding and valuing the
landscape of diverse cultures that constitute our global society. This paper
explores the socio-cultural and ethical challenges stemming from the
implementation of AI algorithms and highlights the necessity for their
culturally responsive development. It also offers recommendations on essential
elements required to enhance AI systems&apos; adaptability to meet the demands of
contemporary multicultural societies. The paper highlights the need for further
multidisciplinary research to create AI models that effectively address these
challenges. It also advocates the significance of AI enculturation and
underlines the importance of regulatory measures to promote cultural
responsibility in AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozegalska_Lukasik_N/0/1/0/all/0/1&quot;&gt;Natalia O&amp;#x17c;egalska-&amp;#x141;ukasik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lukasik_S/0/1/0/all/0/1&quot;&gt;Szymon &amp;#x141;ukasik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08468">
<title>On Diagnostics for Understanding Agent Training Behaviour in Cooperative MARL. (arXiv:2312.08468v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08468</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperative multi-agent reinforcement learning (MARL) has made substantial
strides in addressing the distributed decision-making challenges. However, as
multi-agent systems grow in complexity, gaining a comprehensive understanding
of their behaviour becomes increasingly challenging. Conventionally, tracking
team rewards over time has served as a pragmatic measure to gauge the
effectiveness of agents in learning optimal policies. Nevertheless, we argue
that relying solely on the empirical returns may obscure crucial insights into
agent behaviour. In this paper, we explore the application of explainable AI
(XAI) tools to gain profound insights into agent behaviour. We employ these
diagnostics tools within the context of Level-Based Foraging and Multi-Robot
Warehouse environments and apply them to a diverse array of MARL algorithms. We
demonstrate how our diagnostics can enhance the interpretability and
explainability of MARL systems, providing a better understanding of agent
behaviour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khlifi_W/0/1/0/all/0/1&quot;&gt;Wiem Khlifi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Siddarth Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahjoub_O/0/1/0/all/0/1&quot;&gt;Omayma Mahjoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kock_R/0/1/0/all/0/1&quot;&gt;Ruan de Kock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vall_A/0/1/0/all/0/1&quot;&gt;Abidine Vall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorsane_R/0/1/0/all/0/1&quot;&gt;Rihab Gorsane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pretorius_A/0/1/0/all/0/1&quot;&gt;Arnu Pretorius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08517">
<title>(Debiased) Contrastive Learning Loss for Recommendation (Technical Report). (arXiv:2312.08517v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08517</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we perform a systemic examination of the recommendation
losses, including listwise (softmax), pairwise(BPR), and pointwise
(mean-squared error, MSE, and Cosine Contrastive Loss, CCL) losses through the
lens of contrastive learning. We introduce and study both debiased InfoNCE and
mutual information neural estimator (MINE), for the first time, under the
recommendation setting. We also relate and differentiate these two losses with
the BPR loss through the lower bound analysis. Furthermore, we present the
debiased pointwise loss (for both MSE and CCL) and theoretically certify both
iALS and EASE, two of the most popular linear models, are inherently debiased.
The empirical experimental results demonstrate the effectiveness of the
debiased losses and newly introduced mutual-information losses outperform the
existing (biased) ones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Ruoming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08519">
<title>Toward a More Biologically Plausible Neural Network Model of Latent Cause Inference. (arXiv:2312.08519v1 [q-bio.NC])</title>
<link>http://arxiv.org/abs/2312.08519</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans spontaneously perceive a continuous stream of experience as discrete
events. It has been hypothesized that this ability is supported by latent cause
inference (LCI). We implemented this hypothesis using Latent Cause Network
(LCNet), a neural network model of LCI. LCNet interacts with a Bayesian LCI
mechanism that activates a unique context vector for each inferred latent
cause. This architecture makes LCNet more biologically plausible than existing
models of LCI and supports extraction of shared structure across latent causes.
Across three simulations, we found that LCNet could 1) extract shared structure
across latent causes in a function-learning task while avoiding catastrophic
interference, 2) capture human data on curriculum effects in schema learning,
and 3) infer the underlying event structure when processing naturalistic videos
of daily activities. Our work provides a biologically plausible computational
model that can operate in both laboratory experiment settings and naturalistic
settings, opening up the possibility of providing a unified model of event
cognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qihong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tan T. Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hasson_U/0/1/0/all/0/1&quot;&gt;Uri Hasson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Griffiths_T/0/1/0/all/0/1&quot;&gt;Thomas L. Griffiths&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zacks_J/0/1/0/all/0/1&quot;&gt;Jeffrey M. Zacks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gershman_S/0/1/0/all/0/1&quot;&gt;Samuel J. Gershman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Norman_K/0/1/0/all/0/1&quot;&gt;Kenneth A. Norman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08520">
<title>Revisiting Recommendation Loss Functions through Contrastive Learning (Technical Report). (arXiv:2312.08520v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08520</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the success of contrastive learning, we systematically examine
recommendation losses, including listwise (softmax), pairwise (BPR), and
pointwise (MSE and CCL) losses. In this endeavor, we introduce InfoNCE+, an
optimized generalization of InfoNCE with balance coefficients, and highlight
its performance advantages, particularly when aligned with our new decoupled
contrastive loss, MINE+. We also leverage debiased InfoNCE to debias pointwise
recommendation loss (CCL) as Debiased CCL. Interestingly, our analysis reveals
that linear models like iALS and EASE are inherently debiased. Empirical
results demonstrates the effectiveness of MINE+ and Debiased-CCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_R/0/1/0/all/0/1&quot;&gt;Ruoming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1&quot;&gt;Bin Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08533">
<title>World Models via Policy-Guided Trajectory Diffusion. (arXiv:2312.08533v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08533</link>
<description rdf:parseType="Literal">&lt;p&gt;World models are a powerful tool for developing intelligent agents. By
predicting the outcome of a sequence of actions, world models enable policies
to be optimised via on-policy reinforcement learning (RL) using synthetic data,
i.e. in ``in imagination&apos;&apos;. Existing world models are autoregressive, and
interleave predicting the next state with sampling the next action from the
policy. Thus, the prediction error inevitably compounds as the trajectory
length grows. In this work, we propose a novel world modelling approach that is
not autoregressive and generates entire on-policy trajectories via a single
pass through a diffusion model. Our approach, Policy-Guided Trajectory
Diffusion (PolyGRAD), leverages a denoising model in addition to the gradient
of the action distribution of the policy to diffuse a trajectory of initially
random states and actions into an on-policy synthetic trajectory. We analyse
the capabilities of our approach and demonstrate that it obtains competitive
prediction errors to state-of-the-art autoregressive baselines. PolyGRAD also
enables performant policies to be trained via on-policy RL in imagination. We
believe that PolyGRAD introduces a promising paradigm for world modelling with
many possible extensions to explore in future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rigter_M/0/1/0/all/0/1&quot;&gt;Marc Rigter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamada_J/0/1/0/all/0/1&quot;&gt;Jun Yamada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Posner_I/0/1/0/all/0/1&quot;&gt;Ingmar Posner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08537">
<title>Object-Centric Conformance Alignments with Synchronization (Extended Version). (arXiv:2312.08537v1 [cs.LO])</title>
<link>http://arxiv.org/abs/2312.08537</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-world processes operate on objects that are inter-dependent. To
accurately reflect the nature of such processes, object-centric process mining
techniques are needed, notably conformance checking. However, while the
object-centric perspective has recently gained traction, few concrete process
mining techniques have been presented so far. Moreover, existing approaches are
severely limited in their abilities to keep track of object identity and object
dependencies. Consequently, serious problems in logs remain undetected. In this
paper, we present a new formalism that combines the key modelling features of
two existing approaches, in particular the ability of object-centric Petri nets
to capture one-to-many relations and the one of Petri nets with identifiers to
compare and synchronize objects based on their identity. We call the resulting
formalism &apos;object-centric Petri nets with identifiers&apos;, and define alignments
and the conformance checking task for this setting. We propose a conformance
checking approach for such nets based on an encoding in satisfiability modulo
theories (SMT), and illustrate how it can be effectively used to overcome
shortcomings of earlier work. To assess its practicality, we perform an
evaluation on data from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gianola_A/0/1/0/all/0/1&quot;&gt;Alessandro Gianola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montali_M/0/1/0/all/0/1&quot;&gt;Marco Montali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winkler_S/0/1/0/all/0/1&quot;&gt;Sarah Winkler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08538">
<title>Contractive error feedback for gradient compression. (arXiv:2312.08538v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08538</link>
<description rdf:parseType="Literal">&lt;p&gt;On-device memory concerns in distributed deep learning have become severe due
to (i) the growth of model size in multi-GPU training, and (ii) the wide
adoption of deep neural networks for federated learning on IoT devices which
have limited storage. In such settings, communication efficient optimization
methods are attractive alternatives, however they still struggle with memory
issues. To tackle these challenges, we propose an communication efficient
method called contractive error feedback (ConEF). As opposed to SGD with
error-feedback (EFSGD) that inefficiently manages memory, ConEF obtains the
sweet spot of convergence and memory usage, and achieves communication
efficiency by leveraging biased and all-reducable gradient compression. We
empirically validate ConEF on various learning tasks that include image
classification, language modeling, and machine translation and observe that
ConEF saves 80\% - 90\% of the extra memory in EFSGD with almost no loss on
test performance, while also achieving 1.3x - 5x speedup of SGD. Through our
work, we also demonstrate the feasibility and convergence of ConEF to clear up
the theoretical barrier of integrating ConEF to popular memory efficient
frameworks such as ZeRO-3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bingcong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuai Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raman_P/0/1/0/all/0/1&quot;&gt;Parameswaran Raman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrivastava_A/0/1/0/all/0/1&quot;&gt;Anshumali Shrivastava&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giannakis_G/0/1/0/all/0/1&quot;&gt;Georgios B. Giannakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08550">
<title>Harmonics of Learning: Universal Fourier Features Emerge in Invariant Networks. (arXiv:2312.08550v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08550</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we formally prove that, under certain conditions, if a neural
network is invariant to a finite group then its weights recover the Fourier
transform on that group. This provides a mathematical explanation for the
emergence of Fourier features -- a ubiquitous phenomenon in both biological and
artificial learning systems. The results hold even for non-commutative groups,
in which case the Fourier transform encodes all the irreducible unitary group
representations. Our findings have consequences for the problem of symmetry
discovery. Specifically, we demonstrate that the algebraic structure of an
unknown group can be recovered from the weights of a network that is at least
approximately invariant within certain bounds. Overall, this work contributes
to a foundation for an algebraic learning theory of invariant neural network
representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marchetti_G/0/1/0/all/0/1&quot;&gt;Giovanni Luca Marchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hillar_C/0/1/0/all/0/1&quot;&gt;Christopher Hillar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanborn_S/0/1/0/all/0/1&quot;&gt;Sophia Sanborn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08566">
<title>Learning adaptive planning representations with natural language guidance. (arXiv:2312.08566v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08566</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective planning in the real world requires not only world knowledge, but
the ability to leverage that knowledge to build the right representation of the
task at hand. Decades of hierarchical planning techniques have used
domain-specific temporal action abstractions to support efficient and accurate
planning, almost always relying on human priors and domain knowledge to
decompose hard tasks into smaller subproblems appropriate for a goal or set of
goals. This paper describes Ada (Action Domain Acquisition), a framework for
automatically constructing task-specific planning representations using
task-general background knowledge from language models (LMs). Starting with a
general-purpose hierarchical planner and a low-level goal-conditioned policy,
Ada interactively learns a library of planner-compatible high-level action
abstractions and low-level controllers adapted to a particular domain of
planning tasks. On two language-guided interactive planning benchmarks (Mini
Minecraft and ALFRED Household Tasks), Ada strongly outperforms other
approaches that use LMs for sequential decision-making, offering more accurate
plans and better generalization to complex tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_L/0/1/0/all/0/1&quot;&gt;Lionel Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Pratyusha Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegel_Z/0/1/0/all/0/1&quot;&gt;Zachary S. Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiahai Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korneev_N/0/1/0/all/0/1&quot;&gt;Noa Korneev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08571">
<title>PhasePerturbation: Speech Data Augmentation via Phase Perturbation for Automatic Speech Recognition. (arXiv:2312.08571v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.08571</link>
<description rdf:parseType="Literal">&lt;p&gt;Most of the current speech data augmentation methods operate on either the
raw waveform or the amplitude spectrum of speech. In this paper, we propose a
novel speech data augmentation method called PhasePerturbation that operates
dynamically on the phase spectrum of speech. Instead of statically rotating a
phase by a constant degree, PhasePerturbation utilizes three dynamic phase
spectrum operations, i.e., a randomization operation, a frequency masking
operation, and a temporal masking operation, to enhance the diversity of speech
data. We conduct experiments on wav2vec2.0 pre-trained ASR models by
fine-tuning them with the PhasePerturbation augmented TIMIT corpus. The
experimental results demonstrate 10.9\% relative reduction in the word error
rate (WER) compared with the baseline model fine-tuned without any augmentation
operation. Furthermore, the proposed method achieves additional improvements
(12.9\% and 15.9\%) in WER by complementing the Vocal Tract Length Perturbation
(VTLP) and the SpecAug, which are both amplitude spectrum-based augmentation
methods. The results highlight the capability of PhasePerturbation to improve
the current amplitude spectrum-based augmentation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_C/0/1/0/all/0/1&quot;&gt;Chengxi Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Satwinder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_F/0/1/0/all/0/1&quot;&gt;Feng Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruili Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08604">
<title>Verification of Neural Reachable Tubes via Scenario Optimization and Conformal Prediction. (arXiv:2312.08604v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.08604</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based approaches for controlling safety-critical systems are rapidly
growing in popularity; thus, it is important to assure their performance and
safety. Hamilton-Jacobi (HJ) reachability analysis is a popular formal
verification tool for providing such guarantees, since it can handle general
nonlinear system dynamics, bounded adversarial system disturbances, and state
and input constraints. However, its computational and memory complexity scales
exponentially with the state dimension, making it intractable for large-scale
systems. To overcome this challenge, neural approaches, such as DeepReach, have
been used to synthesize reachable tubes and safety controllers for
high-dimensional systems. However, verifying these neural reachable tubes
remains challenging. In this work, we propose two verification methods, based
on robust scenario optimization and conformal prediction, to provide
probabilistic safety guarantees for neural reachable tubes. Our methods allow a
direct trade-off between resilience to outlier errors in the neural tube, which
are inevitable in a learning-based approach, and the strength of the
probabilistic safety guarantee. Furthermore, we show that split conformal
prediction, a widely used method in the machine learning community for
uncertainty quantification, reduces to a scenario-based approach, making the
two methods equivalent not only for verification of neural reachable tubes but
also more generally. To our knowledge, our proof is the first in the literature
to show a strong relationship between conformal prediction and scenario
optimization. Finally, we propose an outlier-adjusted verification approach
that uses the error distribution in neural reachable tubes to recover greater
safe volumes. We demonstrate the efficacy of the proposed approaches for the
high-dimensional problems of multi-vehicle collision avoidance and rocket
landing with no-go zones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1&quot;&gt;Albert Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Somil Bansal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08611">
<title>UniTeam: Open Vocabulary Mobile Manipulation Challenge. (arXiv:2312.08611v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.08611</link>
<description rdf:parseType="Literal">&lt;p&gt;This report introduces our UniTeam agent - an improved baseline for the
&quot;HomeRobot: Open Vocabulary Mobile Manipulation&quot; challenge. The challenge poses
problems of navigation in unfamiliar environments, manipulation of novel
objects, and recognition of open-vocabulary object classes. This challenge aims
to facilitate cross-cutting research in embodied AI using recent advances in
machine learning, computer vision, natural language, and robotics. In this
work, we conducted an exhaustive evaluation of the provided baseline agent;
identified deficiencies in perception, navigation, and manipulation skills; and
improved the baseline agent&apos;s performance. Notably, enhancements were made in
perception - minimizing misclassifications; navigation - preventing infinite
loop commitments; picking - addressing failures due to changing object
visibility; and placing - ensuring accurate positioning for successful object
placement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melnik_A/0/1/0/all/0/1&quot;&gt;Andrew Melnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buttner_M/0/1/0/all/0/1&quot;&gt;Michael B&amp;#xfc;ttner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harz_L/0/1/0/all/0/1&quot;&gt;Leon Harz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_L/0/1/0/all/0/1&quot;&gt;Lyon Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nandi_G/0/1/0/all/0/1&quot;&gt;Gora Chand Nandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+PS_A/0/1/0/all/0/1&quot;&gt;Arjun PS&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yadav_G/0/1/0/all/0/1&quot;&gt;Gaurav Kumar Yadav&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kala_R/0/1/0/all/0/1&quot;&gt;Rahul Kala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haschke_R/0/1/0/all/0/1&quot;&gt;Robert Haschke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08616">
<title>A Generalized Neural Diffusion Framework on Graphs. (arXiv:2312.08616v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2312.08616</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies reveal the connection between GNNs and the diffusion process,
which motivates many diffusion-based GNNs to be proposed. However, since these
two mechanisms are closely related, one fundamental question naturally arises:
Is there a general diffusion framework that can formally unify these GNNs? The
answer to this question can not only deepen our understanding of the learning
process of GNNs, but also may open a new door to design a broad new class of
GNNs. In this paper, we propose a general diffusion equation framework with the
fidelity term, which formally establishes the relationship between the
diffusion process with more GNNs. Meanwhile, with this framework, we identify
one characteristic of graph diffusion networks, i.e., the current neural
diffusion process only corresponds to the first-order diffusion equation.
However, by an experimental investigation, we show that the labels of
high-order neighbors actually exhibit monophily property, which induces the
similarity based on labels among high-order neighbors without requiring the
similarity among first-order neighbors. This discovery motives to design a new
high-order neighbor-aware diffusion equation, and derive a new type of graph
diffusion network (HiD-Net) based on the framework. With the high-order
diffusion equation, HiD-Net is more robust against attacks and works on both
homophily and heterophily graphs. We not only theoretically analyze the
relation between HiD-Net with high-order random walk, but also provide a
theoretical convergence guarantee. Extensive experimental results well
demonstrate the effectiveness of HiD-Net over state-of-the-art graph diffusion
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yibo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongrui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chuan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08629">
<title>ChatSOS: LLM-based knowledge Q&amp;A system for safety engineering. (arXiv:2312.08629v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08629</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large language models (LLMs) have notably propelled
natural language processing (NLP) capabilities, demonstrating significant
potential in safety engineering applications. Despite these advancements, LLMs
face constraints in processing specialized tasks, attributed to factors such as
corpus size, input processing limitations, and privacy concerns. Obtaining
useful information from reliable sources in a limited time is crucial for LLM.
Addressing this, our study introduces an LLM-based Q&amp;amp;A system for safety
engineering, enhancing the comprehension and response accuracy of the model. We
employed prompt engineering to incorporate external knowledge databases, thus
enriching the LLM with up-to-date and reliable information. The system analyzes
historical incident reports through statistical methods, utilizes vector
embedding to construct a vector database, and offers an efficient
similarity-based search functionality. Our findings indicate that the
integration of external knowledge significantly augments the capabilities of
LLM for in-depth problem analysis and autonomous task assignment. It
effectively summarizes accident reports and provides pertinent recommendations.
This integration approach not only expands LLM applications in safety
engineering but also sets a precedent for future developments towards
automation and intelligent systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;Haiyang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dongping Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Q/0/1/0/all/0/1&quot;&gt;Qingzhao Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08642">
<title>Metacognition-Enhanced Few-Shot Prompting With Positive Reinforcement. (arXiv:2312.08642v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08642</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot prompting elicits the remarkable abilities of large language models
by equipping them with a few demonstration examples in the input. However, the
traditional method of providing large language models with all demonstration
input-output pairs at once may not effectively guide large language models to
learn the specific input-output mapping relationship. In this paper, inspired
by the regulatory and supportive role of metacognition in students&apos; learning,
we propose a novel metacognition-enhanced few-shot prompting, which guides
large language models to reflect on their thought processes to comprehensively
learn the given demonstration examples. Furthermore, considering that positive
reinforcement can improve students&apos; learning motivation, we introduce positive
reinforcement into our metacognition-enhanced few-shot prompting to promote the
few-shot learning of large language models by providing response-based positive
feedback. The experimental results on two real-world datasets show that our
metacognition-enhanced few-shot prompting with positive reinforcement surpasses
traditional few-shot prompting in classification accuracy and macro F1.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Y/0/1/0/all/0/1&quot;&gt;Yu Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Hong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liang He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08656">
<title>MaxK-GNN: Towards Theoretical Speed Limits for Accelerating Graph Neural Networks Training. (arXiv:2312.08656v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08656</link>
<description rdf:parseType="Literal">&lt;p&gt;In the acceleration of deep neural network training, the GPU has become the
mainstream platform. GPUs face substantial challenges on GNNs, such as workload
imbalance and memory access irregularities, leading to underutilized hardware.
Existing solutions such as PyG, DGL with cuSPARSE, and GNNAdvisor frameworks
partially address these challenges but memory traffic is still significant.
&lt;/p&gt;
&lt;p&gt;We argue that drastic performance improvements can only be achieved by the
vertical optimization of algorithm and system innovations, rather than treating
the speedup optimization as an &quot;after-thought&quot; (i.e., (i) given a GNN
algorithm, designing an accelerator, or (ii) given hardware, mainly optimizing
the GNN algorithm). In this paper, we present MaxK-GNN, an advanced
high-performance GPU training system integrating algorithm and system
innovation. (i) We introduce the MaxK nonlinearity and provide a theoretical
analysis of MaxK nonlinearity as a universal approximator, and present the
Compressed Balanced Sparse Row (CBSR) format, designed to store the data and
index of the feature matrix after nonlinearity; (ii) We design a coalescing
enhanced forward computation with row-wise product-based SpGEMM Kernel using
CBSR for input feature matrix fetching and strategic placement of a sparse
output accumulation buffer in shared memory; (iii) We develop an optimized
backward computation with outer product-based and SSpMM Kernel.
&lt;/p&gt;
&lt;p&gt;We conduct extensive evaluations of MaxK-GNN and report the end-to-end system
run-time. Experiments show that MaxK-GNN system could approach the theoretical
speedup limit according to Amdahl&apos;s law. We achieve comparable accuracy to SOTA
GNNs, but at a significantly increased speed: 3.22/4.24 times speedup (vs.
theoretical limits, 5.52/7.27 times) on Reddit compared to DGL and GNNAdvisor
implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hongwu Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shivdikar_K/0/1/0/all/0/1&quot;&gt;Kaustubh Shivdikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;MD Amit Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiahui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaoyi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_O/0/1/0/all/0/1&quot;&gt;Omer Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaeli_D/0/1/0/all/0/1&quot;&gt;David Kaeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08658">
<title>Real-time Autonomous Control of a Continuous Macroscopic Process as Demonstrated by Plastic Forming. (arXiv:2312.08658v1 [cond-mat.soft])</title>
<link>http://arxiv.org/abs/2312.08658</link>
<description rdf:parseType="Literal">&lt;p&gt;To meet the demands for more adaptable and expedient approaches to augment
both research and manufacturing, we report an autonomous system using real-time
in-situ characterization and an autonomous, decision-making processer based on
an active learning algorithm. This system was applied to a plastic film forming
system to highlight its efficiency and accuracy in determining the process
conditions for specified target film dimensions, importantly, without any human
intervention. Application of this system towards nine distinct film dimensions
demonstrated the system ability to quickly determine the appropriate and stable
process conditions (average 11 characterization-adjustment iterations, 19
minutes) and the ability to avoid traps, such as repetitive over-correction.
Furthermore, comparison of the achieved film dimensions to the target values
showed a high accuracy (R2 = 0.87, 0.90) for film width and thickness,
respectively. In addition, the use of an active learning algorithm afforded our
system to proceed optimization with zero initial training data, which was
unavailable due to the complex relationships between the control factors
(material supply rate, applied force, material viscosity) within the plastic
forming process. As our system is intrinsically general and can be applied to
any most material processes, these results have significant implications in
accelerating both research and industrial processes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Muroga_S/0/1/0/all/0/1&quot;&gt;Shun Muroga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Honda_T/0/1/0/all/0/1&quot;&gt;Takashi Honda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Miki_Y/0/1/0/all/0/1&quot;&gt;Yasuaki Miki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Nakajima_H/0/1/0/all/0/1&quot;&gt;Hideaki Nakajima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Futaba_D/0/1/0/all/0/1&quot;&gt;Don N. Futaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Hata_K/0/1/0/all/0/1&quot;&gt;Kenji Hata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08667">
<title>Data and Model Poisoning Backdoor Attacks on Wireless Federated Learning, and the Defense Mechanisms: A Comprehensive Survey. (arXiv:2312.08667v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.08667</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the greatly improved capabilities of devices, massive data, and
increasing concern about data privacy, Federated Learning (FL) has been
increasingly considered for applications to wireless communication networks
(WCNs). Wireless FL (WFL) is a distributed method of training a global deep
learning model in which a large number of participants each train a local model
on their training datasets and then upload the local model updates to a central
server. However, in general, non-independent and identically distributed
(non-IID) data of WCNs raises concerns about robustness, as a malicious
participant could potentially inject a &quot;backdoor&quot; into the global model by
uploading poisoned data or models over WCN. This could cause the model to
misclassify malicious inputs as a specific target class while behaving normally
with benign inputs. This survey provides a comprehensive review of the latest
backdoor attacks and defense mechanisms. It classifies them according to their
targets (data poisoning or model poisoning), the attack phase (local data
collection, training, or aggregation), and defense stage (local training,
before aggregation, during aggregation, or after aggregation). The strengths
and limitations of existing attack strategies and defense mechanisms are
analyzed in detail. Comparisons of existing attack methods and defense designs
are carried out, pointing to noteworthy findings, open challenges, and
potential future research directions related to security and privacy of WFL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yichen Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Youyang Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_W/0/1/0/all/0/1&quot;&gt;Wei Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yong Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_L/0/1/0/all/0/1&quot;&gt;Longxiang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hossain_E/0/1/0/all/0/1&quot;&gt;Ekram Hossain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08670">
<title>Temporal-Spatial Entropy Balancing for Causal Continuous Treatment-Effect Estimation. (arXiv:2312.08670v1 [stat.ME])</title>
<link>http://arxiv.org/abs/2312.08670</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of intracity freight transportation, changes in order volume are
significantly influenced by temporal and spatial factors. When building subsidy
and pricing strategies, predicting the causal effects of these strategies on
order volume is crucial. In the process of calculating causal effects,
confounding variables can have an impact. Traditional methods to control
confounding variables handle data from a holistic perspective, which cannot
ensure the precision of causal effects in specific temporal and spatial
dimensions. However, temporal and spatial dimensions are extremely critical in
the logistics field, and this limitation may directly affect the precision of
subsidy and pricing strategies. To address these issues, this study proposes a
technique based on flexible temporal-spatial grid partitioning. Furthermore,
based on the flexible grid partitioning technique, we further propose a
continuous entropy balancing method in the temporal-spatial domain, which named
TS-EBCT (Temporal-Spatial Entropy Balancing for Causal Continue Treatments).
The method proposed in this paper has been tested on two simulation datasets
and two real datasets, all of which have achieved excellent performance. In
fact, after applying the TS-EBCT method to the intracity freight transportation
field, the prediction accuracy of the causal effect has been significantly
improved. It brings good business benefits to the company&apos;s subsidy and pricing
strategies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hu_T/0/1/0/all/0/1&quot;&gt;Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Honglong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zeng_F/0/1/0/all/0/1&quot;&gt;Fan Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Min Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;XiangKun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yue Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mengran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jihao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08671">
<title>Uplifting the Expressive Power of Graph Neural Networks through Graph Partitioning. (arXiv:2312.08671v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08671</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have paved its way for being a cornerstone in
graph related learning tasks. From a theoretical perspective, the expressive
power of GNNs is primarily characterised according to their ability to
distinguish non-isomorphic graphs. It is a well-known fact that most of the
conventional GNNs are upper-bounded by Weisfeiler-Lehman graph isomorphism test
(1-WL). In this work, we study the expressive power of graph neural networks
through the lens of graph partitioning. This follows from our observation that
permutation invariant graph partitioning enables a powerful way of exploring
structural interactions among vertex sets and subgraphs, and can help uplifting
the expressive power of GNNs efficiently. Based on this, we first establish a
theoretical connection between graph partitioning and graph isomorphism. Then
we introduce a novel GNN architecture, namely Graph Partitioning Neural
Networks (GPNNs). We theoretically analyse how a graph partitioning scheme and
different kinds of structural interactions relate to the k-WL hierarchy.
Empirically, we demonstrate its superior performance over existing GNN models
in a variety of graph benchmark tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hevapathige_A/0/1/0/all/0/1&quot;&gt;Asela Hevapathige&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qing Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08672">
<title>CAT: A Causally Graph Attention Network for Trimming Heterophilic Graph. (arXiv:2312.08672v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08672</link>
<description rdf:parseType="Literal">&lt;p&gt;Local Attention-guided Message Passing Mechanism (LAMP) adopted in Graph
Attention Networks (GATs) is designed to adaptively learn the importance of
neighboring nodes for better local aggregation on the graph, which can bring
the representations of similar neighbors closer effectively, thus showing
stronger discrimination ability. However, existing GATs suffer from a
significant discrimination ability decline in heterophilic graphs because the
high proportion of dissimilar neighbors can weaken the self-attention of the
central node, jointly resulting in the deviation of the central node from
similar nodes in the representation space. This kind of effect generated by
neighboring nodes is called the Distraction Effect (DE) in this paper. To
estimate and weaken the DE of neighboring nodes, we propose a Causally graph
Attention network for Trimming heterophilic graph (CAT). To estimate the DE,
since the DE are generated through two paths (grab the attention assigned to
neighbors and reduce the self-attention of the central node), we use Total
Effect to model DE, which is a kind of causal estimand and can be estimated
from intervened data; To weaken the DE, we identify the neighbors with the
highest DE (we call them Distraction Neighbors) and remove them. We adopt three
representative GATs as the base model within the proposed CAT framework and
conduct experiments on seven heterophilic datasets in three different sizes.
Comparative experiments show that CAT can improve the node classification
accuracy of all base GAT models. Ablation experiments and visualization further
validate the enhancement of discrimination ability brought by CAT. The source
code is available at https://github.com/GeoX-Lab/CAT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_S/0/1/0/all/0/1&quot;&gt;Silu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Q/0/1/0/all/0/1&quot;&gt;Qinyao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xinsha Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Ling Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_R/0/1/0/all/0/1&quot;&gt;Ronghua Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lia_H/0/1/0/all/0/1&quot;&gt;Haifeng Lia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08677">
<title>Adaptive Shortcut Debiasing for Online Continual Learning. (arXiv:2312.08677v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08677</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel framework DropTop that suppresses the shortcut bias in
online continual learning (OCL) while being adaptive to the varying degree of
the shortcut bias incurred by continuously changing environment. By the
observed high-attention property of the shortcut bias, highly-activated
features are considered candidates for debiasing. More importantly, resolving
the limitation of the online environment where prior knowledge and auxiliary
data are not ready, two novel techniques -- feature map fusion and adaptive
intensity shifting -- enable us to automatically determine the appropriate
level and proportion of the candidate shortcut features to be dropped.
Extensive experiments on five benchmark datasets demonstrate that, when
combined with various OCL algorithms, DropTop increases the average accuracy by
up to 10.4% and decreases the forgetting by up to 63.2%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Doyoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1&quot;&gt;Dongmin Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yooju Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bang_J/0/1/0/all/0/1&quot;&gt;Jihwan Bang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_H/0/1/0/all/0/1&quot;&gt;Hwanjun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jae-Gil Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08679">
<title>A Local Appearance Model for Volumetric Capture of Diverse Hairstyle. (arXiv:2312.08679v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08679</link>
<description rdf:parseType="Literal">&lt;p&gt;Hair plays a significant role in personal identity and appearance, making it
an essential component of high-quality, photorealistic avatars. Existing
approaches either focus on modeling the facial region only or rely on
personalized models, limiting their generalizability and scalability. In this
paper, we present a novel method for creating high-fidelity avatars with
diverse hairstyles. Our method leverages the local similarity across different
hairstyles and learns a universal hair appearance prior from multi-view
captures of hundreds of people. This prior model takes 3D-aligned features as
input and generates dense radiance fields conditioned on a sparse point cloud
with color. As our model splits different hairstyles into local primitives and
builds prior at that level, it is capable of handling various hair topologies.
Through experiments, we demonstrate that our model captures a diverse range of
hairstyles and generalizes well to challenging new hairstyles. Empirical
results show that our method improves the state-of-the-art approaches in
capturing and generating photorealistic, personalized avatars with complete
hair.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_G/0/1/0/all/0/1&quot;&gt;Giljoo Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozic_A/0/1/0/all/0/1&quot;&gt;Aljaz Bozic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_C/0/1/0/all/0/1&quot;&gt;Chen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saragih_J/0/1/0/all/0/1&quot;&gt;Jason Saragih&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1&quot;&gt;Michael Zollhoefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodgins_J/0/1/0/all/0/1&quot;&gt;Jessica Hodgins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08680">
<title>Heterogeneous Graph Neural Architecture Search with GPT-4. (arXiv:2312.08680v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08680</link>
<description rdf:parseType="Literal">&lt;p&gt;Heterogeneous graph neural architecture search (HGNAS) represents a powerful
tool for automatically designing effective heterogeneous graph neural networks.
However, existing HGNAS algorithms suffer from inefficient searches and
unstable results. In this paper, we present a new GPT-4 based HGNAS model to
improve the search efficiency and search accuracy of HGNAS. Specifically, we
present a new GPT-4 enhanced Heterogeneous Graph Neural Architecture Search
(GHGNAS for short). The basic idea of GHGNAS is to design a set of prompts that
can guide GPT-4 toward the task of generating new heterogeneous graph neural
architectures. By iteratively asking GPT-4 with the prompts, GHGNAS continually
validates the accuracy of the generated HGNNs and uses the feedback to further
optimize the prompts. Experimental results show that GHGNAS can design new
HGNNs by leveraging the powerful generalization capability of GPT-4. Moreover,
GHGNAS runs more effectively and stably than previous HGNAS models based on
reinforcement learning and differentiable search algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Haoyuan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haishuai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08688">
<title>TigerBot: An Open Multilingual Multitask LLM. (arXiv:2312.08688v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08688</link>
<description rdf:parseType="Literal">&lt;p&gt;We release and introduce the TigerBot family of large language models (LLMs),
consisting of base and chat models, sized from 7, 13, 70 and 180 billion
parameters. We develop our models embarking from Llama-2 and BLOOM, and push
the boundary further in data, training algorithm, infrastructure, and
application tools. Our models yield meaningful performance gain over SOTA
open-source models, e.g., Llama-2, specifically 6\% gain in English and 20\%
gain in Chinese. TigerBot model family also achieves leading performance in
major academic and industrial benchmarks and leaderboards. We believe that
TigerBot represents just a snapshot of lightning-fast progression in LLM
open-source community. Therefore, we are thrilled to give back by publicly
releasing our models and reporting our approach behind, with additional
emphases on building SOTA LLMs in a democratized way and making LLMs of use in
real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ye Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Wei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Liangmin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Z/0/1/0/all/0/1&quot;&gt;Zhanxuan Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Cong Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08695">
<title>CPST: Comprehension-Preserving Style Transfer for Multi-Modal Narratives. (arXiv:2312.08695v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08695</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the challenges of style transfer in multi-modal visual
narratives. Among static visual narratives such as comics and manga, there are
distinct visual styles in terms of presentation. They include style features
across multiple dimensions, such as panel layout, size, shape, and color. They
include both visual and text media elements. The layout of both text and media
elements is also significant in terms of narrative communication. The
sequential transitions between panels are where readers make inferences about
the narrative world. These feature differences provide an interesting challenge
for style transfer in which there are distinctions between the processing of
features for each modality. We introduce the notion of comprehension-preserving
style transfer (CPST) in such multi-modal domains. CPST requires not only
traditional metrics of style transfer but also metrics of narrative
comprehension. To spur further research in this area, we present an annotated
dataset of comics and manga and an initial set of algorithms that utilize
separate style transfer modules for the visual, textual, and layout parameters.
To test whether the style transfer preserves narrative semantics, we evaluate
this algorithm through visual story cloze tests inspired by work in
computational cognition of narrative systems. Understanding the connection
between style and narrative semantics provides insight for applications ranging
from informational brochure designs to data storytelling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhala_A/0/1/0/all/0/1&quot;&gt;Arnav Jhala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08702">
<title>Rational Sensibility: LLM Enhanced Empathetic Response Generation Guided by Self-presentation Theory. (arXiv:2312.08702v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08702</link>
<description rdf:parseType="Literal">&lt;p&gt;Having the ability to empathize is crucial for accurately representing human
behavior during conversations. Despite numerous research aim to improve the
cognitive capability of models by incorporating external knowledge, there has
been limited attention on the sensible and rational expression of the
conversation itself, which are crucial components of the cognitive empathy.
Guided by self-presentation theory in sociology, we have designed an innovative
categorical approach that segregates historical dialogues into sensible and
rational sentences and subsequently elucidate the context through the designed
attention mechanism. However, the rational information within the conversation
is restricted and the external knowledge used in previous methods have
limitations of semantic contradiction and narrow vision field. Considering the
impressive performance of LLM in the domain of intelligent agent. We employ
LLaMA2-70b as a rational brain to analyze the profound logical information
maintained in conversations, which assists the model assessing the balance of
sensibility and rationality to produce quality empathetic responses.
Experimental evaluations demonstrate that our method outperforms other
comparable methods on both automatic and human evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Linzhuang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_J/0/1/0/all/0/1&quot;&gt;Jingxuan Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bihui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_L/0/1/0/all/0/1&quot;&gt;Liping Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08710">
<title>Gradient Informed Proximal Policy Optimization. (arXiv:2312.08710v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08710</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel policy learning method that integrates analytical
gradients from differentiable environments with the Proximal Policy
Optimization (PPO) algorithm. To incorporate analytical gradients into the PPO
framework, we introduce the concept of an {\alpha}-policy that stands as a
locally superior policy. By adaptively modifying the {\alpha} value, we can
effectively manage the influence of analytical policy gradients during
learning. To this end, we suggest metrics for assessing the variance and bias
of analytical gradients, reducing dependence on these gradients when high
variance or bias is detected. Our proposed approach outperforms baseline
algorithms in various scenarios, such as function optimization, physics
simulations, and traffic control environments. Our code can be found online:
https://github.com/SonSang/gippo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Son_S/0/1/0/all/0/1&quot;&gt;Sanghyun Son&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Laura Yu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sullivan_R/0/1/0/all/0/1&quot;&gt;Ryan Sullivan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yi-Ling Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Ming C. Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08720">
<title>Panel Transitions for Genre Analysis in Visual Narratives. (arXiv:2312.08720v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08720</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding how humans communicate and perceive narratives is important for
media technology research and development. This is particularly important in
current times when there are tools and algorithms that are easily available for
amateur users to create high-quality content. Narrative media develops over
time a set of recognizable patterns of features across similar artifacts. Genre
is one such grouping of artifacts for narrative media with similar patterns,
tropes, and story structures. While much work has been done on genre-based
classifications in text and video, we present a novel approach to do a
multi-modal analysis of genre based on comics and manga-style visual
narratives. We present a systematic feature analysis of an annotated dataset
that includes a variety of western and eastern visual books with annotations
for high-level narrative patterns. We then present a detailed analysis of the
contributions of high-level features to genre classification for this medium.
We highlight some of the limitations and challenges of our existing
computational approaches in modeling subjective labels. Our contributions to
the community are: a dataset of annotated manga books, a multi-modal analysis
of visual panels and text in a constrained and popular medium through
high-level features, and a systematic process for incorporating subjective
narrative patterns in computational models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yi-Chun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhala_A/0/1/0/all/0/1&quot;&gt;Arnav Jhala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08722">
<title>Quantifying Divergence for Human-AI Collaboration and Cognitive Trust. (arXiv:2312.08722v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08722</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the collaboration likelihood and measuring cognitive trust to AI
systems is more important than ever. To do that, previous research mostly focus
solely on the model features (e.g., accuracy, confidence) and ignore the human
factor. To address that, we propose several decision-making similarity measures
based on divergence metrics (e.g., KL, JSD) calculated over the labels acquired
from humans and a wide range of models. We conduct a user study on a textual
entailment task, where the users are provided with soft labels from various
models and asked to pick the closest option to them. The users are then shown
the similarities/differences to their most similar model and are surveyed for
their likelihood of collaboration and cognitive trust to the selected system.
Finally, we qualitatively and quantitatively analyze the relation between the
proposed decision-making similarity measures and the survey results. We find
that people tend to collaborate with their most similar models -- measured via
JSD -- yet this collaboration does not necessarily imply a similar level of
cognitive trust. We release all resources related to the user study (e.g.,
design, outputs), models, and metrics at our repo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kural_M/0/1/0/all/0/1&quot;&gt;M&amp;#xfc;ge Kural&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebesce_A/0/1/0/all/0/1&quot;&gt;Ali Gebe&amp;#x15f;&amp;#xe7;e&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chubakov_T/0/1/0/all/0/1&quot;&gt;Tilek Chubakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;zde G&amp;#xfc;l &amp;#x15e;ahin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08724">
<title>Personalized Path Recourse. (arXiv:2312.08724v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08724</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces Personalized Path Recourse, a novel method that
generates recourse paths for an agent. The objective is to achieve desired
goals (e.g., better outcomes compared to the agent&apos;s original paths of action),
while ensuring a high similarity to the agent&apos;s original paths and being
personalized to the agent. Personalization refers to the extent to which the
new path is tailored to the agent&apos;s observed behavior patterns from their
policy function. We train a personalized recourse agent to generate such
personalized paths, which are obtained using reward functions that consider the
goal, similarity, and personalization. The proposed method is applicable to
both reinforcement learning and supervised learning settings for correcting or
improving sequences of actions or sequences of data to achieve a pre-determined
goal. The method is evaluated in various settings and demonstrates promising
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_D/0/1/0/all/0/1&quot;&gt;Dat Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08726">
<title>Labels Need Prompts Too Mask Matching for Natural Language Understanding Tasks. (arXiv:2312.08726v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08726</link>
<description rdf:parseType="Literal">&lt;p&gt;Textual label names (descriptions) are typically semantically rich in many
natural language understanding (NLU) tasks. In this paper, we incorporate the
prompting methodology, which is widely used to enrich model input, into the
label side for the first time. Specifically, we propose a Mask Matching method,
which equips an input with a prompt and its label with another, and then makes
predictions by matching their mask representations. We evaluate our method
extensively on 8 NLU tasks with 14 datasets. The experimental results show that
Mask Matching significantly outperforms its counterparts of fine-tuning and
conventional prompt-tuning, setting up state-of-the-art performances in several
datasets. Mask Matching is particularly good at handling NLU tasks with large
label counts and informative label names. As pioneering efforts that
investigate the label-side prompt, we also discuss open issues for future
study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_W/0/1/0/all/0/1&quot;&gt;Wei Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Quansen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shikun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08762">
<title>Multi-modal Latent Space Learning for Chain-of-Thought Reasoning in Language Models. (arXiv:2312.08762v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08762</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain-of-thought (CoT) reasoning has exhibited impressive performance in
language models for solving complex tasks and answering questions. However,
many real-world questions require multi-modal information, such as text and
images. Previous research on multi-modal CoT has primarily focused on
extracting fixed image features from off-the-shelf vision models and then
fusing them with text using attention mechanisms. This approach has limitations
because these vision models were not designed for complex reasoning tasks and
do not align well with language thoughts. To overcome this limitation, we
introduce a novel approach for multi-modal CoT reasoning that utilizes latent
space learning via diffusion processes to generate effective image features
that align with language thoughts. Our method fuses image features and text
representations at a deep level and improves the complex reasoning ability of
multi-modal CoT. We demonstrate the efficacy of our proposed method on
multi-modal ScienceQA and machine translation benchmarks, achieving
state-of-the-art performance on ScienceQA. Overall, our approach offers a more
robust and effective solution for multi-modal reasoning in language models,
enhancing their ability to tackle complex real-world problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuchao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_X/0/1/0/all/0/1&quot;&gt;Xiantao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08763">
<title>Learning from Polar Representation: An Extreme-Adaptive Model for Long-Term Time Series Forecasting. (arXiv:2312.08763v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08763</link>
<description rdf:parseType="Literal">&lt;p&gt;In the hydrology field, time series forecasting is crucial for efficient
water resource management, improving flood and drought control and increasing
the safety and quality of life for the general population. However, predicting
long-term streamflow is a complex task due to the presence of extreme events.
It requires the capture of long-range dependencies and the modeling of rare but
important extreme values. Existing approaches often struggle to tackle these
dual challenges simultaneously. In this paper, we specifically delve into these
issues and propose Distance-weighted Auto-regularized Neural network (DAN), a
novel extreme-adaptive model for long-range forecasting of stremflow enhanced
by polar representation learning. DAN utilizes a distance-weighted multi-loss
mechanism and stackable blocks to dynamically refine indicator sequences from
exogenous data, while also being able to handle uni-variate time-series by
employing Gaussian Mixture probability modeling to improve robustness to severe
events. We also introduce Kruskal-Wallis sampling and gate control vectors to
handle imbalanced extreme data. On four real-life hydrologic streamflow
datasets, we demonstrate that DAN significantly outperforms both
state-of-the-art hydrologic time series prediction methods and general methods
designed for long-term time series prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anastasiu_D/0/1/0/all/0/1&quot;&gt;David C. Anastasiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08773">
<title>Offshore Wind Plant Instance Segmentation Using Sentinel-1 Time Series, GIS, and Semantic Segmentation Models. (arXiv:2312.08773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08773</link>
<description rdf:parseType="Literal">&lt;p&gt;Offshore wind farms represent a renewable energy source with a significant
global growth trend, and their monitoring is strategic for territorial and
environmental planning. This study&apos;s primary objective is to detect offshore
wind plants at an instance level using semantic segmentation models and
Sentinel-1 time series. The secondary objectives are: (a) to develop a database
consisting of labeled data and S-1 time series; (b) to compare the performance
of five deep semantic segmentation architectures (U-Net, U-Net++, Feature
Pyramid Network - FPN, DeepLabv3+, and LinkNet); (c) develop a novel
augmentation strategy that shuffles the positions of the images within the time
series; (d) investigate different dimensions of time series intervals (1, 5,
10, and 15 images); and (e) evaluate the semantic-to-instance conversion
procedure. LinkNet was the top-performing model, followed by U-Net++ and U-Net,
while FPN and DeepLabv3+ presented the worst results. The evaluation of
semantic segmentation models reveals enhanced Intersection over Union (IoU)
(25%) and F-score metrics (18%) with the augmentation of time series images.
The study showcases the augmentation strategy&apos;s capability to mitigate biases
and precisely detect invariant targets. Furthermore, the conversion from
semantic to instance segmentation demonstrates its efficacy in accurately
isolating individual instances within classified regions - simplifying training
data and reducing annotation effort and complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_O/0/1/0/all/0/1&quot;&gt;Osmar Luiz Ferreira de Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Junior_O/0/1/0/all/0/1&quot;&gt;Osmar Abilio de Carvalho Junior&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albuquerque_A/0/1/0/all/0/1&quot;&gt;Anesmar Olino de Albuquerque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_D/0/1/0/all/0/1&quot;&gt;Daniel Guerreiro e Silva&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08776">
<title>Approximate Integer Solution Counts over Linear Arithmetic Constraints. (arXiv:2312.08776v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2312.08776</link>
<description rdf:parseType="Literal">&lt;p&gt;Counting integer solutions of linear constraints has found interesting
applications in various fields. It is equivalent to the problem of counting
lattice points inside a polytope. However, state-of-the-art algorithms for this
problem become too slow for even a modest number of variables. In this paper,
we propose a new framework to approximate the lattice counts inside a polytope
with a new random-walk sampling method. The counts computed by our approach has
been proved approximately bounded by a $(\epsilon, \delta)$-bound. Experiments
on extensive benchmarks show that our algorithm could solve polytopes with
dozens of dimensions, which significantly outperforms state-of-the-art
counters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1&quot;&gt;Cunjing Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08782">
<title>Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis. (arXiv:2312.08782v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.08782</link>
<description rdf:parseType="Literal">&lt;p&gt;Building general-purpose robots that can operate seamlessly, in any
environment, with any object, and utilizing various skills to complete diverse
tasks has been a long-standing goal in Artificial Intelligence. Unfortunately,
however, most existing robotic systems have been constrained - having been
designed for specific tasks, trained on specific datasets, and deployed within
specific environments. These systems usually require extensively-labeled data,
rely on task-specific models, have numerous generalization issues when deployed
in real-world scenarios, and struggle to remain robust to distribution shifts.
Motivated by the impressive open-set performance and content generation
capabilities of web-scale, large-capacity pre-trained models (i.e., foundation
models) in research fields such as Natural Language Processing (NLP) and
Computer Vision (CV), we devote this survey to exploring (i) how these existing
foundation models from NLP and CV can be applied to the field of robotics, and
also exploring (ii) what a robotics-specific foundation model would look like.
We begin by providing an overview of what constitutes a conventional robotic
system and the fundamental barriers to making it universally applicable. Next,
we establish a taxonomy to discuss current work exploring ways to leverage
existing foundation models for robotics and develop ones catered to robotics.
Finally, we discuss key challenges and promising future directions in using
foundation models for enabling general-purpose robotic systems. We encourage
readers to view our ``living`` GitHub repository of resources, including papers
reviewed in this survey as well as related projects and repositories for
developing foundation models for robotics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yafei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Q/0/1/0/all/0/1&quot;&gt;Quanting Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patrikar_J/0/1/0/all/0/1&quot;&gt;Jay Patrikar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keetha_N/0/1/0/all/0/1&quot;&gt;Nikhil Keetha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seungchan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yaqi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhibo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_Y/0/1/0/all/0/1&quot;&gt;Yu-Quan Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sycara_K/0/1/0/all/0/1&quot;&gt;Katia Sycara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johnson_Roberson_M/0/1/0/all/0/1&quot;&gt;Matthew Johnson-Roberson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scherer_S/0/1/0/all/0/1&quot;&gt;Sebastian Scherer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kira_Z/0/1/0/all/0/1&quot;&gt;Zsolt Kira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08793">
<title>Forbidden Facts: An Investigation of Competing Objectives in Llama-2. (arXiv:2312.08793v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08793</link>
<description rdf:parseType="Literal">&lt;p&gt;LLMs often face competing pressures (for example helpfulness vs.
harmlessness). To understand how models resolve such conflicts, we study
Llama-2-chat models on the forbidden fact task. Specifically, we instruct
Llama-2 to truthfully complete a factual recall statement while forbidding it
from saying the correct answer. This often makes the model give incorrect
answers. We decompose Llama-2 into 1000+ components, and rank each one with
respect to how useful it is for forbidding the correct answer. We find that in
aggregate, around 35 components are enough to reliably implement the full
suppression behavior. However, these components are fairly heterogeneous and
many operate using faulty heuristics. We discover that one of these heuristics
can be exploited via a manually designed adversarial attack which we call The
California Attack. Our results highlight some roadblocks standing in the way of
being able to successfully interpret advanced ML systems. Project website
available at https://forbiddenfacts.github.io .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tony T. Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Miles Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_K/0/1/0/all/0/1&quot;&gt;Kaivu Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shavit_N/0/1/0/all/0/1&quot;&gt;Nir Shavit&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08800">
<title>Evaluating Large Language Models for Health-related Queries with Presuppositions. (arXiv:2312.08800v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08800</link>
<description rdf:parseType="Literal">&lt;p&gt;As corporations rush to integrate large language models (LLMs) to their
search offerings, it is critical that they provide factually accurate
information that is robust to any presuppositions that a user may express. In
this work, we introduce UPHILL, a dataset consisting of health-related queries
with varying degrees of presuppositions. Using UPHILL, we evaluate the factual
accuracy and consistency of InstructGPT, ChatGPT, and BingChat models. We find
that while model responses rarely disagree with true health claims (posed as
questions), they often fail to challenge false claims: responses from
InstructGPT agree with 32% of the false claims, ChatGPT 26% and BingChat 23%.
As we increase the extent of presupposition in input queries, the responses
from InstructGPT and ChatGPT agree with the claim considerably more often,
regardless of its veracity. Responses from BingChat, which rely on retrieved
webpages, are not as susceptible. Given the moderate factual accuracy, and the
inability of models to consistently correct false assumptions, our work calls
for a careful assessment of current LLMs for use in high-stakes scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaur_N/0/1/0/all/0/1&quot;&gt;Navreet Kaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1&quot;&gt;Monojit Choudhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pruthi_D/0/1/0/all/0/1&quot;&gt;Danish Pruthi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08801">
<title>Automated Process Planning Based on a Semantic Capability Model and SMT. (arXiv:2312.08801v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08801</link>
<description rdf:parseType="Literal">&lt;p&gt;In research of manufacturing systems and autonomous robots, the term
capability is used for a machine-interpretable specification of a system
function. Approaches in this research area develop information models that
capture all information relevant to interpret the requirements, effects and
behavior of functions. These approaches are intended to overcome the
heterogeneity resulting from the various types of processes and from the large
number of different vendors. However, these models and associated methods do
not offer solutions for automated process planning, i.e. finding a sequence of
individual capabilities required to manufacture a certain product or to
accomplish a mission using autonomous robots. Instead, this is a typical task
for AI planning approaches, which unfortunately require a high effort to create
the respective planning problem descriptions. In this paper, we present an
approach that combines these two topics: Starting from a semantic capability
model, an AI planning problem is automatically generated. The planning problem
is encoded using Satisfiability Modulo Theories and uses an existing solver to
find valid capability sequences including required parameter values. The
approach also offers possibilities to integrate existing human expertise and to
provide explanations for human operators in order to help understand planning
decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocher_A/0/1/0/all/0/1&quot;&gt;Aljosha K&amp;#xf6;cher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_L/0/1/0/all/0/1&quot;&gt;Luis Miguel Vieira da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fay_A/0/1/0/all/0/1&quot;&gt;Alexander Fay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08815">
<title>Implement services for business scenarios by combining basic emulators. (arXiv:2312.08815v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08815</link>
<description rdf:parseType="Literal">&lt;p&gt;This article mainly introduces how to use various basic emulators to form a
combined emulator in the Jiutian Intelligence Network Simulation Platform to
realize simulation service functions in different business scenarios. Among
them, the combined emulator is included. The business scenarios include
different practical applications such as multi-objective antenna optimization,
high traffic of business, CSI (channel state information) compression feedback,
etc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Miaomiao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08827">
<title>Artificial Intelligence and Human Geography. (arXiv:2312.08827v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08827</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper examines the recent advances and applications of AI in human
geography especially the use of machine (deep) learning, including place
representation and modeling, spatial analysis and predictive mapping, and urban
planning and design. AI technologies have enabled deeper insights into complex
human-environment interactions, contributing to more effective scientific
exploration, understanding of social dynamics, and spatial decision-making.
Furthermore, human geography offers crucial contributions to AI, particularly
in context-aware model development, human-centered design, biases and ethical
considerations, and data privacy. The synergy beween AI and human geography is
essential for addressing global challenges like disaster resilience, poverty,
and equitable resource access. This interdisciplinary collaboration between AI
and geography will help advance the development of GeoAI and promise a better
and sustainable world for all.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Song Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08834">
<title>Speeding up Photoacoustic Imaging using Diffusion Models. (arXiv:2312.08834v1 [physics.med-ph])</title>
<link>http://arxiv.org/abs/2312.08834</link>
<description rdf:parseType="Literal">&lt;p&gt;Background: Photoacoustic Microscopy (PAM) integrates optical and acoustic
imaging, offering enhanced penetration depth for detecting optical-absorbing
components in tissues. Nonetheless, challenges arise in scanning large areas
with high spatial resolution. With speed limitations imposed by laser pulse
repetition rates, the potential role of computational methods is highlighted in
accelerating PAM imaging. Purpose: We are proposing a novel and highly
adaptable DiffPam algorithm that utilizes diffusion models for speeding up the
photoacoustic imaging process. Method: We leveraged a diffusion model trained
exclusively on natural images, comparing its performance with an in-domain
trained U-Net model using a dataset focused on PAM images of mice brain
microvasculature. Results: Our findings indicate that DiffPam achieves
comparable performance to a dedicated U-Net model, without the need for a large
dataset or training a deep learning model. The study also introduces the
efficacy of shortened diffusion processes for reducing computing time without
compromising accuracy. Conclusion: This study underscores the significance of
DiffPam as a practical algorithm for reconstructing undersampled PAM images,
particularly for researchers with limited AI expertise and computational
resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Loc_I/0/1/0/all/0/1&quot;&gt;Irem Loc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Unlu_M/0/1/0/all/0/1&quot;&gt;Mehmet Burcin Unlu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08837">
<title>Learning Safety Constraints From Demonstration Using One-Class Decision Trees. (arXiv:2312.08837v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08837</link>
<description rdf:parseType="Literal">&lt;p&gt;The alignment of autonomous agents with human values is a pivotal challenge
when deploying these agents within physical environments, where safety is an
important concern. However, defining the agent&apos;s objective as a reward and/or
cost function is inherently complex and prone to human errors. In response to
this challenge, we present a novel approach that leverages one-class decision
trees to facilitate learning from expert demonstrations. These decision trees
provide a foundation for representing a set of constraints pertinent to the
given environment as a logical formula in disjunctive normal form. The learned
constraints are subsequently employed within an oracle constrained
reinforcement learning framework, enabling the acquisition of a safe policy. In
contrast to other methods, our approach offers an interpretable representation
of the constraints, a vital feature in safety-critical environments. To
validate the effectiveness of our proposed method, we conduct experiments in
synthetic benchmark domains and a realistic driving environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baert_M/0/1/0/all/0/1&quot;&gt;Mattijs Baert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leroux_S/0/1/0/all/0/1&quot;&gt;Sam Leroux&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simoens_P/0/1/0/all/0/1&quot;&gt;Pieter Simoens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08843">
<title>Diffusion-C: Unveiling the Generative Challenges of Diffusion Models through Corrupted Data. (arXiv:2312.08843v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08843</link>
<description rdf:parseType="Literal">&lt;p&gt;In our contemporary academic inquiry, we present &quot;Diffusion-C,&quot; a
foundational methodology to analyze the generative restrictions of Diffusion
Models, particularly those akin to GANs, DDPM, and DDIM. By employing input
visual data that has been subjected to a myriad of corruption modalities and
intensities, we elucidate the performance characteristics of those Diffusion
Models. The noise component takes center stage in our analysis, hypothesized to
be a pivotal element influencing the mechanics of deep learning systems. In our
rigorous expedition utilizing Diffusion-C, we have discerned the following
critical observations: (I) Within the milieu of generative models under the
Diffusion taxonomy, DDPM emerges as a paragon, consistently exhibiting superior
performance metrics. (II) Within the vast spectrum of corruption frameworks,
the fog and fractal corruptions notably undermine the functional robustness of
both DDPM and DDIM. (III) The vulnerability of Diffusion Models to these
particular corruptions is significantly influenced by topological and
statistical similarities, particularly concerning the alignment between mean
and variance. This scholarly work highlights Diffusion-C&apos;s core understandings
regarding the impacts of various corruptions, setting the stage for future
research endeavors in the realm of generative models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1&quot;&gt;Keywoong Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Suan Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wookey Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08847">
<title>Knowledge-Driven Modulation of Neural Networks with Attention Mechanism for Next Activity Prediction. (arXiv:2312.08847v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08847</link>
<description rdf:parseType="Literal">&lt;p&gt;Predictive Process Monitoring (PPM) aims at leveraging historic process
execution data to predict how ongoing executions will continue up to their
completion. In recent years, PPM techniques for the prediction of the next
activities have matured significantly, mainly thanks to the use of Neural
Networks (NNs) as a predictor. While their performance is difficult to beat in
the general case, there are specific situations where background process
knowledge can be helpful. Such knowledge can be leveraged for improving the
quality of predictions for exceptional process executions or when the process
changes due to a concept drift. In this paper, we present a Symbolic[Neuro]
system that leverages background knowledge expressed in terms of a procedural
process model to offset the under-sampling in the training data. More
specifically, we make predictions using NNs with attention mechanism, an
emerging technology in the NN field. The system has been tested on several
real-life logs showing an improvement in the performance of the prediction
task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donadello_I/0/1/0/all/0/1&quot;&gt;Ivan Donadello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;Jonghyeon Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Maria Maggi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendling_J/0/1/0/all/0/1&quot;&gt;Jan Mendling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riva_F/0/1/0/all/0/1&quot;&gt;Francesco Riva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weidlich_M/0/1/0/all/0/1&quot;&gt;Matthias Weidlich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08873">
<title>Diffusion Cocktail: Fused Generation from Diffusion Models. (arXiv:2312.08873v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08873</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models excel at generating high-quality images and are easy to
extend, making them extremely popular among active users who have created an
extensive collection of diffusion models with various styles by fine-tuning
base models such as Stable Diffusion. Recent work has focused on uncovering
semantic and visual information encoded in various components of a diffusion
model, enabling better generation quality and more fine-grained control.
However, those methods target improving a single model and overlook the vastly
available collection of fine-tuned diffusion models. In this work, we study the
combinations of diffusion models. We propose Diffusion Cocktail (Ditail), a
training-free method that can accurately transfer content information between
two diffusion models. This allows us to perform diverse generations using a set
of diffusion models, resulting in novel images that are unlikely to be obtained
by a single model alone. We also explore utilizing Ditail for style transfer,
with the target style set by a diffusion model instead of an image. Ditail
offers a more detailed manipulation of the diffusion generation, thereby
enabling the vast community to integrate various styles and contents seamlessly
and generate any content of any style.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Haoming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuanhe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_H/0/1/0/all/0/1&quot;&gt;Hongyi Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08898">
<title>Detection and Defense of Unlearnable Examples. (arXiv:2312.08898v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08898</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy preserving has become increasingly critical with the emergence of
social media. Unlearnable examples have been proposed to avoid leaking personal
information on the Internet by degrading generalization abilities of deep
learning models. However, our study reveals that unlearnable examples are
easily detectable. We provide theoretical results on linear separability of
certain unlearnable poisoned dataset and simple network based detection methods
that can identify all existing unlearnable examples, as demonstrated by
extensive experiments. Detectability of unlearnable examples with simple
networks motivates us to design a novel defense method. We propose using
stronger data augmentations coupled with adversarial noises generated by simple
networks, to degrade the detectability and thus provide effective defense
against unlearnable examples with a lower cost. Adversarial training with large
budgets is a widely-used defense method on unlearnable examples. We establish
quantitative criteria between the poison and adversarial budgets which
determine the existence of robust unlearnable examples or the failure of the
adversarial defense.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yifan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lijia Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xiao-Shan Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08901">
<title>Boosting LLM Reasoning: Push the Limits of Few-shot Learning with Reinforced In-Context Pruning. (arXiv:2312.08901v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08901</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown impressive capabilities in various
tasks, yet they still struggle with math reasoning. Despite efforts to optimize
Chain-of-Thoughts (CoT) prompts and fine-tune LLMs, the potential of few-shot
learning remains unexplored. In this work, we propose CoT-Max, a novel approach
pushing the boundaries of few-shot CoT learning to improve LLM math reasoning
capabilities. CoT-Max addresses the challenges of the selection of useful
examples and limited number of examples due to restricted context window
length. Inspired by our observation that natural language inputs contain many
redundancy, we propose a coarse-to-fine pruner as a plug-and-play module for
LLMs, which first identifies crucial CoT examples from a large batch and then
further prunes unimportant tokens. To train the pruner, we collect a math
reasoning dataset with diverse difficulty and steps, introduce a reward to
measure both the input&apos;s effectiveness for math reasoning and token length
constraints, and propose a novel training approach with reinforcement learning.
As a result, CoT-Max significantly outperforms CoT and few-shot prompting
baselines across various LLMs (LLaMA2-7B, 13B, 70B) and 5 mathematical
datasets, achieving up to 4.55% absolute improvements. Remarkably, without any
fine-tuning, LLaMA2-70B with CoT-Max surpasses GPT-3.5 and a wide range of
larger LLMs (PaLM, Minerva, etc.) on the GSM8K.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xijie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Lyna Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Mao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08917">
<title>An Incremental Unified Framework for Small Defect Inspection. (arXiv:2312.08917v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08917</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI)-driven defect inspection is pivotal in
industrial manufacturing. Yet, many methods, tailored to specific pipelines,
grapple with diverse product portfolios and evolving processes. Addressing
this, we present the Incremental Unified Framework (IUF) that can reduce the
feature conflict problem when continuously integrating new objects in the
pipeline, making it advantageous in object-incremental learning scenarios.
Employing a state-of-the-art transformer, we introduce Object-Aware
Self-Attention (OASA) to delineate distinct semantic boundaries. Semantic
Compression Loss (SCL) is integrated to optimize non-primary semantic space,
enhancing network adaptability for novel objects. Additionally, we prioritize
retaining the features of established objects during weight updates.
Demonstrating prowess in both image and pixel-level defect inspection, our
approach achieves state-of-the-art performance, proving indispensable for
dynamic and scalable industrial inspections. Our code will be released at
https://github.com/jqtangust/IUF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruizheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Sixing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_T/0/1/0/all/0/1&quot;&gt;Tsz Wa Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_M/0/1/0/all/0/1&quot;&gt;Ming Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying-Cong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsung_F/0/1/0/all/0/1&quot;&gt;Fugee Tsung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08926">
<title>Modeling Complex Mathematical Reasoning via Large Language Model based MathAgent. (arXiv:2312.08926v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08926</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) face challenges in solving complex mathematical
problems that require comprehensive capacities to parse the statements,
associate domain knowledge, perform compound logical reasoning, and integrate
the intermediate rationales. Tackling all these problems once could be arduous
for LLMs, thus leading to confusion in generation. In this work, we explore the
potential of enhancing LLMs with agents by meticulous decomposition and
modeling of mathematical reasoning process. Specifically, we propose a formal
description of the mathematical solving and extend LLMs with an agent-based
zero-shot framework named
$\bf{P}$lanner-$\bf{R}$easoner-$\bf{E}$xecutor-$\bf{R}$eflector (PRER). We
further provide and implement two MathAgents that define the logical forms and
inherent relations via a pool of actions in different grains and orientations:
MathAgent-M adapts its actions to LLMs, while MathAgent-H aligns with
humankind. Experiments on miniF2F and MATH have demonstrated the effectiveness
of PRER and proposed MathAgents, achieving an increase of
$12.3\%$($53.9\%\xrightarrow{}66.2\%$) on the MiniF2F, $9.2\%$
($49.8\%\xrightarrow{}59.0\%$) on MATH, and
$13.2\%$($23.2\%\xrightarrow{}35.4\%$) for level-5 problems of MATH against
GPT-4. Further analytical results provide more insightful perspectives on
exploiting the behaviors of LLMs as agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Haoran Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Q/0/1/0/all/0/1&quot;&gt;Qinyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shaohua Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanyan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jidong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaohui Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08931">
<title>N-Gram Unsupervised Compoundation and Feature Injection for Better Symbolic Music Understanding. (arXiv:2312.08931v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.08931</link>
<description rdf:parseType="Literal">&lt;p&gt;The first step to apply deep learning techniques for symbolic music
understanding is to transform musical pieces (mainly in MIDI format) into
sequences of predefined tokens like note pitch, note velocity, and chords.
Subsequently, the sequences are fed into a neural sequence model to accomplish
specific tasks. Music sequences exhibit strong correlations between adjacent
elements, making them prime candidates for N-gram techniques from Natural
Language Processing (NLP). Consider classical piano music: specific melodies
might recur throughout a piece, with subtle variations each time. In this
paper, we propose a novel method, NG-Midiformer, for understanding symbolic
music sequences that leverages the N-gram approach. Our method involves first
processing music pieces into word-like sequences with our proposed unsupervised
compoundation, followed by using our N-gram Transformer encoder, which can
effectively incorporate N-gram information to enhance the primary encoder part
for better understanding of music sequences. The pre-training process on
large-scale music datasets enables the model to thoroughly learn the N-gram
information contained within music sequences, and subsequently apply this
information for making inferences during the fine-tuning stage. Experiment on
various datasets demonstrate the effectiveness of our method and achieved
state-of-the-art performance on a series of music understanding downstream
tasks. The code and model weights will be released at
https://github.com/WouuYoauin/NG-Midiformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_J/0/1/0/all/0/1&quot;&gt;Jinhao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuchao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiajia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08935">
<title>Math-Shepherd: A Label-Free Step-by-Step Verifier for LLMs in Mathematical Reasoning. (arXiv:2312.08935v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08935</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated remarkable capabilities across
a wide range of tasks. However, even the most advanced open-source LLMs, such
as the LLaMA family models, still face challenges when it comes to accurately
solving complex multi-step mathematical problems. In this paper, we present an
innovative process-oriented math verifier called \textbf{Math-Shepherd}, which
assigns a reward score to each step of the LLM&apos;s outputs on math problems. The
training of Math-Shepherd is achieved using automatically constructed
process-wise supervision data, breaking the bottleneck of heavy reliance on
manual annotation in existing work. With the guidance of Math-Shepherd, a
series of open-source LLMs demonstrate exceptional performance. Among them,
DeepSeek 67B \citep{DeepSeek-llm} stands out by achieving accuracy rates of
93.3\% on the GSM8K dataset and 48.1\% on the MATH dataset, without external
enhancement such as tool usage. Our Math-Shepherd also outperforms the
self-consistency method and other existing verification models. We believe that
automatic process supervision holds significant potential for the future
evolution of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peiyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Z/0/1/0/all/0/1&quot;&gt;Zhihong Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;R.X. Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_D/0/1/0/all/0/1&quot;&gt;Damai Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Deli Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Y.Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Z/0/1/0/all/0/1&quot;&gt;Zhifang Sui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08948">
<title>LSTM Network Analysis of Vehicle-Type Fatalities on Great Britain&apos;s Roads. (arXiv:2312.08948v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08948</link>
<description rdf:parseType="Literal">&lt;p&gt;This study harnesses the predictive capabilities of Long Short-Term Memory
(LSTM) networks to analyse and predict road traffic accidents in Great Britain.
It addresses the challenge of traffic accident forecasting, which is paramount
for devising effective preventive measures. We utilised an extensive dataset
encompassing reported collisions, casualties, and vehicles involvements from
1926 to 2022, provided by the Department for Transport (DfT). The data
underwent stringent processing to rectify missing values and normalise
features, ensuring robust LSTM network input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oketunji_A/0/1/0/all/0/1&quot;&gt;Abiodun Finbarrs Oketunji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanify_J/0/1/0/all/0/1&quot;&gt;James Hanify&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heffron_Smith_S/0/1/0/all/0/1&quot;&gt;Salter Heffron-Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08958">
<title>LiFT: Unsupervised Reinforcement Learning with Foundation Models as Teachers. (arXiv:2312.08958v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08958</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a framework that leverages foundation models as teachers, guiding
a reinforcement learning agent to acquire semantically meaningful behavior
without human feedback. In our framework, the agent receives task instructions
grounded in a training environment from large language models. Then, a
vision-language model guides the agent in learning the multi-task
language-conditioned policy by providing reward feedback. We demonstrate that
our method can learn semantically meaningful skills in a challenging open-ended
MineDojo environment while prior unsupervised skill discovery methods struggle.
Additionally, we discuss observed challenges of using off-the-shelf foundation
models as teachers and our efforts to address them.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_T/0/1/0/all/0/1&quot;&gt;Taewook Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juyong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jesse Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_S/0/1/0/all/0/1&quot;&gt;Sung Ju Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Joseph J. Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pertsch_K/0/1/0/all/0/1&quot;&gt;Karl Pertsch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08968">
<title>Detecting value-expressive text posts in Russian social media. (arXiv:2312.08968v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.08968</link>
<description rdf:parseType="Literal">&lt;p&gt;Basic values are concepts or beliefs which pertain to desirable end-states
and transcend specific situations. Studying personal values in social media can
illuminate how and why societal values evolve especially when the stimuli-based
methods, such as surveys, are inefficient, for instance, in hard-to-reach
populations. On the other hand, user-generated content is driven by the massive
use of stereotyped, culturally defined speech constructions rather than
authentic expressions of personal values. We aimed to find a model that can
accurately detect value-expressive posts in Russian social media VKontakte. A
training dataset of 5,035 posts was annotated by three experts, 304
crowd-workers and ChatGPT. Crowd-workers and experts showed only moderate
agreement in categorizing posts. ChatGPT was more consistent but struggled with
spam detection. We applied an ensemble of human- and AI-assisted annotation
involving active learning approach, subsequently trained several LLMs and
selected a model based on embeddings from pre-trained fine-tuned rubert-tiny2,
and reached a high quality of value detection with F1 = 0.75 (F1-macro = 0.80).
This model provides a crucial step to a study of values within and between
Russian social media users.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milkova_M/0/1/0/all/0/1&quot;&gt;Maria Milkova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rudnev_M/0/1/0/all/0/1&quot;&gt;Maksim Rudnev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okolskaya_L/0/1/0/all/0/1&quot;&gt;Lidia Okolskaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08975">
<title>On Mask-based Image Set Desensitization with Recognition Support. (arXiv:2312.08975v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.08975</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, Deep Neural Networks (DNN) have emerged as a practical
method for image recognition. The raw data, which contain sensitive
information, are generally exploited within the training process. However, when
the training process is outsourced to a third-party organization, the raw data
should be desensitized before being transferred to protect sensitive
information. Although masks are widely applied to hide important sensitive
information, preventing inpainting masked images is critical, which may restore
the sensitive information. The corresponding models should be adjusted for the
masked images to reduce the degradation of the performance for recognition or
classification tasks due to the desensitization of images. In this paper, we
propose a mask-based image desensitization approach while supporting
recognition. This approach consists of a mask generation algorithm and a model
adjustment method. We propose exploiting an interpretation algorithm to
maintain critical information for the recognition task in the mask generation
algorithm. In addition, we propose a feature selection masknet as the model
adjustment method to improve the performance based on the masked images.
Extensive experimentation results based on multiple image datasets reveal
significant advantages (up to 9.34% in terms of accuracy) of our approach for
image desensitization while supporting recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qilong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Ji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chongsheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_D/0/1/0/all/0/1&quot;&gt;Dejing Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08977">
<title>Weighted Ensemble Models Are Strong Continual Learners. (arXiv:2312.08977v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.08977</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we study the problem of continual learning (CL) where the goal
is to learn a model on a sequence of tasks, such that the data from the
previous tasks becomes unavailable while learning on the current task data. CL
is essentially a balancing act between being able to learn on the new task
(i.e., plasticity) and maintaining the performance on the previously learned
concepts (i.e., stability). With an aim to address the stability-plasticity
trade-off, we propose to perform weight-ensembling of the model parameters of
the previous and current task. This weight-ensembled model, which we call
Continual Model Averaging (or CoMA), attains high accuracy on the current task
by leveraging plasticity, while not deviating too far from the previous weight
configuration, ensuring stability. We also propose an improved variant of CoMA,
named Continual Fisher-weighted Model Averaging (or CoFiMA), that selectively
weighs each parameter in the weight ensemble by leveraging the Fisher
information of the weights of the model. Both the variants are conceptually
simple, easy to implement, and effective in attaining state-of-the-art
performance on several standard CL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marouf_I/0/1/0/all/0/1&quot;&gt;Imad Eddine Marouf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhankar Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lathuiliere_S/0/1/0/all/0/1&quot;&gt;St&amp;#xe9;phane Lathuili&amp;#xe8;re&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08987">
<title>Unbiased organism-agnostic and highly sensitive signal peptide predictor with deep protein language model. (arXiv:2312.08987v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08987</link>
<description rdf:parseType="Literal">&lt;p&gt;Signal peptide (SP) is a short peptide located in the N-terminus of proteins.
It is essential to target and transfer transmembrane and secreted proteins to
correct positions. Compared with traditional experimental methods to identify
signal peptides, computational methods are faster and more efficient, which are
more practical for analyzing thousands or even millions of protein sequences,
especially for metagenomic data. Here we present Unbiased Organism-agnostic
Signal Peptide Network (USPNet), a signal peptide classification and cleavage
site prediction deep learning method that takes advantage of protein language
models. We propose to apply label distribution-aware margin loss to handle data
imbalance problems and use evolutionary information of protein to enrich
representation and overcome species information dependence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_J/0/1/0/all/0/1&quot;&gt;Junbo Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qinze Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shenyang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Q/0/1/0/all/0/1&quot;&gt;Qingxiong Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08990">
<title>Proving Conjectures Acquired by Composing Multiple Biases. (arXiv:2312.08990v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.08990</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the proofs of the conjectures mentioned in the paper published in
the proceedings of the 2024 AAAI conference [1], and discovered by the
decomposition methods presented in the same paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheukam_Ngouonou_J/0/1/0/all/0/1&quot;&gt;Jovial Cheukam-Ngouonou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gindullin_R/0/1/0/all/0/1&quot;&gt;Ramiz Gindullin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beldiceanu_N/0/1/0/all/0/1&quot;&gt;Nicolas Beldiceanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douence_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Douence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quimper_C/0/1/0/all/0/1&quot;&gt;Claude-Guy Quimper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08995">
<title>FrameFinder: Explorative Multi-Perspective Framing Extraction from News Headlines. (arXiv:2312.08995v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2312.08995</link>
<description rdf:parseType="Literal">&lt;p&gt;Revealing the framing of news articles is an important yet neglected task in
information seeking and retrieval. In the present work, we present FrameFinder,
an open tool for extracting and analyzing frames in textual data. FrameFinder
visually represents the frames of text from three perspectives, i.e., (i) frame
labels, (ii) frame dimensions, and (iii) frame structure. By analyzing the
well-established gun violence frame corpus, we demonstrate the merits of our
proposed solution to support social science research and call for subsequent
integration into information interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1&quot;&gt;Markus Reiter-Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klosch_B/0/1/0/all/0/1&quot;&gt;Beate Kl&amp;#xf6;sch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadler_M/0/1/0/all/0/1&quot;&gt;Markus Hadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1&quot;&gt;Elisabeth Lex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08998">
<title>Design, construction and evaluation of emotional multimodal pathological speech database. (arXiv:2312.08998v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.08998</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of an available emotion pathology database is one of the key
obstacles in studying the emotion expression status of patients with
dysarthria. The first Chinese multimodal emotional pathological speech database
containing multi-perspective information is constructed in this paper. It
includes 29 controls and 39 patients with different degrees of motor
dysarthria, expressing happy, sad, angry and neutral emotions. All emotional
speech was labeled for intelligibility, types and discrete dimensional emotions
by developed WeChat mini-program. The subjective analysis justifies from
emotion discrimination accuracy, speech intelligibility, valence-arousal
spatial distribution, and correlation between SCL-90 and disease severity. The
automatic recognition tested on speech and glottal data, with average accuracy
of 78% for controls and 60% for patients in audio, while 51% for controls and
38% for patients in glottal data, indicating an influence of the disease on
emotional expression.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Ting Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Duan_S/0/1/0/all/0/1&quot;&gt;Shufei Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_H/0/1/0/all/0/1&quot;&gt;Huizhi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09007">
<title>LLMind: Orchestrating AI and IoT with LLMs for Complex Task Execution. (arXiv:2312.09007v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2312.09007</link>
<description rdf:parseType="Literal">&lt;p&gt;In this article, we introduce LLMind, an innovative AI framework that
utilizes large language models (LLMs) as a central orchestrator. The framework
integrates LLMs with domain-specific AI modules, enabling IoT devices to
collaborate effectively in executing complex tasks. The LLM performs planning
and generates control scripts using a reliable and precise language-code
transformation approach based on finite state machines (FSMs). The LLM engages
in natural conversations with users, employing role-playing techniques to
generate contextually appropriate responses. Additionally, users can interact
easily with the AI agent via a user-friendly social media platform. The
framework also incorporates semantic analysis and response optimization
techniques to enhance speed and effectiveness. Ultimately, this framework is
designed not only to innovate IoT device control and enrich user experiences
but also to foster an intelligent and integrated IoT device ecosystem that
evolves and becomes more sophisticated through continuing user and machine
interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Hongwei Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yulin Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_S/0/1/0/all/0/1&quot;&gt;Soung Chang Liew&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09009">
<title>Adaptive parameter sharing for multi-agent reinforcement learning. (arXiv:2312.09009v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.09009</link>
<description rdf:parseType="Literal">&lt;p&gt;Parameter sharing, as an important technique in multi-agent systems, can
effectively solve the scalability issue in large-scale agent problems. However,
the effectiveness of parameter sharing largely depends on the environment
setting. When agents have different identities or tasks, naive parameter
sharing makes it difficult to generate sufficiently differentiated strategies
for agents. Inspired by research pertaining to the brain in biology, we propose
a novel parameter sharing method. It maps each type of agent to different
regions within a shared network based on their identity, resulting in distinct
subnetworks. Therefore, our method can increase the diversity of strategies
among different agents without introducing additional training parameters.
Through experiments conducted in multiple environments, our method has shown
better performance than other parameter sharing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dapeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_N/0/1/0/all/0/1&quot;&gt;Na Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1&quot;&gt;Guoliang Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2104.00640">
<title>AmbiFC: Fact-Checking Ambiguous Claims with Evidence. (arXiv:2104.00640v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2104.00640</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated fact-checking systems verify claims against evidence to predict
their veracity. In real-world scenarios, the retrieved evidence may not
unambiguously support or refute the claim and yield conflicting but valid
interpretations. Existing fact-checking datasets assume that the models
developed with them predict a single veracity label for each claim, thus
discouraging the handling of such ambiguity. To address this issue we present
AmbiFC, a fact-checking dataset with 10k claims derived from real-world
information needs. It contains fine-grained evidence annotations of 50k
passages from 5k Wikipedia pages. We analyze the disagreements arising from
ambiguity when comparing claims against evidence in AmbiFC, observing a strong
correlation of annotator disagreement with linguistic phenomena such as
underspecification and probabilistic reasoning. We develop models for
predicting veracity handling this ambiguity via soft labels and find that a
pipeline that learns the label distribution for sentence-level evidence
selection and veracity prediction yields the best performance. We compare
models trained on different subsets of AmbiFC and show that models trained on
the ambiguous instances perform better when faced with the identified
linguistic phenomena.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glockner_M/0/1/0/all/0/1&quot;&gt;Max Glockner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staliunaite_I/0/1/0/all/0/1&quot;&gt;Ieva Stali&amp;#x16b;nait&amp;#x117;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thorne_J/0/1/0/all/0/1&quot;&gt;James Thorne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vallejo_G/0/1/0/all/0/1&quot;&gt;Gisela Vallejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlachos_A/0/1/0/all/0/1&quot;&gt;Andreas Vlachos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.05300">
<title>On syntactically similar logic programs and sequential decompositions. (arXiv:2109.05300v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2109.05300</link>
<description rdf:parseType="Literal">&lt;p&gt;Rule-based reasoning is an essential part of human intelligence prominently
formalized in artificial intelligence research via logic programs. Describing
complex objects as the composition of elementary ones is a common strategy in
computer science and science in general. The author has recently introduced the
sequential composition of logic programs in the context of logic-based
analogical reasoning and learning in logic programming. Motivated by these
applications, in this paper we construct a qualitative and algebraic notion of
syntactic logic program similarity from sequential decompositions of programs.
We then show how similarity can be used to answer queries across different
domains via a one-step reduction. In a broader sense, this paper is a further
step towards an algebraic theory of logic programming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antic_C/0/1/0/all/0/1&quot;&gt;Christian Antic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.10789">
<title>Verification of Locally Tight Programs. (arXiv:2204.10789v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2204.10789</link>
<description rdf:parseType="Literal">&lt;p&gt;Program completion is a translation from the language of logic programs into
the language of first-order theories. Its original definition has been extended
to programs that include integer arithmetic, accept input, and distinguish
between output predicates and auxiliary predicates. For tight programs, that
generalization of completion is known to match the stable model semantics,
which is the basis of answer set programming. We show that the tightness
condition in this theorem can be replaced by a less restrictive &quot;local
tightness&quot; requirement. From this fact we conclude that the proof assistant
anthem-p2p can be used to verify equivalence between locally tight programs.
Under consideration for publication in Theory and Practice of Logic Programming
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fandinno_J/0/1/0/all/0/1&quot;&gt;Jorge Fandinno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lifschitz_V/0/1/0/all/0/1&quot;&gt;Vladimir Lifschitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Temple_N/0/1/0/all/0/1&quot;&gt;Nathan Temple&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10083">
<title>A Unified Experiment Design Approach for Cyclic and Acyclic Causal Models. (arXiv:2205.10083v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10083</link>
<description rdf:parseType="Literal">&lt;p&gt;We study experiment design for unique identification of the causal graph of a
simple SCM, where the graph may contain cycles. The presence of cycles in the
structure introduces major challenges for experiment design as, unlike acyclic
graphs, learning the skeleton of causal graphs with cycles may not be possible
from merely the observational distribution. Furthermore, intervening on a
variable in such graphs does not necessarily lead to orienting all the edges
incident to it. In this paper, we propose an experiment design approach that
can learn both cyclic and acyclic graphs and hence, unifies the task of
experiment design for both types of graphs. We provide a lower bound on the
number of experiments required to guarantee the unique identification of the
causal graph in the worst case, showing that the proposed approach is
order-optimal in terms of the number of experiments up to an additive
logarithmic term. Moreover, we extend our result to the setting where the size
of each experiment is bounded by a constant. For this case, we show that our
approach is optimal in terms of the size of the largest experiment required for
uniquely identifying the causal graph in the worst case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mokhtarian_E/0/1/0/all/0/1&quot;&gt;Ehsan Mokhtarian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salehkaleybar_S/0/1/0/all/0/1&quot;&gt;Saber Salehkaleybar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassami_A/0/1/0/all/0/1&quot;&gt;AmirEmad Ghassami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiyavash_N/0/1/0/all/0/1&quot;&gt;Negar Kiyavash&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11792">
<title>Two-dimensional total absorption spectroscopy with conditional generative adversarial networks. (arXiv:2206.11792v3 [nucl-ex] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11792</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the use of machine learning techniques to remove the response of
large volume $\gamma$-ray detectors from experimental spectra. Segmented
$\gamma$-ray total absorption spectrometers (TAS) allow for the simultaneous
measurement of individual $\gamma$-ray energy (E$_\gamma$) and total excitation
energy (E$_x$). Analysis of TAS detector data is complicated by the fact that
the E$_x$ and E$_\gamma$ quantities are correlated, and therefore, techniques
that simply unfold using E$_x$ and E$_\gamma$ response functions independently
are not as accurate. In this work, we investigate the use of conditional
generative adversarial networks (cGANs) to simultaneously unfold $E_{x}$ and
$E_{\gamma}$ data in TAS detectors. Specifically, we employ a \texttt{Pix2Pix}
cGAN, a generative modeling technique based on recent advances in deep
learning, to treat \rawmatrix~ matrix unfolding as an image-to-image
translation problem. We present results for simulated and experimental matrices
of single-$\gamma$ and double-$\gamma$ decay cascades. Our model demonstrates
characterization capabilities within detector resolution limits for upwards of
93% of simulated test cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/nucl-ex/1/au:+Dembski_C/0/1/0/all/0/1&quot;&gt;Cade Dembski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-ex/1/au:+Kuchera_M/0/1/0/all/0/1&quot;&gt;Michelle P. Kuchera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-ex/1/au:+Liddick_S/0/1/0/all/0/1&quot;&gt;Sean Liddick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-ex/1/au:+Ramanujan_R/0/1/0/all/0/1&quot;&gt;Raghu Ramanujan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/nucl-ex/1/au:+Spyrou_A/0/1/0/all/0/1&quot;&gt;Artemis Spyrou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04723">
<title>Experiential Explanations for Reinforcement Learning. (arXiv:2210.04723v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04723</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning (RL) systems can be complex and non-interpretable,
making it challenging for non-AI experts to understand or intervene in their
decisions. This is due in part to the sequential nature of RL in which actions
are chosen because of future rewards. However, RL agents discard the
qualitative features of their training, making it difficult to recover
user-understandable information for &quot;why&quot; an action is chosen. We propose a
technique, Experiential Explanations, to generate counterfactual explanations
by training influence predictors along with the RL policy. Influence predictors
are models that learn how sources of reward affect the agent in different
states, thus restoring information about how the policy reflects the
environment. A human evaluation study revealed that participants presented with
experiential explanations were better able to correctly guess what an agent
would do than those presented with other standard types of explanation.
Participants also found that experiential explanations are more understandable,
satisfying, complete, useful, and accurate. The qualitative analysis provides
insights into the factors of experiential explanations that are most useful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabdulkarim_A/0/1/0/all/0/1&quot;&gt;Amal Alabdulkarim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Madhuri Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansi_G/0/1/0/all/0/1&quot;&gt;Gennie Mansi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hall_K/0/1/0/all/0/1&quot;&gt;Kaely Hall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedl_M/0/1/0/all/0/1&quot;&gt;Mark O. Riedl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11194">
<title>Controller-Guided Partial Label Consistency Regularization with Unlabeled Data. (arXiv:2210.11194v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11194</link>
<description rdf:parseType="Literal">&lt;p&gt;Partial label learning (PLL) learns from training examples each associated
with multiple candidate labels, among which only one is valid. In recent years,
benefiting from the strong capability of dealing with ambiguous supervision and
the impetus of modern data augmentation methods, consistency
regularization-based PLL methods have achieved a series of successes and become
mainstream. However, as the partial annotation becomes insufficient, their
performances drop significantly. In this paper, we leverage easily accessible
unlabeled examples to facilitate the partial label consistency regularization.
In addition to a partial supervised loss, our method performs a
controller-guided consistency regularization at both the label-level and
representation-level with the help of unlabeled data. To minimize the
disadvantages of insufficient capabilities of the initial supervised model, we
use the controller to estimate the confidence of each current prediction to
guide the subsequent consistency regularization. Furthermore, we dynamically
adjust the confidence thresholds so that the number of samples of each class
participating in consistency regularization remains roughly equal to alleviate
the problem of class-imbalance. Experiments show that our method achieves
satisfactory performances in more practical situations, and its modules can be
applied to existing PLL methods to enhance their capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qian-Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bowen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingyan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tianxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zimo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01313">
<title>Double Equivariance for Inductive Link Prediction for Both New Nodes and New Relation Types. (arXiv:2302.01313v7 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01313</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of inductive link prediction in knowledge graphs (KGs) generally
focuses on test predictions with solely new nodes but not both new nodes and
new relation types. In this work, we formally define the concept of double
permutation-equivariant representations that are equivariant to permutations of
both node identities and edge relation types. We then show how
double-equivariant architectures are able to self-supervise pre-train on
distinct KG domains and zero-shot predict links on a new KG domain (with
completely new entities and new relation types). We also introduce the concept
of distributionally double equivariant positional embeddings designed to
perform the same task. Finally, we empirically demonstrate the capability of
the proposed models against baselines on a set of novel real-world benchmarks.
More interestingly, we show that self-supervised pre-training on more KG
domains increases the zero-shot ability of our model to predict on new relation
types over new entities on unseen KG domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfei Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yangze Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jincheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ribeiro_B/0/1/0/all/0/1&quot;&gt;Bruno Ribeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09639">
<title>An overview of differentiable particle filters for data-adaptive sequential Bayesian inference. (arXiv:2302.09639v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09639</link>
<description rdf:parseType="Literal">&lt;p&gt;By approximating posterior distributions with weighted samples, particle
filters (PFs) provide an efficient mechanism for solving non-linear sequential
state estimation problems. While the effectiveness of particle filters has been
recognised in various applications, their performance relies on the knowledge
of dynamic models and measurement models, as well as the construction of
effective proposal distributions. An emerging trend involves constructing
components of particle filters using neural networks and optimising them by
gradient descent, and such data-adaptive particle filtering approaches are
often called differentiable particle filters. Due to the expressiveness of
neural networks, differentiable particle filters are a promising computational
tool for performing inference on sequential data in complex, high-dimensional
tasks, such as vision-based robot localisation. In this paper, we review recent
advances in differentiable particle filters and their applications. We place
special emphasis on different design choices for key components of
differentiable particle filters, including dynamic models, measurement models,
proposal distributions, optimisation objectives, and differentiable resampling
techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiongjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.01141">
<title>DeepSaDe: Learning Neural Networks that Guarantee Domain Constraint Satisfaction. (arXiv:2303.01141v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.01141</link>
<description rdf:parseType="Literal">&lt;p&gt;As machine learning models, specifically neural networks, are becoming
increasingly popular, there are concerns regarding their trustworthiness,
specially in safety-critical applications, e.g. actions of an autonomous
vehicle must be safe. There are approaches that can train neural networks where
such domain requirements are enforced as constraints, but they either cannot
guarantee that the constraint will be satisfied by all possible predictions
(even on unseen data) or they are limited in the type of constraints that can
be enforced. In this paper, we present an approach to train neural networks
which can enforce a wide variety of constraints and guarantee that the
constraint is satisfied by all possible predictions. The approach builds on
earlier work where learning linear models is formulated as a constraint
satisfaction problem (CSP). To make this idea applicable to neural networks,
two crucial new elements are added: constraint propagation over the network
layers, and weight updates based on a mix of gradient descent and CSP solving.
Evaluation on various machine learning tasks demonstrates that our approach is
flexible enough to enforce a wide variety of domain constraints and is able to
guarantee them in neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_K/0/1/0/all/0/1&quot;&gt;Kshitij Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dumancic_S/0/1/0/all/0/1&quot;&gt;Sebastijan Dumancic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blockeel_H/0/1/0/all/0/1&quot;&gt;Hendrik Blockeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10847">
<title>Large Language Models can be Guided to Evade AI-Generated Text Detection. (arXiv:2305.10847v5 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10847</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have shown remarkable performance in various
tasks and have been extensively utilized by the public. However, the increasing
concerns regarding the misuse of LLMs, such as plagiarism and spamming, have
led to the development of multiple detectors, including fine-tuned classifiers
and statistical methods. In this study, we equip LLMs with prompts, rather than
relying on an external paraphraser, to evaluate the vulnerability of these
detectors. We propose a novel Substitution-based In-Context example
Optimization method (SICO) to automatically construct prompts for evading the
detectors. SICO is cost-efficient as it requires only 40 human-written examples
and a limited number of LLM inferences to generate a prompt. Moreover, once a
task-specific prompt has been constructed, it can be universally used against a
wide range of detectors. Extensive experiments across three real-world tasks
demonstrate that SICO significantly outperforms the paraphraser baselines and
enables GPT-3.5 to successfully evade six detectors, decreasing their AUC by
0.5 on average. Furthermore, a comprehensive human evaluation as well as a
validation experiment in the wild show that the SICO-generated text achieves
human-level readability and task completion rates. Finally, the strong
performance of SICO exhibits its potential as a reliable evaluation tool for
future detectors. The codes and data are located on
https://github.com/ColinLu50/Evade-GPT-Detector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1&quot;&gt;Ning Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengcai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_R/0/1/0/all/0/1&quot;&gt;Rui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1&quot;&gt;Yew-Soon Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_K/0/1/0/all/0/1&quot;&gt;Ke Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13721">
<title>Continual Dialogue State Tracking via Example-Guided Question Answering. (arXiv:2305.13721v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13721</link>
<description rdf:parseType="Literal">&lt;p&gt;Dialogue systems are frequently updated to accommodate new services, but
naively updating them by continually training with data for new services in
diminishing performance on previously learnt services. Motivated by the insight
that dialogue state tracking (DST), a crucial component of dialogue systems
that estimates the user&apos;s goal as a conversation proceeds, is a simple natural
language understanding task, we propose reformulating it as a bundle of
granular example-guided question answering tasks to minimize the task shift
between services and thus benefit continual learning. Our approach alleviates
service-specific memorization and teaches a model to contextualize the given
question and example to extract the necessary information from the
conversation. We find that a model with just 60M parameters can achieve a
significant boost by learning to learn from in-context examples retrieved by a
retriever trained to identify turns with similar dialogue state changes.
Combining our method with dialogue-level memory replay, our approach attains
state of the art performance on DST continual learning metrics without relying
on any complex regularization or parameter expansion methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_H/0/1/0/all/0/1&quot;&gt;Hyundong Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madotto_A/0/1/0/all/0/1&quot;&gt;Andrea Madotto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhaojiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1&quot;&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kottur_S/0/1/0/all/0/1&quot;&gt;Satwik Kottur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+May_J/0/1/0/all/0/1&quot;&gt;Jonathan May&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sankar_C/0/1/0/all/0/1&quot;&gt;Chinnadhurai Sankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01665">
<title>SourceP: Detecting Ponzi Schemes on Ethereum with Source Code. (arXiv:2306.01665v6 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01665</link>
<description rdf:parseType="Literal">&lt;p&gt;As blockchain technology becomes more and more popular, a typical financial
scam, the Ponzi scheme, has also emerged in the blockchain platform Ethereum.
This Ponzi scheme deployed through smart contracts, also known as the smart
Ponzi scheme, has caused a lot of economic losses and negative impacts.
Existing methods for detecting smart Ponzi schemes on Ethereum mainly rely on
bytecode features, opcode features, account features, and transaction behavior
features of smart contracts, which are unable to truly characterize the
behavioral features of Ponzi schemes, and thus generally perform poorly in
terms of detection accuracy and false alarm rates. In this paper, we propose
SourceP, a method to detect smart Ponzi schemes on the Ethereum platform using
pre-trained models and data flow, which only requires using the source code of
smart contracts as features. SourceP reduces the difficulty of data acquisition
and feature extraction of existing detection methods. Specifically, we first
convert the source code of a smart contract into a data flow graph and then
introduce a pre-trained model based on learning code representations to build a
classification model to identify Ponzi schemes in smart contracts. The
experimental results show that SourceP achieves 87.2\% recall and 90.7\%
F-score for detecting smart Ponzi schemes within Ethereum&apos;s smart contract
dataset, outperforming state-of-the-art methods in terms of performance and
sustainability. We also demonstrate through additional experiments that
pre-trained models and data flow play an important contribution to SourceP, as
well as proving that SourceP has a good generalization ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pengcheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Liang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_K/0/1/0/all/0/1&quot;&gt;Keting Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03480">
<title>GSHOT: Few-shot Generative Modeling of Labeled Graphs. (arXiv:2306.03480v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03480</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep graph generative modeling has gained enormous attraction in recent years
due to its impressive ability to directly learn the underlying hidden graph
distribution. Despite their initial success, these techniques, like much of the
existing deep generative methods, require a large number of training samples to
learn a good model. Unfortunately, large number of training samples may not
always be available in scenarios such as drug discovery for rare diseases. At
the same time, recent advances in few-shot learning have opened door to
applications where available training data is limited. In this work, we
introduce the hitherto unexplored paradigm of few-shot graph generative
modeling. Towards this, we develop GSHOT, a meta-learning based framework for
few-shot labeled graph generative modeling. GSHOT learns to transfer
meta-knowledge from similar auxiliary graph datasets. Utilizing these prior
experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced
fine-tuning. Through extensive experiments on datasets from diverse domains
having limited training samples, we establish that GSHOT generates graphs of
superior fidelity compared to existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manchanda_S/0/1/0/all/0/1&quot;&gt;Sahil Manchanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Shubham Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ranu_S/0/1/0/all/0/1&quot;&gt;Sayan Ranu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bedathur_S/0/1/0/all/0/1&quot;&gt;Srikanta Bedathur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02668">
<title>Guided Distillation for Semi-Supervised Instance Segmentation. (arXiv:2308.02668v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02668</link>
<description rdf:parseType="Literal">&lt;p&gt;Although instance segmentation methods have improved considerably, the
dominant paradigm is to rely on fully-annotated training images, which are
tedious to obtain. To alleviate this reliance, and boost results,
semi-supervised approaches leverage unlabeled data as an additional training
signal that limits overfitting to the labeled samples. In this context, we
present novel design choices to significantly improve teacher-student
distillation models. In particular, we (i) improve the distillation approach by
introducing a novel &quot;guided burn-in&quot; stage, and (ii) evaluate different
instance segmentation architectures, as well as backbone networks and
pre-training strategies. Contrary to previous work which uses only supervised
data for the burn-in period of the student model, we also use guidance of the
teacher model to exploit unlabeled data in the burn-in period. Our improved
distillation approach leads to substantial improvements over previous
state-of-the-art results. For example, on the Cityscapes dataset we improve
mask-AP from 23.7 to 33.9 when using labels for 10\% of images, and on the COCO
dataset we improve mask-AP from 18.3 to 34.1 when using labels for only 1\% of
the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berrada_T/0/1/0/all/0/1&quot;&gt;Tariq Berrada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couprie_C/0/1/0/all/0/1&quot;&gt;Camille Couprie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alahari_K/0/1/0/all/0/1&quot;&gt;Karteek Alahari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verbeek_J/0/1/0/all/0/1&quot;&gt;Jakob Verbeek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03358">
<title>RGMComm: Return Gap Minimization Via Discrete Communications In Multi-Agent Reinforcement Learning. (arXiv:2308.03358v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03358</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication is crucial for solving cooperative Multi-Agent Reinforcement
Learning tasks in partially observable Markov Decision Processes. Existing
works often rely on black-box methods to encode local information/features into
messages shared with other agents, leading to the generation of continuous
messages with high communication overhead and poor interpretability. Prior
attempts at discrete communication methods generate one-hot vectors trained as
part of agents&apos; actions and use the Gumbel softmax operation for calculating
gradients, which are all heuristic designs that do not provide any quantitative
guarantees on the expected return. This paper establishes an upper bound on the
return gap between an ideal policy with full observability and an optimal
partially observable policy with discrete communication. This result enables us
to recast multi-agent communication into a novel online clustering problem over
the local observations at each agent, with messages as cluster labels and the
upper bound on the return gap as clustering loss. To minimize the return gap,
we propose the Return-Gap-Minimization Communication (RGMComm) algorithm, which
is a surprisingly simple design of discrete message generation functions and is
integrated with reinforcement learning through the utilization of a novel
Regularized Information Maximization loss function, which incorporates
cosine-distance as the clustering metric. Evaluations show that RGMComm
significantly outperforms state-of-the-art multi-agent communication baselines
and can achieve nearly optimal returns with few-bit messages that are naturally
interpretable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingdi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_T/0/1/0/all/0/1&quot;&gt;Tian Lan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03443">
<title>Doubly Robust Estimator for Off-Policy Evaluation with Large Action Spaces. (arXiv:2308.03443v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03443</link>
<description rdf:parseType="Literal">&lt;p&gt;We study Off-Policy Evaluation (OPE) in contextual bandit settings with large
action spaces. The benchmark estimators suffer from severe bias and variance
tradeoffs. Parametric approaches suffer from bias due to difficulty specifying
the correct model, whereas ones with importance weight suffer from variance. To
overcome these limitations, Marginalized Inverse Propensity Scoring (MIPS) was
proposed to mitigate the estimator&apos;s variance via embeddings of an action.
Nevertheless, MIPS is unbiased under the no direct effect, which assumes that
the action embedding completely mediates the effect of an action on a reward.
To overcome the dependency on these unrealistic assumptions, we propose a
Marginalized Doubly Robust (MDR) estimator. Theoretical analysis shows that the
proposed estimator is unbiased under weaker assumptions than MIPS while
reducing the variance against MIPS. The empirical experiment verifies the
supremacy of MDR against existing estimators with large action spaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shimizu_T/0/1/0/all/0/1&quot;&gt;Tatsuhiro Shimizu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Forastiere_L/0/1/0/all/0/1&quot;&gt;Laura Forastiere&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03669">
<title>Diffusion Model in Causal Inference with Unmeasured Confounders. (arXiv:2308.03669v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03669</link>
<description rdf:parseType="Literal">&lt;p&gt;We study how to extend the use of the diffusion model to answer the causal
question from the observational data under the existence of unmeasured
confounders. In Pearl&apos;s framework of using a Directed Acyclic Graph (DAG) to
capture the causal intervention, a Diffusion-based Causal Model (DCM) was
proposed incorporating the diffusion model to answer the causal questions more
accurately, assuming that all of the confounders are observed. However,
unmeasured confounders in practice exist, which hinders DCM from being
applicable. To alleviate this limitation of DCM, we propose an extended model
called Backdoor Criterion based DCM (BDCM), whose idea is rooted in the
Backdoor criterion to find the variables in DAG to be included in the decoding
process of the diffusion model so that we can extend DCM to the case with
unmeasured confounders. Synthetic data experiment demonstrates that our
proposed model captures the counterfactual distribution more precisely than DCM
under the unmeasured confounders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shimizu_T/0/1/0/all/0/1&quot;&gt;Tatsuhiro Shimizu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06221">
<title>Automated Sizing and Training of Efficient Deep Autoencoders using Second Order Algorithms. (arXiv:2308.06221v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06221</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a multi-step training method for designing generalized linear
classifiers. First, an initial multi-class linear classifier is found through
regression. Then validation error is minimized by pruning of unnecessary
inputs. Simultaneously, desired outputs are improved via a method similar to
the Ho-Kashyap rule. Next, the output discriminants are scaled to be net
functions of sigmoidal output units in a generalized linear classifier. We then
develop a family of batch training algorithm for the multi layer perceptron
that optimizes its hidden layer size and number of training epochs. Next, we
combine pruning with a growing approach. Later, the input units are scaled to
be the net function of the sigmoidal output units that are then feed into as
input to the MLP. We then propose resulting improvements in each of the deep
learning blocks thereby improving the overall performance of the deep
architecture. We discuss the principles and formulation regarding learning
algorithms for deep autoencoders. We investigate several problems in deep
autoencoders networks including training issues, the theoretical, mathematical
and experimental justification that the networks are linear, optimizing the
number of hidden units in each layer and determining the depth of the deep
learning model. A direct implication of the current work is the ability to
construct fast deep learning models using desktop level computational
resources. This, in our opinion, promotes our design philosophy of building
small but powerful algorithms. Performance gains are demonstrated at each step.
Using widely available datasets, the final network&apos;s ten fold testing error is
shown to be less than that of several other linear, generalized linear
classifiers, multi layer perceptron and deep learners reported in the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagi_K/0/1/0/all/0/1&quot;&gt;Kanishka Tyagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rane_C/0/1/0/all/0/1&quot;&gt;Chinmay Rane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manry_M/0/1/0/all/0/1&quot;&gt;Michael Manry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06385">
<title>ZYN: Zero-Shot Reward Models with Yes-No Questions for RLAIF. (arXiv:2308.06385v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06385</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we address the problem of directing the text generation of a
language model (LM) towards a desired behavior, aligning the generated text
with the preferences of the human operator. We propose using another,
instruction-tuned language model as a critic reward model in a zero-shot way
thanks to the prompt of a Yes-No question that represents the user preferences,
without requiring further labeled data. This zero-shot reward model provides
the learning signal to further fine-tune the base LM using Reinforcement
Learning from AI Feedback (RLAIF); yet our approach is also compatible in other
contexts such as quality-diversity search. Extensive evidence of the
capabilities of the proposed ZYN framework is provided through experiments in
different domains related to text generation, including detoxification;
optimizing sentiment of movie reviews, or any other attribute; steering the
opinion about a particular topic the model may have; and personalizing prompt
generators for text-to-image tasks. Code available at
\url{https://github.com/vicgalle/zero-shot-reward-models/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallego_V/0/1/0/all/0/1&quot;&gt;Victor Gallego&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09300">
<title>V2A-Mapper: A Lightweight Solution for Vision-to-Audio Generation by Connecting Foundation Models. (arXiv:2308.09300v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09300</link>
<description rdf:parseType="Literal">&lt;p&gt;Building artificial intelligence (AI) systems on top of a set of foundation
models (FMs) is becoming a new paradigm in AI research. Their representative
and generative abilities learnt from vast amounts of data can be easily adapted
and transferred to a wide range of downstream tasks without extra training from
scratch. However, leveraging FMs in cross-modal generation remains
under-researched when audio modality is involved. On the other hand,
automatically generating semantically-relevant sound from visual input is an
important problem in cross-modal generation studies. To solve this
vision-to-audio (V2A) generation problem, existing methods tend to design and
build complex systems from scratch using modestly sized datasets. In this
paper, we propose a lightweight solution to this problem by leveraging
foundation models, specifically CLIP, CLAP, and AudioLDM. We first investigate
the domain gap between the latent space of the visual CLIP and the auditory
CLAP models. Then we propose a simple yet effective mapper mechanism
(V2A-Mapper) to bridge the domain gap by translating the visual input between
CLIP and CLAP spaces. Conditioned on the translated CLAP embedding, pretrained
audio generative FM AudioLDM is adopted to produce high-fidelity and
visually-aligned sound. Compared to previous approaches, our method only
requires a quick training of the V2A-Mapper. We further analyze and conduct
extensive experiments on the choice of the V2A-Mapper and show that a
generative mapper is better at fidelity and variability (FD) while a regression
mapper is slightly better at relevance (CS). Both objective and subjective
evaluation on two V2A datasets demonstrate the superiority of our proposed
method compared to current state-of-the-art approaches - trained with 86% fewer
parameters but achieving 53% and 19% improvement in FD and CS, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Heng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jianbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascual_S/0/1/0/all/0/1&quot;&gt;Santiago Pascual&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cartwright_R/0/1/0/all/0/1&quot;&gt;Richard Cartwright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_W/0/1/0/all/0/1&quot;&gt;Weidong Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14256">
<title>FaceChain: A Playground for Human-centric Artificial Intelligence Generated Content. (arXiv:2308.14256v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14256</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancement in personalized image generation have unveiled the
intriguing capability of pre-trained text-to-image models on learning identity
information from a collection of portrait images. However, existing solutions
are vulnerable in producing truthful details, and usually suffer from several
defects such as (i) The generated face exhibit its own unique characteristics,
\ie facial shape and facial feature positioning may not resemble key
characteristics of the input, and (ii) The synthesized face may contain warped,
blurred or corrupted regions. In this paper, we present FaceChain, a
personalized portrait generation framework that combines a series of customized
image-generation model and a rich set of face-related perceptual understanding
models (\eg, face detection, deep face embedding extraction, and facial
attribute recognition), to tackle aforementioned challenges and to generate
truthful personalized portraits, with only a handful of portrait images as
input. Concretely, we inject several SOTA face models into the generation
procedure, achieving a more efficient label-tagging, data-processing, and model
post-processing compared to previous solutions, such as DreamBooth
~\cite{ruiz2023dreambooth} , InstantBooth ~\cite{shi2023instantbooth} , or
other LoRA-only approaches ~\cite{hu2021lora} . Besides, based on FaceChain, we
further develop several applications to build a broader playground for better
showing its value, including virtual try on and 2D talking head. We hope it can
grow to serve the burgeoning needs from the communities. Note that this is an
ongoing work that will be consistently refined and improved upon. FaceChain is
open-sourced under Apache-2.0 license at
\url{https://github.com/modelscope/facechain}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Cheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lei Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongyi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xingjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Haoyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weida Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuze Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_C/0/1/0/all/0/1&quot;&gt;Chen Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuan Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wenmeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yingda Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xuansong Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06157">
<title>Robust-MBDL: A Robust Multi-branch Deep Learning Based Model for Remaining Useful Life Prediction and Operational Condition Identification of Rotating Machines. (arXiv:2309.06157v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06157</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a Robust Multi-branch Deep learning-based system for remaining
useful life (RUL) prediction and condition operations (CO) identification of
rotating machines is proposed. In particular, the proposed system comprises
main components: (1) an LSTM-Autoencoder to denoise the vibration data; (2) a
feature extraction to generate time-domain, frequency-domain, and
time-frequency based features from the denoised data; (3) a novel and robust
multi-branch deep learning network architecture to exploit the multiple
features. The performance of our proposed system was evaluated and compared to
the state-of-the-art systems on two benchmark datasets of XJTU-SY and
PRONOSTIA. The experimental results prove that our proposed system outperforms
the state-of-the-art systems and presents potential for real-life applications
on bearing machines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_K/0/1/0/all/0/1&quot;&gt;Khoa Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vu_H/0/1/0/all/0/1&quot;&gt;Hai-Canh Vu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pham_L/0/1/0/all/0/1&quot;&gt;Lam Pham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boudaoud_N/0/1/0/all/0/1&quot;&gt;Nassim Boudaoud&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.13672">
<title>Deep Reinforcement Learning for Image-to-Image Translation. (arXiv:2309.13672v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.13672</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing Image-to-Image Translation (I2IT) methods generate images in a
single run of a deep learning (DL) model. However, designing such a single-step
model is always challenging, requiring a huge number of parameters and easily
falling into bad global minimums and overfitting. In this work, we reformulate
I2IT as a step-wise decision-making problem via deep reinforcement learning
(DRL) and propose a novel framework that performs RL-based I2IT (RL-I2IT). The
key feature in the RL-I2IT framework is to decompose a monolithic learning
process into small steps with a lightweight model to progressively transform a
source image successively to a target image. Considering that it is challenging
to handle high dimensional continuous state and action spaces in the
conventional RL framework, we introduce meta policy with a new concept Plan to
the standard Actor-Critic model, which is of a lower dimension than the
original image and can facilitate the actor to generate a tractable high
dimensional action. In the RL-I2IT framework, we also employ a task-specific
auxiliary learning strategy to stabilize the training process and improve the
performance of the corresponding task. Experiments on several I2IT tasks
demonstrate the effectiveness and robustness of the proposed method when facing
high-dimensional continuous action space problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziwei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_C/0/1/0/all/0/1&quot;&gt;Chengming Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Bin Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_S/0/1/0/all/0/1&quot;&gt;Siwei Lyu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14780">
<title>Transferring climate change knowledge. (arXiv:2309.14780v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14780</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate climate projections are required for climate adaptation and
mitigation. Earth system model simulations, used to project climate change,
inherently make approximations in their representation of small-scale physical
processes, such as the formation of clouds, that are at the root of the
uncertainties in global mean temperature&apos;s response to increased greenhouse gas
concentrations. Several approaches have been developed to use historical
observations to constrain future projections and reduce uncertainties in
climate projections and climate feedbacks. Yet those methods cannot capture the
non-linear complexity inherent in the climate system. Using a Transfer Learning
approach, we show that Machine Learning, in particular Deep Neural Networks,
can be used to optimally leverage and merge the knowledge gained from Earth
system model simulations and historical observations to more accurately project
global surface temperature fields in the 21st century. We reach a reduction in
the 5-95% uncertainty range of global surface air temperature in 2081-2098 of
up to 56% and 52% - across the Shared Socioeconomic Pathways considered - with
respect to state-of-the-art approaches and the Sixth Assessment Report from the
Intergovernmental Panel on Climate Change, respectively. We give evidence that
our novel method provides narrower multi-model uncertainty together with more
accurate climate projections, urgently required for climate adaptation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Immorlano_F/0/1/0/all/0/1&quot;&gt;Francesco Immorlano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Eyring_V/0/1/0/all/0/1&quot;&gt;Veronika Eyring&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gouville_T/0/1/0/all/0/1&quot;&gt;Thomas le Monnier de Gouville&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Accarino_G/0/1/0/all/0/1&quot;&gt;Gabriele Accarino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Elia_D/0/1/0/all/0/1&quot;&gt;Donatello Elia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Aloisio_G/0/1/0/all/0/1&quot;&gt;Giovanni Aloisio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gentine_P/0/1/0/all/0/1&quot;&gt;Pierre Gentine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00268">
<title>Unravel Anomalies: An End-to-end Seasonal-Trend Decomposition Approach for Time Series Anomaly Detection. (arXiv:2310.00268v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00268</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional Time-series Anomaly Detection (TAD) methods often struggle with
the composite nature of complex time-series data and a diverse array of
anomalies. We introduce TADNet, an end-to-end TAD model that leverages
Seasonal-Trend Decomposition to link various types of anomalies to specific
decomposition components, thereby simplifying the analysis of complex
time-series and enhancing detection performance. Our training methodology,
which includes pre-training on a synthetic dataset followed by fine-tuning,
strikes a balance between effective decomposition and precise anomaly
detection. Experimental validation on real-world datasets confirms TADNet&apos;s
state-of-the-art performance across a diverse range of anomalies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhenwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Ran Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yuantao Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02207">
<title>Language Models Represent Space and Time. (arXiv:2310.02207v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02207</link>
<description rdf:parseType="Literal">&lt;p&gt;The capabilities of large language models (LLMs) have sparked debate over
whether such systems just learn an enormous collection of superficial
statistics or a coherent model of the data generation process -- a world model.
We find preliminary evidence for the latter by analyzing the learned
representations of three spatial datasets (world, US, NYC places) and three
temporal datasets (historical figures, artworks, news headlines) in the Llama-2
family of models. We discover that LLMs learn linear representations of space
and time across multiple scales. These representations are robust to prompting
variations and unified across different entity types (e.g. cities and
landmarks). In addition, we identify individual ``space neurons&apos;&apos; and ``time
neurons&apos;&apos; that reliably encode spatial and temporal coordinates. While further
investigation is needed, our results suggest modern LLMs learn rich
spatiotemporal representations of the real world and possess basic ingredients
of a world model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurnee_W/0/1/0/all/0/1&quot;&gt;Wes Gurnee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1&quot;&gt;Max Tegmark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.02299">
<title>Discovering Symmetry Breaking in Physical Systems with Relaxed Group Convolution. (arXiv:2310.02299v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.02299</link>
<description rdf:parseType="Literal">&lt;p&gt;Finding symmetry breaking is essential for understanding the fundamental
changes in the behaviors and properties of physical systems, from microscopic
particle interactions to macroscopic phenomena like fluid dynamics and cosmic
structures. Relaxed group convolution emerges as a solution for instances when
physical systems without perfect symmetries and perfectly equivariant models
are restrictive. In this paper, we provide both theoretical and empirical
evidence that this flexible convolution technique allows the model to maintain
the highest level of equivariance that is consistent with data and discover the
subtle symmetry-breaking factors in various physical systems. We employ various
relaxed group convolution architectures to uncover various symmetry-breaking
factors in different physical systems, including the phase transition of
crystal structure, the isotropy and homogeneity breaking in turbulence, and the
time-reversal symmetry breaking in pendulum systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Han Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smidt_T/0/1/0/all/0/1&quot;&gt;Tess E.Smidt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05804">
<title>Learning Language-guided Adaptive Hyper-modality Representation for Multimodal Sentiment Analysis. (arXiv:2310.05804v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05804</link>
<description rdf:parseType="Literal">&lt;p&gt;Though Multimodal Sentiment Analysis (MSA) proves effective by utilizing rich
information from multiple sources (e.g., language, video, and audio), the
potential sentiment-irrelevant and conflicting information across modalities
may hinder the performance from being further improved. To alleviate this, we
present Adaptive Language-guided Multimodal Transformer (ALMT), which
incorporates an Adaptive Hyper-modality Learning (AHL) module to learn an
irrelevance/conflict-suppressing representation from visual and audio features
under the guidance of language features at different scales. With the obtained
hyper-modality representation, the model can obtain a complementary and joint
representation through multimodal fusion for effective MSA. In practice, ALMT
achieves state-of-the-art performance on several popular datasets (e.g., MOSI,
MOSEI and CH-SIMS) and an abundance of ablation demonstrates the validity and
necessity of our irrelevance/conflict suppression mechanism.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_G/0/1/0/all/0/1&quot;&gt;Guanghao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kejun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuanyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianshu Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.10158">
<title>Character-LLM: A Trainable Agent for Role-Playing. (arXiv:2310.10158v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.10158</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) can be used to serve as agents to simulate human
behaviors, given the powerful ability to understand human instructions and
provide high-quality generated texts. Such ability stimulates us to wonder
whether LLMs can simulate a person in a higher form than simple human
behaviors. Therefore, we aim to train an agent with the profile, experience,
and emotional states of a specific person instead of using limited prompts to
instruct ChatGPT API. In this work, we introduce Character-LLM that teach LLMs
to act as specific people such as Beethoven, Queen Cleopatra, Julius Caesar,
etc. Our method focuses on editing profiles as experiences of a certain
character and training models to be personal simulacra with these experiences.
To assess the effectiveness of our approach, we build a test playground that
interviews trained agents and evaluates whether the agents \textit{memorize}
their characters and experiences. Experimental results show interesting
observations that help build future simulacra of humankind.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Y/0/1/0/all/0/1&quot;&gt;Yunfan Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Junqi Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_X/0/1/0/all/0/1&quot;&gt;Xipeng Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12103">
<title>Quality Diversity through Human Feedback. (arXiv:2310.12103v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12103</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement Learning from Human Feedback (RLHF) has shown potential in
qualitative tasks where clear objectives are lacking. However, its
effectiveness is not fully realized when it is conceptualized merely as a tool
to optimize average human preferences, especially in generative tasks that
demand diverse model responses. Meanwhile, Quality Diversity (QD) algorithms
excel at identifying diverse and high-quality solutions but often rely on
manually crafted diversity metrics. This paper introduces Quality Diversity
through Human Feedback (QDHF), a novel approach integrating human feedback into
the QD framework. QDHF infers diversity metrics from human judgments of
similarity among solutions, thereby enhancing the applicability and
effectiveness of QD algorithms. Our empirical studies show that QDHF
significantly outperforms state-of-the-art methods in automatic diversity
discovery and matches the efficacy of using manually crafted metrics for QD on
standard benchmarks in robotics and reinforcement learning. Notably, in a
latent space illumination task, QDHF substantially enhances the diversity in
images generated by a diffusion model and was more favorably received in user
studies. We conclude by analyzing QDHF&apos;s scalability and the quality of its
derived diversity metrics, emphasizing its potential to improve exploration and
diversity in complex, open-ended optimization tasks. Source code is available
on GitHub: https://github.com/ld-ing/qdhf.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Li Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jenny Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spector_L/0/1/0/all/0/1&quot;&gt;Lee Spector&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15140">
<title>AutoDAN: Interpretable Gradient-Based Adversarial Attacks on Large Language Models. (arXiv:2310.15140v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15140</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety alignment of Large Language Models (LLMs) can be compromised with
manual jailbreak attacks and (automatic) adversarial attacks. Recent studies
suggest that defending against these attacks is possible: adversarial attacks
generate unlimited but unreadable gibberish prompts, detectable by
perplexity-based filters; manual jailbreak attacks craft readable prompts, but
their limited number due to the necessity of human creativity allows for easy
blocking. In this paper, we show that these solutions may be too optimistic. We
introduce AutoDAN, an interpretable, gradient-based adversarial attack that
merges the strengths of both attack types. Guided by the dual goals of
jailbreak and readability, AutoDAN optimizes and generates tokens one by one
from left to right, resulting in readable prompts that bypass perplexity
filters while maintaining high attack success rates. Notably, these prompts,
generated from scratch using gradients, are interpretable and diverse, with
emerging strategies commonly seen in manual jailbreak attacks. They also
generalize to unforeseen harmful behaviors and transfer to black-box LLMs
better than their unreadable counterparts when using limited training data or a
single proxy model. Furthermore, we show the versatility of AutoDAN by
automatically leaking system prompts using a customized objective. Our work
offers a new way to red-team LLMs and understand jailbreak mechanisms via
interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Sicheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruiyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_B/0/1/0/all/0/1&quot;&gt;Bang An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrow_J/0/1/0/all/0/1&quot;&gt;Joe Barrow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zichao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nenkova_A/0/1/0/all/0/1&quot;&gt;Ani Nenkova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tong Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18168">
<title>Personas as a Way to Model Truthfulness in Language Models. (arXiv:2310.18168v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18168</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are trained on vast amounts of text from the
internet, which contains both factual and misleading information about the
world. Can language models discern truth from falsehood in this contradicting
data? Expanding on the view that LLMs can model different communicative agents,
we present the persona hypothesis: LLMs can cluster agents into personas using
common features of their generations. For instance, a truthful persona is a
group of agents that are likely to produce truthful text and that share similar
features like formal writing styles and scientific references. By modeling this
persona, LLMs can generalize truthfulness beyond the specific contexts in which
each agent generated the training text. For example, the model can infer that
the agent &quot;Wikipedia&quot; will behave truthfully on topics that were only generated
by &quot;Science&quot; because they both belong to the truthful persona. We show evidence
for the persona hypothesis via two observations: (1) we can probe whether a
model&apos;s answer will be truthful before it is generated; (2) finetuning a model
on a set of facts improves its truthfulness on unseen topics. Next, using
arithmetics as a synthetic environment, we show that language models can
separate true and false statements, and generalize truthfulness across agents;
but only if agents in the training data share a truthful generative process
that enables the creation of a truthful persona. Overall, our findings suggest
that models can exploit hierarchical structures in the data to learn abstract
concepts like truthfulness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nitish Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rando_J/0/1/0/all/0/1&quot;&gt;Javier Rando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saparov_A/0/1/0/all/0/1&quot;&gt;Abulhair Saparov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_N/0/1/0/all/0/1&quot;&gt;Najoung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18893">
<title>Ever Evolving Evaluator (EV3): Towards Flexible and Reliable Meta-Optimization for Knowledge Distillation. (arXiv:2310.18893v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18893</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce EV3, a novel meta-optimization framework designed to efficiently
train scalable machine learning models through an intuitive
explore-assess-adapt protocol. In each iteration of EV3, we explore various
model parameter updates, assess them using pertinent evaluation methods, and
then adapt the model based on the optimal updates and previous progress
history. EV3 offers substantial flexibility without imposing stringent
constraints like differentiability on the key objectives relevant to the tasks
of interest, allowing for exploratory updates with intentionally-biased
gradients and through a diversity of losses and optimizers. Additionally, the
assessment phase provides reliable safety controls to ensure robust
generalization, and can dynamically prioritize tasks in scenarios with multiple
objectives. With inspiration drawn from evolutionary algorithms, meta-learning,
and neural architecture search, we investigate an application of EV3 to
knowledge distillation. Our experimental results illustrate EV3&apos;s capability to
safely explore the modeling landscape, while hinting at its potential
applicability across numerous domains due to its inherent flexibility and
adaptability. Finally, we provide a JAX implementation of EV3, along with
source code for experiments, available at:
https://github.com/google-research/google-research/tree/master/ev3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Li Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoghi_M/0/1/0/all/0/1&quot;&gt;Masrour Zoghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennenholtz_G/0/1/0/all/0/1&quot;&gt;Guy Tennenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimzadehgan_M/0/1/0/all/0/1&quot;&gt;Maryam Karimzadehgan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19704">
<title>A Survey on Knowledge Editing of Neural Networks. (arXiv:2310.19704v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19704</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are becoming increasingly pervasive in academia and
industry, matching and surpassing human performance on a wide variety of fields
and related tasks. However, just as humans, even the largest artificial neural
networks make mistakes, and once-correct predictions can become invalid as the
world progresses in time. Augmenting datasets with samples that account for
mistakes or up-to-date information has become a common workaround in practical
applications. However, the well-known phenomenon of catastrophic forgetting
poses a challenge in achieving precise changes in the implicitly memorized
knowledge of neural network parameters, often requiring a full model
re-training to achieve desired behaviors. That is expensive, unreliable, and
incompatible with the current trend of large self-supervised pre-training,
making it necessary to find more efficient and effective methods for adapting
neural network models to changing data. To address this need, knowledge editing
is emerging as a novel area of research that aims to enable reliable,
data-efficient, and fast changes to a pre-trained target model, without
affecting model behaviors on previously learned tasks. In this survey, we
provide a brief review of this recent artificial intelligence field of
research. We first introduce the problem of editing neural networks, formalize
it in a common framework and differentiate it from more notorious branches of
research such as continuous learning. Next, we provide a review of the most
relevant knowledge editing approaches and datasets proposed so far, grouping
works under four different families: regularization techniques, meta-learning,
direct model editing, and architectural strategies. Finally, we outline some
intersections with other fields of research and potential directions for future
works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazzia_V/0/1/0/all/0/1&quot;&gt;Vittorio Mazzia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedrani_A/0/1/0/all/0/1&quot;&gt;Alessandro Pedrani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caciolai_A/0/1/0/all/0/1&quot;&gt;Andrea Caciolai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rottmann_K/0/1/0/all/0/1&quot;&gt;Kay Rottmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernardi_D/0/1/0/all/0/1&quot;&gt;Davide Bernardi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03424">
<title>Using Symmetries to Lift Satisfiability Checking. (arXiv:2311.03424v2 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03424</link>
<description rdf:parseType="Literal">&lt;p&gt;We analyze how symmetries can be used to compress structures (also known as
interpretations) onto a smaller domain without loss of information. This
analysis suggests the possibility to solve satisfiability problems in the
compressed domain for better performance. Thus, we propose a 2-step novel
method: (i) the sentence to be satisfied is automatically translated into an
equisatisfiable sentence over a ``lifted&apos;&apos; vocabulary that allows domain
compression; (ii) satisfiability of the lifted sentence is checked by growing
the (initially unknown) compressed domain until a satisfying structure is
found. The key issue is to ensure that this satisfying structure can always be
expanded into an uncompressed structure that satisfies the original sentence to
be satisfied.
&lt;/p&gt;
&lt;p&gt;We present an adequate translation for sentences in typed first-order logic
extended with aggregates. Our experimental evaluation shows large speedups for
generative configuration problems. The method also has applications in the
verification of software operating on complex data structures. Our results
justify further research in automatic translation of sentences for symmetry
reduction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carbonnelle_P/0/1/0/all/0/1&quot;&gt;Pierre Carbonnelle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schenner_G/0/1/0/all/0/1&quot;&gt;Gottfried Schenner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruynooghe_M/0/1/0/all/0/1&quot;&gt;Maurice Bruynooghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogaerts_B/0/1/0/all/0/1&quot;&gt;Bart Bogaerts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denecker_M/0/1/0/all/0/1&quot;&gt;Marc Denecker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04915">
<title>Chain of Empathy: Enhancing Empathetic Response of Large Language Models Based on Psychotherapy Models. (arXiv:2311.04915v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04915</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method, the Chain of Empathy (CoE) prompting, that
utilizes insights from psychotherapy to induce Large Language Models (LLMs) to
reason about human emotional states. This method is inspired by various
psychotherapy approaches including Cognitive Behavioral Therapy (CBT),
Dialectical Behavior Therapy (DBT), Person Centered Therapy (PCT), and Reality
Therapy (RT), each leading to different patterns of interpreting clients&apos;
mental states. LLMs without reasoning generated predominantly exploratory
responses. However, when LLMs used CoE reasoning, we found a more comprehensive
range of empathetic responses aligned with the different reasoning patterns of
each psychotherapy model. The CBT based CoE resulted in the most balanced
generation of empathetic responses. The findings underscore the importance of
understanding the emotional context and how it affects human and AI
communication. Our research contributes to understanding how psychotherapeutic
models can be incorporated into LLMs, facilitating the development of
context-specific, safer, and empathetic AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yoon Kyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Inju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_M/0/1/0/all/0/1&quot;&gt;Minjung Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seoyeon Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_S/0/1/0/all/0/1&quot;&gt;Sowon Hahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.06497">
<title>DRUformer: Enhancing the driving scene Important object detection with driving relationship self-understanding. (arXiv:2311.06497v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.06497</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic accidents frequently lead to fatal injuries, contributing to over 50
million deaths until 2023. To mitigate driving hazards and ensure personal
safety, it is crucial to assist vehicles in anticipating important objects
during travel. Previous research on important object detection primarily
assessed the importance of individual participants, treating them as
independent entities and frequently overlooking the connections between these
participants. Unfortunately, this approach has proven less effective in
detecting important objects in complex scenarios. In response, we introduce
Driving scene Relationship self-Understanding transformer (DRUformer), designed
to enhance the important object detection task. The DRUformer is a
transformer-based multi-modal important object detection model that takes into
account the relationships between all the participants in the driving scenario.
Recognizing that driving intention also significantly affects the detection of
important objects during driving, we have incorporated a module for embedding
driving intention. To assess the performance of our approach, we conducted a
comparative experiment on the DRAMA dataset, pitting our model against other
state-of-the-art (SOTA) models. The results demonstrated a noteworthy 16.2\%
improvement in mIoU and a substantial 12.3\% boost in ACC compared to SOTA
methods. Furthermore, we conducted a qualitative analysis of our model&apos;s
ability to detect important objects across different road scenarios and
classes, highlighting its effectiveness in diverse contexts. Finally, we
conducted various ablation studies to assess the efficiency of the proposed
modules in our DRUformer model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yingjie Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ohtani_K/0/1/0/all/0/1&quot;&gt;Kento Ohtani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carballo_A/0/1/0/all/0/1&quot;&gt;Alexander Carballo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takeda_K/0/1/0/all/0/1&quot;&gt;Kazuya Takeda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11473">
<title>CSGNN: Conquering Noisy Node labels via Dynamic Class-wise Selection. (arXiv:2311.11473v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11473</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have emerged as a powerful tool for
representation learning on graphs, but they often suffer from overfitting and
label noise issues, especially when the data is scarce or imbalanced. Different
from the paradigm of previous methods that rely on single-node confidence, in
this paper, we introduce a novel Class-wise Selection for Graph Neural
Networks, dubbed CSGNN, which employs a neighbor-aggregated latent space to
adaptively select reliable nodes across different classes. Specifically, 1) to
tackle the class imbalance issue, we introduce a dynamic class-wise selection
mechanism, leveraging the clustering technique to identify clean nodes based on
the neighbor-aggregated confidences. In this way, our approach can avoid the
pitfalls of biased sampling which is common with global threshold techniques.
2) To alleviate the problem of noisy labels, built on the concept of the
memorization effect, CSGNN prioritizes learning from clean nodes before noisy
ones, thereby iteratively enhancing model performance while mitigating label
noise. Through extensive experiments, we demonstrate that CSGNN outperforms
state-of-the-art methods in terms of both effectiveness and robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yifan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhen Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1&quot;&gt;Kai Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zongsheng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Y/0/1/0/all/0/1&quot;&gt;Yu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12304">
<title>Discovering Effective Policies for Land-Use Planning. (arXiv:2311.12304v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12304</link>
<description rdf:parseType="Literal">&lt;p&gt;How areas of land are allocated for different uses, such as forests, urban,
and agriculture, has a large effect on carbon balance, and therefore climate
change. Based on available historical data on changes in land use and a
simulation of carbon emissions/absorption, a surrogate model can be learned
that makes it possible to evaluate the different options available to
decision-makers efficiently. An evolutionary search process can then be used to
discover effective land-use policies for specific locations. Such a system was
built on the Project Resilience platform and evaluated with the Land-Use
Harmonization dataset and the BLUE simulator. It generates Pareto fronts that
trade off carbon impact and amount of change customized to different locations,
thus providing a potentially useful tool for land-use planning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francon_O/0/1/0/all/0/1&quot;&gt;Olivier Francon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_D/0/1/0/all/0/1&quot;&gt;Daniel Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meyerson_E/0/1/0/all/0/1&quot;&gt;Elliot Meyerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bieker_J/0/1/0/all/0/1&quot;&gt;Jacob Bieker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_H/0/1/0/all/0/1&quot;&gt;Hugo Cunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hodjat_B/0/1/0/all/0/1&quot;&gt;Babak Hodjat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13018">
<title>GeoLocator: a location-integrated large multimodal model for inferring geo-privacy. (arXiv:2311.13018v2 [cs.CY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13018</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic privacy or geo-privacy refers to the keeping private of one&apos;s
geographic location, especially the restriction of geographical data maintained
by personal electronic equipment. Geo-privacy is a crucial aspect of personal
security, however often goes unnoticed in daily activities. With the surge in
the use of Large Multimodal Models (LMM), such as GPT-4, for Open Source
Intelligence (OSINT), the potential risks associated with geo-privacy breaches
have intensified. This study develops a location-integrated GPT-4 based model
named GeoLocator and designed four-dimensional experiments to demonstrate its
capability in inferring and identifying the locational information of input
imageries and/or social media contents. Our experiments reveal that GeoLocator
generates specific geographic details with high accuracy and consequently
embeds the risk of the model users exposing geospatial information to the
public unintentionally, highlighting the thread of online data sharing,
information gathering technologies and LLM on geo-privacy. We conclude with the
broader implications of GeoLocator and our findings for individuals and the
community at large, by emphasizing the urgency for enhanced awareness and
protective measures against geo-privacy leakage in the era of advanced AI and
widespread social media usage.
&lt;/p&gt;
&lt;p&gt;Keywords: geoprivacy, GPT-4, image comprehension, Large Multimodal Model
(LMM), Open Source Intelligence (OSINT)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Daoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shuju Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Junhong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junzhou He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qingyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16146">
<title>Emulators in JINSP. (arXiv:2311.16146v2 [cs.NI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16146</link>
<description rdf:parseType="Literal">&lt;p&gt;JINSP(Jiutian Intelligence Network Simulation Platform) describes a series of
basic emulators and their combinations, such as the simulation of the protocol
stack for dynamic users in a real environment, which is composed of user
behavior simulation, base station simulation, and terminal simulation. It is
applied in specific business scenarios, such as multi-target antenna
optimization, compression feedback, and so on. This paper provides detailed
descriptions of each emulator and its combination based on this foundation,
including the implementation process of the emulator, integration with the
platform, experimental results, and other aspects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Miaomiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhe_L/0/1/0/all/0/1&quot;&gt;Lv Zhe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.18259">
<title>Ego-Exo4D: Understanding Skilled Human Activity from First- and Third-Person Perspectives. (arXiv:2311.18259v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.18259</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Ego-Exo4D, a diverse, large-scale multimodal multiview video
dataset and benchmark challenge. Ego-Exo4D centers around
simultaneously-captured egocentric and exocentric video of skilled human
activities (e.g., sports, music, dance, bike repair). More than 800
participants from 13 cities worldwide performed these activities in 131
different natural scene contexts, yielding long-form captures from 1 to 42
minutes each and 1,422 hours of video combined. The multimodal nature of the
dataset is unprecedented: the video is accompanied by multichannel audio, eye
gaze, 3D point clouds, camera poses, IMU, and multiple paired language
descriptions -- including a novel &quot;expert commentary&quot; done by coaches and
teachers and tailored to the skilled-activity domain. To push the frontier of
first-person video understanding of skilled human activity, we also present a
suite of benchmark tasks and their annotations, including fine-grained activity
understanding, proficiency estimation, cross-view translation, and 3D hand/body
pose. All resources will be open sourced to fuel new research in the community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westbury_A/0/1/0/all/0/1&quot;&gt;Andrew Westbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1&quot;&gt;Lorenzo Torresani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitani_K/0/1/0/all/0/1&quot;&gt;Kris Kitani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afouras_T/0/1/0/all/0/1&quot;&gt;Triantafyllos Afouras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1&quot;&gt;Kumar Ashutosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baiyya_V/0/1/0/all/0/1&quot;&gt;Vijay Baiyya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_S/0/1/0/all/0/1&quot;&gt;Siddhant Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boote_B/0/1/0/all/0/1&quot;&gt;Bikram Boote&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byrne_E/0/1/0/all/0/1&quot;&gt;Eugene Byrne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavis_Z/0/1/0/all/0/1&quot;&gt;Zach Chavis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Joya Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1&quot;&gt;Feng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_F/0/1/0/all/0/1&quot;&gt;Fu-Jen Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crane_S/0/1/0/all/0/1&quot;&gt;Sean Crane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dasgupta_A/0/1/0/all/0/1&quot;&gt;Avijit Dasgupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Jing Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Escobar_M/0/1/0/all/0/1&quot;&gt;Maria Escobar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forigua_C/0/1/0/all/0/1&quot;&gt;Cristhian Forigua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebreselasie_A/0/1/0/all/0/1&quot;&gt;Abrham Gebreselasie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haresh_S/0/1/0/all/0/1&quot;&gt;Sanjay Haresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md Mohaiminul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Suyog Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khirodkar_R/0/1/0/all/0/1&quot;&gt;Rawal Khirodkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kukreja_D/0/1/0/all/0/1&quot;&gt;Devansh Kukreja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Kevin J Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jia-Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_S/0/1/0/all/0/1&quot;&gt;Sagnik Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Y/0/1/0/all/0/1&quot;&gt;Yongsen Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_M/0/1/0/all/0/1&quot;&gt;Miguel Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavroudi_E/0/1/0/all/0/1&quot;&gt;Effrosyni Mavroudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagarajan_T/0/1/0/all/0/1&quot;&gt;Tushar Nagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ragusa_F/0/1/0/all/0/1&quot;&gt;Francesco Ragusa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_S/0/1/0/all/0/1&quot;&gt;Santhosh Kumar Ramakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seminara_L/0/1/0/all/0/1&quot;&gt;Luigi Seminara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somayazulu_A/0/1/0/all/0/1&quot;&gt;Arjun Somayazulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yale Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shan Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zihui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Edward Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinxu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castillo_A/0/1/0/all/0/1&quot;&gt;Angela Castillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xinzhu Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furuta_R/0/1/0/all/0/1&quot;&gt;Ryosuke Furuta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_C/0/1/0/all/0/1&quot;&gt;Cristina Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Prince Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jiabo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yifei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoo_W/0/1/0/all/0/1&quot;&gt;Weslie Khoo&lt;/a&gt;, et al. (48 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00878">
<title>Grounding Everything: Emerging Localization Properties in Vision-Language Transformers. (arXiv:2312.00878v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00878</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language foundation models have shown remarkable performance in
various zero-shot settings such as image retrieval, classification, or
captioning. But so far, those models seem to fall behind when it comes to
zero-shot localization of referential expressions and objects in images. As a
result, they need to be fine-tuned for this task. In this paper, we show that
pretrained vision-language (VL) models allow for zero-shot open-vocabulary
object localization without any fine-tuning. To leverage those capabilities, we
propose a Grounding Everything Module (GEM) that generalizes the idea of
value-value attention introduced by CLIPSurgery to a self-self attention path.
We show that the concept of self-self attention corresponds to clustering, thus
enforcing groups of tokens arising from the same object to be similar while
preserving the alignment with the language space. To further guide the group
formation, we propose a set of regularizations that allows the model to finally
generalize across datasets and backbones. We evaluate the proposed GEM
framework on various benchmark tasks and datasets for semantic segmentation. It
shows that GEM not only outperforms other training-free open-vocabulary
localization methods, but also achieves state-of-the-art results on the
recently proposed OpenImagesV7 large-scale segmentation benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bousselham_W/0/1/0/all/0/1&quot;&gt;Walid Bousselham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_F/0/1/0/all/0/1&quot;&gt;Felix Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrari_V/0/1/0/all/0/1&quot;&gt;Vittorio Ferrari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuehne_H/0/1/0/all/0/1&quot;&gt;Hilde Kuehne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03664">
<title>Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia. (arXiv:2312.03664v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03664</link>
<description rdf:parseType="Literal">&lt;p&gt;Agent-based modeling has been around for decades, and applied widely across
the social and natural sciences. The scope of this research method is now
poised to grow dramatically as it absorbs the new affordances provided by Large
Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just
classic Agent-Based Models (ABM)s where the agents talk to one another. Rather,
GABMs are constructed using an LLM to apply common sense to situations, act
&quot;reasonably&quot;, recall common semantic knowledge, produce API calls to control
digital technologies like apps, and communicate both within the simulation and
to researchers viewing it from the outside. Here we present Concordia, a
library to facilitate constructing and working with GABMs. Concordia makes it
easy to construct language-mediated simulations of physically- or
digitally-grounded environments. Concordia agents produce their behavior using
a flexible component system which mediates between two fundamental operations:
LLM calls and associative memory retrieval. A special agent called the Game
Master (GM), which was inspired by tabletop role-playing games, is responsible
for simulating the environment where the agents interact. Agents take actions
by describing what they want to do in natural language. The GM then translates
their actions into appropriate implementations. In a simulated physical world,
the GM checks the physical plausibility of agent actions and describes their
effects. In digital environments simulating technologies such as apps and
services, the GM may handle API calls to integrate with external tools such as
general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,
Email, Search, etc.). Concordia was designed to support a wide array of
applications both in scientific research and for evaluating performance of real
digital services by simulating users and/or generating synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vezhnevets_A/0/1/0/all/0/1&quot;&gt;Alexander Sasha Vezhnevets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapiou_J/0/1/0/all/0/1&quot;&gt;John P. Agapiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharon_A/0/1/0/all/0/1&quot;&gt;Avia Aharon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziv_R/0/1/0/all/0/1&quot;&gt;Ron Ziv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matyas_J/0/1/0/all/0/1&quot;&gt;Jayd Matyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duenez_Guzman_E/0/1/0/all/0/1&quot;&gt;Edgar A. Du&amp;#xe9;&amp;#xf1;ez-Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunningham_W/0/1/0/all/0/1&quot;&gt;William A. Cunningham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1&quot;&gt;Simon Osindero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmon_D/0/1/0/all/0/1&quot;&gt;Danny Karmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1&quot;&gt;Joel Z. Leibo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03818">
<title>Alpha-CLIP: A CLIP Model Focusing on Wherever You Want. (arXiv:2312.03818v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03818</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive Language-Image Pre-training (CLIP) plays an essential role in
extracting valuable content information from images across diverse tasks. It
aligns textual and visual modalities to comprehend the entire image, including
all the details, even those irrelevant to specific tasks. However, for a finer
understanding and controlled editing of images, it becomes crucial to focus on
specific regions of interest, which can be indicated as points, masks, or boxes
by humans or perception models. To fulfill the requirements, we introduce
Alpha-CLIP, an enhanced version of CLIP with an auxiliary alpha channel to
suggest attentive regions and fine-tuned with constructed millions of RGBA
region-text pairs. Alpha-CLIP not only preserves the visual recognition ability
of CLIP but also enables precise control over the emphasis of image contents.
It demonstrates effectiveness in various tasks, including but not limited to
open-world recognition, multimodal large language models, and conditional 2D /
3D generation. It has a strong potential to serve as a versatile tool for
image-related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zeyi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Ye Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Pan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shu Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuanjun Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04386">
<title>Model-Based Epistemic Variance of Values for Risk-Aware Policy Optimization. (arXiv:2312.04386v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04386</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of quantifying uncertainty over expected cumulative
rewards in model-based reinforcement learning. In particular, we focus on
characterizing the variance over values induced by a distribution over MDPs.
Previous work upper bounds the posterior variance over values by solving a
so-called uncertainty Bellman equation (UBE), but the over-approximation may
result in inefficient exploration. We propose a new UBE whose solution
converges to the true posterior variance over values and leads to lower regret
in tabular exploration problems. We identify challenges to apply the UBE theory
beyond tabular problems and propose a suitable approximation. Based on this
approximation, we introduce a general-purpose policy optimization algorithm,
Q-Uncertainty Soft Actor-Critic (QU-SAC), that can be applied for either
risk-seeking or risk-averse policy optimization with minimal changes.
Experiments in both online and offline RL demonstrate improved performance
compared to other uncertainty estimation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luis_C/0/1/0/all/0/1&quot;&gt;Carlos E. Luis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bottero_A/0/1/0/all/0/1&quot;&gt;Alessandro G. Bottero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vinogradska_J/0/1/0/all/0/1&quot;&gt;Julia Vinogradska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berkenkamp_F/0/1/0/all/0/1&quot;&gt;Felix Berkenkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_J/0/1/0/all/0/1&quot;&gt;Jan Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06036">
<title>AI Competitions and Benchmarks: towards impactful challenges with post-challenge papers, benchmarks and other dissemination actions. (arXiv:2312.06036v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06036</link>
<description rdf:parseType="Literal">&lt;p&gt;Organising an AI challenge does not end with the final event. The
long-lasting impact also needs to be organised. This chapter covers the various
activities after the challenge is formally finished. The target audience of
different post-challenge activities is identified. The various outputs of the
challenge are listed with the means to collect them. The main part of the
chapter is a template for a typical post-challenge paper, including possible
graphs as well as advice on how to turn the challenge into a long-lasting
benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marot_A/0/1/0/all/0/1&quot;&gt;Antoine Marot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseau_D/0/1/0/all/0/1&quot;&gt;David Rousseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhen Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06887">
<title>Understanding and Leveraging the Learning Phases of Neural Networks. (arXiv:2312.06887v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06887</link>
<description rdf:parseType="Literal">&lt;p&gt;The learning dynamics of deep neural networks are not well understood. The
information bottleneck (IB) theory proclaimed separate fitting and compression
phases. But they have since been heavily debated. We comprehensively analyze
the learning dynamics by investigating a layer&apos;s reconstruction ability of the
input and prediction performance based on the evolution of parameters during
training. We empirically show the existence of three phases using common
datasets and architectures such as ResNet and VGG: (i) near constant
reconstruction loss, (ii) decrease, and (iii) increase. We also derive an
empirically grounded data model and prove the existence of phases for
single-layer networks. Technically, our approach leverages classical complexity
analysis. It differs from IB by relying on measuring reconstruction loss rather
than information theoretic measures to relate information of intermediate
layers and inputs. Our work implies a new best practice for transfer learning:
We show empirically that the pre-training of a classifier should stop well
before its performance is optimal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schneider_J/0/1/0/all/0/1&quot;&gt;Johannes Schneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhushankar_M/0/1/0/all/0/1&quot;&gt;Mohit Prabhushankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07492">
<title>SocialStigmaQA: A Benchmark to Uncover Stigma Amplification in Generative Language Models. (arXiv:2312.07492v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07492</link>
<description rdf:parseType="Literal">&lt;p&gt;Current datasets for unwanted social bias auditing are limited to studying
protected demographic features such as race and gender. In this work, we
introduce a comprehensive benchmark that is meant to capture the amplification
of social bias, via stigmas, in generative language models. We start with a
comprehensive list of 93 stigmas documented in social science literature and
curate a question-answering (QA) dataset which involves simple social
situations. Our benchmark, SocialStigmaQA, contains roughly 10K prompts, with a
variety of prompt styles, carefully constructed to systematically test for both
social bias and model robustness. We present results for SocialStigmaQA with
two widely used open source generative language models and we demonstrate that
the output generated by these models considerably amplifies existing social
bias against stigmatized groups. Specifically, we find that the proportion of
socially biased output ranges from 45% to 59% across a variety of decoding
strategies and prompting styles. We discover that the deliberate design of the
templates in our benchmark (e.g., by adding biasing text to the prompt or
varying the answer that indicates bias) impact the model tendencies to generate
socially biased output. Additionally, we report on patterns in the generated
chain-of-thought output, finding a variety of problems from subtle bias to
evidence of a lack of reasoning.
&lt;/p&gt;
&lt;p&gt;Warning: This paper contains examples of text which is toxic, biased, and
harmful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagireddy_M/0/1/0/all/0/1&quot;&gt;Manish Nagireddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiazor_L/0/1/0/all/0/1&quot;&gt;Lamogha Chiazor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Moninder Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldini_I/0/1/0/all/0/1&quot;&gt;Ioana Baldini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08078">
<title>Fine-Grained Image-Text Alignment in Medical Imaging Enables Cyclic Image-Report Generation. (arXiv:2312.08078v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08078</link>
<description rdf:parseType="Literal">&lt;p&gt;To address these issues, we propose a novel Adaptive patch-word Matching
(AdaMatch) model to correlate chest X-ray (CXR) image regions with words in
medical reports and apply it to CXR-report generation to provide explainability
for the generation process. AdaMatch exploits the fine-grained relation between
adaptive patches and words to provide explanations of specific image regions
with corresponding words. To capture the abnormal regions of varying sizes and
positions, we introduce the Adaptive Patch extraction (AdaPatch) module to
acquire the adaptive patches for these regions adaptively. In order to provide
explicit explainability for CXR-report generation task, we propose an
AdaMatch-based bidirectional large language model for Cyclic CXR-report
generation (AdaMatch-Cyclic). It employs the AdaMatch to obtain the keywords
for CXR images and `keypatches&apos; for medical reports as hints to guide
CXR-report generation. Extensive experiments on two publicly available CXR
datasets prove the effectiveness of our method and its superior performance to
existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenting Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Linlin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08221">
<title>Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-smoothness in Deep GNNs. (arXiv:2312.08221v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08221</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite Graph neural networks&apos; significant performance gain over many classic
techniques in various graph-related downstream tasks, their successes are
restricted in shallow models due to over-smoothness and the difficulties of
optimizations among many other issues. In this paper, to alleviate the
over-smoothing issue, we propose a soft graph normalization method to preserve
the diversities of node embeddings and prevent indiscrimination due to possible
over-closeness. Combined with residual connections, we analyze the reason why
the method can effectively capture the knowledge in both input graph structures
and node features even with deep networks. Additionally, inspired by Curriculum
Learning that learns easy examples before the hard ones, we propose a novel
label-smoothing-based learning framework to enhance the optimization of deep
GNNs, which iteratively smooths labels in an auxiliary graph and constructs
many gradual non-smooth tasks for extracting increasingly complex knowledge and
gradually discriminating nodes from coarse to fine. The method arguably reduces
the risk of overfitting and generalizes better results. Finally, extensive
experiments are carried out to demonstrate the effectiveness and potential of
the proposed model and learning framework through comparison with twelve
existing baselines including the state-of-the-art methods on twelve real-world
node classification benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qirong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuling Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Longkun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yang-Geng Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08274">
<title>High-throughput Biomedical Relation Extraction for Semi-Structured Web Articles Empowered by Large Language Models. (arXiv:2312.08274v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08274</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: To develop a high-throughput biomedical relation extraction system
that takes advantage of the large language models&apos; (LLMs) reading comprehension
ability and biomedical world knowledge in a scalable and evidential manner.
Methods: We formulate the relation extraction task as a simple binary
classification problem for large language models such as ChatGPT. Specifically,
LLMs make the decision based on the external corpus and its world knowledge,
giving the reason for the judgment to factual verification. This method is
tailored for semi-structured web articles, wherein we designate the main title
as the tail entity and explicitly incorporate it into the context, and the
potential head entities are matched based on a biomedical thesaurus. Moreover,
lengthy contents are sliced into text chunks, embedded, and retrieved with
additional embedding models, ensuring compatibility with the context window
size constraints of available open-source LLMs. Results: Using an open-source
LLM, we extracted 304315 relation triplets of three distinct relation types
from four reputable biomedical websites. To assess the efficacy of the basic
pipeline employed for biomedical relation extraction, we curated a benchmark
dataset annotated by a medical expert. Evaluation results indicate that the
pipeline exhibits performance comparable to that of GPT-4. Case studies further
illuminate challenges faced by contemporary LLMs in the context of biomedical
relation extraction for semi-structured web articles. Conclusion: The proposed
method has demonstrated its effectiveness in leveraging the strengths of LLMs
for high-throughput biomedical relation extraction. Its adaptability is
evident, as it can be seamlessly extended to diverse semi-structured biomedical
websites, facilitating the extraction of various types of biomedical relations
with ease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Songchi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sheng Yu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>