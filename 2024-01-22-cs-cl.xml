<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CL updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computation and Language (cs.CL) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computation and Language</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09774" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09785" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10002" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10005" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10045" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10091" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.08090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03128" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08483" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.10321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08417" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.09432">
<title>RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09432</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents RoleCraft-GLM, an innovative framework aimed at enhancing
personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM
addresses the key issue of lacking personalized interactions in conversational
AI, and offers a solution with detailed and emotionally nuanced character
portrayals. We contribute a unique conversational dataset that shifts from
conventional celebrity-centric characters to diverse, non-celebrity personas,
thus enhancing the realism and complexity of language modeling interactions.
Additionally, our approach includes meticulous character development, ensuring
dialogues are both realistic and emotionally resonant. The effectiveness of
RoleCraft-GLM is validated through various case studies, highlighting its
versatility and skill in different scenarios. Our framework excels in
generating dialogues that accurately reflect characters&apos; personality traits and
emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks
a significant leap in personalized AI interactions, and paves the way for more
authentic and immersive AI-assisted role-playing experiences by enabling more
nuanced and emotionally rich dialogues
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Meiling Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xuechen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1&quot;&gt;Tianyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yiting Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09446">
<title>Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09446</link>
<description rdf:parseType="Literal">&lt;p&gt;Memes have become a distinctive and effective form of communication in the
digital era, attracting online communities and cutting across cultural
barriers. Even though memes are frequently linked with humor, they have an
amazing capacity to convey a wide range of emotions, including happiness,
sarcasm, frustration, and more. Understanding and interpreting the sentiment
underlying memes has become crucial in the age of information. Previous
research has explored text-based, image-based, and multimodal approaches,
leading to the development of models like CAPSAN and PromptHate for detecting
various meme categories. However, the study of low-resource languages like
Bengali memes remains scarce, with limited availability of publicly accessible
datasets. A recent contribution includes the introduction of the MemoSen
dataset. However, the achieved accuracy is notably low, and the dataset suffers
from imbalanced distribution. In this study, we employed a multimodal approach
using ResNet50 and BanglishBERT and achieved a satisfactory result of 0.71
weighted F1-score, performed comparison with unimodal approaches, and
interpreted behaviors of the models using explainable artificial intelligence
(XAI) techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_K/0/1/0/all/0/1&quot;&gt;Kazi Toufique Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1&quot;&gt;Tasnuva Binte Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1&quot;&gt;Shakil Shahriar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1&quot;&gt;Samir Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joy_S/0/1/0/all/0/1&quot;&gt;Sajib Kumar Saha Joy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1&quot;&gt;Faisal Muhammad Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09454">
<title>Voila-A: Aligning Vision-Language Models with User&apos;s Gaze Attention. (arXiv:2401.09454v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09454</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the integration of vision and language understanding has led
to significant advancements in artificial intelligence, particularly through
Vision-Language Models (VLMs). However, existing VLMs face challenges in
handling real-world applications with complex scenes and multiple objects, as
well as aligning their focus with the diverse attention patterns of human
users. In this paper, we introduce gaze information, feasibly collected by AR
or VR devices, as a proxy for human attention to guide VLMs and propose a novel
approach, Voila-A, for gaze alignment to enhance the interpretability and
effectiveness of these models in real-world applications. First, we collect
hundreds of minutes of gaze data to demonstrate that we can mimic human gaze
modalities using localized narratives. We then design an automatic data
annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset.
Additionally, we innovate the Voila Perceiver modules to integrate gaze
information into VLMs while preserving their pretrained knowledge. We evaluate
Voila-A using a hold-out validation set and a newly collected VOILA-GAZE
Testset, which features real-life scenarios captured with a gaze-tracking
device. Our experimental results demonstrate that Voila-A significantly
outperforms several baseline models. By aligning model attention with human
gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and
fosters engaging human-AI interaction across a wide range of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuai Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09486">
<title>LoMA: Lossless Compressed Memory Attention. (arXiv:2401.09486v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09486</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to handle long texts is one of the most important capabilities of
Large Language Models (LLMs), but as the text length increases, the consumption
of resources also increases dramatically. At present, reducing resource
consumption by compressing the KV cache is a common approach. Although there
are many existing compression methods, they share a common drawback: the
compression is not lossless. That is, information is inevitably lost during the
compression process. If the compression rate is high, the probability of losing
important information increases dramatically. We propose a new method, Lossless
Compressed Memory Attention (LoMA), which allows for lossless compression of
information into special memory token KV pairs according to a set compression
ratio. Our experiments have achieved remarkable results, demonstrating that
LoMA can be efficiently trained and has very effective performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yumeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhenyang Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09553">
<title>BERTologyNavigator: Advanced Question Answering with BERT-based Semantics. (arXiv:2401.09553v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09553</link>
<description rdf:parseType="Literal">&lt;p&gt;The development and integration of knowledge graphs and language models has
significance in artificial intelligence and natural language processing. In
this study, we introduce the BERTologyNavigator -- a two-phased system that
combines relation extraction techniques and BERT embeddings to navigate the
relationships within the DBLP Knowledge Graph (KG). Our approach focuses on
extracting one-hop relations and labelled candidate pairs in the first phases.
This is followed by employing BERT&apos;s CLS embeddings and additional heuristics
for relation selection in the second phase. Our system reaches an F1 score of
0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score
on the subset of the DBLP QuAD test dataset during the QA phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpal_S/0/1/0/all/0/1&quot;&gt;Shreya Rajpal&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1&quot;&gt;Ricardo Usbeck&lt;/a&gt; (1) ((1) Universit&amp;#xe4;t Hamburg, Hamburg, Germany,(2) Vellore Institute of Technology, Vellore, Tamil Nadu, India)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09555">
<title>Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09555</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of artificial intelligence, where a vast majority of data is
unstructured, obtaining substantial amounts of labeled data to train supervised
machine learning models poses a significant challenge. To address this, we
delve into few-shot and active learning, where are goal is to improve AI models
with human feedback on a few labeled examples. This paper focuses on
understanding how a continuous feedback loop can refine models, thereby
enhancing their accuracy, recall, and precision through incremental human
input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and
SetFit, we aim to analyze the efficacy of using a limited number of labeled
examples to substantially improve model accuracy. We benchmark this approach on
the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to
prove that with just a few labeled examples, we are able to surpass the
accuracy of zero shot large language models to provide enhanced text
classification performance. We demonstrate that rather than needing to manually
label millions of rows of data, we just need to label a few and the model can
effectively predict the rest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidra_N/0/1/0/all/0/1&quot;&gt;Natan Vidra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_T/0/1/0/all/0/1&quot;&gt;Thomas Clifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jijo_K/0/1/0/all/0/1&quot;&gt;Katherine Jijo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_E/0/1/0/all/0/1&quot;&gt;Eden Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09566">
<title>Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09566</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in large language models (LLMs) have demonstrated remarkable
capabilities across a diverse range of applications. These models excel in
generating text completions that are contextually coherent and cover an
extensive array of subjects. However, the vast datasets required for their
training make aligning response styles during the pretraining and instruction
tuning phases challenging. Consequently, an additional alignment phase is
typically employed, wherein the model is further trained with human preference
data to better align its outputs with human expectations. While this process
doesn&apos;t introduce new capabilities per se, it does accentuate generation styles
innate to the model. This paper explores the utilization of counterfactual
prompting within the framework of Direct Preference Optimization (DPO) to align
the model&apos;s style without relying on human intervention. We demonstrate that
this method effectively instils desirable behaviour, mitigates undesirable
ones, and encourages the model to disregard inappropriate instructions. Our
findings suggest that counterfactual prompting with DPO presents a low-resource
way to fine-tune LLMs to meet the demands for responsible and ethically aligned
AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butcher_B/0/1/0/all/0/1&quot;&gt;Bradley Butcher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09615">
<title>Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09615</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) has enabled significant
performance gains in the field of natural language processing. However, recent
studies have found that LLMs often resort to shortcuts when performing tasks,
creating an illusion of enhanced performance while lacking generalizability in
their decision rules. This phenomenon introduces challenges in accurately
assessing natural language understanding in LLMs. Our paper provides a concise
survey of relevant research in this area and puts forth a perspective on the
implications of shortcut learning in the evaluation of language models,
specifically for NLU tasks. This paper urges more research efforts to be put
towards deepening our comprehension of shortcut learning, contributing to the
development of more robust language models, and raising the standards of NLU
evaluation in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihani_G/0/1/0/all/0/1&quot;&gt;Geetanjali Bihani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1&quot;&gt;Julia Taylor Rayz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09637">
<title>Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study. (arXiv:2401.09637v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.09637</link>
<description rdf:parseType="Literal">&lt;p&gt;Patients derive numerous benefits from reading their clinical notes,
including an increased sense of control over their health and improved
understanding of their care plan. However, complex medical concepts and jargon
within clinical notes hinder patient comprehension and may lead to anxiety. We
developed a patient-facing tool to make clinical notes more readable,
leveraging large language models (LLMs) to simplify, extract information from,
and add context to notes. We prompt engineered GPT-4 to perform these
augmentation tasks on real clinical notes donated by breast cancer survivors
and synthetic notes generated by a clinician, a total of 12 notes with 3868
words. In June 2023, 200 female-identifying US-based participants were randomly
assigned three clinical notes with varying levels of augmentations using our
tool. Participants answered questions about each note, evaluating their
understanding of follow-up actions and self-reported confidence. We found that
augmentations were associated with a significant increase in action
understanding score (0.63 $\pm$ 0.04 for select augmentations, compared to 0.54
$\pm$ 0.02 for the control) with p=0.002. In-depth interviews of
self-identifying breast cancer patients (N=7) were also conducted via video
conferencing. Augmentations, especially definitions, elicited positive
responses among the seven participants, with some concerns about relying on
LLMs. Augmentations were evaluated for errors by clinicians, and we found
misleading errors occur, with errors more common in real donated notes than
synthetic notes, illustrating the importance of carefully written clinical
notes. Augmentations improve some but not all readability metrics. This work
demonstrates the potential of LLMs to improve patients&apos; experience with
clinical notes at a lower burden to clinicians. However, having a human in the
loop is important to correct potential model errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannhardt_N/0/1/0/all/0/1&quot;&gt;Niklas Mannhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondi_Kelly_E/0/1/0/all/0/1&quot;&gt;Elizabeth Bondi-Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_B/0/1/0/all/0/1&quot;&gt;Barbara Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnell_C/0/1/0/all/0/1&quot;&gt;Chloe O&amp;#x27;Connell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asiedu_M/0/1/0/all/0/1&quot;&gt;Mercy Asiedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozannar_H/0/1/0/all/0/1&quot;&gt;Hussein Mozannar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1&quot;&gt;Monica Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buendia_A/0/1/0/all/0/1&quot;&gt;Alejandro Buendia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urman_T/0/1/0/all/0/1&quot;&gt;Tatiana Urman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riaz_I/0/1/0/all/0/1&quot;&gt;Irbaz B. Riaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricciardi_C/0/1/0/all/0/1&quot;&gt;Catherine E. Ricciardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09646">
<title>ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09646</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces ClimateGPT, a model family of domain-specific large
language models that synthesize interdisciplinary research on climate change.
We trained two 7B models from scratch on a science-oriented dataset of 300B
tokens. For the first model, the 4.2B domain-specific tokens were included
during pre-training and the second was adapted to the climate domain after
pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously
pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each
model is instruction fine-tuned on a high-quality and human-generated
domain-specific dataset that has been created in close cooperation with climate
scientists. To reduce the number of hallucinations, we optimize the model for
retrieval augmentation and propose a hierarchical retrieval strategy. To
increase the accessibility of our model to non-English speakers, we propose to
make use of cascaded machine translation and show that this approach can
perform comparably to natively multilingual models while being easier to scale
to a large number of languages. Further, to address the intrinsic
interdisciplinary aspect of climate change we consider different research
perspectives. Therefore, the model can produce in-depth answers focusing on
different perspectives in addition to an overall answer. We propose a suite of
automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks,
ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model
while not degrading results on general domain benchmarks. Our human evaluation
confirms the trends we saw in our benchmarks. All models were trained and
evaluated using renewable energy and are released publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1&quot;&gt;David Thulke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yingbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelser_P/0/1/0/all/0/1&quot;&gt;Petrus Pelser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brune_R/0/1/0/all/0/1&quot;&gt;Rein Brune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalota_R/0/1/0/all/0/1&quot;&gt;Rricha Jalota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fok_F/0/1/0/all/0/1&quot;&gt;Floris Fok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_M/0/1/0/all/0/1&quot;&gt;Michael Ramos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyk_I/0/1/0/all/0/1&quot;&gt;Ian van Wyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasir_A/0/1/0/all/0/1&quot;&gt;Abdallah Nasir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_H/0/1/0/all/0/1&quot;&gt;Hayden Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tragemann_T/0/1/0/all/0/1&quot;&gt;Taylor Tragemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Katie Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fowler_A/0/1/0/all/0/1&quot;&gt;Ariana Fowler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanco_A/0/1/0/all/0/1&quot;&gt;Andrew Stanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_J/0/1/0/all/0/1&quot;&gt;Jon Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1&quot;&gt;Jordan Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moro_D/0/1/0/all/0/1&quot;&gt;Dean Moro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsymbalov_E/0/1/0/all/0/1&quot;&gt;Evgenii Tsymbalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waal_J/0/1/0/all/0/1&quot;&gt;Juliette de Waal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matusov_E/0/1/0/all/0/1&quot;&gt;Evgeny Matusov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaghi_M/0/1/0/all/0/1&quot;&gt;Mudar Yaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shihadah_M/0/1/0/all/0/1&quot;&gt;Mohammad Shihadah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1&quot;&gt;Hermann Ney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dugast_C/0/1/0/all/0/1&quot;&gt;Christian Dugast&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dotan_J/0/1/0/all/0/1&quot;&gt;Jonathan Dotan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erasmus_D/0/1/0/all/0/1&quot;&gt;Daniel Erasmus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09647">
<title>Characterizing Online Eating Disorder Communities with Large Language Models. (arXiv:2401.09647v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.09647</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise in eating disorders, a dangerous mental health condition with high
mortality and morbidity, has been linked to the proliferation of idealized body
images on social media. However, the link between social media and eating
disorders is far more complex. We argue that social media platforms create a
feedback loop that amplifies the growth of content and communities that promote
eating disorders like anorexia and bulimia. Specifically, social media
platforms make it easy for vulnerable individuals to find and connect to
like-minded others, while group dynamic processes encourage them to stay
engaged within communities that promote and glorify harmful behaviors linked to
eating disorders. We characterize this dynamic empirically through a
combination of network and language analysis. We describe a novel framework
that leverages large language models to analyze the discourse within online
communities and probe their attitudes on topics related to eating disorders to
identify potentially harmful content. Our work emphasizes the need for better
social media moderation to disrupt harmful feedback loops and protect
vulnerable individuals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_M/0/1/0/all/0/1&quot;&gt;Minh Duc Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnati_A/0/1/0/all/0/1&quot;&gt;Aryan Karnati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zihao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lerman_K/0/1/0/all/0/1&quot;&gt;Kristina Lerman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09699">
<title>Curriculum Recommendations Using Transformer Base Model with InfoNCE Loss And Language Switching Method. (arXiv:2401.09699v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09699</link>
<description rdf:parseType="Literal">&lt;p&gt;The Curriculum Recommendations paradigm is dedicated to fostering learning
equality within the ever-evolving realms of educational technology and
curriculum development. In acknowledging the inherent obstacles posed by
existing methodologies, such as content conflicts and disruptions from language
translation, this paradigm aims to confront and overcome these challenges.
Notably, it addresses content conflicts and disruptions introduced by language
translation, hindrances that can impede the creation of an all-encompassing and
personalized learning experience. The paradigm&apos;s objective is to cultivate an
educational environment that not only embraces diversity but also customizes
learning experiences to suit the distinct needs of each learner. To overcome
these challenges, our approach builds upon notable contributions in curriculum
development and personalized learning, introducing three key innovations. These
include the integration of Transformer Base Model to enhance computational
efficiency, the implementation of InfoNCE Loss for accurate content-topic
matching, and the adoption of a language switching strategy to alleviate
translation-related ambiguities. Together, these innovations aim to
collectively tackle inherent challenges and contribute to forging a more
equitable and effective learning journey for a diverse range of learners.
Competitive cross-validation scores underscore the efficacy of
sentence-transformers/LaBSE, achieving 0.66314, showcasing our methodology&apos;s
effectiveness in diverse linguistic nuances for content alignment prediction.
Index Terms-Curriculum Recommendation, Transformer model with InfoNCE Loss,
Language Switching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaonan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1&quot;&gt;Yongyao Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tianbo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shulin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09724">
<title>Predicting Viral Rumors and Vulnerable Users for Infodemic Surveillance. (arXiv:2401.09724v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.09724</link>
<description rdf:parseType="Literal">&lt;p&gt;In the age of the infodemic, it is crucial to have tools for effectively
monitoring the spread of rampant rumors that can quickly go viral, as well as
identifying vulnerable users who may be more susceptible to spreading such
misinformation. This proactive approach allows for timely preventive measures
to be taken, mitigating the negative impact of false information on society. We
propose a novel approach to predict viral rumors and vulnerable users using a
unified graph neural network model. We pre-train network-based user embeddings
and leverage a cross-attention mechanism between users and posts, together with
a community-enhanced vulnerability propagation (CVP) method to improve user and
propagation graph representations. Furthermore, we employ two multi-task
training strategies to mitigate negative transfer effects among tasks in
different settings, enhancing the overall performance of our approach. We also
construct two datasets with ground-truth annotations on information virality
and user vulnerability in rumor and non-rumor events, which are automatically
derived from existing rumor detection datasets. Extensive evaluation results of
our joint learning model confirm its superiority over strong baselines in all
three tasks: rumor detection, virality prediction, and user vulnerability
scoring. For instance, compared to the best baselines based on the Weibo
dataset, our model makes 3.8\% and 3.0\% improvements on Accuracy and MacF1 for
rumor detection, and reduces mean squared error (MSE) by 23.9\% and 16.5\% for
virality prediction and user vulnerability scoring, respectively. Our findings
suggest that our approach effectively captures the correlation between rumor
virality and user vulnerability, leveraging this information to improve
prediction performance and provide a valuable tool for infodemic surveillance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wei Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09727">
<title>Large Language Model Lateral Spear Phishing: A Comparative Study in Large-Scale Organizational Settings. (arXiv:2401.09727v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.09727</link>
<description rdf:parseType="Literal">&lt;p&gt;The critical threat of phishing emails has been further exacerbated by the
potential of LLMs to generate highly targeted, personalized, and automated
spear phishing attacks. Two critical problems concerning LLM-facilitated
phishing require further investigation: 1) Existing studies on lateral phishing
lack specific examination of LLM integration for large-scale attacks targeting
the entire organization, and 2) Current anti-phishing infrastructure, despite
its extensive development, lacks the capability to prevent LLM-generated
attacks, potentially impacting both employees and IT security incident
management. However, the execution of such investigative studies necessitates a
real-world environment, one that functions during regular business operations
and mirrors the complexity of a large organizational infrastructure. This
setting must also offer the flexibility required to facilitate a diverse array
of experimental conditions, particularly the incorporation of phishing emails
crafted by LLMs. This study is a pioneering exploration into the use of Large
Language Models (LLMs) for the creation of targeted lateral phishing emails,
targeting a large tier 1 university&apos;s operation and workforce of approximately
9,000 individuals over an 11-month period. It also evaluates the capability of
email filtering infrastructure to detect such LLM-generated phishing attempts,
providing insights into their effectiveness and identifying potential areas for
improvement. Based on our findings, we propose machine learning-based detection
techniques for such emails to detect LLM-generated phishing emails that were
missed by the existing infrastructure, with an F1-score of 98.96.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethany_M/0/1/0/all/0/1&quot;&gt;Mazal Bethany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Galiopoulos_A/0/1/0/all/0/1&quot;&gt;Athanasios Galiopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bethany_E/0/1/0/all/0/1&quot;&gt;Emet Bethany&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karkevandi_M/0/1/0/all/0/1&quot;&gt;Mohammad Bahrami Karkevandi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwamitra_N/0/1/0/all/0/1&quot;&gt;Nishant Vishwamitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najafirad_P/0/1/0/all/0/1&quot;&gt;Peyman Najafirad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09758">
<title>Resolving Regular Polysemy in Named Entities. (arXiv:2401.09758v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09758</link>
<description rdf:parseType="Literal">&lt;p&gt;Word sense disambiguation primarily addresses the lexical ambiguity of common
words based on a predefined sense inventory. Conversely, proper names are
usually considered to denote an ad-hoc real-world referent. Once the reference
is decided, the ambiguity is purportedly resolved. However, proper names also
exhibit ambiguities through appellativization, i.e., they act like common words
and may denote different aspects of their referents. We proposed to address the
ambiguities of proper names through the light of regular polysemy, which we
formalized as dot objects. This paper introduces a combined word sense
disambiguation (WSD) model for disambiguating common words against Chinese
Wordnet (CWN) and proper names as dot objects. The model leverages the
flexibility of a gloss-based model architecture, which takes advantage of the
glosses and example sentences of CWN. We show that the model achieves
competitive results on both common and proper nouns, even on a relatively
sparse sense dataset. Aside from being a performant WSD tool, the model further
facilitates the future development of the lexical resource.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_S/0/1/0/all/0/1&quot;&gt;Shu-Kai Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tseng_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsiang Tseng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_H/0/1/0/all/0/1&quot;&gt;Hsin-Yu Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Ching-Wen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yu-Yun Chang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09760">
<title>A Comparative Study on Annotation Quality of Crowdsourcing and LLM via Label Aggregation. (arXiv:2401.09760v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09760</link>
<description rdf:parseType="Literal">&lt;p&gt;Whether Large Language Models (LLMs) can outperform crowdsourcing on the data
annotation task is attracting interest recently. Some works verified this issue
with the average performance of individual crowd workers and LLM workers on
some specific NLP tasks by collecting new datasets. However, on the one hand,
existing datasets for the studies of annotation quality in crowdsourcing are
not yet utilized in such evaluations, which potentially provide reliable
evaluations from a different viewpoint. On the other hand, the quality of these
aggregated labels is crucial because, when utilizing crowdsourcing, the
estimated labels aggregated from multiple crowd labels to the same instances
are the eventually collected labels. Therefore, in this paper, we first
investigate which existing crowdsourcing datasets can be used for a comparative
study and create a benchmark. We then compare the quality between individual
crowd labels and LLM labels and make the evaluations on the aggregated labels.
In addition, we propose a Crowd-LLM hybrid label aggregation method and verify
the performance. We find that adding LLM labels from good LLMs to existing
crowdsourcing datasets can enhance the quality of the aggregated labels of the
datasets, which is also higher than the quality of LLM labels themselves.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiyi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09774">
<title>On the Audio Hallucinations in Large Audio-Video Language Models. (arXiv:2401.09774v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2401.09774</link>
<description rdf:parseType="Literal">&lt;p&gt;Large audio-video language models can generate descriptions for both video
and audio. However, they sometimes ignore audio content, producing audio
descriptions solely reliant on visual information. This paper refers to this as
audio hallucinations and analyzes them in large audio-video language models. We
gather 1,000 sentences by inquiring about audio information and annotate them
whether they contain hallucinations. If a sentence is hallucinated, we also
categorize the type of hallucination. The results reveal that 332 sentences are
hallucinated with distinct trends observed in nouns and verbs for each
hallucination type. Based on this, we tackle a task of audio hallucination
classification using pre-trained audio-text models in the zero-shot and
fine-tuning settings. Our experimental results reveal that the zero-shot models
achieve higher performance (52.2% in F1) than the random (40.3%) and the
fine-tuning models achieve 87.9%, outperforming the zero-shot models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nishimura_T/0/1/0/all/0/1&quot;&gt;Taichi Nishimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakada_S/0/1/0/all/0/1&quot;&gt;Shota Nakada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kondo_M/0/1/0/all/0/1&quot;&gt;Masayoshi Kondo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09775">
<title>Controllable Decontextualization of Yes/No Question and Answers into Factual Statements. (arXiv:2401.09775v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09775</link>
<description rdf:parseType="Literal">&lt;p&gt;Yes/No or polar questions represent one of the main linguistic question
categories. They consist of a main interrogative clause, for which the answer
is binary (assertion or negation). Polar questions and answers (PQA) represent
a valuable knowledge resource present in many community and other curated QA
sources, such as forums or e-commerce applications. Using answers to polar
questions alone in other contexts is not trivial. Answers are contextualized,
and presume that the interrogative question clause and any shared knowledge
between the asker and answerer are provided.
&lt;/p&gt;
&lt;p&gt;We address the problem of controllable rewriting of answers to polar
questions into decontextualized and succinct factual statements. We propose a
Transformer sequence to sequence model that utilizes soft-constraints to ensure
controllable rewriting, such that the output statement is semantically
equivalent to its PQA input. Evaluation on three separate PQA datasets as
measured through automated and human evaluation metrics show that our proposed
approach achieves the best performance when compared to existing baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_L/0/1/0/all/0/1&quot;&gt;Lingbo Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1&quot;&gt;Besnik Fetahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1&quot;&gt;Oleg Rokhlenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1&quot;&gt;Shervin Malmasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09783">
<title>Leveraging Biases in Large Language Models: &quot;bias-kNN&apos;&apos; for Effective Few-Shot Learning. (arXiv:2401.09783v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09783</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown significant promise in various
applications, including zero-shot and few-shot learning. However, their
performance can be hampered by inherent biases. Instead of traditionally sought
methods that aim to minimize or correct these biases, this study introduces a
novel methodology named ``bias-kNN&apos;&apos;. This approach capitalizes on the biased
outputs, harnessing them as primary features for kNN and supplementing with
gold labels. Our comprehensive evaluations, spanning diverse domain text
classification datasets and different GPT-2 model sizes, indicate the
adaptability and efficacy of the ``bias-kNN&apos;&apos; method. Remarkably, this approach
not only outperforms conventional in-context learning in few-shot scenarios but
also demonstrates robustness across a spectrum of samples, templates and
verbalizers. This study, therefore, presents a unique perspective on harnessing
biases, transforming them into assets for enhanced model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hanzhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhitao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Ning Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jing Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianzong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09785">
<title>Instant Answering in E-Commerce Buyer-Seller Messaging. (arXiv:2401.09785v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09785</link>
<description rdf:parseType="Literal">&lt;p&gt;E-commerce customers frequently seek detailed product information for
purchase decisions, commonly contacting sellers directly with extended queries.
This manual response requirement imposes additional costs and disrupts buyer&apos;s
shopping experience with response time fluctuations ranging from hours to days.
We seek to automate buyer inquiries to sellers in a leading e-commerce store
using a domain-specific federated Question Answering (QA) system. The main
challenge is adapting current QA systems, designed for single questions, to
address detailed customer queries. We address this with a low-latency,
sequence-to-sequence approach, MESSAGE-TO-QUESTION ( M2Q ). It reformulates
buyer messages into succinct questions by identifying and extracting the most
salient information from a message. Evaluation against baselines shows that M2Q
yields relative increases of 757% in question understanding, and 1,746% in
answering rate from the federated QA system. Live deployment shows that
automatic answering saves sellers from manually responding to millions of
messages per year, and also accelerates customer purchase decisions by
eliminating the need for buyers to wait for a reply
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fetahu_B/0/1/0/all/0/1&quot;&gt;Besnik Fetahu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_T/0/1/0/all/0/1&quot;&gt;Tejas Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Q/0/1/0/all/0/1&quot;&gt;Qun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vedula_N/0/1/0/all/0/1&quot;&gt;Nikhita Vedula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rokhlenko_O/0/1/0/all/0/1&quot;&gt;Oleg Rokhlenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malmasi_S/0/1/0/all/0/1&quot;&gt;Shervin Malmasi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09798">
<title>All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09798</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) like ChatGPT face `jailbreak&apos; challenges, where
safeguards are bypassed to produce ethically harmful prompts. This study
introduces a simple black-box method to effectively generate jailbreak prompts,
overcoming the limitations of high complexity and computational costs
associated with existing methods. The proposed technique iteratively rewrites
harmful prompts into non-harmful expressions using the target LLM itself, based
on the hypothesis that LLMs can directly sample safeguard-bypassing
expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4)
and Gemini-Pro, this method achieved an attack success rate of over 80% within
an average of 5 iterations and remained effective despite model updates. The
jailbreak prompts generated were naturally-worded and concise, suggesting they
are less detectable. The results indicate that creating effective jailbreak
prompts is simpler than previously considered, and black-box jailbreak attacks
pose a more serious security threat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takemoto_K/0/1/0/all/0/1&quot;&gt;Kazuhiro Takemoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09815">
<title>Simple and effective data augmentation for compositional generalization. (arXiv:2401.09815v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09815</link>
<description rdf:parseType="Literal">&lt;p&gt;Compositional generalization, the ability to predict complex meanings from
training on simpler sentences, poses challenges for powerful pretrained seq2seq
models. In this paper, we show that data augmentation methods that sample MRs
and backtranslate them can be effective for compositional generalization, but
only if we sample from the right distribution. Remarkably, sampling from a
uniform distribution performs almost as well as sampling from the test
distribution, and greatly outperforms earlier methods that sampled from the
training distribution. We further conduct experiments to investigate the reason
why this happens and where the benefit of such data augmentation methods come
from.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yuekun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koller_A/0/1/0/all/0/1&quot;&gt;Alexander Koller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09839">
<title>MatSciRE: Leveraging Pointer Networks to Automate Entity and Relation Extraction for Material Science Knowledge-base Construction. (arXiv:2401.09839v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09839</link>
<description rdf:parseType="Literal">&lt;p&gt;Material science literature is a rich source of factual information about
various categories of entities (like materials and compositions) and various
relations between these entities, such as conductivity, voltage, etc.
Automatically extracting this information to generate a material science
knowledge base is a challenging task. In this paper, we propose MatSciRE
(Material Science Relation Extractor), a Pointer Network-based encoder-decoder
framework, to jointly extract entities and relations from material science
articles as a triplet ($entity1, relation, entity2$). Specifically, we target
the battery materials and identify five relations to work on - conductivity,
coulombic efficiency, capacity, voltage, and energy. Our proposed approach
achieved a much better F1-score (0.771) than a previous attempt using
ChemDataExtractor (0.716). The overall graphical framework of MatSciRE is shown
in Fig 1. The material information is extracted from material science
literature in the form of entity-relation triplets using MatSciRE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullick_A/0/1/0/all/0/1&quot;&gt;Ankan Mullick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_A/0/1/0/all/0/1&quot;&gt;Akash Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaitanya_G/0/1/0/all/0/1&quot;&gt;G Sai Chaitanya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghui_S/0/1/0/all/0/1&quot;&gt;Samir Ghui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_T/0/1/0/all/0/1&quot;&gt;Tapas Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seung-Cheol Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharjee_S/0/1/0/all/0/1&quot;&gt;Satadeep Bhattacharjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_P/0/1/0/all/0/1&quot;&gt;Pawan Goyal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09862">
<title>Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments. (arXiv:2401.09862v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.09862</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) such as ChatGPT has attracted
considerable attention in various domains due to their remarkable performance
and versatility. As the use of these models continues to grow, the importance
of effective prompt engineering has come to the fore. Prompt optimization
emerges as a crucial challenge, as it has a direct impact on model performance
and the extraction of relevant information. Recently, evolutionary algorithms
(EAs) have shown promise in addressing this issue, paving the way for novel
optimization strategies. In this work, we propose a evolutionary
multi-objective (EMO) approach specifically tailored for prompt optimization
called EMO-Prompts, using sentiment analysis as a case study. We use sentiment
analysis capabilities as our experimental targets. Our results demonstrate that
EMO-Prompts effectively generates prompts capable of guiding the LLM to produce
texts embodying two conflicting emotions simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumann_J/0/1/0/all/0/1&quot;&gt;Jill Baumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kramer_O/0/1/0/all/0/1&quot;&gt;Oliver Kramer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09880">
<title>Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition. (arXiv:2401.09880v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.09880</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the interests of modern poultry farming is the vocalization of laying
hens which contain very useful information on health behavior. This information
is used as health and well-being indicators that help breeders better monitor
laying hens, which involves early detection of problems for rapid and more
effective intervention. In this work, we focus on the sound analysis for the
recognition of the types of calls of the laying hens in order to propose a
robust system of characterization of their behavior for a better monitoring. To
do this, we first collected and annotated laying hen call signals, then
designed an optimal acoustic characterization based on the combination of time
and frequency domain features. We then used these features to build the
multi-label classification models based on recurrent neural network to assign a
semantic class to the vocalization that characterize the laying hen behavior.
The results show an overall performance with our model based on the combination
of time and frequency domain features that obtained the highest F1-score
(F1=92.75) with a gain of 17% on the models using the frequency domain features
and of 8% on the compared approaches from the litterature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;jus A. A. Laleye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousse_M/0/1/0/all/0/1&quot;&gt;Mika&amp;#xeb;l A. Mousse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09890">
<title>A Survey on Hardware Accelerators for Large Language Models. (arXiv:2401.09890v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2401.09890</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have emerged as powerful tools for natural
language processing tasks, revolutionizing the field with their ability to
understand and generate human-like text. As the demand for more sophisticated
LLMs continues to grow, there is a pressing need to address the computational
challenges associated with their scale and complexity. This paper presents a
comprehensive survey on hardware accelerators designed to enhance the
performance and energy efficiency of Large Language Models. By examining a
diverse range of accelerators, including GPUs, FPGAs, and custom-designed
architectures, we explore the landscape of hardware solutions tailored to meet
the unique computational demands of LLMs. The survey encompasses an in-depth
analysis of architecture, performance metrics, and energy efficiency
considerations, providing valuable insights for researchers, engineers, and
decision-makers aiming to optimize the deployment of LLMs in real-world
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kachris_C/0/1/0/all/0/1&quot;&gt;Christoforos Kachris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09899">
<title>Meme-ingful Analysis: Enhanced Understanding of Cyberbullying in Memes Through Multimodal Explanations. (arXiv:2401.09899v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09899</link>
<description rdf:parseType="Literal">&lt;p&gt;Internet memes have gained significant influence in communicating political,
psychological, and sociocultural ideas. While memes are often humorous, there
has been a rise in the use of memes for trolling and cyberbullying. Although a
wide variety of effective deep learning-based models have been developed for
detecting offensive multimodal memes, only a few works have been done on
explainability aspect. Recent laws like &quot;right to explanations&quot; of General Data
Protection Regulation, have spurred research in developing interpretable models
rather than only focusing on performance. Motivated by this, we introduce {\em
MultiBully-Ex}, the first benchmark dataset for multimodal explanation from
code-mixed cyberbullying memes. Here, both visual and textual modalities are
highlighted to explain why a given meme is cyberbullying. A Contrastive
Language-Image Pretraining (CLIP) projection-based multimodal shared-private
multitask approach has been proposed for visual and textual explanation of a
meme. Experimental results demonstrate that training with multimodal
explanations improves performance in generating textual justifications and more
accurately identifying the visual evidence supporting a decision with reliable
performance improvements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1&quot;&gt;Prince Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maity_K/0/1/0/all/0/1&quot;&gt;Krishanu Maity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_R/0/1/0/all/0/1&quot;&gt;Raghav Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_A/0/1/0/all/0/1&quot;&gt;Apoorv Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_S/0/1/0/all/0/1&quot;&gt;Sriparna Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattacharyya_P/0/1/0/all/0/1&quot;&gt;Pushpak Bhattacharyya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09967">
<title>Sketch-Guided Constrained Decoding for Boosting Blackbox Large Language Models without Logit Access. (arXiv:2401.09967v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09967</link>
<description rdf:parseType="Literal">&lt;p&gt;Constrained decoding, a technique for enforcing constraints on language model
outputs, offers a way to control text generation without retraining or
architectural modifications. Its application is, however, typically restricted
to models that give users access to next-token distributions (usually via
softmax logits), which poses a limitation with blackbox large language models
(LLMs). This paper introduces sketch-guided constrained decoding (SGCD), a
novel approach to constrained decoding for blackbox LLMs, which operates
without access to the logits of the blackbox LLM. SGCD utilizes a locally
hosted auxiliary model to refine the output of an unconstrained blackbox LLM,
effectively treating this initial output as a &quot;sketch&quot; for further elaboration.
This approach is complementary to traditional logit-based techniques and
enables the application of constrained decoding in settings where full model
transparency is unavailable. We demonstrate the efficacy of SGCD through
experiments in closed information extraction and constituency parsing, showing
how it enhances the utility and flexibility of blackbox LLMs for complex NLP
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1&quot;&gt;Saibo Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doner_B/0/1/0/all/0/1&quot;&gt;Berkay D&amp;#xf6;ner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wendler_C/0/1/0/all/0/1&quot;&gt;Chris Wendler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1&quot;&gt;Martin Josifoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1&quot;&gt;Robert West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09972">
<title>Better Explain Transformers by Illuminating Important Information. (arXiv:2401.09972v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09972</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based models excel in various natural language processing (NLP)
tasks, attracting countless efforts to explain their inner workings. Prior
methods explain Transformers by focusing on the raw gradient and attention as
token attribution scores, where non-relevant information is often considered
during explanation computation, resulting in confusing results. In this work,
we propose highlighting the important information and eliminating irrelevant
information by a refined information flow on top of the layer-wise relevance
propagation (LRP) method. Specifically, we consider identifying syntactic and
positional heads as important attention heads and focus on the relevance
obtained from these important heads. Experimental results demonstrate that
irrelevant information does distort output attribution scores and then should
be masked during explanation computation. Compared to eight baselines on both
classification and question-answering datasets, our method consistently
outperforms with over 3\% to 33\% improvement on explanation metrics, providing
superior explanation performance. Our anonymous code repository is available
at: https://github.com/LinxinS97/Mask-LRP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linxin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yan Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_A/0/1/0/all/0/1&quot;&gt;Ao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecue_F/0/1/0/all/0/1&quot;&gt;Freddy Lecue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_I/0/1/0/all/0/1&quot;&gt;Irene Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09984">
<title>Gradable ChatGPT Translation Evaluation. (arXiv:2401.09984v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09984</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT, as a language model based on large-scale pre-training, has exerted a
profound influence on the domain of machine translation. In ChatGPT, a &quot;Prompt&quot;
refers to a segment of text or instruction employed to steer the model towards
generating a specific category of response. The design of the translation
prompt emerges as a key aspect that can wield influence over factors such as
the style, precision and accuracy of the translation to a certain extent.
However, there is a lack of a common standard and methodology on how to design
and select a translation prompt. Accordingly, this paper proposes a generic
taxonomy, which defines gradable translation prompts in terms of expression
type, translation style, POS information and explicit statement, thus
facilitating the construction of prompts endowed with distinct attributes
tailored for various translation tasks. Specific experiments and cases are
selected to validate and illustrate the effectiveness of the method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_H/0/1/0/all/0/1&quot;&gt;Hui Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_B/0/1/0/all/0/1&quot;&gt;Bei Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_L/0/1/0/all/0/1&quot;&gt;Lu Zong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinwei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10002">
<title>Distantly Supervised Morpho-Syntactic Model for Relation Extraction. (arXiv:2401.10002v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10002</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of Information Extraction (IE) involves automatically converting
unstructured textual content into structured data. Most research in this field
concentrates on extracting all facts or a specific set of relationships from
documents. In this paper, we present a method for the extraction and
categorisation of an unrestricted set of relationships from text. Our method
relies on morpho-syntactic extraction patterns obtained by a distant
supervision method, and creates Syntactic and Semantic Indices to extract and
classify candidate graphs. We evaluate our approach on six datasets built on
Wikidata and Wikipedia. The evaluation shows that our approach can achieve
Precision scores of up to 0.85, but with lower Recall and F1 scores. Our
approach allows to quickly create rule-based systems for Information Extraction
and to build annotated datasets to train machine-learning and deep-learning
based classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutehrle_N/0/1/0/all/0/1&quot;&gt;Nicolas Gutehrl&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atanassova_I/0/1/0/all/0/1&quot;&gt;Iana Atanassova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10005">
<title>Advancing Large Multi-modal Models with Explicit Chain-of-Reasoning and Visual Question Generation. (arXiv:2401.10005v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10005</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing demand for intelligent systems capable of interpreting and
reasoning about visual content requires the development of Large Multi-Modal
Models (LMMs) that are not only accurate but also have explicit reasoning
capabilities. This paper presents a novel approach to imbue an LMM with the
ability to conduct explicit reasoning based on visual content and textual
instructions. We introduce a system that can ask a question to acquire
necessary knowledge, thereby enhancing the robustness and explicability of the
reasoning process. Our method comprises the development of a novel dataset
generated by a Large Language Model (LLM), designed to promote chain-of-thought
reasoning combined with a question-asking mechanism. We designed an LMM, which
has high capabilities on region awareness to address the intricate requirements
of image-text alignment. The model undergoes a three-stage training phase,
starting with large-scale image-text alignment using a large-scale datasets,
followed by instruction tuning, and fine-tuning with a focus on
chain-of-thought reasoning. The results demonstrate a stride toward a more
robust, accurate, and interpretable LMM, capable of reasoning explicitly and
seeking information proactively when confronted with ambiguous visual input.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uehara_K/0/1/0/all/0/1&quot;&gt;Kohei Uehara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goswami_N/0/1/0/all/0/1&quot;&gt;Nabarun Goswami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanqin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baba_T/0/1/0/all/0/1&quot;&gt;Toshiaki Baba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_K/0/1/0/all/0/1&quot;&gt;Kohtaro Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tomohiro Hashimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_R/0/1/0/all/0/1&quot;&gt;Rei Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naoya_T/0/1/0/all/0/1&quot;&gt;Takagi Naoya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Umagami_R/0/1/0/all/0/1&quot;&gt;Ryo Umagami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yingyi Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anakewat_T/0/1/0/all/0/1&quot;&gt;Tanachai Anakewat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harada_T/0/1/0/all/0/1&quot;&gt;Tatsuya Harada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10015">
<title>Towards Hierarchical Spoken Language Dysfluency Modeling. (arXiv:2401.10015v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10015</link>
<description rdf:parseType="Literal">&lt;p&gt;Speech dysfluency modeling is the bottleneck for both speech therapy and
language learning. However, there is no AI solution to systematically tackle
this problem. We first propose to define the concept of dysfluent speech and
dysfluent speech modeling. We then present Hierarchical Unconstrained
Dysfluency Modeling (H-UDM) approach that addresses both dysfluency
transcription and detection to eliminate the need for extensive manual
annotation. Furthermore, we introduce a simulated dysfluent dataset called
VCTK++ to enhance the capabilities of H-UDM in phonetic transcription. Our
experimental results demonstrate the effectiveness and robustness of our
proposed methods in both transcription and detection tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_J/0/1/0/all/0/1&quot;&gt;Jiachen Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anumanchipalli_G/0/1/0/all/0/1&quot;&gt;Gopala Anumanchipalli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10016">
<title>Gender Bias in Machine Translation and The Era of Large Language Models. (arXiv:2401.10016v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10016</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter examines the role of Machine Translation in perpetuating gender
bias, highlighting the challenges posed by cross-linguistic settings and
statistical dependencies. A comprehensive overview of relevant existing work
related to gender bias in both conventional Neural Machine Translation
approaches and Generative Pretrained Transformer models employed as Machine
Translation systems is provided. Through an experiment using ChatGPT (based on
GPT-3.5) in an English-Italian translation context, we further assess ChatGPT&apos;s
current capacity to address gender bias. The findings emphasize the ongoing
need for advancements in mitigating bias in Machine Translation systems and
underscore the importance of fostering fairness and inclusivity in language
technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1&quot;&gt;Eva Vanmassenhove&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10019">
<title>R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10019</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have exhibited great potential in autonomously
completing tasks across real-world applications. Despite this, these LLM agents
introduce unexpected safety risks when operating in interactive environments.
Instead of centering on LLM-generated content safety in most prior studies,
this work addresses the imperative need for benchmarking the behavioral safety
of LLM agents within diverse environments. We introduce R-Judge, a benchmark
crafted to evaluate the proficiency of LLMs in judging safety risks given agent
interaction records. R-Judge comprises 162 agent interaction records,
encompassing 27 key risk scenarios among 7 application categories and 10 risk
types. It incorporates human consensus on safety with annotated safety risk
labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a
comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone
for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
the human score of 89.38%, showing considerable room for enhancing the risk
awareness of LLMs. Notably, leveraging risk descriptions as environment
feedback significantly improves model performance, revealing the importance of
salient safety risk feedback. Furthermore, we design an effective chain of
safety analysis technique to help the judgment of safety risks and conduct an
in-depth case study to facilitate future research. R-Judge is publicly
available at https://github.com/Lordog/R-Judge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_T/0/1/0/all/0/1&quot;&gt;Tongxin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiwei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Lingzhong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ruijie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tian Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lizhen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Binglin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gongshen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10020">
<title>Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10020</link>
<description rdf:parseType="Literal">&lt;p&gt;We posit that to achieve superhuman agents, future models require superhuman
feedback in order to provide an adequate training signal. Current approaches
commonly train reward models from human preferences, which may then be
bottlenecked by human performance level, and secondly these separate frozen
reward models cannot then learn to improve during LLM training. In this work,
we study Self-Rewarding Language Models, where the language model itself is
used via LLM-as-a-Judge prompting to provide its own rewards during training.
We show that during Iterative DPO training that not only does instruction
following ability improve, but also the ability to provide high-quality rewards
to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a
model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,
including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study,
this work opens the door to the possibility of models that can continually
improve in both axes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1&quot;&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1&quot;&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jason Weston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10030">
<title>Framing Analysis of Health-Related Narratives: Conspiracy versus Mainstream Media. (arXiv:2401.10030v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10030</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding how online media frame issues is crucial due to their impact on
public opinion. Research on framing using natural language processing
techniques mainly focuses on specific content features in messages and neglects
their narrative elements. Also, the distinction between framing in different
sources remains an understudied problem. We address those issues and
investigate how the framing of health-related topics, such as COVID-19 and
other diseases, differs between conspiracy and mainstream websites. We
incorporate narrative information into the framing analysis by introducing a
novel frame extraction approach based on semantic graphs. We find that
health-related narratives in conspiracy media are predominantly framed in terms
of beliefs, while mainstream media tend to present them in terms of science. We
hope our work offers new ways for a more nuanced frame analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reiter_Haas_M/0/1/0/all/0/1&quot;&gt;Markus Reiter-Haas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klosch_B/0/1/0/all/0/1&quot;&gt;Beate Kl&amp;#xf6;sch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadler_M/0/1/0/all/0/1&quot;&gt;Markus Hadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lex_E/0/1/0/all/0/1&quot;&gt;Elisabeth Lex&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10034">
<title>Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.10034</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), built upon Transformer-based architectures with
massive pretraining on diverse data, have not only revolutionized natural
language processing but also extended their prowess to various domains, marking
a significant stride towards artificial general intelligence. The interplay
between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives
and methodologies, reveals intriguing parallels, especially in their shared
optimization nature, black-box characteristics, and proficiency in handling
complex problems. Meanwhile, EA can not only provide an optimization framework
for LLM&apos;s further enhancement under black-box settings but also empower LLM
with flexible global search and iterative mechanism in applications. On the
other hand, LLM&apos;s abundant domain knowledge enables EA to perform smarter
searches, while its text processing capability assist in deploying EA across
various tasks. Based on their complementary advantages, this paper presents a
comprehensive review and forward-looking roadmap, categorizing their mutual
inspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM.
Some integrated synergy methods are further introduced to exemplify the
amalgamation of LLMs and EAs in various application scenarios, including neural
architecture search, code generation, software engineering, and text
generation. As the first comprehensive review specifically focused on the EA
research in the era of LLMs, this paper provides a foundational stepping stone
for understanding and harnessing the collaborative potential of LLMs and EAs.
By presenting a comprehensive review, categorization, and critical analysis, we
contribute to the ongoing discourse on the cross-disciplinary study of these
two powerful paradigms. The identified challenges and future directions offer
guidance to unlock the full potential of this innovative collaboration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xingyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sheng-hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jibin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Liang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kay Chen Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10040">
<title>Large Language Models for Scientific Information Extraction: An Empirical Study for Virology. (arXiv:2401.10040v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10040</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we champion the use of structured and semantic content
representation of discourse-based scholarly communication, inspired by tools
like Wikipedia infoboxes or structured Amazon product descriptions. These
representations provide users with a concise overview, aiding scientists in
navigating the dense academic landscape. Our novel automated approach leverages
the robust text generation capabilities of LLMs to produce structured scholarly
contribution summaries, offering both a practical solution and insights into
LLMs&apos; emergent abilities.
&lt;/p&gt;
&lt;p&gt;For LLMs, the prime focus is on improving their general intelligence as
conversational agents. We argue that these models can also be applied
effectively in information extraction (IE), specifically in complex IE tasks
within terse domains like Science. This paradigm shift replaces the traditional
modular, pipelined machine learning approach with a simpler objective expressed
through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer
parameters than the state-of-the-art GPT-davinci is competitive for the task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamsabadi_M/0/1/0/all/0/1&quot;&gt;Mahsa Shamsabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1&quot;&gt;Jennifer D&amp;#x27;Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Auer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10045">
<title>Antonym vs Synonym Distinction using InterlaCed Encoder NETworks (ICE-NET). (arXiv:2401.10045v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10045</link>
<description rdf:parseType="Literal">&lt;p&gt;Antonyms vs synonyms distinction is a core challenge in lexico-semantic
analysis and automated lexical resource construction. These pairs share a
similar distributional context which makes it harder to distinguish them.
Leading research in this regard attempts to capture the properties of the
relation pairs, i.e., symmetry, transitivity, and trans-transitivity. However,
the inability of existing research to appropriately model the relation-specific
properties limits their end performance. In this paper, we propose InterlaCed
Encoder NETworks (i.e., ICE-NET) for antonym vs synonym distinction, that aim
to capture and model the relation-specific properties of the antonyms and
synonyms pairs in order to perform the classification task in a
performance-enhanced manner. Experimental evaluation using the benchmark
datasets shows that ICE-NET outperforms the existing research by a relative
score of upto 1.8% in F1-measure. We release the codes for ICE-NET at
https://github.com/asif6827/ICENET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Muhammad Asif Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jianbin Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10065">
<title>Code Prompting Elicits Conditional Reasoning Abilities in Text+Code LLMs. (arXiv:2401.10065v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10065</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning is a fundamental component for achieving language understanding.
Among the multiple types of reasoning, conditional reasoning, the ability to
draw different conclusions depending on some condition, has been understudied
in large language models (LLMs). Recent prompting methods, such as chain of
thought, have significantly improved LLMs on reasoning tasks. Nevertheless,
there is still little understanding of what triggers reasoning abilities in
LLMs. We hypothesize that code prompts can trigger conditional reasoning in
LLMs trained on text and code. We propose a chain of prompts that transforms a
natural language problem into code and prompts the LLM with the generated code.
Our experiments find that code prompts exhibit a performance boost between 2.6
and 7.7 points on GPT 3.5 across multiple datasets requiring conditional
reasoning. We then conduct experiments to discover how code prompts elicit
conditional reasoning abilities and through which features. We observe that
prompts need to contain natural language text accompanied by high-quality code
that closely represents the semantics of the instance text. Furthermore, we
show that code prompts are more efficient, requiring fewer demonstrations, and
that they trigger superior state tracking of variables or key entities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puerto_H/0/1/0/all/0/1&quot;&gt;Haritz Puerto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tutek_M/0/1/0/all/0/1&quot;&gt;Martin Tutek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aditya_S/0/1/0/all/0/1&quot;&gt;Somak Aditya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaodan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevych_I/0/1/0/all/0/1&quot;&gt;Iryna Gurevych&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10070">
<title>Communication-Efficient Personalized Federated Learning for Speech-to-Text Tasks. (arXiv:2401.10070v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10070</link>
<description rdf:parseType="Literal">&lt;p&gt;To protect privacy and meet legal regulations, federated learning (FL) has
gained significant attention for training speech-to-text (S2T) systems,
including automatic speech recognition (ASR) and speech translation (ST).
However, the commonly used FL approach (i.e., \textsc{FedAvg}) in S2T tasks
typically suffers from extensive communication overhead due to multi-round
interactions based on the whole model and performance degradation caused by
data heterogeneity among clients.To address these issues, we propose a
personalized federated S2T framework that introduces \textsc{FedLoRA}, a
lightweight LoRA module for client-side tuning and interaction with the server
to minimize communication overhead, and \textsc{FedMem}, a global model
equipped with a $k$-nearest-neighbor ($k$NN) classifier that captures
client-specific distributional shifts to achieve personalization and overcome
data heterogeneity. Extensive experiments based on Conformer and Whisper
backbone models on CoVoST and GigaSpeech benchmarks show that our approach
significantly reduces the communication overhead on all S2T tasks and
effectively personalizes the global model to overcome data heterogeneity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yichao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_L/0/1/0/all/0/1&quot;&gt;Linan Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linli Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10091">
<title>Power in Numbers: Robust reading comprehension by finetuning with four adversarial sentences per example. (arXiv:2401.10091v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10091</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent models have achieved human level performance on the Stanford Question
Answering Dataset when using F1 scores to evaluate the reading comprehension
task. Yet, teaching machines to comprehend text has not been solved in the
general case. By appending one adversarial sentence to the context paragraph,
past research has shown that the F1 scores from reading comprehension models
drop almost in half. In this paper, I replicate past adversarial research with
a new model, ELECTRA-Small, and demonstrate that the new model&apos;s F1 score drops
from 83.9% to 29.2%. To improve ELECTRA-Small&apos;s resistance to this attack, I
finetune the model on SQuAD v1.1 training examples with one to five adversarial
sentences appended to the context paragraph. Like past research, I find that
the finetuned model on one adversarial sentence does not generalize well across
evaluation datasets. However, when finetuned on four or five adversarial
sentences the model attains an F1 score of more than 70% on most evaluation
datasets with multiple appended and prepended adversarial sentences. The
results suggest that with enough examples we can make models robust to
adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcus_A/0/1/0/all/0/1&quot;&gt;Ariel Marcus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10111">
<title>Marrying Adapters and Mixup to Efficiently Enhance the Adversarial Robustness of Pre-Trained Language Models for Text Classification. (arXiv:2401.10111v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10111</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing works show that augmenting training data of neural networks using
both clean and adversarial examples can enhance their generalizability under
adversarial attacks. However, this training approach often leads to performance
degradation on clean inputs. Additionally, it requires frequent re-training of
the entire model to account for new attack types, resulting in significant and
costly computations. Such limitations make adversarial training mechanisms less
practical, particularly for complex Pre-trained Language Models (PLMs) with
millions or even billions of parameters. To overcome these challenges while
still harnessing the theoretical benefits of adversarial training, this study
combines two concepts: (1) adapters, which enable parameter-efficient
fine-tuning, and (2) Mixup, which train NNs via convex combinations of pairs
data pairs. Intuitively, we propose to fine-tune PLMs through convex
combinations of non-data pairs of fine-tuned adapters, one trained with clean
and another trained with adversarial examples. Our experiments show that the
proposed method achieves the best trade-off between training efficiency and
predictive performance, both with and without attacks compared to other
baselines on a variety of downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tuc Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_T/0/1/0/all/0/1&quot;&gt;Thai Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10134">
<title>Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10134</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction, a critical component for intelligent transportation
systems, endeavors to foresee future traffic at specific locations using
historical data. Although existing traffic prediction models often emphasize
developing complex neural network structures, their accuracy has not seen
improvements accordingly. Recently, Large Language Models (LLMs) have shown
outstanding capabilities in time series analysis. Differing from existing
models, LLMs progress mainly through parameter expansion and extensive
pre-training while maintaining their fundamental structures. In this paper, we
propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic
prediction. Specifically, ST-LLM redefines the timesteps at each location as
tokens and incorporates a spatial-temporal embedding module to learn the
spatial location and global temporal representations of tokens. Then these
representations are fused to provide each token with unified spatial and
temporal information. Furthermore, we propose a novel partially frozen
attention strategy of the LLM, which is designed to capture spatial-temporal
dependencies for traffic prediction. Comprehensive experiments on real traffic
datasets offer evidence that ST-LLM outperforms state-of-the-art models.
Notably, the ST-LLM also exhibits robust performance in both few-shot and
zero-shot prediction scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qianxiong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhishuai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1&quot;&gt;Cheng Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10186">
<title>Beyond Reference-Based Metrics: Analyzing Behaviors of Open LLMs on Data-to-Text Generation. (arXiv:2401.10186v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10186</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate to which extent open large language models (LLMs) can generate
coherent and relevant text from structured data. To prevent bias from
benchmarks leaked into LLM training data, we collect Quintd-1: an ad-hoc
benchmark for five data-to-text (D2T) generation tasks, consisting of
structured data records in standard formats gathered from public APIs. We
leverage reference-free evaluation metrics and LLMs&apos; in-context learning
capabilities, allowing us to test the models with no human-written references.
Our evaluation focuses on annotating semantic accuracy errors on token-level,
combining human annotators and a metric based on GPT-4. Our systematic
examination of the models&apos; behavior across domains and tasks suggests that
state-of-the-art open LLMs with 7B parameters can generate fluent and coherent
text from various standard data formats in zero-shot settings. However, we also
show that semantic accuracy of the outputs remains a major issue: on our
benchmark, 80% of outputs of open LLMs contain a semantic error according to
human annotators (91% according to GPT-4). Our code, data, and model outputs
are available at https://d2t-llm.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasner_Z/0/1/0/all/0/1&quot;&gt;Zden&amp;#x11b;k Kasner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dusek_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Du&amp;#x161;ek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10189">
<title>Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10189</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained few-shot entity extraction in the chemical domain faces two
unique challenges. First, compared with entity extraction tasks in the general
domain, sentences from chemical papers usually contain more entities. Moreover,
entity extraction models usually have difficulty extracting entities of
long-tailed types. In this paper, we propose Chem-FINESE, a novel
sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to
address these two challenges. Our Chem-FINESE has two components: a seq2seq
entity extractor to extract named entities from the input sentence and a
seq2seq self-validation module to reconstruct the original input sentence from
extracted entities. Inspired by the fact that a good entity extraction system
needs to extract entities faithfully, our new self-validation module leverages
entity extraction results to reconstruct the original input sentence. Besides,
we design a new contrastive loss to reduce excessive copying during the
extraction process. Finally, we release ChemNER+, a new fine-grained chemical
entity extraction dataset that is annotated by domain experts with the ChemNER
schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets
show that our newly proposed framework has contributed up to 8.26% and 6.84%
absolute F1-score gains respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huimin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10208">
<title>MM-Interleaved: Interleaved Image-Text Generative Modeling via Multi-modal Feature Synchronizer. (arXiv:2401.10208v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10208</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing generative models for interleaved image-text data has both
research and practical value. It requires models to understand the interleaved
sequences and subsequently generate images and text. However, existing attempts
are limited by the issue that the fixed number of visual tokens cannot
efficiently capture image details, which is particularly problematic in the
multi-image scenarios. To address this, this paper presents MM-Interleaved, an
end-to-end generative model for interleaved image-text data. It introduces a
multi-scale and multi-image feature synchronizer module, allowing direct access
to fine-grained image features in the previous context during the generation
process. MM-Interleaved is end-to-end pre-trained on both paired and
interleaved image-text corpora. It is further enhanced through a supervised
fine-tuning phase, wherein the model improves its ability to follow complex
multi-modal instructions. Experiments demonstrate the versatility of
MM-Interleaved in recognizing visual details following multi-modal instructions
and generating consistent images following both textual and visual conditions.
Code and models are available at
\url{https://github.com/OpenGVLab/MM-Interleaved}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_C/0/1/0/all/0/1&quot;&gt;Changyao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xizhou Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yuwen Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuntao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jifeng Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10225">
<title>ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce ChatQA, a family of conversational question
answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we
propose a two-stage instruction tuning method that can significantly improve
the zero-shot conversational QA results from large language models (LLMs). To
handle retrieval in conversational QA, we fine-tune a dense retriever on a
multi-turn QA dataset, which provides comparable results to using the
state-of-the-art query rewriting model while largely reducing deployment cost.
Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10
conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic
data from OpenAI GPT models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1&quot;&gt;Rajarshi Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1&quot;&gt;Bryan Catanzaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15629">
<title>Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15629</link>
<description rdf:parseType="Literal">&lt;p&gt;Training generalist agents is difficult across several axes, requiring us to
deal with high-dimensional inputs (space), long horizons (time), and
generalization to novel tasks. Recent advances with architectures have allowed
for improved scaling along one or two of these axes, but are still
computationally prohibitive to use. In this paper, we propose to address all
three axes by leveraging \textbf{L}anguage to \textbf{C}ontrol
\textbf{D}iffusion models as a hierarchical planner conditioned on language
(LCD). We effectively and efficiently scale diffusion models for planning in
extended temporal, state, and task dimensions to tackle long horizon control
problems conditioned on natural language instructions, as a step towards
generalist agents. Comparing LCD with other state-of-the-art models on the
CALVIN language robotics benchmark finds that LCD outperforms other SOTA
methods in multi-task success rates, whilst improving inference speed over
other comparable diffusion models by 3.3x~15x. We show that LCD can
successfully leverage the unique strength of diffusion models to produce
coherent long range plans while addressing their weakness in generating
low-level details and control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Edwin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02472">
<title>ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02472</link>
<description rdf:parseType="Literal">&lt;p&gt;Studies have shown that modern neural networks tend to be poorly calibrated
due to over-confident predictions. Traditionally, post-processing methods have
been used to calibrate the model after training. In recent years, various
trainable calibration measures have been proposed to incorporate them directly
into the training process. However, these methods all incorporate internal
hyperparameters, and the performance of these calibration objectives relies on
tuning these hyperparameters, incurring more computational costs as the size of
neural networks and datasets become larger. As such, we present Expected
Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable
calibration objective loss, where we view the calibration error from the
perspective of the squared difference between the two expectations. With
extensive experiments on several architectures (CNNs, Transformers) and
datasets, we demonstrate that (1) incorporating ESD into the training improves
model calibration in various batch size settings without the need for internal
hyperparameter tuning, (2) ESD yields the best-calibrated results compared with
previous approaches, and (3) ESD drastically improves the computational costs
required for calibration during training due to the absence of internal
hyperparameter. The code is publicly accessible at
https://github.com/hee-suk-yoon/ESD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hee Suk Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tee_J/0/1/0/all/0/1&quot;&gt;Joshua Tian Jin Tee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_E/0/1/0/all/0/1&quot;&gt;Eunseop Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sunjae Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gwangsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09048">
<title>CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09048</link>
<description rdf:parseType="Literal">&lt;p&gt;Current generative knowledge graph construction approaches usually fail to
capture structural knowledge by simply flattening natural language into
serialized texts or a specification language. However, large generative
language model trained on structured data such as code has demonstrated
impressive capability in understanding natural language for structural
prediction and reasoning tasks. Intuitively, we address the task of generative
knowledge graph construction with code language model: given a code-format
natural language input, the target is to generate triples which can be
represented as code completion tasks. Specifically, we develop schema-aware
prompts that effectively utilize the semantic structure within the knowledge
graph. As code inherently possesses structure, such as class and function
definitions, it serves as a useful model for prior semantic structural
knowledge. Furthermore, we employ a rationale-enhanced generation method to
boost the performance. Rationales provide intermediate steps, thereby improving
knowledge extraction abilities. Experimental results indicate that the proposed
approach can obtain better performance on benchmark datasets compared with
baselines. Code and datasets are available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhen Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yinuo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1&quot;&gt;Feiyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11065">
<title>Conversational Process Modeling: Can Generative AI Empower Domain Experts in Creating and Redesigning Process Models?. (arXiv:2304.11065v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11065</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-driven chatbots such as ChatGPT have caused a tremendous hype lately. For
BPM applications, several applications for AI-driven chatbots have been
identified to be promising to generate business value, including explanation of
process mining outcomes and preparation of input data. However, a systematic
analysis of chatbots for their support of conversational process modeling as a
process-oriented capability is missing. This work aims at closing this gap by
providing a systematic analysis of existing chatbots. Application scenarios are
identified along the process life cycle. Then a systematic literature review on
conversational process modeling is performed, resulting in a taxonomy of
application scenarios for conversational process modeling, including
paraphrasing and improvement of process descriptions. In addition, this work
suggests and applies an evaluation method for the output of AI-driven chatbots
with respect to completeness and correctness of the process models. This method
consists of a set of KPIs on a test set, a set of prompts for task and control
flow extraction, as well as a survey with users. Based on the literature and
the evaluation, recommendations for the usage (practical implications) and
further development (research directions) of conversational process modeling
are derived.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klievtsova_N/0/1/0/all/0/1&quot;&gt;Nataliia Klievtsova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzin_J/0/1/0/all/0/1&quot;&gt;Janik-Vasily Benzin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampik_T/0/1/0/all/0/1&quot;&gt;Timotheus Kampik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangler_J/0/1/0/all/0/1&quot;&gt;Juergen Mangler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinderle_Ma_S/0/1/0/all/0/1&quot;&gt;Stefanie Rinderle-Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13971">
<title>Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning. (arXiv:2305.13971v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13971</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their impressive performance, large language models (LMs) still
struggle with reliably generating complex output structures when not finetuned
to follow the required output format exactly. To address this issue,
grammar-constrained decoding (GCD) can be used to control the generation of
LMs, guaranteeing that the output follows a given structure. Most existing GCD
methods are, however, limited to specific tasks, such as parsing or code
generation. In this work, we demonstrate that formal grammars can describe the
output space for a much wider range of tasks and argue that GCD can serve as a
unified framework for structured NLP tasks in general. For increased
flexibility, we introduce input-dependent grammars, which allow the grammar to
depend on the input and thus enable the generation of different output
structures for different inputs. We then empirically demonstrate the power and
flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity
disambiguation, and (3) constituency parsing. Our results indicate that
grammar-constrained LMs substantially outperform unconstrained LMs or even beat
task-specific finetuned models. Grammar constraints thus hold great promise for
harnessing off-the-shelf LMs for a wide range of structured NLP tasks,
especially where training data is scarce or finetuning is expensive. Code and
data: https://github.com/epfl-dlab/GCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1&quot;&gt;Saibo Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1&quot;&gt;Martin Josifoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1&quot;&gt;Maxime Peyrard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1&quot;&gt;Robert West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05535">
<title>Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05535</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing tools to automatically detect check-worthy claims in political
debates and speeches can greatly help moderators of debates, journalists, and
fact-checkers. While previous work on this problem has focused exclusively on
the text modality, here we explore the utility of the audio modality as an
additional input. We create a new multimodal dataset (text and audio in
English) containing 48 hours of speech from past political debates in the USA.
We then experimentally demonstrate that, in the case of multiple speakers,
adding the audio modality yields sizable improvements over using the text
modality alone; moreover, an audio-only model could outperform a text-only one
for a single speaker. With the aim to enable future research, we make all our
data and code publicly available at
https://github.com/petar-iv/audio-checkworthiness-detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanov_P/0/1/0/all/0/1&quot;&gt;Petar Ivanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1&quot;&gt;Ivan Koychev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1&quot;&gt;Momchil Hardalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1&quot;&gt;Preslav Nakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09212">
<title>CMMLU: Measuring massive multitask language understanding in Chinese. (arXiv:2306.09212v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09212</link>
<description rdf:parseType="Literal">&lt;p&gt;As the capabilities of large language models (LLMs) continue to advance,
evaluating their performance becomes increasingly crucial and challenging. This
paper aims to bridge this gap by introducing CMMLU, a comprehensive Chinese
benchmark that covers various subjects, including natural science, social
sciences, engineering, and humanities. We conduct a thorough evaluation of 18
advanced multilingual- and Chinese-oriented LLMs, assessing their performance
across different subjects and settings. The results reveal that most existing
LLMs struggle to achieve an average accuracy of 50%, even when provided with
in-context examples and chain-of-thought prompts, whereas the random baseline
stands at 25%. This highlights significant room for improvement in LLMs.
Additionally, we conduct extensive experiments to identify factors impacting
the models&apos; performance and propose directions for enhancing LLMs. CMMLU fills
the gap in evaluating the knowledge and reasoning capabilities of large
language models within the Chinese context.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haonan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koto_F/0/1/0/all/0/1&quot;&gt;Fajri Koto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yeyun Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldwin_T/0/1/0/all/0/1&quot;&gt;Timothy Baldwin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13269">
<title>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13269</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank adaptations (LoRA) are often employed to fine-tune large language
models (LLMs) for new tasks. This paper investigates LoRA composability for
cross-task generalization and introduces LoraHub, a simple framework devised
for the purposive assembly of LoRA modules trained on diverse given tasks, with
the objective of achieving adaptable performance on unseen tasks. With just a
few examples from a new task, LoraHub can fluidly combine multiple LoRA
modules, eliminating the need for human expertise and assumptions. Notably, the
composition requires neither additional model parameters nor gradients.
Empirical results on the Big-Bench Hard benchmark suggest that LoraHub, while
not surpassing the performance of in-context learning, offers a notable
performance-efficiency trade-off in few-shot scenarios by employing a
significantly reduced number of tokens per example during inference. Notably,
LoraHub establishes a better upper bound compared to in-context learning when
paired with different demonstration examples, demonstrating its potential for
future development. Our vision is to establish a platform for LoRA modules,
empowering users to share their trained LoRA modules. This collaborative
approach facilitates the seamless application of LoRA modules to novel tasks,
contributing to an adaptive ecosystem. Our code is available at
https://github.com/sail-sg/lorahub, and all the pre-trained LoRA modules are
released at https://huggingface.co/lorahub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengsong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.08090">
<title>Separate the Wheat from the Chaff: Model Deficiency Unlearning via Parameter-Efficient Module Operation. (arXiv:2308.08090v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.08090</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have been widely used in various applications
but are known to suffer from issues related to untruthfulness and toxicity.
While parameter-efficient modules (PEMs) have demonstrated their effectiveness
in equipping models with new skills, leveraging PEMs for deficiency unlearning
remains underexplored. In this work, we propose a PEMs operation approach,
namely Extraction-before-Subtraction (Ext-Sub), to enhance the truthfulness and
detoxification of LLMs through the integration of ``expert&apos;&apos; PEM and
``anti-expert&apos;&apos; PEM. Remarkably, even anti-expert PEM possess valuable
capabilities due to their proficiency in generating fabricated content, which
necessitates language modeling and logical narrative competence. Rather than
merely negating the parameters, our approach involves extracting and
eliminating solely the deficiency capability within anti-expert PEM while
preserving the general capabilities. To evaluate the effectiveness of our
approach in terms of truthfulness and detoxification, we conduct extensive
experiments on LLMs, encompassing additional abilities such as language
modeling and mathematical reasoning. Our empirical results demonstrate that our
approach effectively improves truthfulness and detoxification, while largely
preserving the fundamental abilities of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinshuo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dongfang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Baotian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zihao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10462">
<title>Exploring Parameter-Efficient Fine-Tuning Techniques for Code Generation with Large Language Models. (arXiv:2308.10462v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10462</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) demonstrate impressive capabilities to generate
accurate code snippets given natural language intents in zero-shot, i.e.,
without the need for specific fine-tuning. While prior studies have highlighted
the advantages of fine-tuning LLMs, this process incurs high computational
costs, making it impractical in resource-scarce environments, particularly for
models with billions of parameters. To address these challenges, previous
research explored In-Context Learning (ICL) as a strategy to guide the LLM
generative process with task-specific prompt examples. However, ICL introduces
inconveniences, such as the need for designing contextually relevant prompts
and the absence of learning task-specific parameters, thereby limiting
downstream task performance. In this context, we foresee Parameter-Efficient
Fine-Tuning (PEFT) techniques as a promising approach to efficiently specialize
LLMs to task-specific data while maintaining reasonable resource consumption.
In this paper, we deliver a comprehensive study of PEFT techniques for LLMs
under the automated code generation scenario. Our comprehensive investigation
of PEFT techniques for LLMs reveals their superiority and potential over ICL
across a diverse set of LLMs. Additionally, we demonstrate the extended
capabilities of PEFT, showcasing its ability to learn from two distinct
datasets jointly without compromising performance. Furthermore, our study
highlights the potential for tuning larger LLMs and significant reductions in
memory usage by combining PEFT with quantization. Therefore, this study opens
opportunities for broader applications of PEFT in software engineering
scenarios. Our code is available at
https://github.com/martin-wey/peft-llm-code/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weyssow_M/0/1/0/all/0/1&quot;&gt;Martin Weyssow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kisub Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1&quot;&gt;David Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahraoui_H/0/1/0/all/0/1&quot;&gt;Houari Sahraoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05448">
<title>Panoptic Vision-Language Feature Fields. (arXiv:2309.05448v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05448</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, methods have been proposed for 3D open-vocabulary semantic
segmentation. Such methods are able to segment scenes into arbitrary classes
based on text descriptions provided during runtime. In this paper, we propose
to the best of our knowledge the first algorithm for open-vocabulary panoptic
segmentation in 3D scenes. Our algorithm, Panoptic Vision-Language Feature
Fields (PVLFF), learns a semantic feature field of the scene by distilling
vision-language features from a pretrained 2D model, and jointly fits an
instance feature field through contrastive learning using 2D instance segments
on input frames. Despite not being trained on the target classes, our method
achieves panoptic segmentation performance similar to the state-of-the-art
closed-set 3D systems on the HyperSim, ScanNet and Replica dataset and
additionally outperforms current 3D open-vocabulary systems in terms of
semantic segmentation. We ablate the components of our method to demonstrate
the effectiveness of our model architecture. Our code will be available at
https://github.com/ethz-asl/pvlff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blomqvist_K/0/1/0/all/0/1&quot;&gt;Kenneth Blomqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milano_F/0/1/0/all/0/1&quot;&gt;Francesco Milano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1&quot;&gt;Roland Siegwart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07382">
<title>Less is More for Long Document Summary Evaluation by LLMs. (arXiv:2309.07382v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07382</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown promising performance in summary
evaluation tasks, yet they face challenges such as high computational costs and
the Lost-in-the-Middle problem where important information in the middle of
long documents is often overlooked. To address these issues, this paper
introduces a novel approach, Extract-then-Evaluate, which involves extracting
key sentences from a long source document and then evaluating the summary by
prompting LLMs. The results reveal that the proposed method not only
significantly reduces evaluation costs but also exhibits a higher correlation
with human evaluations. Furthermore, we provide practical recommendations for
optimal document length and sentence extraction methods, contributing to the
development of cost-effective yet more accurate methods for LLM-based text
generation evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yunshu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iso_H/0/1/0/all/0/1&quot;&gt;Hayate Iso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pezeshkpour_P/0/1/0/all/0/1&quot;&gt;Pouya Pezeshkpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhutani_N/0/1/0/all/0/1&quot;&gt;Nikita Bhutani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hruschka_E/0/1/0/all/0/1&quot;&gt;Estevam Hruschka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03128">
<title>MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use. (arXiv:2310.03128v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03128</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have garnered significant attention due to their
impressive natural language processing (NLP) capabilities. Recently, many
studies have focused on the tool utilization ability of LLMs. They primarily
investigated how LLMs effectively collaborate with given specific tools.
However, in scenarios where LLMs serve as intelligent agents, as seen in
applications like AutoGPT and MetaGPT, LLMs are expected to engage in intricate
decision-making processes that involve deciding whether to employ a tool and
selecting the most suitable tool(s) from a collection of available tools to
fulfill user requests. Therefore, in this paper, we introduce MetaTool, a
benchmark designed to evaluate whether LLMs have tool usage awareness and can
correctly choose tools. Specifically, we create a dataset called ToolE within
the benchmark. This dataset contains various types of user queries in the form
of prompts that trigger LLMs to use tools, including both single-tool and
multi-tool scenarios. Subsequently, we set the tasks for both tool usage
awareness and tool selection. We define four subtasks from different
perspectives in tool selection, including tool selection with similar choices,
tool selection in specific scenarios, tool selection with possible reliability
issues, and multi-tool selection. We conduct experiments involving eight
popular LLMs and find that the majority of them still struggle to effectively
select tools, highlighting the existing gaps between LLMs and genuine
intelligent agents. However, through the error analysis, we found there is
still significant room for improvement. Finally, we conclude with insights for
tool developers -- we strongly recommend that tool developers choose an
appropriate rewrite model for generating new descriptions based on the
downstream LLM the tool will apply to. Our code is in
\href{https://github.com/HowieHwong/MetaTool}{Github}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiawen Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Chenrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siyuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qihui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yixin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yao Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_N/0/1/0/all/0/1&quot;&gt;Neil Zhenqiang Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08483">
<title>Understanding the Humans Behind Online Misinformation: An Observational Study Through the Lens of the COVID-19 Pandemic. (arXiv:2310.08483v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08483</link>
<description rdf:parseType="Literal">&lt;p&gt;The proliferation of online misinformation has emerged as one of the biggest
threats to society. Considerable efforts have focused on building
misinformation detection models, still the perils of misinformation remain
abound. Mitigating online misinformation and its ramifications requires a
holistic approach that encompasses not only an understanding of its intricate
landscape in relation to the complex issue and topic-rich information ecosystem
online, but also the psychological drivers of individuals behind it. Adopting a
time series analytic technique and robust causal inference-based design, we
conduct a large-scale observational study analyzing over 32 million COVID-19
tweets and 16 million historical timeline tweets. We focus on understanding the
behavior and psychology of users disseminating misinformation during COVID-19
and its relationship with the historical inclinations towards sharing
misinformation on Non-COVID domains before the pandemic. Our analysis
underscores the intricacies inherent to cross-domain misinformation, and
highlights that users&apos; historical inclination toward sharing misinformation is
positively associated with their present behavior pertaining to misinformation
sharing on emergent topics and beyond. This work may serve as a valuable
foundation for designing user-centric inoculation strategies and
ecologically-grounded agile interventions for effectively tackling online
misinformation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_M/0/1/0/all/0/1&quot;&gt;Mohit Chandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mattapalli_A/0/1/0/all/0/1&quot;&gt;Anush Mattapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choudhury_M/0/1/0/all/0/1&quot;&gt;Munmun De Choudhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08744">
<title>Circuit Component Reuse Across Tasks in Transformer Language Models. (arXiv:2310.08744v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08744</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in mechanistic interpretability has shown that behaviors in
language models can be successfully reverse-engineered through circuit
analysis. A common criticism, however, is that each circuit is task-specific,
and thus such analysis cannot contribute to understanding the models at a
higher level. In this work, we present evidence that insights (both low-level
findings about specific heads and higher-level findings about general
algorithms) can indeed generalize across tasks. Specifically, we study the
circuit discovered in Wang et al. (2022) for the Indirect Object Identification
(IOI) task and 1.) show that it reproduces on a larger GPT2 model, and 2.) that
it is mostly reused to solve a seemingly different task: Colored Objects
(Ippolito &amp;amp; Callison-Burch, 2023). We provide evidence that the process
underlying both tasks is functionally very similar, and contains about a 78%
overlap in in-circuit attention heads. We further present a proof-of-concept
intervention experiment, in which we adjust four attention heads in middle
layers in order to &apos;repair&apos; the Colored Objects circuit and make it behave like
the IOI circuit. In doing so, we boost accuracy from 49.6% to 93.7% on the
Colored Objects task and explain most sources of error. The intervention
affects downstream attention heads in specific ways predicted by their
interactions in the IOI circuit, indicating that this subcircuit behavior is
invariant to the different task inputs. Overall, our results provide evidence
that it may yet be possible to explain large language models&apos; behavior in terms
of a relatively small number of interpretable task-general algorithmic building
blocks and computational components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Merullo_J/0/1/0/all/0/1&quot;&gt;Jack Merullo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eickhoff_C/0/1/0/all/0/1&quot;&gt;Carsten Eickhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlick_E/0/1/0/all/0/1&quot;&gt;Ellie Pavlick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11446">
<title>Functional Invariants to Watermark Large Transformers. (arXiv:2310.11446v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11446</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid growth of transformer-based models increases the concerns about
their integrity and ownership insurance. Watermarking addresses this issue by
embedding a unique identifier into the model, while preserving its performance.
However, most existing approaches require to optimize the weights to imprint
the watermark signal, which is not suitable at scale due to the computational
cost. This paper explores watermarks with virtually no computational cost,
applicable to a non-blind white-box setting (assuming access to both the
original and watermarked networks). They generate functionally equivalent
copies by leveraging the models&apos; invariance, via operations like dimension
permutations or scaling/unscaling. This enables to watermark models without any
change in their outputs and remains stealthy. Experiments demonstrate the
effectiveness of the approach and its robustness against various model
transformations (fine-tuning, quantization, pruning), making it a practical
solution to protect the integrity of large models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1&quot;&gt;Pierre Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1&quot;&gt;Guillaume Couairon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1&quot;&gt;Teddy Furon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1&quot;&gt;Matthijs Douze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12086">
<title>FactCHD: Benchmarking Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12086</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their impressive generative capabilities, LLMs are hindered by
fact-conflicting hallucinations in real-world applications. The accurate
identification of hallucinations in texts generated by LLMs, especially in
complex inferential scenarios, is a relatively unexplored area. To address this
gap, we present FactCHD, a dedicated benchmark designed for the detection of
fact-conflicting hallucinations from LLMs. FactCHD features a diverse dataset
that spans various factuality patterns, including vanilla, multi-hop,
comparison, and set operation. A distinctive element of FactCHD is its
integration of fact-based evidence chains, significantly enhancing the depth of
evaluating the detectors&apos; explanations. Experiments on different LLMs expose
the shortcomings of current approaches in detecting factual errors accurately.
Furthermore, we introduce Truth-Triangulator that synthesizes reflective
considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming
to yield more credible detection through the amalgamation of predictive results
and evidence. The benchmark dataset is available at
https://github.com/zjunlp/FactCHD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Duanzheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1&quot;&gt;Honghao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1&quot;&gt;Jiang Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chengfei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12798">
<title>MolCA: Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal Adapter. (arXiv:2310.12798v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12798</link>
<description rdf:parseType="Literal">&lt;p&gt;Language Models (LMs) have demonstrated impressive molecule understanding
ability on various 1D text-related tasks. However, they inherently lack 2D
graph perception - a critical ability of human professionals in comprehending
molecules&apos; topological structures. To bridge this gap, we propose MolCA:
Molecular Graph-Language Modeling with Cross-Modal Projector and Uni-Modal
Adapter. MolCA enables an LM (e.g., Galactica) to understand both text- and
graph-based molecular contents via the cross-modal projector. Specifically, the
cross-modal projector is implemented as a Q-Former to connect a graph encoder&apos;s
representation space and an LM&apos;s text space. Further, MolCA employs a uni-modal
adapter (i.e., LoRA) for the LM&apos;s efficient adaptation to downstream tasks.
Unlike previous studies that couple an LM with a graph encoder via cross-modal
contrastive learning, MolCA retains the LM&apos;s ability of open-ended text
generation and augments it with 2D graph information. To showcase its
effectiveness, we extensively benchmark MolCA on tasks of molecule captioning,
IUPAC name prediction, and molecule-text retrieval, on which MolCA
significantly outperforms the baselines. Our codes and checkpoints can be found
at https://github.com/acharkq/MolCA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sihang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yanchen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1&quot;&gt;Hao Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yixin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawaguchi_K/0/1/0/all/0/1&quot;&gt;Kenji Kawaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-Seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15141">
<title>SpecTr: Fast Speculative Decoding via Optimal Transport. (arXiv:2310.15141v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15141</link>
<description rdf:parseType="Literal">&lt;p&gt;Autoregressive sampling from large language models has led to
state-of-the-art results in several natural language tasks. However,
autoregressive sampling generates tokens one at a time making it slow, and even
prohibitive in certain tasks. One way to speed up sampling is
$\textit{speculative decoding}$: use a small model to sample a $\textit{draft}$
(block or sequence of tokens), and then score all tokens in the draft by the
large language model in parallel. A subset of the tokens in the draft are
accepted (and the rest rejected) based on a statistical method to guarantee
that the final output follows the distribution of the large model. In this
work, we provide a principled understanding of speculative decoding through the
lens of optimal transport (OT) with $\textit{membership cost}$. This framework
can be viewed as an extension of the well-known $\textit{maximal-coupling}$
problem. This new formulation enables us to generalize the speculative decoding
method to allow for a set of $k$ candidates at the token-level, which leads to
an improved optimal membership cost. We show that the optimal draft selection
algorithm (transport plan) can be computed via linear programming, whose
best-known runtime is exponential in $k$. We then propose a valid draft
selection algorithm whose acceptance probability is $(1-1/e)$-optimal
multiplicatively. Moreover, it can be computed in time almost linear with size
of domain of a single token. Using this $new draft selection$ algorithm, we
develop a new autoregressive sampling algorithm called $\textit{SpecTr}$, which
provides speedup in decoding while ensuring that there is no quality
degradation in the decoded output. We experimentally demonstrate that for
state-of-the-art large language models, the proposed approach achieves a wall
clock speedup of 2.13X, a further 1.37X speedup over speculative decoding on
standard benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Ziteng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suresh_A/0/1/0/all/0/1&quot;&gt;Ananda Theertha Suresh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ro_J/0/1/0/all/0/1&quot;&gt;Jae Hun Ro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beirami_A/0/1/0/all/0/1&quot;&gt;Ahmad Beirami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_H/0/1/0/all/0/1&quot;&gt;Himanshu Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Felix Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18913">
<title>Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18913</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are becoming the go-to solution for various language
tasks. However, with growing capacity, models are prone to rely on spurious
correlations stemming from biases and stereotypes present in the training data.
This work proposes a novel method for detecting and mitigating gender bias in
language models. We perform causal analysis to identify problematic model
components and discover that mid-upper feed-forward layers are most prone to
convey biases. Based on the analysis results, we adapt the model by multiplying
these layers by a linear projection. Our titular method, DAMA, significantly
decreases bias as measured by diverse metrics while maintaining the model&apos;s
performance on downstream tasks. We release code for our method and models,
which retrain LLaMA&apos;s state-of-the-art performance while being significantly
less biased.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1&quot;&gt;Tomasz Limisiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1&quot;&gt;David Mare&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musil_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Musil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13184">
<title>Large Language Model-Enhanced Algorithm Selection: Towards Comprehensive Algorithm Representation. (arXiv:2311.13184v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13184</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithm selection aims to identify the most suitable algorithm for solving
a specific problem before execution, which has become a critical process of the
AutoML. Current mainstream algorithm selection techniques rely heavily on
feature representations of various problems and employ the performance of each
algorithm as supervised information. However, there is a significant research
gap concerning the consideration of algorithm features. This gap is primarily
attributed to the inherent complexity of algorithms, making it particularly
challenging to find a universally effective feature extraction method that is
applicable across a diverse range of algorithms. Unfortunately, neglecting this
aspect undoubtedly impacts the accuracy of algorithm selection and indirectly
necessitates an increased volume of problem data for training purposes. This
paper takes a significant stride towards addressing this gap by proposing an
approach that integrates algorithm representation into the algorithm selection
process. Specifically, our proposed model employs distinct modules to extract
representations of both problems and algorithms, where the algorithm
representation leverages the capabilities of pre-trained LLMs in the realm of
code comprehension. Following the extraction of embedding vectors for both
algorithms and problems, the most suitable algorithm is determined through
calculations of matching degrees. Our experiments not only validate the
effectiveness of the proposed model but also showcase the performance of
different embedded pre-trained LLMs, which suggests that the proposed algorithm
selection framework holds the potential to serve as a baseline task for
evaluating the code representation capabilities of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xingyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jibin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bingbing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kay Chen Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03122">
<title>Assertion Enhanced Few-Shot Learning: Instructive Technique for Large Language Models to Generate Educational Explanations. (arXiv:2312.03122v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03122</link>
<description rdf:parseType="Literal">&lt;p&gt;Human educators possess an intrinsic ability to anticipate and seek
educational explanations from students, which drives them to pose
thought-provoking questions when students cannot articulate these explanations
independently. We aim to imbue Intelligent Tutoring Systems with this ability
using few-shot learning capability of Large Language Models. Our work proposes
a novel prompting technique, Assertion Enhanced Few-Shot Learning, to
facilitate the generation of accurate, detailed oriented educational
explanations. Our central hypothesis is that, in educational domain, few-shot
demonstrations are necessary but not a sufficient condition for quality
explanation generation. We conducted a study involving 12 in-service teachers,
comparing our approach to Traditional Few-Shot Learning. The results show that
Assertion Enhanced Few-Shot Learning improves explanation accuracy by 15% and
yields higher-quality explanations, as evaluated by teachers. We also conduct a
qualitative ablation study to factor the impact of assertions to provide
educator-friendly prompting guidelines for generating explanations in their
domain of interest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahriar_T/0/1/0/all/0/1&quot;&gt;Tasmia Shahriar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsuda_N/0/1/0/all/0/1&quot;&gt;Noboru Matsuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_K/0/1/0/all/0/1&quot;&gt;Kelly Ramos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.10321">
<title>LLM-SQL-Solver: Can LLMs Determine SQL Equivalence?. (arXiv:2312.10321v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2312.10321</link>
<description rdf:parseType="Literal">&lt;p&gt;Judging the equivalence between two SQL queries is a fundamental problem with
many practical applications in data management and SQL generation (i.e.,
evaluating the quality of generated SQL queries in text-to-SQL task). While the
research community has reasoned about SQL equivalence for decades, it poses
considerable difficulties and no complete solutions exist. Recently, Large
Language Models (LLMs) have shown strong reasoning capability in conversation,
question answering and solving mathematics challenges. In this paper, we study
if LLMs can be used to determine the equivalence between SQL queries under two
notions of SQL equivalence (semantic equivalence and relaxed equivalence). To
assist LLMs in generating high quality responses, we present two prompting
techniques: Miniature &amp;amp; Mull and Explain &amp;amp; Compare. The former technique is
used to evaluate the semantic equivalence in which it asks LLMs to execute a
query on a simple database instance and then explore if a counterexample exists
by modifying the database. The latter technique is used to evaluate the relaxed
equivalence in which it asks LLMs to explain the queries and then compare if
they contain significant logical differences. Our experiments demonstrate using
our techniques, LLMs is a promising tool to help data engineers in writing
semantically equivalent SQL queries, however challenges still persist, and is a
better metric for evaluating SQL generation than the popular execution
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_F/0/1/0/all/0/1&quot;&gt;Fuheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_L/0/1/0/all/0/1&quot;&gt;Lawrence Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_I/0/1/0/all/0/1&quot;&gt;Ishtiyaque Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_D/0/1/0/all/0/1&quot;&gt;Divyakant Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbadi_A/0/1/0/all/0/1&quot;&gt;Amr El Abbadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14345">
<title>Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs. (arXiv:2312.14345v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14345</link>
<description rdf:parseType="Literal">&lt;p&gt;The unique capabilities of Large Language Models (LLMs), such as the natural
language text generation ability, position them as strong candidates for
providing explanation for recommendations. However, despite the size of the
LLM, most existing models struggle to produce zero-shot explanations reliably.
To address this issue, we propose a framework called Logic-Scaffolding, that
combines the ideas of aspect-based explanation and chain-of-thought prompting
to generate explanations through intermediate reasoning steps. In this paper,
we share our experience in building the framework and present an interactive
demonstration for exploring our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahdari_B/0/1/0/all/0/1&quot;&gt;Behnam Rahdari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Ziwei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yifei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuotong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1&quot;&gt;Anoop Deoras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16171">
<title>Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4. (arXiv:2312.16171v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16171</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces 26 guiding principles designed to streamline the
process of querying and prompting large language models. Our goal is to
simplify the underlying concepts of formulating questions for various scales of
large language models, examining their abilities, and enhancing user
comprehension on the behaviors of different scales of large language models
when feeding into different prompts. Extensive experiments are conducted on
LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the
proposed principles on instructions and prompts design. We hope that this work
can provide a better guide for researchers working on the prompting of large
language models. Project page is available at
https://github.com/VILA-Lab/ATLAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bsharat_S/0/1/0/all/0/1&quot;&gt;Sondos Mahmoud Bsharat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myrzakhan_A/0/1/0/all/0/1&quot;&gt;Aidar Myrzakhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17484">
<title>Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17484</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the great success of large language models (LLMs) in various tasks,
they suffer from generating hallucinations. We introduce Truth Forest, a method
that enhances truthfulness in LLMs by uncovering hidden truth representations
using multi-dimensional orthogonal probes. Specifically, it creates multiple
orthogonal bases for modeling truth by incorporating orthogonal constraints
into the probes. Moreover, we introduce Random Peek, a systematic technique
considering an extended range of positions within the sequence, reducing the
gap between discerning and generating truth features in LLMs. By employing this
approach, we improved the truthfulness of Llama-2-7B from 40.8\% to 74.5\% on
TruthfulQA. Likewise, significant improvements are observed in fine-tuned
models. We conducted a thorough analysis of truth features using probes. Our
visualization results show that orthogonal probes capture complementary
truth-related features, forming well-defined clusters that reveal the inherent
structure of the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhongzhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xingwu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1&quot;&gt;Xianfeng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_F/0/1/0/all/0/1&quot;&gt;Fengzong Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1&quot;&gt;Zhanhui Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cheng-Zhong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05566">
<title>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05566</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1&quot;&gt;Evan Hubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1&quot;&gt;Carson Denison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1&quot;&gt;Jesse Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1&quot;&gt;Mike Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Meg Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1&quot;&gt;Monte MacDiarmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1&quot;&gt;Tamera Lanham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1&quot;&gt;Daniel M. Ziegler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1&quot;&gt;Tim Maxwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Newton Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1&quot;&gt;Adam Jermyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1&quot;&gt;Amanda Askell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1&quot;&gt;Ansh Radhakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1&quot;&gt;Cem Anil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1&quot;&gt;Deep Ganguli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1&quot;&gt;Jack Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1&quot;&gt;Kamal Ndousse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1&quot;&gt;Kshitij Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1&quot;&gt;Michael Sellitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Mrinank Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1&quot;&gt;Nova DasSarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1&quot;&gt;Shauna Kravec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuntao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1&quot;&gt;Zachary Witten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1&quot;&gt;Marina Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1&quot;&gt;Jan Brauner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1&quot;&gt;Holden Karnofsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1&quot;&gt;Paul Christiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1&quot;&gt;Logan Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1&quot;&gt;Jared Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1&quot;&gt;Ryan Greenblatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1&quot;&gt;Buck Shlegeris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1&quot;&gt;Nicholas Schiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06805">
<title>Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning. (arXiv:2401.06805v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06805</link>
<description rdf:parseType="Literal">&lt;p&gt;Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence
(AGI) with abstract reasoning ability is the goal of next-generation AI. Recent
advancements in Large Language Models (LLMs), along with the emerging field of
Multimodal Large Language Models (MLLMs), have demonstrated impressive
capabilities across a wide range of multimodal tasks and applications.
Particularly, various MLLMs, each with distinct model architectures, training
data, and training stages, have been evaluated across a broad range of MLLM
benchmarks. These studies have, to varying degrees, revealed different aspects
of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs
have not been systematically investigated. In this survey, we comprehensively
review the existing evaluation protocols of multimodal reasoning, categorize
and illustrate the frontiers of MLLMs, introduce recent trends in applications
of MLLMs on reasoning-intensive tasks, and finally discuss current practices
and future directions. We believe our survey establishes a solid base and sheds
light on this important topic, multimodal reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wentao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaotian Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xudong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiteng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1&quot;&gt;Bohan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jianbo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1&quot;&gt;Quanzeng You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06951">
<title>E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06951</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically, training LLMs with long context sizes is computationally
expensive, requiring extensive training hours and GPU resources. Existing
long-context extension methods usually need additional training procedures to
support corresponding long-context windows, where the long-context training
data (e.g., 32k) is needed, and high GPU training costs are assumed. To address
the aforementioned issues, we propose an Efficient and Extreme length extension
method for Large Language Models, called E 2 -LLM, with only one training
procedure and dramatically reduced computation cost, which also removes the
need to collect long-context data. Concretely, first, the training data of our
E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost
greatly. Second, the training procedure on the short training context window is
performed only once time, and we can support different evaluation context
windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,
we introduce two different augmentation methods on the scale and position index
parameters for different samples in training. It aims to make the model more
robust to the different relative differences when directly interpolating the
arbitrary context length at inference. Comprehensive experimental results on
multiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on
challenging long-context tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiakai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Que_H/0/1/0/all/0/1&quot;&gt;Haoran Que&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Wenbo Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tiezheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Bo Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07284">
<title>Improving Domain Adaptation through Extended-Text Reading Comprehension. (arXiv:2401.07284v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07284</link>
<description rdf:parseType="Literal">&lt;p&gt;To enhance the domain-specific capabilities of large language models,
continued pre-training on a domain-specific corpus is a prevalent method.
Recent work demonstrates that adapting models using reading comprehension data
formatted by regex-based patterns can significantly improve performance on
domain-specific tasks. However, regex-based patterns are incapable of parsing
raw corpora using domain-specific knowledge. Furthermore, the question and
answer pairs are extracted directly from the corpus in predefined formats
offers limited context. To address this limitation, we improve reading
comprehension via LLM and clustering. LLM focuses on leveraging domain
knowledge within the corpus to refine comprehension stage, while clustering
supplies relevant knowledge by extending the context to enrich reading stage.
Additionally, our method incorporates parameter-efficient fine-tuning to
improve the efficiency of domain adaptation. In comparison to AdaptLLM, our
method achieves an improvement exceeding 5% in domain-specific tasks. Our code
will available at https://github.com/microsoft/LMOps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_T/0/1/0/all/0/1&quot;&gt;Ting Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaohan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shengyue Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zihan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Haizhen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Furu Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_W/0/1/0/all/0/1&quot;&gt;Weiwei Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Feng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Deqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_F/0/1/0/all/0/1&quot;&gt;Fuzhen Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07510">
<title>Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT explores a strategic blueprint of question answering (QA) in
delivering medical diagnosis, treatment recommendations, and other healthcare
support. This is achieved through the increasing incorporation of medical
domain data via natural language processing (NLP) and multimodal paradigms. By
transitioning the distribution of text, images, videos, and other modalities
from the general domain to the medical domain, these techniques have expedited
the progress of medical domain question answering (MDQA). They bridge the gap
between human natural language and sophisticated medical domain knowledge or
expert manual annotations, handling large-scale, diverse, unbalanced, or even
unlabeled data analysis scenarios in medical contexts. Central to our focus is
the utilizing of language models and multimodal paradigms for medical question
answering, aiming to guide the research community in selecting appropriate
mechanisms for their specific medical research requirements. Specialized tasks
such as unimodal-related question answering, reading comprehension, reasoning,
diagnosis, relation extraction, probability modeling, and others, as well as
multimodal-related tasks like vision question answering, image caption,
cross-modal retrieval, report summarization, and generation, are discussed in
detail. Each section delves into the intricate specifics of the respective
method under consideration. This paper highlights the structures and
advancements of medical domain explorations against general domain methods,
emphasizing their applications across different tasks and datasets. It also
outlines current challenges and opportunities for future medical domain
research, paving the way for continued innovation and application in this
rapidly evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07525">
<title>TAROT: A Hierarchical Framework with Multitask Co-Pretraining on Semi-Structured Data towards Effective Person-Job Fit. (arXiv:2401.07525v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07525</link>
<description rdf:parseType="Literal">&lt;p&gt;Person-job fit is an essential part of online recruitment platforms in
serving various downstream applications like Job Search and Candidate
Recommendation. Recently, pretrained large language models have further
enhanced the effectiveness by leveraging richer textual information in user
profiles and job descriptions apart from user behavior features and job
metadata. However, the general domain-oriented design struggles to capture the
unique structural information within user profiles and job descriptions,
leading to a loss of latent semantic correlations. We propose TAROT, a
hierarchical multitask co-pretraining framework, to better utilize structural
and semantic information for informative text embeddings. TAROT targets
semi-structured text in profiles and jobs, and it is co-pretained with
multi-grained pretraining tasks to constrain the acquired semantic information
at each level. Experiments on a real-world LinkedIn dataset show significant
performance improvements, proving its effectiveness in person-job fit tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yihan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Lun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yushu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yanbin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07927">
<title>Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07927</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction-tuned large language models (LLMs) excel at many tasks, and will
even provide explanations for their behavior. Since these models are directly
accessible to the public, there is a risk that convincing and wrong
explanations can lead to unsupported confidence in LLMs. Therefore,
interpretability-faithfulness of self-explanations is an important
consideration for AI Safety. Assessing the interpretability-faithfulness of
these explanations, termed self-explanations, is challenging as the models are
too complex for humans to annotate what is a correct explanation. To address
this, we propose employing self-consistency checks as a measure of
faithfulness. For example, if an LLM says a set of words is important for
making a prediction, then it should not be able to make the same prediction
without these words. While self-consistency checks are a common approach to
faithfulness, they have not previously been applied to LLM&apos;s self-explanations.
We apply self-consistency checks to three types of self-explanations:
counterfactuals, importance measures, and redactions. Our work demonstrate that
faithfulness is both task and model dependent, e.g., for sentiment
classification, counterfactual explanations are more faithful for Llama2,
importance measures for Mistral, and redaction for Falcon 40B. Finally, our
findings are robust to prompt-variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1&quot;&gt;Andreas Madsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1&quot;&gt;Sarath Chandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Siva Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08406">
<title>RAG vs Fine-tuning: Pipelines, Tradeoffs, and a Case Study on Agriculture. (arXiv:2401.08406v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08406</link>
<description rdf:parseType="Literal">&lt;p&gt;There are two common ways in which developers are incorporating proprietary
and domain-specific data when building applications of Large Language Models
(LLMs): Retrieval-Augmented Generation (RAG) and Fine-Tuning. RAG augments the
prompt with the external data, while fine-Tuning incorporates the additional
knowledge into the model itself. However, the pros and cons of both approaches
are not well understood. In this paper, we propose a pipeline for fine-tuning
and RAG, and present the tradeoffs of both for multiple popular LLMs, including
Llama2-13B, GPT-3.5, and GPT-4. Our pipeline consists of multiple stages,
including extracting information from PDFs, generating questions and answers,
using them for fine-tuning, and leveraging GPT-4 for evaluating the results. We
propose metrics to assess the performance of different stages of the RAG and
fine-Tuning pipeline. We conduct an in-depth study on an agricultural dataset.
Agriculture as an industry has not seen much penetration of AI, and we study a
potentially disruptive application - what if we could provide location-specific
insights to a farmer? Our results show the effectiveness of our dataset
generation pipeline in capturing geographic-specific knowledge, and the
quantitative and qualitative benefits of RAG and fine-tuning. We see an
accuracy increase of over 6 p.p. when fine-tuning the model and this is
cumulative with RAG, which increases accuracy by 5 p.p. further. In one
particular experiment, we also demonstrate that the fine-tuned model leverages
information from across geographies to answer specific questions, increasing
answer similarity from 47% to 72%. Overall, the results point to how systems
built using LLMs can be adapted to respond and incorporate knowledge across a
dimension that is critical for a specific industry, paving the way for further
applications of LLMs in other industrial domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balaguer_A/0/1/0/all/0/1&quot;&gt;Angels Balaguer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benara_V/0/1/0/all/0/1&quot;&gt;Vinamra Benara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunha_R/0/1/0/all/0/1&quot;&gt;Renato Luiz de Freitas Cunha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filho_R/0/1/0/all/0/1&quot;&gt;Roberto de M. Estev&amp;#xe3;o Filho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendry_T/0/1/0/all/0/1&quot;&gt;Todd Hendry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holstein_D/0/1/0/all/0/1&quot;&gt;Daniel Holstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marsman_J/0/1/0/all/0/1&quot;&gt;Jennifer Marsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mecklenburg_N/0/1/0/all/0/1&quot;&gt;Nick Mecklenburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malvar_S/0/1/0/all/0/1&quot;&gt;Sara Malvar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nunes_L/0/1/0/all/0/1&quot;&gt;Leonardo O. Nunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padilha_R/0/1/0/all/0/1&quot;&gt;Rafael Padilha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharp_M/0/1/0/all/0/1&quot;&gt;Morris Sharp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Bruno Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Swati Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aski_V/0/1/0/all/0/1&quot;&gt;Vijay Aski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandra_R/0/1/0/all/0/1&quot;&gt;Ranveer Chandra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08417">
<title>Contrastive Preference Optimization: Pushing the Boundaries of LLM Performance in Machine Translation. (arXiv:2401.08417v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08417</link>
<description rdf:parseType="Literal">&lt;p&gt;Moderate-sized large language models (LLMs) -- those with 7B or 13B
parameters -- exhibit promising machine translation (MT) performance. However,
even the top-performing 13B LLM-based translation models, like ALMA, does not
match the performance of state-of-the-art conventional encoder-decoder
translation models or larger-scale LLMs such as GPT-4. In this study, we bridge
this performance gap. We first assess the shortcomings of supervised
fine-tuning for LLMs in the MT task, emphasizing the quality issues present in
the reference data, despite being human-generated. Then, in contrast to SFT
which mimics reference translations, we introduce Contrastive Preference
Optimization (CPO), a novel approach that trains models to avoid generating
adequate but not perfect translations. Applying CPO to ALMA models with only
22K parallel sentences and 12M parameters yields significant improvements. The
resulting model, called ALMA-R, can match or exceed the performance of the WMT
competition winners and GPT-4 on WMT&apos;21, WMT&apos;22 and WMT&apos;23 test datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haoran Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharaf_A/0/1/0/all/0/1&quot;&gt;Amr Sharaf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yunmo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weiting Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lingfeng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durme_B/0/1/0/all/0/1&quot;&gt;Benjamin Van Durme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murray_K/0/1/0/all/0/1&quot;&gt;Kenton Murray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Young Jin Kim&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>