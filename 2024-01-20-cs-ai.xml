<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-18T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09459" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09491" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09498" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09572" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09699" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09757" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09789" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09900" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09983" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09987" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10139" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10148" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10158" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10178" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.12508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.13125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.15629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.13118" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.11854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02472" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01300" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.04640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09048" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17198" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12547" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13269" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03200" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14284" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.05448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19776" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06305" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.14345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.17484" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00132" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02860" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03473" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05163" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05566" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06951" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07510" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09192" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.09424">
<title>Precipitation Prediction Using an Ensemble of Lightweight Learners. (arXiv:2401.09424v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2401.09424</link>
<description rdf:parseType="Literal">&lt;p&gt;Precipitation prediction plays a crucial role in modern agriculture and
industry. However, it poses significant challenges due to the diverse patterns
and dynamics in time and space, as well as the scarcity of high precipitation
events.
&lt;/p&gt;
&lt;p&gt;To address this challenge, we propose an ensemble learning framework that
leverages multiple learners to capture the diverse patterns of precipitation
distribution. Specifically, the framework consists of a precipitation predictor
with multiple lightweight heads (learners) and a controller that combines the
outputs from these heads. The learners and the controller are separately
optimized with a proposed 3-stage training scheme.
&lt;/p&gt;
&lt;p&gt;By utilizing provided satellite images, the proposed approach can effectively
model the intricate rainfall patterns, especially for high precipitation
events. It achieved 1st place on the core test as well as the nowcasting
leaderboards of the Weather4Cast 2023 competition. For detailed implementation,
please refer to our GitHub repository at:
https://github.com/lxz1217/weather4cast-2023-lxz.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xinzhe Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Rui_S/0/1/0/all/0/1&quot;&gt;Sun Rui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Niu_Y/0/1/0/all/0/1&quot;&gt;Yiming Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09432">
<title>RoleCraft-GLM: Advancing Personalized Role-Playing in Large Language Models. (arXiv:2401.09432v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09432</link>
<description rdf:parseType="Literal">&lt;p&gt;This study presents RoleCraft-GLM, an innovative framework aimed at enhancing
personalized role-playing with Large Language Models (LLMs). RoleCraft-GLM
addresses the key issue of lacking personalized interactions in conversational
AI, and offers a solution with detailed and emotionally nuanced character
portrayals. We contribute a unique conversational dataset that shifts from
conventional celebrity-centric characters to diverse, non-celebrity personas,
thus enhancing the realism and complexity of language modeling interactions.
Additionally, our approach includes meticulous character development, ensuring
dialogues are both realistic and emotionally resonant. The effectiveness of
RoleCraft-GLM is validated through various case studies, highlighting its
versatility and skill in different scenarios. Our framework excels in
generating dialogues that accurately reflect characters&apos; personality traits and
emotions, thereby boosting user engagement. In conclusion, RoleCraft-GLM marks
a significant leap in personalized AI interactions, and paves the way for more
authentic and immersive AI-assisted role-playing experiences by enabling more
nuanced and emotionally rich dialogues
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_M/0/1/0/all/0/1&quot;&gt;Meiling Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_X/0/1/0/all/0/1&quot;&gt;Xuechen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_T/0/1/0/all/0/1&quot;&gt;Tianyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yiting Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09435">
<title>Reasoning with random sets: An agenda for the future. (arXiv:2401.09435v1 [math.ST])</title>
<link>http://arxiv.org/abs/2401.09435</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we discuss a potential agenda for future work in the theory of
random sets and belief functions, touching upon a number of focal issues: the
development of a fully-fledged theory of statistical reasoning with random
sets, including the generalisation of logistic regression and of the classical
laws of probability; the further development of the geometric approach to
uncertainty, to include general random sets, a wider range of uncertainty
measures and alternative geometric representations; the application of this new
theory to high-impact areas such as climate change, machine learning and
statistical learning theory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Cuzzolin_F/0/1/0/all/0/1&quot;&gt;Fabio Cuzzolin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09442">
<title>Object Attribute Matters in Visual Question Answering. (arXiv:2401.09442v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09442</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering is a multimodal task that requires the joint
comprehension of visual and textual information. However, integrating visual
and textual semantics solely through attention layers is insufficient to
comprehensively understand and align information from both modalities.
Intuitively, object attributes can naturally serve as a bridge to unify them,
which has been overlooked in previous research. In this paper, we propose a
novel VQA approach from the perspective of utilizing object attribute, aiming
to achieve better object-level visual-language alignment and multimodal scene
understanding. Specifically, we design an attribute fusion module and a
contrastive knowledge distillation module. The attribute fusion module
constructs a multimodal graph neural network to fuse attributes and visual
features through message passing. The enhanced object-level visual features
contribute to solving fine-grained problem like counting-question. The better
object-level visual-language alignment aids in understanding multimodal scenes,
thereby improving the model&apos;s robustness. Furthermore, to augment scene
understanding and the out-of-distribution performance, the contrastive
knowledge distillation module introduces a series of implicit knowledge. We
distill knowledge into attributes through contrastive loss, which further
strengthens the representation learning of attribute features and facilitates
visual-linguistic alignment. Intensive experiments on six datasets, COCO-QA,
VQAv2, VQA-CPv2, VQA-CPv1, VQAvs and TDIUC, show the superiority of the
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peize Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_Q/0/1/0/all/0/1&quot;&gt;Qingyi Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_P/0/1/0/all/0/1&quot;&gt;Peng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09443">
<title>CRD: Collaborative Representation Distance for Practical Anomaly Detection. (arXiv:2401.09443v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09443</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual defect detection plays an important role in intelligent industry.
Patch based methods consider visual images as a collection of image patches
according to positions, which have stronger discriminative ability for small
defects in products, e.g. scratches on pills. However, the nearest neighbor
search for the query image and the stored patches will occupy $O(n)$ complexity
in terms of time and space requirements, posing strict challenges for
deployment in edge environments. In this paper, we propose an alternative
approach to the distance calculation of image patches via collaborative
representation models. Starting from the nearest neighbor distance with $L_0$
constraint, we relax the constraint to $L_2$ constraint and solve the distance
quickly in close-formed without actually accessing the original stored
collection of image patches. Furthermore, we point out that the main
computational burden of this close-formed solution can be pre-computed by
high-performance server before deployment. Consequently, the distance
calculation on edge devices only requires a simple matrix multiplication, which
is extremely lightweight and GPU-friendly. Performance on real industrial
scenarios demonstrates that compared to the existing state-of-the-art methods,
this distance achieves several hundred times improvement in computational
efficiency with slight performance drop, while greatly reducing memory
overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yudong Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09444">
<title>Online Handbook of Argumentation for AI: Volume 4. (arXiv:2401.09444v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09444</link>
<description rdf:parseType="Literal">&lt;p&gt;This volume contains revised versions of the papers selected for the fourth
volume of the Online Handbook of Argumentation for AI (OHAAI). Previously,
formal theories of argument and argument interaction have been proposed and
studied, and this has led to the more recent study of computational models of
argument. Argumentation, as a field within artificial intelligence (AI), is
highly relevant for researchers interested in symbolic representations of
knowledge and defeasible reasoning. The purpose of this handbook is to provide
an open access and curated anthology for the argumentation research community.
OHAAI is designed to serve as a research hub to keep track of the latest and
upcoming PhD-driven research on the theory and application of argumentation in
all areas related to AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengel_L/0/1/0/all/0/1&quot;&gt;Lars Bengel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blumel_L/0/1/0/all/0/1&quot;&gt;Lydia Bl&amp;#xfc;mel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bezou_Vrakatseli_E/0/1/0/all/0/1&quot;&gt;Elfia Bezou-Vrakatseli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castagna_F/0/1/0/all/0/1&quot;&gt;Federico Castagna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DAgostino_G/0/1/0/all/0/1&quot;&gt;Giulia D&amp;#x27;Agostino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhlmann_I/0/1/0/all/0/1&quot;&gt;Isabelle Kuhlmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mumford_J/0/1/0/all/0/1&quot;&gt;Jack Mumford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odekerken_D/0/1/0/all/0/1&quot;&gt;Daphne Odekerken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russo_F/0/1/0/all/0/1&quot;&gt;Fabrizio Russo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkadi_S/0/1/0/all/0/1&quot;&gt;Stefan Sarkadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waller_M/0/1/0/all/0/1&quot;&gt;Madeleine Waller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xydis_A/0/1/0/all/0/1&quot;&gt;Andreas Xydis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09446">
<title>Explainable Multimodal Sentiment Analysis on Bengali Memes. (arXiv:2401.09446v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09446</link>
<description rdf:parseType="Literal">&lt;p&gt;Memes have become a distinctive and effective form of communication in the
digital era, attracting online communities and cutting across cultural
barriers. Even though memes are frequently linked with humor, they have an
amazing capacity to convey a wide range of emotions, including happiness,
sarcasm, frustration, and more. Understanding and interpreting the sentiment
underlying memes has become crucial in the age of information. Previous
research has explored text-based, image-based, and multimodal approaches,
leading to the development of models like CAPSAN and PromptHate for detecting
various meme categories. However, the study of low-resource languages like
Bengali memes remains scarce, with limited availability of publicly accessible
datasets. A recent contribution includes the introduction of the MemoSen
dataset. However, the achieved accuracy is notably low, and the dataset suffers
from imbalanced distribution. In this study, we employed a multimodal approach
using ResNet50 and BanglishBERT and achieved a satisfactory result of 0.71
weighted F1-score, performed comparison with unimodal approaches, and
interpreted behaviors of the models using explainable artificial intelligence
(XAI) techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elahi_K/0/1/0/all/0/1&quot;&gt;Kazi Toufique Elahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_T/0/1/0/all/0/1&quot;&gt;Tasnuva Binte Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahriar_S/0/1/0/all/0/1&quot;&gt;Shakil Shahriar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarker_S/0/1/0/all/0/1&quot;&gt;Samir Sarker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joy_S/0/1/0/all/0/1&quot;&gt;Sajib Kumar Saha Joy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_F/0/1/0/all/0/1&quot;&gt;Faisal Muhammad Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09448">
<title>Tumbug: A pictorial, universal knowledge representation method. (arXiv:2401.09448v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09448</link>
<description rdf:parseType="Literal">&lt;p&gt;Since the key to artificial general intelligence (AGI) is commonly believed
to be commonsense reasoning (CSR) or, roughly equivalently, discovery of a
knowledge representation method (KRM) that is particularly suitable for CSR,
the author developed a custom KRM for CSR. This novel KRM called Tumbug was
designed to be pictorial in nature because there exists increasing evidence
that the human brain uses some pictorial type of KRM, and no well-known prior
research in AGI has researched this KRM possibility. Tumbug is somewhat similar
to Roger Schank&apos;s Conceptual Dependency (CD) theory, but Tumbug is pictorial
and uses about 30 components based on fundamental concepts from the sciences
and human life, in contrast to CD theory, which is textual and uses about 17
components (= 6 Primitive Conceptual Categories + 11 Primitive Acts) based
mainly on human-oriented activities. All the Building Blocks of Tumbug were
found to generalize to only five Basic Building Blocks that exactly correspond
to the three components {O, A, V} of traditional Object-Attribute-Value
representation plus two new components {C, S}, which are Change and System.
Collectively this set of five components, called &quot;SCOVA,&quot; seems to be a
universal foundation for all knowledge representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkins_M/0/1/0/all/0/1&quot;&gt;Mark A. Atkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09450">
<title>Joining Forces for Pathology Diagnostics with AI Assistance: The EMPAIA Initiative. (arXiv:2401.09450v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.09450</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decade, artificial intelligence (AI) methods in pathology have
advanced substantially. However, integration into routine clinical practice has
been slow due to numerous challenges, including technical and regulatory
hurdles in translating research results into clinical diagnostic products and
the lack of standardized interfaces. The open and vendor-neutral EMPAIA
initiative addresses these challenges. Here, we provide an overview of EMPAIA&apos;s
achievements and lessons learned. EMPAIA integrates various stakeholders of the
pathology AI ecosystem, i.e., pathologists, computer scientists, and industry.
In close collaboration, we developed technical interoperability standards,
recommendations for AI testing and product development, and explainability
methods. We implemented the modular and open-source EMPAIA platform and
successfully integrated 11 AI-based image analysis apps from 6 different
vendors, demonstrating how different apps can use a single standardized
interface. We prioritized requirements and evaluated the use of AI in real
clinical settings with 14 different pathology laboratories in Europe and Asia.
In addition to technical developments, we created a forum for all stakeholders
to share information and experiences on digital pathology and AI. Commercial,
clinical, and academic stakeholders can now adopt EMPAIA&apos;s common open-source
interfaces, providing a unique opportunity for large-scale standardization and
streamlining of processes. Further efforts are needed to effectively and
broadly establish AI assistance in routine laboratory use. To this end, a
sustainable infrastructure, the non-profit association EMPAIA International,
has been established to continue standardization and support broad
implementation and advocacy for an AI-assisted digital pathology future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zerbe_N/0/1/0/all/0/1&quot;&gt;Norman Zerbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwen_L/0/1/0/all/0/1&quot;&gt;Lars Ole Schwen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geissler_C/0/1/0/all/0/1&quot;&gt;Christian Gei&amp;#xdf;ler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiesemann_K/0/1/0/all/0/1&quot;&gt;Katja Wiesemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisson_T/0/1/0/all/0/1&quot;&gt;Tom Bisson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boor_P/0/1/0/all/0/1&quot;&gt;Peter Boor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carvalho_R/0/1/0/all/0/1&quot;&gt;Rita Carvalho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franz_M/0/1/0/all/0/1&quot;&gt;Michael Franz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jansen_C/0/1/0/all/0/1&quot;&gt;Christoph Jansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiehl_T/0/1/0/all/0/1&quot;&gt;Tim-Rasmus Kiehl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindequist_B/0/1/0/all/0/1&quot;&gt;Bj&amp;#xf6;rn Lindequist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pohlan_N/0/1/0/all/0/1&quot;&gt;Nora Charlotte Pohlan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmell_S/0/1/0/all/0/1&quot;&gt;Sarah Schmell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strohmenger_K/0/1/0/all/0/1&quot;&gt;Klaus Strohmenger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zakrzewski_F/0/1/0/all/0/1&quot;&gt;Falk Zakrzewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plass_M/0/1/0/all/0/1&quot;&gt;Markus Plass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takla_M/0/1/0/all/0/1&quot;&gt;Michael Takla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuster_T/0/1/0/all/0/1&quot;&gt;Tobias K&amp;#xfc;ster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Homeyer_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Homeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hufnagl_P/0/1/0/all/0/1&quot;&gt;Peter Hufnagl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09451">
<title>Diffusion-Driven Generative Framework for Molecular Conformation Prediction. (arXiv:2401.09451v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.09451</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of inferring three-dimensional molecular configurations from their
two-dimensional graph representations is of critical significance in the
domains of computational chemistry and the development of pharmaceuticals. It
contributes fundamentally to our grasp of molecular mechanisms and
interactions. The rapid evolution of machine learning, especially in the realm
of deep generative networks, has catalyzed breakthroughs in the precision of
such predictive modeling. Traditional methodologies typically employ a
bifurcated strategy: initially estimating interatomic distances followed by
sculpting the spatial molecular structure via solving a distance geometry
problem. This sequential approach, however, occasionally fails to capture the
intricacies of local atomic arrangements accurately, thus compromising the
integrity of the resultant structural models. Addressing these deficiencies,
this work introduces an avant-garde generative framework: \method{}, which is
predicated on the diffusion principles found in classical non-equilibrium
thermodynamics. \method{} envisages atoms as discrete entities and is adept at
guiding the reversal of diffusion morphing a distribution of stochastic noise
back into coherent molecular forms through a process akin to a Markov chain.
This transformation begins with the initial representation of a molecular graph
in an abstract latent space, progressing to the realization of the
three-dimensional forms via an elaborate bilevel optimization scheme, tailored
to respect the task&apos;s specific requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bobin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenghan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09452">
<title>Incorporating Riemannian Geometric Features for Learning Coefficient of Pressure Distributions on Airplane Wings. (arXiv:2401.09452v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09452</link>
<description rdf:parseType="Literal">&lt;p&gt;The aerodynamic coefficients of aircrafts are significantly impacted by its
geometry, especially when the angle of attack (AoA) is large. In the field of
aerodynamics, traditional polynomial-based parameterization uses as few
parameters as possible to describe the geometry of an airfoil. However, because
the 3D geometry of a wing is more complicated than the 2D airfoil,
polynomial-based parameterizations have difficulty in accurately representing
the entire shape of a wing in 3D space. Existing deep learning-based methods
can extract massive latent neural representations for the shape of 2D airfoils
or 2D slices of wings. Recent studies highlight that directly taking geometric
features as inputs to the neural networks can improve the accuracy of predicted
aerodynamic coefficients. Motivated by geometry theory, we propose to
incorporate Riemannian geometric features for learning Coefficient of Pressure
(CP) distributions on wing surfaces. Our method calculates geometric features
(Riemannian metric, connection, and curvature) and further inputs the geometric
features, coordinates and flight conditions into a deep learning model to
predict the CP distribution. Experimental results show that our method,
compared to state-of-the-art Deep Attention Network (DAN), reduces the
predicted mean square error (MSE) of CP by an average of 8.41% for the DLR-F11
aircraft test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Liwei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenyong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Y/0/1/0/all/0/1&quot;&gt;Yu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sommer_S/0/1/0/all/0/1&quot;&gt;Stefan Sommer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09454">
<title>Voila-A: Aligning Vision-Language Models with User&apos;s Gaze Attention. (arXiv:2401.09454v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09454</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the integration of vision and language understanding has led
to significant advancements in artificial intelligence, particularly through
Vision-Language Models (VLMs). However, existing VLMs face challenges in
handling real-world applications with complex scenes and multiple objects, as
well as aligning their focus with the diverse attention patterns of human
users. In this paper, we introduce gaze information, feasibly collected by AR
or VR devices, as a proxy for human attention to guide VLMs and propose a novel
approach, Voila-A, for gaze alignment to enhance the interpretability and
effectiveness of these models in real-world applications. First, we collect
hundreds of minutes of gaze data to demonstrate that we can mimic human gaze
modalities using localized narratives. We then design an automatic data
annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset.
Additionally, we innovate the Voila Perceiver modules to integrate gaze
information into VLMs while preserving their pretrained knowledge. We evaluate
Voila-A using a hold-out validation set and a newly collected VOILA-GAZE
Testset, which features real-life scenarios captured with a gaze-tracking
device. Our experimental results demonstrate that Voila-A significantly
outperforms several baseline models. By aligning model attention with human
gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and
fosters engaging human-AI interaction across a wide range of applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_L/0/1/0/all/0/1&quot;&gt;Lei Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuntao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_N/0/1/0/all/0/1&quot;&gt;Nan Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shuai Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09455">
<title>Dynamic Routing for Integrated Satellite-Terrestrial Networks: A Constrained Multi-Agent Reinforcement Learning Approach. (arXiv:2401.09455v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.09455</link>
<description rdf:parseType="Literal">&lt;p&gt;The integrated satellite-terrestrial network (ISTN) system has experienced
significant growth, offering seamless communication services in remote areas
with limited terrestrial infrastructure. However, designing a routing scheme
for ISTN is exceedingly difficult, primarily due to the heightened complexity
resulting from the inclusion of additional ground stations, along with the
requirement to satisfy various constraints related to satellite service
quality. To address these challenges, we study packet routing with ground
stations and satellites working jointly to transmit packets, while prioritizing
fast communication and meeting energy efficiency and packet loss requirements.
Specifically, we formulate the problem of packet routing with constraints as a
max-min problem using the Lagrange method. Then we propose a novel constrained
Multi-Agent reinforcement learning (MARL) dynamic routing algorithm named
CMADR, which efficiently balances objective improvement and constraint
satisfaction during the updating of policy and Lagrange multipliers. Finally,
we conduct extensive experiments and an ablation study using the OneWeb and
Telesat mega-constellations. Results demonstrate that CMADR reduces the packet
delay by a minimum of 21% and 15%, while meeting stringent energy consumption
and packet loss rate constraints, outperforming several baseline algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yifeng Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Han Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_R/0/1/0/all/0/1&quot;&gt;Rongfei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_J/0/1/0/all/0/1&quot;&gt;Jianping An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shiwen Mao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09459">
<title>What&apos;s my role? Modelling responsibility for AI-based safety-critical systems. (arXiv:2401.09459v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.09459</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-Based Safety-Critical Systems (AI-SCS) are being increasingly deployed in
the real world. These can pose a risk of harm to people and the environment.
Reducing that risk is an overarching priority during development and operation.
As more AI-SCS become autonomous, a layer of risk management via human
intervention has been removed. Following an accident it will be important to
identify causal contributions and the different responsible actors behind those
to learn from mistakes and prevent similar future events. Many authors have
commented on the &quot;responsibility gap&quot; where it is difficult for developers and
manufacturers to be held responsible for harmful behaviour of an AI-SCS. This
is due to the complex development cycle for AI, uncertainty in AI performance,
and dynamic operating environment. A human operator can become a &quot;liability
sink&quot; absorbing blame for the consequences of AI-SCS outputs they weren&apos;t
responsible for creating, and may not have understanding of.
&lt;/p&gt;
&lt;p&gt;This cross-disciplinary paper considers different senses of responsibility
(role, moral, legal and causal), and how they apply in the context of AI-SCS
safety. We use a core concept (Actor(A) is responsible for Occurrence(O)) to
create role responsibility models, producing a practical method to capture
responsibility relationships and provide clarity on the previously identified
responsibility issues. Our paper demonstrates the approach with two examples: a
retrospective analysis of the Tempe Arizona fatal collision involving an
autonomous vehicle, and a safety focused predictive role-responsibility
analysis for an AI-based diabetes co-morbidity predictor. In both examples our
primary focus is on safety, aiming to reduce unfair or disproportionate blame
being placed on operators or developers. We present a discussion and avenues
for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryan_P/0/1/0/all/0/1&quot;&gt;Philippa Ryan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porter_Z/0/1/0/all/0/1&quot;&gt;Zoe Porter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Qaddoumi_J/0/1/0/all/0/1&quot;&gt;Joanna Al-Qaddoumi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDermid_J/0/1/0/all/0/1&quot;&gt;John McDermid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habli_I/0/1/0/all/0/1&quot;&gt;Ibrahim Habli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09466">
<title>Self Supervised Vision for Climate Downscaling. (arXiv:2401.09466v1 [physics.ao-ph])</title>
<link>http://arxiv.org/abs/2401.09466</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change is one of the most critical challenges that our planet is
facing today. Rising global temperatures are already bringing noticeable
changes to Earth&apos;s weather and climate patterns with an increased frequency of
unpredictable and extreme weather events. Future projections for climate change
research are based on Earth System Models (ESMs), the computer models that
simulate the Earth&apos;s climate system. ESMs provide a framework to integrate
various physical systems, but their output is bound by the enormous
computational resources required for running and archiving higher-resolution
simulations. For a given resource budget, the ESMs are generally run on a
coarser grid, followed by a computationally lighter $downscaling$ process to
obtain a finer-resolution output. In this work, we present a deep-learning
model for downscaling ESM simulation data that does not require high-resolution
ground truth data for model optimization. This is realized by leveraging
salient data distribution patterns and the hidden dependencies between weather
variables for an $\textit{individual}$ data point at $\textit{runtime}$.
Extensive evaluation with $2$x, $3$x, and $4$x scaling factors demonstrates
that the proposed model consistently obtains superior performance over that of
various baselines. The improved downscaling performance and no dependence on
high-resolution ground truth data make the proposed method a valuable tool for
climate research and mark it as a promising direction for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Karandeep Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Jeong_C/0/1/0/all/0/1&quot;&gt;Chaeyoon Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Shidqi_N/0/1/0/all/0/1&quot;&gt;Naufal Shidqi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Sungwon Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Nellikkattil_A/0/1/0/all/0/1&quot;&gt;Arjun Nellikkattil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zeller_E/0/1/0/all/0/1&quot;&gt;Elke Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Cha_M/0/1/0/all/0/1&quot;&gt;Meeyoung Cha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09467">
<title>Offline Handwriting Signature Verification: A Transfer Learning and Feature Selection Approach. (arXiv:2401.09467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09467</link>
<description rdf:parseType="Literal">&lt;p&gt;Handwritten signature verification poses a formidable challenge in biometrics
and document authenticity. The objective is to ascertain the authenticity of a
provided handwritten signature, distinguishing between genuine and forged ones.
This issue has many applications in sectors such as finance, legal
documentation, and security. Currently, the field of computer vision and
machine learning has made significant progress in the domain of handwritten
signature verification. The outcomes, however, may be enhanced depending on the
acquired findings, the structure of the datasets, and the used models. Four
stages make up our suggested strategy. First, we collected a large dataset of
12600 images from 420 distinct individuals, and each individual has 30
signatures of a certain kind (All authors signatures are genuine). In the
subsequent stage, the best features from each image were extracted using a deep
learning model named MobileNetV2. During the feature selection step, three
selectors neighborhood component analysis (NCA), Chi2, and mutual info (MI)
were used to pull out 200, 300, 400, and 500 features, giving a total of 12
feature vectors. Finally, 12 results have been obtained by applying machine
learning techniques such as SVM with kernels (rbf, poly, and linear), KNN, DT,
Linear Discriminant Analysis, and Naive Bayes. Without employing feature
selection techniques, our suggested offline signature verification achieved a
classification accuracy of 91.3%, whereas using the NCA feature selection
approach with just 300 features it achieved a classification accuracy of 97.7%.
High classification accuracy was achieved using the designed and suggested
model, which also has the benefit of being a self-organized framework.
Consequently, using the optimum minimally chosen features, the proposed method
could identify the best model performance and result validation prediction
vectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozyurt_F/0/1/0/all/0/1&quot;&gt;Fatih Ozyurt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majidpour_J/0/1/0/all/0/1&quot;&gt;Jafar Majidpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashid_T/0/1/0/all/0/1&quot;&gt;Tarik A. Rashid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koc_C/0/1/0/all/0/1&quot;&gt;Canan Koc&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09473">
<title>Business and ethical concerns in domestic Conversational Generative AI-empowered multi-robot systems. (arXiv:2401.09473v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.09473</link>
<description rdf:parseType="Literal">&lt;p&gt;Business and technology are intricately connected through logic and design.
They are equally sensitive to societal changes and may be devastated by
scandal. Cooperative multi-robot systems (MRSs) are on the rise, allowing
robots of different types and brands to work together in diverse contexts.
Generative artificial intelligence has been a dominant topic in recent
artificial intelligence (AI) discussions due to its capacity to mimic humans
through the use of natural language and the production of media, including deep
fakes. In this article, we focus specifically on the conversational aspects of
generative AI, and hence use the term Conversational Generative artificial
intelligence (CGI). Like MRSs, CGIs have enormous potential for revolutionizing
processes across sectors and transforming the way humans conduct business. From
a business perspective, cooperative MRSs alone, with potential conflicts of
interest, privacy practices, and safety concerns, require ethical examination.
MRSs empowered by CGIs demand multi-dimensional and sophisticated methods to
uncover imminent ethical pitfalls. This study focuses on ethics in
CGI-empowered MRSs while reporting the stages of developing the MORUL model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousi_R/0/1/0/all/0/1&quot;&gt;Rebekah Rousi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samani_H/0/1/0/all/0/1&quot;&gt;Hooman Samani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makitalo_N/0/1/0/all/0/1&quot;&gt;Niko M&amp;#xe4;kitalo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakkuri_V/0/1/0/all/0/1&quot;&gt;Ville Vakkuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Linkola_S/0/1/0/all/0/1&quot;&gt;Simo Linkola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemell_K/0/1/0/all/0/1&quot;&gt;Kai-Kristian Kemell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daubaris_P/0/1/0/all/0/1&quot;&gt;Paulius Daubaris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fronza_I/0/1/0/all/0/1&quot;&gt;Ilenia Fronza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mikkonen_T/0/1/0/all/0/1&quot;&gt;Tommi Mikkonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrahamsson_P/0/1/0/all/0/1&quot;&gt;Pekka Abrahamsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09476">
<title>A Framework for Agricultural Food Supply Chain using Blockchain. (arXiv:2401.09476v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.09476</link>
<description rdf:parseType="Literal">&lt;p&gt;The main aim of the paper is to create a trust and transparency in the food
supply chain system, ensuring food safety for everyone with the help of
Blockchain Technology. Food supply chain is the process of tracing a crop from
the farmer or producer to the buyer. With the advent of blockchain, providing a
safe and fraud-free environment for the provision of numerous agricultural
necessities has become much easier. Because of the globalization of trade, the
present supply chain market today includes various companies involving
integration of data, complex transactions and distribution. Information tamper
resistance, supply-demand relationships, and traceable oversight are all
difficulties that arise as a result of this. Blockchain is a distributed ledger
technology that can provide information that is resistant to tampering. This
strategy can eliminate the need for a centralized trusted authority,
intermediaries, and business histories, allowing for increased production and
security while maintaining the highest levels of integrity, liability, and
safety. In order to have an integrity and transparency in food supply chain in
the agricultural sector, a framework is proposed here based on block chain and
IoT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+N_S/0/1/0/all/0/1&quot;&gt;Sudarssan N&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09479">
<title>Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.09479</link>
<description rdf:parseType="Literal">&lt;p&gt;The risk of hardware Trojans being inserted at various stages of chip
production has increased in a zero-trust fabless era. To counter this, various
machine learning solutions have been developed for the detection of hardware
Trojans. While most of the focus has been on either a statistical or deep
learning approach, the limited number of Trojan-infected benchmarks affects the
detection accuracy and restricts the possibility of detecting zero-day Trojans.
To close the gap, we first employ generative adversarial networks to amplify
our data in two alternative representation modalities, a graph and a tabular,
ensuring that the dataset is distributed in a representative manner. Further,
we propose a multimodal deep learning approach to detect hardware Trojans and
evaluate the results from both early fusion and late fusion strategies. We also
estimate the uncertainty quantification metrics of each prediction for
risk-aware decision-making. The outcomes not only confirms the efficacy of our
proposed hardware Trojan detection method but also opens a new door for future
studies employing multimodality and uncertainty quantification to address other
hardware security challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwakarma_R/0/1/0/all/0/1&quot;&gt;Rahul Vishwakarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Amin Rezaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09489">
<title>PUPAE: Intuitive and Actionable Explanations for Time Series Anomalies. (arXiv:2401.09489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09489</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years there has been significant progress in time series anomaly
detection. However, after detecting an (perhaps tentative) anomaly, can we
explain it? Such explanations would be useful to triage anomalies. For example,
in an oil refinery, should we respond to an anomaly by dispatching a hydraulic
engineer, or an intern to replace the battery on a sensor? There have been some
parallel efforts to explain anomalies, however many proposed techniques produce
explanations that are indirect, and often seem more complex than the anomaly
they seek to explain. Our review of the literature/checklists/user-manuals used
by frontline practitioners in various domains reveals an interesting
near-universal commonality. Most practitioners discuss, explain and report
anomalies in the following format: The anomaly would be like normal data A, if
not for the corruption B. The reader will appreciate that is a type of
counterfactual explanation. In this work we introduce a domain agnostic
counterfactual explanation technique to produce explanations for time series
anomalies. As we will show, our method can produce both visual and text-based
explanations that are objectively correct, intuitive and in many circumstances,
directly actionable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Der_A/0/1/0/all/0/1&quot;&gt;Audrey Der&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keogh_E/0/1/0/all/0/1&quot;&gt;Eamonn J. Keogh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09491">
<title>Memory, Space, and Planning: Multiscale Predictive Representations. (arXiv:2401.09491v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09491</link>
<description rdf:parseType="Literal">&lt;p&gt;Memory is inherently entangled with prediction and planning. Flexible
behavior in biological and artificial agents depends on the interplay of
learning from the past and predicting the future in ever-changing environments.
This chapter reviews computational, behavioral, and neural evidence suggesting
these processes rely on learning the relational structure of experiences, known
as cognitive maps, and draws two key takeaways. First, that these memory
structures are organized as multiscale, compact predictive representations in
hippocampal and prefrontal cortex, or PFC, hierarchies. Second, we argue that
such predictive memory structures are crucial to the complementary functions of
the hippocampus and PFC, both for enabling a recall of detailed and coherent
past episodes as well as generalizing experiences at varying scales for
efficient prediction and planning. These insights advance our understanding of
memory and planning mechanisms in the brain and hold significant implications
for advancing artificial intelligence systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Momennejad_I/0/1/0/all/0/1&quot;&gt;Ida Momennejad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09498">
<title>Technical Report: On the Convergence of Gossip Learning in the Presence of Node Inaccessibility. (arXiv:2401.09498v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09498</link>
<description rdf:parseType="Literal">&lt;p&gt;Gossip learning (GL), as a decentralized alternative to federated learning
(FL), is more suitable for resource-constrained wireless networks, such as
FANETs that are formed by unmanned aerial vehicles (UAVs). GL can significantly
enhance the efficiency and extend the battery life of UAV networks. Despite the
advantages, the performance of GL is strongly affected by data distribution,
communication speed, and network connectivity. However, how these factors
influence the GL convergence is still unclear. Existing work studied the
convergence of GL based on a virtual quantity for the sake of convenience,
which fail to reflect the real state of the network when some nodes are
inaccessible. In this paper, we formulate and investigate the impact of
inaccessible nodes to GL under a dynamic network topology. We first decompose
the weight divergence by whether the node is accessible or not. Then, we
investigate the GL convergence under the dynamic of node accessibility and
theoretically provide how the number of inaccessible nodes, data
non-i.i.d.-ness, and duration of inaccessibility affect the convergence.
Extensive experiments are carried out in practical settings to comprehensively
verify the correctness of our theoretical findings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yue Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xueyang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yecheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09516">
<title>Accelerating Data Generation for Neural Operators via Krylov Subspace Recycling. (arXiv:2401.09516v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09516</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning neural operators for solving partial differential equations (PDEs)
has attracted great attention due to its high inference efficiency. However,
training such operators requires generating a substantial amount of labeled
data, i.e., PDE problems together with their solutions. The data generation
process is exceptionally time-consuming, as it involves solving numerous
systems of linear equations to obtain numerical solutions to the PDEs. Many
existing methods solve these systems independently without considering their
inherent similarities, resulting in extremely redundant computations. To tackle
this problem, we propose a novel method, namely Sorting Krylov Recycling (SKR),
to boost the efficiency of solving these systems, thus significantly
accelerating data generation for neural operators training. To the best of our
knowledge, SKR is the first attempt to address the time-consuming nature of
data generation for learning neural operators. The working horse of SKR is
Krylov subspace recycling, a powerful technique for solving a series of
interrelated systems by leveraging their inherent similarities. Specifically,
SKR employs a sorting algorithm to arrange these systems in a sequence, where
adjacent systems exhibit high similarities. Then it equips a solver with Krylov
subspace recycling to solve the systems sequentially instead of independently,
thus effectively enhancing the solving efficiency. Both theoretical analysis
and extensive experiments demonstrate that SKR can significantly accelerate
neural operator data generation, achieving a remarkable speedup of up to 13.9
times.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1&quot;&gt;Zhongkai Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_Z/0/1/0/all/0/1&quot;&gt;Zijie Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09553">
<title>BERTologyNavigator: Advanced Question Answering with BERT-based Semantics. (arXiv:2401.09553v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09553</link>
<description rdf:parseType="Literal">&lt;p&gt;The development and integration of knowledge graphs and language models has
significance in artificial intelligence and natural language processing. In
this study, we introduce the BERTologyNavigator -- a two-phased system that
combines relation extraction techniques and BERT embeddings to navigate the
relationships within the DBLP Knowledge Graph (KG). Our approach focuses on
extracting one-hop relations and labelled candidate pairs in the first phases.
This is followed by employing BERT&apos;s CLS embeddings and additional heuristics
for relation selection in the second phase. Our system reaches an F1 score of
0.2175 on the DBLP QuAD Final test dataset for Scholarly QALD and 0.98 F1 score
on the subset of the DBLP QuAD test dataset during the QA phase.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajpal_S/0/1/0/all/0/1&quot;&gt;Shreya Rajpal&lt;/a&gt; (1,2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Usbeck_R/0/1/0/all/0/1&quot;&gt;Ricardo Usbeck&lt;/a&gt; (1) ((1) Universit&amp;#xe4;t Hamburg, Hamburg, Germany,(2) Vellore Institute of Technology, Vellore, Tamil Nadu, India)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09555">
<title>Improving Classification Performance With Human Feedback: Label a few, we label the rest. (arXiv:2401.09555v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09555</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of artificial intelligence, where a vast majority of data is
unstructured, obtaining substantial amounts of labeled data to train supervised
machine learning models poses a significant challenge. To address this, we
delve into few-shot and active learning, where are goal is to improve AI models
with human feedback on a few labeled examples. This paper focuses on
understanding how a continuous feedback loop can refine models, thereby
enhancing their accuracy, recall, and precision through incremental human
input. By employing Large Language Models (LLMs) such as GPT-3.5, BERT, and
SetFit, we aim to analyze the efficacy of using a limited number of labeled
examples to substantially improve model accuracy. We benchmark this approach on
the Financial Phrasebank, Banking, Craigslist, Trec, Amazon Reviews datasets to
prove that with just a few labeled examples, we are able to surpass the
accuracy of zero shot large language models to provide enhanced text
classification performance. We demonstrate that rather than needing to manually
label millions of rows of data, we just need to label a few and the model can
effectively predict the rest.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vidra_N/0/1/0/all/0/1&quot;&gt;Natan Vidra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clifford_T/0/1/0/all/0/1&quot;&gt;Thomas Clifford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jijo_K/0/1/0/all/0/1&quot;&gt;Katherine Jijo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_E/0/1/0/all/0/1&quot;&gt;Eden Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09556">
<title>Deep learning enhanced mixed integer optimization: Learning to reduce model dimensionality. (arXiv:2401.09556v1 [math.OC])</title>
<link>http://arxiv.org/abs/2401.09556</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces a framework to address the computational complexity
inherent in Mixed-Integer Programming (MIP) models by harnessing the potential
of deep learning. We compare the effectiveness of (a) feed-forward neural
networks (ANN) and (b) convolutional neural networks (CNN) in approximating the
active dimensions within MIP problems. We utilize multi-label classification to
account for more than one active dimension. To enhance the framework&apos;s
performance, we employ Bayesian optimization for hyperparameter tuning, aiming
to maximize sample-level accuracy. The primary objective is to train the neural
networks to predict all active dimensions accurately, thereby maximizing the
occurrence of global optimum solutions. We apply this framework to a flow-based
facility location allocation Mixed-Integer Linear Programming (MILP)
formulation that describes long-term investment planning and medium-term
tactical planning in a personalized medicine supply chain for cell therapy
manufacturing and distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Triantafyllou_N/0/1/0/all/0/1&quot;&gt;Niki Triantafyllou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Papathanasiou_M/0/1/0/all/0/1&quot;&gt;Maria M. Papathanasiou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09566">
<title>Aligning Large Language Models with Counterfactual DPO. (arXiv:2401.09566v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09566</link>
<description rdf:parseType="Literal">&lt;p&gt;Advancements in large language models (LLMs) have demonstrated remarkable
capabilities across a diverse range of applications. These models excel in
generating text completions that are contextually coherent and cover an
extensive array of subjects. However, the vast datasets required for their
training make aligning response styles during the pretraining and instruction
tuning phases challenging. Consequently, an additional alignment phase is
typically employed, wherein the model is further trained with human preference
data to better align its outputs with human expectations. While this process
doesn&apos;t introduce new capabilities per se, it does accentuate generation styles
innate to the model. This paper explores the utilization of counterfactual
prompting within the framework of Direct Preference Optimization (DPO) to align
the model&apos;s style without relying on human intervention. We demonstrate that
this method effectively instils desirable behaviour, mitigates undesirable
ones, and encourages the model to disregard inappropriate instructions. Our
findings suggest that counterfactual prompting with DPO presents a low-resource
way to fine-tune LLMs to meet the demands for responsible and ethically aligned
AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Butcher_B/0/1/0/all/0/1&quot;&gt;Bradley Butcher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09572">
<title>Handling Large-scale Cardinality in building recommendation systems. (arXiv:2401.09572v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2401.09572</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective recommendation systems rely on capturing user preferences, often
requiring incorporating numerous features such as universally unique
identifiers (UUIDs) of entities. However, the exceptionally high cardinality of
UUIDs poses a significant challenge in terms of model degradation and increased
model size due to sparsity. This paper presents two innovative techniques to
address the challenge of high cardinality in recommendation systems.
Specifically, we propose a bag-of-words approach, combined with layer sharing,
to substantially decrease the model size while improving performance. Our
techniques were evaluated through offline and online experiments on Uber use
cases, resulting in promising results demonstrating our approach&apos;s
effectiveness in optimizing recommendation systems and enhancing their overall
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kurra_D/0/1/0/all/0/1&quot;&gt;Dhruva Dixith Kurra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_B/0/1/0/all/0/1&quot;&gt;Bo Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zh_C/0/1/0/all/0/1&quot;&gt;Chun Zh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashrafzadeh_S/0/1/0/all/0/1&quot;&gt;Seyedshahin Ashrafzadeh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09615">
<title>Learning Shortcuts: On the Misleading Promise of NLU in Language Models. (arXiv:2401.09615v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09615</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) has enabled significant
performance gains in the field of natural language processing. However, recent
studies have found that LLMs often resort to shortcuts when performing tasks,
creating an illusion of enhanced performance while lacking generalizability in
their decision rules. This phenomenon introduces challenges in accurately
assessing natural language understanding in LLMs. Our paper provides a concise
survey of relevant research in this area and puts forth a perspective on the
implications of shortcut learning in the evaluation of language models,
specifically for NLU tasks. This paper urges more research efforts to be put
towards deepening our comprehension of shortcut learning, contributing to the
development of more robust language models, and raising the standards of NLU
evaluation in real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihani_G/0/1/0/all/0/1&quot;&gt;Geetanjali Bihani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rayz_J/0/1/0/all/0/1&quot;&gt;Julia Taylor Rayz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09637">
<title>Impact of Large Language Model Assistance on Patients Reading Clinical Notes: A Mixed-Methods Study. (arXiv:2401.09637v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.09637</link>
<description rdf:parseType="Literal">&lt;p&gt;Patients derive numerous benefits from reading their clinical notes,
including an increased sense of control over their health and improved
understanding of their care plan. However, complex medical concepts and jargon
within clinical notes hinder patient comprehension and may lead to anxiety. We
developed a patient-facing tool to make clinical notes more readable,
leveraging large language models (LLMs) to simplify, extract information from,
and add context to notes. We prompt engineered GPT-4 to perform these
augmentation tasks on real clinical notes donated by breast cancer survivors
and synthetic notes generated by a clinician, a total of 12 notes with 3868
words. In June 2023, 200 female-identifying US-based participants were randomly
assigned three clinical notes with varying levels of augmentations using our
tool. Participants answered questions about each note, evaluating their
understanding of follow-up actions and self-reported confidence. We found that
augmentations were associated with a significant increase in action
understanding score (0.63 $\pm$ 0.04 for select augmentations, compared to 0.54
$\pm$ 0.02 for the control) with p=0.002. In-depth interviews of
self-identifying breast cancer patients (N=7) were also conducted via video
conferencing. Augmentations, especially definitions, elicited positive
responses among the seven participants, with some concerns about relying on
LLMs. Augmentations were evaluated for errors by clinicians, and we found
misleading errors occur, with errors more common in real donated notes than
synthetic notes, illustrating the importance of carefully written clinical
notes. Augmentations improve some but not all readability metrics. This work
demonstrates the potential of LLMs to improve patients&apos; experience with
clinical notes at a lower burden to clinicians. However, having a human in the
loop is important to correct potential model errors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannhardt_N/0/1/0/all/0/1&quot;&gt;Niklas Mannhardt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondi_Kelly_E/0/1/0/all/0/1&quot;&gt;Elizabeth Bondi-Kelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_B/0/1/0/all/0/1&quot;&gt;Barbara Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+OConnell_C/0/1/0/all/0/1&quot;&gt;Chloe O&amp;#x27;Connell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asiedu_M/0/1/0/all/0/1&quot;&gt;Mercy Asiedu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozannar_H/0/1/0/all/0/1&quot;&gt;Hussein Mozannar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agrawal_M/0/1/0/all/0/1&quot;&gt;Monica Agrawal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buendia_A/0/1/0/all/0/1&quot;&gt;Alejandro Buendia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Urman_T/0/1/0/all/0/1&quot;&gt;Tatiana Urman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riaz_I/0/1/0/all/0/1&quot;&gt;Irbaz B. Riaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricciardi_C/0/1/0/all/0/1&quot;&gt;Catherine E. Ricciardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghassemi_M/0/1/0/all/0/1&quot;&gt;Marzyeh Ghassemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontag_D/0/1/0/all/0/1&quot;&gt;David Sontag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09640">
<title>Blackout Mitigation via Physics-guided RL. (arXiv:2401.09640v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2401.09640</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper considers the sequential design of remedial control actions in
response to system anomalies for the ultimate objective of preventing
blackouts. A physics-guided reinforcement learning (RL) framework is designed
to identify effective sequences of real-time remedial look-ahead decisions
accounting for the long-term impact on the system&apos;s stability. The paper
considers a space of control actions that involve both discrete-valued
transmission line-switching decisions (line reconnections and removals) and
continuous-valued generator adjustments. To identify an effective blackout
mitigation policy, a physics-guided approach is designed that uses power-flow
sensitivity factors associated with the power transmission network to guide the
RL exploration during agent training. Comprehensive empirical evaluations using
the open-source Grid2Op platform demonstrate the notable advantages of
incorporating physical signals into RL decisions, establishing the gains of the
proposed physics-guided approach compared to its black box counterparts. One
important observation is that strategically~\emph{removing} transmission lines,
in conjunction with multiple real-time generator adjustments, often renders
effective long-term decisions that are likely to prevent or delay blackouts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dwivedi_A/0/1/0/all/0/1&quot;&gt;Anmol Dwivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Paternain_S/0/1/0/all/0/1&quot;&gt;Santiago Paternain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tajer_A/0/1/0/all/0/1&quot;&gt;Ali Tajer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09646">
<title>ClimateGPT: Towards AI Synthesizing Interdisciplinary Research on Climate Change. (arXiv:2401.09646v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09646</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces ClimateGPT, a model family of domain-specific large
language models that synthesize interdisciplinary research on climate change.
We trained two 7B models from scratch on a science-oriented dataset of 300B
tokens. For the first model, the 4.2B domain-specific tokens were included
during pre-training and the second was adapted to the climate domain after
pre-training. Additionally, ClimateGPT-7B, 13B and 70B are continuously
pre-trained from Llama~2 on a domain-specific dataset of 4.2B tokens. Each
model is instruction fine-tuned on a high-quality and human-generated
domain-specific dataset that has been created in close cooperation with climate
scientists. To reduce the number of hallucinations, we optimize the model for
retrieval augmentation and propose a hierarchical retrieval strategy. To
increase the accessibility of our model to non-English speakers, we propose to
make use of cascaded machine translation and show that this approach can
perform comparably to natively multilingual models while being easier to scale
to a large number of languages. Further, to address the intrinsic
interdisciplinary aspect of climate change we consider different research
perspectives. Therefore, the model can produce in-depth answers focusing on
different perspectives in addition to an overall answer. We propose a suite of
automatic climate-specific benchmarks to evaluate LLMs. On these benchmarks,
ClimateGPT-7B performs on par with the ten times larger Llama-2-70B Chat model
while not degrading results on general domain benchmarks. Our human evaluation
confirms the trends we saw in our benchmarks. All models were trained and
evaluated using renewable energy and are released publicly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thulke_D/0/1/0/all/0/1&quot;&gt;David Thulke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yingbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pelser_P/0/1/0/all/0/1&quot;&gt;Petrus Pelser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brune_R/0/1/0/all/0/1&quot;&gt;Rein Brune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalota_R/0/1/0/all/0/1&quot;&gt;Rricha Jalota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fok_F/0/1/0/all/0/1&quot;&gt;Floris Fok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_M/0/1/0/all/0/1&quot;&gt;Michael Ramos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wyk_I/0/1/0/all/0/1&quot;&gt;Ian van Wyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasir_A/0/1/0/all/0/1&quot;&gt;Abdallah Nasir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_H/0/1/0/all/0/1&quot;&gt;Hayden Goldstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tragemann_T/0/1/0/all/0/1&quot;&gt;Taylor Tragemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Katie Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fowler_A/0/1/0/all/0/1&quot;&gt;Ariana Fowler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanco_A/0/1/0/all/0/1&quot;&gt;Andrew Stanco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabriel_J/0/1/0/all/0/1&quot;&gt;Jon Gabriel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_J/0/1/0/all/0/1&quot;&gt;Jordan Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moro_D/0/1/0/all/0/1&quot;&gt;Dean Moro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsymbalov_E/0/1/0/all/0/1&quot;&gt;Evgenii Tsymbalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waal_J/0/1/0/all/0/1&quot;&gt;Juliette de Waal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matusov_E/0/1/0/all/0/1&quot;&gt;Evgeny Matusov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaghi_M/0/1/0/all/0/1&quot;&gt;Mudar Yaghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shihadah_M/0/1/0/all/0/1&quot;&gt;Mohammad Shihadah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ney_H/0/1/0/all/0/1&quot;&gt;Hermann Ney&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dugast_C/0/1/0/all/0/1&quot;&gt;Christian Dugast&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dotan_J/0/1/0/all/0/1&quot;&gt;Jonathan Dotan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erasmus_D/0/1/0/all/0/1&quot;&gt;Daniel Erasmus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09651">
<title>Convex and Bilevel Optimization for Neuro-Symbolic Inference and Learning. (arXiv:2401.09651v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09651</link>
<description rdf:parseType="Literal">&lt;p&gt;We address a key challenge for neuro-symbolic (NeSy) systems by leveraging
convex and bilevel optimization techniques to develop a general gradient-based
framework for end-to-end neural and symbolic parameter learning. The
applicability of our framework is demonstrated with NeuPSL, a state-of-the-art
NeSy architecture. To achieve this, we propose a smooth primal and dual
formulation of NeuPSL inference and show learning gradients are functions of
the optimal dual variables. Additionally, we develop a dual block coordinate
descent algorithm for the new formulation that naturally exploits warm-starts.
This leads to over 100x learning runtime improvements over the current best
NeuPSL inference method. Finally, we provide extensive empirical evaluations
across $8$ datasets covering a range of tasks and demonstrate our learning
framework achieves up to a 16% point prediction performance improvement over
alternative learning methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dickens_C/0/1/0/all/0/1&quot;&gt;Charles Dickens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Changyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pryor_C/0/1/0/all/0/1&quot;&gt;Connor Pryor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_S/0/1/0/all/0/1&quot;&gt;Stephen Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Getoor_L/0/1/0/all/0/1&quot;&gt;Lise Getoor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09656">
<title>Mobility Accelerates Learning: Convergence Analysis on Hierarchical Federated Learning in Vehicular Networks. (arXiv:2401.09656v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09656</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical federated learning (HFL) enables distributed training of models
across multiple devices with the help of several edge servers and a cloud edge
server in a privacy-preserving manner. In this paper, we consider HFL with
highly mobile devices, mainly targeting at vehicular networks. Through
convergence analysis, we show that mobility influences the convergence speed by
both fusing the edge data and shuffling the edge models. While mobility is
usually considered as a challenge from the perspective of communication, we
prove that it increases the convergence speed of HFL with edge-level
heterogeneous data, since more diverse data can be incorporated. Furthermore,
we demonstrate that a higher speed leads to faster convergence, since it
accelerates the fusion of data. Simulation results show that mobility increases
the model accuracy of HFL by up to 15.1% when training a convolutional neural
network on the CIFAR-10 dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jintao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Sheng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunduz_D/0/1/0/all/0/1&quot;&gt;Deniz G&amp;#xfc;nd&amp;#xfc;z&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_Z/0/1/0/all/0/1&quot;&gt;Zhisheng Niu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09666">
<title>Traffic Smoothing Controllers for Autonomous Vehicles Using Deep Reinforcement Learning and Real-World Trajectory Data. (arXiv:2401.09666v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2401.09666</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing traffic-smoothing cruise controllers that can be deployed onto
autonomous vehicles is a key step towards improving traffic flow, reducing
congestion, and enhancing fuel efficiency in mixed autonomy traffic. We bypass
the common issue of having to carefully fine-tune a large traffic
microsimulator by leveraging real-world trajectory data from the I-24 highway
in Tennessee, replayed in a one-lane simulation. Using standard deep
reinforcement learning methods, we train energy-reducing wave-smoothing
policies. As an input to the agent, we observe the speed and distance of only
the vehicle in front, which are local states readily available on most recent
vehicles, as well as non-local observations about the downstream state of the
traffic. We show that at a low 4% autonomous vehicle penetration rate, we
achieve significant fuel savings of over 15% on trajectories exhibiting many
stop-and-go waves. Finally, we analyze the smoothing effect of the controllers
and demonstrate robustness to adding lane-changing into the simulation as well
as the removal of downstream information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lichtle_N/0/1/0/all/0/1&quot;&gt;Nathan Lichtl&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jang_K/0/1/0/all/0/1&quot;&gt;Kathy Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Adit Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vinitsky_E/0/1/0/all/0/1&quot;&gt;Eugene Vinitsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jonathan W. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bayen_A/0/1/0/all/0/1&quot;&gt;Alexandre M. Bayen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09671">
<title>Towards Identifiable Unsupervised Domain Translation: A Diversified Distribution Matching Approach. (arXiv:2401.09671v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09671</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain translation (UDT) aims to find functions that convert
samples from one domain (e.g., sketches) to another domain (e.g., photos)
without changing the high-level semantic meaning (also referred to as
``content&apos;&apos;). The translation functions are often sought by probability
distribution matching of the transformed source domain and target domain.
CycleGAN stands as arguably the most representative approach among this line of
work. However, it was noticed in the literature that CycleGAN and variants
could fail to identify the desired translation functions and produce
content-misaligned translations. This limitation arises due to the presence of
multiple translation functions -- referred to as ``measure-preserving
automorphism&quot; (MPA) -- in the solution space of the learning criteria. Despite
awareness of such identifiability issues, solutions have remained elusive. This
study delves into the core identifiability inquiry and introduces an MPA
elimination theory. Our analysis shows that MPA is unlikely to exist, if
multiple pairs of diverse cross-domain conditional distributions are matched by
the learning function. Our theory leads to a UDT learner using distribution
matching over auxiliary variable-induced subsets of the domains -- other than
over the entire data domains as in the classical approaches. The proposed
framework is the first to rigorously establish translation identifiability
under reasonable UDT settings, to our best knowledge. Experiments corroborate
with our theoretical claims.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrestha_S/0/1/0/all/0/1&quot;&gt;Sagar Shrestha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiao Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09680">
<title>Tiny Multi-Agent DRL for Twins Migration in UAV Metaverses: A Multi-Leader Multi-Follower Stackelberg Game Approach. (arXiv:2401.09680v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09680</link>
<description rdf:parseType="Literal">&lt;p&gt;The synergy between Unmanned Aerial Vehicles (UAVs) and metaverses is giving
rise to an emerging paradigm named UAV metaverses, which create a unified
ecosystem that blends physical and virtual spaces, transforming drone
interaction and virtual exploration. UAV Twins (UTs), as the digital twins of
UAVs that revolutionize UAV applications by making them more immersive,
realistic, and informative, are deployed and updated on ground base stations,
e.g., RoadSide Units (RSUs), to offer metaverse services for UAV Metaverse
Users (UMUs). Due to the dynamic mobility of UAVs and limited communication
coverages of RSUs, it is essential to perform real-time UT migration to ensure
seamless immersive experiences for UMUs. However, selecting appropriate RSUs
and optimizing the required bandwidth is challenging for achieving reliable and
efficient UT migration. To address the challenges, we propose a tiny machine
learning-based Stackelberg game framework based on pruning techniques for
efficient UT migration in UAV metaverses. Specifically, we formulate a
multi-leader multi-follower Stackelberg model considering a new immersion
metric of UMUs in the utilities of UAVs. Then, we design a Tiny Multi-Agent
Deep Reinforcement Learning (Tiny MADRL) algorithm to obtain the tiny networks
representing the optimal game solution. Specifically, the actor-critic network
leverages the pruning techniques to reduce the number of network parameters and
achieve model size and computation reduction, allowing for efficient
implementation of Tiny MADRL. Numerical results demonstrate that our proposed
schemes have better performance than traditional schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jiawen Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yue Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Minrui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiangtian Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jinbo Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Hongyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_D/0/1/0/all/0/1&quot;&gt;Dongdong Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xumin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shengli Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09691">
<title>Imitation Learning Inputting Image Feature to Each Layer of Neural Network. (arXiv:2401.09691v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09691</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning enables robots to learn and replicate human behavior from
training data. Recent advances in machine learning enable end-to-end learning
approaches that directly process high-dimensional observation data, such as
images. However, these approaches face a critical challenge when processing
data from multiple modalities, inadvertently ignoring data with a lower
correlation to the desired output, especially when using short sampling
periods. This paper presents a useful method to address this challenge, which
amplifies the influence of data with a relatively low correlation to the output
by inputting the data into each neural network layer. The proposed approach
effectively incorporates diverse data sources into the learning process.
Through experiments using a simple pick-and-place operation with raw images and
joint information as input, significant improvements in success rates are
demonstrated even when dealing with data from short sampling periods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamane_K/0/1/0/all/0/1&quot;&gt;Koki Yamane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakaino_S/0/1/0/all/0/1&quot;&gt;Sho Sakaino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsuji_T/0/1/0/all/0/1&quot;&gt;Toshiaki Tsuji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09695">
<title>Should ChatGPT Write Your Breakup Text? Exploring the Role of AI in Relationship Dissolution. (arXiv:2401.09695v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.09695</link>
<description rdf:parseType="Literal">&lt;p&gt;Relationships are essential to our happiness and wellbeing. The dissolution
of a relationship, the final stage of relationship&apos;s lifecycle and one of the
most stressful events in an individual&apos;s life, can have profound and
long-lasting impacts on people. With the breakup process increasingly
facilitated by computer-mediated communication (CMC), and the likely future
influence of AI-mediated communication (AIMC) tools, we conducted a
semi-structured interview study with 21 participants. We aim to understand: 1)
the current role of technology in the breakup process, 2) the needs and support
individuals have during the process, and 3) how AI might address these needs.
Our research shows that people have distinct needs at various stages of ending
a relationship. Presently, technology is used for information gathering and
community support, acting as a catalyst for breakups, enabling ghosting and
blocking, and facilitating communication. Participants anticipate that AI could
aid in sense-making of their relationship leading up to the breakup, act as a
mediator, assist in crafting appropriate wording, tones, and language during
breakup conversations, and support companionship, reflection, recovery, and
growth after a breakup. Our findings also demonstrate an overlap between the
breakup process and the Transtheoretical Model (TTM) of behavior change.
Through the lens of TTM, we explore the potential support and affordances AI
could offer in breakups, including its benefits and the necessary precautions
regarding AI&apos;s role in this sensitive process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yue Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Z/0/1/0/all/0/1&quot;&gt;Zelia Gomes Da Costa Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hiniker_A/0/1/0/all/0/1&quot;&gt;Alexis Hiniker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09699">
<title>Curriculum Recommendations Using Transformer Base Model with InfoNCE Loss And Language Switching Method. (arXiv:2401.09699v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09699</link>
<description rdf:parseType="Literal">&lt;p&gt;The Curriculum Recommendations paradigm is dedicated to fostering learning
equality within the ever-evolving realms of educational technology and
curriculum development. In acknowledging the inherent obstacles posed by
existing methodologies, such as content conflicts and disruptions from language
translation, this paradigm aims to confront and overcome these challenges.
Notably, it addresses content conflicts and disruptions introduced by language
translation, hindrances that can impede the creation of an all-encompassing and
personalized learning experience. The paradigm&apos;s objective is to cultivate an
educational environment that not only embraces diversity but also customizes
learning experiences to suit the distinct needs of each learner. To overcome
these challenges, our approach builds upon notable contributions in curriculum
development and personalized learning, introducing three key innovations. These
include the integration of Transformer Base Model to enhance computational
efficiency, the implementation of InfoNCE Loss for accurate content-topic
matching, and the adoption of a language switching strategy to alleviate
translation-related ambiguities. Together, these innovations aim to
collectively tackle inherent challenges and contribute to forging a more
equitable and effective learning journey for a diverse range of learners.
Competitive cross-validation scores underscore the efficacy of
sentence-transformers/LaBSE, achieving 0.66314, showcasing our methodology&apos;s
effectiveness in diverse linguistic nuances for content alignment prediction.
Index Terms-Curriculum Recommendation, Transformer model with InfoNCE Loss,
Language Switching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaonan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1&quot;&gt;Yongyao Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tianbo Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shulin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09716">
<title>HCVP: Leveraging Hierarchical Contrastive Visual Prompt for Domain Generalization. (arXiv:2401.09716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09716</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain Generalization (DG) endeavors to create machine learning models that
excel in unseen scenarios by learning invariant features. In DG, the prevalent
practice of constraining models to a fixed structure or uniform
parameterization to encapsulate invariant features can inadvertently blend
specific aspects. Such an approach struggles with nuanced differentiation of
inter-domain variations and may exhibit bias towards certain domains, hindering
the precise learning of domain-invariant features. Recognizing this, we
introduce a novel method designed to supplement the model with domain-level and
task-specific characteristics. This approach aims to guide the model in more
effectively separating invariant features from specific characteristics,
thereby boosting the generalization. Building on the emerging trend of visual
prompts in the DG paradigm, our work introduces the novel \textbf{H}ierarchical
\textbf{C}ontrastive \textbf{V}isual \textbf{P}rompt (HCVP) methodology. This
represents a significant advancement in the field, setting itself apart with a
unique generative approach to prompts, alongside an explicit model structure
and specialized loss functions. Differing from traditional visual prompts that
are often shared across entire datasets, HCVP utilizes a hierarchical prompt
generation network enhanced by prompt contrastive learning. These generative
prompts are instance-dependent, catering to the unique characteristics inherent
to different domains and tasks. Additionally, we devise a prompt modulation
network that serves as a bridge, effectively incorporating the generated visual
prompts into the vision transformer backbone. Experiments conducted on five DG
datasets demonstrate the effectiveness of HCVP, outperforming both established
DG algorithms and adaptation protocols.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guanglin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Z/0/1/0/all/0/1&quot;&gt;Zhongyi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shiming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Biwei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lina Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09717">
<title>Parameter Selection for Analyzing Conversations with Autism Spectrum Disorder. (arXiv:2401.09717v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.09717</link>
<description rdf:parseType="Literal">&lt;p&gt;The diagnosis of autism spectrum disorder (ASD) is a complex, challenging
task as it depends on the analysis of interactional behaviors by psychologists
rather than the use of biochemical diagnostics. In this paper, we present a
modeling approach to ASD diagnosis by analyzing acoustic/prosodic and
linguistic features extracted from diagnostic conversations between a
psychologist and children who either are typically developing (TD) or have ASD.
We compare the contributions of different features across a range of
conversation tasks. We focus on finding a minimal set of parameters that
characterize conversational behaviors of children with ASD. Because ASD is
diagnosed through conversational interaction, in addition to analyzing the
behavior of the children, we also investigate whether the psychologist&apos;s
conversational behaviors vary across diagnostic groups. Our results can
facilitate fine-grained analysis of conversation data for children with ASD to
support diagnosis and intervention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chowdhury_T/0/1/0/all/0/1&quot;&gt;Tahiya Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Romero_V/0/1/0/all/0/1&quot;&gt;Veronica Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stent_A/0/1/0/all/0/1&quot;&gt;Amanda Stent&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09748">
<title>Bootstrapping OTS-Funcimg Pre-training Model (Botfip) -- A Comprehensive Symbolic Regression Framework. (arXiv:2401.09748v1 [cs.SC])</title>
<link>http://arxiv.org/abs/2401.09748</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of scientific computing, many problem-solving approaches tend to
focus only on the process and final outcome, even in AI for science, there is a
lack of deep multimodal information mining behind the data, missing a
multimodal framework akin to that in the image-text domain. In this paper, we
take Symbolic Regression(SR) as our focal point and, drawing inspiration from
the BLIP model in the image-text domain, propose a scientific computing
multimodal framework based on Function Images (Funcimg) and Operation Tree
Sequence (OTS), named Bootstrapping OTS-Funcimg Pre-training Model (Botfip). In
SR experiments, we validate the advantages of Botfip in low-complexity SR
problems, showcasing its potential. As a MED framework, Botfip holds promise
for future applications in a broader range of scientific computing problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Pengbo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Haibiao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09756">
<title>Explaining Drift using Shapley Values. (arXiv:2401.09756v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09756</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models often deteriorate in their performance when they are
used to predict the outcomes over data on which they were not trained. These
scenarios can often arise in real world when the distribution of data changes
gradually or abruptly due to major events like a pandemic. There have been many
attempts in machine learning research to come up with techniques that are
resilient to such Concept drifts. However, there is no principled framework to
identify the drivers behind the drift in model performance. In this paper, we
propose a novel framework - DBShap that uses Shapley values to identify the
main contributors of the drift and quantify their respective contributions. The
proposed framework not only quantifies the importance of individual features in
driving the drift but also includes the change in the underlying relation
between the input and output as a possible driver. The explanation provided by
DBShap can be used to understand the root cause behind the drift and use it to
make the model resilient to the drift.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edakunni_N/0/1/0/all/0/1&quot;&gt;Narayanan U. Edakunni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tekriwal_U/0/1/0/all/0/1&quot;&gt;Utkarsh Tekriwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anukriti Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09757">
<title>Cooperative Tri-Point Model-Based Ground-to-Air Coverage Extension in Beyond 5G Networks. (arXiv:2401.09757v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.09757</link>
<description rdf:parseType="Literal">&lt;p&gt;The utilization of existing terrestrial infrastructures to provide coverage
for aerial users is a potentially low-cost solution. However, the already
deployed terrestrial base stations (TBSs) result in weak ground-to-air (G2A)
coverage due to the down-tilted antennas. Furthermore, achieving optimal
coverage across the entire airspace through antenna adjustment is challenging
due to the complex signal coverage requirements in three-dimensional space,
especially in the vertical direction. In this paper, we propose a cooperative
tri-point (CoTP) model-based method that utilizes cooperative beams to enhance
the G2A coverage extension. To utilize existing TBSs for establishing effective
cooperation, we prove that the cooperation among three TBSs can ensure G2A
coverage with a minimum coverage overlap, and design the CoTP model to analyze
the G2A coverage extension. Using the model, a cooperative coverage structure
based on Delaunay triangulation is designed to divide triangular prism-shaped
subspaces and corresponding TBS cooperation sets. To enable TBSs in the
cooperation set to cover different height subspaces while maintaining ground
coverage, we design a cooperative beam generation algorithm to maximize the
coverage in the triangular prism-shaped airspace. The simulation results and
field trials demonstrate that the proposed method can efficiently enhance the
G2A coverage extension while guaranteeing ground coverage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Ziwei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_M/0/1/0/all/0/1&quot;&gt;Min Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junju Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_C/0/1/0/all/0/1&quot;&gt;Chenxi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiandong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09763">
<title>CLIP Model for Images to Textual Prompts Based on Top-k Neighbors. (arXiv:2401.09763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09763</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image synthesis, a subfield of multimodal generation, has gained
significant attention in recent years. We propose a cost-effective approach for
image-to-prompt generation that leverages generative models to generate textual
prompts without the need for large amounts of annotated data. We divide our
method into two stages: online stage and offline stage. We use a combination of
the CLIP model and K-nearest neighbors (KNN) algorithm. The proposed system
consists of two main parts: an offline task and an online task. Our method owns
the highest metric 0.612 among these models, which is 0.013, 0.055, 0.011
higher than Clip, Clip + KNN(top 10) respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;YeMing Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_T/0/1/0/all/0/1&quot;&gt;Tianzhi Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09769">
<title>Towards Learning from Graphs with Heterophily: Progress and Future. (arXiv:2401.09769v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.09769</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphs are structured data that models complex relations between real-world
entities. Heterophilous graphs, where linked nodes are prone to be with
different labels or dissimilar features, have recently attracted significant
attention and found many applications. Meanwhile, increasing efforts have been
made to advance learning from heterophilous graphs. Although there exist
surveys on the relevant topic, they focus on heterophilous GNNs, which are only
sub-topics of heterophilous graph learning. In this survey, we comprehensively
overview existing works on learning from graphs with heterophily.First, we
collect over 180 publications and introduce the development of this field.
Then, we systematically categorize existing methods based on a hierarchical
taxonomy including learning strategies, model architectures and practical
applications. Finally, we discuss the primary challenges of existing studies
and highlight promising avenues for future research.More publication details
and corresponding open-source codes can be accessed and will be continuously
updated at our
repositories:https://github.com/gongchenghua/Awesome-Survey-Graphs-with-Heterophily.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chenghua Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_C/0/1/0/all/0/1&quot;&gt;Caihua Shan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Siqiang Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chuan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09773">
<title>SEINE: Structure Encoding and Interaction Network for Nuclei Instance Segmentation. (arXiv:2401.09773v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09773</link>
<description rdf:parseType="Literal">&lt;p&gt;Nuclei instance segmentation in histopathological images is of great
importance for biological analysis and cancer diagnosis but remains challenging
for two reasons. (1) Similar visual presentation of intranuclear and
extranuclear regions of chromophobe nuclei often causes under-segmentation, and
(2) current methods lack the exploration of nuclei structure, resulting in
fragmented instance predictions. To address these problems, this paper proposes
a structure encoding and interaction network, termed SEINE, which develops the
structure modeling scheme of nuclei and exploits the structure similarity
between nuclei to improve the integrality of each segmented instance.
Concretely, SEINE introduces a contour-based structure encoding (SE) that
considers the correlation between nuclei structure and semantics, realizing a
reasonable representation of the nuclei structure. Based on the encoding, we
propose a structure-guided attention (SGA) that takes the clear nuclei as
prototypes to enhance the structure learning for the fuzzy nuclei. To
strengthen the structural learning ability, a semantic feature fusion (SFF) is
presented to boost the semantic consistency of semantic and structure branches.
Furthermore, a position enhancement (PE) method is applied to suppress
incorrect nuclei boundary predictions. Extensive experiments demonstrate the
superiority of our approaches, and SEINE achieves state-of-the-art (SOTA)
performance on four datasets. The code is available at
\href{https://github.com/zhangye-zoe/SEINE}{https://github.com/zhangye-zoe/SEINE}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Ye Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Linghan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongbing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09786">
<title>Adaptive Self-training Framework for Fine-grained Scene Graph Generation. (arXiv:2401.09786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09786</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene graph generation (SGG) models have suffered from inherent problems
regarding the benchmark datasets such as the long-tailed predicate distribution
and missing annotation problems. In this work, we aim to alleviate the
long-tailed problem of SGG by utilizing unannotated triplets. To this end, we
introduce a Self-Training framework for SGG (ST-SGG) that assigns pseudo-labels
for unannotated triplets based on which the SGG models are trained. While there
has been significant progress in self-training for image recognition, designing
a self-training framework for the SGG task is more challenging due to its
inherent nature such as the semantic ambiguity and the long-tailed distribution
of predicate classes. Hence, we propose a novel pseudo-labeling technique for
SGG, called Class-specific Adaptive Thresholding with Momentum (CATM), which is
a model-agnostic framework that can be applied to any existing SGG models.
Furthermore, we devise a graph structure learner (GSL) that is beneficial when
adopting our proposed self-training framework to the state-of-the-art
message-passing neural network (MPNN)-based SGG models. Our extensive
experiments verify the effectiveness of ST-SGG on various SGG models,
particularly in enhancing the performance on fine-grained predicate classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kibum Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_K/0/1/0/all/0/1&quot;&gt;Kanghoon Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+In_Y/0/1/0/all/0/1&quot;&gt;Yeonjun In&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jinyoung Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chanyoung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09787">
<title>Querying Easily Flip-flopped Samples for Deep Active Learning. (arXiv:2401.09787v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09787</link>
<description rdf:parseType="Literal">&lt;p&gt;Active learning is a machine learning paradigm that aims to improve the
performance of a model by strategically selecting and querying unlabeled data.
One effective selection strategy is to base it on the model&apos;s predictive
uncertainty, which can be interpreted as a measure of how informative a sample
is. The sample&apos;s distance to the decision boundary is a natural measure of
predictive uncertainty, but it is often intractable to compute, especially for
complex decision boundaries formed in multiclass classification tasks. To
address this issue, this paper proposes the {\it least disagree metric} (LDM),
defined as the smallest probability of disagreement of the predicted label, and
an estimator for LDM proven to be asymptotically consistent under mild
assumptions. The estimator is computationally efficient and can be easily
implemented for deep learning models using parameter perturbation. The
LDM-based active learning is performed by querying unlabeled data with the
smallest LDM. Experimental results show that our LDM-based active learning
algorithm obtains state-of-the-art overall performance on all considered
datasets and deep architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Seong Jin Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gwangsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junghyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_J/0/1/0/all/0/1&quot;&gt;Jinwoo Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09789">
<title>A Semantic Approach for Big Data Exploration in Industry 4.0. (arXiv:2401.09789v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09789</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing trends in automation, Internet of Things, big data and cloud
computing technologies have led to the fourth industrial revolution (Industry
4.0), where it is possible to visualize and identify patterns and insights,
which results in a better understanding of the data and can improve the
manufacturing process. However, many times, the task of data exploration
results difficult for manufacturing experts because they might be interested in
analyzing also data that does not appear in pre-designed visualizations and
therefore they must be assisted by Information Technology experts. In this
paper, we present a proposal materialized in a semantic-based visual query
system developed for a real Industry 4.0 scenario that allows domain experts to
explore and visualize data in a friendly way. The main novelty of the system is
the combined use that it makes of captured data that are semantically annotated
first, and a 2D customized digital representation of a machine that is also
linked with semantic descriptions. Those descriptions are expressed using terms
of an ontology, where, among others, the sensors that are used to capture
indicators about the performance of a machine that belongs to a Industry 4.0
scenario have been modeled. Moreover, this semantic description allows to:
formulate queries at a higher level of abstraction, provide customized
graphical visualizations of the results based on the format and nature of the
data, and download enriched data enabling further types of analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berges_I/0/1/0/all/0/1&quot;&gt;Idoia Berges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramirez_Duran_V/0/1/0/all/0/1&quot;&gt;V&amp;#xed;ctor Julio Ram&amp;#xed;rez-Dur&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Illarramendi_A/0/1/0/all/0/1&quot;&gt;Arantza Illarramendi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09795">
<title>A Comparative Analysis on Metaheuristic Algorithms Based Vision Transformer Model for Early Detection of Alzheimer&apos;s Disease. (arXiv:2401.09795v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.09795</link>
<description rdf:parseType="Literal">&lt;p&gt;A number of life threatening neuro-degenerative disorders had degraded the
quality of life for the older generation in particular. Dementia is one such
symptom which may lead to a severe condition called Alzheimer&apos;s disease if not
detected at an early stage. It has been reported that the progression of such
disease from a normal stage is due to the change in several parameters inside
the human brain. In this paper, an innovative metaheuristic algorithms based
ViT model has been proposed for the identification of dementia at different
stage. A sizeable number of test data have been utilized for the validation of
the proposed scheme. It has also been demonstrated that our model exhibits
superior performance in terms of accuracy, precision, recall as well as
F1-score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_A/0/1/0/all/0/1&quot;&gt;Anuvab Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sen_U/0/1/0/all/0/1&quot;&gt;Udayon Sen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_S/0/1/0/all/0/1&quot;&gt;Subhabrata Roy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09798">
<title>All in How You Ask for It: Simple Black-Box Method for Jailbreak Attacks. (arXiv:2401.09798v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.09798</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) like ChatGPT face `jailbreak&apos; challenges, where
safeguards are bypassed to produce ethically harmful prompts. This study
introduces a simple black-box method to effectively generate jailbreak prompts,
overcoming the limitations of high complexity and computational costs
associated with existing methods. The proposed technique iteratively rewrites
harmful prompts into non-harmful expressions using the target LLM itself, based
on the hypothesis that LLMs can directly sample safeguard-bypassing
expressions. Demonstrated through experiments with ChatGPT (GPT-3.5 and GPT-4)
and Gemini-Pro, this method achieved an attack success rate of over 80% within
an average of 5 iterations and remained effective despite model updates. The
jailbreak prompts generated were naturally-worded and concise, suggesting they
are less detectable. The results indicate that creating effective jailbreak
prompts is simpler than previously considered, and black-box jailbreak attacks
pose a more serious security threat.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takemoto_K/0/1/0/all/0/1&quot;&gt;Kazuhiro Takemoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09819">
<title>PPNet: A Novel Neural Network Structure for End-to-End Near-Optimal Path Planning. (arXiv:2401.09819v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09819</link>
<description rdf:parseType="Literal">&lt;p&gt;The classical path planners, such as sampling-based path planners, have the
limitations of sensitivity to the initial solution and slow convergence to the
optimal solution. However, finding a near-optimal solution in a short period is
challenging in many applications such as the autonomous vehicle with limited
power/fuel. To achieve an end-to-end near-optimal path planner, we first divide
the path planning problem into two subproblems, which are path&apos;s space
segmentation and waypoints generation in the given path&apos;s space. We further
propose a two-level cascade neural network named Path Planning Network (PPNet)
to solve the path planning problem by solving the abovementioned subproblems.
Moreover, we propose a novel efficient data generation method for path planning
named EDaGe-PP. The results show the total computation time is less than 1/33
and the success rate of PPNet trained by the dataset that is generated by
EDaGe-PP is about $2 \times$ compared to other methods. We validate PPNet
against state-of-the-art path planning methods. The results show PPNet can find
a near-optimal solution in 15.3ms, which is much shorter than the
state-of-the-art path planners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Q/0/1/0/all/0/1&quot;&gt;Qinglong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Chongkun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xueqian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_S/0/1/0/all/0/1&quot;&gt;Songping Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_B/0/1/0/all/0/1&quot;&gt;Bin Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09833">
<title>Slicer Networks. (arXiv:2401.09833v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.09833</link>
<description rdf:parseType="Literal">&lt;p&gt;In medical imaging, scans often reveal objects with varied contrasts but
consistent internal intensities or textures. This characteristic enables the
use of low-frequency approximations for tasks such as segmentation and
deformation field estimation. Yet, integrating this concept into neural network
architectures for medical image analysis remains underexplored. In this paper,
we propose the Slicer Network, a novel architecture designed to leverage these
traits. Comprising an encoder utilizing models like vision transformers for
feature extraction and a slicer employing a learnable bilateral grid, the
Slicer Network strategically refines and upsamples feature maps via a
splatting-blurring-slicing process. This introduces an edge-preserving
low-frequency approximation for the network outcome, effectively enlarging the
effective receptive field. The enhancement not only reduces computational
complexity but also boosts overall performance. Experiments across different
medical imaging applications, including unsupervised and keypoints-based image
registration and lesion segmentation, have verified the Slicer Network&apos;s
improved accuracy and efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rongguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_R/0/1/0/all/0/1&quot;&gt;Renjiu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongdong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gaolei Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09851">
<title>Behavioral Simulation: Exploring A Possible Next Paradigm for Science. (arXiv:2401.09851v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09851</link>
<description rdf:parseType="Literal">&lt;p&gt;Simulation technologies have been widely utilized in many scientific research
fields such as weather forecasting, fluid mechanics and biological populations.
It is the best tool to handle problems in complex systems, where closed-form
expressions are unavailable and the target distribution in the representation
space is too complex to be fully represented by a deep learning (DL) model. We
believe that the development of simulation technologies is consistent with
scientific paradigms. This paper induces the evolution of scientific paradigms
from the perspective of data, algorithms, and computational power. Building
upon this perspective, we divide simulation technologies into three stages
aligning with the emergence of new paradigms, and find that advanced simulation
technologies are typical instances of paradigms integration. Moreover, we
propose the concept of behavioral simulation (BS), specifically sophisticated
behavioral simulation (SBS), representing a higher degree of paradigms
integration based on foundation models to simulate complex social systems
involving sophisticated human strategies and behaviors. BS and further SBS are
designed to tackle challenges concerning the complex human system that
surpasses the capacity of traditional agent-based modeling simulation (ABMS),
which can be regarded as a possible next paradigm for science. Through this
work, we look forward to more powerful BS and SBS applications in scientific
research branches within social science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chuwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Shirong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_R/0/1/0/all/0/1&quot;&gt;Ronghui Ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09852">
<title>Enhancing the Fairness and Performance of Edge Cameras with Explainable AI. (arXiv:2401.09852v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09852</link>
<description rdf:parseType="Literal">&lt;p&gt;The rising use of Artificial Intelligence (AI) in human detection on Edge
camera systems has led to accurate but complex models, challenging to interpret
and debug. Our research presents a diagnostic method using Explainable AI (XAI)
for model debugging, with expert-driven problem identification and solution
creation. Validated on the Bytetrack model in a real-world office Edge network,
we found the training dataset as the main bias source and suggested model
augmentation as a solution. Our approach helps identify model biases, essential
for achieving fair and trustworthy models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong Thanh Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_V/0/1/0/all/0/1&quot;&gt;Vo Thanh Khang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Quoc Hung Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_V/0/1/0/all/0/1&quot;&gt;Van Binh Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_Q/0/1/0/all/0/1&quot;&gt;Quoc Khanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hung Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09861">
<title>Temporal Insight Enhancement: Mitigating Temporal Hallucination in Multimodal Large Language Models. (arXiv:2401.09861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09861</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in Multimodal Large Language Models (MLLMs) have
significantly enhanced the comprehension of multimedia content, bringing
together diverse modalities such as text, images, and videos. However, a
critical challenge faced by these models, especially when processing video
inputs, is the occurrence of hallucinations - erroneous perceptions or
interpretations, particularly at the event level. This study introduces an
innovative method to address event-level hallucinations in MLLMs, focusing on
specific temporal understanding in video content. Our approach leverages a
novel framework that extracts and utilizes event-specific information from both
the event query and the provided video to refine MLLMs&apos; response. We propose a
unique mechanism that decomposes on-demand event queries into iconic actions.
Subsequently, we employ models like CLIP and BLIP2 to predict specific
timestamps for event occurrences. Our evaluation, conducted using the
Charades-STA dataset, demonstrates a significant reduction in temporal
hallucinations and an improvement in the quality of event-related responses.
This research not only provides a new perspective in addressing a critical
limitation of MLLMs but also contributes a quantitatively measurable method for
evaluating MLLMs in the context of temporal-related questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Li Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jun Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okatani_T/0/1/0/all/0/1&quot;&gt;Takayuki Okatani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09862">
<title>Evolutionary Multi-Objective Optimization of Large Language Model Prompts for Balancing Sentiments. (arXiv:2401.09862v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.09862</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) such as ChatGPT has attracted
considerable attention in various domains due to their remarkable performance
and versatility. As the use of these models continues to grow, the importance
of effective prompt engineering has come to the fore. Prompt optimization
emerges as a crucial challenge, as it has a direct impact on model performance
and the extraction of relevant information. Recently, evolutionary algorithms
(EAs) have shown promise in addressing this issue, paving the way for novel
optimization strategies. In this work, we propose a evolutionary
multi-objective (EMO) approach specifically tailored for prompt optimization
called EMO-Prompts, using sentiment analysis as a case study. We use sentiment
analysis capabilities as our experimental targets. Our results demonstrate that
EMO-Prompts effectively generates prompts capable of guiding the LLM to produce
texts embodying two conflicting emotions simultaneously.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baumann_J/0/1/0/all/0/1&quot;&gt;Jill Baumann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kramer_O/0/1/0/all/0/1&quot;&gt;Oliver Kramer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09865">
<title>Improving fine-grained understanding in image-text pre-training. (arXiv:2401.09865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09865</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SPARse Fine-grained Contrastive Alignment (SPARC), a simple
method for pretraining more fine-grained multimodal representations from
image-text pairs. Given that multiple image patches often correspond to single
words, we propose to learn a grouping of image patches for every token in the
caption. To achieve this, we use a sparse similarity metric between image
patches and language tokens and compute for each token a language-grouped
vision embedding as the weighted average of patches. The token and
language-grouped vision embeddings are then contrasted through a fine-grained
sequence-wise loss that only depends on individual samples and does not require
other batch samples as negatives. This enables more detailed information to be
learned in a computationally inexpensive manner. SPARC combines this
fine-grained loss with a contrastive loss between global image and text
embeddings to learn representations that simultaneously encode global and local
information. We thoroughly evaluate our proposed method and show improved
performance over competing approaches both on image-level tasks relying on
coarse-grained information, e.g. classification, as well as region-level tasks
relying on fine-grained information, e.g. retrieval, object detection, and
segmentation. Moreover, SPARC improves model faithfulness and captioning in
foundational vision-language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bica_I/0/1/0/all/0/1&quot;&gt;Ioana Bica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilic_A/0/1/0/all/0/1&quot;&gt;Anastasija Ili&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bauer_M/0/1/0/all/0/1&quot;&gt;Matthias Bauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdogan_G/0/1/0/all/0/1&quot;&gt;Goker Erdogan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bosnjak_M/0/1/0/all/0/1&quot;&gt;Matko Bo&amp;#x161;njak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplanis_C/0/1/0/all/0/1&quot;&gt;Christos Kaplanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gritsenko_A/0/1/0/all/0/1&quot;&gt;Alexey A. Gritsenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minderer_M/0/1/0/all/0/1&quot;&gt;Matthias Minderer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pascanu_R/0/1/0/all/0/1&quot;&gt;Razvan Pascanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1&quot;&gt;Jovana Mitrovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09870">
<title>Reconciling Spatial and Temporal Abstractions for Goal Representation. (arXiv:2401.09870v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09870</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal representation affects the performance of Hierarchical Reinforcement
Learning (HRL) algorithms by decomposing the complex learning problem into
easier subtasks. Recent studies show that representations that preserve
temporally abstract environment dynamics are successful in solving difficult
problems and provide theoretical guarantees for optimality. These methods
however cannot scale to tasks where environment dynamics increase in complexity
i.e. the temporally abstract transition relations depend on larger number of
variables. On the other hand, other efforts have tried to use spatial
abstraction to mitigate the previous issues. Their limitations include
scalability to high dimensional environments and dependency on prior knowledge.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel three-layer HRL algorithm that introduces,
at different levels of the hierarchy, both a spatial and a temporal goal
abstraction. We provide a theoretical study of the regret bounds of the learned
policies. We evaluate the approach on complex continuous control tasks,
demonstrating the effectiveness of spatial and temporal abstractions learned by
this approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zadem_M/0/1/0/all/0/1&quot;&gt;Mehdi Zadem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mover_S/0/1/0/all/0/1&quot;&gt;Sergio Mover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1&quot;&gt;Sao Mai Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09880">
<title>Attention-Based Recurrent Neural Network For Automatic Behavior Laying Hen Recognition. (arXiv:2401.09880v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2401.09880</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the interests of modern poultry farming is the vocalization of laying
hens which contain very useful information on health behavior. This information
is used as health and well-being indicators that help breeders better monitor
laying hens, which involves early detection of problems for rapid and more
effective intervention. In this work, we focus on the sound analysis for the
recognition of the types of calls of the laying hens in order to propose a
robust system of characterization of their behavior for a better monitoring. To
do this, we first collected and annotated laying hen call signals, then
designed an optimal acoustic characterization based on the combination of time
and frequency domain features. We then used these features to build the
multi-label classification models based on recurrent neural network to assign a
semantic class to the vocalization that characterize the laying hen behavior.
The results show an overall performance with our model based on the combination
of time and frequency domain features that obtained the highest F1-score
(F1=92.75) with a gain of 17% on the models using the frequency domain features
and of 8% on the compared approaches from the litterature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laleye_F/0/1/0/all/0/1&quot;&gt;Fr&amp;#xe9;jus A. A. Laleye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousse_M/0/1/0/all/0/1&quot;&gt;Mika&amp;#xeb;l A. Mousse&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09886">
<title>Cooperative Edge Caching Based on Elastic Federated and Multi-Agent Deep Reinforcement Learning in Next-Generation Network. (arXiv:2401.09886v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09886</link>
<description rdf:parseType="Literal">&lt;p&gt;Edge caching is a promising solution for next-generation networks by
empowering caching units in small-cell base stations (SBSs), which allows user
equipments (UEs) to fetch users&apos; requested contents that have been pre-cached
in SBSs. It is crucial for SBSs to predict accurate popular contents through
learning while protecting users&apos; personal information. Traditional federated
learning (FL) can protect users&apos; privacy but the data discrepancies among UEs
can lead to a degradation in model quality. Therefore, it is necessary to train
personalized local models for each UE to predict popular contents accurately.
In addition, the cached contents can be shared among adjacent SBSs in
next-generation networks, thus caching predicted popular contents in different
SBSs may affect the cost to fetch contents. Hence, it is critical to determine
where the popular contents are cached cooperatively. To address these issues,
we propose a cooperative edge caching scheme based on elastic federated and
multi-agent deep reinforcement learning (CEFMR) to optimize the cost in the
network. We first propose an elastic FL algorithm to train the personalized
model for each UE, where adversarial autoencoder (AAE) model is adopted for
training to improve the prediction accuracy, then {a popular} content
prediction algorithm is proposed to predict the popular contents for each SBS
based on the trained AAE model. Finally, we propose a multi-agent deep
reinforcement learning (MADRL) based algorithm to decide where the predicted
popular contents are collaboratively cached among SBSs. Our experimental
results demonstrate the superiority of our proposed scheme to existing baseline
caching schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhua Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_P/0/1/0/all/0/1&quot;&gt;Pingyi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Q/0/1/0/all/0/1&quot;&gt;Qiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huiling Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Letaief_K/0/1/0/all/0/1&quot;&gt;Khaled B. Letaief&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09900">
<title>XAI-Enhanced Semantic Segmentation Models for Visual Quality Inspection. (arXiv:2401.09900v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09900</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual quality inspection systems, crucial in sectors like manufacturing and
logistics, employ computer vision and machine learning for precise, rapid
defect detection. However, their unexplained nature can hinder trust, error
identification, and system improvement. This paper presents a framework to
bolster visual quality inspection by using CAM-based explanations to refine
semantic segmentation models. Our approach consists of 1) Model Training, 2)
XAI-based Model Explanation, 3) XAI Evaluation, and 4) Annotation Augmentation
for Model Enhancement, informed by explanations and expert insights.
Evaluations show XAI-enhanced models surpass original DeepLabv3-ResNet101
models, especially in intricate object segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clement_T/0/1/0/all/0/1&quot;&gt;Tobias Clement&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Truong Thanh Hung Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelaal_M/0/1/0/all/0/1&quot;&gt;Mohamed Abdelaal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hung Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09942">
<title>Multi-task Learning for Joint Re-identification, Team Affiliation, and Role Classification for Sports Visual Tracking. (arXiv:2401.09942v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.09942</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective tracking and re-identification of players is essential for
analyzing soccer videos. But, it is a challenging task due to the non-linear
motion of players, the similarity in appearance of players from the same team,
and frequent occlusions. Therefore, the ability to extract meaningful
embeddings to represent players is crucial in developing an effective tracking
and re-identification system. In this paper, a multi-purpose part-based person
representation method, called PRTreID, is proposed that performs three tasks of
role classification, team affiliation, and re-identification, simultaneously.
In contrast to available literature, a single network is trained with
multi-task supervision to solve all three tasks, jointly. The proposed joint
method is computationally efficient due to the shared backbone. Also, the
multi-task learning leads to richer and more discriminative representations, as
demonstrated by both quantitative and qualitative results. To demonstrate the
effectiveness of PRTreID, it is integrated with a state-of-the-art tracking
method, using a part-based post-processing module to handle long-term tracking.
The proposed tracking method outperforms all existing tracking methods on the
challenging SoccerNet tracking dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansourian_A/0/1/0/all/0/1&quot;&gt;Amir M. Mansourian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Somers_V/0/1/0/all/0/1&quot;&gt;Vladimir Somers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vleeschouwer_C/0/1/0/all/0/1&quot;&gt;Christophe De Vleeschouwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasaei_S/0/1/0/all/0/1&quot;&gt;Shohreh Kasaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09944">
<title>WindSeer: Real-time volumetric wind prediction over complex terrain aboard a small UAV. (arXiv:2401.09944v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09944</link>
<description rdf:parseType="Literal">&lt;p&gt;Real-time high-resolution wind predictions are beneficial for various
applications including safe manned and unmanned aviation. Current weather
models require too much compute and lack the necessary predictive capabilities
as they are valid only at the scale of multiple kilometers and hours - much
lower spatial and temporal resolutions than these applications require. Our
work, for the first time, demonstrates the ability to predict low-altitude wind
in real-time on limited-compute devices, from only sparse measurement data. We
train a neural network, WindSeer, using only synthetic data from computational
fluid dynamics simulations and show that it can successfully predict real wind
fields over terrain with known topography from just a few noisy and spatially
clustered wind measurements. WindSeer can generate accurate predictions at
different resolutions and domain sizes on previously unseen topography without
retraining. We demonstrate that the model successfully predicts historical wind
data collected by weather stations and wind measured onboard drones.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achermann_F/0/1/0/all/0/1&quot;&gt;Florian Achermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stastny_T/0/1/0/all/0/1&quot;&gt;Thomas Stastny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danciu_B/0/1/0/all/0/1&quot;&gt;Bogdan Danciu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolobov_A/0/1/0/all/0/1&quot;&gt;Andrey Kolobov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jen Jen Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1&quot;&gt;Roland Siegwart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lawrance_N/0/1/0/all/0/1&quot;&gt;Nicholas Lawrance&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09964">
<title>When Neural Code Completion Models Size up the Situation: Attaining Cheaper and Faster Completion through Dynamic Model Inference. (arXiv:2401.09964v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2401.09964</link>
<description rdf:parseType="Literal">&lt;p&gt;Leveraging recent advancements in large language models, modern neural code
completion models have demonstrated the capability to generate highly accurate
code suggestions. However, their massive size poses challenges in terms of
computational costs and environmental impact, hindering their widespread
adoption in practical scenarios. Dynamic inference emerges as a promising
solution, as it allocates minimal computation during inference while
maintaining the model&apos;s performance. In this research, we explore dynamic
inference within the context of code completion. Initially, we conducted an
empirical investigation on GPT-2, focusing on the inference capabilities of
intermediate layers for code completion. We found that 54.4% of tokens can be
accurately generated using just the first layer, signifying significant
computational savings potential. Moreover, despite using all layers, the model
still fails to predict 14.5% of tokens correctly, and the subsequent
completions continued from them are rarely considered helpful, with only a 4.2%
Acceptance Rate. These findings motivate our exploration of dynamic inference
in code completion and inspire us to enhance it with a decision-making
mechanism that stops the generation of incorrect code. We thus propose a novel
dynamic inference method specifically tailored for code completion models. This
method aims not only to produce correct predictions with largely reduced
computation but also to prevent incorrect predictions proactively. Our
extensive evaluation shows that it can averagely skip 1.7 layers out of 16
layers in the models, leading to an 11.2% speedup with only a marginal 1.1%
reduction in ROUGE-L.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhensu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaoning Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_F/0/1/0/all/0/1&quot;&gt;Fu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Li Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09966">
<title>Towards Generative Abstract Reasoning: Completing Raven&apos;s Progressive Matrix via Rule Abstraction and Selection. (arXiv:2401.09966v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.09966</link>
<description rdf:parseType="Literal">&lt;p&gt;Endowing machines with abstract reasoning ability has been a long-term
research topic in artificial intelligence. Raven&apos;s Progressive Matrix (RPM) is
widely used to probe abstract visual reasoning in machine intelligence, where
models need to understand the underlying rules and select the missing
bottom-right images out of candidate sets to complete image matrices. The
participators can display powerful reasoning ability by inferring the
underlying attribute-changing rules and imagining the missing images at
arbitrary positions. However, existing solvers can hardly manifest such an
ability in realistic RPM problems. In this paper, we propose a conditional
generative model to solve answer generation problems through Rule AbstractIon
and SElection (RAISE) in the latent space. RAISE encodes image attributes as
latent concepts and decomposes underlying rules into atomic rules by means of
concepts, which are abstracted as global learnable parameters. When generating
the answer, RAISE selects proper atomic rules out of the global knowledge set
for each concept and composes them into the integrated rule of an RPM. In most
configurations, RAISE outperforms the compared generative solvers in tasks of
generating bottom-right and arbitrary-position answers. We test RAISE in the
odd-one-out task and two held-out configurations to demonstrate how learning
decoupled latent concepts and atomic rules helps find the image breaking the
underlying rules and handle RPMs with unseen combinations of rules and
attributes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_F/0/1/0/all/0/1&quot;&gt;Fan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_X/0/1/0/all/0/1&quot;&gt;Xiangyang Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09983">
<title>Multiobjective Optimization Analysis for Finding Infrastructure-as-Code Deployment Configurations. (arXiv:2401.09983v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.09983</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiobjective optimization is a hot topic in the artificial intelligence and
operations research communities. The design and development of multiobjective
methods is a frequent task for researchers and practitioners. As a result of
this vibrant activity, a myriad of techniques have been proposed in the
literature to date, demonstrating a significant effectiveness for dealing with
situations coming from a wide range of real-world areas. This paper is focused
on a multiobjective problem related to optimizing Infrastructure-as-Code
deployment configurations. The system implemented for solving this problem has
been coined as IaC Optimizer Platform (IOP). Despite the fact that a
prototypical version of the IOP has been introduced in the literature before, a
deeper analysis focused on the resolution of the problem is needed, in order to
determine which is the most appropriate multiobjective method for embedding in
the IOP. The main motivation behind the analysis conducted in this work is to
enhance the IOP performance as much as possible. This is a crucial aspect of
this system, deeming that it will be deployed in a real environment, as it is
being developed as part of a H2020 European project. Going deeper, we resort in
this paper to nine different evolutionary computation-based multiobjective
algorithms. For assessing the quality of the considered solvers, 12 different
problem instances have been generated based on real-world settings. Results
obtained by each method after 10 independent runs have been compared using
Friedman&apos;s non-parametric tests. Findings reached from the tests carried out
lad to the creation of a multi-algorithm system, capable of applying different
techniques according to the user&apos;s needs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osaba_E/0/1/0/all/0/1&quot;&gt;Eneko Osaba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_de_Arcaya_J/0/1/0/all/0/1&quot;&gt;Josu Diaz-de-Arcaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_J/0/1/0/all/0/1&quot;&gt;Juncal Alonso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobo_J/0/1/0/all/0/1&quot;&gt;Jesus L. Lobo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benguria_G/0/1/0/all/0/1&quot;&gt;Gorka Benguria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Etxaniz_I/0/1/0/all/0/1&quot;&gt;I&amp;#xf1;aki Etxaniz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09986">
<title>FLex&amp;Chill: Improving Local Federated Learning Training with Logit Chilling. (arXiv:2401.09986v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.09986</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning are inherently hampered by data heterogeneity: non-iid
distributed training data over local clients. We propose a novel model training
approach for federated learning, FLex&amp;amp;Chill, which exploits the Logit Chilling
method. Through extensive evaluations, we demonstrate that, in the presence of
non-iid data characteristics inherent in federated learning systems, this
approach can expedite model convergence and improve inference accuracy.
Quantitatively, from our experiments, we observe up to 6X improvement in the
global federated learning model convergence time, and up to 3.37% improvement
in inference accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kichang Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Songkuk Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;JeongGil Ko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09987">
<title>A-KIT: Adaptive Kalman-Informed Transformer. (arXiv:2401.09987v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.09987</link>
<description rdf:parseType="Literal">&lt;p&gt;The extended Kalman filter (EKF) is a widely adopted method for sensor fusion
in navigation applications. A crucial aspect of the EKF is the online
determination of the process noise covariance matrix reflecting the model
uncertainty. While common EKF implementation assumes a constant process noise,
in real-world scenarios, the process noise varies, leading to inaccuracies in
the estimated state and potentially causing the filter to diverge. To cope with
such situations, model-based adaptive EKF methods were proposed and
demonstrated performance improvements, highlighting the need for a robust
adaptive approach. In this paper, we derive and introduce A-KIT, an adaptive
Kalman-informed transformer to learn the varying process noise covariance
online. The A-KIT framework is applicable to any type of sensor fusion. Here,
we present our approach to nonlinear sensor fusion based on an inertial
navigation system and Doppler velocity log. By employing real recorded data
from an autonomous underwater vehicle, we show that A-KIT outperforms the
conventional EKF by more than 49.5% and model-based adaptive EKF by an average
of 35.4% in terms of position accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1&quot;&gt;Nadav Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_I/0/1/0/all/0/1&quot;&gt;Itzik Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10016">
<title>Gender Bias in Machine Translation and The Era of Large Language Models. (arXiv:2401.10016v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10016</link>
<description rdf:parseType="Literal">&lt;p&gt;This chapter examines the role of Machine Translation in perpetuating gender
bias, highlighting the challenges posed by cross-linguistic settings and
statistical dependencies. A comprehensive overview of relevant existing work
related to gender bias in both conventional Neural Machine Translation
approaches and Generative Pretrained Transformer models employed as Machine
Translation systems is provided. Through an experiment using ChatGPT (based on
GPT-3.5) in an English-Italian translation context, we further assess ChatGPT&apos;s
current capacity to address gender bias. The findings emphasize the ongoing
need for advancements in mitigating bias in Machine Translation systems and
underscore the importance of fostering fairness and inclusivity in language
technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanmassenhove_E/0/1/0/all/0/1&quot;&gt;Eva Vanmassenhove&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10019">
<title>R-Judge: Benchmarking Safety Risk Awareness for LLM Agents. (arXiv:2401.10019v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10019</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have exhibited great potential in autonomously
completing tasks across real-world applications. Despite this, these LLM agents
introduce unexpected safety risks when operating in interactive environments.
Instead of centering on LLM-generated content safety in most prior studies,
this work addresses the imperative need for benchmarking the behavioral safety
of LLM agents within diverse environments. We introduce R-Judge, a benchmark
crafted to evaluate the proficiency of LLMs in judging safety risks given agent
interaction records. R-Judge comprises 162 agent interaction records,
encompassing 27 key risk scenarios among 7 application categories and 10 risk
types. It incorporates human consensus on safety with annotated safety risk
labels and high-quality risk descriptions. Utilizing R-Judge, we conduct a
comprehensive evaluation of 8 prominent LLMs commonly employed as the backbone
for agents. The best-performing model, GPT-4, achieves 72.29% in contrast to
the human score of 89.38%, showing considerable room for enhancing the risk
awareness of LLMs. Notably, leveraging risk descriptions as environment
feedback significantly improves model performance, revealing the importance of
salient safety risk feedback. Furthermore, we design an effective chain of
safety analysis technique to help the judgment of safety risks and conduct an
in-depth case study to facilitate future research. R-Judge is publicly
available at https://github.com/Lordog/R-Judge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_T/0/1/0/all/0/1&quot;&gt;Tongxin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhiwei He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_L/0/1/0/all/0/1&quot;&gt;Lingzhong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ruijie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tian Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Lizhen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Binglin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhuosheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_G/0/1/0/all/0/1&quot;&gt;Gongshen Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10020">
<title>Self-Rewarding Language Models. (arXiv:2401.10020v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10020</link>
<description rdf:parseType="Literal">&lt;p&gt;We posit that to achieve superhuman agents, future models require superhuman
feedback in order to provide an adequate training signal. Current approaches
commonly train reward models from human preferences, which may then be
bottlenecked by human performance level, and secondly these separate frozen
reward models cannot then learn to improve during LLM training. In this work,
we study Self-Rewarding Language Models, where the language model itself is
used via LLM-as-a-Judge prompting to provide its own rewards during training.
We show that during Iterative DPO training that not only does instruction
following ability improve, but also the ability to provide high-quality rewards
to itself. Fine-tuning Llama 2 70B on three iterations of our approach yields a
model that outperforms many existing systems on the AlpacaEval 2.0 leaderboard,
including Claude 2, Gemini Pro, and GPT-4 0613. While only a preliminary study,
this work opens the door to the possibility of models that can continually
improve in both axes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Weizhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_R/0/1/0/all/0/1&quot;&gt;Richard Yuanzhe Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukhbaatar_S/0/1/0/all/0/1&quot;&gt;Sainbayar Sukhbaatar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weston_J/0/1/0/all/0/1&quot;&gt;Jason Weston&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10032">
<title>FreGrad: Lightweight and Fast Frequency-aware Diffusion Vocoder. (arXiv:2401.10032v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.10032</link>
<description rdf:parseType="Literal">&lt;p&gt;The goal of this paper is to generate realistic audio with a lightweight and
fast diffusion-based vocoder, named FreGrad. Our framework consists of the
following three key components: (1) We employ discrete wavelet transform that
decomposes a complicated waveform into sub-band wavelets, which helps FreGrad
to operate on a simple and concise feature space, (2) We design a
frequency-aware dilated convolution that elevates frequency awareness,
resulting in generating speech with accurate frequency information, and (3) We
introduce a bag of tricks that boosts the generation quality of the proposed
model. In our experiments, FreGrad achieves 3.7 times faster training time and
2.2 times faster inference speed compared to our baseline while reducing the
model size by 0.6 times (only 1.78M parameters) without sacrificing the output
quality. Audio samples are available at:
https://mm.kaist.ac.kr/projects/FreGrad.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Tan Dat Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Ji-Hoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jang_Y/0/1/0/all/0/1&quot;&gt;Youngjoon Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jaehun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Joon Son Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10034">
<title>Evolutionary Computation in the Era of Large Language Model: Survey and Roadmap. (arXiv:2401.10034v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.10034</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs), built upon Transformer-based architectures with
massive pretraining on diverse data, have not only revolutionized natural
language processing but also extended their prowess to various domains, marking
a significant stride towards artificial general intelligence. The interplay
between LLMs and Evolutionary Algorithms (EAs), despite differing in objectives
and methodologies, reveals intriguing parallels, especially in their shared
optimization nature, black-box characteristics, and proficiency in handling
complex problems. Meanwhile, EA can not only provide an optimization framework
for LLM&apos;s further enhancement under black-box settings but also empower LLM
with flexible global search and iterative mechanism in applications. On the
other hand, LLM&apos;s abundant domain knowledge enables EA to perform smarter
searches, while its text processing capability assist in deploying EA across
various tasks. Based on their complementary advantages, this paper presents a
comprehensive review and forward-looking roadmap, categorizing their mutual
inspiration into LLM-enhanced evolutionary optimization and EA-enhanced LLM.
Some integrated synergy methods are further introduced to exemplify the
amalgamation of LLMs and EAs in various application scenarios, including neural
architecture search, code generation, software engineering, and text
generation. As the first comprehensive review specifically focused on the EA
research in the era of LLMs, this paper provides a foundational stepping stone
for understanding and harnessing the collaborative potential of LLMs and EAs.
By presenting a comprehensive review, categorization, and critical analysis, we
contribute to the ongoing discourse on the cross-disciplinary study of these
two powerful paradigms. The identified challenges and future directions offer
guidance to unlock the full potential of this innovative collaboration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xingyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Sheng-hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jibin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_L/0/1/0/all/0/1&quot;&gt;Liang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_K/0/1/0/all/0/1&quot;&gt;Kay Chen Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10036">
<title>LOCALINTEL: Generating Organizational Threat Intelligence from Global and Local Cyber Knowledge. (arXiv:2401.10036v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.10036</link>
<description rdf:parseType="Literal">&lt;p&gt;Security Operations Center (SoC) analysts gather threat reports from openly
accessible global threat databases and customize them manually to suit a
particular organization&apos;s needs. These analysts also depend on internal
repositories, which act as private local knowledge database for an
organization. Credible cyber intelligence, critical operational details, and
relevant organizational information are all stored in these local knowledge
databases. Analysts undertake a labor intensive task utilizing these global and
local knowledge databases to manually create organization&apos;s unique threat
response and mitigation strategies. Recently, Large Language Models (LLMs) have
shown the capability to efficiently process large diverse knowledge sources. We
leverage this ability to process global and local knowledge databases to
automate the generation of organization-specific threat intelligence.
&lt;/p&gt;
&lt;p&gt;In this work, we present LOCALINTEL, a novel automated knowledge
contextualization system that, upon prompting, retrieves threat reports from
the global threat repositories and uses its local knowledge database to
contextualize them for a specific organization. LOCALINTEL comprises of three
key phases: global threat intelligence retrieval, local knowledge retrieval,
and contextualized completion generation. The former retrieves intelligence
from global threat repositories, while the second retrieves pertinent knowledge
from the local knowledge database. Finally, the fusion of these knowledge
sources is orchestrated through a generator to produce a contextualized
completion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Shaswata Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neupane_S/0/1/0/all/0/1&quot;&gt;Subash Neupane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Trisha Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sudip Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piplai_A/0/1/0/all/0/1&quot;&gt;Aritran Piplai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaur_M/0/1/0/all/0/1&quot;&gt;Manas Gaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_S/0/1/0/all/0/1&quot;&gt;Shahram Rahimi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10040">
<title>Large Language Models for Scientific Information Extraction: An Empirical Study for Virology. (arXiv:2401.10040v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10040</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we champion the use of structured and semantic content
representation of discourse-based scholarly communication, inspired by tools
like Wikipedia infoboxes or structured Amazon product descriptions. These
representations provide users with a concise overview, aiding scientists in
navigating the dense academic landscape. Our novel automated approach leverages
the robust text generation capabilities of LLMs to produce structured scholarly
contribution summaries, offering both a practical solution and insights into
LLMs&apos; emergent abilities.
&lt;/p&gt;
&lt;p&gt;For LLMs, the prime focus is on improving their general intelligence as
conversational agents. We argue that these models can also be applied
effectively in information extraction (IE), specifically in complex IE tasks
within terse domains like Science. This paradigm shift replaces the traditional
modular, pipelined machine learning approach with a simpler objective expressed
through instructions. Our results show that finetuned FLAN-T5 with 1000x fewer
parameters than the state-of-the-art GPT-davinci is competitive for the task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shamsabadi_M/0/1/0/all/0/1&quot;&gt;Mahsa Shamsabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DSouza_J/0/1/0/all/0/1&quot;&gt;Jennifer D&amp;#x27;Souza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auer_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Auer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10061">
<title>DiffusionGPT: LLM-Driven Text-to-Image Generation System. (arXiv:2401.10061v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10061</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have opened up new avenues for the field of image
generation, resulting in the proliferation of high-quality models shared on
open-source platforms. However, a major challenge persists in current
text-to-image systems are often unable to handle diverse inputs, or are limited
to single model results. Current unified attempts often fall into two
orthogonal aspects: i) parse Diverse Prompts in input stage; ii) activate
expert model to output. To combine the best of both worlds, we propose
DiffusionGPT, which leverages Large Language Models (LLM) to offer a unified
generation system capable of seamlessly accommodating various types of prompts
and integrating domain-expert models. DiffusionGPT constructs domain-specific
Trees for various generative models based on prior knowledge. When provided
with an input, the LLM parses the prompt and employs the Trees-of-Thought to
guide the selection of an appropriate model, thereby relaxing input constraints
and ensuring exceptional performance across diverse domains. Moreover, we
introduce Advantage Databases, where the Tree-of-Thought is enriched with human
feedback, aligning the model selection process with human preferences. Through
extensive experiments and comparisons, we demonstrate the effectiveness of
DiffusionGPT, showcasing its potential for pushing the boundaries of image
synthesis in diverse domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jie Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weifeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuxi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huixia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hefeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xuefeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1&quot;&gt;Shilei Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10101">
<title>Counterfactual Reasoning with Probabilistic Graphical Models for Analyzing Socioecological Systems. (arXiv:2401.10101v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.10101</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal and counterfactual reasoning are emerging directions in data science
that allow us to reason about hypothetical scenarios. This is particularly
useful in domains where experimental data are usually not available. In the
context of environmental and ecological sciences, causality enables us, for
example, to predict how an ecosystem would respond to hypothetical
interventions. A structural causal model is a class of probabilistic graphical
models for causality, which, due to its intuitive nature, can be easily
understood by experts in multiple fields. However, certain queries, called
unidentifiable, cannot be calculated in an exact and precise manner. This paper
proposes applying a novel and recent technique for bounding unidentifiable
queries within the domain of socioecological systems. Our findings indicate
that traditional statistical analysis, including probabilistic graphical
models, can identify the influence between variables. However, such methods do
not offer insights into the nature of the relationship, specifically whether it
involves necessity or sufficiency. This is where counterfactual reasoning
becomes valuable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabanas_R/0/1/0/all/0/1&quot;&gt;Rafael Caba&amp;#xf1;as&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maldonado_A/0/1/0/all/0/1&quot;&gt;Ana D. Maldonado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morales_M/0/1/0/all/0/1&quot;&gt;Mar&amp;#xed;a Morales&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aguilera_P/0/1/0/all/0/1&quot;&gt;Pedro A. Aguilera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salmeron_A/0/1/0/all/0/1&quot;&gt;Antonio Salmer&amp;#xf3;n&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10119">
<title>Towards Principled Graph Transformers. (arXiv:2401.10119v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10119</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph learning architectures based on the k-dimensional Weisfeiler-Leman
(k-WL) hierarchy offer a theoretically well-understood expressive power.
However, such architectures often fail to deliver solid predictive performance
on real-world tasks, limiting their practical impact. In contrast, global
attention-based models such as graph transformers demonstrate strong
performance in practice, but comparing their expressive power with the k-WL
hierarchy remains challenging, particularly since these architectures rely on
positional or structural encodings for their expressivity and predictive
performance. To address this, we show that the recently proposed Edge
Transformer, a global attention model operating on node pairs instead of nodes,
has at least 3-WL expressive power. Empirically, we demonstrate that the Edge
Transformer surpasses other theoretically aligned architectures regarding
predictive performance while not relying on positional or structural encodings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_L/0/1/0/all/0/1&quot;&gt;Luis M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morris_C/0/1/0/all/0/1&quot;&gt;Christopher Morris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10139">
<title>Model Compression Techniques in Biometrics Applications: A Survey. (arXiv:2401.10139v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10139</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of deep learning algorithms has extensively empowered
humanity&apos;s task automatization capacity. However, the huge improvement in the
performance of these models is highly correlated with their increasing level of
complexity, limiting their usefulness in human-oriented applications, which are
usually deployed in resource-constrained devices. This led to the development
of compression techniques that drastically reduce the computational and memory
costs of deep learning models without significant performance degradation. This
paper aims to systematize the current literature on this topic by presenting a
comprehensive survey of model compression techniques in biometrics
applications, namely quantization, knowledge distillation and pruning. We
conduct a critical analysis of the comparative value of these techniques,
focusing on their advantages and disadvantages and presenting suggestions for
future work directions that can potentially improve the current methods.
Additionally, we discuss and analyze the link between model bias and model
compression, highlighting the need to direct compression research toward model
fairness in future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caldeira_E/0/1/0/all/0/1&quot;&gt;Eduarda Caldeira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_P/0/1/0/all/0/1&quot;&gt;Pedro C. Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huber_M/0/1/0/all/0/1&quot;&gt;Marco Huber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sequeira_A/0/1/0/all/0/1&quot;&gt;Ana F. Sequeira&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10148">
<title>Explicitly Disentangled Representations in Object-Centric Learning. (arXiv:2401.10148v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10148</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting structured representations from raw visual data is an important
and long-standing challenge in machine learning. Recently, techniques for
unsupervised learning of object-centric representations have raised growing
interest. In this context, enhancing the robustness of the latent features can
improve the efficiency and effectiveness of the training of downstream tasks. A
promising step in this direction is to disentangle the factors that cause
variation in the data. Previously, Invariant Slot Attention disentangled
position, scale, and orientation from the remaining features. Extending this
approach, we focus on separating the shape and texture components. In
particular, we propose a novel architecture that biases object-centric models
toward disentangling shape and texture components into two non-overlapping
subsets of the latent space dimensions. These subsets are known a priori, hence
before the training process. Experiments on a range of object-centric
benchmarks reveal that our approach achieves the desired disentanglement while
also numerically improving baseline performance in most cases. In addition, we
show that our method can generate novel textures for a specific object or
transfer textures between objects with distinct shapes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majellaro_R/0/1/0/all/0/1&quot;&gt;Riccardo Majellaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collu_J/0/1/0/all/0/1&quot;&gt;Jonathan Collu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plaat_A/0/1/0/all/0/1&quot;&gt;Aske Plaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moerland_T/0/1/0/all/0/1&quot;&gt;Thomas M. Moerland&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10158">
<title>DISTINQT: A Distributed Privacy Aware Learning Framework for QoS Prediction for Future Mobile and Wireless Networks. (arXiv:2401.10158v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.10158</link>
<description rdf:parseType="Literal">&lt;p&gt;Beyond 5G and 6G networks are expected to support new and challenging use
cases and applications that depend on a certain level of Quality of Service
(QoS) to operate smoothly. Predicting the QoS in a timely manner is of high
importance, especially for safety-critical applications as in the case of
vehicular communications. Although until recent years the QoS prediction has
been carried out by centralized Artificial Intelligence (AI) solutions, a
number of privacy, computational, and operational concerns have emerged.
Alternative solutions have been surfaced (e.g. Split Learning, Federated
Learning), distributing AI tasks of reduced complexity across nodes, while
preserving the privacy of the data. However, new challenges rise when it comes
to scalable distributed learning approaches, taking into account the
heterogeneous nature of future wireless networks. The current work proposes
DISTINQT, a privacy-aware distributed learning framework for QoS prediction.
Our framework supports multiple heterogeneous nodes, in terms of data types and
model architectures, by sharing computations across them. This, enables the
incorporation of diverse knowledge into a sole learning process that will
enhance the robustness and generalization capabilities of the final QoS
prediction model. DISTINQT also contributes to data privacy preservation by
encoding any raw input data into a non-linear latent representation before any
transmission. Evaluation results showcase that our framework achieves a
statistically identical performance compared to its centralized version and an
average performance improvement of up to 65% against six state-of-the-art
centralized baseline solutions in the Tele-Operated Driving use case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koursioumpas_N/0/1/0/all/0/1&quot;&gt;Nikolaos Koursioumpas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magoula_L/0/1/0/all/0/1&quot;&gt;Lina Magoula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stavrakakis_I/0/1/0/all/0/1&quot;&gt;Ioannis Stavrakakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonistioti_N/0/1/0/all/0/1&quot;&gt;Nancy Alonistioti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_Estevez_M/0/1/0/all/0/1&quot;&gt;M. A. Gutierrez-Estevez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalili_R/0/1/0/all/0/1&quot;&gt;Ramin Khalili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10178">
<title>Neural Echos: Depthwise Convolutional Filters Replicate Biological Receptive Fields. (arXiv:2401.10178v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10178</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we present evidence suggesting that depthwise convolutional
kernels are effectively replicating the structural intricacies of the
biological receptive fields observed in the mammalian retina. We provide
analytics of trained kernels from various state-of-the-art models
substantiating this evidence. Inspired by this intriguing discovery, we propose
an initialization scheme that draws inspiration from the biological receptive
fields. Experimental analysis of the ImageNet dataset with multiple CNN
architectures featuring depthwise convolutions reveals a marked enhancement in
the accuracy of the learned model when initialized with biologically derived
weights. This underlies the potential for biologically inspired computational
models to further our understanding of vision processing systems and to improve
the efficacy of convolutional networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babaiee_Z/0/1/0/all/0/1&quot;&gt;Zahra Babaiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiasari_P/0/1/0/all/0/1&quot;&gt;Peyman M. Kiasari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_D/0/1/0/all/0/1&quot;&gt;Daniela Rus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosu_R/0/1/0/all/0/1&quot;&gt;Radu Grosu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10189">
<title>Chem-FINESE: Validating Fine-Grained Few-shot Entity Extraction through Text Reconstruction. (arXiv:2401.10189v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10189</link>
<description rdf:parseType="Literal">&lt;p&gt;Fine-grained few-shot entity extraction in the chemical domain faces two
unique challenges. First, compared with entity extraction tasks in the general
domain, sentences from chemical papers usually contain more entities. Moreover,
entity extraction models usually have difficulty extracting entities of
long-tailed types. In this paper, we propose Chem-FINESE, a novel
sequence-to-sequence (seq2seq) based few-shot entity extraction approach, to
address these two challenges. Our Chem-FINESE has two components: a seq2seq
entity extractor to extract named entities from the input sentence and a
seq2seq self-validation module to reconstruct the original input sentence from
extracted entities. Inspired by the fact that a good entity extraction system
needs to extract entities faithfully, our new self-validation module leverages
entity extraction results to reconstruct the original input sentence. Besides,
we design a new contrastive loss to reduce excessive copying during the
extraction process. Finally, we release ChemNER+, a new fine-grained chemical
entity extraction dataset that is annotated by domain experts with the ChemNER
schema. Experiments in few-shot settings with both ChemNER+ and CHEMET datasets
show that our newly proposed framework has contributed up to 8.26% and 6.84%
absolute F1-score gains respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huimin Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10207">
<title>Eclectic Rule Extraction for Explainability of Deep Neural Network based Intrusion Detection Systems. (arXiv:2401.10207v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.10207</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses trust issues created from the ubiquity of black box
algorithms and surrogate explainers in Explainable Intrusion Detection Systems
(X-IDS). While Explainable Artificial Intelligence (XAI) aims to enhance
transparency, black box surrogate explainers, such as Local Interpretable
Model-Agnostic Explanation (LIME) and SHapley Additive exPlanation (SHAP), are
difficult to trust. The black box nature of these surrogate explainers makes
the process behind explanation generation opaque and difficult to understand.
To avoid this problem, one can use transparent white box algorithms such as
Rule Extraction (RE). There are three types of RE algorithms: pedagogical,
decompositional, and eclectic. Pedagogical methods offer fast but untrustworthy
white-box explanations, while decompositional RE provides trustworthy
explanations with poor scalability. This work explores eclectic rule
extraction, which strikes a balance between scalability and trustworthiness. By
combining techniques from pedagogical and decompositional approaches, eclectic
rule extraction leverages the advantages of both, while mitigating some of
their drawbacks. The proposed Hybrid X-IDS architecture features eclectic RE as
a white box surrogate explainer for black box Deep Neural Networks (DNN). The
presented eclectic RE algorithm extracts human-readable rules from hidden
layers, facilitating explainable and trustworthy rulesets. Evaluations on
UNSW-NB15 and CIC-IDS-2017 datasets demonstrate the algorithm&apos;s ability to
generate rulesets with 99.9% accuracy, mimicking DNN outputs. The contributions
of this work include the hybrid X-IDS architecture, the eclectic rule
extraction algorithm applicable to intrusion detection datasets, and a thorough
analysis of performance and explainability, demonstrating the trade-offs
involved in rule extraction speed and accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ables_J/0/1/0/all/0/1&quot;&gt;Jesse Ables&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Childers_N/0/1/0/all/0/1&quot;&gt;Nathaniel Childers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_W/0/1/0/all/0/1&quot;&gt;William Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sudip Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahimi_S/0/1/0/all/0/1&quot;&gt;Shahram Rahimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banicescu_I/0/1/0/all/0/1&quot;&gt;Ioana Banicescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seale_M/0/1/0/all/0/1&quot;&gt;Maria Seale&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10210">
<title>Mastery Guided Non-parametric Clustering to Scale-up Strategy Prediction. (arXiv:2401.10210v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.10210</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the strategy (sequence of concepts) that a student is likely to
use in problem-solving helps Adaptive Instructional Systems (AISs) better adapt
themselves to different types of learners based on their learning abilities.
This can lead to a more dynamic, engaging, and personalized experience for
students. To scale up training a prediction model (such as LSTMs) over
large-scale education datasets, we develop a non-parametric approach to cluster
symmetric instances in the data. Specifically, we learn a representation based
on Node2Vec that encodes symmetries over mastery or skill level since, to solve
a problem, it is natural that a student&apos;s strategy is likely to involve
concepts in which they have gained mastery. Using this representation, we use
DP-Means to group symmetric instances through a coarse-to-fine refinement of
the clusters. We apply our model to learn strategies for Math learning from
large-scale datasets from MATHia, a leading AIS for middle-school math
learning. Our results illustrate that our approach can consistently achieve
high accuracy using a small sample that is representative of the full dataset.
Further, we show that this approach helps us learn strategies with high
accuracy for students at different skill levels, i.e., leveraging symmetries
improves fairness in the prediction model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakya_A/0/1/0/all/0/1&quot;&gt;Anup Shakya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rus_V/0/1/0/all/0/1&quot;&gt;Vasile Rus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venugopal_D/0/1/0/all/0/1&quot;&gt;Deepak Venugopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10211">
<title>Improving PTM Site Prediction by Coupling of Multi-Granularity Structure and Multi-Scale Sequence Representation. (arXiv:2401.10211v1 [q-bio.QM])</title>
<link>http://arxiv.org/abs/2401.10211</link>
<description rdf:parseType="Literal">&lt;p&gt;Protein post-translational modification (PTM) site prediction is a
fundamental task in bioinformatics. Several computational methods have been
developed to predict PTM sites. However, existing methods ignore the structure
information and merely utilize protein sequences. Furthermore, designing a more
fine-grained structure representation learning method is urgently needed as PTM
is a biological event that occurs at the atom granularity. In this paper, we
propose a PTM site prediction method by Coupling of Multi-Granularity structure
and Multi-Scale sequence representation, PTM-CMGMS for brevity. Specifically,
multigranularity structure-aware representation learning is designed to learn
neighborhood structure representations at the amino acid, atom, and whole
protein granularity from AlphaFold predicted structures, followed by utilizing
contrastive learning to optimize the structure representations.Additionally,
multi-scale sequence representation learning is used to extract context
sequence information, and motif generated by aligning all context sequences of
PTM sites assists the prediction. Extensive experiments on three datasets show
that PTM-CMGMS outperforms the state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Menglu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lida Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wen Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10222">
<title>Supervised Fine-tuning in turn Improves Visual Foundation Models. (arXiv:2401.10222v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10222</link>
<description rdf:parseType="Literal">&lt;p&gt;Image-text training like CLIP has dominated the pretraining of vision
foundation models in recent years. Subsequent efforts have been made to
introduce region-level visual learning into CLIP&apos;s pretraining but face
scalability challenges due to the lack of large-scale region-level datasets.
Drawing inspiration from supervised fine-tuning (SFT) in natural language
processing such as instruction tuning, we explore the potential of fine-grained
SFT in enhancing the generation of vision foundation models after their
pretraining. Thus a two-stage method ViSFT (Vision SFT) is proposed to unleash
the fine-grained knowledge of vision foundation models. In ViSFT, the vision
foundation model is enhanced by performing visual joint learning on some
in-domain tasks and then tested on out-of-domain benchmarks. With updating
using ViSFT on 8 V100 GPUs in less than 2 days, a vision transformer with over
4.4B parameters shows improvements across various out-of-domain benchmarks
including vision and vision-linguistic scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xiaohu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yixiao Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yuying Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10225">
<title>ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce ChatQA, a family of conversational question
answering (QA) models, that obtain GPT-4 level accuracies. Specifically, we
propose a two-stage instruction tuning method that can significantly improve
the zero-shot conversational QA results from large language models (LLMs). To
handle retrieval in conversational QA, we fine-tune a dense retriever on a
multi-turn QA dataset, which provides comparable results to using the
state-of-the-art query rewriting model while largely reducing deployment cost.
Notably, our ChatQA-70B can outperform GPT-4 in terms of average score on 10
conversational QA datasets (54.14 vs. 53.90), without relying on any synthetic
data from OpenAI GPT models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1&quot;&gt;Rajarshi Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1&quot;&gt;Bryan Catanzaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.12508">
<title>From Procedures, Objects, Actors, Components, Services, to Agents -- A Comparative Analysis of the History and Evolution of Programming Abstractions. (arXiv:2112.12508v4 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2112.12508</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of this chapter is to propose some retrospective analysis of
the evolution of programming abstractions, from {\em procedures}, {\em
objects}, {\em actors}, {\em components}, {\em services}, up to {\em agents},
%have some compare concepts of software component and of agent (and multi-agent
system), %The method chosen is to by replacing them within a general historical
perspective. Some common referential with three axes/dimensions is chosen: {\em
action selection} at the level of one entity, {\em coupling flexibility}
between entities, and {\em abstraction level}. We indeed may observe some
continuous quest for higher flexibility (through notions such as {\em late
binding}, or {\em reification} of {\em connections}) and higher level of {\em
abstraction}. Concepts of components, services and agents have some common
objectives (notably, {\em software modularity and reconfigurability}), with
multi-agent systems raising further concepts of {\em autonomy} and {\em
coordination}. notably through the notion of {\em auto-organization} and the
use of {\em knowledge}. We hope that this analysis helps at highlighting some
of the basic forces motivating the progress of programming abstractions and
therefore that it may provide some seeds for the reflection about future
programming abstractions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briot_J/0/1/0/all/0/1&quot;&gt;Jean-Pierre Briot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.13125">
<title>Normality-Guided Distributional Reinforcement Learning for Continuous Control. (arXiv:2208.13125v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.13125</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning a predictive model of the mean return, or value function, plays a
critical role in many reinforcement learning algorithms. Distributional
reinforcement learning (DRL) has been shown to improve performance by modeling
the value distribution, not just the mean. We study the value distribution in
several continuous control tasks and find that the learned value distribution
is empirical quite close to normal. We design a method that exploits this
property, employ variances predicted from a variance network, along with
returns, to analytically compute target quantile bars representing a normal for
our distributional value function. In addition, we propose a policy update
strategy based on the correctness as measured by structural characteristics of
the value distribution not present in the standard value function. The approach
we outline is compatible with many DRL structures. We use two representative
on-policy algorithms, PPO and TRPO, as testbeds. Our method yields
statistically significant improvements in 10 out of 16 continuous task
settings, while utilizing a reduced number of weights and achieving faster
training time compared to an ensemble-based method for quantifying value
distribution uncertainty.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_J/0/1/0/all/0/1&quot;&gt;Ju-Seung Byun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perrault_A/0/1/0/all/0/1&quot;&gt;Andrew Perrault&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.15629">
<title>Language Control Diffusion: Efficiently Scaling through Space, Time, and Tasks. (arXiv:2210.15629v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.15629</link>
<description rdf:parseType="Literal">&lt;p&gt;Training generalist agents is difficult across several axes, requiring us to
deal with high-dimensional inputs (space), long horizons (time), and
generalization to novel tasks. Recent advances with architectures have allowed
for improved scaling along one or two of these axes, but are still
computationally prohibitive to use. In this paper, we propose to address all
three axes by leveraging \textbf{L}anguage to \textbf{C}ontrol
\textbf{D}iffusion models as a hierarchical planner conditioned on language
(LCD). We effectively and efficiently scale diffusion models for planning in
extended temporal, state, and task dimensions to tackle long horizon control
problems conditioned on natural language instructions, as a step towards
generalist agents. Comparing LCD with other state-of-the-art models on the
CALVIN language robotics benchmark finds that LCD outperforms other SOTA
methods in multi-task success rates, whilst improving inference speed over
other comparable diffusion models by 3.3x~15x. We show that LCD can
successfully leverage the unique strength of diffusion models to produce
coherent long range plans while addressing their weakness in generating
low-level details and control.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Edwin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yujie Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;William Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11086">
<title>An Embarrassingly Simple Baseline for Imbalanced Semi-Supervised Learning. (arXiv:2211.11086v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11086</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised learning (SSL) has shown great promise in leveraging
unlabeled data to improve model performance. While standard SSL assumes uniform
data distribution, we consider a more realistic and challenging setting called
imbalanced SSL, where imbalanced class distributions occur in both labeled and
unlabeled data. Although there are existing endeavors to tackle this challenge,
their performance degenerates when facing severe imbalance since they can not
reduce the class imbalance sufficiently and effectively. In this paper, we
study a simple yet overlooked baseline -- SimiS -- which tackles data imbalance
by simply supplementing labeled data with pseudo-labels, according to the
difference in class distribution from the most frequent class. Such a simple
baseline turns out to be highly effective in reducing class imbalance. It
outperforms existing methods by a significant margin, e.g., 12.8%, 13.6%, and
16.7% over previous SOTA on CIFAR100-LT, FOOD101-LT, and ImageNet127
respectively. The reduced imbalance results in faster convergence and better
pseudo-label accuracy of SimiS. The simplicity of our method also makes it
possible to be combined with other re-balancing techniques to improve the
performance further. Moreover, our method shows great robustness to a wide
range of data distributions, which holds enormous potential in practice. Code
will be publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yue Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yidong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jindong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiele_B/0/1/0/all/0/1&quot;&gt;Bernt Schiele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xing Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savvides_M/0/1/0/all/0/1&quot;&gt;Marios Savvides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.13118">
<title>Decision Diagram-Based Branch-and-Bound with Caching for Dominance and Suboptimality Detection. (arXiv:2211.13118v5 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2211.13118</link>
<description rdf:parseType="Literal">&lt;p&gt;The branch-and-bound algorithm based on decision diagrams introduced by
Bergman et al. in 2016 is a framework for solving discrete optimization
problems with a dynamic programming formulation. It works by compiling a series
of bounded-width decision diagrams that can provide lower and upper bounds for
any given subproblem. Eventually, every part of the search space will be either
explored or pruned by the algorithm, thus proving optimality. This paper
presents new ingredients to speed up the search by exploiting the structure of
dynamic programming models. The key idea is to prevent the repeated expansion
of nodes corresponding to the same dynamic programming states by querying
expansion thresholds cached throughout the search. These thresholds are based
on dominance relations between partial solutions previously found and on the
pruning inequalities of the filtering techniques introduced by Gillard et al.
in 2021. Computational experiments show that the pruning brought by this
caching mechanism allows significantly reducing the number of nodes expanded by
the algorithm. This results in more benchmark instances of difficult
optimization problems being solved in less time while using narrower decision
diagrams.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coppe_V/0/1/0/all/0/1&quot;&gt;Vianney Copp&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gillard_X/0/1/0/all/0/1&quot;&gt;Xavier Gillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaus_P/0/1/0/all/0/1&quot;&gt;Pierre Schaus&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.11854">
<title>Data-Centric Artificial Intelligence. (arXiv:2212.11854v4 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2212.11854</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-centric artificial intelligence (data-centric AI) represents an emerging
paradigm emphasizing that the systematic design and engineering of data is
essential for building effective and efficient AI-based systems. The objective
of this article is to introduce practitioners and researchers from the field of
Information Systems (IS) to data-centric AI. We define relevant terms, provide
key characteristics to contrast the data-centric paradigm to the model-centric
one, and introduce a framework for data-centric AI. We distinguish data-centric
AI from related concepts and discuss its longer-term implications for the IS
community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakubik_J/0/1/0/all/0/1&quot;&gt;Johannes Jakubik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vossing_M/0/1/0/all/0/1&quot;&gt;Michael V&amp;#xf6;ssing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuhl_N/0/1/0/all/0/1&quot;&gt;Niklas K&amp;#xfc;hl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walk_J/0/1/0/all/0/1&quot;&gt;Jannis Walk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Satzger_G/0/1/0/all/0/1&quot;&gt;Gerhard Satzger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02472">
<title>ESD: Expected Squared Difference as a Tuning-Free Trainable Calibration Measure. (arXiv:2303.02472v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02472</link>
<description rdf:parseType="Literal">&lt;p&gt;Studies have shown that modern neural networks tend to be poorly calibrated
due to over-confident predictions. Traditionally, post-processing methods have
been used to calibrate the model after training. In recent years, various
trainable calibration measures have been proposed to incorporate them directly
into the training process. However, these methods all incorporate internal
hyperparameters, and the performance of these calibration objectives relies on
tuning these hyperparameters, incurring more computational costs as the size of
neural networks and datasets become larger. As such, we present Expected
Squared Difference (ESD), a tuning-free (i.e., hyperparameter-free) trainable
calibration objective loss, where we view the calibration error from the
perspective of the squared difference between the two expectations. With
extensive experiments on several architectures (CNNs, Transformers) and
datasets, we demonstrate that (1) incorporating ESD into the training improves
model calibration in various batch size settings without the need for internal
hyperparameter tuning, (2) ESD yields the best-calibrated results compared with
previous approaches, and (3) ESD drastically improves the computational costs
required for calibration during training due to the absence of internal
hyperparameter. The code is publicly accessible at
https://github.com/hee-suk-yoon/ESD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hee Suk Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tee_J/0/1/0/all/0/1&quot;&gt;Joshua Tian Jin Tee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_E/0/1/0/all/0/1&quot;&gt;Eunseop Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sunjae Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_G/0/1/0/all/0/1&quot;&gt;Gwangsu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoo_C/0/1/0/all/0/1&quot;&gt;Chang D. Yoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12307">
<title>Curvature-Balanced Feature Manifold Learning for Long-Tailed Classification. (arXiv:2303.12307v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12307</link>
<description rdf:parseType="Literal">&lt;p&gt;To address the challenges of long-tailed classification, researchers have
proposed several approaches to reduce model bias, most of which assume that
classes with few samples are weak classes. However, recent studies have shown
that tail classes are not always hard to learn, and model bias has been
observed on sample-balanced datasets, suggesting the existence of other factors
that affect model bias. In this work, we systematically propose a series of
geometric measurements for perceptual manifolds in deep neural networks, and
then explore the effect of the geometric characteristics of perceptual
manifolds on classification difficulty and how learning shapes the geometric
characteristics of perceptual manifolds. An unanticipated finding is that the
correlation between the class accuracy and the separation degree of perceptual
manifolds gradually decreases during training, while the negative correlation
with the curvature gradually increases, implying that curvature imbalance leads
to model bias. Therefore, we propose curvature regularization to facilitate the
model to learn curvature-balanced and flatter perceptual manifolds. Evaluations
on multiple long-tailed and non-long-tailed datasets show the excellent
performance and exciting generality of our approach, especially in achieving
significant performance improvements based on current state-of-the-art
techniques. Our work opens up a geometric analysis perspective on model bias
and reminds researchers to pay attention to model bias on non-long-tailed and
even sample-balanced datasets. The code and model will be made public.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yanbiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_L/0/1/0/all/0/1&quot;&gt;Licheng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Shuyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lingling Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01300">
<title>On Mitigating the Utility-Loss in Differentially Private Learning: A new Perspective by a Geometrically Inspired Kernel Approach. (arXiv:2304.01300v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01300</link>
<description rdf:parseType="Literal">&lt;p&gt;Privacy-utility tradeoff remains as one of the fundamental issues of
differentially private machine learning. This paper introduces a geometrically
inspired kernel-based approach to mitigate the accuracy-loss issue in
classification. In this approach, a representation of the affine hull of given
data points is learned in Reproducing Kernel Hilbert Spaces (RKHS). This leads
to a novel distance measure that hides privacy-sensitive information about
individual data points and improves the privacy-utility tradeoff via
significantly reducing the risk of membership inference attacks. The
effectiveness of the approach is demonstrated through experiments on MNIST
dataset, Freiburg groceries dataset, and a real biomedical dataset. It is
verified that the approach remains computationally practical. The application
of the approach to federated learning is considered and it is observed that the
accuracy-loss due to data being distributed is either marginal or not
significantly high.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_M/0/1/0/all/0/1&quot;&gt;Mohit Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moser_B/0/1/0/all/0/1&quot;&gt;Bernhard A. Moser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_L/0/1/0/all/0/1&quot;&gt;Lukas Fischer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.04640">
<title>NeuroBench: A Framework for Benchmarking Neuromorphic Computing Algorithms and Systems. (arXiv:2304.04640v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.04640</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic computing shows promise for advancing computing efficiency and
capabilities of AI applications using brain-inspired principles. However, the
neuromorphic research field currently lacks standardized benchmarks, making it
difficult to accurately measure technological advancements, compare performance
with conventional methods, and identify promising future research directions.
Prior neuromorphic computing benchmark efforts have not seen widespread
adoption due to a lack of inclusive, actionable, and iterative benchmark design
and guidelines. To address these shortcomings, we present NeuroBench: a
benchmark framework for neuromorphic computing algorithms and systems.
NeuroBench is a collaboratively-designed effort from an open community of
nearly 100 co-authors across over 50 institutions in industry and academia,
aiming to provide a representative structure for standardizing the evaluation
of neuromorphic approaches. The NeuroBench framework introduces a common set of
tools and systematic methodology for inclusive benchmark measurement,
delivering an objective reference framework for quantifying neuromorphic
approaches in both hardware-independent (algorithm track) and
hardware-dependent (system track) settings. In this article, we present initial
performance baselines across various model architectures on the algorithm track
and outline the system track benchmark tasks and guidelines. NeuroBench is
intended to continually expand its benchmarks and features to foster and track
the progress made by the research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yik_J/0/1/0/all/0/1&quot;&gt;Jason Yik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berghe_K/0/1/0/all/0/1&quot;&gt;Korneel Van den Berghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blanken_D/0/1/0/all/0/1&quot;&gt;Douwe den Blanken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouhadjar_Y/0/1/0/all/0/1&quot;&gt;Younes Bouhadjar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabre_M/0/1/0/all/0/1&quot;&gt;Maxime Fabre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hueber_P/0/1/0/all/0/1&quot;&gt;Paul Hueber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleyko_D/0/1/0/all/0/1&quot;&gt;Denis Kleyko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacik_Nelson_N/0/1/0/all/0/1&quot;&gt;Noah Pacik-Nelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Pao-Sheng Vincent Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1&quot;&gt;Guangzhi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shenqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Biyan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Soikat Hasan Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joseph_G/0/1/0/all/0/1&quot;&gt;George Vathakkattil Joseph&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leto_B/0/1/0/all/0/1&quot;&gt;Benedetto Leto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheli_A/0/1/0/all/0/1&quot;&gt;Aurora Micheli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Anurag Kumar Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lenz_G/0/1/0/all/0/1&quot;&gt;Gregor Lenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_Z/0/1/0/all/0/1&quot;&gt;Zergham Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akl_M/0/1/0/all/0/1&quot;&gt;Mahmoud Akl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anderson_B/0/1/0/all/0/1&quot;&gt;Brian Anderson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreou_A/0/1/0/all/0/1&quot;&gt;Andreas G. Andreou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bartolozzi_C/0/1/0/all/0/1&quot;&gt;Chiara Bartolozzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_A/0/1/0/all/0/1&quot;&gt;Arindam Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdan_P/0/1/0/all/0/1&quot;&gt;Petrut Bogdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bohte_S/0/1/0/all/0/1&quot;&gt;Sander Bohte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buckley_S/0/1/0/all/0/1&quot;&gt;Sonia Buckley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cauwenberghs_G/0/1/0/all/0/1&quot;&gt;Gert Cauwenberghs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chicca_E/0/1/0/all/0/1&quot;&gt;Elisabetta Chicca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corradi_F/0/1/0/all/0/1&quot;&gt;Federico Corradi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Croon_G/0/1/0/all/0/1&quot;&gt;Guido de Croon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danielescu_A/0/1/0/all/0/1&quot;&gt;Andreea Danielescu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daram_A/0/1/0/all/0/1&quot;&gt;Anurag Daram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_M/0/1/0/all/0/1&quot;&gt;Mike Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirag_Y/0/1/0/all/0/1&quot;&gt;Yigit Demirag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eshraghian_J/0/1/0/all/0/1&quot;&gt;Jason Eshraghian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fischer_T/0/1/0/all/0/1&quot;&gt;Tobias Fischer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forest_J/0/1/0/all/0/1&quot;&gt;Jeremy Forest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fra_V/0/1/0/all/0/1&quot;&gt;Vittorio Fra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furber_S/0/1/0/all/0/1&quot;&gt;Steve Furber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furlong_P/0/1/0/all/0/1&quot;&gt;P. Michael Furlong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilpin_W/0/1/0/all/0/1&quot;&gt;William Gilpin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilra_A/0/1/0/all/0/1&quot;&gt;Aditya Gilra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonzalez_H/0/1/0/all/0/1&quot;&gt;Hector A. Gonzalez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Indiveri_G/0/1/0/all/0/1&quot;&gt;Giacomo Indiveri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Siddharth Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karia_V/0/1/0/all/0/1&quot;&gt;Vedant Karia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khacef_L/0/1/0/all/0/1&quot;&gt;Lyes Khacef&lt;/a&gt;, et al. (49 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09048">
<title>CodeKGC: Code Language Model for Generative Knowledge Graph Construction. (arXiv:2304.09048v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09048</link>
<description rdf:parseType="Literal">&lt;p&gt;Current generative knowledge graph construction approaches usually fail to
capture structural knowledge by simply flattening natural language into
serialized texts or a specification language. However, large generative
language model trained on structured data such as code has demonstrated
impressive capability in understanding natural language for structural
prediction and reasoning tasks. Intuitively, we address the task of generative
knowledge graph construction with code language model: given a code-format
natural language input, the target is to generate triples which can be
represented as code completion tasks. Specifically, we develop schema-aware
prompts that effectively utilize the semantic structure within the knowledge
graph. As code inherently possesses structure, such as class and function
definitions, it serves as a useful model for prior semantic structural
knowledge. Furthermore, we employ a rationale-enhanced generation method to
boost the performance. Rationales provide intermediate steps, thereby improving
knowledge extraction abilities. Experimental results indicate that the proposed
approach can obtain better performance on benchmark datasets compared with
baselines. Code and datasets are available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_Z/0/1/0/all/0/1&quot;&gt;Zhen Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jing Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yinuo Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_F/0/1/0/all/0/1&quot;&gt;Feiyu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_W/0/1/0/all/0/1&quot;&gt;Wei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11065">
<title>Conversational Process Modeling: Can Generative AI Empower Domain Experts in Creating and Redesigning Process Models?. (arXiv:2304.11065v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11065</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-driven chatbots such as ChatGPT have caused a tremendous hype lately. For
BPM applications, several applications for AI-driven chatbots have been
identified to be promising to generate business value, including explanation of
process mining outcomes and preparation of input data. However, a systematic
analysis of chatbots for their support of conversational process modeling as a
process-oriented capability is missing. This work aims at closing this gap by
providing a systematic analysis of existing chatbots. Application scenarios are
identified along the process life cycle. Then a systematic literature review on
conversational process modeling is performed, resulting in a taxonomy of
application scenarios for conversational process modeling, including
paraphrasing and improvement of process descriptions. In addition, this work
suggests and applies an evaluation method for the output of AI-driven chatbots
with respect to completeness and correctness of the process models. This method
consists of a set of KPIs on a test set, a set of prompts for task and control
flow extraction, as well as a survey with users. Based on the literature and
the evaluation, recommendations for the usage (practical implications) and
further development (research directions) of conversational process modeling
are derived.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klievtsova_N/0/1/0/all/0/1&quot;&gt;Nataliia Klievtsova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benzin_J/0/1/0/all/0/1&quot;&gt;Janik-Vasily Benzin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampik_T/0/1/0/all/0/1&quot;&gt;Timotheus Kampik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mangler_J/0/1/0/all/0/1&quot;&gt;Juergen Mangler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinderle_Ma_S/0/1/0/all/0/1&quot;&gt;Stefanie Rinderle-Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12788">
<title>GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. (arXiv:2305.12788v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12788</link>
<description rdf:parseType="Literal">&lt;p&gt;Clinical predictive models often rely on patients&apos; electronic health records
(EHR), but integrating medical knowledge to enhance predictions and
decision-making is challenging. This is because personalized predictions
require personalized knowledge graphs (KGs), which are difficult to generate
from patient EHR data. To address this, we propose \textsc{GraphCare}, an
open-world framework that uses external KGs to improve EHR-based predictions.
Our method extracts knowledge from large language models (LLMs) and external
biomedical KGs to build patient-specific KGs, which are then used to train our
proposed Bi-attention AugmenTed (BAT) graph neural network (GNN) for healthcare
predictions. On two public datasets, MIMIC-III and MIMIC-IV, \textsc{GraphCare}
surpasses baselines in four vital healthcare prediction tasks: mortality,
readmission, length of stay (LOS), and drug recommendation. On MIMIC-III, it
boosts AUROC by 17.6\% and 6.6\% for mortality and readmission, and F1-score by
7.9\% and 10.8\% for LOS and drug recommendation, respectively. Notably,
\textsc{GraphCare} demonstrates a substantial edge in scenarios with limited
data availability. Our findings highlight the potential of using external KGs
in healthcare prediction tasks and demonstrate the promise of
\textsc{GraphCare} in generating personalized KGs for promoting personalized
medicine.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_P/0/1/0/all/0/1&quot;&gt;Pengcheng Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Cao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cross_A/0/1/0/all/0/1&quot;&gt;Adam Cross&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jimeng Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13971">
<title>Grammar-Constrained Decoding for Structured NLP Tasks without Finetuning. (arXiv:2305.13971v6 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13971</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their impressive performance, large language models (LMs) still
struggle with reliably generating complex output structures when not finetuned
to follow the required output format exactly. To address this issue,
grammar-constrained decoding (GCD) can be used to control the generation of
LMs, guaranteeing that the output follows a given structure. Most existing GCD
methods are, however, limited to specific tasks, such as parsing or code
generation. In this work, we demonstrate that formal grammars can describe the
output space for a much wider range of tasks and argue that GCD can serve as a
unified framework for structured NLP tasks in general. For increased
flexibility, we introduce input-dependent grammars, which allow the grammar to
depend on the input and thus enable the generation of different output
structures for different inputs. We then empirically demonstrate the power and
flexibility of GCD-enhanced LMs on (1) information extraction, (2) entity
disambiguation, and (3) constituency parsing. Our results indicate that
grammar-constrained LMs substantially outperform unconstrained LMs or even beat
task-specific finetuned models. Grammar constraints thus hold great promise for
harnessing off-the-shelf LMs for a wide range of structured NLP tasks,
especially where training data is scarce or finetuning is expensive. Code and
data: https://github.com/epfl-dlab/GCD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1&quot;&gt;Saibo Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Josifoski_M/0/1/0/all/0/1&quot;&gt;Martin Josifoski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peyrard_M/0/1/0/all/0/1&quot;&gt;Maxime Peyrard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_R/0/1/0/all/0/1&quot;&gt;Robert West&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17198">
<title>A Model-Based Solution to the Offline Multi-Agent Reinforcement Learning Coordination Problem. (arXiv:2305.17198v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17198</link>
<description rdf:parseType="Literal">&lt;p&gt;Training multiple agents to coordinate is an essential problem with
applications in robotics, game theory, economics, and social sciences. However,
most existing Multi-Agent Reinforcement Learning (MARL) methods are online and
thus impractical for real-world applications in which collecting new
interactions is costly or dangerous. While these algorithms should leverage
offline data when available, doing so gives rise to what we call the offline
coordination problem. Specifically, we identify and formalize the strategy
agreement (SA) and the strategy fine-tuning (SFT) coordination challenges, two
issues at which current offline MARL algorithms fail. Concretely, we reveal
that the prevalent model-free methods are severely deficient and cannot handle
coordination-intensive offline multi-agent tasks in either toy or MuJoCo
domains. To address this setback, we emphasize the importance of inter-agent
interactions and propose the very first model-based offline MARL method. Our
resulting algorithm, Model-based Offline Multi-Agent Proximal Policy
Optimization (MOMA-PPO) generates synthetic interaction data and enables agents
to converge on a strategy while fine-tuning their policies accordingly. This
simple model-based solution solves the coordination-intensive offline tasks,
significantly outperforming the prevalent model-free methods even under severe
partial observability and with learned world models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barde_P/0/1/0/all/0/1&quot;&gt;Paul Barde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foerster_J/0/1/0/all/0/1&quot;&gt;Jakob Foerster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Amy Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00323">
<title>Thought Cloning: Learning to Think while Acting by Imitating Human Thinking. (arXiv:2306.00323v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00323</link>
<description rdf:parseType="Literal">&lt;p&gt;Language is often considered a key aspect of human thinking, providing us
with exceptional abilities to generalize, explore, plan, replan, and adapt to
new situations. However, Reinforcement Learning (RL) agents are far from
human-level performance in any of these abilities. We hypothesize one reason
for such cognitive deficiencies is that they lack the benefits of thinking in
language and that we can improve AI agents by training them to think like
humans do. We introduce a novel Imitation Learning framework, Thought Cloning,
where the idea is to not just clone the behaviors of human demonstrators, but
also the thoughts humans have as they perform these behaviors. While we expect
Thought Cloning to truly shine at scale on internet-sized datasets of humans
thinking out loud while acting (e.g. online videos with transcripts), here we
conduct experiments in a domain where the thinking and action data are
synthetically generated. Results reveal that Thought Cloning learns much faster
than Behavioral Cloning and its performance advantage grows the further out of
distribution test tasks are, highlighting its ability to better handle novel
situations. Thought Cloning also provides important benefits for AI Safety and
Interpretability, and makes it easier to debug and improve AI. Because we can
observe the agent&apos;s thoughts, we can (1) more easily diagnose why things are
going wrong, making it easier to fix the problem, (2) steer the agent by
correcting its thinking, or (3) prevent it from doing unsafe things it plans to
do. Overall, by training agents how to think as well as behave, Thought Cloning
creates safer, more powerful agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengran Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03354">
<title>Simulation-Based Counterfactual Causal Discovery on Real World Driver Behaviour. (arXiv:2306.03354v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03354</link>
<description rdf:parseType="Literal">&lt;p&gt;Being able to reason about how one&apos;s behaviour can affect the behaviour of
others is a core skill required of intelligent driving agents. Despite this,
the state of the art struggles to meet the need of agents to discover causal
links between themselves and others. Observational approaches struggle because
of the non-stationarity of causal links in dynamic environments, and the
sparsity of causal interactions while requiring the approaches to work in an
online fashion. Meanwhile interventional approaches are impractical as a
vehicle cannot experiment with its actions on a public road. To counter the
issue of non-stationarity we reformulate the problem in terms of extracted
events, while the previously mentioned restriction upon interventions can be
overcome with the use of counterfactual simulation. We present three variants
of the proposed counterfactual causal discovery method and evaluate these
against state of the art observational temporal causal discovery methods across
3396 causal scenes extracted from a real world driving dataset. We find that
the proposed method significantly outperforms the state of the art on the
proposed task quantitatively and can offer additional insights by comparing the
outcome of an alternate series of decisions in a way that observational and
interventional approaches cannot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_R/0/1/0/all/0/1&quot;&gt;Rhys Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kunze_L/0/1/0/all/0/1&quot;&gt;Lars Kunze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05535">
<title>Detecting Check-Worthy Claims in Political Debates, Speeches, and Interviews Using Audio Data. (arXiv:2306.05535v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05535</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing tools to automatically detect check-worthy claims in political
debates and speeches can greatly help moderators of debates, journalists, and
fact-checkers. While previous work on this problem has focused exclusively on
the text modality, here we explore the utility of the audio modality as an
additional input. We create a new multimodal dataset (text and audio in
English) containing 48 hours of speech from past political debates in the USA.
We then experimentally demonstrate that, in the case of multiple speakers,
adding the audio modality yields sizable improvements over using the text
modality alone; moreover, an audio-only model could outperform a text-only one
for a single speaker. With the aim to enable future research, we make all our
data and code publicly available at
https://github.com/petar-iv/audio-checkworthiness-detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanov_P/0/1/0/all/0/1&quot;&gt;Petar Ivanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koychev_I/0/1/0/all/0/1&quot;&gt;Ivan Koychev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hardalov_M/0/1/0/all/0/1&quot;&gt;Momchil Hardalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakov_P/0/1/0/all/0/1&quot;&gt;Preslav Nakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09138">
<title>Exploiting Uncertainty for Querying Inconsistent Description Logics Knowledge Bases. (arXiv:2306.09138v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09138</link>
<description rdf:parseType="Literal">&lt;p&gt;The necessity to manage inconsistency in Description Logics Knowledge
Bases~(KBs) has come to the fore with the increasing importance gained by the
Semantic Web, where information comes from different sources that constantly
change their content and may contain contradictory descriptions when considered
either alone or together. Classical reasoning algorithms do not handle
inconsistent KBs, forcing the debugging of the KB in order to remove the
inconsistency. In this paper, we exploit an existing probabilistic semantics
called DISPONTE to overcome this problem and allow queries also in case of
inconsistent KBs. We implemented our approach in the reasoners TRILL and BUNDLE
and empirically tested the validity of our proposal. Moreover, we formally
compare the presented approach to that of the repair semantics, one of the most
established semantics when considering DL reasoning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zese_R/0/1/0/all/0/1&quot;&gt;Riccardo Zese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamma_E/0/1/0/all/0/1&quot;&gt;Evelina Lamma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riguzzi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Riguzzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02140">
<title>Towards Open Federated Learning Platforms: Survey and Vision from Technical and Legal Perspectives. (arXiv:2307.02140v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02140</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional Federated Learning (FL) follows a server-domincated cooperation
paradigm which narrows the application scenarios of FL and decreases the
enthusiasm of data holders to participate. To fully unleash the potential of
FL, we advocate rethinking the design of current FL frameworks and extending it
to a more generalized concept: Open Federated Learning Platforms. We propose
two reciprocal cooperation frameworks for FL to achieve this: query-based FL
and contract-based FL. In this survey, we conduct a comprehensive review of the
feasibility of constructing an open FL platform from both technical and legal
perspectives. We begin by reviewing the definition of FL and summarizing its
inherent limitations, including server-client coupling, low model reusability,
and non-public. In the query-based FL platform, which is an open model sharing
and reusing platform empowered by the community for model mining, we explore a
wide range of valuable topics, including the availability of up-to-date model
repositories for model querying, legal compliance analysis between different
model licenses, and copyright issues and intellectual property protection in
model reusing. In particular, we introduce a novel taxonomy to streamline the
analysis of model license compatibility in FL studies that involve batch model
reusing methods, including combination, amalgamation, distillation, and
generation. This taxonomy provides a systematic framework for identifying the
corresponding clauses of licenses and facilitates the identification of
potential legal implications and restrictions when reusing models. Through this
survey, we uncover the the current dilemmas faced by FL and advocate for the
development of sustainable open FL platforms. We aim to provide guidance for
establishing such platforms in the future, while identifying potential problems
and challenges that need to be addressed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_M/0/1/0/all/0/1&quot;&gt;Moming Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12547">
<title>Knapsack: Connectedness, Path, and Shortest-Path. (arXiv:2307.12547v3 [cs.DS] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12547</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the knapsack problem with graph theoretic constraints. That is, we
assume that there exists a graph structure on the set of items of knapsack and
the solution also needs to satisfy certain graph theoretic properties on top of
knapsack constraints. In particular, we need to compute in the connected
knapsack problem a connected subset of items which has maximum value subject to
the size of knapsack constraint. We show that this problem is strongly
NP-complete even for graphs of maximum degree four and NP-complete even for
star graphs. On the other hand, we develop an algorithm running in time
$O\left(2^{tw\log tw}\cdot\text{poly}(\min\{s^2,d^2\})\right)$ where $tw,s,d$
are respectively treewidth of the graph, size, and target value of the
knapsack. We further exhibit a $(1-\epsilon)$ factor approximation algorithm
running in time $O\left(2^{tw\log tw}\cdot\text{poly}(n,1/\epsilon)\right)$ for
every $\epsilon&amp;gt;0$. We show similar results for several other graph theoretic
properties, namely path and shortest-path under the problem names path-knapsack
and shortestpath-knapsack. Our results seems to indicate that
connected-knapsack is computationally hardest followed by path-knapsack and
shortestpath-knapsack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1&quot;&gt;Palash Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolay_S/0/1/0/all/0/1&quot;&gt;Sudeshna Kolay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sipra Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13269">
<title>LoraHub: Efficient Cross-Task Generalization via Dynamic LoRA Composition. (arXiv:2307.13269v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13269</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank adaptations (LoRA) are often employed to fine-tune large language
models (LLMs) for new tasks. This paper investigates LoRA composability for
cross-task generalization and introduces LoraHub, a simple framework devised
for the purposive assembly of LoRA modules trained on diverse given tasks, with
the objective of achieving adaptable performance on unseen tasks. With just a
few examples from a new task, LoraHub can fluidly combine multiple LoRA
modules, eliminating the need for human expertise and assumptions. Notably, the
composition requires neither additional model parameters nor gradients.
Empirical results on the Big-Bench Hard benchmark suggest that LoraHub, while
not surpassing the performance of in-context learning, offers a notable
performance-efficiency trade-off in few-shot scenarios by employing a
significantly reduced number of tokens per example during inference. Notably,
LoraHub establishes a better upper bound compared to in-context learning when
paired with different demonstration examples, demonstrating its potential for
future development. Our vision is to establish a platform for LoRA modules,
empowering users to share their trained LoRA modules. This collaborative
approach facilitates the seamless application of LoRA modules to novel tasks,
contributing to an adaptive ecosystem. Our code is available at
https://github.com/sail-sg/lorahub, and all the pre-trained LoRA modules are
released at https://huggingface.co/lorahub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengsong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_T/0/1/0/all/0/1&quot;&gt;Tianyu Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Chao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Min Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13275">
<title>CTAGE: Curvature-Based Topology-Aware Graph Embedding for Learning Molecular Representations. (arXiv:2307.13275v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13275</link>
<description rdf:parseType="Literal">&lt;p&gt;AI-driven drug design relies significantly on predicting molecular
properties, which is a complex task. In current approaches, the most commonly
used feature representations for training deep neural network models are based
on SMILES and molecular graphs. While these methods are concise and efficient,
they have limitations in capturing complex spatial information. Recently,
researchers have recognized the importance of incorporating three-dimensional
information of molecular structures into models. However, capturing spatial
information requires the introduction of additional units in the generator,
bringing additional design and computational costs. Therefore, it is necessary
to develop a method for predicting molecular properties that effectively
combines spatial structural information while maintaining the simplicity and
efficiency of graph neural networks. In this work, we propose an embedding
approach CTAGE, utilizing $k$-hop discrete Ricci curvature to extract
structural insights from molecular graph data. This effectively integrates
spatial structural information while preserving the training complexity of the
network. Experimental results indicate that introducing node curvature
significantly improves the performance of current graph neural network
frameworks, validating that the information from k-hop node curvature
effectively reflects the relationship between molecular structure and function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zheng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xian Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03200">
<title>Uncovering local aggregated air quality index with smartphone captured images leveraging efficient deep convolutional neural network. (arXiv:2308.03200v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03200</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevalence and mobility of smartphones make these a widely used tool for
environmental health research. However, their potential for determining
aggregated air quality index (AQI) based on PM2.5 concentration in specific
locations remains largely unexplored in the existing literature. In this paper,
we thoroughly examine the challenges associated with predicting
location-specific PM2.5 concentration using images taken with smartphone
cameras. The focus of our study is on Dhaka, the capital of Bangladesh, due to
its significant air pollution levels and the large population exposed to it.
Our research involves the development of a Deep Convolutional Neural Network
(DCNN), which we train using over a thousand outdoor images taken and
annotated. These photos are captured at various locations in Dhaka, and their
labels are based on PM2.5 concentration data obtained from the local US
consulate, calculated using the NowCast algorithm. Through supervised learning,
our model establishes a correlation index during training, enhancing its
ability to function as a Picture-based Predictor of PM2.5 Concentration (PPPC).
This enables the algorithm to calculate an equivalent daily averaged AQI index
from a smartphone image. Unlike, popular overly parameterized models, our model
shows resource efficiency since it uses fewer parameters. Furthermore, test
results indicate that our model outperforms popular models like ViT and INN, as
well as popular CNN-based models such as VGG19, ResNet50, and MobileNetV2, in
predicting location-specific PM2.5 concentration. Our dataset is the first
publicly available collection that includes atmospheric images and
corresponding PM2.5 measurements from Dhaka. Our codes and dataset are
available at https://github.com/lepotatoguy/aqi.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_J/0/1/0/all/0/1&quot;&gt;Joyanta Jyoti Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Farhadul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_R/0/1/0/all/0/1&quot;&gt;Raima Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhidi_N/0/1/0/all/0/1&quot;&gt;Nowsin Kabir Rhidi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newaz_S/0/1/0/all/0/1&quot;&gt;Sarfaraz Newaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manab_M/0/1/0/all/0/1&quot;&gt;Meem Arafat Manab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_A/0/1/0/all/0/1&quot;&gt;A. B. M. Alim Al Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noor_J/0/1/0/all/0/1&quot;&gt;Jannatun Noor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14284">
<title>Prompt to Transfer: Sim-to-Real Transfer for Traffic Signal Control with Prompt Learning. (arXiv:2308.14284v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14284</link>
<description rdf:parseType="Literal">&lt;p&gt;Numerous solutions are proposed for the Traffic Signal Control (TSC) tasks
aiming to provide efficient transportation and mitigate congestion waste. In
recent, promising results have been attained by Reinforcement Learning (RL)
methods through trial and error in simulators, bringing confidence in solving
cities&apos; congestion headaches. However, there still exist performance gaps when
simulator-trained policies are deployed to the real world. This issue is mainly
introduced by the system dynamic difference between the training simulator and
the real-world environments. The Large Language Models (LLMs) are trained on
mass knowledge and proved to be equipped with astonishing inference abilities.
In this work, we leverage LLMs to understand and profile the system dynamics by
a prompt-based grounded action transformation. Accepting the cloze prompt
template, and then filling in the answer based on accessible context, the
pre-trained LLM&apos;s inference ability is exploited and applied to understand how
weather conditions, traffic states, and road types influence traffic dynamics,
being aware of this, the policies&apos; action is taken and grounded based on
realistic dynamics, thus help the agent learn a more realistic policy. We
conduct experiments using DQN to show the effectiveness of the proposed
PromptGAT&apos;s ability in mitigating the performance gap from simulation to
reality (sim-to-real).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_L/0/1/0/all/0/1&quot;&gt;Longchao Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_M/0/1/0/all/0/1&quot;&gt;Minchiuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.05448">
<title>Panoptic Vision-Language Feature Fields. (arXiv:2309.05448v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.05448</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, methods have been proposed for 3D open-vocabulary semantic
segmentation. Such methods are able to segment scenes into arbitrary classes
based on text descriptions provided during runtime. In this paper, we propose
to the best of our knowledge the first algorithm for open-vocabulary panoptic
segmentation in 3D scenes. Our algorithm, Panoptic Vision-Language Feature
Fields (PVLFF), learns a semantic feature field of the scene by distilling
vision-language features from a pretrained 2D model, and jointly fits an
instance feature field through contrastive learning using 2D instance segments
on input frames. Despite not being trained on the target classes, our method
achieves panoptic segmentation performance similar to the state-of-the-art
closed-set 3D systems on the HyperSim, ScanNet and Replica dataset and
additionally outperforms current 3D open-vocabulary systems in terms of
semantic segmentation. We ablate the components of our method to demonstrate
the effectiveness of our model architecture. Our code will be available at
https://github.com/ethz-asl/pvlff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haoran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blomqvist_K/0/1/0/all/0/1&quot;&gt;Kenneth Blomqvist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milano_F/0/1/0/all/0/1&quot;&gt;Francesco Milano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegwart_R/0/1/0/all/0/1&quot;&gt;Roland Siegwart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12971">
<title>Higher-order Graph Convolutional Network with Flower-Petals Laplacians on Simplicial Complexes. (arXiv:2309.12971v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12971</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the recent successes of vanilla Graph Neural Networks (GNNs) on
various tasks, their foundation on pairwise networks inherently limits their
capacity to discern latent higher-order interactions in complex systems. To
bridge this capability gap, we propose a novel approach exploiting the rich
mathematical theory of simplicial complexes (SCs) - a robust tool for modeling
higher-order interactions. Current SC-based GNNs are burdened by high
complexity and rigidity, and quantifying higher-order interaction strengths
remains challenging. Innovatively, we present a higher-order Flower-Petals (FP)
model, incorporating FP Laplacians into SCs. Further, we introduce a
Higher-order Graph Convolutional Network (HiGCN) grounded in FP Laplacians,
capable of discerning intrinsic features across varying topological scales. By
employing learnable graph filters, a parameter group within each FP Laplacian
domain, we can identify diverse patterns where the filters&apos; weights serve as a
quantifiable measure of higher-order interaction strengths. The theoretical
underpinnings of HiGCN&apos;s advanced expressiveness are rigorously demonstrated.
Additionally, our empirical investigations reveal that the proposed model
accomplishes state-of-the-art performance on a range of graph tasks and
provides a scalable and flexible solution to explore higher-order interactions
in graphs. Codes and datasets are available at
https://github.com/Yiminghh/HiGCN.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yiming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yujie Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;Qiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Linyuan L&amp;#xfc;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11446">
<title>Functional Invariants to Watermark Large Transformers. (arXiv:2310.11446v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11446</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid growth of transformer-based models increases the concerns about
their integrity and ownership insurance. Watermarking addresses this issue by
embedding a unique identifier into the model, while preserving its performance.
However, most existing approaches require to optimize the weights to imprint
the watermark signal, which is not suitable at scale due to the computational
cost. This paper explores watermarks with virtually no computational cost,
applicable to a non-blind white-box setting (assuming access to both the
original and watermarked networks). They generate functionally equivalent
copies by leveraging the models&apos; invariance, via operations like dimension
permutations or scaling/unscaling. This enables to watermark models without any
change in their outputs and remains stealthy. Experiments demonstrate the
effectiveness of the approach and its robustness against various model
transformations (fine-tuning, quantization, pruning), making it a practical
solution to protect the integrity of large models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1&quot;&gt;Pierre Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1&quot;&gt;Guillaume Couairon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1&quot;&gt;Teddy Furon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1&quot;&gt;Matthijs Douze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12086">
<title>FactCHD: Benchmarking Fact-Conflicting Hallucination Detection. (arXiv:2310.12086v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12086</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their impressive generative capabilities, LLMs are hindered by
fact-conflicting hallucinations in real-world applications. The accurate
identification of hallucinations in texts generated by LLMs, especially in
complex inferential scenarios, is a relatively unexplored area. To address this
gap, we present FactCHD, a dedicated benchmark designed for the detection of
fact-conflicting hallucinations from LLMs. FactCHD features a diverse dataset
that spans various factuality patterns, including vanilla, multi-hop,
comparison, and set operation. A distinctive element of FactCHD is its
integration of fact-based evidence chains, significantly enhancing the depth of
evaluating the detectors&apos; explanations. Experiments on different LLMs expose
the shortcomings of current approaches in detecting factual errors accurately.
Furthermore, we introduce Truth-Triangulator that synthesizes reflective
considerations by tool-enhanced ChatGPT and LoRA-tuning based on Llama2, aiming
to yield more credible detection through the amalgamation of predictive results
and evidence. The benchmark dataset is available at
https://github.com/zjunlp/FactCHD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Duanzheng Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1&quot;&gt;Honghao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_J/0/1/0/all/0/1&quot;&gt;Jiang Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_C/0/1/0/all/0/1&quot;&gt;Chengfei Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17212">
<title>Affective Video Content Analysis: Decade Review and New Perspectives. (arXiv:2310.17212v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17212</link>
<description rdf:parseType="Literal">&lt;p&gt;Video content is rich in semantics and has the ability to evoke various
emotions in viewers. In recent years, with the rapid development of affective
computing and the explosive growth of visual data, affective video content
analysis (AVCA) as an essential branch of affective computing has become a
widely researched topic. In this study, we comprehensively review the
development of AVCA over the past decade, particularly focusing on the most
advanced methods adopted to address the three major challenges of video feature
extraction, expression subjectivity, and multimodal feature fusion. We first
introduce the widely used emotion representation models in AVCA and describe
commonly used datasets. We summarize and compare representative methods in the
following aspects: (1) unimodal AVCA models, including facial expression
recognition and posture emotion recognition; (2) multimodal AVCA models,
including feature fusion, decision fusion, and attention-based multimodal
models; (3) model performance evaluation standards. Finally, we discuss future
challenges and promising research directions, such as emotion recognition and
public opinion analysis, human-computer interaction, and emotional
intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Junxiao Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuecheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18913">
<title>Debiasing Algorithm through Model Adaptation. (arXiv:2310.18913v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18913</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models are becoming the go-to solution for various language
tasks. However, with growing capacity, models are prone to rely on spurious
correlations stemming from biases and stereotypes present in the training data.
This work proposes a novel method for detecting and mitigating gender bias in
language models. We perform causal analysis to identify problematic model
components and discover that mid-upper feed-forward layers are most prone to
convey biases. Based on the analysis results, we adapt the model by multiplying
these layers by a linear projection. Our titular method, DAMA, significantly
decreases bias as measured by diverse metrics while maintaining the model&apos;s
performance on downstream tasks. We release code for our method and models,
which retrain LLaMA&apos;s state-of-the-art performance while being significantly
less biased.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Limisiewicz_T/0/1/0/all/0/1&quot;&gt;Tomasz Limisiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marecek_D/0/1/0/all/0/1&quot;&gt;David Mare&amp;#x10d;ek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musil_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;&amp;#x161; Musil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19776">
<title>Learn to Categorize or Categorize to Learn? Self-Coding for Generalized Category Discovery. (arXiv:2310.19776v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19776</link>
<description rdf:parseType="Literal">&lt;p&gt;In the quest for unveiling novel categories at test time, we confront the
inherent limitations of traditional supervised recognition models that are
restricted by a predefined category set. While strides have been made in the
realms of self-supervised and open-world learning towards test-time category
discovery, a crucial yet often overlooked question persists: what exactly
delineates a category? In this paper, we conceptualize a category through the
lens of optimization, viewing it as an optimal solution to a well-defined
problem. Harnessing this unique conceptualization, we propose a novel,
efficient and self-supervised method capable of discovering previously unknown
categories at test time. A salient feature of our approach is the assignment of
minimum length category codes to individual data instances, which encapsulates
the implicit category hierarchy prevalent in real-world datasets. This
mechanism affords us enhanced control over category granularity, thereby
equipping our model to handle fine-grained categories adeptly. Experimental
evaluations, bolstered by state-of-the-art benchmark comparisons, testify to
the efficacy of our solution in managing unknown categories at test time.
Furthermore, we fortify our proposition with a theoretical foundation,
providing proof of its optimality. Our code is available at
https://github.com/SarahRastegar/InfoSieve.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastegar_S/0/1/0/all/0/1&quot;&gt;Sarah Rastegar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G. M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04938">
<title>Improved DDIM Sampling with Moment Matching Gaussian Mixtures. (arXiv:2311.04938v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04938</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose using a Gaussian Mixture Model (GMM) as reverse transition
operator (kernel) within the Denoising Diffusion Implicit Models (DDIM)
framework, which is one of the most widely used approaches for accelerated
sampling from pre-trained Denoising Diffusion Probabilistic Models (DDPM).
Specifically we match the first and second order central moments of the DDPM
forward marginals by constraining the parameters of the GMM. We see that moment
matching is sufficient to obtain samples with equal or better quality than the
original DDIM with Gaussian kernels. We provide experimental results with
unconditional models trained on CelebAHQ and FFHQ and class-conditional models
trained on ImageNet datasets respectively. Our results suggest that using the
GMM kernel leads to significant improvements in the quality of the generated
samples when the number of sampling steps is small, as measured by FID and IS
metrics. For example on ImageNet 256x256, using 10 sampling steps, we achieve a
FID of 6.94 and IS of 207.85 with a GMM kernel compared to 10.15 and 196.73
respectively with a Gaussian kernel.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gabbur_P/0/1/0/all/0/1&quot;&gt;Prasad Gabbur&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09262">
<title>Disentangling the Potential Impacts of Papers into Diffusion, Conformity, and Contribution Values. (arXiv:2311.09262v2 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09262</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential impact of an academic paper is determined by various factors,
including its popularity and contribution. Existing models usually estimate
original citation counts based on static graphs and fail to differentiate
values from nuanced perspectives. In this study, we propose a novel graph
neural network to Disentangle the Potential impacts of Papers into Diffusion,
Conformity, and Contribution values (called DPPDCC). Given a target paper,
DPPDCC encodes temporal and structural features within the constructed dynamic
heterogeneous graph. Particularly, to capture the knowledge flow, we emphasize
the importance of comparative and co-cited/citing information between papers
and aggregate snapshots evolutionarily. To unravel popularity, we contrast
augmented graphs to extract the essence of diffusion and predict the
accumulated citation binning to model conformity. We further apply orthogonal
constraints to encourage distinct modeling of each perspective and preserve the
inherent value of contribution. To evaluate models&apos; generalization for papers
published at various times, we reformulate the problem by partitioning data
based on specific time points to mirror real-world conditions. Extensive
experimental results on three datasets demonstrate that DPPDCC significantly
outperforms baselines for previously, freshly, and immediately published
papers. Further analyses confirm its robust capabilities. We will make our
datasets and codes publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zhikai Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1&quot;&gt;Guoxiu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhuoren Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_S/0/1/0/all/0/1&quot;&gt;Sichen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yangyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Star Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Wei Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09868">
<title>INTERVENOR: Prompt the Coding Ability of Large Language Models with the Interactive Chain of Repairing. (arXiv:2311.09868v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09868</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes INTERactiVE chaiN Of Repairing (INTERVENOR), which mimics
human code repairing behavior (iteratively judging, rethinking, and repairing)
and prompts the coding ability of regard Large Language Models (LLMs).
Specifically, INTERVENOR employs two LLM based agents, Code Learner and Code
Teacher, to play different roles in code repairing and work interactively to
repair the generated codes. The Code Learner is asked to generate and repair
code according to the instructions from the Code Teacher. The Code Teacher
rethinks the code errors according to the corresponding feedback from compilers
and iteratively generates the chain-of-repairing (CoR) to guide the code
repairing process for Code Learner. Our experiments show that INTERVENOR
outperforms the state-of-the-art methods and achieves about 13% and 4.5%
improvements over the GPT-3.5 model in code generation and code translation
tasks, respectively. Our further analyses show that CoR can illuminate the bug
reasons and solution plans via natural language. With the feedback of code
compilers, INTERVENOR can accurately identify the syntax errors and assertion
errors in the code and provide precise instructions to repair codes. All data
and codes are available at https://github.com/NEUIR/INTERVENOR
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanbin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_G/0/1/0/all/0/1&quot;&gt;Ganqu Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_N/0/1/0/all/0/1&quot;&gt;Ning Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Ge Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13594">
<title>Labeling Neural Representations with Inverse Recognition. (arXiv:2311.13594v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13594</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Neural Networks (DNNs) demonstrate remarkable capabilities in learning
complex hierarchical data representations, but the nature of these
representations remains largely unknown. Existing global explainability
methods, such as Network Dissection, face limitations such as reliance on
segmentation masks, lack of statistical significance testing, and high
computational demands. We propose Inverse Recognition (INVERT), a scalable
approach for connecting learned representations with human-understandable
concepts by leveraging their capacity to discriminate between these concepts.
In contrast to prior work, INVERT is capable of handling diverse types of
neurons, exhibits less computational complexity, and does not rely on the
availability of segmentation masks. Moreover, INVERT provides an interpretable
metric assessing the alignment between the representation and its corresponding
explanation and delivering a measure of statistical significance. We
demonstrate the applicability of INVERT in various scenarios, including the
identification of representations affected by spurious correlations, and the
interpretation of the hierarchical structure of decision-making within the
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bykov_K/0/1/0/all/0/1&quot;&gt;Kirill Bykov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kopf_L/0/1/0/all/0/1&quot;&gt;Laura Kopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakajima_S/0/1/0/all/0/1&quot;&gt;Shinichi Nakajima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloft_M/0/1/0/all/0/1&quot;&gt;Marius Kloft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohne_M/0/1/0/all/0/1&quot;&gt;Marina M.-C. H&amp;#xf6;hne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06305">
<title>A Meta-Level Learning Algorithm for Sequential Hyper-Parameter Space Reduction in AutoML. (arXiv:2312.06305v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06305</link>
<description rdf:parseType="Literal">&lt;p&gt;AutoML platforms have numerous options for the algorithms to try for each
step of the analysis, i.e., different possible algorithms for imputation,
transformations, feature selection, and modelling. Finding the optimal
combination of algorithms and hyper-parameter values is computationally
expensive, as the number of combinations to explore leads to an exponential
explosion of the space. In this paper, we present the Sequential
Hyper-parameter Space Reduction (SHSR) algorithm that reduces the space for an
AutoML tool with negligible drop in its predictive performance. SHSR is a
meta-level learning algorithm that analyzes past runs of an AutoML tool on
several datasets and learns which hyper-parameter values to filter out from
consideration on a new dataset to analyze. SHSR is evaluated on 284
classification and 375 regression problems, showing an approximate 30%
reduction in execution time with a performance drop of less than 0.1%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borboudakis_G/0/1/0/all/0/1&quot;&gt;Giorgos Borboudakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Charonyktakis_P/0/1/0/all/0/1&quot;&gt;Paulos Charonyktakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paraschakis_K/0/1/0/all/0/1&quot;&gt;Konstantinos Paraschakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsamardinos_I/0/1/0/all/0/1&quot;&gt;Ioannis Tsamardinos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08722">
<title>Quantifying Divergence for Human-AI Collaboration and Cognitive Trust. (arXiv:2312.08722v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08722</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the collaboration likelihood and measuring cognitive trust to AI
systems is more important than ever. To do that, previous research mostly focus
solely on the model features (e.g., accuracy, confidence) and ignore the human
factor. To address that, we propose several decision-making similarity measures
based on divergence metrics (e.g., KL, JSD) calculated over the labels acquired
from humans and a wide range of models. We conduct a user study on a textual
entailment task, where the users are provided with soft labels from various
models and asked to pick the closest option to them. The users are then shown
the similarities/differences to their most similar model and are surveyed for
their likelihood of collaboration and cognitive trust to the selected system.
Finally, we qualitatively and quantitatively analyze the relation between the
proposed decision-making similarity measures and the survey results. We find
that people tend to collaborate with their most similar models -- measured via
JSD -- yet this collaboration does not necessarily imply a similar level of
cognitive trust. We release all resources related to the user study (e.g.,
design, outputs), models, and metrics at our repo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kural_M/0/1/0/all/0/1&quot;&gt;M&amp;#xfc;ge Kural&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebesce_A/0/1/0/all/0/1&quot;&gt;Ali Gebe&amp;#x15f;&amp;#xe7;e&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chubakov_T/0/1/0/all/0/1&quot;&gt;Tilek Chubakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sahin_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;zde G&amp;#xfc;l &amp;#x15e;ahin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12469">
<title>Distilling Autoregressive Models to Obtain High-Performance Non-Autoregressive Solvers for Vehicle Routing Problems with Faster Inference Speed. (arXiv:2312.12469v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12469</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural construction models have shown promising performance for Vehicle
Routing Problems (VRPs) by adopting either the Autoregressive (AR) or
Non-Autoregressive (NAR) learning approach. While AR models produce
high-quality solutions, they generally have a high inference latency due to
their sequential generation nature. Conversely, NAR models generate solutions
in parallel with a low inference latency but generally exhibit inferior
performance. In this paper, we propose a generic Guided Non-Autoregressive
Knowledge Distillation (GNARKD) method to obtain high-performance NAR models
having a low inference latency. GNARKD removes the constraint of sequential
generation in AR models while preserving the learned pivotal components in the
network architecture to obtain the corresponding NAR models through knowledge
distillation. We evaluate GNARKD by applying it to three widely adopted AR
models to obtain NAR VRP solvers for both synthesized and real-world instances.
The experimental results demonstrate that GNARKD significantly reduces the
inference time (4-5 times faster) with acceptable performance drop (2-3\%). To
the best of our knowledge, this study is first-of-its-kind to obtain NAR VRP
solvers from AR ones through knowledge distillation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yubin Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingzhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Changliang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;You Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.14345">
<title>Logic-Scaffolding: Personalized Aspect-Instructed Recommendation Explanation Generation using LLMs. (arXiv:2312.14345v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.14345</link>
<description rdf:parseType="Literal">&lt;p&gt;The unique capabilities of Large Language Models (LLMs), such as the natural
language text generation ability, position them as strong candidates for
providing explanation for recommendations. However, despite the size of the
LLM, most existing models struggle to produce zero-shot explanations reliably.
To address this issue, we propose a framework called Logic-Scaffolding, that
combines the ideas of aspect-based explanation and chain-of-thought prompting
to generate explanations through intermediate reasoning steps. In this paper,
we share our experience in building the framework and present an interactive
demonstration for exploring our results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahdari_B/0/1/0/all/0/1&quot;&gt;Behnam Rahdari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hao Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Ziwei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yifei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuotong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deoras_A/0/1/0/all/0/1&quot;&gt;Anoop Deoras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kveton_B/0/1/0/all/0/1&quot;&gt;Branislav Kveton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16171">
<title>Principled Instructions Are All You Need for Questioning LLaMA-1/2, GPT-3.5/4. (arXiv:2312.16171v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16171</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces 26 guiding principles designed to streamline the
process of querying and prompting large language models. Our goal is to
simplify the underlying concepts of formulating questions for various scales of
large language models, examining their abilities, and enhancing user
comprehension on the behaviors of different scales of large language models
when feeding into different prompts. Extensive experiments are conducted on
LLaMA-1/2 (7B, 13B and 70B), GPT-3.5/4 to verify the effectiveness of the
proposed principles on instructions and prompts design. We hope that this work
can provide a better guide for researchers working on the prompting of large
language models. Project page is available at
https://github.com/VILA-Lab/ATLAS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bsharat_S/0/1/0/all/0/1&quot;&gt;Sondos Mahmoud Bsharat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myrzakhan_A/0/1/0/all/0/1&quot;&gt;Aidar Myrzakhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.17484">
<title>Truth Forest: Toward Multi-Scale Truthfulness in Large Language Models through Intervention without Tuning. (arXiv:2312.17484v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.17484</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the great success of large language models (LLMs) in various tasks,
they suffer from generating hallucinations. We introduce Truth Forest, a method
that enhances truthfulness in LLMs by uncovering hidden truth representations
using multi-dimensional orthogonal probes. Specifically, it creates multiple
orthogonal bases for modeling truth by incorporating orthogonal constraints
into the probes. Moreover, we introduce Random Peek, a systematic technique
considering an extended range of positions within the sequence, reducing the
gap between discerning and generating truth features in LLMs. By employing this
approach, we improved the truthfulness of Llama-2-7B from 40.8\% to 74.5\% on
TruthfulQA. Likewise, significant improvements are observed in fine-tuned
models. We conducted a thorough analysis of truth features using probes. Our
visualization results show that orthogonal probes capture complementary
truth-related features, forming well-defined clusters that reveal the inherent
structure of the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhongzhi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xingwu Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_X/0/1/0/all/0/1&quot;&gt;Xianfeng Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lian_F/0/1/0/all/0/1&quot;&gt;Fengzong Lian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Z/0/1/0/all/0/1&quot;&gt;Zhanhui Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Cheng-Zhong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00132">
<title>Contrastive learning-based agent modeling for deep reinforcement learning. (arXiv:2401.00132v2 [cs.MA] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00132</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent systems often require agents to collaborate with or compete
against other agents with diverse goals, behaviors, or strategies. Agent
modeling is essential when designing adaptive policies for intelligent machine
agents in multiagent systems, as this is the means by which the ego agent
understands other agents&apos; behavior and extracts their meaningful policy
representations. These representations can be used to enhance the ego agent&apos;s
adaptive policy which is trained by reinforcement learning. However, existing
agent modeling approaches typically assume the availability of local
observations from other agents (modeled agents) during training or a long
observation trajectory for policy adaption. To remove these constrictive
assumptions and improve agent modeling performance, we devised a Contrastive
Learning-based Agent Modeling (CLAM) method that relies only on the local
observations from the ego agent during training and execution. With these
observations, CLAM is capable of generating consistent high-quality policy
representations in real-time right from the beginning of each episode. We
evaluated the efficacy of our approach in both cooperative and competitive
multi-agent environments. Our experiments demonstrate that our approach
achieves state-of-the-art on both cooperative and competitive tasks,
highlighting the potential of contrastive learning-based agent modeling for
enhancing reinforcement learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_W/0/1/0/all/0/1&quot;&gt;Wenhao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yu-Cheng Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chin-Teng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02860">
<title>Framework for Variable-lag Motif Following Relation Inference In Time Series using Matrix Profile analysis. (arXiv:2401.02860v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02860</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowing who follows whom and what patterns they are following are crucial
steps to understand collective behaviors (e.g. a group of human, a school of
fish, or a stock market). Time series is one of resources that can be used to
get insight regarding following relations. However, the concept of following
patterns or motifs and the solution to find them in time series are not
obvious. In this work, we formalize a concept of following motifs between two
time series and present a framework to infer following patterns between two
time series. The framework utilizes one of efficient and scalable methods to
retrieve motifs from time series called the Matrix Profile Method. We compare
our proposed framework with several baselines. The framework performs better
than baselines in the simulation datasets. In the dataset of sound recording,
the framework is able to retrieve the following motifs within a pair of time
series that two singers sing following each other. In the cryptocurrency
dataset, the framework is capable of capturing the following motifs within a
pair of time series from two digital currencies, which implies that the values
of one currency follow the values of another currency patterns. Our framework
can be utilized in any field of time series to get insight regarding following
patterns between time series.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinpattanakarn_N/0/1/0/all/0/1&quot;&gt;Naaek Chinpattanakarn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amornbunchornvej_C/0/1/0/all/0/1&quot;&gt;Chainarong Amornbunchornvej&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03473">
<title>ICMC-ASR: The ICASSP 2024 In-Car Multi-Channel Automatic Speech Recognition Challenge. (arXiv:2401.03473v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03473</link>
<description rdf:parseType="Literal">&lt;p&gt;To promote speech processing and recognition research in driving scenarios,
we build on the success of the Intelligent Cockpit Speech Recognition Challenge
(ICSRC) held at ISCSLP 2022 and launch the ICASSP 2024 In-Car Multi-Channel
Automatic Speech Recognition (ICMC-ASR) Challenge. This challenge collects over
100 hours of multi-channel speech data recorded inside a new energy vehicle and
40 hours of noise for data augmentation. Two tracks, including automatic speech
recognition (ASR) and automatic speech diarization and recognition (ASDR) are
set up, using character error rate (CER) and concatenated minimum permutation
character error rate (cpCER) as evaluation metrics, respectively. Overall, the
ICMC-ASR Challenge attracts 98 participating teams and receives 53 valid
results in both tracks. In the end, first-place team USTCiflytek achieves a CER
of 13.16% in the ASR track and a cpCER of 21.48% in the ASDR track, showing an
absolute improvement of 13.08% and 51.4% compared to our challenge baseline,
respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;He Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_P/0/1/0/all/0/1&quot;&gt;Pengcheng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Ao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiayao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_L/0/1/0/all/0/1&quot;&gt;Lei Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_H/0/1/0/all/0/1&quot;&gt;Hui Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Binbin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jian Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Longbiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chng_E/0/1/0/all/0/1&quot;&gt;Eng Siong Chng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05163">
<title>MISS: A Generative Pretraining and Finetuning Approach for Med-VQA. (arXiv:2401.05163v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05163</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical visual question answering (VQA) is a challenging multimodal task,
where Vision-Language Pre-training (VLP) models can effectively improve the
generalization performance. However, most methods in the medical field treat
VQA as an answer classification task which is difficult to transfer to
practical application scenarios. Additionally, due to the privacy of medical
images and the expensive annotation process, large-scale medical image-text
pairs datasets for pretraining are severely lacking. In this paper, we propose
a large-scale MultI-task Self-Supervised learning based framework (MISS) for
medical VQA tasks. Unlike existing methods, we treat medical VQA as a
generative task. We unify the text encoder and multimodal encoder and align
image-text features through multi-task learning. Furthermore, we propose a
Transfer-and-Caption method that extends the feature space of single-modal
image datasets using large language models (LLMs), enabling those traditional
medical vision field task data to be applied to VLP. Experiments show that our
method achieves excellent results with fewer multimodal datasets and
demonstrates the advantages of generative VQA models. The code and model
weights will be released upon the paper&apos;s acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiawei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yue Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05566">
<title>Sleeper Agents: Training Deceptive LLMs that Persist Through Safety Training. (arXiv:2401.05566v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05566</link>
<description rdf:parseType="Literal">&lt;p&gt;Humans are capable of strategically deceptive behavior: behaving helpfully in
most situations, but then behaving very differently in order to pursue
alternative objectives when given the opportunity. If an AI system learned such
a deceptive strategy, could we detect it and remove it using current
state-of-the-art safety training techniques? To study this question, we
construct proof-of-concept examples of deceptive behavior in large language
models (LLMs). For example, we train models that write secure code when the
prompt states that the year is 2023, but insert exploitable code when the
stated year is 2024. We find that such backdoor behavior can be made
persistent, so that it is not removed by standard safety training techniques,
including supervised fine-tuning, reinforcement learning, and adversarial
training (eliciting unsafe behavior and then training to remove it). The
backdoor behavior is most persistent in the largest models and in models
trained to produce chain-of-thought reasoning about deceiving the training
process, with the persistence remaining even when the chain-of-thought is
distilled away. Furthermore, rather than removing backdoors, we find that
adversarial training can teach models to better recognize their backdoor
triggers, effectively hiding the unsafe behavior. Our results suggest that,
once a model exhibits deceptive behavior, standard techniques could fail to
remove such deception and create a false impression of safety.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubinger_E/0/1/0/all/0/1&quot;&gt;Evan Hubinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denison_C/0/1/0/all/0/1&quot;&gt;Carson Denison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_J/0/1/0/all/0/1&quot;&gt;Jesse Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_M/0/1/0/all/0/1&quot;&gt;Mike Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Meg Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacDiarmid_M/0/1/0/all/0/1&quot;&gt;Monte MacDiarmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanham_T/0/1/0/all/0/1&quot;&gt;Tamera Lanham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziegler_D/0/1/0/all/0/1&quot;&gt;Daniel M. Ziegler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maxwell_T/0/1/0/all/0/1&quot;&gt;Tim Maxwell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_N/0/1/0/all/0/1&quot;&gt;Newton Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jermyn_A/0/1/0/all/0/1&quot;&gt;Adam Jermyn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Askell_A/0/1/0/all/0/1&quot;&gt;Amanda Askell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radhakrishnan_A/0/1/0/all/0/1&quot;&gt;Ansh Radhakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anil_C/0/1/0/all/0/1&quot;&gt;Cem Anil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duvenaud_D/0/1/0/all/0/1&quot;&gt;David Duvenaud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguli_D/0/1/0/all/0/1&quot;&gt;Deep Ganguli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clark_J/0/1/0/all/0/1&quot;&gt;Jack Clark&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ndousse_K/0/1/0/all/0/1&quot;&gt;Kamal Ndousse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_K/0/1/0/all/0/1&quot;&gt;Kshitij Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sellitto_M/0/1/0/all/0/1&quot;&gt;Michael Sellitto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_M/0/1/0/all/0/1&quot;&gt;Mrinank Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DasSarma_N/0/1/0/all/0/1&quot;&gt;Nova DasSarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grosse_R/0/1/0/all/0/1&quot;&gt;Roger Grosse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kravec_S/0/1/0/all/0/1&quot;&gt;Shauna Kravec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yuntao Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Witten_Z/0/1/0/all/0/1&quot;&gt;Zachary Witten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Favaro_M/0/1/0/all/0/1&quot;&gt;Marina Favaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brauner_J/0/1/0/all/0/1&quot;&gt;Jan Brauner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnofsky_H/0/1/0/all/0/1&quot;&gt;Holden Karnofsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christiano_P/0/1/0/all/0/1&quot;&gt;Paul Christiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bowman_S/0/1/0/all/0/1&quot;&gt;Samuel R. Bowman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graham_L/0/1/0/all/0/1&quot;&gt;Logan Graham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplan_J/0/1/0/all/0/1&quot;&gt;Jared Kaplan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindermann_S/0/1/0/all/0/1&quot;&gt;S&amp;#xf6;ren Mindermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Greenblatt_R/0/1/0/all/0/1&quot;&gt;Ryan Greenblatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlegeris_B/0/1/0/all/0/1&quot;&gt;Buck Shlegeris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiefer_N/0/1/0/all/0/1&quot;&gt;Nicholas Schiefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Ethan Perez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06805">
<title>Exploring the Reasoning Abilities of Multimodal Large Language Models (MLLMs): A Comprehensive Survey on Emerging Trends in Multimodal Reasoning. (arXiv:2401.06805v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06805</link>
<description rdf:parseType="Literal">&lt;p&gt;Strong Artificial Intelligence (Strong AI) or Artificial General Intelligence
(AGI) with abstract reasoning ability is the goal of next-generation AI. Recent
advancements in Large Language Models (LLMs), along with the emerging field of
Multimodal Large Language Models (MLLMs), have demonstrated impressive
capabilities across a wide range of multimodal tasks and applications.
Particularly, various MLLMs, each with distinct model architectures, training
data, and training stages, have been evaluated across a broad range of MLLM
benchmarks. These studies have, to varying degrees, revealed different aspects
of the current capabilities of MLLMs. However, the reasoning abilities of MLLMs
have not been systematically investigated. In this survey, we comprehensively
review the existing evaluation protocols of multimodal reasoning, categorize
and illustrate the frontiers of MLLMs, introduce recent trends in applications
of MLLMs on reasoning-intensive tasks, and finally discuss current practices
and future directions. We believe our survey establishes a solid base and sheds
light on this important topic, multimodal reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wentao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_X/0/1/0/all/0/1&quot;&gt;Xiaotian Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xudong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiteng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_B/0/1/0/all/0/1&quot;&gt;Bohan Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jianbo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Q/0/1/0/all/0/1&quot;&gt;Quanzeng You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hongxia Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06951">
<title>E^2-LLM: Efficient and Extreme Length Extension of Large Language Models. (arXiv:2401.06951v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.06951</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically, training LLMs with long context sizes is computationally
expensive, requiring extensive training hours and GPU resources. Existing
long-context extension methods usually need additional training procedures to
support corresponding long-context windows, where the long-context training
data (e.g., 32k) is needed, and high GPU training costs are assumed. To address
the aforementioned issues, we propose an Efficient and Extreme length extension
method for Large Language Models, called E 2 -LLM, with only one training
procedure and dramatically reduced computation cost, which also removes the
need to collect long-context data. Concretely, first, the training data of our
E 2 -LLM only requires a short length (e.g., 4k), which reduces the tuning cost
greatly. Second, the training procedure on the short training context window is
performed only once time, and we can support different evaluation context
windows at inference. Third, in E 2 - LLM, based on RoPE position embeddings,
we introduce two different augmentation methods on the scale and position index
parameters for different samples in training. It aims to make the model more
robust to the different relative differences when directly interpolating the
arbitrary context length at inference. Comprehensive experimental results on
multiple benchmark datasets demonstrate the effectiveness of our E 2 -LLM on
challenging long-context tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanxing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chenchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiakai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Que_H/0/1/0/all/0/1&quot;&gt;Haoran Que&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Wenbo Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tiezheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_B/0/1/0/all/0/1&quot;&gt;Bo Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07450">
<title>Hierarchical Fashion Design with Multi-stage Diffusion Models. (arXiv:2401.07450v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07450</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modal fashion synthesis and editing offer intelligent support to
fashion designers by enabling the automatic generation and local modification
of design drafts.While current diffusion models demonstrate commendable
stability and controllability in image synthesis,they still face significant
challenges in generating fashion design from abstract design elements and
fine-grained editing.Abstract sensory expressions, \eg office, business, and
party, form the high-level design concepts, while measurable aspects like
sleeve length, collar type, and pant length are considered the low-level
attributes of clothing.Controlling and editing fashion images using lengthy
text descriptions poses a difficulty.In this paper, we propose HieraFashDiff,a
novel fashion design method using the shared multi-stage diffusion model
encompassing high-level design concepts and low-level clothing attributes in a
hierarchical structure.Specifically, we categorized the input text into
different levels and fed them in different time step to the diffusion model
according to the criteria of professional clothing designers.HieraFashDiff
allows designers to add low-level attributes after high-level prompts for
interactive editing incrementally.In addition, we design a differentiable loss
function in the sampling process with a mask to keep non-edit
areas.Comprehensive experiments performed on our newly conducted Hierarchical
fashion dataset,demonstrate that our proposed method outperforms other
state-of-the-art competitors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Z/0/1/0/all/0/1&quot;&gt;Zhifeng Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+li_H/0/1/0/all/0/1&quot;&gt;Hao li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Huiming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengtian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Ying Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07510">
<title>Developing ChatGPT for Biology and Medicine: A Complete Review of Biomedical Question Answering. (arXiv:2401.07510v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07510</link>
<description rdf:parseType="Literal">&lt;p&gt;ChatGPT explores a strategic blueprint of question answering (QA) in
delivering medical diagnosis, treatment recommendations, and other healthcare
support. This is achieved through the increasing incorporation of medical
domain data via natural language processing (NLP) and multimodal paradigms. By
transitioning the distribution of text, images, videos, and other modalities
from the general domain to the medical domain, these techniques have expedited
the progress of medical domain question answering (MDQA). They bridge the gap
between human natural language and sophisticated medical domain knowledge or
expert manual annotations, handling large-scale, diverse, unbalanced, or even
unlabeled data analysis scenarios in medical contexts. Central to our focus is
the utilizing of language models and multimodal paradigms for medical question
answering, aiming to guide the research community in selecting appropriate
mechanisms for their specific medical research requirements. Specialized tasks
such as unimodal-related question answering, reading comprehension, reasoning,
diagnosis, relation extraction, probability modeling, and others, as well as
multimodal-related tasks like vision question answering, image caption,
cross-modal retrieval, report summarization, and generation, are discussed in
detail. Each section delves into the intricate specifics of the respective
method under consideration. This paper highlights the structures and
advancements of medical domain explorations against general domain methods,
emphasizing their applications across different tasks and datasets. It also
outlines current challenges and opportunities for future medical domain
research, paving the way for continued innovation and application in this
rapidly evolving field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07525">
<title>TAROT: A Hierarchical Framework with Multitask Co-Pretraining on Semi-Structured Data towards Effective Person-Job Fit. (arXiv:2401.07525v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07525</link>
<description rdf:parseType="Literal">&lt;p&gt;Person-job fit is an essential part of online recruitment platforms in
serving various downstream applications like Job Search and Candidate
Recommendation. Recently, pretrained large language models have further
enhanced the effectiveness by leveraging richer textual information in user
profiles and job descriptions apart from user behavior features and job
metadata. However, the general domain-oriented design struggles to capture the
unique structural information within user profiles and job descriptions,
leading to a loss of latent semantic correlations. We propose TAROT, a
hierarchical multitask co-pretraining framework, to better utilize structural
and semantic information for informative text embeddings. TAROT targets
semi-structured text in profiles and jobs, and it is co-pretained with
multi-grained pretraining tasks to constrain the acquired semantic information
at each level. Experiments on a real-world LinkedIn dataset show significant
performance improvements, proving its effectiveness in person-job fit tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yihan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Lun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Q/0/1/0/all/0/1&quot;&gt;Qiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yushu Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yanbin Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07927">
<title>Are self-explanations from Large Language Models faithful?. (arXiv:2401.07927v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07927</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction-tuned large language models (LLMs) excel at many tasks, and will
even provide explanations for their behavior. Since these models are directly
accessible to the public, there is a risk that convincing and wrong
explanations can lead to unsupported confidence in LLMs. Therefore,
interpretability-faithfulness of self-explanations is an important
consideration for AI Safety. Assessing the interpretability-faithfulness of
these explanations, termed self-explanations, is challenging as the models are
too complex for humans to annotate what is a correct explanation. To address
this, we propose employing self-consistency checks as a measure of
faithfulness. For example, if an LLM says a set of words is important for
making a prediction, then it should not be able to make the same prediction
without these words. While self-consistency checks are a common approach to
faithfulness, they have not previously been applied to LLM&apos;s self-explanations.
We apply self-consistency checks to three types of self-explanations:
counterfactuals, importance measures, and redactions. Our work demonstrate that
faithfulness is both task and model dependent, e.g., for sentiment
classification, counterfactual explanations are more faithful for Llama2,
importance measures for Mistral, and redaction for Falcon 40B. Finally, our
findings are robust to prompt-variations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madsen_A/0/1/0/all/0/1&quot;&gt;Andreas Madsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandar_S/0/1/0/all/0/1&quot;&gt;Sarath Chandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_S/0/1/0/all/0/1&quot;&gt;Siva Reddy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08727">
<title>MA2GCN: Multi Adjacency relationship Attention Graph Convolutional Networks for Traffic Prediction using Trajectory data. (arXiv:2401.08727v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08727</link>
<description rdf:parseType="Literal">&lt;p&gt;The problem of traffic congestion not only causes a large amount of economic
losses, but also seriously endangers the urban environment. Predicting traffic
congestion has important practical significance. So far, most studies have been
based on historical data from sensors placed on different roads to predict
future traffic flow and speed, to analyze the traffic congestion conditions of
a certain road segment. However, due to the fixed position of sensors, it is
difficult to mine new information. On the other hand, vehicle trajectory data
is more flexible and can extract traffic information as needed. Therefore, we
proposed a new traffic congestion prediction model - Multi Adjacency
relationship Attention Graph Convolutional Networks(MA2GCN). This model
transformed vehicle trajectory data into graph structured data in grid form,
and proposed a vehicle entry and exit matrix based on the mobility between
different grids. At the same time, in order to improve the performance of the
model, this paper also built a new adaptive adjacency matrix generation method
and adjacency matrix attention module. This model mainly used gated temporal
convolution and graph convolution to extract temporal and spatial information,
respectively. Compared with multiple baselines, our model achieved the best
performance on Shanghai taxi GPS trajectory dataset. The code is available at
https://github.com/zachysun/Taxi_Traffic_Benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhengke Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yuliang Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09192">
<title>Preparing Lessons for Progressive Training on Language Models. (arXiv:2401.09192v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09192</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid progress of Transformers in artificial intelligence has come at the
cost of increased resource consumption and greenhouse gas emissions due to
growing model sizes. Prior work suggests using pretrained small models to
improve training efficiency, but this approach may not be suitable for new
model structures. On the other hand, training from scratch can be slow, and
progressively stacking layers often fails to achieve significant acceleration.
To address these challenges, we propose a novel method called Apollo, which
prep\textbf{a}res lessons for ex\textbf{p}anding \textbf{o}perations by
\textbf{l}earning high-\textbf{l}ayer functi\textbf{o}nality during training of
low layers. Our approach involves low-value-prioritized sampling (LVPS) to
train different depths and weight sharing to facilitate efficient expansion. We
also introduce an interpolation method for stable model depth extension.
Experiments demonstrate that Apollo achieves state-of-the-art acceleration
ratios, even rivaling methods using pretrained models, making it a universal
and efficient solution for training deep models while reducing time, financial,
and environmental costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yu Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Ye Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Y/0/1/0/all/0/1&quot;&gt;Yichun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jiaxin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zenglin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_L/0/1/0/all/0/1&quot;&gt;Lifeng Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qun Liu&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>