<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-06T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02998" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03013" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03022" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03038" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03042" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03077" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03186" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03245" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03248" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03263" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03291" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03297" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03360" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03365" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03395" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03521" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03580" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03606" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03641" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03664" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03682" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03700" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.10890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.10629" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.01278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.08204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10706" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.03932" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.07624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12524" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.17390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09597" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.04673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00694" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.04256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09428" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01454" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02125" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02439" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02663" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08289" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00045" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.02984">
<title>Diff-GO: Diffusion Goal-Oriented Communications to Achieve Ultra-High Spectrum Efficiency. (arXiv:2312.02984v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.02984</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest advances in artificial intelligence (AI) present many
unprecedented opportunities to achieve much improved bandwidth saving in
communications. Unlike conventional communication systems focusing on packet
transport, rich datasets and AI makes it possible to efficiently transfer only
the information most critical to the goals of message recipients. One of the
most exciting advances in generative AI known as diffusion model presents a
unique opportunity for designing ultra-fast communication systems well beyond
language-based messages. This work presents an ultra-efficient communication
design by utilizing generative AI-based on diffusion models as a specific
example of the general goal-oriented communication framework. To better control
the regenerated message at the receiver output, our diffusion system design
includes a local regeneration module with finite dimensional noise latent. The
critical significance of noise latent control and sharing residing on our
Diff-GO is the ability to introduce the concept of &quot;local generative feedback&quot;
(Local-GF), which enables the transmitter to monitor the quality and gauge the
quality or accuracy of the message recovery at the semantic system receiver. To
this end, we propose a new low-dimensional noise space for the training of
diffusion models, which significantly reduces the communication overhead and
achieves satisfactory message recovery performance. Our experimental results
demonstrate that the proposed noise space and the diffusion-based generative
model achieve ultra-high spectrum efficiency and accurate recovery of
transmitted image signals. By trading off computation for bandwidth efficiency
(C4BE), this new framework provides an important avenue to achieve exceptional
computation-bandwidth tradeoff.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wijesinghe_A/0/1/0/all/0/1&quot;&gt;Achintha Wijesinghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wanninayaka_S/0/1/0/all/0/1&quot;&gt;Suchinthaka Wanninayaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weiwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Zhi Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02998">
<title>Personality of AI. (arXiv:2312.02998v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.02998</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper delves into the evolving landscape of fine-tuning large
language models (LLMs) to align with human users, extending beyond basic
alignment to propose &quot;personality alignment&quot; for language models in
organizational settings. Acknowledging the impact of training methods on the
formation of undefined personality traits in AI models, the study draws
parallels with human fitting processes using personality tests. Through an
original case study, we demonstrate the necessity of personality fine-tuning
for AIs and raise intriguing questions about applying human-designed tests to
AIs, engineering specialized AI personality tests, and shaping AI personalities
to suit organizational roles. The paper serves as a starting point for
discussions and developments in the burgeoning field of AI personality
alignment, offering a foundational anchor for future exploration in
human-machine teaming and co-existence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Byunggu Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junwhan Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03003">
<title>Explore, Select, Derive, and Recall: Augmenting LLM with Human-like Memory for Mobile Task Automation. (arXiv:2312.03003v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.03003</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) has opened up new opportunities in
the field of mobile task automation. Their superior language understanding and
reasoning capabilities allow users to automate complex and repetitive tasks.
However, due to the inherent unreliability and high operational cost of LLMs,
their practical applicability is quite limited. To address these issues, this
paper introduces MemoDroid, an innovative LLM-based mobile task automator
enhanced with a unique app memory. MemoDroid emulates the cognitive process of
humans interacting with a mobile app -- explore, select, derive, and recall.
This approach allows for a more precise and efficient learning of a task&apos;s
procedure by breaking it down into smaller, modular components that can be
re-used, re-arranged, and adapted for various objectives. We implement
MemoDroid using online LLMs services (GPT-3.5 and GPT-4) and evaluate its
performance on 50 unique mobile tasks across 5 widely used mobile apps. The
results indicate that MemoDroid can adapt learned tasks to varying contexts
with 100% accuracy and reduces their latency and cost by 69.22% and 77.36%
compared to a GPT-4 powered baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sunjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Junyoung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungjae Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hojun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_S/0/1/0/all/0/1&quot;&gt;Steven Y. Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Sangeun Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_I/0/1/0/all/0/1&quot;&gt;Insik Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03004">
<title>Learning Multi-graph Structure for Temporal Knowledge Graph Reasoning. (arXiv:2312.03004v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03004</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Knowledge Graph (TKG) reasoning that forecasts future events based
on historical snapshots distributed over timestamps is denoted as extrapolation
and has gained significant attention. Owing to its extreme versatility and
variation in spatial and temporal correlations, TKG reasoning presents a
challenging task, demanding efficient capture of concurrent structures and
evolutional interactions among facts. While existing methods have made strides
in this direction, they still fall short of harnessing the diverse forms of
intrinsic expressive semantics of TKGs, which encompass entity correlations
across multiple timestamps and periodicity of temporal information. This
limitation constrains their ability to thoroughly reflect historical
dependencies and future trends. In response to these drawbacks, this paper
proposes an innovative reasoning approach that focuses on Learning Multi-graph
Structure (LMS). Concretely, it comprises three distinct modules concentrating
on multiple aspects of graph structure knowledge within TKGs, including
concurrent and evolutional patterns along timestamps, query-specific
correlations across timestamps, and semantic dependencies of timestamps, which
capture TKG features from various perspectives. Besides, LMS incorporates an
adaptive gate for merging entity representations both along and across
timestamps effectively. Moreover, it integrates timestamp semantics into graph
attention calculations and time-aware decoders, in order to impose temporal
constraints on events and narrow down prediction scopes with historical
statistics. Extensive experimental results on five event-based benchmark
datasets demonstrate that LMS outperforms state-of-the-art extrapolation
models, indicating the superiority of modeling a multi-graph perspective for
TKG reasoning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jinchuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_B/0/1/0/all/0/1&quot;&gt;Bei Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_C/0/1/0/all/0/1&quot;&gt;Chong Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Ling Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03006">
<title>Cone Ranking for Multi-Criteria Decision Making. (arXiv:2312.03006v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03006</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently introduced cone distribution functions from statistics are turned
into multi-criteria decision making (MCDM) tools. It is demonstrated that this
procedure can be considered as an upgrade of the weighted sum scalarization
insofar as it absorbs a whole collection of weighted sum scalarizations at once
instead of fixing a particular one in advance. Moreover, situations are
characterized in which different types of rank reversal occur, and it is
explained why this might even be useful for analyzing the ranking procedure. A
few examples will be discussed and a potential application in machine learning
is outlined.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamel_A/0/1/0/all/0/1&quot;&gt;Andreas H Hamel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kostner_D/0/1/0/all/0/1&quot;&gt;Daniel Kostner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03008">
<title>Deep Reinforcement Learning for Community Battery Scheduling under Uncertainties of Load, PV Generation, and Energy Prices. (arXiv:2312.03008v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03008</link>
<description rdf:parseType="Literal">&lt;p&gt;In response to the growing uptake of distributed energy resources (DERs),
community batteries have emerged as a promising solution to support renewable
energy integration, reduce peak load, and enhance grid reliability. This paper
presents a deep reinforcement learning (RL) strategy, centered around the soft
actor-critic (SAC) algorithm, to schedule a community battery system in the
presence of uncertainties, such as solar photovoltaic (PV) generation, local
demand, and real-time energy prices. We position the community battery to play
a versatile role, in integrating local PV energy, reducing peak load, and
exploiting energy price fluctuations for arbitrage, thereby minimizing the
system cost. To improve exploration and convergence during RL training, we
utilize the noisy network technique. This paper conducts a comparative study of
different RL algorithms, including proximal policy optimization (PPO) and deep
deterministic policy gradient (DDPG) algorithms, to evaluate their
effectiveness in the community battery scheduling problem. The results
demonstrate the potential of RL in addressing community battery scheduling
challenges and show that the SAC algorithm achieves the best performance
compared to RL and optimization benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jiarong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03009">
<title>I-PHYRE: Interactive Physical Reasoning. (arXiv:2312.03009v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03009</link>
<description rdf:parseType="Literal">&lt;p&gt;Current evaluation protocols predominantly assess physical reasoning in
stationary scenes, creating a gap in evaluating agents&apos; abilities to interact
with dynamic events. While contemporary methods allow agents to modify initial
scene configurations and observe consequences, they lack the capability to
interact with events in real time. To address this, we introduce I-PHYRE, a
framework that challenges agents to simultaneously exhibit intuitive physical
reasoning, multi-step planning, and in-situ intervention. Here, intuitive
physical reasoning refers to a quick, approximate understanding of physics to
address complex problems; multi-step denotes the need for extensive sequence
planning in I-PHYRE, considering each intervention can significantly alter
subsequent choices; and in-situ implies the necessity for timely object
manipulation within a scene, where minor timing deviations can result in task
failure. We formulate four game splits to scrutinize agents&apos; learning and
generalization of essential principles of interactive physical reasoning,
fostering learning through interaction with representative scenarios. Our
exploration involves three planning strategies and examines several supervised
and reinforcement agents&apos; zero-shot generalization proficiency on I-PHYRE. The
outcomes highlight a notable gap between existing learning algorithms and human
performance, emphasizing the imperative for more research in enhancing agents
with interactive physical reasoning capabilities. The environment and baselines
will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiqian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kewen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yixin Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03011">
<title>InstructBooth: Instruction-following Personalized Text-to-Image Generation. (arXiv:2312.03011v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03011</link>
<description rdf:parseType="Literal">&lt;p&gt;Personalizing text-to-image models using a limited set of images for a
specific object has been explored in subject-specific image generation.
However, existing methods often encounter challenges in aligning with text
prompts due to overfitting to the limited training images. In this work, we
introduce InstructBooth, a novel method designed to enhance image-text
alignment in personalized text-to-image models. Our approach first personalizes
text-to-image models with a small number of subject-specific images using a
unique identifier. After personalization, we fine-tune personalized
text-to-image models using reinforcement learning to maximize a reward that
quantifies image-text alignment. Additionally, we propose complementary
techniques to increase the synergy between these two processes. Our method
demonstrates superior image-text alignment compared to baselines while
maintaining personalization ability. In human evaluations, InstructBooth
outperforms DreamBooth when considering all comprehensive factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chae_D/0/1/0/all/0/1&quot;&gt;Daewon Chae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Nokyung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinkyu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kimin Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03013">
<title>Breast Ultrasound Report Generation using LangChain. (arXiv:2312.03013v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.03013</link>
<description rdf:parseType="Literal">&lt;p&gt;Breast ultrasound (BUS) is a critical diagnostic tool in the field of breast
imaging, aiding in the early detection and characterization of breast
abnormalities. Interpreting breast ultrasound images commonly involves creating
comprehensive medical reports, containing vital information to promptly assess
the patient&apos;s condition. However, the ultrasound imaging system necessitates
capturing multiple images of various parts to compile a single report,
presenting a time-consuming challenge. To address this problem, we propose the
integration of multiple image analysis tools through a LangChain using Large
Language Models (LLM), into the breast reporting process. Through a combination
of designated tools and text generation through LangChain, our method can
accurately extract relevant features from ultrasound images, interpret them in
a clinical context, and produce comprehensive and standardized reports. This
approach not only reduces the burden on radiologists and healthcare
professionals but also enhances the consistency and quality of reports. The
extensive experiments shows that each tools involved in the proposed method can
offer qualitatively and quantitatively significant results. Furthermore,
clinical evaluation on the generated reports demonstrates that the proposed
method can make report in clinically meaningful way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huh_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Huh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Hyun Jeong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jong Chul Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03014">
<title>Foundation Models for Weather and Climate Data Understanding: A Comprehensive Survey. (arXiv:2312.03014v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03014</link>
<description rdf:parseType="Literal">&lt;p&gt;As artificial intelligence (AI) continues to rapidly evolve, the realm of
Earth and atmospheric sciences is increasingly adopting data-driven models,
powered by progressive developments in deep learning (DL). Specifically, DL
techniques are extensively utilized to decode the chaotic and nonlinear aspects
of Earth systems, and to address climate challenges via understanding weather
and climate data. Cutting-edge performance on specific tasks within narrower
spatio-temporal scales has been achieved recently through DL. The rise of large
models, specifically large language models (LLMs), has enabled fine-tuning
processes that yield remarkable outcomes across various downstream tasks,
thereby propelling the advancement of general AI. However, we are still
navigating the initial stages of crafting general AI for weather and climate.
In this survey, we offer an exhaustive, timely overview of state-of-the-art AI
methodologies specifically engineered for weather and climate data, with a
special focus on time series and text data. Our primary coverage encompasses
four critical aspects: types of weather and climate data, principal model
architectures, model scopes and applications, and datasets for weather and
climate. Furthermore, in relation to the creation and application of foundation
models for weather and climate data understanding, we delve into the field&apos;s
prevailing challenges, offer crucial insights, and propose detailed avenues for
future research. This comprehensive approach equips practitioners with the
requisite knowledge to make substantial progress in this domain. Our survey
encapsulates the most recent breakthroughs in research on large, data-driven
models for weather and climate data understanding, emphasizing robust
foundations, current advancements, practical applications, crucial resources,
and prospective research opportunities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shengchao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_G/0/1/0/all/0/1&quot;&gt;Guodong Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dikai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengqi Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03015">
<title>PartSLIP++: Enhancing Low-Shot 3D Part Segmentation via Multi-View Instance Segmentation and Maximum Likelihood Estimation. (arXiv:2312.03015v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03015</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-world 3D part segmentation is pivotal in diverse applications such as
robotics and AR/VR. Traditional supervised methods often grapple with limited
3D data availability and struggle to generalize to unseen object categories.
PartSLIP, a recent advancement, has made significant strides in zero- and
few-shot 3D part segmentation. This is achieved by harnessing the capabilities
of the 2D open-vocabulary detection module, GLIP, and introducing a heuristic
method for converting and lifting multi-view 2D bounding box predictions into
3D segmentation masks. In this paper, we introduce PartSLIP++, an enhanced
version designed to overcome the limitations of its predecessor. Our approach
incorporates two major improvements. First, we utilize a pre-trained 2D
segmentation model, SAM, to produce pixel-wise 2D segmentations, yielding more
precise and accurate annotations than the 2D bounding boxes used in PartSLIP.
Second, PartSLIP++ replaces the heuristic 3D conversion process with an
innovative modified Expectation-Maximization algorithm. This algorithm
conceptualizes 3D instance segmentation as unobserved latent variables, and
then iteratively refines them through an alternating process of 2D-3D matching
and optimization with gradient descent. Through extensive evaluations, we show
that PartSLIP++ demonstrates better performance over PartSLIP in both low-shot
3D semantic and instance-based object part segmentation tasks. Code released at
https://github.com/zyc00/PartSLIP2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiayuan Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03022">
<title>Beyond Isolation: Multi-Agent Synergy for Improving Knowledge Graph Construction. (arXiv:2312.03022v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03022</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph construction (KGC) is a multifaceted undertaking involving
the extraction of entities, relations, and events. Traditionally, large
language models (LLMs) have been viewed as solitary task-solving agents in this
complex landscape. However, this paper challenges this paradigm by introducing
a novel framework, CooperKGC. Departing from the conventional approach,
CooperKGC establishes a collaborative processing network, assembling a KGC
collaboration team capable of concurrently addressing entity, relation, and
event extraction tasks. Our experiments unequivocally demonstrate that
fostering collaboration and information interaction among diverse agents within
CooperKGC yields superior results compared to individual cognitive processes
operating in isolation. Importantly, our findings reveal that the collaboration
facilitated by CooperKGC enhances knowledge selection, correction, and
aggregation capabilities across multiple rounds of interactions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hongbin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gui_H/0/1/0/all/0/1&quot;&gt;Honghao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_A/0/1/0/all/0/1&quot;&gt;Aijia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wei Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Weiqiang Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03025">
<title>Training on Synthetic Data Beats Real Data in Multimodal Relation Extraction. (arXiv:2312.03025v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03025</link>
<description rdf:parseType="Literal">&lt;p&gt;The task of multimodal relation extraction has attracted significant research
attention, but progress is constrained by the scarcity of available training
data. One natural thought is to extend existing datasets with cross-modal
generative models. In this paper, we consider a novel problem setting, where
only unimodal data, either text or image, are available during training. We aim
to train a multimodal classifier from synthetic data that perform well on real
multimodal test data. However, training with synthetic data suffers from two
obstacles: lack of data diversity and label information loss. To alleviate the
issues, we propose Mutual Information-aware Multimodal Iterated Relational dAta
GEneration (MI2RAGE), which applies Chained Cross-modal Generation (CCG) to
promote diversity in the generated data and exploits a teacher network to
select valuable training samples with high mutual information with the
ground-truth labels. Comparing our method to direct training on synthetic data,
we observed a significant improvement of 24.06% F1 with synthetic text and
26.42% F1 with synthetic images. Notably, our best model trained on completely
synthetic images outperforms prior state-of-the-art models trained on real
multimodal data by a margin of 3.76% in F1. Our codebase will be made available
upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Z/0/1/0/all/0/1&quot;&gt;Zilin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_X/0/1/0/all/0/1&quot;&gt;Xu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03038">
<title>Sample-based Dynamic Hierarchical Transformer with Layer and Head Flexibility via Contextual Bandit. (arXiv:2312.03038v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03038</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer requires a fixed number of layers and heads which makes them
inflexible to the complexity of individual samples and expensive in training
and inference. To address this, we propose a sample-based Dynamic Hierarchical
Transformer (DHT) model whose layers and heads can be dynamically configured
with single data samples via solving contextual bandit problems. To determine
the number of layers and heads, we use the Uniform Confidence Bound while we
deploy combinatorial Thompson Sampling in order to select specific head
combinations given their number. Different from previous work that focuses on
compressing trained networks for inference only, DHT is not only advantageous
for adaptively optimizing the underlying network architecture during training
but also has a flexible network for efficient inference. To the best of our
knowledge, this is the first comprehensive data-driven dynamic transformer
without any additional auxiliary neural networks that implement the dynamic
system. According to the experiment results, we achieve up to 74% computational
savings for both training and inference with a minimal loss of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanfei Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lele Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03042">
<title>Inherent limitations of LLMs regarding spatial information. (arXiv:2312.03042v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03042</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the significant advancements in natural language processing
capabilities demonstrated by large language models such as ChatGPT, their
proficiency in comprehending and processing spatial information, especially
within the domains of 2D and 3D route planning, remains notably underdeveloped.
This paper investigates the inherent limitations of ChatGPT and similar models
in spatial reasoning and navigation-related tasks, an area critical for
applications ranging from autonomous vehicle guidance to assistive technologies
for the visually impaired. In this paper, we introduce a novel evaluation
framework complemented by a baseline dataset, meticulously crafted for this
study. This dataset is structured around three key tasks: plotting spatial
points, planning routes in two-dimensional (2D) spaces, and devising pathways
in three-dimensional (3D) environments. We specifically developed this dataset
to assess the spatial reasoning abilities of ChatGPT. Our evaluation reveals
key insights into the model&apos;s capabilities and limitations in spatial
understanding.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;He Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xinyao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiangpeng Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_K/0/1/0/all/0/1&quot;&gt;Kai Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shiqi Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03043">
<title>Navigating the Synthetic Realm: Harnessing Diffusion-based Models for Laparoscopic Text-to-Image Generation. (arXiv:2312.03043v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2312.03043</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in synthetic imaging open up opportunities for obtaining
additional data in the field of surgical imaging. This data can provide
reliable supplements supporting surgical applications and decision-making
through computer vision. Particularly the field of image-guided surgery, such
as laparoscopic and robotic-assisted surgery, benefits strongly from synthetic
image datasets and virtual surgical training methods. Our study presents an
intuitive approach for generating synthetic laparoscopic images from short text
prompts using diffusion-based generative models. We demonstrate the usage of
state-of-the-art text-to-image architectures in the context of laparoscopic
imaging with regard to the surgical removal of the gallbladder as an example.
Results on fidelity and diversity demonstrate that diffusion-based models can
acquire knowledge about the style and semantics in the field of image-guided
surgery. A validation study with a human assessment survey underlines the
realistic nature of our synthetic data, as medical personnel detects actual
images in a pool with generated images causing a false-positive rate of 66%. In
addition, the investigation of a state-of-the-art machine learning model to
recognize surgical actions indicates enhanced results when trained with
additional generated images of up to 5.20%. Overall, the achieved image quality
contributes to the usage of computer-generated images in surgical applications
and enhances its path to maturity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Allmendinger_S/0/1/0/all/0/1&quot;&gt;Simeon Allmendinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hemmer_P/0/1/0/all/0/1&quot;&gt;Patrick Hemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Queisner_M/0/1/0/all/0/1&quot;&gt;Moritz Queisner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sauer_I/0/1/0/all/0/1&quot;&gt;Igor Sauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_L/0/1/0/all/0/1&quot;&gt;Leopold M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jakubik_J/0/1/0/all/0/1&quot;&gt;Johannes Jakubik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vossing_M/0/1/0/all/0/1&quot;&gt;Michael V&amp;#xf6;ssing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuhl_N/0/1/0/all/0/1&quot;&gt;Niklas K&amp;#xfc;hl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03051">
<title>Generating Interpretable Networks using Hypernetworks. (arXiv:2312.03051v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03051</link>
<description rdf:parseType="Literal">&lt;p&gt;An essential goal in mechanistic interpretability to decode a network, i.e.,
to convert a neural network&apos;s raw weights to an interpretable algorithm. Given
the difficulty of the decoding problem, progress has been made to understand
the easier encoding problem, i.e., to convert an interpretable algorithm into
network weights. Previous works focus on encoding existing algorithms into
networks, which are interpretable by definition. However, focusing on encoding
limits the possibility of discovering new algorithms that humans have never
stumbled upon, but that are nevertheless interpretable. In this work, we
explore the possibility of using hypernetworks to generate interpretable
networks whose underlying algorithms are not yet known. The hypernetwork is
carefully designed such that it can control network complexity, leading to a
diverse family of interpretable algorithms ranked by their complexity. All of
them are interpretable in hindsight, although some of them are less intuitive
to humans, hence providing new insights regarding how to &quot;think&quot; like a neural
network. For the task of computing L1 norms, hypernetworks find three
algorithms: (a) the double-sided algorithm, (b) the convexity algorithm, (c)
the pudding algorithm, although only the first algorithm was expected by the
authors before experiments. We automatically classify these algorithms and
analyze how these algorithmic phases develop during training, as well as how
they are affected by complexity control. Furthermore, we show that a trained
hypernetwork can correctly construct models for input dimensions not seen in
training, demonstrating systematic generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_I/0/1/0/all/0/1&quot;&gt;Isaac Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tegmark_M/0/1/0/all/0/1&quot;&gt;Max Tegmark&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03077">
<title>Clinical Notes Reveal Physician Fatigue. (arXiv:2312.03077v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03077</link>
<description rdf:parseType="Literal">&lt;p&gt;Physicians write notes about patients. In doing so, they reveal much about
themselves. Using data from 129,228 emergency room visits, we train a model to
identify notes written by fatigued physicians -- those who worked 5 or more of
the prior 7 days. In a hold-out set, the model accurately identifies notes
written by these high-workload physicians, and also flags notes written in
other high-fatigue settings: on overnight shifts, and after high patient
volumes. Model predictions also correlate with worse decision-making on at
least one important metric: yield of testing for heart attack is 18% lower with
each standard deviation increase in model-predicted fatigue. Finally, the model
indicates that notes written about Black and Hispanic patients have 12% and 21%
higher predicted fatigue than Whites -- larger than overnight vs. daytime
differences. These results have an important implication for large language
models (LLMs). Our model indicates that fatigued doctors write more predictable
notes. Perhaps unsurprisingly, because word prediction is the core of how LLMs
work, we find that LLM-written notes have 17% higher predicted fatigue than
real physicians&apos; notes. This indicates that LLMs may introduce distortions in
generated text that are not yet fully understood.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsu_C/0/1/0/all/0/1&quot;&gt;Chao-Chun Hsu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obermeyer_Z/0/1/0/all/0/1&quot;&gt;Ziad Obermeyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chenhao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03093">
<title>RESIN-EDITOR: A Schema-guided Hierarchical Event Graph Visualizer and Editor. (arXiv:2312.03093v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.03093</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present RESIN-EDITOR, an interactive event graph visualizer
and editor designed for analyzing complex events. Our RESIN-EDITOR system
allows users to render and freely edit hierarchical event graphs extracted from
multimedia and multi-document news clusters with guidance from human-curated
event schemas. RESIN-EDITOR&apos;s unique features include hierarchical graph
visualization, comprehensive source tracing, and interactive user editing,
which is more powerful and versatile than existing Information Extraction (IE)
visualization tools. In our evaluation of RESIN-EDITOR, we demonstrate ways in
which our tool is effective in understanding complex events and enhancing
system performance. The source code, a video demonstration, and a live website
for RESIN-EDITOR have been made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_K/0/1/0/all/0/1&quot;&gt;Khanh Duy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchocki_R/0/1/0/all/0/1&quot;&gt;Reece Suchocki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Sha Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palmer_M/0/1/0/all/0/1&quot;&gt;Martha Palmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_S/0/1/0/all/0/1&quot;&gt;Susan Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiawei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03096">
<title>Incidental Polysemanticity. (arXiv:2312.03096v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03096</link>
<description rdf:parseType="Literal">&lt;p&gt;Polysemantic neurons (neurons that activate for a set of unrelated features)
have been seen as a significant obstacle towards interpretability of
task-optimized deep networks, with implications for AI safety. The classic
origin story of polysemanticity is that the data contains more &quot;features&quot; than
neurons, such that learning to perform a task forces the network to co-allocate
multiple unrelated features to the same neuron, endangering our ability to
understand the network&apos;s internal processing. In this work, we present a second
and non-mutually exclusive origin story of polysemanticity. We show that
polysemanticity can arise incidentally, even when there are ample neurons to
represent all features in the data, using a combination of theory and
experiments. This second type of polysemanticity occurs because random
initialization can, by chance alone, initially assign multiple features to the
same neuron, and the training dynamics then strengthen such overlap. Due to its
origin, we term this \textit{incidental polysemanticity}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lecomte_V/0/1/0/all/0/1&quot;&gt;Victor Lecomte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thaman_K/0/1/0/all/0/1&quot;&gt;Kushal Thaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chow_T/0/1/0/all/0/1&quot;&gt;Trevor Chow&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaeffer_R/0/1/0/all/0/1&quot;&gt;Rylan Schaeffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koyejo_S/0/1/0/all/0/1&quot;&gt;Sanmi Koyejo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03120">
<title>The Landscape of Modern Machine Learning: A Review of Machine, Distributed and Federated Learning. (arXiv:2312.03120v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03120</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advance of the powerful heterogeneous, parallel and distributed
computing systems and ever increasing immense amount of data, machine learning
has become an indispensable part of cutting-edge technology, scientific
research and consumer products. In this study, we present a review of modern
machine and deep learning. We provide a high-level overview for the latest
advanced machine learning algorithms, applications, and frameworks. Our
discussion encompasses parallel distributed learning, deep learning as well as
federated learning. As a result, our work serves as an introductory text to the
vast field of modern machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subasi_O/0/1/0/all/0/1&quot;&gt;Omer Subasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bel_O/0/1/0/all/0/1&quot;&gt;Oceane Bel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzano_J/0/1/0/all/0/1&quot;&gt;Joseph Manzano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barker_K/0/1/0/all/0/1&quot;&gt;Kevin Barker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03121">
<title>Evaluating Agents using Social Choice Theory. (arXiv:2312.03121v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03121</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that many general evaluation problems can be viewed through the lens
of voting theory. Each task is interpreted as a separate voter, which requires
only ordinal rankings or pairwise comparisons of agents to produce an overall
evaluation. By viewing the aggregator as a social welfare function, we are able
to leverage centuries of research in social choice theory to derive principled
evaluation frameworks with axiomatic foundations. These evaluations are
interpretable and flexible, while avoiding many of the problems currently
facing cross-task evaluation. We apply this Voting-as-Evaluation (VasE)
framework across multiple settings, including reinforcement learning, large
language models, and humans. In practice, we observe that VasE can be more
robust than popular evaluation frameworks (Elo and Nash averaging), discovers
properties in the evaluation data not evident from scores alone, and can
predict outcomes better than Elo in a complex seven-player game. We identify
one particular approach, maximal lotteries, that satisfies important
consistency properties relevant to evaluation, is computationally efficient
(polynomial in the size of the evaluation data), and identifies game-theoretic
cycles
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lanctot_M/0/1/0/all/0/1&quot;&gt;Marc Lanctot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larson_K/0/1/0/all/0/1&quot;&gt;Kate Larson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bachrach_Y/0/1/0/all/0/1&quot;&gt;Yoram Bachrach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marris_L/0/1/0/all/0/1&quot;&gt;Luke Marris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhoopchand_A/0/1/0/all/0/1&quot;&gt;Avishkar Bhoopchand&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anthony_T/0/1/0/all/0/1&quot;&gt;Thomas Anthony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanner_B/0/1/0/all/0/1&quot;&gt;Brian Tanner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koop_A/0/1/0/all/0/1&quot;&gt;Anna Koop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03126">
<title>Learning Curricula in Open-Ended Worlds. (arXiv:2312.03126v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03126</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) provides powerful methods for training
optimal sequential decision-making agents. As collecting real-world
interactions can entail additional costs and safety risks, the common paradigm
of sim2real conducts training in a simulator, followed by real-world
deployment. Unfortunately, RL agents easily overfit to the choice of simulated
training environments, and worse still, learning ends when the agent masters
the specific set of simulated environments. In contrast, the real world is
highly open-ended, featuring endlessly evolving environments and challenges,
making such RL approaches unsuitable. Simply randomizing over simulated
environments is insufficient, as it requires making arbitrary distributional
assumptions and can be combinatorially less likely to sample specific
environment instances that are useful for learning. An ideal learning process
should automatically adapt the training environment to maximize the learning
potential of the agent over an open-ended task space that matches or surpasses
the complexity of the real world. This thesis develops a class of methods
called Unsupervised Environment Design (UED), which aim to produce such
open-ended processes. Given an environment design space, UED automatically
generates an infinite sequence or curriculum of training environments at the
frontier of the learning agent&apos;s capabilities. Through extensive empirical
studies and theoretical arguments founded on minimax-regret decision theory and
game theory, the findings in this thesis show that UED autocurricula can
produce RL agents exhibiting significantly improved robustness and
generalization to previously unseen environment instances. Such autocurricula
are promising paths toward open-ended learning systems that achieve more
general intelligence by continually generating and mastering additional
challenges of their own design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03140">
<title>FlexModel: A Framework for Interpretability of Distributed Large Language Models. (arXiv:2312.03140v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03140</link>
<description rdf:parseType="Literal">&lt;p&gt;With the growth of large language models, now incorporating billions of
parameters, the hardware prerequisites for their training and deployment have
seen a corresponding increase. Although existing tools facilitate model
parallelization and distributed training, deeper model interactions, crucial
for interpretability and responsible AI techniques, still demand thorough
knowledge of distributed computing. This often hinders contributions from
researchers with machine learning expertise but limited distributed computing
background. Addressing this challenge, we present FlexModel, a software package
providing a streamlined interface for engaging with models distributed across
multi-GPU and multi-node configurations. The library is compatible with
existing model distribution libraries and encapsulates PyTorch models. It
exposes user-registerable HookFunctions to facilitate straightforward
interaction with distributed model internals, bridging the gap between
distributed and single-device model paradigms. Primarily, FlexModel enhances
accessibility by democratizing model interactions and promotes more inclusive
research in the domain of large-scale neural networks. The package is found at
https://github.com/VectorInstitute/flex_model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Matthew Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asif_M/0/1/0/all/0/1&quot;&gt;Muhammad Adil Asif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willes_J/0/1/0/all/0/1&quot;&gt;John Willes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emerson_D/0/1/0/all/0/1&quot;&gt;David Emerson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03173">
<title>A Comparative Study of AI-Generated (GPT-4) and Human-crafted MCQs in Programming Education. (arXiv:2312.03173v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.03173</link>
<description rdf:parseType="Literal">&lt;p&gt;There is a constant need for educators to develop and maintain effective
up-to-date assessments. While there is a growing body of research in computing
education on utilizing large language models (LLMs) in generation and
engagement with coding exercises, the use of LLMs for generating programming
MCQs has not been extensively explored. We analyzed the capability of GPT-4 to
produce multiple-choice questions (MCQs) aligned with specific learning
objectives (LOs) from Python programming classes in higher education.
Specifically, we developed an LLM-powered (GPT-4) system for generation of MCQs
from high-level course context and module-level LOs. We evaluated 651
LLM-generated and 449 human-crafted MCQs aligned to 246 LOs from 6 Python
courses. We found that GPT-4 was capable of producing MCQs with clear language,
a single correct choice, and high-quality distractors. We also observed that
the generated MCQs appeared to be well-aligned with the LOs. Our findings can
be leveraged by educators wishing to take advantage of the state-of-the-art
generative models to support MCQ authoring efforts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_J/0/1/0/all/0/1&quot;&gt;Jacob Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Z/0/1/0/all/0/1&quot;&gt;Zipiao Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bompelli_A/0/1/0/all/0/1&quot;&gt;Anishka Bompelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qayum_J/0/1/0/all/0/1&quot;&gt;Jubahed Qayum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Taozhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Juran Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yujia Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doyle_A/0/1/0/all/0/1&quot;&gt;Aidan Doyle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sridhar_P/0/1/0/all/0/1&quot;&gt;Pragnya Sridhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarwal_A/0/1/0/all/0/1&quot;&gt;Arav Agarwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogart_C/0/1/0/all/0/1&quot;&gt;Christopher Bogart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keylor_E/0/1/0/all/0/1&quot;&gt;Eric Keylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kultur_C/0/1/0/all/0/1&quot;&gt;Can Kultur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savelka_J/0/1/0/all/0/1&quot;&gt;Jaromir Savelka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sakr_M/0/1/0/all/0/1&quot;&gt;Majd Sakr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03177">
<title>Using Curiosity for an Even Representation of Tasks in Continual Offline Reinforcement Learning. (arXiv:2312.03177v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03177</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we investigate the means of using curiosity on replay buffers
to improve offline multi-task continual reinforcement learning when tasks,
which are defined by the non-stationarity in the environment, are non labeled
and not evenly exposed to the learner in time. In particular, we investigate
the use of curiosity both as a tool for task boundary detection and as a
priority metric when it comes to retaining old transition tuples, which we
respectively use to propose two different buffers. Firstly, we propose a Hybrid
Reservoir Buffer with Task Separation (HRBTS), where curiosity is used to
detect task boundaries that are not known due to the task agnostic nature of
the problem. Secondly, by using curiosity as a priority metric when it comes to
retaining old transition tuples, a Hybrid Curious Buffer (HCB) is proposed. We
ultimately show that these buffers, in conjunction with regular reinforcement
learning algorithms, can be used to alleviate the catastrophic forgetting issue
suffered by the state of the art on replay buffers when the agent&apos;s exposure to
tasks is not equal along time. We evaluate catastrophic forgetting and the
efficiency of our proposed buffers against the latest works such as the Hybrid
Reservoir Buffer (HRB) and the Multi-Time Scale Replay Buffer (MTR) in three
different continual reinforcement learning settings. Experiments were done on
classical control tasks and Metaworld environment. Experiments show that our
proposed replay buffers display better immunity to catastrophic forgetting
compared to existing works in most of the settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathmanathan_P/0/1/0/all/0/1&quot;&gt;Pankayaraj Pathmanathan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_Rodriguez_N/0/1/0/all/0/1&quot;&gt;Natalia D&amp;#xed;az-Rodr&amp;#xed;guez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ser_J/0/1/0/all/0/1&quot;&gt;Javier Del Ser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03186">
<title>Data-Driven Traffic Reconstruction and Kernel Methods for Identifying Stop-and-Go Congestion. (arXiv:2312.03186v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03186</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying stop-and-go events (SAGs) in traffic flow presents an important
avenue for advancing data-driven research for climate change mitigation and
sustainability, owing to their substantial impact on carbon emissions, travel
time, fuel consumption, and roadway safety. In fact, SAGs are estimated to
account for 33-50% of highway driving externalities. However, insufficient
attention has been paid to precisely quantifying where, when, and how much
these SAGs take place -necessary for downstream decision making, such as
intervention design and policy analysis. A key challenge is that the data
available to researchers and governments are typically sparse and aggregated to
a granularity that obscures SAGs. To overcome such data limitations, this study
thus explores the use of traffic reconstruction techniques for SAG
identification. In particular, we introduce a kernel-based method for
identifying spatio-temporal features in traffic and leverage bootstrapping to
quantify the uncertainty of the reconstruction process. Experimental results on
California highway data demonstrate the promise of the method for capturing
SAGs. This work contributes to a foundation for data-driven decision making to
advance sustainability of traffic systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_E/0/1/0/all/0/1&quot;&gt;Edgar Ramirez Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghavan_S/0/1/0/all/0/1&quot;&gt;Shreyaa Raghavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Cathy Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03187">
<title>FERGI: Automatic Annotation of User Preferences for Text-to-Image Generation from Spontaneous Facial Expression Reaction. (arXiv:2312.03187v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03187</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers have proposed to use data of human preference feedback to
fine-tune text-to-image generative models. However, the scalability of human
feedback collection has been limited by its reliance on manual annotation.
Therefore, we develop and test a method to automatically annotate user
preferences from their spontaneous facial expression reaction to the generated
images. We collect a dataset of Facial Expression Reaction to Generated Images
(FERGI) and show that the activations of multiple facial action units (AUs) are
highly correlated with user evaluations of the generated images. Specifically,
AU4 (brow lowerer) is most consistently reflective of negative evaluations of
the generated image. This can be useful in two ways. Firstly, we can
automatically annotate user preferences between image pairs with substantial
difference in AU4 responses to them with an accuracy significantly
outperforming state-of-the-art scoring models. Secondly, directly integrating
the AU4 responses with the scoring models improves their consistency with human
preferences. Additionally, the AU4 response best reflects the user&apos;s evaluation
of the image fidelity, making it complementary to the state-of-the-art scoring
models, which are generally better at reflecting image-text alignment. Finally,
this method of automatic annotation with facial expression analysis can be
potentially generalized to other generation tasks. The code is available at
https://github.com/ShuangquanFeng/FERGI, and the dataset is also available at
the same link for research purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shuangquan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Junhua Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sa_V/0/1/0/all/0/1&quot;&gt;Virginia R. de Sa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03216">
<title>SDSRA: A Skill-Driven Skill-Recombination Algorithm for Efficient Policy Learning. (arXiv:2312.03216v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03216</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel algorithm - the Skill-Driven Skill
Recombination Algorithm (SDSRA) - an innovative framework that significantly
enhances the efficiency of achieving maximum entropy in reinforcement learning
tasks. We find that SDSRA achieves faster convergence compared to the
traditional Soft Actor-Critic (SAC) algorithm and produces improved policies.
By integrating skill-based strategies within the robust Actor-Critic framework,
SDSRA demonstrates remarkable adaptability and performance across a wide array
of complex and diverse benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_E/0/1/0/all/0/1&quot;&gt;Eric H. Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lizarraga_A/0/1/0/all/0/1&quot;&gt;Andrew Lizarraga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03231">
<title>Deep Multimodal Fusion for Surgical Feedback Classification. (arXiv:2312.03231v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03231</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantification of real-time informal feedback delivered by an experienced
surgeon to a trainee during surgery is important for skill improvements in
surgical training. Such feedback in the live operating room is inherently
multimodal, consisting of verbal conversations (e.g., questions and answers) as
well as non-verbal elements (e.g., through visual cues like pointing to
anatomic elements). In this work, we leverage a clinically-validated
five-category classification of surgical feedback: &quot;Anatomic&quot;, &quot;Technical&quot;,
&quot;Procedural&quot;, &quot;Praise&quot; and &quot;Visual Aid&quot;. We then develop a multi-label machine
learning model to classify these five categories of surgical feedback from
inputs of text, audio, and video modalities. The ultimate goal of our work is
to help automate the annotation of real-time contextual surgical feedback at
scale. Our automated classification of surgical feedback achieves AUCs ranging
from 71.5 to 77.6 with the fusion improving performance by 3.1%. We also show
that high-quality manual transcriptions of feedback audio from experts improve
AUCs to between 76.5 and 96.2, which demonstrates a clear path toward future
improvements. Empirically, we find that the Staged training strategy, with
first pre-training each modality separately and then training them jointly, is
more effective than training different modalities altogether. We also present
intuitive findings on the importance of modalities for different feedback
categories. This work offers an important first look at the feasibility of
automated classification of real-world live surgical feedback based on text,
audio, and video modalities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocielnik_R/0/1/0/all/0/1&quot;&gt;Rafal Kocielnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Elyssa Y. Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1&quot;&gt;Timothy N. Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lydia Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;De-An Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiayun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_A/0/1/0/all/0/1&quot;&gt;Andrew J. Hung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03236">
<title>Multicoated and Folded Graph Neural Networks with Strong Lottery Tickets. (arXiv:2312.03236v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03236</link>
<description rdf:parseType="Literal">&lt;p&gt;The Strong Lottery Ticket Hypothesis (SLTH) demonstrates the existence of
high-performing subnetworks within a randomly initialized model, discoverable
through pruning a convolutional neural network (CNN) without any weight
training. A recent study, called Untrained GNNs Tickets (UGT), expanded SLTH
from CNNs to shallow graph neural networks (GNNs). However, discrepancies
persist when comparing baseline models with learned dense weights.
Additionally, there remains an unexplored area in applying SLTH to deeper GNNs,
which, despite delivering improved accuracy with additional layers, suffer from
excessive memory requirements. To address these challenges, this work utilizes
Multicoated Supermasks (M-Sup), a scalar pruning mask method, and implements it
in GNNs by proposing a strategy for setting its pruning thresholds adaptively.
In the context of deep GNNs, this research uncovers the existence of untrained
recurrent networks, which exhibit performance on par with their trained
feed-forward counterparts. This paper also introduces the Multi-Stage Folding
and Unshared Masks methods to expand the search space in terms of both
architecture and parameters. Through the evaluation of various datasets,
including the Open Graph Benchmark (OGB), this work establishes a triple-win
scenario for SLTH-based GNNs: by achieving high sparsity, competitive
performance, and high memory efficiency with up to 98.7\% reduction, it
demonstrates suitability for energy-efficient graph processing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiale Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ito_H/0/1/0/all/0/1&quot;&gt;Hiroaki Ito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Arias_A/0/1/0/all/0/1&quot;&gt;&amp;#xc1;ngel L&amp;#xf3;pez Garc&amp;#xed;a-Arias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okoshi_Y/0/1/0/all/0/1&quot;&gt;Yasuyuki Okoshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otsuka_H/0/1/0/all/0/1&quot;&gt;Hikari Otsuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kawamura_K/0/1/0/all/0/1&quot;&gt;Kazushi Kawamura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_T/0/1/0/all/0/1&quot;&gt;Thiem Van Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Motomura_M/0/1/0/all/0/1&quot;&gt;Masato Motomura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03245">
<title>A Simple Framework to Enhance the Adversarial Robustness of Deep Learning-based Intrusion Detection System. (arXiv:2312.03245v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.03245</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based intrusion detection systems (DL-based IDS) have emerged
as one of the best choices for providing security solutions against various
network intrusion attacks. However, due to the emergence and development of
adversarial deep learning technologies, it becomes challenging for the adoption
of DL models into IDS. In this paper, we propose a novel IDS architecture that
can enhance the robustness of IDS against adversarial attacks by combining
conventional machine learning (ML) models and Deep Learning models. The
proposed DLL-IDS consists of three components: DL-based IDS, adversarial
example (AE) detector, and ML-based IDS. We first develop a novel AE detector
based on the local intrinsic dimensionality (LID). Then, we exploit the low
attack transferability between DL models and ML models to find a robust ML
model that can assist us in determining the maliciousness of AEs. If the input
traffic is detected as an AE, the ML-based IDS will predict the maliciousness
of input traffic, otherwise the DL-based IDS will work for the prediction. The
fusion mechanism can leverage the high prediction accuracy of DL models and low
attack transferability between DL models and ML models to improve the
robustness of the whole system. In our experiments, we observe a significant
improvement in the prediction performance of the IDS when subjected to
adversarial attack, achieving high accuracy with low resource consumption.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xinwei Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_S/0/1/0/all/0/1&quot;&gt;Shu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_H/0/1/0/all/0/1&quot;&gt;Hongliang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_X/0/1/0/all/0/1&quot;&gt;Xianglong Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03248">
<title>Customizable Combination of Parameter-Efficient Modules for Multi-Task Learning. (arXiv:2312.03248v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03248</link>
<description rdf:parseType="Literal">&lt;p&gt;Modular and composable transfer learning is an emerging direction in the
field of Parameter Efficient Fine-Tuning, as it enables neural networks to
better organize various aspects of knowledge, leading to improved cross-task
generalization. In this paper, we introduce a novel approach Customized
Polytropon C-Poly that combines task-common skills and task-specific skills,
while the skill parameters being highly parameterized using low-rank
techniques. Each task is associated with a customizable number of exclusive
specialized skills and also benefits from skills shared with peer tasks. A
skill assignment matrix is jointly learned. To evaluate our approach, we
conducted extensive experiments on the Super-NaturalInstructions and the
SuperGLUE benchmarks. Our findings demonstrate that C-Poly outperforms
fully-shared, task-specific, and skill-indistinguishable baselines,
significantly enhancing the sample efficiency in multi-task learning scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haowen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_T/0/1/0/all/0/1&quot;&gt;Tao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_C/0/1/0/all/0/1&quot;&gt;Cong Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jinjie Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03263">
<title>Weathering Ongoing Uncertainty: Learning and Planning in a Time-Varying Partially Observable Environment. (arXiv:2312.03263v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.03263</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimal decision-making presents a significant challenge for autonomous
systems operating in uncertain, stochastic and time-varying environments.
Environmental variability over time can significantly impact the system&apos;s
optimal decision making strategy for mission completion. To model such
environments, our work combines the previous notion of Time-Varying Markov
Decision Processes (TVMDP) with partial observability and introduces
Time-Varying Partially Observable Markov Decision Processes (TV-POMDP). We
propose a two-pronged approach to accurately estimate and plan within the
TV-POMDP: 1) Memory Prioritized State Estimation (MPSE), which leverages
weighted memory to provide more accurate time-varying transition estimates; and
2) an MPSE-integrated planning strategy that optimizes long-term rewards while
accounting for temporal constraint. We validate the proposed framework and
algorithms using simulations and hardware, with robots exploring a partially
observable, time-varying environments. Our results demonstrate superior
performance over standard methods, highlighting the framework&apos;s effectiveness
in stochastic, uncertain, time-varying domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puthumanaillam_G/0/1/0/all/0/1&quot;&gt;Gokul Puthumanaillam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiangyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehr_N/0/1/0/all/0/1&quot;&gt;Negar Mehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ornik_M/0/1/0/all/0/1&quot;&gt;Melkior Ornik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03275">
<title>VLFM: Vision-Language Frontier Maps for Zero-Shot Semantic Navigation. (arXiv:2312.03275v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.03275</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding how humans leverage semantic knowledge to navigate unfamiliar
environments and decide where to explore next is pivotal for developing robots
capable of human-like search behaviors. We introduce a zero-shot navigation
approach, Vision-Language Frontier Maps (VLFM), which is inspired by human
reasoning and designed to navigate towards unseen semantic objects in novel
environments. VLFM builds occupancy maps from depth observations to identify
frontiers, and leverages RGB observations and a pre-trained vision-language
model to generate a language-grounded value map. VLFM then uses this map to
identify the most promising frontier to explore for finding an instance of a
given target object category. We evaluate VLFM in photo-realistic environments
from the Gibson, Habitat-Matterport 3D (HM3D), and Matterport 3D (MP3D)
datasets within the Habitat simulator. Remarkably, VLFM achieves
state-of-the-art results on all three datasets as measured by success weighted
by path length (SPL) for the Object Goal Navigation task. Furthermore, we show
that VLFM&apos;s zero-shot nature enables it to be readily deployed on real-world
robots such as the Boston Dynamics Spot mobile manipulation platform. We deploy
VLFM on Spot and demonstrate its capability to efficiently navigate to target
objects within an office building in the real world, without any prior
knowledge of the environment. The accomplishments of VLFM underscore the
promising potential of vision-language models in advancing the field of
semantic navigation. Videos of real-world deployment can be viewed at
naoki.io/vlfm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yokoyama_N/0/1/0/all/0/1&quot;&gt;Naoki Yokoyama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_S/0/1/0/all/0/1&quot;&gt;Sehoon Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batra_D/0/1/0/all/0/1&quot;&gt;Dhruv Batra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiuguang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bucher_B/0/1/0/all/0/1&quot;&gt;Bernadette Bucher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03288">
<title>STEP CATFormer: Spatial-Temporal Effective Body-Part Cross Attention Transformer for Skeleton-based Action Recognition. (arXiv:2312.03288v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03288</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks (GCNs) have been widely used and achieved
remarkable results in skeleton-based action recognition. We think the key to
skeleton-based action recognition is a skeleton hanging in frames, so we focus
on how the Graph Convolutional Convolution networks learn different topologies
and effectively aggregate joint features in the global temporal and local
temporal. In this work, we propose three Channel-wise Tolopogy Graph
Convolution based on Channel-wise Topology Refinement Graph Convolution
(CTR-GCN). Combining CTR-GCN with two joint cross-attention modules can capture
the upper-lower body part and hand-foot relationship skeleton features. After
that, to capture features of human skeletons changing in frames we design the
Temporal Attention Transformers to extract skeletons effectively. The Temporal
Attention Transformers can learn the temporal features of human skeleton
sequences. Finally, we fuse the temporal features output scale with MLP and
classification. We develop a powerful graph convolutional network named Spatial
Temporal Effective Body-part Cross Attention Transformer which notably
high-performance on the NTU RGB+D, NTU RGB+D 120 datasets. Our code and models
are available at https://github.com/maclong01/STEP-CATFormer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_N/0/1/0/all/0/1&quot;&gt;Nguyen Huu Bao Long&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03290">
<title>Can language agents be alternatives to PPO? A Preliminary Empirical Study On OpenAI Gym. (arXiv:2312.03290v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03290</link>
<description rdf:parseType="Literal">&lt;p&gt;The formidable capacity for zero- or few-shot decision-making in language
agents encourages us to pose a compelling question: Can language agents be
alternatives to PPO agents in traditional sequential decision-making tasks? To
investigate this, we first take environments collected in OpenAI Gym as our
testbeds and ground them to textual environments that construct the TextGym
simulator. This allows for straightforward and efficient comparisons between
PPO agents and language agents, given the widespread adoption of OpenAI Gym. To
ensure a fair and effective benchmarking, we introduce $5$ levels of scenario
for accurate domain-knowledge controlling and a unified RL-inspired framework
for language agents. Additionally, we propose an innovative
explore-exploit-guided language (EXE) agent to solve tasks within TextGym.
Through numerical experiments and ablation studies, we extract valuable
insights into the decision-making capabilities of language agents and make a
preliminary evaluation of their potential to be alternatives to PPO in
classical sequential decision-making problems. This paper sheds light on the
performance of language agents and paves the way for future research in this
exciting domain. Our code is publicly available
at~\url{https://github.com/mail-ecnu/Text-Gym-Agents}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_J/0/1/0/all/0/1&quot;&gt;Junjie Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zixiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chuyun Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_Y/0/1/0/all/0/1&quot;&gt;Yun Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bo Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_H/0/1/0/all/0/1&quot;&gt;Hongyuan Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiangfeng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03291">
<title>OMNIINPUT: A Model-centric Evaluation Framework through Output Distribution. (arXiv:2312.03291v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03291</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel model-centric evaluation framework, OmniInput, to evaluate
the quality of an AI/ML model&apos;s predictions on all possible inputs (including
human-unrecognizable ones), which is crucial for AI safety and reliability.
Unlike traditional data-centric evaluation based on pre-defined test sets, the
test set in OmniInput is self-constructed by the model itself and the model
quality is evaluated by investigating its output distribution. We employ an
efficient sampler to obtain representative inputs and the output distribution
of the trained model, which, after selective annotation, can be used to
estimate the model&apos;s precision and recall at different output values and a
comprehensive precision-recall curve. Our experiments demonstrate that
OmniInput enables a more fine-grained comparison between models, especially
when their performance is almost the same on pre-defined datasets, leading to
new findings and insights for how to train more robust, generalizable models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Weitang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Ying Wai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianle Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yi-Zhuang You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_J/0/1/0/all/0/1&quot;&gt;Jingbo Shang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03297">
<title>SoftMAC: Differentiable Soft Body Simulation with Forecast-based Contact Model and Two-way Coupling with Articulated Rigid Bodies and Clothes. (arXiv:2312.03297v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.03297</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentiable physics simulation provides an avenue for tackling previously
intractable challenges through gradient-based optimization, thereby greatly
improving the efficiency of solving robotics-related problems. To apply
differentiable simulation in diverse robotic manipulation scenarios, a key
challenge is to integrate various materials in a unified framework. We present
SoftMAC, a differentiable simulation framework coupling soft bodies with
articulated rigid bodies and clothes. SoftMAC simulates soft bodies with the
continuum-mechanics-based Material Point Method (MPM). We provide a
forecast-based contact model for MPM, which greatly reduces artifacts like
penetration and unnatural rebound. To couple MPM particles with deformable and
non-volumetric clothes meshes, we also propose a penetration tracing algorithm
that reconstructs the signed distance field in local area. Based on simulators
for each modality and the contact model, we develop a differentiable coupling
mechanism to simulate the interactions between soft bodies and the other two
types of materials. Comprehensive experiments are conducted to validate the
effectiveness and accuracy of the proposed differentiable pipeline in
downstream robotic manipulation applications. Supplementary materials and
videos are available on our project website at
https://sites.google.com/view/softmac.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Min Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Gang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Siyuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Lin Shao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03303">
<title>Dyport: Dynamic Importance-based Hypothesis Generation Benchmarking Technique. (arXiv:2312.03303v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03303</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel benchmarking framework Dyport for evaluating
biomedical hypothesis generation systems. Utilizing curated datasets, our
approach tests these systems under realistic conditions, enhancing the
relevance of our evaluations. We integrate knowledge from the curated databases
into a dynamic graph, accompanied by a method to quantify discovery importance.
This not only assesses hypothesis accuracy but also their potential impact in
biomedical research which significantly extends traditional link prediction
benchmarks. Applicability of our benchmarking process is demonstrated on
several link prediction systems applied on biomedical semantic knowledge
graphs. Being flexible, our benchmarking system is designed for broad
application in hypothesis generation quality verification, aiming to expand the
scope of scientific discovery within the biomedical research community.
Availability and implementation: Dyport framework is fully open-source. All
code and datasets are available at: https://github.com/IlyaTyagin/Dyport
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyagin_I/0/1/0/all/0/1&quot;&gt;Ilya Tyagin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safro_I/0/1/0/all/0/1&quot;&gt;Ilya Safro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03309">
<title>Benchmarking Continual Learning from Cognitive Perspectives. (arXiv:2312.03309v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03309</link>
<description rdf:parseType="Literal">&lt;p&gt;Continual learning addresses the problem of continuously acquiring and
transferring knowledge without catastrophic forgetting of old concepts. While
humans achieve continual learning via diverse neurocognitive mechanisms, there
is a mismatch between cognitive properties and evaluation methods of continual
learning models. First, the measurement of continual learning models mostly
relies on evaluation metrics at a micro-level, which cannot characterize
cognitive capacities of the model. Second, the measurement is method-specific,
emphasizing model strengths in one aspect while obscuring potential weaknesses
in other respects. To address these issues, we propose to integrate model
cognitive capacities and evaluation metrics into a unified evaluation paradigm.
We first characterize model capacities via desiderata derived from cognitive
properties supporting human continual learning. The desiderata concern (1)
adaptability in varying lengths of task sequence; (2) sensitivity to dynamic
task variations; and (3) efficiency in memory usage and training time
consumption. Then we design evaluation protocols for each desideratum to assess
cognitive capacities of recent continual learning models. Experimental results
show that no method we consider has satisfied all the desiderata and is still
far away from realizing truly continual learning. Although some methods exhibit
some degree of adaptability and efficiency, no method is able to identify task
relationships when encountering dynamic task variations, or achieve a trade-off
in learning similarities and differences between tasks. Inspired by these
results, we discuss possible factors that influence model performance in these
desiderata and provide guidance for the improvement of continual learning
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoqian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_P/0/1/0/all/0/1&quot;&gt;Peipei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03341">
<title>Online Vectorized HD Map Construction using Geometry. (arXiv:2312.03341v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03341</link>
<description rdf:parseType="Literal">&lt;p&gt;The construction of online vectorized High-Definition (HD) maps is critical
for downstream prediction and planning. Recent efforts have built strong
baselines for this task, however, shapes and relations of instances in urban
road systems are still under-explored, such as parallelism, perpendicular, or
rectangle-shape. In our work, we propose GeMap ($\textbf{Ge}$ometry
$\textbf{Map}$), which end-to-end learns Euclidean shapes and relations of map
instances beyond basic perception. Specifically, we design a geometric loss
based on angle and distance clues, which is robust to rigid transformations. We
also decouple self-attention to independently handle Euclidean shapes and
relations. Our method achieves new state-of-the-art performance on the NuScenes
and Argoverse 2 datasets. Remarkably, it reaches a 71.8% mAP on the large-scale
Argoverse 2 dataset, outperforming MapTR V2 by +4.4% and surpassing the 70% mAP
threshold for the first time. Code is available at
https://github.com/cnzzx/GeMap
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaohan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_F/0/1/0/all/0/1&quot;&gt;Fusheng Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03360">
<title>Teaching Specific Scientific Knowledge into Large Language Models through Additional Training. (arXiv:2312.03360v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03360</link>
<description rdf:parseType="Literal">&lt;p&gt;Through additional training, we explore embedding specialized scientific
knowledge into the Llama 2 Large Language Model (LLM). Key findings reveal that
effective knowledge integration requires reading texts from multiple
perspectives, especially in instructional formats. We utilize text augmentation
to tackle the scarcity of specialized texts, including style conversions and
translations. Hyperparameter optimization proves crucial, with different size
models (7b, 13b, and 70b) reasonably undergoing additional training. Validating
our methods, we construct a dataset of 65,000 scientific papers. Although we
have succeeded in partially embedding knowledge, the study highlights the
complexities and limitations of incorporating specialized information into
LLMs, suggesting areas for further improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatakeyama_Sato_K/0/1/0/all/0/1&quot;&gt;Kan Hatakeyama-Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igarashi_Y/0/1/0/all/0/1&quot;&gt;Yasuhiko Igarashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katakami_S/0/1/0/all/0/1&quot;&gt;Shun Katakami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabae_Y/0/1/0/all/0/1&quot;&gt;Yuta Nabae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayakawa_T/0/1/0/all/0/1&quot;&gt;Teruaki Hayakawa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03365">
<title>Demand response for residential building heating: Effective Monte Carlo Tree Search control based on physics-informed neural networks. (arXiv:2312.03365v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2312.03365</link>
<description rdf:parseType="Literal">&lt;p&gt;Controlling energy consumption in buildings through demand response (DR) has
become increasingly important to reduce global carbon emissions and limit
climate change. In this paper, we specifically focus on controlling the heating
system of a residential building to optimize its energy consumption while
respecting user&apos;s thermal comfort. Recent works in this area have mainly
focused on either model-based control, e.g., model predictive control (MPC), or
model-free reinforcement learning (RL) to implement practical DR algorithms. A
specific RL method that recently has achieved impressive success in domains
such as board games (go, chess) is Monte Carlo Tree Search (MCTS). Yet, for
building control it has remained largely unexplored. Thus, we study MCTS
specifically for building demand response. Its natural structure allows a
flexible optimization that implicitly integrate exogenous constraints (as
opposed, for example, to conventional RL solutions), making MCTS a promising
candidate for DR control problems. We demonstrate how to improve MCTS control
performance by incorporating a Physics-informed Neural Network (PiNN) model for
its underlying thermal state prediction, as opposed to traditional purely
data-driven Black-Box approaches. Our MCTS implementation aligned with a PiNN
model is able to obtain a 3% increment of the obtained reward compared to a
rule-based controller; leading to a 10% cost reduction and 35% reduction on
temperature difference with the desired one when applied to an artificial price
profile. We further implemented a Deep Learning layer into the Monte Carlo Tree
Search technique using a neural network that leads the tree search through more
optimal nodes. We then compared this addition with its Vanilla version, showing
the improvement in computational cost required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pavirani_F/0/1/0/all/0/1&quot;&gt;Fabio Pavirani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gokhale_G/0/1/0/all/0/1&quot;&gt;Gargya Gokhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Claessens_B/0/1/0/all/0/1&quot;&gt;Bert Claessens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Develder_C/0/1/0/all/0/1&quot;&gt;Chris Develder&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03395">
<title>Diffused Task-Agnostic Milestone Planner. (arXiv:2312.03395v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.03395</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing decision-making problems using sequence modeling to predict future
trajectories shows promising results in recent years. In this paper, we take a
step further to leverage the sequence predictive method in wider areas such as
long-term planning, vision-based control, and multi-task decision-making. To
this end, we propose a method to utilize a diffusion-based generative sequence
model to plan a series of milestones in a latent space and to have an agent to
follow the milestones to accomplish a given task. The proposed method can learn
control-relevant, low-dimensional latent representations of milestones, which
makes it possible to efficiently perform long-term planning and vision-based
control. Furthermore, our approach exploits generation flexibility of the
diffusion model, which makes it possible to plan diverse trajectories for
multi-task decision-making. We demonstrate the proposed method across offline
reinforcement learning (RL) benchmarks and an visual manipulation environment.
The results show that our approach outperforms offline RL methods in solving
long-horizon, sparse-reward tasks and multi-task problems, while also achieving
the state-of-the-art performance on the most challenging vision-based
manipulation benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_M/0/1/0/all/0/1&quot;&gt;Mineui Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_M/0/1/0/all/0/1&quot;&gt;Minjae Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Songhwai Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03397">
<title>Generalized Contrastive Divergence: Joint Training of Energy-Based Model and Diffusion Model through Inverse Reinforcement Learning. (arXiv:2312.03397v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03397</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Generalized Contrastive Divergence (GCD), a novel objective
function for training an energy-based model (EBM) and a sampler simultaneously.
GCD generalizes Contrastive Divergence (Hinton, 2002), a celebrated algorithm
for training EBM, by replacing Markov Chain Monte Carlo (MCMC) distribution
with a trainable sampler, such as a diffusion model. In GCD, the joint training
of EBM and a diffusion model is formulated as a minimax problem, which reaches
an equilibrium when both models converge to the data distribution. The minimax
learning with GCD bears interesting equivalence to inverse reinforcement
learning, where the energy corresponds to a negative reward, the diffusion
model is a policy, and the real data is expert demonstrations. We present
preliminary yet promising results showing that joint training is beneficial for
both EBM and a diffusion model. GCD enables EBM training without MCMC while
improving the sample quality of a diffusion model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sangwoong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_D/0/1/0/all/0/1&quot;&gt;Dohyun Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_H/0/1/0/all/0/1&quot;&gt;Himchan Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noh_Y/0/1/0/all/0/1&quot;&gt;Yung-Kyun Noh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_F/0/1/0/all/0/1&quot;&gt;Frank C. Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03413">
<title>Approximating Solutions to the Knapsack Problem using the Lagrangian Dual Framework. (arXiv:2312.03413v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03413</link>
<description rdf:parseType="Literal">&lt;p&gt;The Knapsack Problem is a classic problem in combinatorial optimisation.
Solving these problems may be computationally expensive. Recent years have seen
a growing interest in the use of deep learning methods to approximate the
solutions to such problems. A core problem is how to enforce or encourage
constraint satisfaction in predicted solutions. A promising approach for
predicting solutions to constrained optimisation problems is the Lagrangian
Dual Framework which builds on the method of Lagrangian Relaxation. In this
paper we develop neural network models to approximate Knapsack Problem
solutions using the Lagrangian Dual Framework while improving constraint
satisfaction. We explore the problems of output interpretation and model
selection within this context. Experimental results show strong constraint
satisfaction with a minor reduction of optimality as compared to a baseline
neural network which does not explicitly model the constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keegan_M/0/1/0/all/0/1&quot;&gt;Mitchell Keegan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abolghasemi_M/0/1/0/all/0/1&quot;&gt;Mahdi Abolghasemi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03446">
<title>Visual Hindsight Self-Imitation Learning for Interactive Navigation. (arXiv:2312.03446v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03446</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive visual navigation tasks, which involve following instructions to
reach and interact with specific targets, are challenging not only because
successful experiences are very rare but also because the complex visual inputs
require a substantial number of samples. Previous methods for these tasks often
rely on intricately designed dense rewards or the use of expensive expert data
for imitation learning. To tackle these challenges, we propose a novel
approach, Visual Hindsight Self-Imitation Learning (VHS) for enhancing sample
efficiency through hindsight goal re-labeling and self-imitation. We also
introduce a prototypical goal embedding method derived from experienced goal
observations, that is particularly effective in vision-based and partially
observable environments. This embedding technique allows the agent to visually
reinterpret its unsuccessful attempts, enabling vision-based goal re-labeling
and self-imitation from enhanced successful experiences. Experimental results
show that VHS outperforms existing techniques in interactive visual navigation
tasks, confirming its superior performance and sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kibeom Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_K/0/1/0/all/0/1&quot;&gt;Kisung Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Min Whoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Moonhoen Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minsu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Byoung-Tak Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03447">
<title>Quantum-Inspired Neural Network Model of Optical Illusions. (arXiv:2312.03447v1 [physics.soc-ph])</title>
<link>http://arxiv.org/abs/2312.03447</link>
<description rdf:parseType="Literal">&lt;p&gt;Ambiguous optical illusions have been a paradigmatic object of fascination,
research and inspiration in arts, psychology and video games. However, accurate
computational models of perception of ambiguous figures have been elusive. In
this paper, we design and train a deep neural network model to simulate the
human&apos;s perception of the Necker cube, an ambiguous drawing with several
alternating possible interpretations. Defining the weights of the neural
network connection using a quantum generator of truly random numbers, in
agreement with the emerging concepts of quantum artificial intelligence and
quantum cognition we reveal that the actual perceptual state of the Necker cube
is a qubit-like superposition of the two fundamental perceptual states
predicted by classical theories. Our results will find applications in video
games and virtual reality systems employed for training of astronauts and
operators of unmanned aerial vehicles. They will also be useful for researchers
working in the fields of machine learning and vision, psychology of perception
and quantum-mechanical models of human mind and decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Maksymov_I/0/1/0/all/0/1&quot;&gt;Ivan S. Maksymov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03455">
<title>Data is Overrated: Perceptual Metrics Can Lead Learning in the Absence of Training Data. (arXiv:2312.03455v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.03455</link>
<description rdf:parseType="Literal">&lt;p&gt;Perceptual metrics are traditionally used to evaluate the quality of natural
signals, such as images and audio. They are designed to mimic the perceptual
behaviour of human observers and usually reflect structures found in natural
signals. This motivates their use as loss functions for training generative
models such that models will learn to capture the structure held in the metric.
We take this idea to the extreme in the audio domain by training a compressive
autoencoder to reconstruct uniform noise, in lieu of natural data. We show that
training with perceptual losses improves the reconstruction of spectrograms and
re-synthesized audio at test time over models trained with a standard Euclidean
loss. This demonstrates better generalisation to unseen natural signals when
using perceptual metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namgyal_T/0/1/0/all/0/1&quot;&gt;Tashi Namgyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hepburn_A/0/1/0/all/0/1&quot;&gt;Alexander Hepburn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Santos_Rodriguez_R/0/1/0/all/0/1&quot;&gt;Raul Santos-Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laparra_V/0/1/0/all/0/1&quot;&gt;Valero Laparra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malo_J/0/1/0/all/0/1&quot;&gt;Jesus Malo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03475">
<title>Molecule Joint Auto-Encoding: Trajectory Pretraining with 2D and 3D Diffusion. (arXiv:2312.03475v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03475</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, artificial intelligence for drug discovery has raised increasing
interest in both machine learning and chemistry domains. The fundamental
building block for drug discovery is molecule geometry and thus, the molecule&apos;s
geometrical representation is the main bottleneck to better utilize machine
learning techniques for drug discovery. In this work, we propose a pretraining
method for molecule joint auto-encoding (MoleculeJAE). MoleculeJAE can learn
both the 2D bond (topology) and 3D conformation (geometry) information, and a
diffusion process model is applied to mimic the augmented trajectories of such
two modalities, based on which, MoleculeJAE will learn the inherent chemical
structure in a self-supervised manner. Thus, the pretrained geometrical
representation in MoleculeJAE is expected to benefit downstream
geometry-related tasks. Empirically, MoleculeJAE proves its effectiveness by
reaching state-of-the-art performance on 15 out of 20 tasks by comparing it
with 12 competitive baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_W/0/1/0/all/0/1&quot;&gt;Weitao Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiujiu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuecang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhiming Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shengchao Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03479">
<title>JAMMIN-GPT: Text-based Improvisation using LLMs in Ableton Live. (arXiv:2312.03479v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2312.03479</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a system that allows users of Ableton Live to create MIDI-clips
by naming them with musical descriptions. Users can compose by typing the
desired musical content directly in Ableton&apos;s clip view, which is then inserted
by our integrated system. This allows users to stay in the flow of their
creative process while quickly generating musical ideas. The system works by
prompting ChatGPT to reply using one of several text-based musical formats,
such as ABC notation, chord symbols, or drum tablature. This is an important
step in integrating generative AI tools into pre-existing musical workflows,
and could be valuable for content makers who prefer to express their creative
vision through descriptive language. Code is available at
https://github.com/supersational/JAMMIN-GPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hollowell_S/0/1/0/all/0/1&quot;&gt;Sven Hollowell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Namgyal_T/0/1/0/all/0/1&quot;&gt;Tashi Namgyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marshall_P/0/1/0/all/0/1&quot;&gt;Paul Marshall&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03492">
<title>Learning From Scenarios for Stochastic Repairable Scheduling. (arXiv:2312.03492v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03492</link>
<description rdf:parseType="Literal">&lt;p&gt;When optimizing problems with uncertain parameter values in a linear
objective, decision-focused learning enables end-to-end learning of these
values. We are interested in a stochastic scheduling problem, in which
processing times are uncertain, which brings uncertain values in the
constraints, and thus repair of an initial schedule may be needed. Historical
realizations of the stochastic processing times are available. We show how
existing decision-focused learning techniques based on stochastic smoothing can
be adapted to this scheduling problem. We include an extensive experimental
evaluation to investigate in which situations decision-focused learning
outperforms the state of the art for such situations: scenario-based stochastic
optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houten_K/0/1/0/all/0/1&quot;&gt;Kim van den Houten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tax_D/0/1/0/all/0/1&quot;&gt;David M.J. Tax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Freydell_E/0/1/0/all/0/1&quot;&gt;Esteban Freydell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weerdt_M/0/1/0/all/0/1&quot;&gt;Mathijs de Weerdt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03497">
<title>Speculative Exploration on the Concept of Artificial Agents Conducting Autonomous Research. (arXiv:2312.03497v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03497</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper engages in a speculative exploration of the concept of an
artificial agent capable of conducting research. Initially, it examines how the
act of research can be conceptually characterized, aiming to provide a starting
point for discussions about what it means to create such agents. The focus then
shifts to the core components of research: question formulation, hypothesis
generation, and hypothesis verification. This discussion includes a
consideration of the potential and challenges associated with enabling machines
to autonomously perform these tasks. Subsequently, this paper briefly considers
the overlapping themes and interconnections that underlie them. Finally, the
paper presents preliminary thoughts on prototyping as an initial step towards
uncovering the challenges involved in developing these research-capable agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takagi_S/0/1/0/all/0/1&quot;&gt;Shiro Takagi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03517">
<title>FRDiff: Feature Reuse for Exquisite Zero-shot Acceleration of Diffusion Models. (arXiv:2312.03517v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03517</link>
<description rdf:parseType="Literal">&lt;p&gt;The substantial computational costs of diffusion models, particularly due to
the repeated denoising steps crucial for high-quality image generation, present
a major obstacle to their widespread adoption. While several studies have
attempted to address this issue by reducing the number of score function
evaluations using advanced ODE solvers without fine-tuning, the decreased
number of denoising iterations misses the opportunity to update fine details,
resulting in noticeable quality degradation. In our work, we introduce an
advanced acceleration technique that leverages the temporal redundancy inherent
in diffusion models. Reusing feature maps with high temporal similarity opens
up a new opportunity to save computation without sacrificing output quality. To
realize the practical benefits of this intuition, we conduct an extensive
analysis and propose a novel method, FRDiff. FRDiff is designed to harness the
advantages of both reduced NFE and feature reuse, achieving a Pareto frontier
that balances fidelity and latency trade-offs in various generative tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+So_J/0/1/0/all/0/1&quot;&gt;Junhyuk So&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungwon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunhyeok Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03519">
<title>Active Wildfires Detection and Dynamic Escape Routes Planning for Humans through Information Fusion between Drones and Satellites. (arXiv:2312.03519v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03519</link>
<description rdf:parseType="Literal">&lt;p&gt;UAVs are playing an increasingly important role in the field of wilderness
rescue by virtue of their flexibility. This paper proposes a fusion of UAV
vision technology and satellite image analysis technology for active wildfires
detection and road networks extraction of wildfire areas and real-time dynamic
escape route planning for people in distress. Firstly, the fire source location
and the segmentation of smoke and flames are targeted based on Sentinel 2
satellite imagery. Secondly, the road segmentation and the road condition
assessment are performed by D-linkNet and NDVI values in the central area of
the fire source by UAV. Finally, the dynamic optimal route planning for humans
in real time is performed by the weighted A* algorithm in the road network with
the dynamic fire spread model. Taking the Chongqing wildfire on August 24,
2022, as a case study, the results demonstrate that the dynamic escape route
planning algorithm can provide an optimal real-time navigation path for humans
in the presence of fire through the information fusion of UAVs and satellites.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sziranyi_T/0/1/0/all/0/1&quot;&gt;Tamas Sziranyi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03520">
<title>Defense Against Adversarial Attacks using Convolutional Auto-Encoders. (arXiv:2312.03520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03520</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models, while achieving state-of-the-art performance on many
tasks, are susceptible to adversarial attacks that exploit inherent
vulnerabilities in their architectures. Adversarial attacks manipulate the
input data with imperceptible perturbations, causing the model to misclassify
the data or produce erroneous outputs. This work is based on enhancing the
robustness of targeted classifier models against adversarial attacks. To
achieve this, an convolutional autoencoder-based approach is employed that
effectively counters adversarial perturbations introduced to the input images.
By generating images closely resembling the input images, the proposed
methodology aims to restore the model&apos;s accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mandal_S/0/1/0/all/0/1&quot;&gt;Shreyasi Mandal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03521">
<title>Optimal Wildfire Escape Route Planning for Drones under Dynamic Fire and Smoke. (arXiv:2312.03521v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.03521</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the increasing prevalence and intensity of wildfires have
posed significant challenges to emergency response teams. The utilization of
unmanned aerial vehicles (UAVs), commonly known as drones, has shown promise in
aiding wildfire management efforts. This work focuses on the development of an
optimal wildfire escape route planning system specifically designed for drones,
considering dynamic fire and smoke models. First, the location of the source of
the wildfire can be well located by information fusion between UAV and
satellite, and the road conditions in the vicinity of the fire can be assessed
and analyzed using multi-channel remote sensing data. Second, the road network
can be extracted and segmented in real time using UAV vision technology, and
each road in the road network map can be given priority based on the results of
road condition classification. Third, the spread model of dynamic fires
calculates the new location of the fire source based on the fire intensity,
wind speed and direction, and the radius increases as the wildfire spreads.
Smoke is generated around the fire source to create a visual representation of
a burning fire. Finally, based on the improved A* algorithm, which considers
all the above factors, the UAV can quickly plan an escape route based on the
starting and destination locations that avoid the location of the fire source
and the area where it is spreading. By considering dynamic fire and smoke
models, the proposed system enhances the safety and efficiency of drone
operations in wildfire environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sziranyi_T/0/1/0/all/0/1&quot;&gt;Tamas Sziranyi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03526">
<title>On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm. (arXiv:2312.03526v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03526</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary machine learning requires training large neural networks on
massive datasets and thus faces the challenges of high computational demands.
Dataset distillation, as a recent emerging strategy, aims to compress
real-world datasets for efficient training. However, this line of research
currently struggle with large-scale and high-resolution datasets, hindering its
practicality and feasibility. To this end, we re-examine the existing dataset
distillation methods and identify three properties required for large-scale
real-world applications, namely, realism, diversity, and efficiency. As a
remedy, we propose RDED, a novel computationally-efficient yet effective data
distillation paradigm, to enable both diversity and realism of the distilled
data. Extensive empirical results over various neural architectures and
datasets demonstrate the advancement of RDED: we can distill the full
ImageNet-1K to a small dataset comprising 10 images per class within 7 minutes,
achieving a notable 42% top-1 accuracy with ResNet-18 on a single RTX-4090 GPU
(while the SOTA only achieves 21% but requires 6 hours).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_D/0/1/0/all/0/1&quot;&gt;Daiwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03543">
<title>GPT-4 Enhanced Multimodal Grounding for Autonomous Driving: Leveraging Cross-Modal Attention with Large Language Models. (arXiv:2312.03543v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03543</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of autonomous vehicles (AVs), accurately discerning commander
intent and executing linguistic commands within a visual context presents a
significant challenge. This paper introduces a sophisticated encoder-decoder
framework, developed to address visual grounding in AVs.Our Context-Aware
Visual Grounding (CAVG) model is an advanced system that integrates five core
encoders-Text, Image, Context, and Cross-Modal-with a Multimodal decoder. This
integration enables the CAVG model to adeptly capture contextual semantics and
to learn human emotional features, augmented by state-of-the-art Large Language
Models (LLMs) including GPT-4. The architecture of CAVG is reinforced by the
implementation of multi-head cross-modal attention mechanisms and a
Region-Specific Dynamic (RSD) layer for attention modulation. This
architectural design enables the model to efficiently process and interpret a
range of cross-modal inputs, yielding a comprehensive understanding of the
correlation between verbal commands and corresponding visual scenes. Empirical
evaluations on the Talk2Car dataset, a real-world benchmark, demonstrate that
CAVG establishes new standards in prediction accuracy and operational
efficiency. Notably, the model exhibits exceptional performance even with
limited training data, ranging from 50% to 75% of the full dataset. This
feature highlights its effectiveness and potential for deployment in practical
AV applications. Moreover, CAVG has shown remarkable robustness and
adaptability in challenging scenarios, including long-text command
interpretation, low-light conditions, ambiguous command contexts, inclement
weather conditions, and densely populated urban environments. The code for the
proposed model is available at our Github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_H/0/1/0/all/0/1&quot;&gt;Haicheng Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Huanming Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenning Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guofa Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bie_Y/0/1/0/all/0/1&quot;&gt;Yiming Bie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chengzhong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03580">
<title>Invariance &amp; Causal Representation Learning: Prospects and Limitations. (arXiv:2312.03580v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.03580</link>
<description rdf:parseType="Literal">&lt;p&gt;In causal models, a given mechanism is assumed to be invariant to changes of
other mechanisms. While this principle has been utilized for inference in
settings where the causal variables are observed, theoretical insights when the
variables of interest are latent are largely missing. We assay the connection
between invariance and causal representation learning by establishing
impossibility results which show that invariance alone is insufficient to
identify latent causal variables. Together with practical considerations, we
use these theoretical findings to highlight the need for additional constraints
in order to identify representations by exploiting invariance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bing_S/0/1/0/all/0/1&quot;&gt;Simon Bing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wahl_J/0/1/0/all/0/1&quot;&gt;Jonas Wahl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ninad_U/0/1/0/all/0/1&quot;&gt;Urmi Ninad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Runge_J/0/1/0/all/0/1&quot;&gt;Jakob Runge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03585">
<title>Foundation Model Assisted Weakly Supervised Semantic Segmentation. (arXiv:2312.03585v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03585</link>
<description rdf:parseType="Literal">&lt;p&gt;This work aims to leverage pre-trained foundation models, such as contrastive
language-image pre-training (CLIP) and segment anything model (SAM), to address
weakly supervised semantic segmentation (WSSS) using image-level labels. To
this end, we propose a coarse-to-fine framework based on CLIP and SAM for
generating high-quality segmentation seeds. Specifically, we construct an image
classification task and a seed segmentation task, which are jointly performed
by CLIP with frozen weights and two sets of learnable task-specific prompts. A
SAM-based seeding (SAMS) module is designed and applied to each task to produce
either coarse or fine seed maps. Moreover, we design a multi-label contrastive
loss supervised by image-level labels and a CAM activation loss supervised by
the generated coarse seed map. These losses are used to learn the prompts,
which are the only parts need to be learned in our framework. Once the prompts
are learned, we input each image along with the learned segmentation-specific
prompts into CLIP and the SAMS module to produce high-quality segmentation
seeds. These seeds serve as pseudo labels to train an off-the-shelf
segmentation network like other two-stage WSSS methods. Experiments show that
our method achieves the state-of-the-art performance on PASCAL VOC 2012 and
competitive results on MS COCO 2014.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaobo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_X/0/1/0/all/0/1&quot;&gt;Xiaojin Gong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03596">
<title>MMM: Generative Masked Motion Model. (arXiv:2312.03596v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03596</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in text-to-motion generation using diffusion and
autoregressive models have shown promising results. However, these models often
suffer from a trade-off between real-time performance, high fidelity, and
motion editability. To address this gap, we introduce MMM, a novel yet simple
motion generation paradigm based on Masked Motion Model. MMM consists of two
key components: (1) a motion tokenizer that transforms 3D human motion into a
sequence of discrete tokens in latent space, and (2) a conditional masked
motion transformer that learns to predict randomly masked motion tokens,
conditioned on the pre-computed text tokens. By attending to motion and text
tokens in all directions, MMM explicitly captures inherent dependency among
motion tokens and semantic mapping between motion and text tokens. During
inference, this allows parallel and iterative decoding of multiple motion
tokens that are highly consistent with fine-grained text descriptions,
therefore simultaneously achieving high-fidelity and high-speed motion
generation. In addition, MMM has innate motion editability. By simply placing
mask tokens in the place that needs editing, MMM automatically fills the gaps
while guaranteeing smooth transitions between editing and non-editing parts.
Extensive experiments on the HumanML3D and KIT-ML datasets demonstrate that MMM
surpasses current leading methods in generating high-quality motion (evidenced
by superior FID scores of 0.08 and 0.429), while offering advanced editing
features such as body-part modification, motion in-betweening, and the
synthesis of long motion sequences. In addition, MMM is two orders of magnitude
faster on a single mid-range GPU than editable motion diffusion models. Our
project page is available at \url{https://exitudio.github.io/MMM-page}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinyoanuntapong_E/0/1/0/all/0/1&quot;&gt;Ekkasit Pinyoanuntapong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minwoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03606">
<title>DiffusionSat: A Generative Foundation Model for Satellite Imagery. (arXiv:2312.03606v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03606</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved state-of-the-art results on many modalities
including images, speech, and video. However, existing models are not tailored
to support remote sensing data, which is widely used in important applications
including environmental monitoring and crop-yield prediction. Satellite images
are significantly different from natural images -- they can be multi-spectral,
irregularly sampled across time -- and existing diffusion models trained on
images from the Web do not support them. Furthermore, remote sensing data is
inherently spatio-temporal, requiring conditional generation tasks not
supported by traditional methods based on captions or images. In this paper, we
present DiffusionSat, to date the largest generative foundation model trained
on a collection of publicly available large, high-resolution remote sensing
datasets. As text-based captions are sparsely available for satellite images,
we incorporate the associated metadata such as geolocation as conditioning
information. Our method produces realistic samples and can be used to solve
multiple generative tasks including temporal generation, superresolution given
multi-spectral inputs and in-painting. Our method outperforms previous
state-of-the-art methods for satellite image generation and is the first
large-scale $\textit{generative}$ foundation model for satellite imagery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_S/0/1/0/all/0/1&quot;&gt;Samar Khanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_P/0/1/0/all/0/1&quot;&gt;Patrick Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Linqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_C/0/1/0/all/0/1&quot;&gt;Chenlin Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rombach_R/0/1/0/all/0/1&quot;&gt;Robin Rombach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burke_M/0/1/0/all/0/1&quot;&gt;Marshall Burke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lobell_D/0/1/0/all/0/1&quot;&gt;David Lobell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03611">
<title>DreamComposer: Controllable 3D Object Generation via Multi-View Conditions. (arXiv:2312.03611v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03611</link>
<description rdf:parseType="Literal">&lt;p&gt;Utilizing pre-trained 2D large-scale generative models, recent works are
capable of generating high-quality novel views from a single in-the-wild image.
However, due to the lack of information from multiple views, these works
encounter difficulties in generating controllable novel views. In this paper,
we present DreamComposer, a flexible and scalable framework that can enhance
existing view-aware diffusion models by injecting multi-view conditions.
Specifically, DreamComposer first uses a view-aware 3D lifting module to obtain
3D representations of an object from multiple views. Then, it renders the
latent features of the target view from 3D representations with the multi-view
feature fusion module. Finally the target view features extracted from
multi-view inputs are injected into a pre-trained diffusion model. Experiments
show that DreamComposer is compatible with state-of-the-art diffusion models
for zero-shot novel view synthesis, further enhancing them to generate
high-fidelity novel view images with multi-view conditions, ready for
controllable 3D object reconstruction and various other applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yunhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yukun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaoyang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuan-Chen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Song-Hai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xihui Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03631">
<title>MOCHa: Multi-Objective Reinforcement Mitigating Caption Hallucinations. (arXiv:2312.03631v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03631</link>
<description rdf:parseType="Literal">&lt;p&gt;While recent years have seen rapid progress in image-conditioned text
generation, image captioning still suffers from the fundamental issue of
hallucinations, the generation of spurious details that cannot be inferred from
the given image. Dedicated methods for reducing hallucinations in image
captioning largely focus on closed-vocabulary object tokens, ignoring most
types of hallucinations that occur in practice. In this work, we propose MOCHa,
an approach that harnesses advancements in reinforcement learning (RL) to
address the sequence-level nature of hallucinations in an open-world setup. To
optimize for caption fidelity to the input image, we leverage ground-truth
reference captions as proxies to measure the logical consistency of generated
captions. However, optimizing for caption fidelity alone fails to preserve the
semantic adequacy of generations; therefore, we propose a multi-objective
reward function that jointly targets these qualities, without requiring any
strong supervision. We demonstrate that these goals can be simultaneously
optimized with our framework, enhancing performance for various captioning
models of different scales. Our qualitative and quantitative results
demonstrate MOCHa&apos;s superior performance across various established metrics. We
also demonstrate the benefit of our method in the open-vocabulary setting. To
this end, we contribute OpenCHAIR, a new benchmark for quantifying
open-vocabulary hallucinations in image captioning models, constructed using
generative foundation models. We will release our code, benchmark, and trained
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Kish_A/0/1/0/all/0/1&quot;&gt;Assaf Ben-Kish&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yanuka_M/0/1/0/all/0/1&quot;&gt;Moran Yanuka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alper_M/0/1/0/all/0/1&quot;&gt;Morris Alper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giryes_R/0/1/0/all/0/1&quot;&gt;Raja Giryes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Averbuch_Elor_H/0/1/0/all/0/1&quot;&gt;Hadar Averbuch-Elor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03633">
<title>Not All Large Language Models (LLMs) Succumb to the &quot;Reversal Curse&quot;: A Comparative Study of Deductive Logical Reasoning in BERT and GPT Models. (arXiv:2312.03633v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.03633</link>
<description rdf:parseType="Literal">&lt;p&gt;The &quot;Reversal Curse&quot; refers to the scenario where auto-regressive decoder
large language models (LLMs), such as ChatGPT, trained on &quot;A is B&quot; fail to
learn &quot;B is A&quot;, demonstrating a basic failure of logical deduction. This raises
a red flag in the use of GPT models for certain general tasks such as
constructing knowledge graphs, considering their adherence to this symmetric
principle. In our study, we examined a bidirectional LLM, BERT, and found that
it is immune to the reversal curse. Driven by ongoing efforts to construct
biomedical knowledge graphs with LLMs, we also embarked on evaluating more
complex but essential deductive reasoning capabilities. This process included
first training encoder and decoder language models to master the intersection
($\cap$) and union ($\cup$) operations on two sets and then moving on to assess
their capability to infer different combinations of union ($\cup$) and
intersection ($\cap$) operations on three newly created sets. The findings
showed that while both encoder and decoder language models, trained for tasks
involving two sets (union/intersection), were proficient in such scenarios,
they encountered difficulties when dealing with operations that included three
sets (various combinations of union and intersection). Our research highlights
the distinct characteristics of encoder and decoder models in simple and
complex logical reasoning. In practice, the choice between BERT and GPT should
be guided by the specific requirements and nature of the task at hand,
leveraging their respective strengths in bidirectional context comprehension
and sequence prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingye Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Da Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03641">
<title>MotionCtrl: A Unified and Flexible Motion Controller for Video Generation. (arXiv:2312.03641v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03641</link>
<description rdf:parseType="Literal">&lt;p&gt;Motions in a video primarily consist of camera motion, induced by camera
movement, and object motion, resulting from object movement. Accurate control
of both camera and object motion is essential for video generation. However,
existing works either mainly focus on one type of motion or do not clearly
distinguish between the two, limiting their control capabilities and diversity.
Therefore, this paper presents MotionCtrl, a unified and flexible motion
controller for video generation designed to effectively and independently
control camera and object motion. The architecture and training strategy of
MotionCtrl are carefully devised, taking into account the inherent properties
of camera motion, object motion, and imperfect training data. Compared to
previous methods, MotionCtrl offers three main advantages: 1) It effectively
and independently controls camera motion and object motion, enabling more
fine-grained motion control and facilitating flexible and diverse combinations
of both types of motion. 2) Its motion conditions are determined by camera
poses and trajectories, which are appearance-free and minimally impact the
appearance or shape of objects in generated videos. 3) It is a relatively
generalizable model that can adapt to a wide array of camera poses and
trajectories once trained. Extensive qualitative and quantitative experiments
have been conducted to demonstrate the superiority of MotionCtrl over existing
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhouxia Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Ziyang Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xintao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianshui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Menghan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Ping Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_Y/0/1/0/all/0/1&quot;&gt;Ying Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03654">
<title>Efficient Inverse Design Optimization through Multi-fidelity Simulations, Machine Learning, and Search Space Reduction Strategies. (arXiv:2312.03654v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2312.03654</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a methodology designed to augment the inverse design
optimization process in scenarios constrained by limited compute, through the
strategic synergy of multi-fidelity evaluations, machine learning models, and
optimization algorithms. The proposed methodology is analyzed on two distinct
engineering inverse design problems: airfoil inverse design and the scalar
field reconstruction problem. It leverages a machine learning model trained
with low-fidelity simulation data, in each optimization cycle, thereby
proficiently predicting a target variable and discerning whether a
high-fidelity simulation is necessitated, which notably conserves computational
resources. Additionally, the machine learning model is strategically deployed
prior to optimization to reduce the search space, thereby further accelerating
convergence toward the optimal solution. The methodology has been employed to
enhance two optimization algorithms, namely Differential Evolution and Particle
Swarm Optimization. Comparative analyses illustrate performance improvements
across both algorithms. Notably, this method is adeptly adaptable across any
inverse design application, facilitating a harmonious synergy between a
representative low-fidelity machine learning model, and high-fidelity
simulation, and can be seamlessly applied across any variety of
population-based optimization algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grbcic_L/0/1/0/all/0/1&quot;&gt;Luka Grbcic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muller_J/0/1/0/all/0/1&quot;&gt;Juliane M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jong_W/0/1/0/all/0/1&quot;&gt;Wibe Albert de Jong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03664">
<title>Generative agent-based modeling with actions grounded in physical, social, or digital space using Concordia. (arXiv:2312.03664v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.03664</link>
<description rdf:parseType="Literal">&lt;p&gt;Agent-based modeling has been around for decades, and applied widely across
the social and natural sciences. The scope of this research method is now
poised to grow dramatically as it absorbs the new affordances provided by Large
Language Models (LLM)s. Generative Agent-Based Models (GABM) are not just
classic Agent-Based Models (ABM)s where the agents talk to one another. Rather,
GABMs are constructed using an LLM to apply common sense to situations, act
&quot;reasonably&quot;, recall common semantic knowledge, produce API calls to control
digital technologies like apps, and communicate both within the simulation and
to researchers viewing it from the outside. Here we present Concordia, a
library to facilitate constructing and working with GABMs. Concordia makes it
easy to construct language-mediated simulations of physically- or
digitally-grounded environments. Concordia agents produce their behavior using
a flexible component system which mediates between two fundamental operations:
LLM calls and associative memory retrieval. A special agent called the Game
Master (GM), which was inspired by tabletop role-playing games, is responsible
for simulating the environment where the agents interact. Agents take actions
by describing what they want to do in natural language. The GM then translates
their actions into appropriate implementations. In a simulated physical world,
the GM checks the physical plausibility of agent actions and describes their
effects. In digital environments simulating technologies such as apps and
services, the GM may handle API calls to integrate with external tools such as
general AI assistants (e.g., Bard, ChatGPT), and digital apps (e.g., Calendar,
Email, Search, etc.). Concordia was designed to support a wide array of
applications both in scientific research and for evaluating performance of real
digital services by simulating users and/or generating synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vezhnevets_A/0/1/0/all/0/1&quot;&gt;Alexander Sasha Vezhnevets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agapiou_J/0/1/0/all/0/1&quot;&gt;John P. Agapiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aharon_A/0/1/0/all/0/1&quot;&gt;Avia Aharon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziv_R/0/1/0/all/0/1&quot;&gt;Ron Ziv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matyas_J/0/1/0/all/0/1&quot;&gt;Jayd Matyas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duenez_Guzman_E/0/1/0/all/0/1&quot;&gt;Edgar A. Du&amp;#xe9;&amp;#xf1;ez-Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cunningham_W/0/1/0/all/0/1&quot;&gt;William A. Cunningham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Osindero_S/0/1/0/all/0/1&quot;&gt;Simon Osindero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karmon_D/0/1/0/all/0/1&quot;&gt;Danny Karmon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1&quot;&gt;Joel Z. Leibo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03668">
<title>An Integration of Pre-Trained Speech and Language Models for End-to-End Speech Recognition. (arXiv:2312.03668v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2312.03668</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in machine learning have made it possible to perform various text
and speech processing tasks, including automatic speech recognition (ASR), in
an end-to-end (E2E) manner. Since typical E2E approaches require large amounts
of training data and resources, leveraging pre-trained foundation models
instead of training from scratch is gaining attention. Although there have been
attempts to use pre-trained speech and language models in ASR, most of them are
limited to using either. This paper explores the potential of integrating a
pre-trained speech representation model with a large language model (LLM) for
E2E ASR. The proposed model enables E2E ASR by generating text tokens in an
autoregressive manner via speech representations as speech prompts, taking
advantage of the vast knowledge provided by the LLM. Furthermore, the proposed
model can incorporate remarkable developments for LLM utilization, such as
inference optimization and parameter-efficient domain adaptation. Experimental
results show that the proposed model achieves performance comparable to modern
E2E ASR models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hono_Y/0/1/0/all/0/1&quot;&gt;Yukiya Hono&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitsuda_K/0/1/0/all/0/1&quot;&gt;Koh Mitsuda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mitsui_K/0/1/0/all/0/1&quot;&gt;Kentaro Mitsui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wakatsuki_T/0/1/0/all/0/1&quot;&gt;Toshiaki Wakatsuki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sawada_K/0/1/0/all/0/1&quot;&gt;Kei Sawada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03682">
<title>What Planning Problems Can A Relational Neural Network Solve?. (arXiv:2312.03682v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.03682</link>
<description rdf:parseType="Literal">&lt;p&gt;Goal-conditioned policies are generally understood to be &quot;feed-forward&quot;
circuits, in the form of neural networks that map from the current state and
the goal specification to the next action to take. However, under what
circumstances such a policy can be learned and how efficient the policy will be
are not well understood. In this paper, we present a circuit complexity
analysis for relational neural networks (such as graph neural networks and
transformers) representing policies for planning problems, by drawing
connections with serialized goal regression search (S-GRS). We show that there
are three general classes of planning problems, in terms of the growth of
circuit width and depth as a function of the number of objects and planning
horizon, providing constructive proofs. We also illustrate the utility of this
analysis for designing neural networks for policy learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_J/0/1/0/all/0/1&quot;&gt;Jiayuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_Perez_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s Lozano-P&amp;#xe9;rez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tenenbaum_J/0/1/0/all/0/1&quot;&gt;Joshua B. Tenenbaum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaelbling_L/0/1/0/all/0/1&quot;&gt;Leslie Pack Kaelbling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03687">
<title>MatterGen: a generative model for inorganic materials design. (arXiv:2312.03687v1 [cond-mat.mtrl-sci])</title>
<link>http://arxiv.org/abs/2312.03687</link>
<description rdf:parseType="Literal">&lt;p&gt;The design of functional materials with desired properties is essential in
driving technological advances in areas like energy storage, catalysis, and
carbon capture. Generative models provide a new paradigm for materials design
by directly generating entirely novel materials given desired property
constraints. Despite recent progress, current generative models have low
success rate in proposing stable crystals, or can only satisfy a very limited
set of property constraints. Here, we present MatterGen, a model that generates
stable, diverse inorganic materials across the periodic table and can further
be fine-tuned to steer the generation towards a broad range of property
constraints. To enable this, we introduce a new diffusion-based generative
process that produces crystalline structures by gradually refining atom types,
coordinates, and the periodic lattice. We further introduce adapter modules to
enable fine-tuning towards any given property constraints with a labeled
dataset. Compared to prior generative models, structures produced by MatterGen
are more than twice as likely to be novel and stable, and more than 15 times
closer to the local energy minimum. After fine-tuning, MatterGen successfully
generates stable, novel materials with desired chemistry, symmetry, as well as
mechanical, electronic and magnetic properties. Finally, we demonstrate
multi-property materials design capabilities by proposing structures that have
both high magnetic density and a chemical composition with low supply-chain
risk. We believe that the quality of generated materials and the breadth of
MatterGen&apos;s capabilities represent a major advancement towards creating a
universal generative model for materials design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zeni_C/0/1/0/all/0/1&quot;&gt;Claudio Zeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Pinsler_R/0/1/0/all/0/1&quot;&gt;Robert Pinsler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Zugner_D/0/1/0/all/0/1&quot;&gt;Daniel Z&amp;#xfc;gner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Fowler_A/0/1/0/all/0/1&quot;&gt;Andrew Fowler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Horton_M/0/1/0/all/0/1&quot;&gt;Matthew Horton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xiang Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Shysheya_S/0/1/0/all/0/1&quot;&gt;Sasha Shysheya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Crabbe_J/0/1/0/all/0/1&quot;&gt;Jonathan Crabb&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lixin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Smith_J/0/1/0/all/0/1&quot;&gt;Jake Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Tomioka_R/0/1/0/all/0/1&quot;&gt;Ryota Tomioka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cond-mat/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tian Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03698">
<title>Intrinsic Harmonization for Illumination-Aware Compositing. (arXiv:2312.03698v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03698</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite significant advancements in network-based image harmonization
techniques, there still exists a domain disparity between typical training
pairs and real-world composites encountered during inference. Most existing
methods are trained to reverse global edits made on segmented image regions,
which fail to accurately capture the lighting inconsistencies between the
foreground and background found in composited images. In this work, we
introduce a self-supervised illumination harmonization approach formulated in
the intrinsic image domain. First, we estimate a simple global lighting model
from mid-level vision representations to generate a rough shading for the
foreground region. A network then refines this inferred shading to generate a
harmonious re-shading that aligns with the background scene. In order to match
the color appearance of the foreground and background, we utilize ideas from
prior harmonization approaches to perform parameterized image edits in the
albedo domain. To validate the effectiveness of our approach, we present
results from challenging real-world composites and conduct a user study to
objectively measure the enhanced realism achieved compared to state-of-the-art
harmonization methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Careaga_C/0/1/0/all/0/1&quot;&gt;Chris Careaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aksoy_Y/0/1/0/all/0/1&quot;&gt;Ya&amp;#x11f;&amp;#x131;z Aksoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miangoleh_S/0/1/0/all/0/1&quot;&gt;S. Mahdi H. Miangoleh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03700">
<title>OneLLM: One Framework to Align All Modalities with Language. (arXiv:2312.03700v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.03700</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal large language models (MLLMs) have gained significant attention
due to their strong multimodal understanding capability. However, existing
works rely heavily on modality-specific encoders, which usually differ in
architecture and are limited to common modalities. In this paper, we present
OneLLM, an MLLM that aligns eight modalities to language using a unified
framework. We achieve this through a unified multimodal encoder and a
progressive multimodal alignment pipeline. In detail, we first train an image
projection module to connect a vision encoder with LLM. Then, we build a
universal projection module (UPM) by mixing multiple image projection modules
and dynamic routing. Finally, we progressively align more modalities to LLM
with the UPM. To fully leverage the potential of OneLLM in following
instructions, we also curated a comprehensive multimodal instruction dataset,
including 2M items from image, audio, video, point cloud, depth/normal map, IMU
and fMRI brain activity. OneLLM is evaluated on 25 diverse benchmarks,
encompassing tasks such as multimodal captioning, question answering and
reasoning, where it delivers excellent performance. Code, data, model and
online demo are available at https://github.com/csuhan/OneLLM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_J/0/1/0/all/0/1&quot;&gt;Jiaming Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kaipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_Y/0/1/0/all/0/1&quot;&gt;Yu Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_P/0/1/0/all/0/1&quot;&gt;Peng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.10890">
<title>Revisiting Game Representations: The Hidden Costs of Efficiency in Sequential Decision-making Algorithms. (arXiv:2112.10890v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2112.10890</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in algorithms for sequential decision-making under
imperfect information have shown remarkable success in large games such as
limit- and no-limit poker. These algorithms traditionally formalize the games
using the extensive-form game formalism, which, as we show, while theoretically
sound, is memory-inefficient and computationally intensive in practice. To
mitigate these challenges, a popular workaround involves using a specialized
representation based on player specific information-state trees. However, as we
show, this alternative significantly narrows the set of games that can be
represented efficiently.
&lt;/p&gt;
&lt;p&gt;In this study, we identify the set of large games on which modern algorithms
have been benchmarked as being naturally represented by Sequential Bayesian
Games. We elucidate the critical differences between extensive-form game and
sequential Bayesian game representations, both theoretically and empirically.
We further argue that the impressive experimental results often cited in the
literature may be skewed, as they frequently stem from testing these algorithms
only on this restricted class of games. By understanding these nuances, we aim
to guide future research in developing more universally applicable and
efficient algorithms for sequential decision-making under imperfect
information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kovarik_V/0/1/0/all/0/1&quot;&gt;Vojt&amp;#x11b;ch Kova&amp;#x159;&amp;#xed;k&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milec_D/0/1/0/all/0/1&quot;&gt;David Milec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sustr_M/0/1/0/all/0/1&quot;&gt;Michal &amp;#x160;ustr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seitz_D/0/1/0/all/0/1&quot;&gt;Dominik Seitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lisy_V/0/1/0/all/0/1&quot;&gt;Viliam Lis&amp;#xfd;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.10629">
<title>Model Reprogramming: Resource-Efficient Cross-Domain Machine Learning. (arXiv:2202.10629v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.10629</link>
<description rdf:parseType="Literal">&lt;p&gt;In data-rich domains such as vision, language, and speech, deep learning
prevails to deliver high-performance task-specific models and can even learn
general task-agnostic representations for efficient finetuning to downstream
tasks. However, deep learning in resource-limited domains still faces multiple
challenges including (i) limited data, (ii) constrained model development cost,
and (iii) lack of adequate pre-trained models for effective finetuning. This
paper provides an overview of model reprogramming to bridge this gap. Model
reprogramming enables resource-efficient cross-domain machine learning by
repurposing and reusing a well-developed pre-trained model from a source domain
to solve tasks in a target domain without model finetuning, where the source
and target domains can be vastly different. In many applications, model
reprogramming outperforms transfer learning and training from scratch. This
paper elucidates the methodology of model reprogramming, summarizes existing
use cases, provides a theoretical explanation of the success of model
reprogramming, and concludes with a discussion on open-ended research questions
and opportunities. A list of model reprogramming studies is actively maintained
and updated at https://github.com/IBM/model-reprogramming.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_P/0/1/0/all/0/1&quot;&gt;Pin-Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.01278">
<title>Technical Report on Subspace Pyramid Fusion Network for Semantic Segmentation. (arXiv:2204.01278v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2204.01278</link>
<description rdf:parseType="Literal">&lt;p&gt;The following is a technical report to test the validity of the proposed
Subspace Pyramid Fusion Module (SPFM) to capture multi-scale feature
representations, which is more useful for semantic segmentation. In this
investigation, we have proposed the Efficient Shuffle Attention Module(ESAM) to
reconstruct the skip-connections paths by fusing multi-level global context
features. Experimental results on two well-known semantic segmentation
datasets, including Camvid and Cityscapes, show the effectiveness of our
proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elhassan_M/0/1/0/all/0/1&quot;&gt;Mohammed A. M. Elhassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenhui Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chenxi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munea_T/0/1/0/all/0/1&quot;&gt;Tewodros Legesse Munea&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.08204">
<title>Inherent Inconsistencies of Feature Importance. (arXiv:2206.08204v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.08204</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapid advancement and widespread adoption of machine learning-driven
technologies have underscored the practical and ethical need for creating
interpretable artificial intelligence systems. Feature importance, a method
that assigns scores to the contribution of individual features on prediction
outcomes, seeks to bridge this gap as a tool for enhancing human comprehension
of these systems. Feature importance serves as an explanation of predictions in
diverse contexts, whether by providing a global interpretation of a phenomenon
across the entire dataset or by offering a localized explanation for the
outcome of a specific data point. Furthermore, feature importance is being used
both for explaining models and for identifying plausible causal relations in
the data, independently from the model. However, it is worth noting that these
various contexts have traditionally been explored in isolation, with limited
theoretical foundations.
&lt;/p&gt;
&lt;p&gt;This paper presents an axiomatic framework designed to establish coherent
relationships among the different contexts of feature importance scores.
Notably, our work unveils a surprising conclusion: when we combine the proposed
properties with those previously outlined in the literature, we demonstrate the
existence of an inconsistency. This inconsistency highlights that certain
essential properties of feature importance scores cannot coexist harmoniously
within a single framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harel_N/0/1/0/all/0/1&quot;&gt;Nimrod Harel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Obolski_U/0/1/0/all/0/1&quot;&gt;Uri Obolski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilad_Bachrach_R/0/1/0/all/0/1&quot;&gt;Ran Gilad-Bachrach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10706">
<title>TraSE: Towards Tackling Authorial Style from a Cognitive Science Perspective. (arXiv:2206.10706v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10706</link>
<description rdf:parseType="Literal">&lt;p&gt;Stylistic analysis of text is a key task in research areas ranging from
authorship attribution to forensic analysis and personality profiling. The
existing approaches for stylistic analysis are plagued by issues like topic
influence, lack of discriminability for large number of authors and the
requirement for large amounts of diverse data. In this paper, the source of
these issues are identified along with the necessity for a cognitive
perspective on authorial style in addressing them. A novel feature
representation, called Trajectory-based Style Estimation (TraSE), is introduced
to support this purpose. Authorship attribution experiments with over 27,000
authors and 1.4 million samples in a cross-domain scenario resulted in 90%
attribution accuracy suggesting that the feature representation is immune to
such negative influences and an excellent candidate for stylistic analysis.
Finally, a qualitative analysis is performed on TraSE using physical human
characteristics, like age, to validate its claim on capturing cognitive traits.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_R/0/1/0/all/0/1&quot;&gt;Ronald Wilson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhandarkar_A/0/1/0/all/0/1&quot;&gt;Avanti Bhandarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woodard_D/0/1/0/all/0/1&quot;&gt;Damon Woodard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02160">
<title>A Comprehensive Review of Visual-Textual Sentiment Analysis from Social Media Networks. (arXiv:2207.02160v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02160</link>
<description rdf:parseType="Literal">&lt;p&gt;Social media networks have become a significant aspect of people&apos;s lives,
serving as a platform for their ideas, opinions and emotions. Consequently,
automated sentiment analysis (SA) is critical for recognising people&apos;s feelings
in ways that other information sources cannot. The analysis of these feelings
revealed various applications, including brand evaluations, YouTube film
reviews and healthcare applications. As social media continues to develop,
people post a massive amount of information in different forms, including text,
photos, audio and video. Thus, traditional SA algorithms have become limited,
as they do not consider the expressiveness of other modalities. By including
such characteristics from various material sources, these multimodal data
streams provide new opportunities for optimising the expected results beyond
text-based SA. Our study focuses on the forefront field of multimodal SA, which
examines visual and textual data posted on social media networks. Many people
are more likely to utilise this information to express themselves on these
platforms. To serve as a resource for academics in this rapidly growing field,
we introduce a comprehensive overview of textual and visual SA, including data
pre-processing, feature extraction techniques, sentiment benchmark datasets,
and the efficacy of multiple classification methodologies suited to each field.
We also provide a brief introduction of the most frequently utilised data
fusion strategies and a summary of existing research on visual-textual SA.
Finally, we highlight the most significant challenges and investigate several
important sentiment applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Al_Tameemi_I/0/1/0/all/0/1&quot;&gt;Israa Khalaf Salman Al-Tameemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_Derakhshi_M/0/1/0/all/0/1&quot;&gt;Mohammad-Reza Feizi-Derakhshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pashazadeh_S/0/1/0/all/0/1&quot;&gt;Saeed Pashazadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asadpour_M/0/1/0/all/0/1&quot;&gt;Mohammad Asadpour&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.03932">
<title>Memory-free Online Change-point Detection: A Novel Neural Network Approach. (arXiv:2207.03932v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.03932</link>
<description rdf:parseType="Literal">&lt;p&gt;Change-point detection (CPD), which detects abrupt changes in the data
distribution, is recognized as one of the most significant tasks in time series
analysis. Despite the extensive literature on offline CPD, unsupervised online
CPD still suffers from major challenges, including scalability, hyperparameter
tuning, and learning constraints. To mitigate some of these challenges, in this
paper, we propose a novel deep learning approach for unsupervised online CPD
from multi-dimensional time series, named Adaptive LSTM-Autoencoder
Change-Point Detection (ALACPD). ALACPD exploits an LSTM-autoencoder-based
neural network to perform unsupervised online CPD. It continuously adapts to
the incoming samples without keeping the previously received input, thus being
memory-free. We perform an extensive evaluation on several real-world time
series CPD benchmarks. We show that ALACPD, on average, ranks first among
state-of-the-art CPD algorithms in terms of quality of the time series
segmentation, and it is on par with the best performer in terms of the accuracy
of the estimated change-points. The implementation of ALACPD is available
online on Github\footnote{\url{https://github.com/zahraatashgahi/ALACPD}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atashgahi_Z/0/1/0/all/0/1&quot;&gt;Zahra Atashgahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mocanu_D/0/1/0/all/0/1&quot;&gt;Decebal Constantin Mocanu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veldhuis_R/0/1/0/all/0/1&quot;&gt;Raymond Veldhuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechenizkiy_M/0/1/0/all/0/1&quot;&gt;Mykola Pechenizkiy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00779">
<title>DADAO: Decoupled Accelerated Decentralized Asynchronous Optimization. (arXiv:2208.00779v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00779</link>
<description rdf:parseType="Literal">&lt;p&gt;This work introduces DADAO: the first decentralized, accelerated,
asynchronous, primal, first-order algorithm to minimize a sum of $L$-smooth and
$\mu$-strongly convex functions distributed over a given network of size $n$.
Our key insight is based on modeling the local gradient updates and gossip
communication procedures with separate independent Poisson Point Processes.
This allows us to decouple the computation and communication steps, which can
be run in parallel, while making the whole approach completely asynchronous.
This leads to communication acceleration compared to synchronous approaches.
Our new method employs primal gradients and does not use a multi-consensus
inner loop nor other ad-hoc mechanisms such as Error Feedback, Gradient
Tracking, or a Proximal operator. By relating the inverse of the smallest
positive eigenvalue of the Laplacian matrix $\chi_1$ and the maximal resistance
$\chi_2\leq \chi_1$ of the graph to a sufficient minimal communication rate
between the nodes of the network, we show that our algorithm requires
$\mathcal{O}(n\sqrt{\frac{L}{\mu}}\log(\frac{1}{\epsilon}))$ local gradients
and only
$\mathcal{O}(n\sqrt{\chi_1\chi_2}\sqrt{\frac{L}{\mu}}\log(\frac{1}{\epsilon}))$
communications to reach a precision $\epsilon$, up to logarithmic terms. Thus,
we simultaneously obtain an accelerated rate for both computations and
communications, leading to an improvement over state-of-the-art works, our
simulations further validating the strength of our relatively unconstrained
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nabli_A/0/1/0/all/0/1&quot;&gt;Adel Nabli&lt;/a&gt; (MLIA, ISIR, MILA), &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Oyallon_E/0/1/0/all/0/1&quot;&gt;Edouard Oyallon&lt;/a&gt; (MLIA, ISIR)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.07624">
<title>Neuroevolution of Physics-Informed Neural Nets: Benchmark Problems and Comparative Results. (arXiv:2212.07624v3 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2212.07624</link>
<description rdf:parseType="Literal">&lt;p&gt;The potential of learned models for fundamental scientific research and
discovery is drawing increasing attention worldwide. Physics-informed neural
networks (PINNs), where the loss function directly embeds governing equations
of scientific phenomena, is one of the key techniques at the forefront of
recent advances. PINNs are typically trained using stochastic gradient descent
methods, akin to their deep learning counterparts. However, analysis in this
paper shows that PINNs&apos; unique loss formulations lead to a high degree of
complexity and ruggedness that may not be conducive for gradient descent.
Unlike in standard deep learning, PINN training requires globally optimum
parameter values that satisfy physical laws as closely as possible. Spurious
local optimum, indicative of erroneous physics, must be avoided. Hence,
neuroevolution algorithms, with their superior global search capacity, may be a
better choice for PINNs relative to gradient descent methods. Here, we propose
a set of five benchmark problems, with open-source codes, spanning diverse
physical phenomena for novel neuroevolution algorithm development. Using this,
we compare two neuroevolution algorithms against the commonly used stochastic
gradient descent, and our baseline results support the claim that
neuroevolution can surpass gradient descent, ensuring better physics compliance
in the predicted outputs. %Furthermore, implementing neuroevolution with JAX
leads to orders of magnitude speedup relative to standard implementations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_N/0/1/0/all/0/1&quot;&gt;Nicholas Sung Wei Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_J/0/1/0/all/0/1&quot;&gt;Jian Cheng Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiu_P/0/1/0/all/0/1&quot;&gt;Pao-Hsiung Chiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ooi_C/0/1/0/all/0/1&quot;&gt;Chinchun Ooi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_Y/0/1/0/all/0/1&quot;&gt;Yew-Soon Ong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09710">
<title>Continual Learning for Instruction Following from Realtime Feedback. (arXiv:2212.09710v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09710</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and deploy an approach to continually train an
instruction-following agent from feedback provided by users during
collaborative interactions. During interaction, human users instruct an agent
using natural language, and provide realtime binary feedback as they observe
the agent following their instructions. We design a contextual bandit learning
approach, converting user feedback to immediate reward. We evaluate through
thousands of human-agent interactions, demonstrating 15.4% absolute improvement
in instruction execution accuracy over time. We also show our approach is
robust to several design variations, and that the feedback signal is roughly
equivalent to the learning signal of supervised demonstration data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suhr_A/0/1/0/all/0/1&quot;&gt;Alane Suhr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Artzi_Y/0/1/0/all/0/1&quot;&gt;Yoav Artzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05403">
<title>Completeness, Recall, and Negation in Open-World Knowledge Bases: A Survey. (arXiv:2305.05403v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05403</link>
<description rdf:parseType="Literal">&lt;p&gt;General-purpose knowledge bases (KBs) are a cornerstone of knowledge-centric
AI. Many of them are constructed pragmatically from Web sources, and are thus
far from complete. This poses challenges for the consumption as well as the
curation of their content. While several surveys target the problem of
completing incomplete KBs, the first problem is arguably to know whether and
where the KB is incomplete in the first place, and to which degree.
&lt;/p&gt;
&lt;p&gt;In this survey we discuss how knowledge about completeness, recall, and
negation in KBs can be expressed, extracted, and inferred. We cover (i) the
logical foundations of knowledge representation and querying under partial
closed-world semantics; (ii) the estimation of this information via statistical
patterns; (iii) the extraction of information about recall from KBs and text;
(iv) the identification of interesting negative statements; and (v) relaxed
notions of relative recall.
&lt;/p&gt;
&lt;p&gt;This survey is targeted at two types of audiences: (1) practitioners who are
interested in tracking KB quality, focusing extraction efforts, and building
quality-aware downstream applications; and (2) data management, knowledge base
and semantic web researchers who wish to understand the state of the art of
knowledge bases beyond the open-world assumption. Consequently, our survey
presents both fundamental methodologies and their working, and gives
practice-oriented recommendations on how to choose between different approaches
for a problem at hand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razniewski_S/0/1/0/all/0/1&quot;&gt;Simon Razniewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arnaout_H/0/1/0/all/0/1&quot;&gt;Hiba Arnaout&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shrestha Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suchanek_F/0/1/0/all/0/1&quot;&gt;Fabian Suchanek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12524">
<title>TheoremQA: A Theorem-driven Question Answering dataset. (arXiv:2305.12524v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12524</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent LLMs like GPT-4 and PaLM-2 have made tremendous progress in
solving fundamental math problems like GSM8K by achieving over 90% accuracy.
However, their capabilities to solve more challenging math problems which
require domain-specific knowledge (i.e. theorem) have yet to be investigated.
In this paper, we introduce TheoremQA, the first theorem-driven
question-answering dataset designed to evaluate AI models&apos; capabilities to
apply theorems to solve challenging science problems. TheoremQA is curated by
domain experts containing 800 high-quality questions covering 350 theorems
(e.g. Taylor&apos;s theorem, Lagrange&apos;s theorem, Huffman coding, Quantum Theorem,
Elasticity Theorem, etc) from Math, Physics, EE&amp;amp;CS, and Finance. We evaluate a
wide spectrum of 16 large language and code models with different prompting
strategies like Chain-of-Thoughts and Program-of-Thoughts. We found that
GPT-4&apos;s capabilities to solve these problems are unparalleled, achieving an
accuracy of 51% with Program-of-Thoughts Prompting. All the existing
open-sourced models are below 15%, barely surpassing the random-guess baseline.
Given the diversity and broad coverage of TheoremQA, we believe it can be used
as a better benchmark to evaluate LLMs&apos; capabilities to solve challenging
science problems. The data and code are released in
https://github.com/wenhuchen/TheoremQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Ming Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ku_M/0/1/0/all/0/1&quot;&gt;Max Ku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_P/0/1/0/all/0/1&quot;&gt;Pan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_Y/0/1/0/all/0/1&quot;&gt;Yixin Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xueguang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jianyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_T/0/1/0/all/0/1&quot;&gt;Tony Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14387">
<title>AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback. (arXiv:2305.14387v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14387</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) such as ChatGPT have seen widespread adoption
due to their ability to follow user instructions well. Developing these LLMs
involves a complex yet poorly understood workflow requiring training with human
feedback. Replicating and understanding this instruction-following process
faces three major challenges: the high cost of data collection, the lack of
trustworthy evaluation, and the absence of reference method implementations. We
address these challenges with AlpacaFarm, a simulator that enables research and
development for learning from feedback at a low cost. First, we design LLM
prompts to simulate human feedback that are 45x cheaper than crowdworkers and
display high agreement with humans. Second, we propose an automatic evaluation
and validate it against human instructions obtained on real-world interactions.
Third, we contribute reference implementations for several methods (PPO, DPO,
best-of-n, expert iteration, and more) that learn from pairwise feedback.
Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate
eleven models on 10k pairs of real human feedback and show that rankings of
models trained in AlpacaFarm match rankings of models trained on human data. As
a demonstration of the research possible in AlpacaFarm, we find that methods
that use a reward model can substantially improve over supervised fine-tuning
and that our reference PPO implementation leads to a +10% improvement in
win-rate against Davinci003. We release all components of AlpacaFarm at
https://github.com/tatsu-lab/alpaca_farm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubois_Y/0/1/0/all/0/1&quot;&gt;Yann Dubois&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuechen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taori_R/0/1/0/all/0/1&quot;&gt;Rohan Taori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gulrajani_I/0/1/0/all/0/1&quot;&gt;Ishaan Gulrajani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ba_J/0/1/0/all/0/1&quot;&gt;Jimmy Ba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guestrin_C/0/1/0/all/0/1&quot;&gt;Carlos Guestrin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Percy Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hashimoto_T/0/1/0/all/0/1&quot;&gt;Tatsunori B. Hashimoto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.17390">
<title>SwiftSage: A Generative Agent with Fast and Slow Thinking for Complex Interactive Tasks. (arXiv:2305.17390v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.17390</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce SwiftSage, a novel agent framework inspired by the dual-process
theory of human cognition, designed to excel in action planning for complex
interactive reasoning tasks. SwiftSage integrates the strengths of behavior
cloning and prompting large language models (LLMs) to enhance task completion
performance. The framework comprises two primary modules: the Swift module,
representing fast and intuitive thinking, and the Sage module, emulating
deliberate thought processes. The Swift module is a small encoder-decoder LM
fine-tuned on the oracle agent&apos;s action trajectories, while the Sage module
employs LLMs such as GPT-4 for subgoal planning and grounding. We develop a
heuristic method to harmoniously integrate the two modules, resulting in a more
efficient and robust problem-solving process. In 30 tasks from the ScienceWorld
benchmark, SwiftSage significantly outperforms other methods such as SayCan,
ReAct, and Reflexion, demonstrating its effectiveness in solving complex
interactive tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yicheng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Karina Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brahman_F/0/1/0/all/0/1&quot;&gt;Faeze Brahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shiyu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhagavatula_C/0/1/0/all/0/1&quot;&gt;Chandra Bhagavatula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ammanabrolu_P/0/1/0/all/0/1&quot;&gt;Prithviraj Ammanabrolu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18731">
<title>Epistemic Graph: A Plug-And-Play Module For Hybrid Representation Learning. (arXiv:2305.18731v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18731</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, deep models have achieved remarkable success in various
vision tasks. However, their performance heavily relies on large training
datasets. In contrast, humans exhibit hybrid learning, seamlessly integrating
structured knowledge for cross-domain recognition or relying on a smaller
amount of data samples for few-shot learning. Motivated by this human-like
epistemic process, we aim to extend hybrid learning to computer vision tasks by
integrating structured knowledge with data samples for more effective
representation learning. Nevertheless, this extension faces significant
challenges due to the substantial gap between structured knowledge and deep
features learned from data samples, encompassing both dimensions and knowledge
granularity. In this paper, a novel Epistemic Graph Layer (EGLayer) is
introduced to enable hybrid learning, enhancing the exchange of information
between deep features and a structured knowledge graph. Our EGLayer is composed
of three major parts, including a local graph module, a query aggregation
model, and a novel correlation alignment loss function to emulate human
epistemic ability. Serving as a plug-and-play module that can replace the
standard linear classifier, EGLayer significantly improves the performance of
deep models. Extensive experiments demonstrates that EGLayer can greatly
enhance representation learning for the tasks of cross-domain recognition and
few-shot learning, and the visualization of knowledge graphs can aid in model
interpretation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Jin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yangzhou Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Z/0/1/0/all/0/1&quot;&gt;Zhongchao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_X/0/1/0/all/0/1&quot;&gt;Xin Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jianping Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rui_Y/0/1/0/all/0/1&quot;&gt;Yong Rui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09597">
<title>Clickbait Detection via Large Language Models. (arXiv:2306.09597v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09597</link>
<description rdf:parseType="Literal">&lt;p&gt;Clickbait, which aims to induce users with some surprising and even thrilling
headlines for increasing click-through rates, permeates almost all online
content publishers, such as news portals and social media. Recently, Large
Language Models (LLMs) have emerged as a powerful instrument and achieved
tremendous success in a series of NLP downstream tasks. However, it is not yet
known whether LLMs can be served as a high-quality clickbait detection system.
In this paper, we analyze the performance of LLMs in the few-shot and zero-shot
scenarios on several English and Chinese benchmark datasets. Experimental
results show that LLMs cannot achieve the best results compared to the
state-of-the-art deep and fine-tuning PLMs methods. Different from human
intuition, the experiments demonstrated that LLMs cannot make satisfied
clickbait detection just by the headlines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Han Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ye Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yunhao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_J/0/1/0/all/0/1&quot;&gt;Jipeng Qiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14468">
<title>A General Framework for Sequential Decision-Making under Adaptivity Constraints. (arXiv:2306.14468v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14468</link>
<description rdf:parseType="Literal">&lt;p&gt;We take the first step in studying general sequential decision-making under
two adaptivity constraints: rare policy switch and batch learning. First, we
provide a general class called the Eluder Condition class, which includes a
wide range of reinforcement learning classes. Then, for the rare policy switch
constraint, we provide a generic algorithm to achieve a
$\widetilde{\mathcal{O}}(\log K) $ switching cost with a
$\widetilde{\mathcal{O}}(\sqrt{K})$ regret on the EC class. For the batch
learning constraint, we provide an algorithm that provides a
$\widetilde{\mathcal{O}}(\sqrt{K}+K/B)$ regret with the number of batches $B.$
This paper is the first work considering rare policy switch and batch learning
under general function classes, which covers nearly all the models studied in
the previous works such as tabular MDP (Bai et al. 2019; Zhang et al. 2020),
linear MDP (Wang et al. 2021; Gao et al. 2021), low eluder dimension MDP (Kong
et al. 2021; Gao et al. 2021), generalized linear function approximation (Qiao
et al. 2023), and also some new classes such as the low $D_\Delta$-type Bellman
eluder dimension problem, linear mixture MDP, kernelized nonlinear regulator
and undercomplete partially observed Markov decision process (POMDP).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_N/0/1/0/all/0/1&quot;&gt;Nuoya Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhaoran Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00952">
<title>Towards Explainable AI for Channel Estimation in Wireless Communications. (arXiv:2307.00952v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00952</link>
<description rdf:parseType="Literal">&lt;p&gt;Research into 6G networks has been initiated to support a variety of critical
artificial intelligence (AI) assisted applications such as autonomous driving.
In such applications, AI-based decisions should be performed in a real-time
manner. These decisions include resource allocation, localization, channel
estimation, etc. Considering the black-box nature of existing AI-based models,
it is highly challenging to understand and trust the decision-making behavior
of such models. Therefore, explaining the logic behind those models through
explainable AI (XAI) techniques is essential for their employment in critical
applications. This manuscript proposes a novel XAI-based channel estimation
(XAI-CHEST) scheme that provides detailed reasonable interpretability of the
deep learning (DL) models that are employed in doubly-selective channel
estimation. The aim of the proposed XAI-CHEST scheme is to identify the
relevant model inputs by inducing high noise on the irrelevant ones. As a
result, the behavior of the studied DL-based channel estimators can be further
analyzed and evaluated based on the generated interpretations. Simulation
results show that the proposed XAI-CHEST scheme provides valid interpretations
of the DL-based channel estimators for different scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gizzini_A/0/1/0/all/0/1&quot;&gt;Abdul Karim Gizzini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Medjahdi_Y/0/1/0/all/0/1&quot;&gt;Yahia Medjahdi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghandour_A/0/1/0/all/0/1&quot;&gt;Ali J. Ghandour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clavier_L/0/1/0/all/0/1&quot;&gt;Laurent Clavier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03761">
<title>DyEdgeGAT: Dynamic Edge via Graph Attention for Early Fault Detection in IIoT Systems. (arXiv:2307.03761v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03761</link>
<description rdf:parseType="Literal">&lt;p&gt;In the industrial Internet of Things, condition monitoring sensor signals
from complex systems often exhibit strong nonlinear and stochastic
spatial-temporal dynamics under varying operating conditions. Such complex
dynamics make fault detection particularly challenging. Although previously
proposed methods effectively model these dynamics, they often neglect the
dynamic evolution of relationships between sensor signals. Undetected shifts in
these relationships can potentially result in significant system failures.
Another limitation is their inability to effectively distinguish between novel
operating conditions and actual faults. To address this gap, we propose
DyEdgeGAT (Dynamic Edge via Graph Attention), a novel approach capable of
detecting various faults, especially those characterized by relationship
changes at early stages, while distinguishing faults from novel operating
conditions. DyEdgeGAT is a graph-based framework that provides a novel graph
inference scheme for multivariate time series that dynamically constructs edges
to represent and track the evolution of relationships between time series.
Additionally, it addresses a commonly overlooked aspect: the cause-and-effect
relationships within the system, such as between control inputs and
measurements. By incorporating system-independent variables as contexts of
operating conditions into node dynamics extraction, DyEdgeGAT enhances its
robustness against novel operating conditions. We rigorously evaluate
DyEdgeGAT&apos;s performance using both a synthetic dataset, designed to simulate
varying levels of fault severity and a real-world industrial-scale benchmark
containing a variety of fault types with different detection complexities. Our
findings demonstrate that DyEdgeGAT is highly effective in fault detection,
showing particular strength in early fault detection while maintaining
robustness under novel operating conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Mengjie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fink_O/0/1/0/all/0/1&quot;&gt;Olga Fink&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04749">
<title>Divide, Evaluate, and Refine: Evaluating and Improving Text-to-Image Alignment with Iterative VQA Feedback. (arXiv:2307.04749v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04749</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of text-conditioned image generation has made unparalleled progress
with the recent advent of latent diffusion models. While remarkable, as the
complexity of given text input increases, the state-of-the-art diffusion models
may still fail in generating images which accurately convey the semantics of
the given prompt. Furthermore, it has been observed that such misalignments are
often left undetected by pretrained multi-modal models such as CLIP. To address
these problems, in this paper we explore a simple yet effective decompositional
approach towards both evaluation and improvement of text-to-image alignment. In
particular, we first introduce a Decompositional-Alignment-Score which given a
complex prompt decomposes it into a set of disjoint assertions. The alignment
of each assertion with generated images is then measured using a VQA model.
Finally, alignment scores for different assertions are combined aposteriori to
give the final text-to-image alignment score. Experimental analysis reveals
that the proposed alignment metric shows significantly higher correlation with
human ratings as opposed to traditional CLIP, BLIP scores. Furthermore, we also
find that the assertion level alignment scores provide a useful feedback which
can then be used in a simple iterative procedure to gradually increase the
expression of different assertions in the final image outputs. Human user
studies indicate that the proposed approach surpasses previous state-of-the-art
by 8.7% in overall text-to-image alignment accuracy. Project page for our paper
is available at https://1jsingh.github.io/divide-evaluate-and-refine
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Liang Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09477">
<title>Towards Ordinal Data Science. (arXiv:2307.09477v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09477</link>
<description rdf:parseType="Literal">&lt;p&gt;Order is one of the main instruments to measure the relationship between
objects in (empirical) data. However, compared to methods that use numerical
properties of objects, the amount of ordinal methods developed is rather small.
One reason for this is the limited availability of computational resources in
the last century that would have been required for ordinal computations.
Another reason -- particularly important for this line of research -- is that
order-based methods are often seen as too mathematically rigorous for applying
them to real-world data. In this paper, we will therefore discuss different
means for measuring and &apos;calculating&apos; with ordinal structures -- a specific
class of directed graphs -- and show how to infer knowledge from them. Our aim
is to establish Ordinal Data Science as a fundamentally new research agenda.
Besides cross-fertilization with other cornerstone machine learning and
knowledge representation methods, a broad range of disciplines will benefit
from this endeavor, including, psychology, sociology, economics, web science,
knowledge engineering, scientometrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stumme_G/0/1/0/all/0/1&quot;&gt;Gerd Stumme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durrschnabel_D/0/1/0/all/0/1&quot;&gt;Dominik D&amp;#xfc;rrschnabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanika_T/0/1/0/all/0/1&quot;&gt;Tom Hanika&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13912">
<title>Embedding Democratic Values into Social Media AIs via Societal Objective Functions. (arXiv:2307.13912v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13912</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we design artificial intelligence (AI) systems that rank our social media
feeds to consider democratic values such as mitigating partisan animosity as
part of their objective functions? We introduce a method for translating
established, vetted social scientific constructs into AI objective functions,
which we term societal objective functions, and demonstrate the method with
application to the political science construct of anti-democratic attitudes.
Traditionally, we have lacked observable outcomes to use to train such models,
however, the social sciences have developed survey instruments and qualitative
codebooks for these constructs, and their precision facilitates translation
into detailed prompts for large language models. We apply this method to create
a democratic attitude model that estimates the extent to which a social media
post promotes anti-democratic attitudes, and test this democratic attitude
model across three studies. In Study 1, we first test the attitudinal and
behavioral effectiveness of the intervention among US partisans (N=1,380) by
manually annotating (alpha=.895) social media posts with anti-democratic
attitude scores and testing several feed ranking conditions based on these
scores. Removal (d=.20) and downranking feeds (d=.25) reduced participants&apos;
partisan animosity without compromising their experience and engagement. In
Study 2, we scale up the manual labels by creating the democratic attitude
model, finding strong agreement with manual labels (rho=.75). Finally, in Study
3, we replicate Study 1 using the democratic attitude model instead of manual
labels to test its attitudinal and behavioral impact (N=558), and again find
that the feed downranking using the societal objective function reduced
partisan animosity (d=.25). This method presents a novel strategy to draw on
social science theory and methods to mitigate societal harms in social media
AIs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chenyan Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_M/0/1/0/all/0/1&quot;&gt;Michelle S. Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_M/0/1/0/all/0/1&quot;&gt;Minh Chau Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hancock_J/0/1/0/all/0/1&quot;&gt;Jeff Hancock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bernstein_M/0/1/0/all/0/1&quot;&gt;Michael S. Bernstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.04673">
<title>SSL-Auth: An Authentication Framework by Fragile Watermarking for Pre-trained Encoders in Self-supervised Learning. (arXiv:2308.04673v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2308.04673</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL), a paradigm harnessing unlabeled datasets to
train robust encoders, has recently witnessed substantial success. These
encoders serve as pivotal feature extractors for downstream tasks, demanding
significant computational resources. Nevertheless, recent studies have shed
light on vulnerabilities in pre-trained encoders, including backdoor and
adversarial threats. Safeguarding the intellectual property of encoder trainers
and ensuring the trustworthiness of deployed encoders pose notable challenges
in SSL. To bridge these gaps, we introduce SSL-Auth, the first authentication
framework designed explicitly for pre-trained encoders. SSL-Auth leverages
selected key samples and employs a well-trained generative network to
reconstruct watermark information, thus affirming the integrity of the encoder
without compromising its performance. By comparing the reconstruction outcomes
of the key samples, we can identify any malicious alterations. Comprehensive
evaluations conducted on a range of encoders and diverse downstream tasks
demonstrate the effectiveness of our proposed SSL-Auth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaobei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_C/0/1/0/all/0/1&quot;&gt;Changchun Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liyue Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Liming Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Run Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chenhao Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07992">
<title>An Automated Machine Learning Approach for Detecting Anomalous Peak Patterns in Time Series Data from a Research Watershed in the Northeastern United States Critical Zone. (arXiv:2309.07992v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07992</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an automated machine learning framework designed to
assist hydrologists in detecting anomalies in time series data generated by
sensors in a research watershed in the northeastern United States critical
zone. The framework specifically focuses on identifying peak-pattern anomalies,
which may arise from sensor malfunctions or natural phenomena. However, the use
of classification methods for anomaly detection poses challenges, such as the
requirement for labeled data as ground truth and the selection of the most
suitable deep learning model for the given task and dataset. To address these
challenges, our framework generates labeled datasets by injecting synthetic
peak patterns into synthetically generated time series data and incorporates an
automated hyperparameter optimization mechanism. This mechanism generates an
optimized model instance with the best architectural and training parameters
from a pool of five selected models, namely Temporal Convolutional Network
(TCN), InceptionTime, MiniRocket, Residual Networks (ResNet), and Long
Short-Term Memory (LSTM). The selection is based on the user&apos;s preferences
regarding anomaly detection accuracy and computational cost. The framework
employs Time-series Generative Adversarial Networks (TimeGAN) as the synthetic
dataset generator. The generated model instances are evaluated using a
combination of accuracy and computational cost metrics, including training time
and memory, during the anomaly detection process. Performance evaluation of the
framework was conducted using a dataset from a watershed, demonstrating
consistent selection of the most fitting model instance that satisfies the
user&apos;s preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haq_I/0/1/0/all/0/1&quot;&gt;Ijaz Ul Haq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_B/0/1/0/all/0/1&quot;&gt;Byung Suk Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rizzo_D/0/1/0/all/0/1&quot;&gt;Donna M. Rizzo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perdrial_J/0/1/0/all/0/1&quot;&gt;Julia N Perdrial&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14209">
<title>Continual Driving Policy Optimization with Closed-Loop Individualized Curricula. (arXiv:2309.14209v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14209</link>
<description rdf:parseType="Literal">&lt;p&gt;The safety of autonomous vehicles (AV) has been a long-standing top concern,
stemming from the absence of rare and safety-critical scenarios in the
long-tail naturalistic driving distribution. To tackle this challenge, a surge
of research in scenario-based autonomous driving has emerged, with a focus on
generating high-risk driving scenarios and applying them to conduct
safety-critical testing of AV models. However, limited work has been explored
on the reuse of these extensive scenarios to iteratively improve AV models.
Moreover, it remains intractable and challenging to filter through gigantic
scenario libraries collected from other AV models with distinct behaviors,
attempting to extract transferable information for current AV improvement.
Therefore, we develop a continual driving policy optimization framework
featuring Closed-Loop Individualized Curricula (CLIC), which we factorize into
a set of standardized sub-modules for flexible implementation choices: AV
Evaluation, Scenario Selection, and AV Training. CLIC frames AV Evaluation as a
collision prediction task, where it estimates the chance of AV failures in
these scenarios at each iteration. Subsequently, by re-sampling from historical
scenarios based on these failure probabilities, CLIC tailors individualized
curricula for downstream training, aligning them with the evaluated capability
of AV. Accordingly, CLIC not only maximizes the utilization of the vast
pre-collected scenario library for closed-loop driving policy optimization but
also facilitates AV improvement by individualizing its training with more
challenging cases out of those poorly organized scenarios. Experimental results
clearly indicate that CLIC surpasses other curriculum-based training
strategies, showing substantial improvement in managing risky scenarios, while
still maintaining proficiency in handling simpler cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_H/0/1/0/all/0/1&quot;&gt;Haoyi Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xingjian Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jianming Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00752">
<title>TIGERScore: Towards Building Explainable Metric for All Text Generation Tasks. (arXiv:2310.00752v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00752</link>
<description rdf:parseType="Literal">&lt;p&gt;We present TIGERScore, a \textbf{T}rained metric that follows
\textbf{I}nstruction \textbf{G}uidance to perform \textbf{E}xplainable, and
\textbf{R}eference-free evaluation over a wide spectrum of text generation
tasks. Different from other automatic evaluation methods that only provide
arcane scores, TIGERScore is guided by natural language instruction to provide
error analysis to pinpoint the mistakes in the generated text. Our metric is
based on LLaMA-2, trained on our meticulously curated instruction-tuning
dataset MetricInstruct which covers 6 text generation tasks and 23 text
generation datasets. The dataset consists of 42K quadruple in the form of
(instruction, input, system output $\rightarrow$ error analysis). We collected
the `system outputs&apos; through from a large variety of models to cover different
types of errors. To quantitatively assess our metric, we evaluate its
correlation with human ratings on 5 held-in datasets, 2 held-out datasets and
show that TIGERScore can achieve the open-source SoTA correlation with human
ratings across these datasets and almost approaches GPT-4 evaluator. As a
reference-free metric, its correlation can even surpass the best existing
reference-based metrics. To further qualitatively assess the rationale
generated by our metric, we conduct human evaluation on the generated
explanations and found that the explanations are 70.8\% accurate. Through these
experimental results, we believe TIGERScore demonstrates the possibility of
building universal explainable metrics to evaluate any text generation task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yishan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Ge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenhu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03059">
<title>Point-PEFT: Parameter-Efficient Fine-Tuning for 3D Pre-trained Models. (arXiv:2310.03059v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03059</link>
<description rdf:parseType="Literal">&lt;p&gt;The popularity of pre-trained large models has revolutionized downstream
tasks across diverse fields, such as language, vision, and multi-modality. To
minimize the adaption cost for downstream tasks, many Parameter-Efficient
Fine-Tuning (PEFT) techniques are proposed for language and 2D image
pre-trained models. However, the specialized PEFT method for 3D pre-trained
models is still under-explored. To this end, we introduce Point-PEFT, a novel
framework for adapting point cloud pre-trained models with minimal learnable
parameters. Specifically, for a pre-trained 3D model, we freeze most of its
parameters, and only tune the newly added PEFT modules on downstream tasks,
which consist of a Point-prior Prompt and a Geometry-aware Adapter. The
Point-prior Prompt adopts a set of learnable prompt tokens, for which we
propose to construct a memory bank with domain-specific knowledge, and utilize
a parameter-free attention to enhance the prompt tokens. The Geometry-aware
Adapter aims to aggregate point cloud features within spatial neighborhoods to
capture fine-grained geometric information through local interactions.
Extensive experiments indicate that our Point-PEFT can achieve better
performance than the full fine-tuning on various downstream tasks, while using
only 5% of the trainable parameters, demonstrating the efficiency and
effectiveness of our approach. Code will be released at
https://github.com/Even-JK/PEFT-3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_I/0/1/0/all/0/1&quot;&gt;Ivan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ray Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zoey Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xianzheng Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19378">
<title>Few-shot Hybrid Domain Adaptation of Image Generators. (arXiv:2310.19378v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19378</link>
<description rdf:parseType="Literal">&lt;p&gt;Can a pre-trained generator be adapted to the hybrid of multiple target
domains and generate images with integrated attributes of them? In this work,
we introduce a new task -- Few-shot Hybrid Domain Adaptation (HDA). Given a
source generator and several target domains, HDA aims to acquire an adapted
generator that preserves the integrated attributes of all target domains,
without overriding the source domain&apos;s characteristics. Compared with Domain
Adaptation (DA), HDA offers greater flexibility and versatility to adapt
generators to more composite and expansive domains. Simultaneously, HDA also
presents more challenges than DA as we have access only to images from
individual target domains and lack authentic images from the hybrid domain. To
address this issue, we introduce a discriminator-free framework that directly
encodes different domains&apos; images into well-separable subspaces. To achieve
HDA, we propose a novel directional subspace loss comprised of a distance loss
and a direction loss. Concretely, the distance loss blends the attributes of
all target domains by reducing the distances from generated images to all
target subspaces. The direction loss preserves the characteristics from the
source domain by guiding the adaptation along the perpendicular to subspaces.
Experiments show that our method can obtain numerous domain-specific attributes
in a single adapted generator, which surpasses the baseline methods in semantic
similarity, image fidelity, and cross-domain consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hengjia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_L/0/1/0/all/0/1&quot;&gt;Linxuan Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yuqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_X/0/1/0/all/0/1&quot;&gt;Xiaohui Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaobo Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xiaofei He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19786">
<title>From External to Swap Regret 2.0: An Efficient Reduction and Oblivious Adversary for Large Action Spaces. (arXiv:2310.19786v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19786</link>
<description rdf:parseType="Literal">&lt;p&gt;We provide a novel reduction from swap-regret minimization to external-regret
minimization, which improves upon the classical reductions of Blum-Mansour
[BM07] and Stolz-Lugosi [SL05] in that it does not require finiteness of the
space of actions. We show that, whenever there exists a no-external-regret
algorithm for some hypothesis class, there must also exist a no-swap-regret
algorithm for that same class. For the problem of learning with expert advice,
our result implies that it is possible to guarantee that the swap regret is
bounded by {\epsilon} after $\log(N)^{O(1/\epsilon)}$ rounds and with $O(N)$
per iteration complexity, where $N$ is the number of experts, while the
classical reductions of Blum-Mansour and Stolz-Lugosi require $O(N/\epsilon^2)$
rounds and at least $\Omega(N^2)$ per iteration complexity. Our result comes
with an associated lower bound, which -- in contrast to that in [BM07] -- holds
for oblivious and $\ell_1$-constrained adversaries and learners that can employ
distributions over experts, showing that the number of rounds must be
$\tilde\Omega(N/\epsilon^2)$ or exponential in $1/\epsilon$.
&lt;/p&gt;
&lt;p&gt;Our reduction implies that, if no-regret learning is possible in some game,
then this game must have approximate correlated equilibria, of arbitrarily good
approximation. This strengthens the folklore implication of no-regret learning
that approximate coarse correlated equilibria exist. Importantly, it provides a
sufficient condition for the existence of correlated equilibrium which vastly
extends the requirement that the action set is finite, thus answering a
question left open by [DG22; Ass+23]. Moreover, it answers several outstanding
questions about equilibrium computation and learning in games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dagan_Y/0/1/0/all/0/1&quot;&gt;Yuval Dagan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daskalakis_C/0/1/0/all/0/1&quot;&gt;Constantinos Daskalakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fishelson_M/0/1/0/all/0/1&quot;&gt;Maxwell Fishelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Golowich_N/0/1/0/all/0/1&quot;&gt;Noah Golowich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00694">
<title>Unleashing the Creative Mind: Language Model As Hierarchical Policy For Improved Exploration on Challenging Problem Solving. (arXiv:2311.00694v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00694</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have achieved tremendous progress, yet they
still often struggle with challenging reasoning problems. Current approaches
address this challenge by sampling or searching detailed and low-level
reasoning chains. However, these methods are still limited in their exploration
capabilities, making it challenging for correct solutions to stand out in the
huge solution space. In this work, we unleash LLMs&apos; creative potential for
exploring multiple diverse problem solving strategies by framing an LLM as a
hierarchical policy via in-context learning. This policy comprises of a
visionary leader that proposes multiple diverse high-level problem-solving
tactics as hints, accompanied by a follower that executes detailed
problem-solving processes following each of the high-level instruction. The
follower uses each of the leader&apos;s directives as a guide and samples multiple
reasoning chains to tackle the problem, generating a solution group for each
leader proposal. Additionally, we propose an effective and efficient
tournament-based approach to select among these explored solution groups to
reach the final answer. Our approach produces meaningful and inspiring hints,
enhances problem-solving strategy exploration, and improves the final answer
accuracy on challenging problems in the MATH dataset. Code will be released at
https://github.com/lz1oceani/LLM-As-Hierarchical-Policy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_T/0/1/0/all/0/1&quot;&gt;Tongzhou Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Mingu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourreza_R/0/1/0/all/0/1&quot;&gt;Reza Pourreza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Memisevic_R/0/1/0/all/0/1&quot;&gt;Roland Memisevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03393">
<title>Sketching Multidimensional Time Series for Fast Discord Mining. (arXiv:2311.03393v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series discords are a useful primitive for time series anomaly
detection, and the matrix profile is capable of capturing discord effectively.
There exist many research efforts to improve the scalability of discord
discovery with respect to the length of time series. However, there is
surprisingly little work focused on reducing the time complexity of matrix
profile computation associated with dimensionality of a multidimensional time
series. In this work, we propose a sketch for discord mining among
multi-dimensional time series. After an initial pre-processing of the sketch as
fast as reading the data, the discord mining has runtime independent of the
dimensionality of the original data. On several real world examples from water
treatment and transportation, the proposed algorithm improves the throughput by
at least an order of magnitude (50X) and only has minimal impact on the quality
of the approximated solution. Additionally, the proposed method can handle the
dynamic addition or deletion of dimensions inconsequential overhead. This
allows a data analyst to consider &quot;what-if&quot; scenarios in real time while
exploring the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1&quot;&gt;Menghai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1&quot;&gt;Jeff M. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keogh_E/0/1/0/all/0/1&quot;&gt;Eamonn Keogh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.04256">
<title>Foundational propositions of hesitant fuzzy sets and parameter reductions of hesitant fuzzy information systems. (arXiv:2311.04256v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.04256</link>
<description rdf:parseType="Literal">&lt;p&gt;Hesitant fuzzy sets are widely used in the instances of uncertainty and
hesitation. The inclusion relationship is an important and foundational
definition for sets. Hesitant fuzzy set, as a kind of set, needs explicit
definition of inclusion relationship. Base on the hesitant fuzzy membership
degree of discrete form, several kinds of inclusion relationships for hesitant
fuzzy sets are proposed. And then some foundational propositions of hesitant
fuzzy sets and the families of hesitant fuzzy sets are presented. Finally, some
foundational propositions of hesitant fuzzy information systems with respect to
parameter reductions are put forward, and an example and an algorithm are given
to illustrate the processes of parameter reductions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shizhan Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09428">
<title>Beyond Detection: Unveiling Fairness Vulnerabilities in Abusive Language Models. (arXiv:2311.09428v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09428</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the potential of undermining both fairness and
detection performance in abusive language detection. In a dynamic and complex
digital world, it is crucial to investigate the vulnerabilities of these
detection models to adversarial fairness attacks to improve their fairness
robustness. We propose a simple yet effective framework FABLE that leverages
backdoor attacks as they allow targeted control over the fairness and detection
performance. FABLE explores three types of trigger designs (i.e., rare,
artificial, and natural triggers) and novel sampling strategies. Specifically,
the adversary can inject triggers into samples in the minority group with the
favored outcome (i.e., &quot;non-abusive&quot;) and flip their labels to the unfavored
outcome, i.e., &quot;abusive&quot;. Experiments on benchmark datasets demonstrate the
effectiveness of FABLE attacking fairness and utility in abusive language
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yueqing Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Lu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Payani_A/0/1/0/all/0/1&quot;&gt;Ali Payani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_K/0/1/0/all/0/1&quot;&gt;Kai Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13750">
<title>Towards Transferable Multi-modal Perception Representation Learning for Autonomy: NeRF-Supervised Masked AutoEncoder. (arXiv:2311.13750v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13750</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes a unified self-supervised pre-training framework for
transferable multi-modal perception representation learning via masked
multi-modal reconstruction in Neural Radiance Field (NeRF), namely
NeRF-Supervised Masked AutoEncoder (NS-MAE). Specifically, conditioned on
certain view directions and locations, multi-modal embeddings extracted from
corrupted multi-modal input signals, i.e., Lidar point clouds and images, are
rendered into projected multi-modal feature maps via neural rendering. Then,
original multi-modal signals serve as reconstruction targets for the rendered
multi-modal feature maps to enable self-supervised representation learning.
Extensive experiments show that the representation learned via NS-MAE shows
promising transferability for diverse multi-modal and single-modal (camera-only
and Lidar-only) perception models on diverse 3D perception downstream tasks (3D
object detection and BEV map segmentation) with diverse amounts of fine-tuning
labeled data. Moreover, we empirically find that NS-MAE enjoys the synergy of
both the mechanism of masked autoencoder and neural radiance field. We hope
this study can inspire exploration of more general multi-modal representation
learning for autonomous agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaohao Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14948">
<title>Effective Backdoor Mitigation Depends on the Pre-training Objective. (arXiv:2311.14948v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14948</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the advanced capabilities of contemporary machine learning (ML)
models, they remain vulnerable to adversarial and backdoor attacks. This
vulnerability is particularly concerning in real-world deployments, where
compromised models may exhibit unpredictable behavior in critical scenarios.
Such risks are heightened by the prevalent practice of collecting massive,
internet-sourced datasets for pre-training multimodal models, as these datasets
may harbor backdoors. Various techniques have been proposed to mitigate the
effects of backdooring in these models such as CleanCLIP which is the current
state-of-the-art approach. In this work, we demonstrate that the efficacy of
CleanCLIP in mitigating backdoors is highly dependent on the particular
objective used during model pre-training. We observe that stronger pre-training
objectives correlate with harder to remove backdoors behaviors. We show this by
training multimodal models on two large datasets consisting of 3 million (CC3M)
and 6 million (CC6M) datapoints, under various pre-training objectives,
followed by poison removal using CleanCLIP. We find that CleanCLIP is
ineffective when stronger pre-training objectives are used, even with extensive
hyperparameter tuning. Our findings underscore critical considerations for ML
practitioners who pre-train models using large-scale web-curated data and are
concerned about potential backdoor threats. Notably, our results suggest that
simpler pre-training objectives are more amenable to effective backdoor
removal. This insight is pivotal for practitioners seeking to balance the
trade-offs between using stronger pre-training objectives and security against
backdoor attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Verma_S/0/1/0/all/0/1&quot;&gt;Sahil Verma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatt_G/0/1/0/all/0/1&quot;&gt;Gantavya Bhatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwarzschild_A/0/1/0/all/0/1&quot;&gt;Avi Schwarzschild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_S/0/1/0/all/0/1&quot;&gt;Soumye Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_A/0/1/0/all/0/1&quot;&gt;Arnav Mohanty Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_C/0/1/0/all/0/1&quot;&gt;Chirag Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dickerson_J/0/1/0/all/0/1&quot;&gt;John P Dickerson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bilmes_J/0/1/0/all/0/1&quot;&gt;Jeff Bilmes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16173">
<title>Conditions for Length Generalization in Learning Reasoning Skills. (arXiv:2311.16173v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16173</link>
<description rdf:parseType="Literal">&lt;p&gt;Reasoning is a fundamental capability of AI agents. Recently, large language
models (LLMs) have shown remarkable abilities to perform reasoning tasks.
However, numerous evaluations of the reasoning capabilities of LLMs have also
showed some limitations. An outstanding limitation is length generalization,
meaning that when trained on reasoning problems of smaller lengths or sizes,
the resulting models struggle with problems of larger sizes or lengths. This
potentially indicates some theoretical limitations of generalization in
learning reasoning skills. These evaluations and their observations motivated
us to perform a theoretical study of the length generalization problem. This
work focuses on reasoning tasks that can be formulated as Markov dynamic
processes (MDPs) and/or directed acyclic graphs (DAGs). It identifies and
proves conditions that decide whether the length generalization problem can be
solved or not for a reasoning task in a particular representation. Experiments
are also conducted to verify the theoretical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Changnan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bing Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17030">
<title>Is This the Subspace You Are Looking for? An Interpretability Illusion for Subspace Activation Patching. (arXiv:2311.17030v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17030</link>
<description rdf:parseType="Literal">&lt;p&gt;Mechanistic interpretability aims to understand model behaviors in terms of
specific, interpretable features, often hypothesized to manifest as
low-dimensional subspaces of activations. Specifically, recent studies have
explored subspace interventions (such as activation patching) as a way to
simultaneously manipulate model behavior and attribute the features behind it
to given subspaces.
&lt;/p&gt;
&lt;p&gt;In this work, we demonstrate that these two aims diverge, potentially leading
to an illusory sense of interpretability. Counterintuitively, even if a
subspace intervention makes the model&apos;s output behave as if the value of a
feature was changed, this effect may be achieved by activating a dormant
parallel pathway leveraging another subspace that is causally disconnected from
model outputs. We demonstrate this phenomenon in a distilled mathematical
example, in two real-world domains (the indirect object identification task and
factual recall), and present evidence for its prevalence in practice. In the
context of factual recall, we further show a link to rank-1 fact editing,
providing a mechanistic explanation for previous work observing an
inconsistency between fact editing performance and fact localization.
&lt;/p&gt;
&lt;p&gt;However, this does not imply that activation patching of subspaces is
intrinsically unfit for interpretability. To contextualize our findings, we
also show what a success case looks like in a task (indirect object
identification) where prior manual circuit analysis informs an understanding of
the location of a feature. We explore the additional evidence needed to argue
that a patched subspace is faithful.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makelov_A/0/1/0/all/0/1&quot;&gt;Aleksandar Makelov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_G/0/1/0/all/0/1&quot;&gt;Georg Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nanda_N/0/1/0/all/0/1&quot;&gt;Neel Nanda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00886">
<title>Nash Learning from Human Feedback. (arXiv:2312.00886v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00886</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning from human feedback (RLHF) has emerged as the main
paradigm for aligning large language models (LLMs) with human preferences.
Typically, RLHF involves the initial step of learning a reward model from human
feedback, often expressed as preferences between pairs of text generations
produced by a pre-trained LLM. Subsequently, the LLM&apos;s policy is fine-tuned by
optimizing it to maximize the reward model through a reinforcement learning
algorithm. However, an inherent limitation of current reward models is their
inability to fully represent the richness of human preferences and their
dependency on the sampling distribution.
&lt;/p&gt;
&lt;p&gt;In this study, we introduce an alternative pipeline for the fine-tuning of
LLMs using pairwise human feedback. Our approach entails the initial learning
of a preference model, which is conditioned on two inputs given a prompt,
followed by the pursuit of a policy that consistently generates responses
preferred over those generated by any competing policy, thus defining the Nash
equilibrium of this preference model. We term this approach Nash learning from
human feedback (NLHF).
&lt;/p&gt;
&lt;p&gt;In the context of a tabular policy representation, we present a novel
algorithmic solution, Nash-MD, founded on the principles of mirror descent.
This algorithm produces a sequence of policies, with the last iteration
converging to the regularized Nash equilibrium. Additionally, we explore
parametric representations of policies and introduce gradient descent
algorithms for deep-learning architectures. To demonstrate the effectiveness of
our approach, we present experimental results involving the fine-tuning of a
LLM for a text summarization task. We believe NLHF offers a compelling avenue
for preference learning and policy optimization with the potential of advancing
the field of aligning LLMs with human preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Munos_R/0/1/0/all/0/1&quot;&gt;R&amp;#xe9;mi Munos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Valko_M/0/1/0/all/0/1&quot;&gt;Michal Valko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Calandriello_D/0/1/0/all/0/1&quot;&gt;Daniele Calandriello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Azar_M/0/1/0/all/0/1&quot;&gt;Mohammad Gheshlaghi Azar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rowland_M/0/1/0/all/0/1&quot;&gt;Mark Rowland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhaohan Daniel Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Geist_M/0/1/0/all/0/1&quot;&gt;Matthieu Geist&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mesnard_T/0/1/0/all/0/1&quot;&gt;Thomas Mesnard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Michi_A/0/1/0/all/0/1&quot;&gt;Andrea Michi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Selvi_M/0/1/0/all/0/1&quot;&gt;Marco Selvi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Girgin_S/0/1/0/all/0/1&quot;&gt;Sertan Girgin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Momchev_N/0/1/0/all/0/1&quot;&gt;Nikola Momchev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bachem_O/0/1/0/all/0/1&quot;&gt;Olivier Bachem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mankowitz_D/0/1/0/all/0/1&quot;&gt;Daniel J. Mankowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Precup_D/0/1/0/all/0/1&quot;&gt;Doina Precup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Piot_B/0/1/0/all/0/1&quot;&gt;Bilal Piot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01454">
<title>D-Bot: Database Diagnosis System using Large Language Models. (arXiv:2312.01454v2 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01454</link>
<description rdf:parseType="Literal">&lt;p&gt;Database administrators (DBAs) play an important role in managing,
maintaining and optimizing database systems. However, it is hard and tedious
for DBAs to manage a large number of databases and give timely response
(waiting for hours is intolerable in many online cases). In addition, existing
empirical methods only support limited diagnosis scenarios, which are also
labor-intensive to update the diagnosis rules for database version updates.
Recently large language models (LLMs) have shown great potential in various
fields. Thus, we propose D-Bot, an LLM-based database diagnosis system that can
automatically acquire knowledge from diagnosis documents, and generate
reasonable and well-founded diagnosis report (i.e., identifying the root causes
and solutions) within acceptable time (e.g., under 10 minutes compared to hours
by a DBA). The techniques in D-Bot include (i) offline knowledge extraction
from documents, (ii) automatic prompt generation (e.g., knowledge matching,
tool retrieval), (iii) root cause analysis using tree search algorithm, and
(iv) collaborative mechanism for complex anomalies with multiple root causes.
We verify D-Bot on real benchmarks (including 539 anomalies of six typical
applications), and the results show that D-Bot can effectively analyze the root
causes of unseen anomalies and significantly outperforms traditional methods
and vanilla models like GPT-4.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xuanhe Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guoliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zhaoyan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Weize Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jianming Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiesi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_R/0/1/0/all/0/1&quot;&gt;Ruohang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_G/0/1/0/all/0/1&quot;&gt;Guoyang Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02010">
<title>Towards Learning a Generalist Model for Embodied Navigation. (arXiv:2312.02010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02010</link>
<description rdf:parseType="Literal">&lt;p&gt;Building a generalist agent that can interact with the world is the
intriguing target of AI systems, thus spurring the research for embodied
navigation, where an agent is required to navigate according to instructions or
respond to queries. Despite the major progress attained, previous works
primarily focus on task-specific agents and lack generalizability to unseen
scenarios. Recently, LLMs have presented remarkable capabilities across various
fields, and provided a promising opportunity for embodied navigation. Drawing
on this, we propose the first generalist model for embodied navigation,
NaviLLM. It adapts LLMs to embodied navigation by introducing schema-based
instruction. The schema-based instruction flexibly casts various tasks into
generation problems, thereby unifying a wide range of tasks. This approach
allows us to integrate diverse data sources from various datasets into the
training, equipping NaviLLM with a wide range of capabilities required by
embodied navigation. We conduct extensive experiments to evaluate the
performance and generalizability of our model. The experimental results
demonstrate that our unified model achieves state-of-the-art performance on
CVDN, SOON, and ScanQA. Specifically, it surpasses the previous
stats-of-the-art method by a significant margin of 29% in goal progress on
CVDN. Moreover, our model also demonstrates strong generalizability and
presents impressive results on unseen tasks, e.g., embodied question answering
and 3D captioning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_D/0/1/0/all/0/1&quot;&gt;Duo Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shijia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yiwu Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02125">
<title>TPPoet: Transformer-Based Persian Poem Generation using Minimal Data and Advanced Decoding Techniques. (arXiv:2312.02125v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02125</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in language models (LMs), have demonstrated significant
efficacy in tasks related to the arts and humanities. While LMs have exhibited
exceptional performance across a wide range of natural language processing
tasks, there are notable challenges associated with their utilization on small
datasets and their ability to replicate more creative human capacities. In this
study, we aim to address these challenges by training a Persian classical
poetry generation model using a transformer architecture on a specialized
dataset with no pretraining. Additionally, we propose a novel decoding method
to enhance coherence and meaningfulness in the generated poetry, effectively
managing the tradeoff between diversity and quality. Furthermore, the results
of our training approach and the proposed decoding method are evaluated through
comprehensive set of automatic and human evaluations and showed its superior
capability to generate coherent and meaningful poetry in compare to other
decoding methods and an existing Persian large language model (LLM).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panahandeh_A/0/1/0/all/0/1&quot;&gt;Amir Panahandeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asemi_H/0/1/0/all/0/1&quot;&gt;Hanie Asemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourani_E/0/1/0/all/0/1&quot;&gt;Esmaeil Nourani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02439">
<title>Let&apos;s Think Outside the Box: Exploring Leap-of-Thought in Large Language Models with Creative Humor Generation. (arXiv:2312.02439v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02439</link>
<description rdf:parseType="Literal">&lt;p&gt;Chain-of-Thought (CoT) guides large language models (LLMs) to reason
step-by-step, and can motivate their logical reasoning ability. While effective
for logical tasks, CoT is not conducive to creative problem-solving which often
requires out-of-box thoughts and is crucial for innovation advancements. In
this paper, we explore the Leap-of-Thought (LoT) abilities within LLMs -- a
non-sequential, creative paradigm involving strong associations and knowledge
leaps. To this end, we study LLMs on the popular Oogiri game which needs
participants to have good creativity and strong associative thinking for
responding unexpectedly and humorously to the given image, text, or both, and
thus is suitable for LoT study. Then to investigate LLMs&apos; LoT ability in the
Oogiri game, we first build a multimodal and multilingual Oogiri-GO dataset
which contains over 130,000 samples from the Oogiri game, and observe the
insufficient LoT ability or failures of most existing LLMs on the Oogiri game.
Accordingly, we introduce a creative Leap-of-Thought (CLoT) paradigm to improve
LLM&apos;s LoT ability. CLoT first formulates the Oogiri-GO dataset into
LoT-oriented instruction tuning data to train pretrained LLM for achieving
certain LoT humor generation and discrimination abilities. Then CLoT designs an
explorative self-refinement that encourages the LLM to generate more creative
LoT data via exploring parallels between seemingly unrelated concepts and
selects high-quality data to train itself for self-refinement. CLoT not only
excels in humor generation in the Oogiri game but also boosts creative
abilities in various tasks like cloud guessing game and divergent association
task. These findings advance our understanding and offer a pathway to improve
LLMs&apos; creative capacities for innovative applications across domains. The
dataset, code, and models will be released online.
https://zhongshsh.github.io/CLoT/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shanghua Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wushao Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zitnik_M/0/1/0/all/0/1&quot;&gt;Marinka Zitnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pan Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02663">
<title>FaceStudio: Put Your Face Everywhere in Seconds. (arXiv:2312.02663v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02663</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigates identity-preserving image synthesis, an intriguing
task in image generation that seeks to maintain a subject&apos;s identity while
adding a personalized, stylistic touch. Traditional methods, such as Textual
Inversion and DreamBooth, have made strides in custom image creation, but they
come with significant drawbacks. These include the need for extensive resources
and time for fine-tuning, as well as the requirement for multiple reference
images. To overcome these challenges, our research introduces a novel approach
to identity-preserving synthesis, with a particular focus on human images. Our
model leverages a direct feed-forward mechanism, circumventing the need for
intensive fine-tuning, thereby facilitating quick and efficient image
generation. Central to our innovation is a hybrid guidance framework, which
combines stylized images, facial images, and textual prompts to guide the image
generation process. This unique combination enables our model to produce a
variety of applications, such as artistic portraits and identity-blended
images. Our experimental results, including both qualitative and quantitative
evaluations, demonstrate the superiority of our method over existing baseline
models and previous works, particularly in its remarkable efficiency and
ability to preserve the subject&apos;s identity with high fidelity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yichao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Gege Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_P/0/1/0/all/0/1&quot;&gt;Pei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_G/0/1/0/all/0/1&quot;&gt;Gang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_B/0/1/0/all/0/1&quot;&gt;Bin Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02858">
<title>Towards Causal Representations of Climate Model Data. (arXiv:2312.02858v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02858</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate models, such as Earth system models (ESMs), are crucial for
simulating future climate change based on projected Shared Socioeconomic
Pathways (SSP) greenhouse gas emissions scenarios. While ESMs are sophisticated
and invaluable, machine learning-based emulators trained on existing simulation
data can project additional climate scenarios much faster and are
computationally efficient. However, they often lack generalizability and
interpretability. This work delves into the potential of causal representation
learning, specifically the \emph{Causal Discovery with Single-parent Decoding}
(CDSD) method, which could render climate model emulation efficient
\textit{and} interpretable. We evaluate CDSD on multiple climate datasets,
focusing on emissions, temperature, and precipitation. Our findings shed light
on the challenges, limitations, and promise of using CDSD as a stepping stone
towards more interpretable and robust climate model emulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boussard_J/0/1/0/all/0/1&quot;&gt;Julien Boussard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nagda_C/0/1/0/all/0/1&quot;&gt;Chandni Nagda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaltenborn_J/0/1/0/all/0/1&quot;&gt;Julia Kaltenborn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lange_C/0/1/0/all/0/1&quot;&gt;Charlotte Emilie Elektra Lange&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brouillard_P/0/1/0/all/0/1&quot;&gt;Philippe Brouillard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurwicz_Y/0/1/0/all/0/1&quot;&gt;Yaniv Gurwicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowack_P/0/1/0/all/0/1&quot;&gt;Peer Nowack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolnick_D/0/1/0/all/0/1&quot;&gt;David Rolnick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08289">
<title>$\textbf{A}^2\textbf{CiD}^2$: Accelerating Asynchronous Communication in Decentralized Deep Learning. (arXiv:2306.08289v2 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2306.08289</link>
<description rdf:parseType="Literal">&lt;p&gt;Distributed training of Deep Learning models has been critical to many recent
successes in the field. Current standard methods primarily rely on synchronous
centralized algorithms which induce major communication bottlenecks and
synchronization locks at scale. Decentralized asynchronous algorithms are
emerging as a potential alternative but their practical applicability still
lags. In order to mitigate the increase in communication cost that naturally
comes with scaling the number of workers, we introduce a principled
asynchronous, randomized, gossip-based optimization algorithm which works
thanks to a continuous local momentum named $\textbf{A}^2\textbf{CiD}^2$. Our
method allows each worker to continuously process mini-batches without
stopping, and run a peer-to-peer averaging routine in parallel, reducing idle
time. In addition to inducing a significant communication acceleration at no
cost other than adding a local momentum variable, minimal adaptation is
required to incorporate $\textbf{A}^2\textbf{CiD}^2$ to standard asynchronous
approaches. Our theoretical analysis proves accelerated rates compared to
previous asynchronous decentralized baselines and we empirically show that
using our $\textbf{A}^2\textbf{CiD}^2$ momentum significantly decrease
communication costs in poorly connected networks. In particular, we show
consistent improvement on the ImageNet dataset using up to 64 asynchronous
workers (A100 GPUs) and various communication network topologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nabli_A/0/1/0/all/0/1&quot;&gt;Adel Nabli&lt;/a&gt; (MLIA, Mila), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belilovsky_E/0/1/0/all/0/1&quot;&gt;Eugene Belilovsky&lt;/a&gt; (Mila), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oyallon_E/0/1/0/all/0/1&quot;&gt;Edouard Oyallon&lt;/a&gt; (MLIA)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00045">
<title>AI-driven E-Liability Knowledge Graphs: A Comprehensive Framework for Supply Chain Carbon Accounting and Emissions Liability Management. (arXiv:2312.00045v1 [cs.CY] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2312.00045</link>
<description rdf:parseType="Literal">&lt;p&gt;While carbon accounting plays a fundamental role in our fight against climate
change, it is not without its challenges. We begin the paper with a critique of
the conventional carbon accounting practices, after which we proceed to
introduce the E-liability carbon accounting methodology and Emissions Liability
Management (ELM) originally proposed by Kaplan and Ramanna, highlighting their
strengths. Recognizing the immense value of this novel approach for real-world
carbon accounting improvement, we introduce a novel data-driven integrative
framework that leverages AI and computation - the E-Liability Knowledge Graph
framework - to achieve real-world implementation of the E-liability carbon
accounting methodology. In addition to providing a path-to-implementation, our
proposed framework brings clarity to the complex environmental interactions
within supply chains, thus enabling better informed and more responsible
decision-making. We analyze the implementation aspects of this framework and
conclude with a discourse on the role of this AI-aided knowledge graph in
ensuring the transparency and decarbonization of global supply chains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oladeji_O/0/1/0/all/0/1&quot;&gt;Olamide Oladeji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousavi_S/0/1/0/all/0/1&quot;&gt;Seyed Shahabeddin Mousavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roston_M/0/1/0/all/0/1&quot;&gt;Marc Roston&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>