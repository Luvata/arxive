<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-23T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12232" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12236" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12275" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12344" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12369" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12413" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12418" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12440" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12455" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12476" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12478" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12485" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12496" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12522" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12532" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12546" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12576" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12611" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12627" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12644" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12683" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12686" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12806" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12849" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12923" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12926" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12930" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12934" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.01135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.07869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.11334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.11628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.08082" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.13883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2204.13209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.05173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.05587" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.13743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.00775" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02059" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.11492" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.02829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.07805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.01407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.03281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.12970" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13069" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.04378" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02444" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07846" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14391" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.06075" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.14624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00384" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.10487" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11477" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.14190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.10140" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05207" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.15318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18304" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18382" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03131" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10309" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16536" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03311" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.09126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12433" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12937" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15282" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04079" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.08919" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09943" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11202" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11447" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11488" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11687" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.11792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.12890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.06980" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.12196" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.12225">
<title>Multimodal Data Curation via Object Detection and Filter Ensembles. (arXiv:2401.12225v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12225</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an approach for curating multimodal data that we used for our
entry in the 2023 DataComp competition filtering track. Our technique combines
object detection and weak supervision-based ensembling. In the first of two
steps in our approach, we employ an out-of-the-box zero-shot object detection
model to extract granular information and produce a variety of filter designs.
In the second step, we employ weak supervision to ensemble filtering rules.
This approach results in a 4% performance improvement when compared to the
best-performing baseline, producing the top-ranking position in the small scale
track at the time of writing. Furthermore, in the medium scale track, we
achieve a noteworthy 4.2% improvement over the baseline by simply ensembling
existing baselines with weak supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzu-Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Changho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_S/0/1/0/all/0/1&quot;&gt;Sui Jiet Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1&quot;&gt;Dyah Adila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12230">
<title>Computing in the Era of Large Generative Models: From Cloud-Native to AI-Native. (arXiv:2401.12230v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2401.12230</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we investigate the intersection of large generative AI models
and cloud-native computing architectures. Recent large models such as ChatGPT,
while revolutionary in their capabilities, face challenges like escalating
costs and demand for high-end GPUs. Drawing analogies between
large-model-as-a-service (LMaaS) and cloud database-as-a-service (DBaaS), we
describe an AI-native computing paradigm that harnesses the power of both
cloud-native technologies (e.g., multi-tenancy and serverless computing) and
advanced machine learning runtime (e.g., batched LoRA inference). These joint
efforts aim to optimize costs-of-goods-sold (COGS) and improve resource
accessibility. The journey of merging these two domains is just at the
beginning and we hope to stimulate future research and development in this
area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Song Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lequn Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yongjun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_Y/0/1/0/all/0/1&quot;&gt;Yulong Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lentz_M/0/1/0/all/0/1&quot;&gt;Matthew Lentz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Beibin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jialin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Rui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_K/0/1/0/all/0/1&quot;&gt;Kexin Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianguo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yingjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yongji Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huanchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Minjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qizhen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuo_D/0/1/0/all/0/1&quot;&gt;Danyang Zhuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12231">
<title>Disentangled Condensation for Large-scale Graphs. (arXiv:2401.12231v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.12231</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph condensation has emerged as an intriguing technique to provide Graph
Neural Networks for large-scale graphs with a more compact yet informative
small graph to save the expensive costs of large-scale graph learning. Despite
the promising results achieved, previous graph condensation methods often
employ an entangled condensation strategy that involves condensing nodes and
edges simultaneously, leading to substantial GPU memory demands. This entangled
strategy has considerably impeded the scalability of graph condensation,
impairing its capability to condense extremely large-scale graphs and produce
condensed graphs with high fidelity. Therefore, this paper presents
Disentangled Condensation for large-scale graphs, abbreviated as DisCo, to
provide scalable graph condensation for graphs of varying sizes. At the heart
of DisCo are two complementary components, namely node and edge condensation
modules, that realize the condensation of nodes and edges in a disentangled
manner. In the node condensation module, we focus on synthesizing condensed
nodes that exhibit a similar node feature distribution to original nodes using
a pre-trained node classification model while incorporating class centroid
alignment and anchor attachment regularizers. After node condensation, in the
edge condensation module, we preserve the topology structure by transferring
the link prediction model of the original graph to the condensed nodes,
generating the corresponding condensed edges. Based on the disentangled
strategy, the proposed DisCo can successfully scale up to the ogbn-papers100M
graph with over 100 million nodes and 1 billion edges with flexible reduction
rates. Extensive experiments on five common datasets further demonstrate that
the proposed DisCo yields results superior to state-of-the-art counterparts by
a significant margin. The source code is available at
https://github.com/BangHonor/DisCo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Z/0/1/0/all/0/1&quot;&gt;Zhenbang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shunyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tongya Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12232">
<title>Machine Learning Modeling Of SiRNA Structure-Potency Relationship With Applications Against Sars-Cov-2 Spike Gene. (arXiv:2401.12232v1 [q-bio.BM])</title>
<link>http://arxiv.org/abs/2401.12232</link>
<description rdf:parseType="Literal">&lt;p&gt;The pharmaceutical Research and development (R&amp;amp;D) process is lengthy and
costly, taking nearly a decade to bring a new drug to the market. However,
advancements in biotechnology, computational methods, and machine learning
algorithms have the potential to revolutionize drug discovery, speeding up the
process and improving patient outcomes. The COVID-19 pandemic has further
accelerated and deepened the recognition of the potential of these techniques,
especially in the areas of drug repurposing and efficacy predictions.
Meanwhile, non-small molecule therapeutic modalities such as cell therapies,
monoclonal antibodies, and RNA interference (RNAi) technology have gained
importance due to their ability to target specific disease pathways and/or
patient populations. In the field of RNAi, many experiments have been carried
out to design and select highly efficient siRNAs. However, the established
patterns for efficient siRNAs are sometimes contradictory and unable to
consistently determine the most potent siRNA molecules against a target mRNA.
Thus, this paper focuses on developing machine learning models based on the
cheminformatics representation of the nucleotide composition (i.e. AUTGC) of
siRNA to predict their potency and aid the selection of the most efficient
siRNAs for further development. The PLS (Partial Least Square) and SVR (Support
Vector Regression) machine learning models built in this work outperformed
previously published models. These models can help in predicting siRNA potency
and aid in selecting the best siRNA molecules for experimental validation and
further clinical development. The study has demonstrated the potential of
AI/machine learning models to help expedite siRNA-based drug discovery
including the discovery of potent siRNAs against SARS-CoV-2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Oshunyinka_D/0/1/0/all/0/1&quot;&gt;Damilola Oshunyinka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12233">
<title>Memorization in Self-Supervised Learning Improves Downstream Generalization. (arXiv:2401.12233v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12233</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has recently received significant attention
due to its ability to train high-performance encoders purely on unlabeled
data-often scraped from the internet. This data can still be sensitive and
empirical evidence suggests that SSL encoders memorize private information of
their training data and can disclose them at inference time. Since existing
theoretical definitions of memorization from supervised learning rely on
labels, they do not transfer to SSL. To address this gap, we propose SSLMem, a
framework for defining memorization within SSL. Our definition compares the
difference in alignment of representations for data points and their augmented
views returned by both encoders that were trained on these data points and
encoders that were not. Through comprehensive empirical analysis on diverse
encoder architectures and datasets we highlight that even though SSL relies on
large datasets and strong augmentations-both known in supervised learning as
regularization techniques that reduce overfitting-still significant fractions
of training data points experience high memorization. Through our empirical
results, we show that this memorization is essential for encoders to achieve
higher generalization performance on different downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaleem_M/0/1/0/all/0/1&quot;&gt;Muhammad Ahmad Kaleem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dziedzic_A/0/1/0/all/0/1&quot;&gt;Adam Dziedzic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Backes_M/0/1/0/all/0/1&quot;&gt;Michael Backes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1&quot;&gt;Nicolas Papernot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boenisch_F/0/1/0/all/0/1&quot;&gt;Franziska Boenisch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12234">
<title>A Lightweight FPGA-based IDS-ECU Architecture for Automotive CAN. (arXiv:2401.12234v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2401.12234</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen an exponential rise in complex software-driven
functionality in vehicles, leading to a rising number of electronic control
units (ECUs), network capabilities, and interfaces. These expanded capabilities
also bring-in new planes of vulnerabilities making intrusion detection and
management a critical capability; however, this can often result in more ECUs
and network elements due to the high computational overheads. In this paper, we
present a consolidated ECU architecture incorporating an Intrusion Detection
System (IDS) for Automotive Controller Area Network (CAN) along with
traditional ECU functionality on an off-the-shelf hybrid FPGA device, with
near-zero overhead for the ECU functionality. We propose two quantised
multi-layer perceptrons (QMLP&apos;s) as isolated IDSs for detecting a range of
attack vectors including Denial-of-Service, Fuzzing and Spoofing, which are
accelerated using off-the-shelf deep-learning processing unit (DPU) IP block
from Xilinx, operating fully transparently to the software on the ECU. The
proposed models achieve the state-of-the-art classification accuracy for all
the attacks, while we observed a 15x reduction in power consumption when
compared against the GPU-based implementation of the same models quantised
using Nvidia libraries. We also achieved a 2.3x speed up in per-message
processing latency (at 0.24 ms from the arrival of a CAN message) to meet the
strict end-to-end latency on critical CAN nodes and a 2.6x reduction in power
consumption for inference when compared to the state-of-the-art IDS models on
embedded IDS and loosely coupled IDS accelerators (GPUs) discussed in the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_S/0/1/0/all/0/1&quot;&gt;Shashwat Khandelwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanker_S/0/1/0/all/0/1&quot;&gt;Shreejith Shanker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12235">
<title>Stochastic Dynamic Power Dispatch with High Generalization and Few-Shot Adaption via Contextual Meta Graph Reinforcement Learning. (arXiv:2401.12235v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12235</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning is an emerging approaches to facilitate multi-stage
sequential decision-making problems. This paper studies a real-time multi-stage
stochastic power dispatch considering multivariate uncertainties. Current
researches suffer from low generalization and practicality, that is, the
learned dispatch policy can only handle a specific dispatch scenario, its
performance degrades significantly if actual samples and training samples are
inconsistent. To fill these gaps, a novel contextual meta graph reinforcement
learning (Meta-GRL) for a highly generalized multi-stage optimal dispatch
policy is proposed. Specifically, a more general contextual Markov decision
process (MDP) and scalable graph representation are introduced to achieve a
more generalized multi-stage stochastic power dispatch modeling. An upper
meta-learner is proposed to encode context for different dispatch scenarios and
learn how to achieve dispatch task identification while the lower policy
learner learns context-specified dispatch policy. After sufficient offline
learning, this approach can rapidly adapt to unseen and undefined scenarios
with only a few updations of the hypothesis judgments generated by the
meta-learner. Numerical comparisons with state-of-the-art policies and
traditional reinforcement learning verify the optimality, efficiency,
adaptability, and scalability of the proposed Meta-GRL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_B/0/1/0/all/0/1&quot;&gt;Bairong Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhenning Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuehan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yufeng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Q/0/1/0/all/0/1&quot;&gt;Qiaoyi Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12236">
<title>The Surprising Harmfulness of Benign Overfitting for Adversarial Robustness. (arXiv:2401.12236v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12236</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent empirical and theoretical studies have established the generalization
capabilities of large machine learning models that are trained to
(approximately or exactly) fit noisy data. In this work, we prove a surprising
result that even if the ground truth itself is robust to adversarial examples,
and the benignly overfitted model is benign in terms of the ``standard&apos;&apos;
out-of-sample risk objective, this benign overfitting process can be harmful
when out-of-sample data are subject to adversarial manipulation. More
specifically, our main results contain two parts: (i) the min-norm estimator in
overparameterized linear model always leads to adversarial vulnerability in the
``benign overfitting&apos;&apos; setting; (ii) we verify an asymptotic trade-off result
between the standard risk and the ``adversarial&apos;&apos; risk of every ridge
regression estimator, implying that under suitable conditions these two items
cannot both be small at the same time by any single choice of the ridge
regularization parameter. Furthermore, under the lazy training regime, we
demonstrate parallel results on two-layer neural tangent kernel (NTK) model,
which align with empirical observations in deep neural networks. Our finding
provides theoretical insights into the puzzling phenomenon observed in
practice, where the true target function (e.g., human) is robust against
adverasrial attack, while beginly overfitted neural networks lead to models
that are not robust.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yifan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12237">
<title>A distribution-guided Mapper algorithm. (arXiv:2401.12237v1 [math.AT])</title>
<link>http://arxiv.org/abs/2401.12237</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivation: The Mapper algorithm is an essential tool to explore shape of
data in topology data analysis. With a dataset as an input, the Mapper
algorithm outputs a graph representing the topological features of the whole
dataset. This graph is often regarded as an approximation of a reeb graph of
data. The classic Mapper algorithm uses fixed interval lengths and overlapping
ratios, which might fail to reveal subtle features of data, especially when the
underlying structure is complex.
&lt;/p&gt;
&lt;p&gt;Results: In this work, we introduce a distribution guided Mapper algorithm
named D-Mapper, that utilizes the property of the probability model and data
intrinsic characteristics to generate density guided covers and provides
enhanced topological features. Our proposed algorithm is a probabilistic
model-based approach, which could serve as an alternative to non-prababilistic
ones. Moreover, we introduce a metric accounting for both the quality of
overlap clustering and extended persistence homology to measure the performance
of Mapper type algorithm. Our numerical experiments indicate that the D-Mapper
outperforms the classical Mapper algorithm in various scenarios. We also apply
the D-Mapper to a SARS-COV-2 coronavirus RNA sequences dataset to explore the
topological structure of different virus variants. The results indicate that
the D-Mapper algorithm can reveal both vertical and horizontal evolution
processes of the viruses.
&lt;/p&gt;
&lt;p&gt;Availability: Our package is available at
https://github.com/ShufeiGe/D-Mapper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tao_Y/0/1/0/all/0/1&quot;&gt;Yuyang Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ge_S/0/1/0/all/0/1&quot;&gt;Shufei Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12238">
<title>Spatial Scaper: A Library to Simulate and Augment Soundscapes for Sound Event Localization and Detection in Realistic Rooms. (arXiv:2401.12238v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.12238</link>
<description rdf:parseType="Literal">&lt;p&gt;Sound event localization and detection (SELD) is an important task in machine
listening. Major advancements rely on simulated data with sound events in
specific rooms and strong spatio-temporal labels. SELD data is simulated by
convolving spatialy-localized room impulse responses (RIRs) with sound
waveforms to place sound events in a soundscape. However, RIRs require manual
collection in specific rooms. We present SpatialScaper, a library for SELD data
simulation and augmentation. Compared to existing tools, SpatialScaper emulates
virtual rooms via parameters such as size and wall absorption. This allows for
parameterized placement (including movement) of foreground and background sound
sources. SpatialScaper also includes data augmentation pipelines that can be
applied to existing SELD data. As a case study, we use SpatialScaper to add
rooms to the DCASE SELD data. Training a model with our data led to progressive
performance improves as a direct function of acoustic diversity. These results
show that SpatialScaper is valuable to train robust SELD models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roman_I/0/1/0/all/0/1&quot;&gt;Iran R. Roman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ick_C/0/1/0/all/0/1&quot;&gt;Christopher Ick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Sivan Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roman_A/0/1/0/all/0/1&quot;&gt;Adrian S. Roman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+McFee_B/0/1/0/all/0/1&quot;&gt;Brian McFee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bello_J/0/1/0/all/0/1&quot;&gt;Juan P. Bello&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12240">
<title>Quantised Neural Network Accelerators for Low-Power IDS in Automotive Networks. (arXiv:2401.12240v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.12240</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore low-power custom quantised Multi-Layer Perceptrons
(MLPs) as an Intrusion Detection System (IDS) for automotive controller area
network (CAN). We utilise the FINN framework from AMD/Xilinx to quantise, train
and generate hardware IP of our MLP to detect denial of service (DoS) and
fuzzying attacks on CAN network, using ZCU104 (XCZU7EV) FPGA as our target ECU
architecture with integrated IDS capabilities. Our approach achieves
significant improvements in latency (0.12 ms per-message processing latency)
and inference energy consumption (0.25 mJ per inference) while achieving
similar classification performance as state-of-the-art approaches in the
literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khandelwal_S/0/1/0/all/0/1&quot;&gt;Shashwat Khandelwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walsh_A/0/1/0/all/0/1&quot;&gt;Anneliese Walsh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shreejith_S/0/1/0/all/0/1&quot;&gt;Shanker Shreejith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12242">
<title>BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models. (arXiv:2401.12242v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.12242</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) are shown to benefit from chain-of-thought (COT)
prompting, particularly when tackling tasks that require systematic reasoning
processes. On the other hand, COT prompting also poses new vulnerabilities in
the form of backdoor attacks, wherein the model will output unintended
malicious content under specific backdoor-triggered conditions during
inference. Traditional methods for launching backdoor attacks involve either
contaminating the training dataset with backdoored instances or directly
manipulating the model parameters during deployment. However, these approaches
are not practical for commercial LLMs that typically operate via API access. In
this paper, we propose BadChain, the first backdoor attack against LLMs
employing COT prompting, which does not require access to the training dataset
or model parameters and imposes low computational overhead. BadChain leverages
the inherent reasoning capabilities of LLMs by inserting a backdoor reasoning
step into the sequence of reasoning steps of the model output, thereby altering
the final response when a backdoor trigger exists in the query prompt.
Empirically, we show the effectiveness of BadChain for two COT strategies
across four LLMs (Llama2, GPT-3.5, PaLM2, and GPT-4) and six complex benchmark
tasks encompassing arithmetic, commonsense, and symbolic reasoning. Moreover,
we show that LLMs endowed with stronger reasoning capabilities exhibit higher
susceptibility to BadChain, exemplified by a high average attack success rate
of 97.0% across the six benchmark tasks on GPT-4. Finally, we propose two
defenses based on shuffling and demonstrate their overall ineffectiveness
against BadChain. Therefore, BadChain remains a severe threat to LLMs,
underscoring the urgency for the development of robust and effective future
defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_Z/0/1/0/all/0/1&quot;&gt;Zhen Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_F/0/1/0/all/0/1&quot;&gt;Fengqing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zidi Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramasubramanian_B/0/1/0/all/0/1&quot;&gt;Bhaskar Ramasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poovendran_R/0/1/0/all/0/1&quot;&gt;Radha Poovendran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12243">
<title>Constraint-Generation Policy Optimization (CGPO): Nonlinear Programming for Policy Optimization in Mixed Discrete-Continuous MDPs. (arXiv:2401.12243v1 [math.OC])</title>
<link>http://arxiv.org/abs/2401.12243</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Constraint-Generation Policy Optimization (CGPO) for optimizing
policy parameters within compact and interpretable policy classes for mixed
discrete-continuous Markov Decision Processes (DC-MDPs). CGPO is not only able
to provide bounded policy error guarantees over an infinite range of initial
states for many DC-MDPs with expressive nonlinear dynamics, but it can also
provably derive optimal policies in cases where it terminates with zero error.
Furthermore, CGPO can generate worst-case state trajectories to diagnose policy
deficiencies and provide counterfactual explanations of optimal actions. To
achieve such results, CGPO proposes a bi-level mixed-integer nonlinear
optimization framework for optimizing policies within defined expressivity
classes (i.e. piecewise (non)-linear) and reduces it to an optimal constraint
generation methodology that adversarially generates worst-case state
trajectories. Furthermore, leveraging modern nonlinear optimizers, CGPO can
obtain solutions with bounded optimality gap guarantees. We handle stochastic
transitions through explicit marginalization (where applicable) or
chance-constraints, providing high-probability policy performance guarantees.
We also present a road-map for understanding the computational complexities
associated with different expressivity classes of policy, reward, and
transition dynamics. We experimentally demonstrate the applicability of CGPO in
diverse domains, including inventory control, management of a system of water
reservoirs, and physics control. In summary, we provide a solution for deriving
structured, compact, and explainable policies with bounded performance
guarantees, enabling worst-case scenario generation and counterfactual policy
diagnostics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gimelfarb_M/0/1/0/all/0/1&quot;&gt;Michael Gimelfarb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Taitler_A/0/1/0/all/0/1&quot;&gt;Ayal Taitler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sanner_S/0/1/0/all/0/1&quot;&gt;Scott Sanner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12244">
<title>Large-scale Reinforcement Learning for Diffusion Models. (arXiv:2401.12244v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12244</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image diffusion models are a class of deep generative models that
have demonstrated an impressive capacity for high-quality image generation.
However, these models are susceptible to implicit biases that arise from
web-scale text-image training pairs and may inaccurately model aspects of
images we care about. This can result in suboptimal samples, model bias, and
images that do not align with human ethics and preferences. In this paper, we
present an effective scalable algorithm to improve diffusion models using
Reinforcement Learning (RL) across a diverse set of reward functions, such as
human preference, compositionality, and fairness over millions of images. We
illustrate how our approach substantially outperforms existing methods for
aligning diffusion models with human preferences. We further illustrate how
this substantially improves pretrained Stable Diffusion (SD) models, generating
samples that are preferred by humans 80.3% of the time over those from the base
SD model while simultaneously improving both the composition and diversity of
generated samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzeng_E/0/1/0/all/0/1&quot;&gt;Eric Tzeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yilun Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kislyuk_D/0/1/0/all/0/1&quot;&gt;Dmitry Kislyuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12246">
<title>Orion-14B: Open-source Multilingual Large Language Models. (arXiv:2401.12246v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12246</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we introduce Orion-14B, a collection of multilingual large
language models with 14 billion parameters. We utilize a data scheduling
approach to train a foundational model on a diverse corpus of 2.5 trillion
tokens, sourced from texts in English, Chinese, Japanese, Korean, and other
languages. Additionally, we fine-tuned a series of models tailored for
conversational applications and other specific use cases. Our evaluation
results demonstrate that Orion-14B achieves state-of-the-art performance across
a broad spectrum of tasks. We make the Orion-14B model family and its
associated code publicly accessible https://github.com/OrionStarAI/Orion,
aiming to inspire future research and practical applications in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Du Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaopu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Haihui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Leichao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dacheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Kun Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12251">
<title>Diffusion Representation for Asymmetric Kernels. (arXiv:2401.12251v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12251</link>
<description rdf:parseType="Literal">&lt;p&gt;We extend the diffusion-map formalism to data sets that are induced by
asymmetric kernels. Analytical convergence results of the resulting expansion
are proved, and an algorithm is proposed to perform the dimensional reduction.
In this work we study data sets in which its geometry structure is induced by
an asymmetric kernel. We use a priori coordinate system to represent this
geometry and, thus, be able to improve the computational complexity of reducing
the dimensionality of data sets. A coordinate system connected to the tensor
product of Fourier basis is used to represent the underlying geometric
structure obtained by the diffusion-map, thus reducing the dimensionality of
the data set and making use of the speedup provided by the two-dimensional Fast
Fourier Transform algorithm (2-D FFT). We compare our results with those
obtained by other eigenvalue expansions, and verify the efficiency of the
algorithms with synthetic data, as well as with real data from applications
including climate change studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_A/0/1/0/all/0/1&quot;&gt;Alvaro Almeida Gomez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neto_A/0/1/0/all/0/1&quot;&gt;Antonio Silva Neto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+zubelli_J/0/1/0/all/0/1&quot;&gt;Jorge zubelli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12253">
<title>Accelerating Sinkhorn Algorithm with Sparse Newton Iterations. (arXiv:2401.12253v1 [math.OC])</title>
<link>http://arxiv.org/abs/2401.12253</link>
<description rdf:parseType="Literal">&lt;p&gt;Computing the optimal transport distance between statistical distributions is
a fundamental task in machine learning. One remarkable recent advancement is
entropic regularization and the Sinkhorn algorithm, which utilizes only matrix
scaling and guarantees an approximated solution with near-linear runtime.
Despite the success of the Sinkhorn algorithm, its runtime may still be slow
due to the potentially large number of iterations needed for convergence. To
achieve possibly super-exponential convergence, we present
Sinkhorn-Newton-Sparse (SNS), an extension to the Sinkhorn algorithm, by
introducing early stopping for the matrix scaling steps and a second stage
featuring a Newton-type subroutine. Adopting the variational viewpoint that the
Sinkhorn algorithm maximizes a concave Lyapunov potential, we offer the insight
that the Hessian matrix of the potential function is approximately sparse.
Sparsification of the Hessian results in a fast $O(n^2)$ per-iteration
complexity, the same as the Sinkhorn algorithm. In terms of total iteration
count, we observe that the SNS algorithm converges orders of magnitude faster
across a wide range of practical cases, including optimal transportation
between empirical distributions and calculating the Wasserstein $W_1, W_2$
distance of discretized densities. The empirical performance is corroborated by
a rigorous bound on the approximate sparsity of the Hessian matrix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shavlovsky_M/0/1/0/all/0/1&quot;&gt;Michael Shavlovsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Rahmanian_H/0/1/0/all/0/1&quot;&gt;Holakou Rahmanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tardini_E/0/1/0/all/0/1&quot;&gt;Elisa Tardini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Thekumparampil_K/0/1/0/all/0/1&quot;&gt;Kiran Koshy Thekumparampil&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tesi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ying_L/0/1/0/all/0/1&quot;&gt;Lexing Ying&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12254">
<title>Transfer learning-assisted inverse modeling in nanophotonics based on mixture density networks. (arXiv:2401.12254v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12254</link>
<description rdf:parseType="Literal">&lt;p&gt;The simulation of nanophotonic structures relies on electromagnetic solvers,
which play a crucial role in understanding their behavior. However, these
solvers often come with a significant computational cost, making their
application in design tasks, such as optimization, impractical. To address this
challenge, machine learning techniques have been explored for accurate and
efficient modeling and design of photonic devices. Deep neural networks, in
particular, have gained considerable attention in this field. They can be used
to create both forward and inverse models. An inverse modeling approach avoids
the need for coupling a forward model with an optimizer and directly performs
the prediction of the optimal design parameters values.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose an inverse modeling method for nanophotonic
structures, based on a mixture density network model enhanced by transfer
learning. Mixture density networks can predict multiple possible solutions at a
time including their respective importance as Gaussian distributions. However,
multiple challenges exist for mixture density network models. An important
challenge is that an upper bound on the number of possible simultaneous
solutions needs to be specified in advance. Also, another challenge is that the
model parameters must be jointly optimized, which can result computationally
expensive. Moreover, optimizing all parameters simultaneously can be
numerically unstable and can lead to degenerate predictions. The proposed
approach allows overcoming these limitations using transfer learning-based
techniques, while preserving a high accuracy in the prediction capability of
the design solutions given an optical response as an input. A dimensionality
reduction step is also explored. Numerical results validate the proposed
method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_L/0/1/0/all/0/1&quot;&gt;Liang Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_P/0/1/0/all/0/1&quot;&gt;Prashant Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferranti_F/0/1/0/all/0/1&quot;&gt;Francesco Ferranti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12258">
<title>Emergent Dominance Hierarchies in Reinforcement Learning Agents. (arXiv:2401.12258v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.12258</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern Reinforcement Learning (RL) algorithms are able to outperform humans
in a wide variety of tasks. Multi-agent reinforcement learning (MARL) settings
present additional challenges, and successful cooperation in mixed-motive
groups of agents depends on a delicate balancing act between individual and
group objectives. Social conventions and norms, often inspired by human
institutions, are used as tools for striking this balance.
&lt;/p&gt;
&lt;p&gt;In this paper, we examine a fundamental, well-studied social convention that
underlies cooperation in both animal and human societies: Dominance
hierarchies.
&lt;/p&gt;
&lt;p&gt;We adapt the ethological theory of dominance hierarchies to artificial
agents, borrowing the established terminology and definitions with as few
amendments as possible. We demonstrate that populations of RL agents, operating
without explicit programming or intrinsic rewards, can invent, learn, enforce,
and transmit a dominance hierarchy to new populations. The dominance
hierarchies that emerge have a similar structure to those studied in chickens,
mice, fish, and other species.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rachum_R/0/1/0/all/0/1&quot;&gt;Ram Rachum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakar_Y/0/1/0/all/0/1&quot;&gt;Yonatan Nakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomlinson_B/0/1/0/all/0/1&quot;&gt;Bill Tomlinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alon_N/0/1/0/all/0/1&quot;&gt;Nitay Alon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirsky_R/0/1/0/all/0/1&quot;&gt;Reuth Mirsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12262">
<title>Machine learning-based network intrusion detection for big and imbalanced data using oversampling, stacking feature embedding and feature extraction. (arXiv:2401.12262v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2401.12262</link>
<description rdf:parseType="Literal">&lt;p&gt;Cybersecurity has emerged as a critical global concern. Intrusion Detection
Systems (IDS) play a critical role in protecting interconnected networks by
detecting malicious actors and activities. Machine Learning (ML)-based behavior
analysis within the IDS has considerable potential for detecting dynamic cyber
threats, identifying abnormalities, and identifying malicious conduct within
the network. However, as the number of data grows, dimension reduction becomes
an increasingly difficult task when training ML models. Addressing this, our
paper introduces a novel ML-based network intrusion detection model that uses
Random Oversampling (RO) to address data imbalance and Stacking Feature
Embedding based on clustering results, as well as Principal Component Analysis
(PCA) for dimension reduction and is specifically designed for large and
imbalanced datasets. This model&apos;s performance is carefully evaluated using
three cutting-edge benchmark datasets: UNSW-NB15, CIC-IDS-2017, and
CIC-IDS-2018. On the UNSW-NB15 dataset, our trials show that the RF and ET
models achieve accuracy rates of 99.59% and 99.95%, respectively. Furthermore,
using the CIC-IDS2017 dataset, DT, RF, and ET models reach 99.99% accuracy,
while DT and RF models obtain 99.94% accuracy on CIC-IDS2018. These performance
results continuously outperform the state-of-art, indicating significant
progress in the field of network intrusion detection. This achievement
demonstrates the efficacy of the suggested methodology, which can be used
practically to accurately monitor and identify network traffic intrusions,
thereby blocking possible threats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Talukder_M/0/1/0/all/0/1&quot;&gt;Md. Alamin Talukder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Manowarul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uddin_M/0/1/0/all/0/1&quot;&gt;Md Ashraf Uddin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_K/0/1/0/all/0/1&quot;&gt;Khondokar Fida Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharmin_S/0/1/0/all/0/1&quot;&gt;Selina Sharmin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alyami_S/0/1/0/all/0/1&quot;&gt;Salem A. Alyami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moni_M/0/1/0/all/0/1&quot;&gt;Mohammad Ali Moni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12272">
<title>Transfer Learning for Nonparametric Regression: Non-asymptotic Minimax Analysis and Adaptive Procedure. (arXiv:2401.12272v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.12272</link>
<description rdf:parseType="Literal">&lt;p&gt;Transfer learning for nonparametric regression is considered. We first study
the non-asymptotic minimax risk for this problem and develop a novel estimator
called the confidence thresholding estimator, which is shown to achieve the
minimax optimal risk up to a logarithmic factor. Our results demonstrate two
unique phenomena in transfer learning: auto-smoothing and super-acceleration,
which differentiate it from nonparametric regression in a traditional setting.
We then propose a data-driven algorithm that adaptively achieves the minimax
risk up to a logarithmic factor across a wide range of parameter spaces.
Simulation studies are conducted to evaluate the numerical performance of the
adaptive transfer learning algorithm, and a real-world example is provided to
demonstrate the benefits of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cai_T/0/1/0/all/0/1&quot;&gt;T. Tony Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pu_H/0/1/0/all/0/1&quot;&gt;Hongming Pu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12275">
<title>Multi-Agent Dynamic Relational Reasoning for Social Robot Navigation. (arXiv:2401.12275v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12275</link>
<description rdf:parseType="Literal">&lt;p&gt;Social robot navigation can be helpful in various contexts of daily life but
requires safe human-robot interactions and efficient trajectory planning. While
modeling pairwise relations has been widely studied in multi-agent interacting
systems, the ability to capture larger-scale group-wise activities is limited.
In this paper, we propose a systematic relational reasoning approach with
explicit inference of the underlying dynamically evolving relational
structures, and we demonstrate its effectiveness for multi-agent trajectory
prediction and social robot navigation. In addition to the edges between pairs
of nodes (i.e., agents), we propose to infer hyperedges that adaptively connect
multiple nodes to enable group-wise reasoning in an unsupervised manner. Our
approach infers dynamically evolving relation graphs and hypergraphs to capture
the evolution of relations, which the trajectory predictor employs to generate
future states. Meanwhile, we propose to regularize the sharpness and sparsity
of the learned relations and the smoothness of the relation evolution, which
proves to enhance training stability and model performance. The proposed
approach is validated on synthetic crowd simulations and real-world benchmark
datasets. Experiments demonstrate that the approach infers reasonable relations
and achieves state-of-the-art prediction performance. In addition, we present a
deep reinforcement learning (DRL) framework for social robot navigation, which
incorporates relational reasoning and trajectory prediction systematically. In
a group-based crowd simulation, our method outperforms the strongest baseline
by a significant margin in terms of safety, efficiency, and social compliance
in dense, interactive scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_C/0/1/0/all/0/1&quot;&gt;Chuanbo Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_H/0/1/0/all/0/1&quot;&gt;Hengbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jinkyoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dax_V/0/1/0/all/0/1&quot;&gt;Victoria Dax&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel J. Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12332">
<title>A Precise Characterization of SGD Stability Using Loss Surface Geometry. (arXiv:2401.12332v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12332</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic Gradient Descent (SGD) stands as a cornerstone optimization
algorithm with proven real-world empirical successes but relatively limited
theoretical understanding. Recent research has illuminated a key factor
contributing to its practical efficacy: the implicit regularization it
instigates. Several studies have investigated the linear stability property of
SGD in the vicinity of a stationary point as a predictive proxy for sharpness
and generalization error in overparameterized neural networks (Wu et al., 2022;
Jastrzebski et al., 2019; Cohen et al., 2021). In this paper, we delve deeper
into the relationship between linear stability and sharpness. More
specifically, we meticulously delineate the necessary and sufficient conditions
for linear stability, contingent on hyperparameters of SGD and the sharpness at
the optimum. Towards this end, we introduce a novel coherence measure of the
loss Hessian that encapsulates pertinent geometric properties of the loss
function that are relevant to the linear stability of SGD. It enables us to
provide a simplified sufficient condition for identifying linear instability at
an optimum. Notably, compared to previous works, our analysis relies on
significantly milder assumptions and is applicable for a broader class of loss
functions than known before, encompassing not only mean-squared error but also
cross-entropy loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dexter_G/0/1/0/all/0/1&quot;&gt;Gregory Dexter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ocejo_B/0/1/0/all/0/1&quot;&gt;Borja Ocejo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keerthi_S/0/1/0/all/0/1&quot;&gt;Sathiya Keerthi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Aman Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Acharya_A/0/1/0/all/0/1&quot;&gt;Ayan Acharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanna_R/0/1/0/all/0/1&quot;&gt;Rajiv Khanna&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12340">
<title>Contrastive Learning and Cycle Consistency-based Transductive Transfer Learning for Target Annotation. (arXiv:2401.12340v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12340</link>
<description rdf:parseType="Literal">&lt;p&gt;Annotating automatic target recognition (ATR) is a highly challenging task,
primarily due to the unavailability of labeled data in the target domain.
Hence, it is essential to construct an optimal target domain classifier by
utilizing the labeled information of the source domain images. The transductive
transfer learning (TTL) method that incorporates a CycleGAN-based unpaired
domain translation network has been previously proposed in the literature for
effective ATR annotation. Although this method demonstrates great potential for
ATR, it severely suffers from lower annotation performance, higher Fr\&apos;echet
Inception Distance (FID) score, and the presence of visual artifacts in the
synthetic images. To address these issues, we propose a hybrid contrastive
learning base unpaired domain translation (H-CUT) network that achieves a
significantly lower FID score. It incorporates both attention and entropy to
emphasize the domain-specific region, a noisy feature mixup module to generate
high variational synthetic negative patches, and a modulated noise contrastive
estimation (MoNCE) loss to reweight all negative patches using optimal
transport for better performance. Our proposed contrastive learning and
cycle-consistency-based TTL (C3TTL) framework consists of two H-CUT networks
and two classifiers. It simultaneously optimizes cycle-consistency, MoNCE, and
identity losses. In C3TTL, two H-CUT networks have been employed through a
bijection mapping to feed the reconstructed source domain images into a
pretrained classifier to guide the optimal target domain classifier. Extensive
experimental analysis conducted on three ATR datasets demonstrates that the
proposed C3TTL method is effective in annotating civilian and military
vehicles, as well as ship targets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sami_S/0/1/0/all/0/1&quot;&gt;Shoaib Meraj Sami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_M/0/1/0/all/0/1&quot;&gt;Md Mahedi Hasan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasrabadi_N/0/1/0/all/0/1&quot;&gt;Nasser M. Nasrabadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_R/0/1/0/all/0/1&quot;&gt;Raghuveer Rao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12344">
<title>OCT-SelfNet: A Self-Supervised Framework with Multi-Modal Datasets for Generalized and Robust Retinal Disease Detection. (arXiv:2401.12344v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12344</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the revolutionary impact of AI and the development of locally trained
algorithms, achieving widespread generalized learning from multi-modal data in
medical AI remains a significant challenge. This gap hinders the practical
deployment of scalable medical AI solutions. Addressing this challenge, our
research contributes a self-supervised robust machine learning framework,
OCT-SelfNet, for detecting eye diseases using optical coherence tomography
(OCT) images. In this work, various data sets from various institutions are
combined enabling a more comprehensive range of representation. Our method
addresses the issue using a two-phase training approach that combines
self-supervised pretraining and supervised fine-tuning with a mask autoencoder
based on the SwinV2 backbone by providing a solution for real-world clinical
deployment. Extensive experiments on three datasets with different encoder
backbones, low data settings, unseen data settings, and the effect of
augmentation show that our method outperforms the baseline model, Resnet-50 by
consistently attaining AUC-ROC performance surpassing 77% across all tests,
whereas the baseline model exceeds 54%. Moreover, in terms of the AUC-PR
metric, our proposed method exceeded 42%, showcasing a substantial increase of
at least 10% in performance compared to the baseline, which exceeded only 33%.
This contributes to our understanding of our approach&apos;s potential and
emphasizes its usefulness in clinical settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jannat_F/0/1/0/all/0/1&quot;&gt;Fatema-E Jannat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholami_S/0/1/0/all/0/1&quot;&gt;Sina Gholami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Minhaj Nur Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabkhi_H/0/1/0/all/0/1&quot;&gt;Hamed Tabkhi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12350">
<title>Scaling Up Quantization-Aware Neural Architecture Search for Efficient Deep Learning on the Edge. (arXiv:2401.12350v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12350</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Architecture Search (NAS) has become the de-facto approach for
designing accurate and efficient networks for edge devices. Since models are
typically quantized for edge deployment, recent work has investigated
quantization-aware NAS (QA-NAS) to search for highly accurate and efficient
quantized models. However, existing QA-NAS approaches, particularly few-bit
mixed-precision (FB-MP) methods, do not scale to larger tasks. Consequently,
QA-NAS has mostly been limited to low-scale tasks and tiny networks. In this
work, we present an approach to enable QA-NAS (INT8 and FB-MP) on large-scale
tasks by leveraging the block-wise formulation introduced by block-wise NAS. We
demonstrate strong results for the semantic segmentation task on the Cityscapes
dataset, finding FB-MP models 33% smaller and INT8 models 17.6% faster than
DeepLabV3 (INT8) without compromising task performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_H/0/1/0/all/0/1&quot;&gt;Hiram Rayo Torres Rodriguez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogel_S/0/1/0/all/0/1&quot;&gt;Sebastian Vogel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waterlaat_N/0/1/0/all/0/1&quot;&gt;Nick van de Waterlaat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jancura_P/0/1/0/all/0/1&quot;&gt;Pavol Jancura&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12356">
<title>Efficient Collaborations through Weight-Driven Coalition Dynamics in Federated Learning Systems. (arXiv:2401.12356v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12356</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of the Internet of Things (IoT), decentralized paradigms for
machine learning are gaining prominence. In this paper, we introduce a
federated learning model that capitalizes on the Euclidean distance between
device model weights to assess their similarity and disparity. This is
foundational for our system, directing the formation of coalitions among
devices based on the closeness of their model weights. Furthermore, the concept
of a barycenter, representing the average of model weights, helps in the
aggregation of updates from multiple devices. We evaluate our approach using
homogeneous and heterogeneous data distribution, comparing it against
traditional federated learning averaging algorithm. Numerical results
demonstrate its potential in offering structured, outperformed and
communication-efficient model for IoT-based machine learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanjri_M/0/1/0/all/0/1&quot;&gt;Mohammed El Hanjri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reguieg_H/0/1/0/all/0/1&quot;&gt;Hamza Reguieg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Attiaoui_A/0/1/0/all/0/1&quot;&gt;Adil Attiaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abouaomar_A/0/1/0/all/0/1&quot;&gt;Amine Abouaomar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobbane_A/0/1/0/all/0/1&quot;&gt;Abdellatif Kobbane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamili_M/0/1/0/all/0/1&quot;&gt;Mohamed El Kamili&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12362">
<title>VC dimension of Graph Neural Networks with Pfaffian activation functions. (arXiv:2401.12362v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.12362</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have emerged in recent years as a powerful tool
to learn tasks across a wide range of graph domains in a data-driven fashion;
based on a message passing mechanism, GNNs have gained increasing popularity
due to their intuitive formulation, closely linked with the Weisfeiler-Lehman
(WL) test for graph isomorphism, to which they have proven equivalent. From a
theoretical point of view, GNNs have been shown to be universal approximators,
and their generalization capability (namely, bounds on the Vapnik Chervonekis
(VC) dimension) has recently been investigated for GNNs with piecewise
polynomial activation functions. The aim of our work is to extend this analysis
on the VC dimension of GNNs to other commonly used activation functions, such
as sigmoid and hyperbolic tangent, using the framework of Pfaffian function
theory. Bounds are provided with respect to architecture parameters (depth,
number of neurons, input size) as well as with respect to the number of colors
resulting from the 1-WL test applied on the graph domain. The theoretical
analysis is supported by a preliminary experimental study.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+DInverno_G/0/1/0/all/0/1&quot;&gt;Giuseppe Alessio D&amp;#x27;Inverno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bianchini_M/0/1/0/all/0/1&quot;&gt;Monica Bianchini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scarselli_F/0/1/0/all/0/1&quot;&gt;Franco Scarselli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12369">
<title>SubgroupTE: Advancing Treatment Effect Estimation with Subgroup Identification. (arXiv:2401.12369v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12369</link>
<description rdf:parseType="Literal">&lt;p&gt;Precise estimation of treatment effects is crucial for evaluating
intervention effectiveness. While deep learning models have exhibited promising
performance in learning counterfactual representations for treatment effect
estimation (TEE), a major limitation in most of these models is that they treat
the entire population as a homogeneous group, overlooking the diversity of
treatment effects across potential subgroups that have varying treatment
effects. This limitation restricts the ability to precisely estimate treatment
effects and provide subgroup-specific treatment recommendations. In this paper,
we propose a novel treatment effect estimation model, named SubgroupTE, which
incorporates subgroup identification in TEE. SubgroupTE identifies
heterogeneous subgroups with different treatment responses and more precisely
estimates treatment effects by considering subgroup-specific causal effects. In
addition, SubgroupTE iteratively optimizes subgrouping and treatment effect
estimation networks to enhance both estimation and subgroup identification.
Comprehensive experiments on the synthetic and semi-synthetic datasets exhibit
the outstanding performance of SubgroupTE compared with the state-of-the-art
models on treatment effect estimation. Additionally, a real-world study
demonstrates the capabilities of SubgroupTE in enhancing personalized treatment
recommendations for patients with opioid use disorder (OUD) by advancing
treatment effect estimation with subgroup identification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seungyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruoqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_W/0/1/0/all/0/1&quot;&gt;Wenyu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Lang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Ping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12382">
<title>Longitudinal Sentiment Classification of Reddit Posts. (arXiv:2401.12382v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12382</link>
<description rdf:parseType="Literal">&lt;p&gt;We report results of a longitudinal sentiment classification of Reddit posts
written by students of four major Canadian universities. We work with the texts
of the posts, concentrating on the years 2020-2023. By finely tuning a
sentiment threshold to a range of [-0.075,0.075], we successfully built
classifiers proficient in categorizing post sentiments into positive and
negative categories. Noticeably, our sentiment classification results are
consistent across the four university data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nwaoha_F/0/1/0/all/0/1&quot;&gt;Fabian Nwaoha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaffar_Z/0/1/0/all/0/1&quot;&gt;Ziyad Gaffar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_H/0/1/0/all/0/1&quot;&gt;Ho Joon Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sokolova_M/0/1/0/all/0/1&quot;&gt;Marina Sokolova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12406">
<title>Enhancing In-context Learning via Linear Probe Calibration. (arXiv:2401.12406v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12406</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) is a new paradigm for natural language processing
that utilizes Generative Pre-trained Transformer (GPT)-like models. This
approach uses prompts that include in-context demonstrations to generate the
corresponding output for a new query input. However, applying ICL in real cases
does not scale with the number of samples, and lacks robustness to different
prompt templates and demonstration permutations. In this paper, we first show
that GPT-like models using ICL result in unreliable predictions based on a new
metric based on Shannon entropy. Then, to solve this problem, we propose a new
technique called the Linear Probe Calibration (LinC), a method that calibrates
the model&apos;s output probabilities, resulting in reliable predictions and
improved performance, while requiring only minimal additional samples (as few
as five labeled data samples). LinC significantly enhances the ICL test
performance of GPT models on various benchmark datasets, with an average
improvement of up to 21%, and up to a 50% improvement in some cases, and
significantly boosts the performance of PEFT methods, especially in the low
resource regime. Moreover, LinC achieves lower expected calibration error, and
is highly robust to varying label proportions, prompt templates, and
demonstration permutations. Our code is available at
\url{https://github.com/mominabbass/LinC}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbas_M/0/1/0/all/0/1&quot;&gt;Momin Abbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ram_P/0/1/0/all/0/1&quot;&gt;Parikshit Ram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1&quot;&gt;Nathalie Baracaldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Samulowitz_H/0/1/0/all/0/1&quot;&gt;Horst Samulowitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salonidis_T/0/1/0/all/0/1&quot;&gt;Theodoros Salonidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12413">
<title>How Far Can 100 Samples Go? Unlocking Overall Zero-Shot Multilingual Translation via Tiny Multi-Parallel Data. (arXiv:2401.12413v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12413</link>
<description rdf:parseType="Literal">&lt;p&gt;Zero-shot translation is an open problem, aiming to translate between
language pairs unseen during training in Multilingual Machine Translation
(MMT). A common, albeit resource-consuming, solution is to mine as many
translation directions as possible to add to the parallel corpus. In this
paper, we show that the zero-shot capability of an English-centric model can be
easily enhanced by fine-tuning with a very small amount of multi-parallel data.
For example, on the EC30 dataset, we show that up to +21.7 ChrF non-English
overall improvements (870 directions) can be achieved by using only 100
multi-parallel samples, meanwhile preserving capability in English-centric
directions. We further study the size effect of fine-tuning data and its
transfer capabilities. Surprisingly, our empirical analysis shows that
comparable overall improvements can be achieved even through fine-tuning in a
small, randomly sampled direction set (10\%). Also, the resulting non-English
performance is quite close to the upper bound (complete translation). Due to
its high efficiency and practicality, we encourage the community 1) to consider
the use of the fine-tuning method as a strong baseline for zero-shot
translation and 2) to construct more comprehensive and high-quality
multi-parallel data to cover real-world demand.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_S/0/1/0/all/0/1&quot;&gt;Shaomu Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stap_D/0/1/0/all/0/1&quot;&gt;David Stap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monz_C/0/1/0/all/0/1&quot;&gt;Christof Monz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12416">
<title>Enhancing Reliability of Neural Networks at the Edge: Inverted Normalization with Stochastic Affine Transformations. (arXiv:2401.12416v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12416</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian Neural Networks (BayNNs) naturally provide uncertainty in their
predictions, making them a suitable choice in safety-critical applications.
Additionally, their realization using memristor-based in-memory computing (IMC)
architectures enables them for resource-constrained edge applications. In
addition to predictive uncertainty, however, the ability to be inherently
robust to noise in computation is also essential to ensure functional safety.
In particular, memristor-based IMCs are susceptible to various sources of
non-idealities such as manufacturing and runtime variations, drift, and
failure, which can significantly reduce inference accuracy. In this paper, we
propose a method to inherently enhance the robustness and inference accuracy of
BayNNs deployed in IMC architectures. To achieve this, we introduce a novel
normalization layer combined with stochastic affine transformations. Empirical
results in various benchmark datasets show a graceful degradation in inference
accuracy, with an improvement of up to $58.11\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_S/0/1/0/all/0/1&quot;&gt;Soyed Tuhin Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Danouchi_K/0/1/0/all/0/1&quot;&gt;Kamal Danouchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenat_G/0/1/0/all/0/1&quot;&gt;Guillaume Prenat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anghel_L/0/1/0/all/0/1&quot;&gt;Lorena Anghel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tahoori_M/0/1/0/all/0/1&quot;&gt;Mehdi B. Tahoori&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12418">
<title>Towards Improved Variational Inference for Deep Bayesian Models. (arXiv:2401.12418v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12418</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has revolutionized the last decade, being at the forefront of
extraordinary advances in a wide range of tasks including computer vision,
natural language processing, and reinforcement learning, to name but a few.
However, it is well-known that deep models trained via maximum likelihood
estimation tend to be overconfident and give poorly-calibrated predictions.
Bayesian deep learning attempts to address this by placing priors on the model
parameters, which are then combined with a likelihood to perform posterior
inference. Unfortunately, for deep models, the true posterior is intractable,
forcing the user to resort to approximations. In this thesis, we explore the
use of variational inference (VI) as an approximation, as it is unique in
simultaneously approximating the posterior and providing a lower bound to the
marginal likelihood. If tight enough, this lower bound can be used to optimize
hyperparameters and to facilitate model selection. However, this capacity has
rarely been used to its full extent for Bayesian neural networks, likely
because the approximate posteriors typically used in practice can lack the
flexibility to effectively bound the marginal likelihood. We therefore explore
three aspects of Bayesian learning for deep models: 1) we ask whether it is
necessary to perform inference over as many parameters as possible, or whether
it is reasonable to treat many of them as optimizable hyperparameters; 2) we
propose a variational posterior that provides a unified view of inference in
Bayesian neural networks and deep Gaussian processes; 3) we demonstrate how VI
can be improved in certain deep Gaussian process models by analytically
removing symmetries from the posterior, and performing inference on Gram
matrices instead of features. We hope that our contributions will provide a
stepping stone to fully realize the promises of VI in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ober_S/0/1/0/all/0/1&quot;&gt;Sebastian W. Ober&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12424">
<title>DALex: Lexicase-like Selection via Diverse Aggregation. (arXiv:2401.12424v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2401.12424</link>
<description rdf:parseType="Literal">&lt;p&gt;Lexicase selection has been shown to provide advantages over other selection
algorithms in several areas of evolutionary computation and machine learning.
In its standard form, lexicase selection filters a population or other
collection based on randomly ordered training cases that are considered one at
a time. This iterated filtering process can be time-consuming, particularly in
settings with large numbers of training cases. In this paper, we propose a new
method that is nearly equivalent to lexicase selection in terms of the
individuals that it selects, but which does so significantly more quickly. The
new method, called DALex (for Diversely Aggregated Lexicase), selects the best
individual with respect to a weighted sum of training case errors, where the
weights are randomly sampled. This allows us to formulate the core computation
required for selection as matrix multiplication instead of recursive loops of
comparisons, which in turn allows us to take advantage of optimized and
parallel algorithms designed for matrix multiplication for speedup.
Furthermore, we show that we can interpolate between the behavior of lexicase
selection and its &quot;relaxed&quot; variants, such as epsilon or batch lexicase
selection, by adjusting a single hyperparameter, named &quot;particularity
pressure,&quot; which represents the importance granted to each individual training
case. Results on program synthesis, deep learning, symbolic regression, and
learning classifier systems demonstrate that DALex achieves significant
speedups over lexicase selection and its relaxed variants while maintaining
almost identical problem-solving performance. Under a fixed computational
budget, these savings free up resources that can be directed towards increasing
population size or the number of generations, enabling the potential for
solving more difficult problems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_A/0/1/0/all/0/1&quot;&gt;Andrew Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Li Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spector_L/0/1/0/all/0/1&quot;&gt;Lee Spector&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12425">
<title>The Neglected Tails of Vision-Language Models. (arXiv:2401.12425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12425</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language models (VLMs) excel in zero-shot recognition but exhibit
drastically imbalanced performance across visual concepts. For example, CLIP,
despite an impressive mean zero-shot accuracy on ImageNet (72.7%), yields
$&amp;lt;$10% on ten concepts (e.g., gyromitra and night snake), presumably, because
these concepts are under-represented in VLMs&apos; imbalanced pretraining data. Yet,
assessing this imbalance is challenging as it is non-trivial to calculate the
frequency of specific concepts within VLMs&apos; large-scale pretraining data. Our
work makes the first attempt to measure the concept frequency by analyzing
pretraining texts. We use off-the-shelf language models to help count relevant
texts that contain synonyms of the given concepts and resolve linguistic
ambiguity. We confirm that popular VLM datasets like LAION indeed exhibit
long-tailed concept distributions, which strongly correlate with per-class
accuracies. Further, contemporary multimodal systems, e.g., visual chatbots and
text-to-image generators, also struggle with the rare concepts identified by
our method. To mitigate VLMs&apos; imbalanced performance in zero-shot recognition,
we propose REtrieval-Augmented Learning REAL. First, instead of prompting VLMs
using the original class names, REAL uses their most frequent synonyms found in
VLMs&apos; pretraining texts. This already outperforms human-engineered and
LLM-generated prompts over nine benchmark datasets, likely because VLMs have
seen more images associated with the frequently used synonyms. Second, REAL
uses all the concept synonyms to retrieve a small, class-balanced set of
pretraining data to train a robust classifier. REAL surpasses the recent
retrieval-augmented solution REACT, using 400x less storage and 10,000x less
training time!
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parashar_S/0/1/0/all/0/1&quot;&gt;Shubham Parashar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhiqiu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_X/0/1/0/all/0/1&quot;&gt;Xiangjue Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caverlee_J/0/1/0/all/0/1&quot;&gt;James Caverlee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_S/0/1/0/all/0/1&quot;&gt;Shu Kong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12435">
<title>Quantitative Analysis of Molecular Transport in the Extracellular Space Using Physics-Informed Neural Network. (arXiv:2401.12435v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12435</link>
<description rdf:parseType="Literal">&lt;p&gt;The brain extracellular space (ECS), an irregular, extremely tortuous
nanoscale space located between cells or between cells and blood vessels, is
crucial for nerve cell survival. It plays a pivotal role in high-level brain
functions such as memory, emotion, and sensation. However, the specific form of
molecular transport within the ECS remain elusive. To address this challenge,
this paper proposes a novel approach to quantitatively analyze the molecular
transport within the ECS by solving an inverse problem derived from the
advection-diffusion equation (ADE) using a physics-informed neural network
(PINN). PINN provides a streamlined solution to the ADE without the need for
intricate mathematical formulations or grid settings. Additionally, the
optimization of PINN facilitates the automatic computation of the diffusion
coefficient governing long-term molecule transport and the velocity of
molecules driven by advection. Consequently, the proposed method allows for the
quantitative analysis and identification of the specific pattern of molecular
transport within the ECS through the calculation of the Peclet number.
Experimental validation on two datasets of magnetic resonance images (MRIs)
captured at different time points showcases the effectiveness of the proposed
method. Notably, our simulations reveal identical molecular transport patterns
between datasets representing rats with tracer injected into the same brain
region. These findings highlight the potential of PINN as a promising tool for
comprehensively exploring molecular transport within the ECS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_J/0/1/0/all/0/1&quot;&gt;Jiayi Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongfeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Q/0/1/0/all/0/1&quot;&gt;Qingrui Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hanbo Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zu_L/0/1/0/all/0/1&quot;&gt;Lingyun Zu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaobo Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_H/0/1/0/all/0/1&quot;&gt;Hongbin Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12436">
<title>Wasserstein Differential Privacy. (arXiv:2401.12436v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12436</link>
<description rdf:parseType="Literal">&lt;p&gt;Differential privacy (DP) has achieved remarkable results in the field of
privacy-preserving machine learning. However, existing DP frameworks do not
satisfy all the conditions for becoming metrics, which prevents them from
deriving better basic private properties and leads to exaggerated values on
privacy budgets. We propose Wasserstein differential privacy (WDP), an
alternative DP framework to measure the risk of privacy leakage, which
satisfies the properties of symmetry and triangle inequality. We show and prove
that WDP has 13 excellent properties, which can be theoretical supports for the
better performance of WDP than other DP frameworks. In addition, we derive a
general privacy accounting method called Wasserstein accountant, which enables
WDP to be applied in stochastic gradient descent (SGD) scenarios containing
sub-sampling. Experiments on basic mechanisms, compositions and deep learning
show that the privacy budgets obtained by Wasserstein accountant are relatively
stable and less influenced by order. Moreover, the overestimation on privacy
budgets can be effectively alleviated. The code is available at
https://github.com/Hifipsysta/WDP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chengyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_J/0/1/0/all/0/1&quot;&gt;Jiayin Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Aimin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12438">
<title>Secure Federated Learning Approaches to Diagnosing COVID-19. (arXiv:2401.12438v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.12438</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent pandemic has underscored the importance of accurately diagnosing
COVID-19 in hospital settings. A major challenge in this regard is
differentiating COVID-19 from other respiratory illnesses based on chest
X-rays, compounded by the restrictions of HIPAA compliance which limit the
comparison of patient X-rays. This paper introduces a HIPAA-compliant model to
aid in the diagnosis of COVID-19, utilizing federated learning. Federated
learning is a distributed machine learning approach that allows for algorithm
training across multiple decentralized devices using local data samples,
without the need for data sharing. Our model advances previous efforts in chest
X-ray diagnostic models. We examined leading models from established
competitions in this domain and developed our own models tailored to be
effective with specific hospital data. Considering the model&apos;s operation in a
federated learning context, we explored the potential impact of biased data
updates on the model&apos;s performance. To enhance hospital understanding of the
model&apos;s decision-making process and to verify that the model is not focusing on
irrelevant features, we employed a visualization technique that highlights key
features in chest X-rays indicative of a positive COVID-19 diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Adhikari_R/0/1/0/all/0/1&quot;&gt;Rittika Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Settles_C/0/1/0/all/0/1&quot;&gt;Christopher Settles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12440">
<title>Post-Training Embedding Alignment for Decoupling Enrollment and Runtime Speaker Recognition Models. (arXiv:2401.12440v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2401.12440</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated speaker identification (SID) is a crucial step for the
personalization of a wide range of speech-enabled services. Typical SID systems
use a symmetric enrollment-verification framework with a single model to derive
embeddings both offline for voice profiles extracted from enrollment
utterances, and online from runtime utterances. Due to the distinct
circumstances of enrollment and runtime, such as different computation and
latency constraints, several applications would benefit from an asymmetric
enrollment-verification framework that uses different models for enrollment and
runtime embedding generation. To support this asymmetric SID where each of the
two models can be updated independently, we propose using a lightweight neural
network to map the embeddings from the two independent models to a shared
speaker embedding space. Our results show that this approach significantly
outperforms cosine scoring in a shared speaker logit space for models that were
trained with a contrastive loss on large datasets with many speaker identities.
This proposed Neural Embedding Speaker Space Alignment (NESSA) combined with an
asymmetric update of only one of the models delivers at least 60% of the
performance gain achieved by updating both models in the standard symmetric SID
approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chenyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Desplanques_B/0/1/0/all/0/1&quot;&gt;Brecht Desplanques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ju_C/0/1/0/all/0/1&quot;&gt;Chelsea J.-T. Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chadha_A/0/1/0/all/0/1&quot;&gt;Aman Chadha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stolcke_A/0/1/0/all/0/1&quot;&gt;Andreas Stolcke&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12455">
<title>Multi-agent deep reinforcement learning with centralized training and decentralized execution for transportation infrastructure management. (arXiv:2401.12455v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.12455</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a multi-agent Deep Reinforcement Learning (DRL) framework for
managing large transportation infrastructure systems over their life-cycle.
Life-cycle management of such engineering systems is a computationally
intensive task, requiring appropriate sequential inspection and maintenance
decisions able to reduce long-term risks and costs, while dealing with
different uncertainties and constraints that lie in high-dimensional spaces. To
date, static age- or condition-based maintenance methods and risk-based or
periodic inspection plans have mostly addressed this class of optimization
problems. However, optimality, scalability, and uncertainty limitations are
often manifested under such approaches. The optimization problem in this work
is cast in the framework of constrained Partially Observable Markov Decision
Processes (POMDPs), which provides a comprehensive mathematical basis for
stochastic sequential decision settings with observation uncertainties, risk
considerations, and limited resources. To address significantly large state and
action spaces, a Deep Decentralized Multi-agent Actor-Critic (DDMAC) DRL method
with Centralized Training and Decentralized Execution (CTDE), termed as
DDMAC-CTDE is developed. The performance strengths of the DDMAC-CTDE method are
demonstrated in a generally representative and realistic example application of
an existing transportation network in Virginia, USA. The network includes
several bridge and pavement components with nonstationary degradation,
agency-imposed constraints, and traffic delay and risk considerations. Compared
to traditional management policies for transportation networks, the proposed
DDMAC-CTDE method vastly outperforms its counterparts. Overall, the proposed
algorithmic framework provides near optimal solutions for transportation
infrastructure management under real-world constraints and complexities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saifullah_M/0/1/0/all/0/1&quot;&gt;M. Saifullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papakonstantinou_K/0/1/0/all/0/1&quot;&gt;K.G. Papakonstantinou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andriotis_C/0/1/0/all/0/1&quot;&gt;C.P. Andriotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoffels_S/0/1/0/all/0/1&quot;&gt;S.M. Stoffels&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12470">
<title>Reinforcement Learning for Graph Coloring: Understanding the Power and Limits of Non-Label Invariant Representations. (arXiv:2401.12470v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12470</link>
<description rdf:parseType="Literal">&lt;p&gt;Register allocation is one of the most important problems for modern
compilers. With a practically unlimited number of user variables and a small
number of CPU registers, assigning variables to registers without conflicts is
a complex task. This work demonstrates the use of casting the register
allocation problem as a graph coloring problem. Using technologies such as
PyTorch and OpenAI Gymnasium Environments we will show that a Proximal Policy
Optimization model can learn to solve the graph coloring problem. We will also
show that the labeling of a graph is critical to the performance of the model
by taking the matrix representation of a graph and permuting it. We then test
the model&apos;s effectiveness on each of these permutations and show that it is not
effective when given a relabeling of the same graph. Our main contribution lies
in showing the need for label reordering invariant representations of graphs
for machine learning models to achieve consistent performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cummins_C/0/1/0/all/0/1&quot;&gt;Chase Cummins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veras_R/0/1/0/all/0/1&quot;&gt;Richard Veras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12474">
<title>Large Language Models are Superpositions of All Characters: Attaining Arbitrary Role-play via Self-Alignment. (arXiv:2401.12474v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12474</link>
<description rdf:parseType="Literal">&lt;p&gt;Considerable efforts have been invested in augmenting the role-playing
proficiency of open-source large language models (LLMs) by emulating
proprietary counterparts. Nevertheless, we posit that LLMs inherently harbor
role-play capabilities, owing to the extensive knowledge of characters and
potential dialogues ingrained in their vast training corpora. Thus, in this
study, we introduce Ditto, a self-alignment method for role-play. Ditto
capitalizes on character knowledge, encouraging an instruction-following LLM to
simulate role-play dialogues as a variant of reading comprehension. This method
creates a role-play training set comprising 4,000 characters, surpassing the
scale of currently available datasets by tenfold regarding the number of roles.
Subsequently, we fine-tune the LLM using this self-generated dataset to augment
its role-playing capabilities. Upon evaluating our meticulously constructed and
reproducible role-play benchmark and the roleplay subset of MT-Bench, Ditto, in
various parameter scales, consistently maintains a consistent role identity and
provides accurate role-specific knowledge in multi-turn role-play
conversations. Notably, it outperforms all open-source role-play baselines,
showcasing performance levels comparable to advanced proprietary chatbots.
Furthermore, we present the first comprehensive cross-supervision alignment
experiment in the role-play domain, revealing that the intrinsic capabilities
of LLMs confine the knowledge within role-play. Meanwhile, the role-play styles
can be easily acquired with the guidance of smaller models. We open-source
related resources at https://github.com/OFA-Sys/Ditto.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Keming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Bowen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12476">
<title>Bayesian identification of nonseparable Hamiltonians with multiplicative noise using deep learning and reduced-order modeling. (arXiv:2401.12476v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.12476</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a structure-preserving Bayesian approach for learning
nonseparable Hamiltonian systems using stochastic dynamic models allowing for
statistically-dependent, vector-valued additive and multiplicative measurement
noise. The approach is comprised of three main facets. First, we derive a
Gaussian filter for a statistically-dependent, vector-valued, additive and
multiplicative noise model that is needed to evaluate the likelihood within the
Bayesian posterior. Second, we develop a novel algorithm for cost-effective
application of Bayesian system identification to high-dimensional systems.
Third, we demonstrate how structure-preserving methods can be incorporated into
the proposed framework, using nonseparable Hamiltonians as an illustrative
system class. We compare the Bayesian method to a state-of-the-art machine
learning method on a canonical nonseparable Hamiltonian model and a chaotic
double pendulum model with small, noisy training datasets. The results show
that using the Bayesian posterior as a training objective can yield upwards of
724 times improvement in Hamiltonian mean squared error using training data
with up to 10% multiplicative noise compared to a standard training objective.
Lastly, we demonstrate the utility of the novel algorithm for parameter
estimation of a 64-dimensional model of the spatially-discretized nonlinear
Schr\&quot;odinger equation with data corrupted by up to 20% multiplicative noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Galioto_N/0/1/0/all/0/1&quot;&gt;Nicholas Galioto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Harsh Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kramer_B/0/1/0/all/0/1&quot;&gt;Boris Kramer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gorodetsky_A/0/1/0/all/0/1&quot;&gt;Alex Arkady Gorodetsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12478">
<title>Mini-batch Submodular Maximization. (arXiv:2401.12478v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12478</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first mini-batch algorithm for maximizing a non-negative
monotone decomposable submodular function, $F=\sum_{i=1}^N f^i$, under a set of
constraints. We improve over the sparsifier based approach both in theory and
in practice. We experimentally observe that our algorithm generates solutions
that are far superior to those generated by the sparsifier based approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartzman_G/0/1/0/all/0/1&quot;&gt;Gregory Schwartzman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12485">
<title>Adiabatic Quantum Support Vector Machines. (arXiv:2401.12485v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12485</link>
<description rdf:parseType="Literal">&lt;p&gt;Adiabatic quantum computers can solve difficult optimization problems (e.g.,
the quadratic unconstrained binary optimization problem), and they seem well
suited to train machine learning models. In this paper, we describe an
adiabatic quantum approach for training support vector machines. We show that
the time complexity of our quantum approach is an order of magnitude better
than the classical approach. Next, we compare the test accuracy of our quantum
approach against a classical approach that uses the Scikit-learn library in
Python across five benchmark datasets (Iris, Wisconsin Breast Cancer (WBC),
Wine, Digits, and Lambeq). We show that our quantum approach obtains accuracies
on par with the classical approach. Finally, we perform a scalability study in
which we compute the total training times of the quantum approach and the
classical approach with increasing number of features and number of data points
in the training dataset. Our scalability results show that the quantum approach
obtains a 3.5--4.5 times speedup over the classical approach on datasets with
many (millions of) features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Date_P/0/1/0/all/0/1&quot;&gt;Prasanna Date&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woun_D/0/1/0/all/0/1&quot;&gt;Dong Jun Woun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamilton_K/0/1/0/all/0/1&quot;&gt;Kathleen Hamilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_E/0/1/0/all/0/1&quot;&gt;Eduardo A. Coello Perez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shekhar_M/0/1/0/all/0/1&quot;&gt;Mayanka Chandra Shekhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rios_F/0/1/0/all/0/1&quot;&gt;Francisco Rios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gounley_J/0/1/0/all/0/1&quot;&gt;John Gounley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suh_I/0/1/0/all/0/1&quot;&gt;In-Saeng Suh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Humble_T/0/1/0/all/0/1&quot;&gt;Travis Humble&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tourassi_G/0/1/0/all/0/1&quot;&gt;Georgia Tourassi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12489">
<title>Unsupervised Learning Method for the Wave Equation Based on Finite Difference Residual Constraints Loss. (arXiv:2401.12489v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12489</link>
<description rdf:parseType="Literal">&lt;p&gt;The wave equation is an important physical partial differential equation, and
in recent years, deep learning has shown promise in accelerating or replacing
traditional numerical methods for solving it. However, existing deep learning
methods suffer from high data acquisition costs, low training efficiency, and
insufficient generalization capability for boundary conditions. To address
these issues, this paper proposes an unsupervised learning method for the wave
equation based on finite difference residual constraints. We construct a novel
finite difference residual constraint based on structured grids and finite
difference methods, as well as an unsupervised training strategy, enabling
convolutional neural networks to train without data and predict the forward
propagation process of waves. Experimental results show that finite difference
residual constraints have advantages over physics-informed neural networks
(PINNs) type physical information constraints, such as easier fitting, lower
computational costs, and stronger source term generalization capability, making
our method more efficient in training and potent in application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xin Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jia-Xian Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lai-Ping Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xiao-Gang Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12492">
<title>Comparing Human-Centered Language Modeling: Is it Better to Model Groups, Individual Traits, or Both?. (arXiv:2401.12492v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12492</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language processing has made progress in incorporating human context
into its models, but whether it is more effective to use group-wise attributes
(e.g., over-45-year-olds) or model individuals remains open. Group attributes
are technically easier but coarse: not all 45-year-olds write the same way. In
contrast, modeling individuals captures the complexity of each person&apos;s
identity. It allows for a more personalized representation, but we may have to
model an infinite number of users and require data that may be impossible to
get. We compare modeling human context via group attributes, individual users,
and combined approaches. Combining group and individual features significantly
benefits user-level regression tasks like age estimation or personality
assessment from a user&apos;s documents. Modeling individual users significantly
improves the performance of single document-level classification tasks like
stance and topic detection. We also find that individual-user modeling does
well even without user&apos;s historical data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soni_N/0/1/0/all/0/1&quot;&gt;Nikita Soni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_N/0/1/0/all/0/1&quot;&gt;Niranjan Balasubramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwartz_H/0/1/0/all/0/1&quot;&gt;H. Andrew Schwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hovy_D/0/1/0/all/0/1&quot;&gt;Dirk Hovy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12496">
<title>DexTouch: Learning to Seek and Manipulate Objects with Tactile Dexterity. (arXiv:2401.12496v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12496</link>
<description rdf:parseType="Literal">&lt;p&gt;The sense of touch is an essential ability for skillfully performing a
variety of tasks, providing the capacity to search and manipulate objects
without relying on visual information. Extensive research has been conducted
over time to apply these human tactile abilities to robots. In this paper, we
introduce a multi-finger robot system designed to search for and manipulate
objects using the sense of touch without relying on visual information.
Randomly located target objects are searched using tactile sensors, and the
objects are manipulated for tasks that mimic daily-life. The objective of the
study is to endow robots with human-like tactile capabilities. To achieve this,
binary tactile sensors are implemented on one side of the robot hand to
minimize the Sim2Real gap. Training the policy through reinforcement learning
in simulation and transferring the trained policy to the real environment, we
demonstrate that object search and manipulation using tactile sensors is
possible even in an environment without vision information. In addition, an
ablation study was conducted to analyze the effect of tactile information on
manipulative tasks. Our project page is available at
https://lee-kangwon.github.io/dextouch/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kang-Won Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yuzhe Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaolong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Soo-Chul Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12497">
<title>Building Minimal and Reusable Causal State Abstractions for Reinforcement Learning. (arXiv:2401.12497v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12497</link>
<description rdf:parseType="Literal">&lt;p&gt;Two desiderata of reinforcement learning (RL) algorithms are the ability to
learn from relatively little experience and the ability to learn policies that
generalize to a range of problem specifications. In factored state spaces, one
approach towards achieving both goals is to learn state abstractions, which
only keep the necessary variables for learning the tasks at hand. This paper
introduces Causal Bisimulation Modeling (CBM), a method that learns the causal
relationships in the dynamics and reward functions for each task to derive a
minimal, task-specific abstraction. CBM leverages and improves implicit
modeling to train a high-fidelity causal dynamics model that can be reused for
all tasks in the same environment. Empirical validation on manipulation
environments and Deepmind Control Suite reveals that CBM&apos;s learned implicit
dynamics models identify the underlying causal relationships and state
abstractions more accurately than explicit ones. Furthermore, the derived state
abstractions allow a task learner to achieve near-oracle levels of sample
efficiency and outperform baselines on all tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zizhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Caroline Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xuesu Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12508">
<title>On the Stochastic (Variance-Reduced) Proximal Gradient Method for Regularized Expected Reward Optimization. (arXiv:2401.12508v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12508</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a regularized expected reward optimization problem in the
non-oblivious setting that covers many existing problems in reinforcement
learning (RL). In order to solve such an optimization problem, we apply and
analyze the classical stochastic proximal gradient method. In particular, the
method has shown to admit an $O(\epsilon^{-4})$ sample complexity to an
$\epsilon$-stationary point, under standard conditions. Since the variance of
the classical stochastic gradient estimator is typically large which slows down
the convergence, we also apply an efficient stochastic variance-reduce proximal
gradient method with an importance sampling based ProbAbilistic Gradient
Estimator (PAGE). To the best of our knowledge, the application of this method
represents a novel approach in addressing the general regularized reward
optimization problem. Our analysis shows that the sample complexity can be
improved from $O(\epsilon^{-4})$ to $O(\epsilon^{-3})$ under additional
conditions. Our results on the stochastic (variance-reduced) proximal gradient
method match the sample complexity of their most competitive counterparts under
similar settings in the RL literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Ling Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haizhao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12509">
<title>Digital cloning of online social networks for language-sensitive agent-based modeling of misinformation spread. (arXiv:2401.12509v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.12509</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a simulation framework for studying misinformation spread within
online social networks that blends agent-based modeling and natural language
processing techniques. While many other agent-based simulations exist in this
space, their ability to provide actionable insights in in part limited by their
lack of fidelity and generalizability to existing networks. To partially
address these concerns, we create a &apos;digital clone&apos; of a known misinformation
sharing network by downloading social media histories for over ten thousand of
its users. We parse these histories to both extract the structure of the
network and model the nuanced ways in which information is shared and spread
among its members. Unlike many other agent-based methods in this space,
information sharing between users in our framework is sensitive to topic of
discussion, user preferences, and online community dynamics. To evaluate the
fidelity of our method, we seed our cloned network with a set of posts recorded
in the base network and compare propagation dynamics between the two, observing
reasonable agreement across the twin networks over a variety of metrics.
Lastly, we explore how the cloned network may serve as a flexible, low-cost
testbed for misinformation countermeasure evaluation and red teaming analysis.
We hope the tools explored here augment existing efforts in the space and
unlock new opportunities for misinformation countermeasure evaluation, a field
that may become increasingly important to consider with the anticipated rise of
misinformation campaigns fueled by generative artificial intelligence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Puri_P/0/1/0/all/0/1&quot;&gt;Prateek Puri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassler_G/0/1/0/all/0/1&quot;&gt;Gabriel Hassler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shenk_A/0/1/0/all/0/1&quot;&gt;Anton Shenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katragadda_S/0/1/0/all/0/1&quot;&gt;Sai Katragadda&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12517">
<title>DDMI: Domain-Agnostic Latent Diffusion Models for Synthesizing High-Quality Implicit Neural Representations. (arXiv:2401.12517v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12517</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have introduced a new class of generative models for
synthesizing implicit neural representations (INRs) that capture arbitrary
continuous signals in various domains. These models opened the door for
domain-agnostic generative models, but they often fail to achieve high-quality
generation. We observed that the existing methods generate the weights of
neural networks to parameterize INRs and evaluate the network with fixed
positional embeddings (PEs). Arguably, this architecture limits the expressive
power of generative models and results in low-quality INR generation. To
address this limitation, we propose Domain-agnostic Latent Diffusion Model for
INRs (DDMI) that generates adaptive positional embeddings instead of neural
networks&apos; weights. Specifically, we develop a Discrete-to-continuous space
Variational AutoEncoder (D2C-VAE), which seamlessly connects discrete data and
the continuous signal functions in the shared latent space. Additionally, we
introduce a novel conditioning mechanism for evaluating INRs with the
hierarchically decomposed PEs to further enhance expressive power. Extensive
experiments across four modalities, e.g., 2D images, 3D shapes, Neural Radiance
Fields, and videos, with seven benchmark datasets, demonstrate the versatility
of DDMI and its superior performance compared to the existing INR generative
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_D/0/1/0/all/0/1&quot;&gt;Dogyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sihyeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sojin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyunwoo J. Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12520">
<title>Key Information Retrieval to Classify the Unstructured Data Content of Preferential Trade Agreements. (arXiv:2401.12520v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12520</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid proliferation of textual data, predicting long texts has
emerged as a significant challenge in the domain of natural language
processing. Traditional text prediction methods encounter substantial
difficulties when grappling with long texts, primarily due to the presence of
redundant and irrelevant information, which impedes the model&apos;s capacity to
capture pivotal insights from the text. To address this issue, we introduce a
novel approach to long-text classification and prediction. Initially, we employ
embedding techniques to condense the long texts, aiming to diminish the
redundancy therein. Subsequently,the Bidirectional Encoder Representations from
Transformers (BERT) embedding method is utilized for text classification
training. Experimental outcomes indicate that our method realizes considerable
performance enhancements in classifying long texts of Preferential Trade
Agreements. Furthermore, the condensation of text through embedding methods not
only augments prediction accuracy but also substantially reduces computational
complexity. Overall, this paper presents a strategy for long-text prediction,
offering a valuable reference for researchers and engineers in the natural
language processing sphere.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jiahui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Ziyi Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordeev_S/0/1/0/all/0/1&quot;&gt;Stepan Gordeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zijie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_D/0/1/0/all/0/1&quot;&gt;Dongjin Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Steinbach_S/0/1/0/all/0/1&quot;&gt;Sandro Steinbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_C/0/1/0/all/0/1&quot;&gt;Caiwen Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12522">
<title>BiTA: Bi-Directional Tuning for Lossless Acceleration in Large Language Models. (arXiv:2401.12522v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12522</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) commonly employ autoregressive generation during
inference, leading to high memory bandwidth demand and consequently extended
latency. To mitigate this inefficiency, we present Bi-directional Tuning for
lossless Acceleration (BiTA), an innovative method expediting LLMs via
streamlined semi-autoregressive generation and draft verification. Inspired by
the concept of prompt tuning, we enhance LLMs with a parameter-efficient design
called bi-directional tuning for the capability in semi-autoregressive
generation. Employing efficient tree-based decoding, the models perform draft
candidate generation and verification in parallel, ensuring outputs identical
to their autoregressive counterparts under greedy sampling. BiTA serves as a
lightweight plug-in module, seamlessly boosting the inference efficiency of
existing LLMs without requiring additional assistance models or incurring
significant extra memory costs. Applying the proposed BiTA, LLaMA-2-70B-Chat
achieves a 2.7$\times$ speedup on the MT-Bench benchmark. Extensive experiments
confirm our method surpasses state-of-the-art acceleration techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Feng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_H/0/1/0/all/0/1&quot;&gt;Hanling Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yifan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xiaotian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_G/0/1/0/all/0/1&quot;&gt;Guangming Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_R/0/1/0/all/0/1&quot;&gt;Rong Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12532">
<title>DAFA: Distance-Aware Fair Adversarial Training. (arXiv:2401.12532v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12532</link>
<description rdf:parseType="Literal">&lt;p&gt;The disparity in accuracy between classes in standard training is amplified
during adversarial training, a phenomenon termed the robust fairness problem.
Existing methodologies aimed to enhance robust fairness by sacrificing the
model&apos;s performance on easier classes in order to improve its performance on
harder ones. However, we observe that under adversarial attacks, the majority
of the model&apos;s predictions for samples from the worst class are biased towards
classes similar to the worst class, rather than towards the easy classes.
Through theoretical and empirical analysis, we demonstrate that robust fairness
deteriorates as the distance between classes decreases. Motivated by these
insights, we introduce the Distance-Aware Fair Adversarial training (DAFA)
methodology, which addresses robust fairness by taking into account the
similarities between classes. Specifically, our method assigns distinct loss
weights and adversarial margins to each class and adjusts them to encourage a
trade-off in robustness among similar classes. Experimental results across
various datasets demonstrate that our method not only maintains average robust
accuracy but also significantly improves the worst robust accuracy, indicating
a marked improvement in robust fairness compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Saehyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1&quot;&gt;Hyemi Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Junsung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_H/0/1/0/all/0/1&quot;&gt;Ho Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12533">
<title>Efficient Constrained $k$-Center Clustering with Background Knowledge. (arXiv:2401.12533v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12533</link>
<description rdf:parseType="Literal">&lt;p&gt;Center-based clustering has attracted significant research interest from both
theory and practice. In many practical applications, input data often contain
background knowledge that can be used to improve clustering results. In this
work, we build on widely adopted $k$-center clustering and model its input
background knowledge as must-link (ML) and cannot-link (CL) constraint sets.
However, most clustering problems including $k$-center are inherently
$\mathcal{NP}$-hard, while the more complex constrained variants are known to
suffer severer approximation and computation barriers that significantly limit
their applicability. By employing a suite of techniques including reverse
dominating sets, linear programming (LP) integral polyhedron, and LP duality,
we arrive at the first efficient approximation algorithm for constrained
$k$-center with the best possible ratio of 2. We also construct competitive
baseline algorithms and empirically evaluate our approximation algorithm
against them on a variety of real datasets. The results validate our
theoretical findings and demonstrate the great advantages of our algorithm in
terms of clustering cost, clustering quality, and running time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_L/0/1/0/all/0/1&quot;&gt;Longkun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chaoqi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_K/0/1/0/all/0/1&quot;&gt;Kewen Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhigang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Minhui Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12546">
<title>On Building Myopic MPC Policies using Supervised Learning. (arXiv:2401.12546v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12546</link>
<description rdf:parseType="Literal">&lt;p&gt;The application of supervised learning techniques in combination with model
predictive control (MPC) has recently generated significant interest,
particularly in the area of approximate explicit MPC, where function
approximators like deep neural networks are used to learn the MPC policy via
optimal state-action pairs generated offline. While the aim of approximate
explicit MPC is to closely replicate the MPC policy, substituting online
optimization with a trained neural network, the performance guarantees that
come with solving the online optimization problem are typically lost. This
paper considers an alternative strategy, where supervised learning is used to
learn the optimal value function offline instead of learning the optimal
policy. This can then be used as the cost-to-go function in a myopic MPC with a
very short prediction horizon, such that the online computation burden reduces
significantly without affecting the controller performance. This approach
differs from existing work on value function approximations in the sense that
it learns the cost-to-go function by using offline-collected state-value pairs,
rather than closed-loop performance data. The cost of generating the
state-value pairs used for training is addressed using a sensitivity-based data
augmentation scheme.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orrico_C/0/1/0/all/0/1&quot;&gt;Christopher A. Orrico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bokan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnamoorthy_D/0/1/0/all/0/1&quot;&gt;Dinesh Krishnamoorthy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12550">
<title>UR4NNV: Neural Network Verification, Under-approximation Reachability Works!. (arXiv:2401.12550v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12550</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, formal verification of deep neural networks (DNNs) has garnered
considerable attention, and over-approximation based methods have become
popular due to their effectiveness and efficiency. However, these strategies
face challenges in addressing the &quot;unknown dilemma&quot; concerning whether the
exact output region or the introduced approximation error violates the property
in question. To address this, this paper introduces the UR4NNV verification
framework, which utilizes under-approximation reachability analysis for DNN
verification for the first time. UR4NNV focuses on DNNs with Rectified Linear
Unit (ReLU) activations and employs a binary tree branch-based
under-approximation algorithm. In each epoch, UR4NNV under-approximates a
sub-polytope of the reachable set and verifies this polytope against the given
property. Through a trial-and-error approach, UR4NNV effectively falsifies DNN
properties while providing confidence levels when reaching verification epoch
bounds and failing falsifying properties. Experimental comparisons with
existing verification methods demonstrate the effectiveness and efficiency of
UR4NNV, significantly reducing the impact of the &quot;unknown dilemma&quot;.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Taoran Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Ran Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bai Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Ji Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenjing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shaojun Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wanwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12564">
<title>Graph Contrastive Invariant Learning from the Causal Perspective. (arXiv:2401.12564v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12564</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph contrastive learning (GCL), learning the node representation by
contrasting two augmented graphs in a self-supervised way, has attracted
considerable attention. GCL is usually believed to learn the invariant
representation. However, does this understanding always hold in practice? In
this paper, we first study GCL from the perspective of causality. By analyzing
GCL with the structural causal model (SCM), we discover that traditional GCL
may not well learn the invariant representations due to the non-causal
information contained in the graph. How can we fix it and encourage the current
GCL to learn better invariant representations? The SCM offers two requirements
and motives us to propose a novel GCL method. Particularly, we introduce the
spectral graph augmentation to simulate the intervention upon non-causal
factors. Then we design the invariance objective and independence objective to
better capture the causal factors. Specifically, (i) the invariance objective
encourages the encoder to capture the invariant information contained in causal
variables, and (ii) the independence objective aims to reduce the influence of
confounders on the causal variables. Experimental results demonstrate the
effectiveness of our approach on node classification tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_Y/0/1/0/all/0/1&quot;&gt;Yanhu Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Shaohua Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chuan Shi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12576">
<title>LLMCheckup: Conversational Examination of Large Language Models via Interpretability Tools. (arXiv:2401.12576v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2401.12576</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretability tools that offer explanations in the form of a dialogue have
demonstrated their efficacy in enhancing users&apos; understanding, as one-off
explanations may occasionally fall short in providing sufficient information to
the user. Current solutions for dialogue-based explanations, however, require
many dependencies and are not easily transferable to tasks they were not
designed for. With LLMCheckup, we present an easily accessible tool that allows
users to chat with any state-of-the-art large language model (LLM) about its
behavior. We enable LLMs to generate all explanations by themselves and take
care of intent recognition without fine-tuning, by connecting them with a broad
spectrum of Explainable AI (XAI) tools, e.g. feature attributions,
embedding-based similarity, and prompting strategies for counterfactual and
rationale generation. LLM (self-)explanations are presented as an interactive
dialogue that supports follow-up questions and generates suggestions.
LLMCheckup provides tutorials for operations available in the system, catering
to individuals with varying levels of expertise in XAI and supports multiple
input modalities. We introduce a new parsing strategy called multi-prompt
parsing substantially enhancing the parsing accuracy of LLMs. Finally, we
showcase the tasks of fact checking and commonsense question answering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qianli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anikina_T/0/1/0/all/0/1&quot;&gt;Tatiana Anikina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldhus_N/0/1/0/all/0/1&quot;&gt;Nils Feldhus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Genabith_J/0/1/0/all/0/1&quot;&gt;Josef van Genabith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hennig_L/0/1/0/all/0/1&quot;&gt;Leonhard Hennig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_S/0/1/0/all/0/1&quot;&gt;Sebastian M&amp;#xf6;ller&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12588">
<title>Interpreting Equivariant Representations. (arXiv:2401.12588v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12588</link>
<description rdf:parseType="Literal">&lt;p&gt;Latent representations are used extensively for downstream tasks, such as
visualization, interpolation or feature extraction of deep learning models.
Invariant and equivariant neural networks are powerful and well-established
models for enforcing inductive biases. In this paper, we demonstrate that the
inductive bias imposed on the by an equivariant model must also be taken into
account when using latent representations. We show how not accounting for the
inductive biases leads to decreased performance on downstream tasks, and vice
versa, how accounting for inductive biases can be done effectively by using an
invariant projection of the latent representations. We propose principles for
how to choose such a projection, and show the impact of using these principles
in two common examples: First, we study a permutation equivariant variational
auto-encoder trained for molecule graph generation; here we show that invariant
projections can be designed that incur no loss of information in the resulting
invariant representation. Next, we study a rotation-equivariant representation
used for image classification. Here, we illustrate how random invariant
projections can be used to obtain an invariant representation with a high
degree of retained information. In both cases, the analysis of invariant latent
representations proves superior to their equivariant counterparts. Finally, we
illustrate that the phenomena documented here for equivariant neural networks
have counterparts in standard neural networks where invariance is encouraged
via augmentation. Thus, while these ambiguities may be known by experienced
developers of equivariant models, we make both the knowledge as well as
effective tools to handle the ambiguities available to the broader community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hansen_A/0/1/0/all/0/1&quot;&gt;Andreas Abildtrup Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calissano_A/0/1/0/all/0/1&quot;&gt;Anna Calissano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feragen_A/0/1/0/all/0/1&quot;&gt;Aasa Feragen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12609">
<title>Fast Semi-supervised Unmixing using Non-convex Optimization. (arXiv:2401.12609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12609</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel linear model tailored for
semisupervised/library-based unmixing. Our model incorporates considerations
for library mismatch while enabling the enforcement of the abundance sum-to-one
constraint (ASC). Unlike conventional sparse unmixing methods, this model
involves nonconvex optimization, presenting significant computational
challenges. We demonstrate the efficacy of Alternating Methods of Multipliers
(ADMM) in cyclically solving these intricate problems. We propose two
semisupervised unmixing approaches, each relying on distinct priors applied to
the new model in addition to the ASC: sparsity prior and convexity constraint.
Our experimental results validate that enforcing the convexity constraint
outperforms the sparsity prior for the endmember library. These results are
corroborated across three simulated datasets (accounting for spectral
variability and varying pixel purity levels) and the Cuprite dataset.
Additionally, our comparison with conventional sparse unmixing methods
showcases considerable advantages of our proposed model, which entails
nonconvex optimization. Notably, our implementations of the proposed
algorithms-fast semisupervised unmixing (FaSUn) and sparse unmixing using
soft-shrinkage (SUnS)-prove considerably more efficient than traditional sparse
unmixing methods. SUnS and FaSUn were implemented using PyTorch and provided in
a dedicated Python package called Fast Semisupervised Unmixing (FUnmix), which
is open-source and available at https://github.com/BehnoodRasti/FUnmix
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasti_B/0/1/0/all/0/1&quot;&gt;Behnood Rasti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zouaoui_A/0/1/0/all/0/1&quot;&gt;Alexandre Zouaoui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mairal_J/0/1/0/all/0/1&quot;&gt;Julien Mairal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanussot_J/0/1/0/all/0/1&quot;&gt;Jocelyn Chanussot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12610">
<title>The twin peaks of learning neural networks. (arXiv:2401.12610v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12610</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent works demonstrated the existence of a double-descent phenomenon for
the generalization error of neural networks, where highly overparameterized
models escape overfitting and achieve good test performance, at odds with the
standard bias-variance trade-off described by statistical learning theory. In
the present work, we explore a link between this phenomenon and the increase of
complexity and sensitivity of the function represented by neural networks. In
particular, we study the Boolean mean dimension (BMD), a metric developed in
the context of Boolean function analysis. Focusing on a simple teacher-student
setting for the random feature model, we derive a theoretical analysis based on
the replica method that yields an interpretable expression for the BMD, in the
high dimensional regime where the number of data points, the number of
features, and the input size grow to infinity. We find that, as the degree of
overparameterization of the network is increased, the BMD reaches an evident
peak at the interpolation threshold, in correspondence with the generalization
error peak, and then slowly approaches a low asymptotic value. The same
phenomenology is then traced in numerical experiments with different model
classes and training setups. Moreover, we find empirically that adversarially
initialized models tend to show higher BMD values, and that models that are
more robust to adversarial attacks exhibit a lower BMD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demyanenko_E/0/1/0/all/0/1&quot;&gt;Elizaveta Demyanenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feinauer_C/0/1/0/all/0/1&quot;&gt;Christoph Feinauer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malatesta_E/0/1/0/all/0/1&quot;&gt;Enrico M. Malatesta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saglietti_L/0/1/0/all/0/1&quot;&gt;Luca Saglietti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12611">
<title>Prompt Smells: An Omen for Undesirable Generative AI Outputs. (arXiv:2401.12611v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12611</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent Generative Artificial Intelligence (GenAI) trends focus on various
applications, including creating stories, illustrations, poems, articles,
computer code, music compositions, and videos. Extrinsic hallucinations are a
critical limitation of such GenAI, which can lead to significant challenges in
achieving and maintaining the trustworthiness of GenAI. In this paper, we
propose two new concepts that we believe will aid the research community in
addressing limitations associated with the application of GenAI models. First,
we propose a definition for the &quot;desirability&quot; of GenAI outputs and three
factors which are observed to influence it. Second, drawing inspiration from
Martin Fowler&apos;s code smells, we propose the concept of &quot;prompt smells&quot; and the
adverse effects they are observed to have on the desirability of GenAI outputs.
We expect our work will contribute to the ongoing conversation about the
desirability of GenAI outputs and help advance the field in a meaningful way.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronanki_K/0/1/0/all/0/1&quot;&gt;Krishna Ronanki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cabrero_Daniel_B/0/1/0/all/0/1&quot;&gt;Beatriz Cabrero-Daniel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berger_C/0/1/0/all/0/1&quot;&gt;Christian Berger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12617">
<title>The Joint Effect of Task Similarity and Overparameterization on Catastrophic Forgetting -- An Analytical Model. (arXiv:2401.12617v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12617</link>
<description rdf:parseType="Literal">&lt;p&gt;In continual learning, catastrophic forgetting is affected by multiple
aspects of the tasks. Previous works have analyzed separately how forgetting is
affected by either task similarity or overparameterization. In contrast, our
paper examines how task similarity and overparameterization jointly affect
forgetting in an analyzable model. Specifically, we focus on two-task continual
linear regression, where the second task is a random orthogonal transformation
of an arbitrary first task (an abstraction of random permutation tasks). We
derive an exact analytical expression for the expected forgetting - and uncover
a nuanced pattern. In highly overparameterized models, intermediate task
similarity causes the most forgetting. However, near the interpolation
threshold, forgetting decreases monotonically with the expected task
similarity. We validate our findings with linear regression on synthetic data,
and with neural networks on established permutation task benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Evron_I/0/1/0/all/0/1&quot;&gt;Itay Evron&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldfarb_D/0/1/0/all/0/1&quot;&gt;Daniel Goldfarb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinberger_N/0/1/0/all/0/1&quot;&gt;Nir Weinberger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soudry_D/0/1/0/all/0/1&quot;&gt;Daniel Soudry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hand_P/0/1/0/all/0/1&quot;&gt;Paul Hand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12624">
<title>Knowledge Distillation from Language-Oriented to Emergent Communication for Multi-Agent Remote Control. (arXiv:2401.12624v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12624</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we compare emergent communication (EC) built upon multi-agent
deep reinforcement learning (MADRL) and language-oriented semantic
communication (LSC) empowered by a pre-trained large language model (LLM) using
human language. In a multi-agent remote navigation task, with multimodal input
data comprising location and channel maps, it is shown that EC incurs high
training cost and struggles when using multimodal data, whereas LSC yields high
inference computing cost due to the LLM&apos;s large size. To address their
respective bottlenecks, we propose a novel framework of language-guided EC
(LEC) by guiding the EC training using LSC via knowledge distillation (KD).
Simulations corroborate that LEC achieves faster travel time while avoiding
areas with poor channel conditions, as well as speeding up the MADRL training
convergence by up to 61.8% compared to EC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yongjun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Sejin Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jihong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bennis_M/0/1/0/all/0/1&quot;&gt;Mehdi Bennis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seong-Lyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Junil Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12627">
<title>Blind Channel Estimation and Joint Symbol Detection with Data-Driven Factor Graphs. (arXiv:2401.12627v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.12627</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the application of the factor graph framework for blind joint
channel estimation and symbol detection on time-variant linear inter-symbol
interference channels. In particular, we consider the expectation maximization
(EM) algorithm for maximum likelihood estimation, which typically suffers from
high complexity as it requires the computation of the symbol-wise posterior
distributions in every iteration. We address this issue by efficiently
approximating the posteriors using the belief propagation (BP) algorithm on a
suitable factor graph. By interweaving the iterations of BP and EM, the
detection complexity can be further reduced to a single BP iteration per EM
step. In addition, we propose a data-driven version of our algorithm that
introduces momentum in the BP updates and learns a suitable EM parameter update
schedule, thereby significantly improving the performance-complexity tradeoff
with a few offline training samples. Our numerical experiments demonstrate the
excellent performance of the proposed blind detector and show that it even
outperforms coherent BP detection in high signal-to-noise scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmid_L/0/1/0/all/0/1&quot;&gt;Luca Schmid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raviv_T/0/1/0/all/0/1&quot;&gt;Tomer Raviv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shlezinger_N/0/1/0/all/0/1&quot;&gt;Nir Shlezinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmalen_L/0/1/0/all/0/1&quot;&gt;Laurent Schmalen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12630">
<title>Full-Stack Optimization for CAM-Only DNN Inference. (arXiv:2401.12630v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2401.12630</link>
<description rdf:parseType="Literal">&lt;p&gt;The accuracy of neural networks has greatly improved across various domains
over the past years. Their ever-increasing complexity, however, leads to
prohibitively high energy demands and latency in von Neumann systems. Several
computing-in-memory (CIM) systems have recently been proposed to overcome this,
but trade-offs involving accuracy, hardware reliability, and scalability for
large models remain a challenge. Additionally, for some CIM designs, the
activation movement still requires considerable time and energy. This paper
explores the combination of algorithmic optimizations for ternary weight neural
networks and associative processors (APs) implemented using racetrack memory
(RTM). We propose a novel compilation flow to optimize convolutions on APs by
reducing their arithmetic intensity. By leveraging the benefits of RTM-based
APs, this approach substantially reduces data transfers within the memory while
addressing accuracy, energy efficiency, and reliability concerns. Concretely,
our solution improves the energy efficiency of ResNet-18 inference on ImageNet
by 7.5x compared to crossbar in-memory accelerators while retaining software
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lima_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Paulo C. de Lima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asif Ali Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carro_L/0/1/0/all/0/1&quot;&gt;Luigi Carro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castrillon_J/0/1/0/all/0/1&quot;&gt;Jeronimo Castrillon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12631">
<title>A Reply to Makelov et al. (2023)&apos;s &quot;Interpretability Illusion&quot; Arguments. (arXiv:2401.12631v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12631</link>
<description rdf:parseType="Literal">&lt;p&gt;We respond to the recent paper by Makelov et al. (2023), which reviews
subspace interchange intervention methods like distributed alignment search
(DAS; Geiger et al. 2023) and claims that these methods potentially cause
&quot;interpretability illusions&quot;. We first review Makelov et al. (2023)&apos;s technical
notion of what an &quot;interpretability illusion&quot; is, and then we show that even
intuitive and desirable explanations can qualify as illusions in this sense. As
a result, their method of discovering &quot;illusions&quot; can reject explanations they
consider &quot;non-illusory&quot;. We then argue that the illusions Makelov et al. (2023)
see in practice are artifacts of their training and evaluation paradigms. We
close by emphasizing that, though we disagree with their core characterization,
Makelov et al. (2023)&apos;s examples and discussion have undoubtedly pushed the
field of interpretability forward.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhengxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Atticus Geiger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arora_A/0/1/0/all/0/1&quot;&gt;Aryaman Arora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Icard_T/0/1/0/all/0/1&quot;&gt;Thomas Icard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potts_C/0/1/0/all/0/1&quot;&gt;Christopher Potts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodman_N/0/1/0/all/0/1&quot;&gt;Noah D. Goodman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12644">
<title>Binary Feature Mask Optimization for Feature Selection. (arXiv:2401.12644v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12644</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate feature selection problem for generic machine learning (ML)
models. We introduce a novel framework that selects features considering the
predictions of the model. Our framework innovates by using a novel feature
masking approach to eliminate the features during the selection process,
instead of completely removing them from the dataset. This allows us to use the
same ML model during feature selection, unlike other feature selection methods
where we need to train the ML model again as the dataset has different
dimensions on each iteration. We obtain the mask operator using the predictions
of the ML model, which offers a comprehensive view on the subsets of the
features essential for the predictive performance of the model. A variety of
approaches exist in the feature selection literature. However, no study has
introduced a training-free framework for a generic ML model to select features
while considering the importance of the feature subsets as a whole, instead of
focusing on the individual features. We demonstrate significant performance
improvements on the real-life datasets under different settings using LightGBM
and Multi-Layer Perceptron as our ML models. Additionally, we openly share the
implementation code for our methods to encourage the research and the
contributions in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorasdagi_M/0/1/0/all/0/1&quot;&gt;Mehmet E. Lorasdagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turali_M/0/1/0/all/0/1&quot;&gt;Mehmet Y. Turali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koc_A/0/1/0/all/0/1&quot;&gt;Ali T. Koc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozat_S/0/1/0/all/0/1&quot;&gt;Suleyman S. Kozat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12645">
<title>On the Robustness of Deep Learning-aided Symbol Detectors to Varying Conditions and Imperfect Channel Knowledge. (arXiv:2401.12645v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.12645</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, a data-driven Bahl-Cocke-Jelinek-Raviv (BCJR) algorithm tailored to
channels with intersymbol interference has been introduced. This so-called
BCJRNet algorithm utilizes neural networks to calculate channel likelihoods.
BCJRNet has demonstrated resilience against inaccurate channel tap estimations
when applied to a time-invariant channel with ideal exponential decay profiles.
However, its generalization capabilities for practically-relevant time-varying
channels, where the receiver can only access incorrect channel parameters,
remain largely unexplored. The primary contribution of this paper is to expand
upon the results from existing literature to encompass a variety of imperfect
channel knowledge cases that appear in real-world transmissions. Our findings
demonstrate that BCJRNet significantly outperforms the conventional BCJR
algorithm for stationary transmission scenarios when learning from noisy
channel data and with imperfect channel decay profiles. However, this advantage
is shown to diminish when the operating channel is also rapidly time-varying.
Our results also show the importance of memory assumptions for conventional
BCJR and BCJRNet. An underestimation of the memory largely degrades the
performance of both BCJR and BCJRNet, especially in a slow-decaying channel. To
mimic a situation closer to a practical scenario, we also combined channel tap
uncertainty with imperfect channel memory knowledge. Somewhat surprisingly, our
results revealed improved performance when employing the conventional BCJR with
an underestimated memory assumption. BCJRNet, on the other hand, showed a
consistent performance improvement as the level of accurate memory knowledge
increased.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chin-Hung Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karanov_B/0/1/0/all/0/1&quot;&gt;Boris Karanov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houtum_W/0/1/0/all/0/1&quot;&gt;Wim van Houtum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_W/0/1/0/all/0/1&quot;&gt;Wu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Young_A/0/1/0/all/0/1&quot;&gt;Alex Young&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarado_A/0/1/0/all/0/1&quot;&gt;Alex Alvarado&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12648">
<title>Consistency Enhancement-Based Deep Multiview Clustering via Contrastive Learning. (arXiv:2401.12648v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12648</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiview clustering (MVC) segregates data samples into meaningful clusters
by synthesizing information across multiple views. Moreover, deep
learning-based methods have demonstrated their strong feature learning
capabilities in MVC scenarios. However, effectively generalizing feature
representations while maintaining consistency is still an intractable problem.
In addition, most existing deep clustering methods based on contrastive
learning overlook the consistency of the clustering representations during the
clustering process. In this paper, we show how the above problems can be
overcome and propose a consistent enhancement-based deep MVC method via
contrastive learning (CCEC). Specifically, semantic connection blocks are
incorporated into a feature representation to preserve the consistent
information among multiple views. Furthermore, the representation process for
clustering is enhanced through spectral clustering, and the consistency across
multiple views is improved. Experiments conducted on five datasets demonstrate
the effectiveness and superiority of our method in comparison with the
state-of-the-art (SOTA) methods. The code for this method can be accessed at
https://anonymous.4open.science/r/CCEC-E84E/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_H/0/1/0/all/0/1&quot;&gt;Hua Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_W/0/1/0/all/0/1&quot;&gt;Wai Lok Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xi Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12662">
<title>Integrating Human Expertise in Continuous Spaces: A Novel Interactive Bayesian Optimization Framework with Preference Expected Improvement. (arXiv:2401.12662v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12662</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive Machine Learning (IML) seeks to integrate human expertise into
machine learning processes. However, most existing algorithms cannot be applied
to Realworld Scenarios because their state spaces and/or action spaces are
limited to discrete values. Furthermore, the interaction of all existing
methods is restricted to deciding between multiple proposals. We therefore
propose a novel framework based on Bayesian Optimization (BO). Interactive
Bayesian Optimization (IBO) enables collaboration between machine learning
algorithms and humans. This framework captures user preferences and provides an
interface for users to shape the strategy by hand. Additionally, we&apos;ve
incorporated a new acquisition function, Preference Expected Improvement (PEI),
to refine the system&apos;s efficiency using a probabilistic model of the user
preferences. Our approach is geared towards ensuring that machines can benefit
from human expertise, aiming for a more aligned and effective learning process.
In the course of this work, we applied our method to simulations and in a real
world task using a Franka Panda robot to show human-robot collaboration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feith_N/0/1/0/all/0/1&quot;&gt;Nikolaus Feith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_E/0/1/0/all/0/1&quot;&gt;Elmar Rueckert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12667">
<title>Feature Selection via Robust Weighted Score for High Dimensional Binary Class-Imbalanced Gene Expression Data. (arXiv:2401.12667v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.12667</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, a robust weighted score for unbalanced data (ROWSU) is
proposed for selecting the most discriminative feature for high dimensional
gene expression binary classification with class-imbalance problem. The method
addresses one of the most challenging problems of highly skewed class
distributions in gene expression datasets that adversely affect the performance
of classification algorithms. First, the training dataset is balanced by
synthetically generating data points from minority class observations. Second,
a minimum subset of genes is selected using a greedy search approach. Third, a
novel weighted robust score, where the weights are computed by support vectors,
is introduced to obtain a refined set of genes. The highest-scoring genes based
on this approach are combined with the minimum subset of genes selected by the
greedy search approach to form the final set of genes. The novel method ensures
the selection of the most discriminative genes, even in the presence of skewed
class distribution, thus improving the performance of the classifiers. The
performance of the proposed ROWSU method is evaluated on $6$ gene expression
datasets. Classification accuracy and sensitivity are used as performance
metrics to compare the proposed ROWSU algorithm with several other
state-of-the-art methods. Boxplots and stability plots are also constructed for
a better understanding of the results. The results show that the proposed
method outperforms the existing feature selection procedures based on
classification performance from k nearest neighbours (kNN) and random forest
(RF) classifiers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Khan_Z/0/1/0/all/0/1&quot;&gt;Zardad Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ali_A/0/1/0/all/0/1&quot;&gt;Amjad Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aldahmani_S/0/1/0/all/0/1&quot;&gt;Saeed Aldahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12681">
<title>Non-Neighbors Also Matter to Kriging: A New Contrastive-Prototypical Learning. (arXiv:2401.12681v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12681</link>
<description rdf:parseType="Literal">&lt;p&gt;Kriging aims at estimating the attributes of unsampled geo-locations from
observations in the spatial vicinity or physical connections, which helps
mitigate skewed monitoring caused by under-deployed sensors. Existing works
assume that neighbors&apos; information offers the basis for estimating the
attributes of the unobserved target while ignoring non-neighbors. However,
non-neighbors could also offer constructive information, and neighbors could
also be misleading. To this end, we propose ``Contrastive-Prototypical&apos;&apos;
self-supervised learning for Kriging (KCP) to refine valuable information from
neighbors and recycle the one from non-neighbors. As a pre-trained paradigm, we
conduct the Kriging task from a new perspective of representation: we aim to
first learn robust and general representations and then recover attributes from
representations. A neighboring contrastive module is designed that coarsely
learns the representations by narrowing the representation distance between the
target and its neighbors while pushing away the non-neighbors. In parallel, a
prototypical module is introduced to identify similar representations via
exchanged prediction, thus refining the misleading neighbors and recycling the
useful non-neighbors from the neighboring contrast component. As a result, not
all the neighbors and some of the non-neighbors will be used to infer the
target. To encourage the two modules above to learn general and robust
representations, we design an adaptive augmentation module that incorporates
data-driven attribute augmentation and centrality-based topology augmentation
over the spatiotemporal Kriging graph data. Extensive experiments on real-world
datasets demonstrate the superior performance of KCP compared to its peers with
6% improvements and exceptional transferability and robustness. The code is
available at https://github.com/bonaldli/KCP
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhishuai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_Y/0/1/0/all/0/1&quot;&gt;Yunhao Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yisheng Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12683">
<title>LLpowershap: Logistic Loss-based Automated Shapley Values Feature Selection Method. (arXiv:2401.12683v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12683</link>
<description rdf:parseType="Literal">&lt;p&gt;Shapley values have been used extensively in machine learning, not only to
explain black box machine learning models, but among other tasks, also to
conduct model debugging, sensitivity and fairness analyses and to select
important features for robust modelling and for further follow-up analyses.
Shapley values satisfy certain axioms that promote fairness in distributing
contributions of features toward prediction or reducing error, after accounting
for non-linear relationships and interactions when complex machine learning
models are employed. Recently, a number of feature selection methods utilising
Shapley values have been introduced. Here, we present a novel feature selection
method, LLpowershap, which makes use of loss-based Shapley values to identify
informative features with minimal noise among the selected sets of features.
Our simulation results show that LLpowershap not only identifies higher number
of informative features but outputs fewer noise features compared to other
state-of-the-art feature selection methods. Benchmarking results on four
real-world datasets demonstrate higher or at par predictive performance of
LLpowershap compared to other Shapley based wrapper methods, or filter methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madakkatel_I/0/1/0/all/0/1&quot;&gt;Iqbal Madakkatel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hypponen_E/0/1/0/all/0/1&quot;&gt;Elina Hypp&amp;#xf6;nen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12686">
<title>Learning Mean Field Games on Sparse Graphs: A Hybrid Graphex Approach. (arXiv:2401.12686v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2401.12686</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning the behavior of large agent populations is an important task for
numerous research areas. Although the field of multi-agent reinforcement
learning (MARL) has made significant progress towards solving these systems,
solutions for many agents often remain computationally infeasible and lack
theoretical guarantees. Mean Field Games (MFGs) address both of these issues
and can be extended to Graphon MFGs (GMFGs) to include network structures
between agents. Despite their merits, the real world applicability of GMFGs is
limited by the fact that graphons only capture dense graphs. Since most
empirically observed networks show some degree of sparsity, such as power law
graphs, the GMFG framework is insufficient for capturing these network
topologies. Thus, we introduce the novel concept of Graphex MFGs (GXMFGs) which
builds on the graph theoretical concept of graphexes. Graphexes are the
limiting objects to sparse graph sequences that also have other desirable
features such as the small world property. Learning equilibria in these games
is challenging due to the rich and sparse structure of the underlying graphs.
To tackle these challenges, we design a new learning algorithm tailored to the
GXMFG setup. This hybrid graphex learning approach leverages that the system
mainly consists of a highly connected core and a sparse periphery. After
defining the system and providing a theoretical analysis, we state our learning
approach and demonstrate its learning capabilities on both synthetic graphs and
real-world networks. This comparison shows that our GXMFG learning algorithm
successfully extends MFGs to a highly relevant class of hard, realistic
learning problems that are not accurately addressed by current MARL and MFG
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabian_C/0/1/0/all/0/1&quot;&gt;Christian Fabian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kai Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koeppl_H/0/1/0/all/0/1&quot;&gt;Heinz Koeppl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12687">
<title>DVL Calibration using Data-driven Methods. (arXiv:2401.12687v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12687</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous underwater vehicles (AUVs) are used in a wide range of underwater
applications, ranging from seafloor mapping to industrial operations. While
underwater, the AUV navigation solution commonly relies on the fusion between
inertial sensors and Doppler velocity logs (DVL). To achieve accurate DVL
measurements a calibration procedure should be conducted before the mission
begins. Model-based calibration approaches include filtering approaches
utilizing global navigation satellite system signals. In this paper, we propose
an end-to-end deep-learning framework for the calibration procedure. Using
stimulative data, we show that our proposed approach outperforms model-based
approaches by 35% in accuracy and 80% in the required calibration time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yampolsky_Z/0/1/0/all/0/1&quot;&gt;Zeev Yampolsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_I/0/1/0/all/0/1&quot;&gt;Itzik Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12689">
<title>Energy-based Automated Model Evaluation. (arXiv:2401.12689v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12689</link>
<description rdf:parseType="Literal">&lt;p&gt;The conventional evaluation protocols on machine learning models rely heavily
on a labeled, i.i.d-assumed testing dataset, which is not often present in real
world applications. The Automated Model Evaluation (AutoEval) shows an
alternative to this traditional workflow, by forming a proximal prediction
pipeline of the testing performance without the presence of ground-truth
labels. Despite its recent successes, the AutoEval frameworks still suffer from
an overconfidence issue, substantial storage and computational cost. In that
regard, we propose a novel measure -- Meta-Distribution Energy (MDE) -- that
allows the AutoEval framework to be both more efficient and effective. The core
of the MDE is to establish a meta-distribution statistic, on the information
(energy) associated with individual samples, then offer a smoother
representation enabled by energy-based learning. We further provide our
theoretical insights by connecting the MDE with the classification loss. We
provide extensive experiments across modalities, datasets and different
architectural backbones to validate MDE&apos;s validity, together with its
superiority compared with prior approaches. We also prove MDE&apos;s versatility by
showing its seamless integration with large-scale models, and easy adaption to
learning scenarios with noisy- or imbalanced- labels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_R/0/1/0/all/0/1&quot;&gt;Ru Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Heming Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haobo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yawen Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zenan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junbo Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12708">
<title>Deep Neural Network Benchmarks for Selective Classification. (arXiv:2401.12708v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12708</link>
<description rdf:parseType="Literal">&lt;p&gt;With the increasing deployment of machine learning models in many
socially-sensitive tasks, there is a growing demand for reliable and
trustworthy predictions. One way to accomplish these requirements is to allow a
model to abstain from making a prediction when there is a high risk of making
an error. This requires adding a selection mechanism to the model, which
selects those examples for which the model will provide a prediction. The
selective classification framework aims to design a mechanism that balances the
fraction of rejected predictions (i.e., the proportion of examples for which
the model does not make a prediction) versus the improvement in predictive
performance on the selected predictions. Multiple selective classification
frameworks exist, most of which rely on deep neural network architectures.
However, the empirical evaluation of the existing approaches is still limited
to partial comparisons among methods and settings, providing practitioners with
little insight into their relative merits. We fill this gap by benchmarking 18
baselines on a diverse set of 44 datasets that includes both image and tabular
data. Moreover, there is a mix of binary and multiclass tasks. We evaluate
these approaches using several criteria, including selective error rate,
empirical coverage, distribution of rejected instance&apos;s classes, and
performance on out-of-distribution instances. The results indicate that there
is not a single clear winner among the surveyed baselines, and the best method
depends on the users&apos; objectives.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pugnana_A/0/1/0/all/0/1&quot;&gt;Andrea Pugnana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perini_L/0/1/0/all/0/1&quot;&gt;Lorenzo Perini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davis_J/0/1/0/all/0/1&quot;&gt;Jesse Davis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruggieri_S/0/1/0/all/0/1&quot;&gt;Salvatore Ruggieri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12711">
<title>When Redundancy Matters: Machine Teaching of Representations. (arXiv:2401.12711v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12711</link>
<description rdf:parseType="Literal">&lt;p&gt;In traditional machine teaching, a teacher wants to teach a concept to a
learner, by means of a finite set of examples, the witness set. But concepts
can have many equivalent representations. This redundancy strongly affects the
search space, to the extent that teacher and learner may not be able to easily
determine the equivalence class of each representation. In this common
situation, instead of teaching concepts, we explore the idea of teaching
representations. We work with several teaching schemas that exploit
representation and witness size (Eager, Greedy and Optimal) and analyze the
gains in teaching effectiveness for some representational languages (DNF
expressions and Turing-complete P3 programs). Our theoretical and experimental
results indicate that there are various types of redundancy, handled better by
the Greedy schema introduced here than by the Eager schema, although both can
be arbitrarily far away from the Optimal. For P3 programs we found that witness
sets are usually smaller than the programs they identify, which is an
illuminating justification of why machine teaching from examples makes sense at
all.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferri_C/0/1/0/all/0/1&quot;&gt;C&amp;#xe8;sar Ferri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garigliotti_D/0/1/0/all/0/1&quot;&gt;Dario Garigliotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haavardstun_B/0/1/0/all/0/1&quot;&gt;Brigt Arve Toppe H&amp;#xe5;vardstun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hernandez_Orallo_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe8; Hern&amp;#xe1;ndez-Orallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telle_J/0/1/0/all/0/1&quot;&gt;Jan Arne Telle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12717">
<title>Gas trap prediction from 3D seismic and well test data using machine learning. (arXiv:2401.12717v1 [physics.geo-ph])</title>
<link>http://arxiv.org/abs/2401.12717</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of this work is to create and apply a methodological approach for
predicting gas traps from 3D seismic data and gas well testing. The paper
formalizes the approach to creating a training dataset by selecting volumes
with established gas saturation and filtration properties within the seismic
wavefield. The training dataset thus created is used in a process stack of
sequential application of data processing methods and ensemble machine learning
algorithms. As a result, a cube of calibrated probabilities of belonging of the
study space to gas reservoirs was obtained. The high efficiency of this
approach is shown on a delayed test sample of three wells (blind wells). The
final value of the gas reservoir prediction quality metric f1 score was
0.893846.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ivlev_D/0/1/0/all/0/1&quot;&gt;Dmitry Ivlev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12722">
<title>Falcon: Fair Active Learning using Multi-armed Bandits. (arXiv:2401.12722v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12722</link>
<description rdf:parseType="Literal">&lt;p&gt;Biased data can lead to unfair machine learning models, highlighting the
importance of embedding fairness at the beginning of data analysis,
particularly during dataset curation and labeling. In response, we propose
Falcon, a scalable fair active learning framework. Falcon adopts a data-centric
approach that improves machine learning model fairness via strategic sample
selection. Given a user-specified group fairness measure, Falcon identifies
samples from &quot;target groups&quot; (e.g., (attribute=female, label=positive)) that
are the most informative for improving fairness. However, a challenge arises
since these target groups are defined using ground truth labels that are not
available during sample selection. To handle this, we propose a novel
trial-and-error method, where we postpone using a sample if the predicted label
is different from the expected one and falls outside the target group. We also
observe the trade-off that selecting more informative samples results in higher
likelihood of postponing due to undesired label prediction, and the optimal
balance varies per dataset. We capture the trade-off between informativeness
and postpone rate as policies and propose to automatically select the best
policy using adversarial multi-armed bandit methods, given their computational
efficiency and theoretical guarantees. Experiments show that Falcon
significantly outperforms existing fair active learning approaches in terms of
fairness and accuracy and is more efficient. In particular, only Falcon
supports a proper trade-off between accuracy and fairness where its maximum
fairness score is 1.8-4.5x higher than the second-best results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tae_K/0/1/0/all/0/1&quot;&gt;Ki Hyun Tae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hantian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jaeyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_K/0/1/0/all/0/1&quot;&gt;Kexin Rong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whang_S/0/1/0/all/0/1&quot;&gt;Steven Euijong Whang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12729">
<title>Enhancing Object Detection Performance for Small Objects through Synthetic Data Generation and Proportional Class-Balancing Technique: A Comparative Study in Industrial Scenarios. (arXiv:2401.12729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12729</link>
<description rdf:parseType="Literal">&lt;p&gt;Object Detection (OD) has proven to be a significant computer vision method
in extracting localized class information and has multiple applications in the
industry. Although many of the state-of-the-art (SOTA) OD models perform well
on medium and large sized objects, they seem to under perform on small objects.
In most of the industrial use cases, it is difficult to collect and annotate
data for small objects, as it is time-consuming and prone to human errors.
Additionally, those datasets are likely to be unbalanced and often result in an
inefficient model convergence. To tackle this challenge, this study presents a
novel approach that injects additional data points to improve the performance
of the OD models. Using synthetic data generation, the difficulties in data
collection and annotations for small object data points can be minimized and to
create a dataset with balanced distribution. This paper discusses the effects
of a simple proportional class-balancing technique, to enable better anchor
matching of the OD models. A comparison was carried out on the performances of
the SOTA OD models: YOLOv5, YOLOv7 and SSD, for combinations of real and
synthetic datasets within an industrial use case.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antony_J/0/1/0/all/0/1&quot;&gt;Jibinraj Antony&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hegiste_V/0/1/0/all/0/1&quot;&gt;Vinit Hegiste&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nazeri_A/0/1/0/all/0/1&quot;&gt;Ali Nazeri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tavakoli_H/0/1/0/all/0/1&quot;&gt;Hooman Tavakoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walunj_S/0/1/0/all/0/1&quot;&gt;Snehal Walunj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plociennik_C/0/1/0/all/0/1&quot;&gt;Christiane Plociennik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruskowski_M/0/1/0/all/0/1&quot;&gt;Martin Ruskowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12731">
<title>The Distributional Uncertainty of the SHAP score in Explainable Machine Learning. (arXiv:2401.12731v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12731</link>
<description rdf:parseType="Literal">&lt;p&gt;Attribution scores reflect how important the feature values in an input
entity are for the output of a machine learning model. One of the most popular
attribution scores is the SHAP score, which is an instantiation of the general
Shapley value used in coalition game theory. The definition of this score
relies on a probability distribution on the entity population. Since the exact
distribution is generally unknown, it needs to be assigned subjectively or be
estimated from data, which may lead to misleading feature scores. In this
paper, we propose a principled framework for reasoning on SHAP scores under
unknown entity population distributions. In our framework, we consider an
uncertainty region that contains the potential distributions, and the SHAP
score of a feature becomes a function defined over this region. We study the
basic problems of finding maxima and minima of this function, which allows us
to determine tight ranges for the SHAP scores of all features. In particular,
we pinpoint the complexity of these problems, and other related ones, showing
them to be NP-complete. Finally, we present experiments on a real-world
dataset, showing that our framework may contribute to a more robust feature
scoring.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cifuentes_S/0/1/0/all/0/1&quot;&gt;Santiago Cifuentes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertossi_L/0/1/0/all/0/1&quot;&gt;Leopoldo Bertossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pardal_N/0/1/0/all/0/1&quot;&gt;Nina Pardal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abriola_S/0/1/0/all/0/1&quot;&gt;Sergio Abriola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_M/0/1/0/all/0/1&quot;&gt;Maria Vanina Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_M/0/1/0/all/0/1&quot;&gt;Miguel Romero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12733">
<title>TNANet: A Temporal-Noise-Aware Neural Network for Suicidal Ideation Prediction with Noisy Physiological Data. (arXiv:2401.12733v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.12733</link>
<description rdf:parseType="Literal">&lt;p&gt;The robust generalization of deep learning models in the presence of inherent
noise remains a significant challenge, especially when labels are subjective
and noise is indiscernible in natural settings. This problem is particularly
pronounced in many practical applications. In this paper, we address a special
and important scenario of monitoring suicidal ideation, where time-series data,
such as photoplethysmography (PPG), is susceptible to such noise. Current
methods predominantly focus on image and text data or address artificially
introduced noise, neglecting the complexities of natural noise in time-series
analysis. To tackle this, we introduce a novel neural network model tailored
for analyzing noisy physiological time-series data, named TNANet, which merges
advanced encoding techniques with confidence learning, enhancing prediction
accuracy. Another contribution of our work is the collection of a specialized
dataset of PPG signals derived from real-world environments for suicidal
ideation prediction. Employing this dataset, our TNANet achieves the prediction
accuracy of 63.33% in a binary classification task, outperforming
state-of-the-art models. Furthermore, comprehensive evaluations were conducted
on three other well-known public datasets with artificially introduced noise to
rigorously test the TNANet&apos;s capabilities. These tests consistently
demonstrated TNANet&apos;s superior performance by achieving an accuracy improvement
of more than 10% compared to baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Niqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_W/0/1/0/all/0/1&quot;&gt;Wenqi Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xinxin Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Guozhen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1&quot;&gt;Wenting Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong-Jin Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12745">
<title>On the Utility of Probing Trajectories for Algorithm-Selection. (arXiv:2401.12745v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12745</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine-learning approaches to algorithm-selection typically take data
describing an instance as input. Input data can take the form of features
derived from the instance description or fitness landscape, or can be a direct
representation of the instance itself, i.e. an image or textual description.
Regardless of the choice of input, there is an implicit assumption that
instances that are similar will elicit similar performance from algorithm, and
that a model is capable of learning this relationship. We argue that viewing
algorithm-selection purely from an instance perspective can be misleading as it
fails to account for how an algorithm `views&apos; similarity between instances. We
propose a novel `algorithm-centric&apos; method for describing instances that can be
used to train models for algorithm-selection: specifically, we use short
probing trajectories calculated by applying a solver to an instance for a very
short period of time. The approach is demonstrated to be promising, providing
comparable or better results to computationally expensive landscape-based
feature-based approaches. Furthermore, projecting the trajectories into a
2-dimensional space illustrates that functions that are similar from an
algorithm-perspective do not necessarily correspond to the accepted
categorisation of these functions from a human perspective.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Renau_Q/0/1/0/all/0/1&quot;&gt;Quentin Renau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hart_E/0/1/0/all/0/1&quot;&gt;Emma Hart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12764">
<title>Fast Nonlinear Two-Time-Scale Stochastic Approximation: Achieving $\mathcal{O}(1/k)$ Finite-Sample Complexity. (arXiv:2401.12764v1 [math.OC])</title>
<link>http://arxiv.org/abs/2401.12764</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes to develop a new variant of the two-time-scale stochastic
approximation to find the roots of two coupled nonlinear operators, assuming
only noisy samples of these operators can be observed. Our key idea is to
leverage the classic Ruppert-Polyak averaging technique to dynamically estimate
the operators through their samples. The estimated values of these averaging
steps will then be used in the two-time-scale stochastic approximation updates
to find the desired solution. Our main theoretical result is to show that under
the strongly monotone condition of the underlying nonlinear operators the
mean-squared errors of the iterates generated by the proposed method converge
to zero at an optimal rate $\mathcal{O}(1/k)$, where $k$ is the number of
iterations. Our result significantly improves the existing result of
two-time-scale stochastic approximation, where the best known finite-time
convergence rate is $\mathcal{O}(1/k^{2/3})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Doan_T/0/1/0/all/0/1&quot;&gt;Thinh T. Doan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12780">
<title>DeepRicci: Self-supervised Graph Structure-Feature Co-Refinement for Alleviating Over-squashing. (arXiv:2401.12780v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12780</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have shown great power for learning and mining
on graphs, and Graph Structure Learning (GSL) plays an important role in
boosting GNNs with a refined graph. In the literature, most GSL solutions
either primarily focus on structure refinement with task-specific supervision
(i.e., node classification), or overlook the inherent weakness of GNNs
themselves (e.g., over-squashing), resulting in suboptimal performance despite
sophisticated designs. In light of these limitations, we propose to study
self-supervised graph structure-feature co-refinement for effectively
alleviating the issue of over-squashing in typical GNNs. In this paper, we take
a fundamentally different perspective of the Ricci curvature in Riemannian
geometry, in which we encounter the challenges of modeling, utilizing and
computing Ricci curvature. To tackle these challenges, we present a
self-supervised Riemannian model, DeepRicci. Specifically, we introduce a
latent Riemannian space of heterogeneous curvatures to model various Ricci
curvatures, and propose a gyrovector feature mapping to utilize Ricci curvature
for typical GNNs. Thereafter, we refine node features by geometric contrastive
learning among different geometric views, and simultaneously refine graph
structure by backward Ricci flow based on a novel formulation of differentiable
Ricci curvature. Finally, extensive experiments on public datasets show the
superiority of DeepRicci, and the connection between backward Ricci flow and
over-squashing. Codes of our work are given in https://github.com/RiemanGraph/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Li Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhenhao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hua Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Junda Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Hao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_P/0/1/0/all/0/1&quot;&gt;Philip S. Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12783">
<title>A Review of Deep Learning Methods for Photoplethysmography Data. (arXiv:2401.12783v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12783</link>
<description rdf:parseType="Literal">&lt;p&gt;Photoplethysmography (PPG) is a highly promising device due to its advantages
in portability, user-friendly operation, and non-invasive capabilities to
measure a wide range of physiological information. Recent advancements in deep
learning have demonstrated remarkable outcomes by leveraging PPG signals for
tasks related to personal health management and other multifaceted
applications. In this review, we systematically reviewed papers that applied
deep learning models to process PPG data between January 1st of 2017 and July
31st of 2023 from Google Scholar, PubMed and Dimensions. Each paper is analyzed
from three key perspectives: tasks, models, and data. We finally extracted 193
papers where different deep learning frameworks were used to process PPG
signals. Based on the tasks addressed in these papers, we categorized them into
two major groups: medical-related, and non-medical-related. The medical-related
tasks were further divided into seven subgroups, including blood pressure
analysis, cardiovascular monitoring and diagnosis, sleep health, mental health,
respiratory monitoring and analysis, blood glucose analysis, as well as others.
The non-medical-related tasks were divided into four subgroups, which encompass
signal processing, biometric identification, electrocardiogram reconstruction,
and human activity recognition. In conclusion, significant progress has been
made in the field of using deep learning methods to process PPG data recently.
This allows for a more thorough exploration and utilization of the information
contained in PPG signals. However, challenges remain, such as limited quantity
and quality of publicly available databases, a lack of effective validation in
real-world scenarios, and concerns about the interpretability, scalability, and
complexity of deep learning models. Moreover, there are still emerging research
areas that require further investigation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_G/0/1/0/all/0/1&quot;&gt;Guangkun Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiabao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_G/0/1/0/all/0/1&quot;&gt;Gongzheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Deyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geng_S/0/1/0/all/0/1&quot;&gt;Shijia Geng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qinghao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_S/0/1/0/all/0/1&quot;&gt;Shenda Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12790">
<title>MORPH: Towards Automated Concept Drift Adaptation for Malware Detection. (arXiv:2401.12790v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12790</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept drift is a significant challenge for malware detection, as the
performance of trained machine learning models degrades over time, rendering
them impractical. While prior research in malware concept drift adaptation has
primarily focused on active learning, which involves selecting representative
samples to update the model, self-training has emerged as a promising approach
to mitigate concept drift. Self-training involves retraining the model using
pseudo labels to adapt to shifting data distributions. In this research, we
propose MORPH -- an effective pseudo-label-based concept drift adaptation
method specifically designed for neural networks. Through extensive
experimental analysis of Android and Windows malware datasets, we demonstrate
the efficacy of our approach in mitigating the impact of concept drift. Our
method offers the advantage of reducing annotation efforts when combined with
active learning. Furthermore, our method significantly improves over existing
works in automated concept drift adaptation for malware detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_M/0/1/0/all/0/1&quot;&gt;Md Tanvirul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fieblinger_R/0/1/0/all/0/1&quot;&gt;Romy Fieblinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahara_A/0/1/0/all/0/1&quot;&gt;Ashim Mahara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_N/0/1/0/all/0/1&quot;&gt;Nidhi Rastogi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12800">
<title>Deep Learning in Physical Layer: Review on Data Driven End-to-End Communication Systems and their Enabling Semantic Applications. (arXiv:2401.12800v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.12800</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Learning (DL) has enabled a paradigm shift in wireless communication
system with data driven end-to-end (E2E) learning and optimization of the
Physical Layer (PHY). By leveraging the representation learning of DL, E2E
systems exhibit enhanced adaptability and performance in complex wireless
environments, fulfilling the demands of 5G and beyond network systems and
applications. The evolution of data-driven techniques in the PHY has enabled
advanced semantic applications across various modalities including text, image,
audio, video, and multi-modal transmissions. These applications transcend from
traditional bit-level communication to semantic-level intelligent communication
systems, which are capable of understanding and adapting to the context and
intent of the data transmission. Although PHY as a DL architecture for
data-driven E2E communication is a key factor in enabling semantic
communication systems (SemCom), and various studies in recent years have
surveyed them separately, their combination has not been thoroughly reviewed.
Additionally, these are emerging fields that are still in their infancy, with
several techniques having been developed and evolved in recent years.
Therefore, this article provides a holistic review of data-driven PHY for E2E
communication system, and their enabling semantic applications across different
modalities. Furthermore, it identifies critical challenges and prospective
research directions, providing a pivotal reference for future development of DL
in PHY and SemCom.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_N/0/1/0/all/0/1&quot;&gt;Nazmul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Seokjoo Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12801">
<title>Deep Learning-based Target-To-User Association in Integrated Sensing and Communication Systems. (arXiv:2401.12801v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.12801</link>
<description rdf:parseType="Literal">&lt;p&gt;In Integrated Sensing and Communication (ISAC) systems, matching the radar
targets with communication user equipments (UEs) is functional to several
communication tasks, such as proactive handover and beam prediction. In this
paper, we consider a radar-assisted communication system where a base station
(BS) is equipped with a multiple-input-multiple-output (MIMO) radar that has a
double aim: (i) associate vehicular radar targets to vehicular equipments (VEs)
in the communication beamspace and (ii) predict the beamforming vector for each
VE from radar data. The proposed target-to-user (T2U) association consists of
two stages. First, vehicular radar targets are detected from range-angle
images, and, for each, a beamforming vector is estimated. Then, the inferred
per-target beamforming vectors are matched with the ones utilized at the BS for
communication to perform target-to-user (T2U) association. Joint multi-target
detection and beam inference is obtained by modifying the you only look once
(YOLO) model, which is trained over simulated range-angle radar images.
Simulation results over different urban vehicular mobility scenarios show that
the proposed T2U method provides a probability of correct association that
increases with the size of the BS antenna array, highlighting the respective
increase of the separability of the VEs in the beamspace. Moreover, we show
that the modified YOLO architecture can effectively perform both beam
prediction and radar target detection, with similar performance in mean average
precision on the latter over different antenna array sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cazzella_L/0/1/0/all/0/1&quot;&gt;Lorenzo Cazzella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mizmizi_M/0/1/0/all/0/1&quot;&gt;Marouan Mizmizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tagliaferri_D/0/1/0/all/0/1&quot;&gt;Dario Tagliaferri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badini_D/0/1/0/all/0/1&quot;&gt;Damiano Badini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matteucci_M/0/1/0/all/0/1&quot;&gt;Matteo Matteucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spagnolini_U/0/1/0/all/0/1&quot;&gt;Umberto Spagnolini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12803">
<title>Enhancements for 5G NR PRACH Reception: An AI/ML Approach. (arXiv:2401.12803v1 [cs.IT])</title>
<link>http://arxiv.org/abs/2401.12803</link>
<description rdf:parseType="Literal">&lt;p&gt;Random Access is an important step in enabling the initial attachment of a
User Equipment (UE) to a Base Station (gNB). The UE identifies itself by
embedding a Preamble Index (RAPID) in the phase rotation of a known base
sequence, which it transmits on the Physical Random Access Channel (PRACH). The
signal on the PRACH also enables the estimation of propagation delay, often
known as Timing Advance (TA), which is induced by virtue of the UE&apos;s position.
Traditional receivers estimate the RAPID and TA using correlation-based
techniques. This paper presents an alternative receiver approach that uses
AI/ML models, wherein two neural networks are proposed, one for the RAPID and
one for the TA. Different from other works, these two models can run in
parallel as opposed to sequentially. Experiments with both simulated data and
over-the-air hardware captures highlight the improved performance of the
proposed AI/ML-based techniques compared to conventional correlation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rohit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerrapragada_A/0/1/0/all/0/1&quot;&gt;Anil Kumar Yerrapragada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_J/0/1/0/all/0/1&quot;&gt;Jeeva Keshav S&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganti_R/0/1/0/all/0/1&quot;&gt;Radha Krishna Ganti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12806">
<title>Binary structured physics-informed neural networks for solving equations with rapidly changing solutions. (arXiv:2401.12806v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12806</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural networks (PINNs), rooted in deep learning, have
emerged as a promising approach for solving partial differential equations
(PDEs). By embedding the physical information described by PDEs into
feedforward neural networks, PINNs are trained as surrogate models to
approximate solutions without the need for label data. Nevertheless, even
though PINNs have shown remarkable performance, they can face difficulties,
especially when dealing with equations featuring rapidly changing solutions.
These difficulties encompass slow convergence, susceptibility to becoming
trapped in local minima, and reduced solution accuracy. To address these
issues, we propose a binary structured physics-informed neural network (BsPINN)
framework, which employs binary structured neural network (BsNN) as the neural
network component. By leveraging a binary structure that reduces inter-neuron
connections compared to fully connected neural networks, BsPINNs excel in
capturing the local features of solutions more effectively and efficiently.
These features are particularly crucial for learning the rapidly changing in
the nature of solutions. In a series of numerical experiments solving Burgers
equation, Euler equation, Helmholtz equation, and high-dimension Poisson
equation, BsPINNs exhibit superior convergence speed and heightened accuracy
compared to PINNs. From these experiments, we discover that BsPINNs resolve the
issues caused by increased hidden layers in PINNs resulting in over-smoothing,
and prevent the decline in accuracy due to non-smoothness of PDEs solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanzhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_R/0/1/0/all/0/1&quot;&gt;Ruifan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Ying Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12819">
<title>Dynamic Layer Tying for Parameter-Efficient Transformers. (arXiv:2401.12819v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12819</link>
<description rdf:parseType="Literal">&lt;p&gt;In the pursuit of reducing the number of trainable parameters in deep
transformer networks, we employ Reinforcement Learning to dynamically select
layers during training and tie them together. Every few iterations, the RL
agent is asked whether to train each layer $i$ independently or to copy the
weights of a previous layer $j&amp;lt;i$. This facilitates weight sharing, reduces the
number of trainable parameters, and also serves as an effective regularization
technique. Experimental evaluations validate that our model modestly
outperforms the baseline transformer model with regard to perplexity and
drastically reduces the number of trainable parameters. In particular, the
memory consumption during training is up to one order of magnitude less than
the conventional training method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_T/0/1/0/all/0/1&quot;&gt;Tamir David Hay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lior Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12820">
<title>DatUS^2: Data-driven Unsupervised Semantic Segmentation with Pre-trained Self-supervised Vision Transformer. (arXiv:2401.12820v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12820</link>
<description rdf:parseType="Literal">&lt;p&gt;Successive proposals of several self-supervised training schemes continue to
emerge, taking one step closer to developing a universal foundation model. In
this process, the unsupervised downstream tasks are recognized as one of the
evaluation methods to validate the quality of visual features learned with a
self-supervised training scheme. However, unsupervised dense semantic
segmentation has not been explored as a downstream task, which can utilize and
evaluate the quality of semantic information introduced in patch-level feature
representations during self-supervised training of a vision transformer.
Therefore, this paper proposes a novel data-driven approach for unsupervised
semantic segmentation (DatUS^2) as a downstream task. DatUS^2 generates
semantically consistent and dense pseudo annotate segmentation masks for the
unlabeled image dataset without using any visual-prior or synchronized data. We
compare these pseudo-annotated segmentation masks with ground truth masks for
evaluating recent self-supervised training schemes to learn shared semantic
properties at the patch level and discriminative semantic properties at the
segment level. Finally, we evaluate existing state-of-the-art self-supervised
training schemes with our proposed downstream task, i.e., DatUS^2. Also, the
best version of DatUS^2 outperforms the existing state-of-the-art method for
the unsupervised dense semantic segmentation task with 15.02% MiOU and 21.47%
Pixel accuracy on the SUIM dataset. It also achieves a competitive level of
accuracy for a large-scale and complex dataset, i.e., the COCO dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sonal Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sur_A/0/1/0/all/0/1&quot;&gt;Arijit Sur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baruah_R/0/1/0/all/0/1&quot;&gt;Rashmi Dutta Baruah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12822">
<title>Deep Learning Based Simulators for the Phosphorus Removal Process Control in Wastewater Treatment via Deep Reinforcement Learning Algorithms. (arXiv:2401.12822v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2401.12822</link>
<description rdf:parseType="Literal">&lt;p&gt;Phosphorus removal is vital in wastewater treatment to reduce reliance on
limited resources. Deep reinforcement learning (DRL) is a machine learning
technique that can optimize complex and nonlinear systems, including the
processes in wastewater treatment plants, by learning control policies through
trial and error. However, applying DRL to chemical and biological processes is
challenging due to the need for accurate simulators. This study trained six
models to identify the phosphorus removal process and used them to create a
simulator for the DRL environment. Although the models achieved high accuracy
(&amp;gt;97%), uncertainty and incorrect prediction behavior limited their performance
as simulators over longer horizons. Compounding errors in the models&apos;
predictions were identified as one of the causes of this problem. This approach
for improving process control involves creating simulation environments for DRL
algorithms, using data from supervisory control and data acquisition (SCADA)
systems with a sufficient historical horizon without complex system modeling or
parameter estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mohammadi_E/0/1/0/all/0/1&quot;&gt;Esmaeel Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stokholm_Bjerregaard_M/0/1/0/all/0/1&quot;&gt;Mikkel Stokholm-Bjerregaard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hansen_A/0/1/0/all/0/1&quot;&gt;Aviaja Anna Hansen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nielsen_P/0/1/0/all/0/1&quot;&gt;Per Halkj&amp;#xe6;r Nielsen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ortiz_Arroyo_D/0/1/0/all/0/1&quot;&gt;Daniel Ortiz-Arroyo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Durdevic_P/0/1/0/all/0/1&quot;&gt;Petar Durdevic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12824">
<title>MAPPING: Debiasing Graph Neural Networks for Fair Node Classification with Limited Sensitive Information Leakage. (arXiv:2401.12824v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12824</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite remarkable success in diverse web-based applications, Graph Neural
Networks(GNNs) inherit and further exacerbate historical discrimination and
social stereotypes, which critically hinder their deployments in high-stake
domains such as online clinical diagnosis, financial crediting, etc. However,
current fairness research that primarily craft on i.i.d data, cannot be
trivially replicated to non-i.i.d. graph structures with topological dependence
among samples. Existing fair graph learning typically favors pairwise
constraints to achieve fairness but fails to cast off dimensional limitations
and generalize them into multiple sensitive attributes; besides, most studies
focus on in-processing techniques to enforce and calibrate fairness,
constructing a model-agnostic debiasing GNN framework at the pre-processing
stage to prevent downstream misuses and improve training reliability is still
largely under-explored. Furthermore, previous work on GNNs tend to enhance
either fairness or privacy individually but few probe into their interplays. In
this paper, we propose a novel model-agnostic debiasing framework named MAPPING
(\underline{M}asking \underline{A}nd \underline{P}runing and
Message-\underline{P}assing train\underline{ING}) for fair node classification,
in which we adopt the distance covariance($dCov$)-based fairness constraints to
simultaneously reduce feature and topology biases in arbitrary dimensions, and
combine them with adversarial debiasing to confine the risks of attribute
inference attacks. Experiments on real-world datasets with different GNN
variants demonstrate the effectiveness and flexibility of MAPPING. Our results
show that MAPPING can achieve better trade-offs between utility and fairness,
and mitigate privacy risks of sensitive information leakage.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Ying Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palanisamy_B/0/1/0/all/0/1&quot;&gt;Balaji Palanisamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12830">
<title>Enhancing Next Destination Prediction: A Novel LSTM Approach Using Real-World Airline Data. (arXiv:2401.12830v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12830</link>
<description rdf:parseType="Literal">&lt;p&gt;In the modern transportation industry, accurate prediction of travelers&apos; next
destinations brings multiple benefits to companies, such as customer
satisfaction and targeted marketing. This study focuses on developing a precise
model that captures the sequential patterns and dependencies in travel data,
enabling accurate predictions of individual travelers&apos; future destinations. To
achieve this, a novel model architecture with a sliding window approach based
on Long Short-Term Memory (LSTM) is proposed for destination prediction in the
transportation industry. The experimental results highlight satisfactory
performance and high scores achieved by the proposed model across different
data sizes and performance metrics. This research contributes to advancing
destination prediction methods, empowering companies to deliver personalized
recommendations and optimize customer experiences in the dynamic travel
landscape.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salihoglu_S/0/1/0/all/0/1&quot;&gt;Salih Salihoglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koksal_G/0/1/0/all/0/1&quot;&gt;Gulser Koksal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abar_O/0/1/0/all/0/1&quot;&gt;Orhan Abar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12842">
<title>Iterated Relevance Matrix Analysis (IRMA) for the identification of class-discriminative subspaces. (arXiv:2401.12842v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12842</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce and investigate the iterated application of Generalized Matrix
Learning Vector Quantizaton for the analysis of feature relevances in
classification problems, as well as for the construction of
class-discriminative subspaces. The suggested Iterated Relevance Matrix
Analysis (IRMA) identifies a linear subspace representing the classification
specific information of the considered data sets using Generalized Matrix
Learning Vector Quantization (GMLVQ). By iteratively determining a new
discriminative subspace while projecting out all previously identified ones, a
combined subspace carrying all class-specific information can be found. This
facilitates a detailed analysis of feature relevances, and enables improved
low-dimensional representations and visualizations of labeled data sets.
Additionally, the IRMA-based class-discriminative subspace can be used for
dimensionality reduction and the training of robust classifiers with
potentially improved performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovdal_S/0/1/0/all/0/1&quot;&gt;Sofie L&amp;#xf6;vdal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biehl_M/0/1/0/all/0/1&quot;&gt;Michael Biehl&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12843">
<title>An embedding-based distance for temporal graphs. (arXiv:2401.12843v1 [cs.SI])</title>
<link>http://arxiv.org/abs/2401.12843</link>
<description rdf:parseType="Literal">&lt;p&gt;We define a distance between temporal graphs based on graph embeddings built
using time-respecting random walks. We study both the case of matched graphs,
when there exists a known relation between the nodes, and the unmatched case,
when such a relation is unavailable and the graphs may be of different sizes.
We illustrate the interest of our distance definition, using both real and
synthetic temporal network data, by showing its ability to discriminate between
graphs with different structural and temporal properties. Leveraging
state-of-the-art machine learning techniques, we propose an efficient
implementation of distance computation that is viable for large-scale temporal
graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DallAmico_L/0/1/0/all/0/1&quot;&gt;Lorenzo Dall&amp;#x27;Amico&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrat_A/0/1/0/all/0/1&quot;&gt;Alain Barrat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cattuto_C/0/1/0/all/0/1&quot;&gt;Ciro Cattuto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12849">
<title>Learning safety critics via a non-contractive binary bellman operator. (arXiv:2401.12849v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12849</link>
<description rdf:parseType="Literal">&lt;p&gt;The inability to naturally enforce safety in Reinforcement Learning (RL),
with limited failures, is a core challenge impeding its use in real-world
applications. One notion of safety of vast practical relevance is the ability
to avoid (unsafe) regions of the state space. Though such a safety goal can be
captured by an action-value-like function, a.k.a. safety critics, the
associated operator lacks the desired contraction and uniqueness properties
that the classical Bellman operator enjoys. In this work, we overcome the
non-contractiveness of safety critic operators by leveraging that safety is a
binary property. To that end, we study the properties of the binary safety
critic associated with a deterministic dynamical system that seeks to avoid
reaching an unsafe region. We formulate the corresponding binary Bellman
equation (B2E) for safety and study its properties. While the resulting
operator is still non-contractive, we fully characterize its fixed points
representing--except for a spurious solution--maximal persistently safe regions
of the state space that can always avoid failure. We provide an algorithm that,
by design, leverages axiomatic knowledge of safe data to avoid spurious fixed
points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castellano_A/0/1/0/all/0/1&quot;&gt;Agustin Castellano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_H/0/1/0/all/0/1&quot;&gt;Hancheng Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bazerque_J/0/1/0/all/0/1&quot;&gt;Juan Andr&amp;#xe9;s Bazerque&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallada_E/0/1/0/all/0/1&quot;&gt;Enrique Mallada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12851">
<title>Classification of grapevine varieties using UAV hyperspectral imaging. (arXiv:2401.12851v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.12851</link>
<description rdf:parseType="Literal">&lt;p&gt;The classification of different grapevine varieties is a relevant phenotyping
task in Precision Viticulture since it enables estimating the growth of
vineyard rows dedicated to different varieties, among other applications
concerning the wine industry. This task can be performed with destructive
methods that require time-consuming tasks, including data collection and
analysis in the laboratory. However, Unmanned Aerial Vehicles (UAV) provide a
more efficient and less prohibitive approach to collecting hyperspectral data,
despite acquiring noisier data. Therefore, the first task is the processing of
these data to correct and downsample large amounts of data. In addition, the
hyperspectral signatures of grape varieties are very similar. In this work, a
Convolutional Neural Network (CNN) is proposed for classifying seventeen
varieties of red and white grape variants. Rather than classifying single
samples, these are processed together with their neighbourhood. Hence, the
extraction of spatial and spectral features is addressed with 1) a spatial
attention layer and 2) Inception blocks. The pipeline goes from processing to
dataset elaboration, finishing with the training phase. The fitted model is
evaluated in terms of response time, accuracy and data separability, and
compared with other state-of-the-art CNNs for classifying hyperspectral data.
Our network was proven to be much more lightweight with a reduced number of
input bands, a lower number of trainable weights and therefore, reduced
training time. Despite this, the evaluated metrics showed much better results
for our network (~99% overall accuracy), in comparison with previous works
barely achieving 81% OA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lopez_A/0/1/0/all/0/1&quot;&gt;Alfonso L&amp;#xf3;pez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ogayar_C/0/1/0/all/0/1&quot;&gt;Carlos Javier Ogayar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feito_F/0/1/0/all/0/1&quot;&gt;Francisco Ram&amp;#xf3;n Feito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sousa_J/0/1/0/all/0/1&quot;&gt;Joaquim Jo&amp;#xe3;o Sousa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12866">
<title>Evaluating Collaborative and Autonomous Agents in Data-Stream-Supported Coordination of Mobile Crowdsourcing. (arXiv:2401.12866v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2401.12866</link>
<description rdf:parseType="Literal">&lt;p&gt;Mobile crowdsourcing refers to systems where the completion of tasks
necessarily requires physical movement of crowdworkers in an on-demand
workforce. Evidence suggests that in such systems, tasks often get assigned to
crowdworkers who struggle to complete those tasks successfully, resulting in
high failure rates and low service quality. A promising solution to ensure
higher quality of service is to continuously adapt the assignment and respond
to failure-causing events by transferring tasks to better-suited workers who
use different routes or vehicles. However, implementing task transfers in
mobile crowdsourcing is difficult because workers are autonomous and may reject
transfer requests. Moreover, task outcomes are uncertain and need to be
predicted. In this paper, we propose different mechanisms to achieve outcome
prediction and task coordination in mobile crowdsourcing. First, we analyze
different data stream learning approaches for the prediction of task outcomes.
Second, based on the suggested prediction model, we propose and evaluate two
different approaches for task coordination with different degrees of autonomy:
an opportunistic approach for crowdshipping with collaborative, but
non-autonomous workers, and a market-based model with autonomous workers for
crowdsensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruns_R/0/1/0/all/0/1&quot;&gt;Ralf Bruns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dotterl_J/0/1/0/all/0/1&quot;&gt;Jeremias D&amp;#xf6;tterl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunkel_J/0/1/0/all/0/1&quot;&gt;J&amp;#xfc;rgen Dunkel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ossowski_S/0/1/0/all/0/1&quot;&gt;Sascha Ossowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12882">
<title>Model-Free $\delta$-Policy Iteration Based on Damped Newton Method for Nonlinear Continuous-Time H$\infty$ Tracking Control. (arXiv:2401.12882v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12882</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a {\delta}-PI algorithm which is based on damped Newton
method for the H{\infty} tracking control problem of unknown continuous-time
nonlinear system. A discounted performance function and an augmented system are
used to get the tracking Hamilton-Jacobi-Isaac (HJI) equation. Tracking HJI
equation is a nonlinear partial differential equation, traditional
reinforcement learning methods for solving the tracking HJI equation are mostly
based on the Newton method, which usually only satisfies local convergence and
needs a good initial guess. Based upon the damped Newton iteration operator
equation, a generalized tracking Bellman equation is derived firstly. The
{\delta}-PI algorithm can seek the optimal solution of the tracking HJI
equation by iteratively solving the generalized tracking Bellman equation.
On-policy learning and off-policy learning {\delta}-PI reinforcement learning
methods are provided, respectively. Off-policy version {\delta}-PI algorithm is
a model-free algorithm which can be performed without making use of a priori
knowledge of the system dynamics. NN-based implementation scheme for the
off-policy {\delta}-PI algorithms is shown. The suitability of the model-free
{\delta}-PI algorithm is illustrated with a nonlinear system simulation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12923">
<title>Deep multitask neural networks for solving some stochastic optimal control problems. (arXiv:2401.12923v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.12923</link>
<description rdf:parseType="Literal">&lt;p&gt;Most existing neural network-based approaches for solving stochastic optimal
control problems using the associated backward dynamic programming principle
rely on the ability to simulate the underlying state variables. However, in
some problems, this simulation is infeasible, leading to the discretization of
state variable space and the need to train one neural network for each data
point. This approach becomes computationally inefficient when dealing with
large state variable spaces. In this paper, we consider a class of this type of
stochastic optimal control problems and introduce an effective solution
employing multitask neural networks. To train our multitask neural network, we
introduce a novel scheme that dynamically balances the learning across tasks.
Through numerical experiments on real-world derivatives pricing problems, we
prove that our method outperforms state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yeo_C/0/1/0/all/0/1&quot;&gt;Christian Yeo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12924">
<title>Performance Analysis of Support Vector Machine (SVM) on Challenging Datasets for Forest Fire Detection. (arXiv:2401.12924v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.12924</link>
<description rdf:parseType="Literal">&lt;p&gt;This article delves into the analysis of performance and utilization of
Support Vector Machines (SVMs) for the critical task of forest fire detection
using image datasets. With the increasing threat of forest fires to ecosystems
and human settlements, the need for rapid and accurate detection systems is of
utmost importance. SVMs, renowned for their strong classification capabilities,
exhibit proficiency in recognizing patterns associated with fire within images.
By training on labeled data, SVMs acquire the ability to identify distinctive
attributes associated with fire, such as flames, smoke, or alterations in the
visual characteristics of the forest area. The document thoroughly examines the
use of SVMs, covering crucial elements like data preprocessing, feature
extraction, and model training. It rigorously evaluates parameters such as
accuracy, efficiency, and practical applicability. The knowledge gained from
this study aids in the development of efficient forest fire detection systems,
enabling prompt responses and improving disaster management. Moreover, the
correlation between SVM accuracy and the difficulties presented by
high-dimensional datasets is carefully investigated, demonstrated through a
revealing case study. The relationship between accuracy scores and the
different resolutions used for resizing the training datasets has also been
discussed in this article. These comprehensive studies result in a definitive
overview of the difficulties faced and the potential sectors requiring further
improvement and focus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kar_A/0/1/0/all/0/1&quot;&gt;Ankan Kar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nath_N/0/1/0/all/0/1&quot;&gt;Nirjhar Nath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kemprai_U/0/1/0/all/0/1&quot;&gt;Utpalraj Kemprai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Aman/0/1/0/all/0/1&quot;&gt;Aman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12926">
<title>DsDm: Model-Aware Dataset Selection with Datamodels. (arXiv:2401.12926v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12926</link>
<description rdf:parseType="Literal">&lt;p&gt;When selecting data for training large-scale models, standard practice is to
filter for examples that match human notions of data quality. Such filtering
yields qualitatively clean datapoints that intuitively should improve model
behavior. However, in practice the opposite can often happen: we find that
selecting according to similarity with &quot;high quality&quot; data sources may not
increase (and can even hurt) performance compared to randomly selecting data.
&lt;/p&gt;
&lt;p&gt;To develop better methods for selecting data, we start by framing dataset
selection as an optimization problem that we can directly solve for: given
target tasks, a learning algorithm, and candidate data, select the subset that
maximizes model performance. This framework thus avoids handpicked notions of
data quality, and instead models explicitly how the learning process uses train
datapoints to predict on the target tasks. Our resulting method greatly
improves language model (LM) performance on both pre-specified tasks and
previously unseen tasks. Specifically, choosing target tasks representative of
standard LM problems and evaluating on diverse held-out benchmarks, our
selected datasets provide a 2x compute multiplier over baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Engstrom_L/0/1/0/all/0/1&quot;&gt;Logan Engstrom&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feldmann_A/0/1/0/all/0/1&quot;&gt;Axel Feldmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madry_A/0/1/0/all/0/1&quot;&gt;Aleksander Madry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12930">
<title>pyAKI - An Open Source Solution to Automated KDIGO classification. (arXiv:2401.12930v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12930</link>
<description rdf:parseType="Literal">&lt;p&gt;Acute Kidney Injury (AKI) is a frequent complication in critically ill
patients, affecting up to 50% of patients in the intensive care units. The lack
of standardized and open-source tools for applying the Kidney Disease Improving
Global Outcomes (KDIGO) criteria to time series data has a negative impact on
workload and study quality. This project introduces pyAKI, an open-source
pipeline addressing this gap by providing a comprehensive solution for
consistent KDIGO criteria implementation.
&lt;/p&gt;
&lt;p&gt;The pyAKI pipeline was developed and validated using a subset of the Medical
Information Mart for Intensive Care (MIMIC)-IV database, a commonly used
database in critical care research. We defined a standardized data model in
order to ensure reproducibility. Validation against expert annotations
demonstrated pyAKI&apos;s robust performance in implementing KDIGO criteria.
Comparative analysis revealed its ability to surpass the quality of human
labels.
&lt;/p&gt;
&lt;p&gt;This work introduces pyAKI as an open-source solution for implementing the
KDIGO criteria for AKI diagnosis using time series data with high accuracy and
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porschen_C/0/1/0/all/0/1&quot;&gt;Christian Porschen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ernsting_J/0/1/0/all/0/1&quot;&gt;Jan Ernsting&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brauckmann_P/0/1/0/all/0/1&quot;&gt;Paul Brauckmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weiss_R/0/1/0/all/0/1&quot;&gt;Raphael Weiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wurdemann_T/0/1/0/all/0/1&quot;&gt;Till W&amp;#xfc;rdemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Booke_H/0/1/0/all/0/1&quot;&gt;Hendrik Booke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amini_W/0/1/0/all/0/1&quot;&gt;Wida Amini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maidowski_L/0/1/0/all/0/1&quot;&gt;Ludwig Maidowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risse_B/0/1/0/all/0/1&quot;&gt;Benjamin Risse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hahn_T/0/1/0/all/0/1&quot;&gt;Tim Hahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Groote_T/0/1/0/all/0/1&quot;&gt;Thilo von Groote&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12934">
<title>Reward-Relevance-Filtered Linear Offline Reinforcement Learning. (arXiv:2401.12934v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2401.12934</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies offline reinforcement learning with linear function
approximation in a setting with decision-theoretic, but not estimation
sparsity. The structural restrictions of the data-generating process presume
that the transitions factor into a sparse component that affects the reward and
could affect additional exogenous dynamics that do not affect the reward.
Although the minimally sufficient adjustment set for estimation of full-state
transition properties depends on the whole state, the optimal policy and
therefore state-action value function depends only on the sparse component: we
call this causal/decision-theoretic sparsity. We develop a method for
reward-filtering the estimation of the state-action value function to the
sparse component by a modification of thresholded lasso in least-squares policy
evaluation. We provide theoretical guarantees for our reward-filtered linear
fitted-Q-iteration, with sample complexity depending only on the size of the
sparse component.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zhou_A/0/1/0/all/0/1&quot;&gt;Angela Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12950">
<title>Bayesian Semi-structured Subspace Inference. (arXiv:2401.12950v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.12950</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-structured regression models enable the joint modeling of interpretable
structured and complex unstructured feature effects. The structured model part
is inspired by statistical models and can be used to infer the input-output
relationship for features of particular importance. The complex unstructured
part defines an arbitrary deep neural network and thereby provides enough
flexibility to achieve competitive prediction performance. While these models
can also account for aleatoric uncertainty, there is still a lack of work on
accounting for epistemic uncertainty. In this paper, we address this problem by
presenting a Bayesian approximation for semi-structured regression models using
subspace inference. To this end, we extend subspace inference for joint
posterior sampling from a full parameter space for structured effects and a
subspace for unstructured effects. Apart from this hybrid sampling scheme, our
method allows for tunable complexity of the subspace and can capture multiple
minima in the loss landscape. Numerical experiments validate our approach&apos;s
efficacy in recovering structured effect parameter posteriors in
semi-structured models and approaching the full-space posterior distribution of
MCMC for increasing subspace dimension. Further, our approach exhibits
competitive predictive performance across simulated and real-world datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dold_D/0/1/0/all/0/1&quot;&gt;Daniel Dold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rugamer_D/0/1/0/all/0/1&quot;&gt;David R&amp;#xfc;gamer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Beate Sick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durr_O/0/1/0/all/0/1&quot;&gt;Oliver D&amp;#xfc;rr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12961">
<title>Chatterbox: Robust Transport for LLM Token Streaming under Unstable Network. (arXiv:2401.12961v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2401.12961</link>
<description rdf:parseType="Literal">&lt;p&gt;To render each generated token in real time, the LLM server generates
response tokens one by one and streams each generated token (or group of a few
tokens) through the network to the user right after it is generated, which we
refer to as LLM token streaming. However, under unstable network conditions,
the LLM token streaming experience could suffer greatly from stalls since one
packet loss could block the rendering of tokens contained in subsequent packets
even if they arrive on time. With a real-world measurement study, we show that
current applications including ChatGPT, Claude, and Bard all suffer from
increased stall under unstable network.
&lt;/p&gt;
&lt;p&gt;For this emerging token streaming problem in LLM Chatbots, we propose a novel
transport layer scheme, called Chatterbox, which puts new generated tokens as
well as currently unacknowledged tokens in the next outgoing packet. This
ensures that each packet contains some new tokens and can be independently
rendered when received, thus avoiding aforementioned stalls caused by missing
packets. Through simulation under various network conditions, we show
Chatterbox reduces stall ratio (proportion of token rendering wait time) by
71.0% compared to the token streaming method commonly used by real chatbot
applications and by 31.6% compared to a custom packet duplication scheme. By
tailoring Chatterbox to fit the token-by-token generation of LLM, we enable the
Chatbots to respond like an eloquent speaker for users to better enjoy
pervasive AI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hanchen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuhan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yihua Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_S/0/1/0/all/0/1&quot;&gt;Siddhant Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_K/0/1/0/all/0/1&quot;&gt;Kuntai Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junchen Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12963">
<title>AutoRT: Embodied Foundation Models for Large Scale Orchestration of Robotic Agents. (arXiv:2401.12963v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.12963</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models that incorporate language, vision, and more recently
actions have revolutionized the ability to harness internet scale data to
reason about useful tasks. However, one of the key challenges of training
embodied foundation models is the lack of data grounded in the physical world.
In this paper, we propose AutoRT, a system that leverages existing foundation
models to scale up the deployment of operational robots in completely unseen
scenarios with minimal human supervision. AutoRT leverages vision-language
models (VLMs) for scene understanding and grounding, and further uses large
language models (LLMs) for proposing diverse and novel instructions to be
performed by a fleet of robots. Guiding data collection by tapping into the
knowledge of foundation models enables AutoRT to effectively reason about
autonomy tradeoffs and safety while significantly scaling up data collection
for robot learning. We demonstrate AutoRT proposing instructions to over 20
robots across multiple buildings and collecting 77k real robot episodes via
both teleoperation and autonomous robot policies. We experimentally show that
such &quot;in-the-wild&quot; data collected by AutoRT is significantly more diverse, and
that AutoRT&apos;s use of LLMs allows for instruction following data collection
robots that can align to human preferences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_M/0/1/0/all/0/1&quot;&gt;Michael Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwibedi_D/0/1/0/all/0/1&quot;&gt;Debidatta Dwibedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arenas_M/0/1/0/all/0/1&quot;&gt;Montse Gonzalez Arenas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalakrishnan_K/0/1/0/all/0/1&quot;&gt;Keerthana Gopalakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Irpan_A/0/1/0/all/0/1&quot;&gt;Alex Irpan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_N/0/1/0/all/0/1&quot;&gt;Nikhil Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Julian_R/0/1/0/all/0/1&quot;&gt;Ryan Julian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirmani_S/0/1/0/all/0/1&quot;&gt;Sean Kirmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1&quot;&gt;Isabel Leal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_E/0/1/0/all/0/1&quot;&gt;Edward Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leal_I/0/1/0/all/0/1&quot;&gt;Isabel Leal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maddineni_S/0/1/0/all/0/1&quot;&gt;Sharath Maddineni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_K/0/1/0/all/0/1&quot;&gt;Kanishka Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanketi_P/0/1/0/all/0/1&quot;&gt;Pannag Sanketi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1&quot;&gt;Pierre Sermanet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuong_Q/0/1/0/all/0/1&quot;&gt;Quan Vuong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Welker_S/0/1/0/all/0/1&quot;&gt;Stefan Welker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Steve Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhuo Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.01135">
<title>MNL-Bandit with Knapsacks: a near-optimal algorithm. (arXiv:2106.01135v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2106.01135</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider a dynamic assortment selection problem where a seller has a fixed
inventory of $N$ substitutable products and faces an unknown demand that
arrives sequentially over $T$ periods. In each period, the seller needs to
decide on the assortment of products (satisfying certain constraints) to offer
to the customers. The customer&apos;s response follows an unknown multinomial logit
model (MNL) with parameter $\boldsymbol{v}$. If customer selects product $i \in
[N]$, the seller receives revenue $r_i$. The goal of the seller is to maximize
the total expected revenue from the $T$ customers given the fixed initial
inventory of $N$ products. We present MNLwK-UCB, a UCB-based algorithm and
characterize its regret under different regimes of inventory size. We show that
when the inventory size grows quasi-linearly in time, MNLwK-UCB achieves a
$\tilde{O}(N + \sqrt{NT})$ regret bound. We also show that for a smaller
inventory (with growth $\sim T^{\alpha}$, $\alpha &amp;lt; 1$), MNLwK-UCB achieves a
$\tilde{O}(N(1 + T^{\frac{1 - \alpha}{2}}) + \sqrt{NT})$. In particular, over a
long time horizon $T$, the rate $\tilde{O}(\sqrt{NT})$ is always achieved
regardless of the constraints and the size of the inventory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aznag_A/0/1/0/all/0/1&quot;&gt;Abdellah Aznag&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_V/0/1/0/all/0/1&quot;&gt;Vineet Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perivier_N/0/1/0/all/0/1&quot;&gt;Noemie Perivier&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.07869">
<title>DPGNN: Dual-Perception Graph Neural Network for Representation Learning. (arXiv:2110.07869v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.07869</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have drawn increasing attention in recent years
and achieved remarkable performance in many graph-based tasks, especially in
semi-supervised learning on graphs. However, most existing GNNs are based on
the message-passing paradigm to iteratively aggregate neighborhood information
in a single topology space. Despite their success, the expressive power of GNNs
is limited by some drawbacks, such as inflexibility of message source
expansion, negligence of node-level message output discrepancy, and restriction
of single message space. To address these drawbacks, we present a novel
message-passing paradigm, based on the properties of multi-step message source,
node-specific message output, and multi-space message interaction. To verify
its validity, we instantiate the new message-passing paradigm as a
Dual-Perception Graph Neural Network (DPGNN), which applies a node-to-step
attention mechanism to aggregate node-specific multi-step neighborhood
information adaptively. Our proposed DPGNN can capture the structural
neighborhood information and the feature-related information simultaneously for
graph representation learning. Experimental results on six benchmark datasets
with different topological structures demonstrate that our method outperforms
the latest state-of-the-art models, which proves the superiority and
versatility of our method. To our knowledge, we are the first to consider
node-specific message passing in the GNNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_L/0/1/0/all/0/1&quot;&gt;Li Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_D/0/1/0/all/0/1&quot;&gt;Dingyi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shaohuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wanlong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Malu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_H/0/1/0/all/0/1&quot;&gt;Hong Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.11334">
<title>Generalized Out-of-Distribution Detection: A Survey. (arXiv:2110.11334v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.11334</link>
<description rdf:parseType="Literal">&lt;p&gt;Out-of-distribution (OOD) detection is critical to ensuring the reliability
and safety of machine learning systems. For instance, in autonomous driving, we
would like the driving system to issue an alert and hand over the control to
humans when it detects unusual scenes or objects that it has never seen during
training time and cannot make a safe decision. The term, OOD detection, first
emerged in 2017 and since then has received increasing attention from the
research community, leading to a plethora of methods developed, ranging from
classification-based to density-based to distance-based ones. Meanwhile,
several other problems, including anomaly detection (AD), novelty detection
(ND), open set recognition (OSR), and outlier detection (OD), are closely
related to OOD detection in terms of motivation and methodology. Despite common
goals, these topics develop in isolation, and their subtle differences in
definition and problem setting often confuse readers and practitioners. In this
survey, we first present a unified framework called generalized OOD detection,
which encompasses the five aforementioned problems, i.e., AD, ND, OSR, OOD
detection, and OD. Under our framework, these five problems can be seen as
special cases or sub-tasks, and are easier to distinguish. We then review each
of these five areas by summarizing their recent technical developments, with a
special focus on OOD detection methodologies. We conclude this survey with open
challenges and potential research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaiyang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yixuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.11628">
<title>SkipNode: On Alleviating Performance Degradation for Deep Graph Convolutional Networks. (arXiv:2112.11628v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.11628</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Convolutional Networks (GCNs) suffer from performance degradation when
models go deeper. However, earlier works only attributed the performance
degeneration to over-smoothing. In this paper, we conduct theoretical and
experimental analysis to explore the fundamental causes of performance
degradation in deep GCNs: over-smoothing and gradient vanishing have a mutually
reinforcing effect that causes the performance to deteriorate more quickly in
deep GCNs. On the other hand, existing anti-over-smoothing methods all perform
full convolutions up to the model depth. They could not well resist the
exponential convergence of over-smoothing due to model depth increasing. In
this work, we propose a simple yet effective plug-and-play module, Skipnode, to
overcome the performance degradation of deep GCNs. It samples graph nodes in
each convolutional layer to skip the convolution operation. In this way, both
over-smoothing and gradient vanishing can be effectively suppressed since (1)
not all nodes&apos;features propagate through full layers and, (2) the gradient can
be directly passed back through ``skipped&apos;&apos; nodes. We provide both theoretical
analysis and empirical evaluation to demonstrate the efficacy of Skipnode and
its superiority over SOTA baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_W/0/1/0/all/0/1&quot;&gt;Weigang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_Y/0/1/0/all/0/1&quot;&gt;Yibing Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Binbin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Z/0/1/0/all/0/1&quot;&gt;Ziyu Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Liu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_B/0/1/0/all/0/1&quot;&gt;Baosheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.08082">
<title>Regenerative Particle Thompson Sampling. (arXiv:2203.08082v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.08082</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes regenerative particle Thompson sampling (RPTS), a
flexible variation of Thompson sampling. Thompson sampling itself is a Bayesian
heuristic for solving stochastic bandit problems, but it is hard to implement
in practice due to the intractability of maintaining a continuous posterior
distribution. Particle Thompson sampling (PTS) is an approximation of Thompson
sampling obtained by simply replacing the continuous distribution by a discrete
distribution supported at a set of weighted static particles. We observe that
in PTS, the weights of all but a few fit particles converge to zero. RPTS is
based on the heuristic: delete the decaying unfit particles and regenerate new
particles in the vicinity of fit surviving particles. Empirical evidence shows
uniform improvement from PTS to RPTS and flexibility and efficacy of RPTS
across a set of representative bandit problems, including an application to 5G
network slicing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajek_B/0/1/0/all/0/1&quot;&gt;Bruce Hajek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_N/0/1/0/all/0/1&quot;&gt;Nakjung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walid_A/0/1/0/all/0/1&quot;&gt;Anwar Walid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.13883">
<title>Multi-modal Misinformation Detection: Approaches, Challenges and Opportunities. (arXiv:2203.13883v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.13883</link>
<description rdf:parseType="Literal">&lt;p&gt;As social media platforms are evolving from text-based forums into
multi-modal environments, the nature of misinformation in social media is also
transforming accordingly. Taking advantage of the fact that visual modalities
such as images and videos are more favorable and attractive to the users and
textual contents are sometimes skimmed carelessly, misinformation spreaders
have recently targeted contextual connections between the modalities e.g., text
and image. Hence many researchers have developed automatic techniques for
detecting possible cross-modal discordance in web-based content. We analyze,
categorize and identify existing approaches in addition to challenges and
shortcomings they face in order to unearth new research opportunities in the
field of multi-modal misinformation detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdali_S/0/1/0/all/0/1&quot;&gt;Sara Abdali&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2204.13209">
<title>Robust stabilization of polytopic systems via fast and reliable neural network-based approximations. (arXiv:2204.13209v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2204.13209</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the design of fast and reliable neural network (NN)-based
approximations of traditional stabilizing controllers for linear systems with
polytopic uncertainty, including control laws with variable structure and those
based on a (minimal) selection policy. Building upon recent approaches for the
design of reliable control surrogates with guaranteed structural properties, we
develop a systematic procedure to certify the closed-loop stability and
performance of a linear uncertain system when a trained rectified linear unit
(ReLU)-based approximation replaces such traditional controllers. First, we
provide a sufficient condition, which involves the worst-case approximation
error between ReLU-based and traditional controller-based state-to-input
mappings, ensuring that the system is ultimately bounded within a set with
adjustable size and convergence rate. Then, we develop an offline,
mixed-integer optimization-based method that allows us to compute that quantity
exactly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fabiani_F/0/1/0/all/0/1&quot;&gt;Filippo Fabiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goulart_P/0/1/0/all/0/1&quot;&gt;Paul J. Goulart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.05173">
<title>Self-Supervised Anomaly Detection in Computer Vision and Beyond: A Survey and Outlook. (arXiv:2205.05173v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.05173</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection (AD) plays a crucial role in various domains, including
cybersecurity, finance, and healthcare, by identifying patterns or events that
deviate from normal behaviour. In recent years, significant progress has been
made in this field due to the remarkable growth of deep learning models.
Notably, the advent of self-supervised learning has sparked the development of
novel AD algorithms that outperform the existing state-of-the-art approaches by
a considerable margin. This paper aims to provide a comprehensive review of the
current methodologies in self-supervised anomaly detection. We present
technical details of the standard methods and discuss their strengths and
drawbacks. We also compare the performance of these models against each other
and other state-of-the-art anomaly detection models. Finally, the paper
concludes with a discussion of future directions for self-supervised anomaly
detection, including the development of more effective and efficient algorithms
and the integration of these techniques with other related fields, such as
multi-modal learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hojjati_H/0/1/0/all/0/1&quot;&gt;Hadi Hojjati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_T/0/1/0/all/0/1&quot;&gt;Thi Kieu Khanh Ho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armanfard_N/0/1/0/all/0/1&quot;&gt;Narges Armanfard&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.05587">
<title>Choice of training label matters: how to best use deep learning for quantitative MRI parameter estimation. (arXiv:2205.05587v3 [physics.med-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2205.05587</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) is gaining popularity as a parameter estimation method for
quantitative MRI. A range of competing implementations have been proposed,
relying on either supervised or self-supervised learning. Self-supervised
approaches, sometimes referred to as unsupervised, have been loosely based on
auto-encoders, whereas supervised methods have, to date, been trained on
groundtruth labels. These two learning paradigms have been shown to have
distinct strengths. Notably, self-supervised approaches have offered lower-bias
parameter estimates than their supervised alternatives. This result is
counterintuitive - incorporating prior knowledge with supervised labels should,
in theory, lead to improved accuracy. In this work, we show that this apparent
limitation of supervised approaches stems from the naive choice of groundtruth
training labels. By training on labels which are deliberately not groundtruth,
we show that the low-bias parameter estimation previously associated with
self-supervised methods can be replicated - and improved on - within a
supervised learning framework. This approach sets the stage for a single,
unifying, deep learning parameter estimation framework, based on supervised
learning, where trade-offs between bias and variance are made by careful
adjustment of training label.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Epstein_S/0/1/0/all/0/1&quot;&gt;Sean C. Epstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Bray_T/0/1/0/all/0/1&quot;&gt;Timothy J. P. Bray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hall_Craggs_M/0/1/0/all/0/1&quot;&gt;Margaret Hall-Craggs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hui Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.13743">
<title>Personalized Algorithmic Recourse with Preference Elicitation. (arXiv:2205.13743v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2205.13743</link>
<description rdf:parseType="Literal">&lt;p&gt;Algorithmic Recourse (AR) is the problem of computing a sequence of actions
that -- once performed by a user -- overturns an undesirable machine decision.
It is paramount that the sequence of actions does not require too much effort
for users to implement. Yet, most approaches to AR assume that actions cost the
same for all users, and thus may recommend unfairly expensive recourse plans to
certain users. Prompted by this observation, we introduce PEAR, the first
human-in-the-loop approach capable of providing personalized algorithmic
recourse tailored to the needs of any end-user. PEAR builds on insights from
Bayesian Preference Elicitation to iteratively refine an estimate of the costs
of actions by asking choice set queries to the target user. The queries
themselves are computed by maximizing the Expected Utility of Selection, a
principled measure of information gain accounting for uncertainty on both the
cost estimate and the user&apos;s responses. PEAR integrates elicitation into a
Reinforcement Learning agent coupled with Monte Carlo Tree Search to quickly
identify promising recourse plans. Our empirical evaluation on real-world
datasets highlights how PEAR produces high-quality personalized recourse in
only a handful of iterations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_G/0/1/0/all/0/1&quot;&gt;Giovanni De Toni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Viappiani_P/0/1/0/all/0/1&quot;&gt;Paolo Viappiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teso_S/0/1/0/all/0/1&quot;&gt;Stefano Teso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lepri_B/0/1/0/all/0/1&quot;&gt;Bruno Lepri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Passerini_A/0/1/0/all/0/1&quot;&gt;Andrea Passerini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.00775">
<title>Adaptive Local Neighborhood-based Neural Networks for MR Image Reconstruction from Undersampled Data. (arXiv:2206.00775v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.00775</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent medical image reconstruction techniques focus on generating
high-quality medical images suitable for clinical use at the lowest possible
cost and with the fewest possible adverse effects on patients. Recent works
have shown significant promise for reconstructing MR images from sparsely
sampled k-space data using deep learning. In this work, we propose a technique
that rapidly estimates deep neural networks directly at reconstruction time by
fitting them on small adaptively estimated neighborhoods of a training set. In
brief, our algorithm alternates between searching for neighbors in a data set
that are similar to the test reconstruction, and training a local network on
these neighbors followed by updating the test reconstruction. Because our
reconstruction model is learned on a dataset that is in some sense similar to
the image being reconstructed rather than being fit on a large, diverse
training set, it is more adaptive to new scans. It can also handle changes in
training sets and flexible scan settings, while being relatively fast. Our
approach, dubbed LONDN-MRI, was validated on multiple data sets using deep
unrolled reconstruction networks. Reconstructions were performed at four fold
and eight fold undersampling of k-space with 1D variable-density random
phase-encode undersampling masks. Our results demonstrate that our proposed
locally-trained method produces higher-quality reconstructions compared to
models trained globally on larger datasets as well as other scan-adaptive
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Shijun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lahiri_A/0/1/0/all/0/1&quot;&gt;Anish Lahiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ravishankar_S/0/1/0/all/0/1&quot;&gt;Saiprasad Ravishankar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02059">
<title>Empowering GNNs via Edge-Aware Weisfeiler-Leman Algorithm. (arXiv:2206.02059v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02059</link>
<description rdf:parseType="Literal">&lt;p&gt;Message passing graph neural networks (GNNs) are known to have their
expressiveness upper-bounded by 1-dimensional Weisfeiler-Leman (1-WL)
algorithm. To achieve more powerful GNNs, existing attempts either require ad
hoc features, or involve operations that incur high time and space
complexities. In this work, we propose a general and provably powerful GNN
framework that preserves the scalability of the message passing scheme. In
particular, we first propose to empower 1-WL for graph isomorphism test by
considering edges among neighbors, giving rise to NC-1-WL. The expressiveness
of NC-1-WL is shown to be strictly above 1-WL and below 3-WL theoretically.
Further, we propose the NC-GNN framework as a differentiable neural version of
NC-1-WL. Our simple implementation of NC-GNN is provably as powerful as
NC-1-WL. Experiments demonstrate that our NC-GNN performs effectively and
efficiently on various benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haiyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shuiwang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.11492">
<title>Gradual Domain Adaptation via Normalizing Flows. (arXiv:2206.11492v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2206.11492</link>
<description rdf:parseType="Literal">&lt;p&gt;Standard domain adaptation methods do not work well when a large gap exists
between the source and target domains. Gradual domain adaptation is one of the
approaches used to address the problem. It involves leveraging the intermediate
domain, which gradually shifts from the source domain to the target domain. In
previous work, it is assumed that the number of intermediate domains is large
and the distance between adjacent domains is small; hence, the gradual domain
adaptation algorithm, involving self-training with unlabeled datasets, is
applicable. In practice, however, gradual self-training will fail because the
number of intermediate domains is limited and the distance between adjacent
domains is large. We propose the use of normalizing flows to deal with this
problem while maintaining the framework of unsupervised domain adaptation. The
proposed method learns a transformation from the distribution of the target
domain to the Gaussian mixture distribution via the source domain. We evaluate
our proposed method by experiments using real-world datasets and confirm that
it mitigates the above-explained problem and improves the classification
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sagawa_S/0/1/0/all/0/1&quot;&gt;Shogo Sagawa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hino_H/0/1/0/all/0/1&quot;&gt;Hideitsu Hino&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.02829">
<title>Online Bilevel Optimization: Regret Analysis of Online Alternating Gradient Methods. (arXiv:2207.02829v5 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2207.02829</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an \textit{online bilevel optimization} setting in
which a sequence of time-varying bilevel problems are revealed one after the
other. We extend the known regret bounds for single-level online algorithms to
the bilevel setting. Specifically, we provide new notions of \textit{bilevel
regret}, develop an online alternating time-averaged gradient method that is
capable of leveraging smoothness, and give regret bounds in terms of the
path-length of the inner and outer minimizer sequences.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Tarzanagh_D/0/1/0/all/0/1&quot;&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Nazari_P/0/1/0/all/0/1&quot;&gt;Parvin Nazari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Bojian Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Li Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Balzano_L/0/1/0/all/0/1&quot;&gt;Laura Balzano&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04883">
<title>Neural-Rendezvous: Provably Robust Guidance and Control to Encounter Interstellar Objects. (arXiv:2208.04883v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04883</link>
<description rdf:parseType="Literal">&lt;p&gt;Interstellar objects (ISOs) are likely representatives of primitive materials
invaluable in understanding exoplanetary star systems. Due to their poorly
constrained orbits with generally high inclinations and relative velocities,
however, exploring ISOs with conventional human-in-the-loop approaches is
significantly challenging. This paper presents Neural-Rendezvous, a deep
learning-based guidance and control framework for encountering fast-moving
objects, including ISOs, robustly, accurately, and autonomously in real time.
It uses pointwise minimum norm tracking control on top of a guidance policy
modeled by a spectrally-normalized deep neural network, where its
hyperparameters are tuned with a loss function directly penalizing the MPC
state trajectory tracking error. We show that Neural-Rendezvous provides a high
probability exponential bound on the expected spacecraft delivery error, the
proof of which leverages stochastic incremental stability analysis. In
particular, it is used to construct a non-negative function with a
supermartingale property, explicitly accounting for the ISO state uncertainty
and the local nature of nonlinear state estimation guarantees. In numerical
simulations, Neural-Rendezvous is demonstrated to satisfy the expected error
bound for 100 ISO candidates. This performance is also empirically validated
using our spacecraft simulator and in high-conflict and distributed UAV swarm
reconfiguration with up to 20 UAVs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsukamoto_H/0/1/0/all/0/1&quot;&gt;Hiroyasu Tsukamoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;Soon-Jo Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donitz_B/0/1/0/all/0/1&quot;&gt;Benjamin Donitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingham_M/0/1/0/all/0/1&quot;&gt;Michel Ingham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mages_D/0/1/0/all/0/1&quot;&gt;Declan Mages&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakka_Y/0/1/0/all/0/1&quot;&gt;Yashwanth Kumar Nakka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.07805">
<title>A Comprehensive Benchmark for COVID-19 Predictive Modeling Using Electronic Health Records in Intensive Care. (arXiv:2209.07805v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.07805</link>
<description rdf:parseType="Literal">&lt;p&gt;The COVID-19 pandemic has posed a heavy burden to the healthcare system
worldwide and caused huge social disruption and economic loss. Many deep
learning models have been proposed to conduct clinical predictive tasks such as
mortality prediction for COVID-19 patients in intensive care units using
Electronic Health Record (EHR) data. Despite their initial success in certain
clinical applications, there is currently a lack of benchmarking results to
achieve a fair comparison so that we can select the optimal model for clinical
use. Furthermore, there is a discrepancy between the formulation of traditional
prediction tasks and real-world clinical practice in intensive care. To fill
these gaps, we propose two clinical prediction tasks, Outcome-specific
length-of-stay prediction and Early mortality prediction for COVID-19 patients
in intensive care units. The two tasks are adapted from the naive
length-of-stay and mortality prediction tasks to accommodate the clinical
practice for COVID-19 patients. We propose fair, detailed, open-source
data-preprocessing pipelines and evaluate 17 state-of-the-art predictive models
on two tasks, including 5 machine learning models, 6 basic deep learning models
and 6 deep learning predictive models specifically designed for EHR data. We
provide benchmarking results using data from two real-world COVID-19 EHR
datasets. One dataset is publicly available without needing any inquiry and
another dataset can be accessed on request. We provide fair, reproducible
benchmarking results for two tasks. We deploy all experiment results and models
on an online platform. We also allow clinicians and researchers to upload their
data to the platform and get quick prediction results using our trained models.
We hope our efforts can further facilitate deep learning and machine learning
research for COVID-19 predictive modeling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junyi Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yinghao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_W/0/1/0/all/0/1&quot;&gt;Wen Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_E/0/1/0/all/0/1&quot;&gt;Ewen M. Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Liantao Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.01407">
<title>Homotopy-based training of NeuralODEs for accurate dynamics discovery. (arXiv:2210.01407v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.01407</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Ordinary Differential Equations (NeuralODEs) present an attractive way
to extract dynamical laws from time series data, as they bridge neural networks
with the differential equation-based modeling paradigm of the physical
sciences. However, these models often display long training times and
suboptimal results, especially for longer duration data. While a common
strategy in the literature imposes strong constraints to the NeuralODE
architecture to inherently promote stable model dynamics, such methods are
ill-suited for dynamics discovery as the unknown governing equation is not
guaranteed to satisfy the assumed constraints. In this paper, we develop a new
training method for NeuralODEs, based on synchronization and homotopy
optimization, that does not require changes to the model architecture. We show
that synchronizing the model dynamics and the training data tames the
originally irregular loss landscape, which homotopy optimization can then
leverage to enhance training. Through benchmark experiments, we demonstrate our
method achieves competitive or better training loss while often requiring less
than half the number of training epochs compared to other model-agnostic
techniques. Furthermore, models trained with our method display better
extrapolation capabilities, highlighting the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ko_J/0/1/0/all/0/1&quot;&gt;Joon-Hyuk Ko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koh_H/0/1/0/all/0/1&quot;&gt;Hankyul Koh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Nojun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jhe_W/0/1/0/all/0/1&quot;&gt;Wonho Jhe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01758">
<title>Optimal Algorithms for Stochastic Complementary Composite Minimization. (arXiv:2211.01758v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01758</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by regularization techniques in statistics and machine learning, we
study complementary composite minimization in the stochastic setting. This
problem corresponds to the minimization of the sum of a (weakly) smooth
function endowed with a stochastic first-order oracle, and a structured
uniformly convex (possibly nonsmooth and non-Lipschitz) regularization term.
Despite intensive work on closely related settings, prior to our work no
complexity bounds for this problem were known. We close this gap by providing
novel excess risk bounds, both in expectation and with high probability. Our
algorithms are nearly optimal, which we prove via novel lower complexity bounds
for this class of problems. We conclude by providing numerical results
comparing our methods to the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+dAspremont_A/0/1/0/all/0/1&quot;&gt;Alexandre d&amp;#x27;Aspremont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guzman_C/0/1/0/all/0/1&quot;&gt;Crist&amp;#xf3;bal Guzm&amp;#xe1;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lezane_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Lezane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.03281">
<title>Copula Conformal Prediction for Multi-step Time Series Forecasting. (arXiv:2212.03281v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.03281</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate uncertainty measurement is a key step to building robust and
reliable machine learning systems. Conformal prediction is a distribution-free
uncertainty quantification algorithm popular for its ease of implementation,
statistical coverage guarantees, and versatility for underlying forecasters.
However, existing conformal prediction algorithms for time series are limited
to single-step prediction without considering the temporal dependency. In this
paper we propose a Copula Conformal Prediction algorithm for multivariate,
multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has
finite sample validity guarantee. On several synthetic and real-world
multivariate time series datasets, we show that CopulaCPTS produces more
calibrated and sharp confidence intervals for multi-step prediction tasks than
existing techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Sophia Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Rose Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04631">
<title>The Normalized Cross Density Functional: A Framework to Quantify Statistical Dependence for Random Processes. (arXiv:2212.04631v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04631</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel multivariate definition of statistical dependence
between two continuous random processes (r.p.) using a functional methodology
inspired by Alfr\&apos;ed R\&apos;enyi. The argument of the logarithm of mutual
information between pairs of samples of a r.p., named here the normalized cross
density (NCD), defines a symmetric and self-adjoint positive definite function.
We show that maximizing the alternating covariance estimation (ACE) recursion,
applied to each of the joint probability density of input sample pairs, obeys
all the properties of Renyi&apos;s maximal correlation. We propose the NCD&apos;s
eigenspectrum as a novel multivariate measure of the statistical dependence
between the input and output r.p.
&lt;/p&gt;
&lt;p&gt;The multivariate statistical dependence can also be estimated directly from
r.p. realizations. The proposed functional maximum correlation algorithm (FMCA)
is applied to a machine learning architecture built from two neural networks
that learn concurrently by approximating each others&apos; outputs. We prove that
the FMCA optimal solution is an equilibrium point that estimates the
eigenspectrum of the cross density kernel. Preliminary results with synthetic
data and medium size image datasets corroborate the theory. Four different
strategies of applying the cross density kernel are proposed and thoroughly
discussed to show the versatility and stability of the methodology, which
transcends supervised learning. More specifically, when the two random
processes are high-dimensional real-world images and a white uniform noise
process, the algorithm learns a factorial code i.e., the occurrence of a code
guarantees that a certain input in the training image set was present, which is
quite important for feature learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_B/0/1/0/all/0/1&quot;&gt;Bo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Principe_J/0/1/0/all/0/1&quot;&gt;Jose C. Principe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.12970">
<title>Refined Edge Usage of Graph Neural Networks for Edge Prediction. (arXiv:2212.12970v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.12970</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs), originally proposed for node classification,
have also motivated many recent works on edge prediction (a.k.a., link
prediction). However, existing methods lack elaborate design regarding the
distinctions between two tasks that have been frequently overlooked: (i) edges
only constitute the topology in the node classification task but can be used as
both the topology and the supervisions (i.e., labels) in the edge prediction
task; (ii) the node classification makes prediction over each individual node,
while the edge prediction is determinated by each pair of nodes. To this end,
we propose a novel edge prediction paradigm named Edge-aware Message PassIng
neuRal nEtworks (EMPIRE). Concretely, we first introduce an edge splitting
technique to specify use of each edge where each edge is solely used as either
the topology or the supervision (named as topology edge or supervision edge).
We then develop a new message passing mechanism that generates the messages to
source nodes (through topology edges) being aware of target nodes (through
supervision edges). In order to emphasize the differences between pairs
connected by supervision edges and pairs unconnected, we further weight the
messages to highlight the relative ones that can reflect the differences. In
addition, we design a novel negative node-pair sampling trick that efficiently
samples &apos;hard&apos; negative instances in the supervision instances, and can
significantly improve the performance. Experimental results verify that the
proposed method can significantly outperform existing state-of-the-art models
regarding the edge prediction task on multiple homogeneous and heterogeneous
graph datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jiarui Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yangkun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weinan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1&quot;&gt;Quan Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_X/0/1/0/all/0/1&quot;&gt;Xiang Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wipf_D/0/1/0/all/0/1&quot;&gt;David Wipf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13069">
<title>Homophily modulates double descent generalization in graph convolution networks. (arXiv:2212.13069v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13069</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) excel in modeling relational data such as
biological, social, and transportation networks, but the underpinnings of their
success are not well understood. Traditional complexity measures from
statistical learning theory fail to account for observed phenomena like the
double descent or the impact of relational semantics on generalization error.
Motivated by experimental observations of ``transductive&apos;&apos; double descent in
key networks and datasets, we use analytical tools from statistical physics and
random matrix theory to precisely characterize generalization in simple graph
convolution networks on the contextual stochastic block model. Our results
illuminate the nuances of learning on homophilic versus heterophilic data and
predict double descent whose existence in GNNs has been questioned by recent
work. We show how risk is shaped by the interplay between the graph noise,
feature noise, and the number of training labels. Our findings apply beyond
stylized models, capturing qualitative trends in real-world GNNs and datasets.
As a case in point, we use our analytic insights to improve performance of
state-of-the-art graph convolution networks on heterophilic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Cheng Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liming Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Hong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dokmanic_I/0/1/0/all/0/1&quot;&gt;Ivan Dokmani&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02424">
<title>Conformal Loss-Controlling Prediction. (arXiv:2301.02424v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02424</link>
<description rdf:parseType="Literal">&lt;p&gt;Conformal prediction is a learning framework controlling prediction coverage
of prediction sets, which can be built on any learning algorithm for point
prediction. This work proposes a learning framework named conformal
loss-controlling prediction, which extends conformal prediction to the
situation where the value of a loss function needs to be controlled. Different
from existing works about risk-controlling prediction sets and conformal risk
control with the purpose of controlling the expected values of loss functions,
the proposed approach in this paper focuses on the loss for any test object,
which is an extension of conformal prediction from miscoverage loss to some
general loss. The controlling guarantee is proved under the assumption of
exchangeability of data in finite-sample cases and the framework is tested
empirically for classification with a class-varying loss and statistical
postprocessing of numerical weather forecasting applications, which are
introduced as point-wise classification and point-wise regression problems. All
theoretical analysis and experimental results confirm the effectiveness of our
loss-controlling approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Ping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_Z/0/1/0/all/0/1&quot;&gt;Zhong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaojun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyue Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.04378">
<title>Loss-Controlling Calibration for Predictive Models. (arXiv:2301.04378v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.04378</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a learning framework for calibrating predictive models to make
loss-controlling prediction for exchangeable data, which extends our recently
proposed conformal loss-controlling prediction for more general cases. By
comparison, the predictors built by the proposed loss-controlling approach are
not limited to set predictors, and the loss function can be any measurable
function without the monotone assumption. To control the loss values in an
efficient way, we introduce transformations preserving exchangeability to prove
finite-sample controlling guarantee when the test label is obtained, and then
develop an approximation approach to construct predictors. The transformations
can be built on any predefined function, which include using optimization
algorithms for parameter searching. This approach is a natural extension of
conformal loss-controlling prediction, since it can be reduced to the latter
when the set predictors have the nesting property and the loss functions are
monotone. Our proposed method is applied to selective regression and
high-impact weather forecasting problems, which demonstrates its effectiveness
for general loss-controlling prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Junzhi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pingping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_S/0/1/0/all/0/1&quot;&gt;Shuo Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyue Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02444">
<title>Calibrating Transformers via Sparse Gaussian Processes. (arXiv:2303.02444v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02444</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer models have achieved profound success in prediction tasks in a
wide range of applications in natural language processing, speech recognition
and computer vision. Extending Transformer&apos;s success to safety-critical domains
requires calibrated uncertainty estimation which remains under-explored. To
address this, we propose Sparse Gaussian Process attention (SGPA), which
performs Bayesian inference directly in the output space of multi-head
attention blocks (MHAs) in transformer to calibrate its uncertainty. It
replaces the scaled dot-product operation with a valid symmetric kernel and
uses sparse Gaussian processes (SGP) techniques to approximate the posterior
processes of MHA outputs. Empirically, on a suite of prediction tasks on text,
images and graphs, SGPA-based Transformers achieve competitive predictive
accuracy, while noticeably improving both in-distribution calibration and
out-of-distribution robustness and detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenlong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingzhen Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07846">
<title>Sample-efficient Adversarial Imitation Learning. (arXiv:2303.07846v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07846</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning, in which learning is performed by demonstration, has been
studied and advanced for sequential decision-making tasks in which a reward
function is not predefined. However, imitation learning methods still require
numerous expert demonstration samples to successfully imitate an expert&apos;s
behavior. To improve sample efficiency, we utilize self-supervised
representation learning, which can generate vast training signals from the
given data. In this study, we propose a self-supervised representation-based
adversarial imitation learning method to learn state and action representations
that are robust to diverse distortions and temporally predictive, on non-image
control tasks. In particular, in comparison with existing self-supervised
learning methods for tabular data, we propose a different corruption method for
state and action representations that is robust to diverse distortions. We
theoretically and empirically observe that making an informative feature
manifold with less sample complexity significantly improves the performance of
imitation learning. The proposed method shows a 39% relative improvement over
existing adversarial imitation learning methods on MuJoCo in a setting limited
to 100 expert state-action pairs. Moreover, we conduct comprehensive ablations
and additional experiments using demonstrations with varying optimality to
provide insights into a range of factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1&quot;&gt;Dahuin Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyungyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14391">
<title>Energy-based Models are Zero-Shot Planners for Compositional Scene Rearrangement. (arXiv:2304.14391v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14391</link>
<description rdf:parseType="Literal">&lt;p&gt;Language is compositional; an instruction can express multiple relation
constraints to hold among objects in a scene that a robot is tasked to
rearrange. Our focus in this work is an instructable scene-rearranging
framework that generalizes to longer instructions and to spatial concept
compositions never seen at training time. We propose to represent
language-instructed spatial concepts with energy functions over relative object
arrangements. A language parser maps instructions to corresponding energy
functions and an open-vocabulary visual-language model grounds their arguments
to relevant objects in the scene. We generate goal scene configurations by
gradient descent on the sum of energy functions, one per language predicate in
the instruction. Local vision-based policies then re-locate objects to the
inferred goal locations. We test our model on established instruction-guided
manipulation benchmarks, as well as benchmarks of compositional instructions we
introduce. We show our model can execute highly compositional instructions
zero-shot in simulation and in the real world. It outperforms
language-to-action reactive policies and Large Language Model planners by a
large margin, especially for long instructions that involve compositions of
multiple spatial concepts. Simulation and real-world robot execution videos, as
well as our code and datasets are publicly available on our website:
https://ebmplanner.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gkanatsios_N/0/1/0/all/0/1&quot;&gt;Nikolaos Gkanatsios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Ayush Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_Z/0/1/0/all/0/1&quot;&gt;Zhou Xian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunchu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atkeson_C/0/1/0/all/0/1&quot;&gt;Christopher Atkeson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fragkiadaki_K/0/1/0/all/0/1&quot;&gt;Katerina Fragkiadaki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03053">
<title>ZipIt! Merging Models from Different Tasks without Training. (arXiv:2305.03053v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03053</link>
<description rdf:parseType="Literal">&lt;p&gt;Typical deep visual recognition models are capable of performing the one task
they were trained on. In this paper, we tackle the extremely difficult problem
of combining distinct models with different initializations, each solving a
separate task, into one multi-task model without any additional training. Prior
work in model merging permutes one model to the space of the other then
averages them together. While this works for models trained on the same task,
we find that this fails to account for the differences in models trained on
disjoint tasks. Thus, we introduce &quot;ZipIt!&quot;, a general method for merging two
arbitrary models of the same architecture that incorporates two simple
strategies. First, in order to account for features that aren&apos;t shared between
models, we expand the model merging problem to allow for merging features
within each model by defining a general &quot;zip&quot; operation. Second, we add support
for partially zipping the models up until a specified layer, naturally creating
a multi-head model. We find that these two changes combined account for 20-60%
improvement over prior work, making it more feasible to merge models trained on
disjoint tasks without retraining.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoica_G/0/1/0/all/0/1&quot;&gt;George Stoica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bolya_D/0/1/0/all/0/1&quot;&gt;Daniel Bolya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorner_J/0/1/0/all/0/1&quot;&gt;Jakob Bjorner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_P/0/1/0/all/0/1&quot;&gt;Pratik Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hearn_T/0/1/0/all/0/1&quot;&gt;Taylor Hearn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09781">
<title>SpecInfer: Accelerating Generative Large Language Model Serving with Tree-based Speculative Inference and Verification. (arXiv:2305.09781v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09781</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces SpecInfer, a system that accelerates generative large
language model (LLM) serving with tree-based speculative inference and
verification. The key idea behind SpecInfer is leveraging small speculative
models to predict the LLM&apos;s outputs; the predictions are organized as a token
tree, whose nodes each represent a candidate token sequence. The correctness of
all candidate token sequences represented by a token tree is verified against
the LLM in parallel using a novel tree-based parallel decoding mechanism.
SpecInfer uses an LLM as a token tree verifier instead of an incremental
decoder, which significantly reduces the end-to-end latency and computational
requirement for serving generative LLMs while provably preserving model
quality. Our evaluation shows that SpecInfer outperforms existing LLM serving
systems by 1.5-2.8x for distributed LLM inference and by 2.6-3.5x for
offloading-based LLM inference, while preserving the same generative
performance. SpecInfer is publicly available at
https://github.com/flexflow/FlexFlow/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_X/0/1/0/all/0/1&quot;&gt;Xupeng Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliaro_G/0/1/0/all/0/1&quot;&gt;Gabriele Oliaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhihao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xinhao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhengxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1&quot;&gt;Rae Ying Yee Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1&quot;&gt;Alan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lijie Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xiaoxiang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_C/0/1/0/all/0/1&quot;&gt;Chunan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhuoming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arfeen_D/0/1/0/all/0/1&quot;&gt;Daiyaan Arfeen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abhyankar_R/0/1/0/all/0/1&quot;&gt;Reyna Abhyankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Zhihao Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14259">
<title>Learning to Generate Novel Scientific Directions with Contextualized Literature-based Discovery. (arXiv:2305.14259v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14259</link>
<description rdf:parseType="Literal">&lt;p&gt;Literature-Based Discovery (LBD) aims to discover new scientific knowledge by
mining papers and generating hypotheses. Standard LBD is limited to predicting
pairwise relations between discrete concepts (e.g., drug-disease links), and
ignores critical contexts like experimental settings (e.g., a specific patient
population where a drug is evaluated) and background motivations (e.g., to find
drugs without specific side effects). We address these limitations with a novel
formulation of contextualized-LBD (C-LBD): generating scientific hypotheses in
natural language, while grounding them in a context that controls the
hypothesis search space. We present a modeling framework using retrieval of
``inspirations&apos;&apos; from past scientific papers. Our evaluations reveal that GPT-4
tends to generate ideas with overall low technical depth and novelty, while our
inspiration prompting approaches partially mitigate this issue. Our work
represents a first step toward building language models that generate new ideas
derived from scientific literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qingyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Downey_D/0/1/0/all/0/1&quot;&gt;Doug Downey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hope_T/0/1/0/all/0/1&quot;&gt;Tom Hope&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18417">
<title>Determinantal Point Process Attention Over Grid Cell Code Supports Out of Distribution Generalization. (arXiv:2305.18417v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18417</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have made tremendous gains in emulating human-like
intelligence, and have been used increasingly as ways of understanding how the
brain may solve the complex computational problems on which this relies.
However, these still fall short of, and therefore fail to provide insight into
how the brain supports strong forms of generalization of which humans are
capable. One such case is out-of-distribution (OOD) generalization-successful
performance on test examples that lie outside the distribution of the training
set. Here, we identify properties of processing in the brain that may
contribute to this ability. We describe a two-part algorithm that draws on
specific features of neural computation to achieve OOD generalization, and
provide a proof of concept by evaluating performance on two challenging
cognitive tasks. First we draw on the fact that the mammalian brain represents
metric spaces using grid cell code (e.g., in the entorhinal cortex): abstract
representations of relational structure, organized in recurring motifs that
cover the representational space. Second, we propose an attentional mechanism
that operates over the grid cell code using Determinantal Point Process (DPP),
that we call DPP attention (DPP-A) -- a transformation that ensures maximum
sparseness in the coverage of that space. We show that a loss function that
combines standard task-optimized error with DPP-A can exploit the recurring
motifs in the grid cell code, and can be integrated with common architectures
to achieve strong OOD generalization performance on analogy and arithmetic
tasks. This provides both an interpretation of how the grid cell code in the
mammalian brain may contribute to generalization performance, and at the same
time a potential means for improving such capabilities in artificial neural
networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mondal_S/0/1/0/all/0/1&quot;&gt;Shanka Subhra Mondal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frankland_S/0/1/0/all/0/1&quot;&gt;Steven Frankland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webb_T/0/1/0/all/0/1&quot;&gt;Taylor Webb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Cohen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19004">
<title>Policy Gradient Algorithms for Robust MDPs with Non-Rectangular Uncertainty Sets. (arXiv:2305.19004v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19004</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose policy gradient algorithms for robust infinite-horizon Markov
decision processes (MDPs) with non-rectangular uncertainty sets, thereby
addressing an open challenge in the robust MDP literature. Indeed, uncertainty
sets that display statistical optimality properties and make optimal use of
limited data often fail to be rectangular. Unfortunately, the corresponding
robust MDPs cannot be solved with dynamic programming techniques and are in
fact provably intractable. We first present a randomized projected Langevin
dynamics algorithm that solves the robust policy evaluation problem to global
optimality but is inefficient. We also propose a deterministic policy gradient
method that is efficient but solves the robust policy evaluation problem only
approximately, and we prove that the approximation error scales with a new
measure of non-rectangularity of the uncertainty set. Finally, we describe an
actor-critic algorithm that finds an $\epsilon$-optimal solution for the robust
policy improvement problem in $\mathcal{O}(1/\epsilon^4)$ iterations. We thus
present the first complete solution scheme for robust MDPs with non-rectangular
uncertainty sets offering global optimality guarantees. Numerical experiments
show that our algorithms compare favorably against state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mengmeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Kuhn_D/0/1/0/all/0/1&quot;&gt;Daniel Kuhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Sutter_T/0/1/0/all/0/1&quot;&gt;Tobias Sutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02869">
<title>Data-Driven Online Model Selection With Regret Guarantees. (arXiv:2306.02869v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02869</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider model selection for sequential decision making in stochastic
environments with bandit feedback, where a meta-learner has at its disposal a
pool of base learners, and decides on the fly which action to take based on the
policies recommended by each base learner. Model selection is performed by
regret balancing but, unlike the recent literature on this subject, we do not
assume any prior knowledge about the base learners like candidate regret
guarantees; instead, we uncover these quantities in a data-driven manner. The
meta-learner is therefore able to leverage the realized regret incurred by each
base learner for the learning environment at hand (as opposed to the expected
regret), and single out the best such regret. We design two model selection
algorithms operating with this more ambitious notion of regret and, besides
proving model selection guarantees via regret balancing, we experimentally
demonstrate the compelling practical benefits of dealing with actual regrets
instead of candidate regret bounds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pacchiano_A/0/1/0/all/0/1&quot;&gt;Aldo Pacchiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dann_C/0/1/0/all/0/1&quot;&gt;Christoph Dann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gentile_C/0/1/0/all/0/1&quot;&gt;Claudio Gentile&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03401">
<title>A Lightweight Method for Tackling Unknown Participation Statistics in Federated Averaging. (arXiv:2306.03401v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03401</link>
<description rdf:parseType="Literal">&lt;p&gt;In federated learning (FL), clients usually have diverse participation
statistics that are unknown a priori, which can significantly harm the
performance of FL if not handled properly. Existing works aiming at addressing
this problem are usually based on global variance reduction, which requires a
substantial amount of additional memory in a multiplicative factor equal to the
total number of clients. An important open problem is to find a lightweight
method for FL in the presence of clients with unknown participation rates. In
this paper, we address this problem by adapting the aggregation weights in
federated averaging (FedAvg) based on the participation history of each client.
We first show that, with heterogeneous participation statistics, FedAvg with
non-optimal aggregation weights can diverge from the optimal solution of the
original FL objective, indicating the need of finding optimal aggregation
weights. However, it is difficult to compute the optimal weights when the
participation statistics are unknown. To address this problem, we present a new
algorithm called FedAU, which improves FedAvg by adaptively weighting the
client updates based on online estimates of the optimal weights without knowing
the statistics of client participation. We provide a theoretical convergence
analysis of FedAU using a novel methodology to connect the estimation error and
convergence. Our theoretical results reveal important and interesting insights,
while showing that FedAU converges to an optimal solution of the original
objective and has desirable properties such as linear speedup. Our experimental
results also verify the advantage of FedAU over baseline methods with various
participation patterns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_M/0/1/0/all/0/1&quot;&gt;Mingyue Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05739">
<title>Leaping through tree space: continuous phylogenetic inference for rooted and unrooted trees. (arXiv:2306.05739v4 [q-bio.PE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05739</link>
<description rdf:parseType="Literal">&lt;p&gt;Phylogenetics is now fundamental in life sciences, providing insights into
the earliest branches of life and the origins and spread of epidemics. However,
finding suitable phylogenies from the vast space of possible trees remains
challenging. To address this problem, for the first time, we perform both tree
exploration and inference in a continuous space where the computation of
gradients is possible. This continuous relaxation allows for major leaps across
tree space in both rooted and unrooted trees, and is less susceptible to
convergence to local minima. Our approach outperforms the current best methods
for inference on unrooted trees and, in simulation, accurately infers the tree
and root in ultrametric cases. The approach is effective in cases of empirical
data with negligible amounts of data, which we demonstrate on the phylogeny of
jawed vertebrates. Indeed, only a few genes with an ultrametric signal were
generally sufficient for resolving the major lineages of vertebrates.
Optimisation is possible via automatic differentiation and our method presents
an effective way forwards for exploring the most difficult, data-deficient
phylogenetic questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Penn_M/0/1/0/all/0/1&quot;&gt;Matthew J Penn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Scheidwasser_N/0/1/0/all/0/1&quot;&gt;Neil Scheidwasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Penn_J/0/1/0/all/0/1&quot;&gt;Joseph Penn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Donnelly_C/0/1/0/all/0/1&quot;&gt;Christl A Donnelly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Duchene_D/0/1/0/all/0/1&quot;&gt;David A Duch&amp;#xea;ne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Bhatt_S/0/1/0/all/0/1&quot;&gt;Samir Bhatt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.06075">
<title>DeepSeaNet: Improving Underwater Object Detection using EfficientDet. (arXiv:2306.06075v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.06075</link>
<description rdf:parseType="Literal">&lt;p&gt;Marine animals and deep underwater objects are difficult to recognize and
monitor for safety of aquatic life. There is an increasing challenge when the
water is saline with granular particles and impurities. In such natural
adversarial environment, traditional approaches like CNN start to fail and are
expensive to compute. This project involves implementing and evaluating various
object detection models, including EfficientDet, YOLOv5, YOLOv8, and
Detectron2, on an existing annotated underwater dataset, called the
Brackish-Dataset. The dataset comprises annotated image sequences of fish,
crabs, starfish, and other aquatic animals captured in Limfjorden water with
limited visibility. The aim of this research project is to study the efficiency
of newer models on the same dataset and contrast them with the previous results
based on accuracy and inference time. Firstly, I compare the results of YOLOv3
(31.10% mean Average Precision (mAP)), YOLOv4 (83.72% mAP), YOLOv5 (97.6%),
YOLOv8 (98.20%), EfficientDet (98.56% mAP) and Detectron2 (95.20% mAP) on the
same dataset. Secondly, I provide a modified BiSkFPN mechanism (BiFPN neck with
skip connections) to perform complex feature fusion in adversarial noise which
makes modified EfficientDet robust to perturbations. Third, analyzed the effect
on accuracy of EfficientDet (98.63% mAP) and YOLOv5 by adversarial learning
(98.04% mAP). Last, I provide class activation map based explanations (CAM) for
the two models to promote Explainability in black box models. Overall, the
results indicate that modified EfficientDet achieved higher accuracy with
five-fold cross validation than the other models with 88.54% IoU of feature
maps.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_S/0/1/0/all/0/1&quot;&gt;Sanyam Jain&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09549">
<title>QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules. (arXiv:2306.09549v3 [physics.chem-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09549</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised machine learning approaches have been increasingly used in
accelerating electronic structure prediction as surrogates of first-principle
computational methods, such as density functional theory (DFT). While numerous
quantum chemistry datasets focus on chemical properties and atomic forces, the
ability to achieve accurate and efficient prediction of the Hamiltonian matrix
is highly desired, as it is the most important and fundamental physical
quantity that determines the quantum states of physical systems and chemical
properties. In this work, we generate a new Quantum Hamiltonian dataset, named
as QH9, to provide precise Hamiltonian matrices for 999 molecular dynamics
trajectories and 130,831 stable molecular geometries, based on the QM9 dataset.
By designing benchmark tasks with various molecules, we show that current
machine learning models have the capacity to predict Hamiltonian matrices for
arbitrary molecules. Both the QH9 dataset and the baseline models are provided
to the community through an open-source benchmark, which can be highly valuable
for developing machine learning methods and accelerating molecular and
materials design for scientific and technological applications. Our benchmark
is publicly available at
https://github.com/divelab/AIRS/tree/main/OpenDFT/QHBench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haiyang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Youzhi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Strasser_A/0/1/0/all/0/1&quot;&gt;Alex Strasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Qian_X/0/1/0/all/0/1&quot;&gt;Xiaoning Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shuiwang Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.14624">
<title>Insights From Insurance for Fair Machine Learning. (arXiv:2306.14624v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.14624</link>
<description rdf:parseType="Literal">&lt;p&gt;We argue that insurance can act as an analogon for the social situatedness of
machine learning systems, hence allowing machine learning scholars to take
insights from the rich and interdisciplinary insurance literature. Tracing the
interaction of uncertainty, fairness and responsibility in insurance provides a
fresh perspective on fairness in machine learning. We link insurance fairness
conceptions to their machine learning relatives, and use this bridge to
problematize fairness as calibration. In this process, we bring to the
forefront two themes that have been largely overlooked in the machine learning
literature: responsibility and aggregate-individual tensions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frohlich_C/0/1/0/all/0/1&quot;&gt;Christian Fr&amp;#xf6;hlich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williamson_R/0/1/0/all/0/1&quot;&gt;Robert C. Williamson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17396">
<title>Koopman operator learning using invertible neural networks. (arXiv:2306.17396v2 [math.NA] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17396</link>
<description rdf:parseType="Literal">&lt;p&gt;In Koopman operator theory, a finite-dimensional nonlinear system is
transformed into an infinite but linear system using a set of observable
functions. However, manually selecting observable functions that span the
invariant subspace of the Koopman operator based on prior knowledge is
inefficient and challenging, particularly when little or no information is
available about the underlying systems. Furthermore, current methodologies tend
to disregard the importance of the invertibility of observable functions, which
leads to inaccurate results. To address these challenges, we propose the
so-called FlowDMD, aka Flow-based Dynamic Mode Decomposition, that utilizes the
Coupling Flow Invertible Neural Network (CF-INN) framework. FlowDMD leverages
the intrinsically invertible characteristics of the CF-INN to learn the
invariant subspaces of the Koopman operator and accurately reconstruct state
variables. Numerical experiments demonstrate the superior performance of our
algorithm compared to state-of-the-art methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Meng_Y/0/1/0/all/0/1&quot;&gt;Yuhuang Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jianguo Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Qiu_Y/0/1/0/all/0/1&quot;&gt;Yue Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00384">
<title>CasTGAN: Cascaded Generative Adversarial Network for Realistic Tabular Data Synthesis. (arXiv:2307.00384v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00384</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative adversarial networks (GANs) have drawn considerable attention in
recent years for their proven capability in generating synthetic data which can
be utilised for multiple purposes. While GANs have demonstrated tremendous
successes in producing synthetic data samples that replicate the dynamics of
the original datasets, the validity of the synthetic data and the underlying
privacy concerns represent major challenges which are not sufficiently
addressed. In this work, we design a cascaded tabular GAN framework (CasTGAN)
for generating realistic tabular data with a specific focus on the validity of
the output. In this context, validity refers to the the dependency between
features that can be found in the real data, but is typically misrepresented by
traditional generative models. Our key idea entails that employing a cascaded
architecture in which a dedicated generator samples each feature, the synthetic
output becomes more representative of the real data. Our experimental results
demonstrate that our model is capable of generating synthetic tabular data that
can be used for fitting machine learning models. In addition, our model
captures well the constraints and the correlations between the features of the
real data, especially the high dimensional datasets. Furthermore, we evaluate
the risk of white-box privacy attacks on our model and subsequently show that
applying some perturbations to the auxiliary learners in CasTGAN increases the
overall robustness of our model against targeted attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alshantti_A/0/1/0/all/0/1&quot;&gt;Abdallah Alshantti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varagnolo_D/0/1/0/all/0/1&quot;&gt;Damiano Varagnolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasheed_A/0/1/0/all/0/1&quot;&gt;Adil Rasheed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmati_A/0/1/0/all/0/1&quot;&gt;Aria Rahmati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westad_F/0/1/0/all/0/1&quot;&gt;Frank Westad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02764">
<title>When Does Confidence-Based Cascade Deferral Suffice?. (arXiv:2307.02764v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02764</link>
<description rdf:parseType="Literal">&lt;p&gt;Cascades are a classical strategy to enable inference cost to vary adaptively
across samples, wherein a sequence of classifiers are invoked in turn. A
deferral rule determines whether to invoke the next classifier in the sequence,
or to terminate prediction. One simple deferral rule employs the confidence of
the current classifier, e.g., based on the maximum predicted softmax
probability. Despite being oblivious to the structure of the cascade -- e.g.,
not modelling the errors of downstream models -- such confidence-based deferral
often works remarkably well in practice. In this paper, we seek to better
understand the conditions under which confidence-based deferral may fail, and
when alternate deferral strategies can perform better. We first present a
theoretical characterisation of the optimal deferral rule, which precisely
characterises settings under which confidence-based deferral may suffer. We
then study post-hoc deferral mechanisms, and demonstrate they can significantly
improve upon confidence-based deferral in settings where (i) downstream models
are specialists that only work well on a subset of inputs, (ii) samples are
subject to label noise, and (iii) there is distribution shift between the train
and test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jitkrittum_W/0/1/0/all/0/1&quot;&gt;Wittawat Jitkrittum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_N/0/1/0/all/0/1&quot;&gt;Neha Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_A/0/1/0/all/0/1&quot;&gt;Aditya Krishna Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_H/0/1/0/all/0/1&quot;&gt;Harikrishna Narasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_A/0/1/0/all/0/1&quot;&gt;Ankit Singh Rawat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sanjiv Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03212">
<title>Region-Wise Attentive Multi-View Representation Learning for Urban Region Embeddings. (arXiv:2307.03212v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03212</link>
<description rdf:parseType="Literal">&lt;p&gt;Urban region embedding is an important and yet highly challenging issue due
to the complexity and constantly changing nature of urban data. To address the
challenges, we propose a Region-Wise Multi-View Representation Learning (ROMER)
to capture multi-view dependencies and learn expressive representations of
urban regions without the constraints of rigid neighbourhood region conditions.
Our model focus on learn urban region representation from multi-source urban
data. First, we capture the multi-view correlations from mobility flow
patterns, POI semantics and check-in dynamics. Then, we adopt global graph
attention networks to learn similarity of any two vertices in graphs. To
comprehensively consider and share features of multiple views, a two-stage
fusion module is further proposed to learn weights with external attention to
fuse multi-view embeddings. Extensive experiments for two downstream tasks on
real-world datasets demonstrate that our model outperforms state-of-the-art
methods by up to 17\% improvement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_W/0/1/0/all/0/1&quot;&gt;Weiliang Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Q/0/1/0/all/0/1&quot;&gt;Qianqian Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.10487">
<title>Deciphering Raw Data in Neuro-Symbolic Learning with Provable Guarantees. (arXiv:2308.10487v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.10487</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuro-symbolic hybrid systems are promising for integrating machine learning
and symbolic reasoning, where perception models are facilitated with
information inferred from a symbolic knowledge base through logical reasoning.
Despite empirical evidence showing the ability of hybrid systems to learn
accurate perception models, the theoretical understanding of learnability is
still lacking. Hence, it remains unclear why a hybrid system succeeds for a
specific task and when it may fail given a different knowledge base. In this
paper, we introduce a novel way of characterising supervision signals from a
knowledge base, and establish a criterion for determining the knowledge&apos;s
efficacy in facilitating successful learning. This, for the first time, allows
us to address the two questions above by inspecting the knowledge base under
investigation. Our analysis suggests that many knowledge bases satisfy the
criterion, thus enabling effective learning, while some fail to satisfy it,
indicating potential failures. Comprehensive experiments confirm the utility of
our criterion on benchmark tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_L/0/1/0/all/0/1&quot;&gt;Lue Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wang-Zhou Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuan Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11477">
<title>An improved column-generation-based matheuristic for learning classification trees. (arXiv:2308.11477v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11477</link>
<description rdf:parseType="Literal">&lt;p&gt;Decision trees are highly interpretable models for solving classification
problems in machine learning (ML). The standard ML algorithms for training
decision trees are fast but generate suboptimal trees in terms of accuracy.
Other discrete optimization models in the literature address the optimality
problem but only work well on relatively small datasets. \cite{firat2020column}
proposed a column-generation-based heuristic approach for learning decision
trees. This approach improves scalability and can work with large datasets. In
this paper, we describe improvements to this column generation approach. First,
we modify the subproblem model to significantly reduce the number of
subproblems in multiclass classification instances. Next, we show that the
data-dependent constraints in the master problem are implied, and use them as
cutting planes. Furthermore, we describe a separation model to generate data
points for which the linear programming relaxation solution violates their
corresponding constraints. We conclude by presenting computational results that
show that these modifications result in better scalability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1&quot;&gt;Krunal Kishor Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Desaulniers_G/0/1/0/all/0/1&quot;&gt;Guy Desaulniers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lodi_A/0/1/0/all/0/1&quot;&gt;Andrea Lodi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11624">
<title>Revolutionizing TCAD Simulations with Universal Device Encoding and Graph Attention Networks. (arXiv:2308.11624v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11624</link>
<description rdf:parseType="Literal">&lt;p&gt;An innovative methodology that leverages artificial intelligence (AI) and
graph representation for semiconductor device encoding in TCAD device
simulation is proposed. A graph-based universal encoding scheme is presented
that not only considers material-level and device-level embeddings, but also
introduces a novel spatial relationship embedding inspired by interpolation
operations typically used in finite element meshing. Universal physical laws
from device simulations are leveraged for comprehensive data-driven modeling,
which encompasses surrogate Poisson emulation and current-voltage (IV)
prediction based on drift-diffusion model. Both are achieved using a novel
graph attention network, referred to as RelGAT. Comprehensive technical details
based on the device simulator Sentaurus TCAD are presented, empowering
researchers to adopt the proposed AI-driven Electronic Design Automation (EDA)
solution at the device level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_G/0/1/0/all/0/1&quot;&gt;Guangxi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_L/0/1/0/all/0/1&quot;&gt;Leilai Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_K/0/1/0/all/0/1&quot;&gt;Kain Lu Low&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.14190">
<title>Score-Based Generative Models for PET Image Reconstruction. (arXiv:2308.14190v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.14190</link>
<description rdf:parseType="Literal">&lt;p&gt;Score-based generative models have demonstrated highly promising results for
medical image reconstruction tasks in magnetic resonance imaging or computed
tomography. However, their application to Positron Emission Tomography (PET) is
still largely unexplored. PET image reconstruction involves a variety of
challenges, including Poisson noise with high variance and a wide dynamic
range. To address these challenges, we propose several PET-specific adaptations
of score-based generative models. The proposed framework is developed for both
2D and 3D PET. In addition, we provide an extension to guided reconstruction
using magnetic resonance images. We validate the approach through extensive 2D
and 3D $\textit{in-silico}$ experiments with a model trained on
patient-realistic data without lesions, and evaluate on data without lesions as
well as out-of-distribution data with lesions. This demonstrates the proposed
method&apos;s robustness and significant potential for improved PET reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singh_I/0/1/0/all/0/1&quot;&gt;Imraj RD Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Denker_A/0/1/0/all/0/1&quot;&gt;Alexander Denker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barbano_R/0/1/0/all/0/1&quot;&gt;Riccardo Barbano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kereta_Z/0/1/0/all/0/1&quot;&gt;&amp;#x17d;eljko Kereta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_B/0/1/0/all/0/1&quot;&gt;Bangti Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thielemans_K/0/1/0/all/0/1&quot;&gt;Kris Thielemans&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maass_P/0/1/0/all/0/1&quot;&gt;Peter Maass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Arridge_S/0/1/0/all/0/1&quot;&gt;Simon Arridge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10016">
<title>Evaluation of GPT-3 for Anti-Cancer Drug Sensitivity Prediction. (arXiv:2309.10016v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10016</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we investigated the potential of GPT-3 for the anti-cancer
drug sensitivity prediction task using structured pharmacogenomics data across
five tissue types and evaluated its performance with zero-shot prompting and
fine-tuning paradigms. The drug&apos;s smile representation and cell line&apos;s genomic
mutation features were predictive of the drug response. The results from this
study have the potential to pave the way for designing more efficient treatment
protocols in precision oncology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_S/0/1/0/all/0/1&quot;&gt;Shaika Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajaganapathy_S/0/1/0/all/0/1&quot;&gt;Sivaraman Rajaganapathy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Lichao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerhan_J/0/1/0/all/0/1&quot;&gt;James Cerhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zong_N/0/1/0/all/0/1&quot;&gt;Nansu Zong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.10140">
<title>A Geometric Framework for Neural Feature Learning. (arXiv:2309.10140v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.10140</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel framework for learning system design based on neural
feature extractors. First, we introduce the feature geometry, which unifies
statistical dependence and features in the same function space with geometric
structures. By applying the feature geometry, we formulate each learning
problem as solving the optimal feature approximation of the dependence
component specified by the learning setting. We propose a nesting technique for
designing learning algorithms to learn the optimal features from data samples,
which can be applied to off-the-shelf network architectures and optimizers. To
demonstrate the applications of the nesting technique, we further discuss
multivariate learning problems, including conditioned inference and multimodal
learning, where we present the optimal features and reveal their connections to
classical approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Lizhong Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03025">
<title>Retrieval meets Long Context Large Language Models. (arXiv:2310.03025v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03025</link>
<description rdf:parseType="Literal">&lt;p&gt;Extending the context window of large language models (LLMs) is getting
popular recently, while the solution of augmenting LLMs with retrieval has
existed for years. The natural questions are: i) Retrieval-augmentation versus
long context window, which one is better for downstream tasks? ii) Can both
methods be combined to get the best of both worlds? In this work, we answer
these questions by studying both solutions using two state-of-the-art
pretrained LLMs, i.e., a proprietary 43B GPT and Llama2-70B. Perhaps
surprisingly, we find that LLM with 4K context window using simple
retrieval-augmentation at generation can achieve comparable performance to
finetuned LLM with 16K context window via positional interpolation on long
context tasks, while taking much less computation. More importantly, we
demonstrate that retrieval can significantly improve the performance of LLMs
regardless of their extended context window sizes. Our best model,
retrieval-augmented Llama2-70B with 32K context window, outperforms
GPT-3.5-turbo-16k and Davinci003 in terms of average score on nine long context
tasks including question answering, query-based summarization, and in-context
few-shot learning tasks. It also outperforms its non-retrieval Llama2-70B-32k
baseline by a margin, while being much faster at generation. Our study provides
general insights on the choice of retrieval-augmentation versus long context
extension of LLM for practitioners.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xianchao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McAfee_L/0/1/0/all/0/1&quot;&gt;Lawrence McAfee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanian_S/0/1/0/all/0/1&quot;&gt;Sandeep Subramanian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakhturina_E/0/1/0/all/0/1&quot;&gt;Evelina Bakhturina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1&quot;&gt;Bryan Catanzaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05207">
<title>Boosting Facial Action Unit Detection Through Jointly Learning Facial Landmark Detection and Domain Separation and Reconstruction. (arXiv:2310.05207v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05207</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently how to introduce large amounts of unlabeled facial images in the
wild into supervised Facial Action Unit (AU) detection frameworks has become a
challenging problem. In this paper, we propose a new AU detection framework
where multi-task learning is introduced to jointly learn AU domain separation
and reconstruction and facial landmark detection by sharing the parameters of
homostructural facial extraction modules. In addition, we propose a new feature
alignment scheme based on contrastive learning by simple projectors and an
improved contrastive loss, which adds four additional intermediate supervisors
to promote the feature reconstruction process. Experimental results on two
benchmarks demonstrate our superiority against the state-of-the-art methods for
AU detection in the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_Z/0/1/0/all/0/1&quot;&gt;Ziqiao Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.15318">
<title>HetGPT: Harnessing the Power of Prompt Tuning in Pre-Trained Heterogeneous Graph Neural Networks. (arXiv:2310.15318v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.15318</link>
<description rdf:parseType="Literal">&lt;p&gt;Graphs have emerged as a natural choice to represent and analyze the
intricate patterns and rich information of the Web, enabling applications such
as online page classification and social recommendation. The prevailing
&quot;pre-train, fine-tune&quot; paradigm has been widely adopted in graph machine
learning tasks, particularly in scenarios with limited labeled nodes. However,
this approach often exhibits a misalignment between the training objectives of
pretext tasks and those of downstream tasks. This gap can result in the
&quot;negative transfer&quot; problem, wherein the knowledge gained from pre-training
adversely affects performance in the downstream tasks. The surge in
prompt-based learning within Natural Language Processing (NLP) suggests the
potential of adapting a &quot;pre-train, prompt&quot; paradigm to graphs as an
alternative. However, existing graph prompting techniques are tailored to
homogeneous graphs, neglecting the inherent heterogeneity of Web graphs. To
bridge this gap, we propose HetGPT, a general post-training prompting framework
to improve the predictive performance of pre-trained heterogeneous graph neural
networks (HGNNs). The key is the design of a novel prompting function that
integrates a virtual class prompt and a heterogeneous feature prompt, with the
aim to reformulate downstream tasks to mirror pretext tasks. Moreover, HetGPT
introduces a multi-view neighborhood aggregation mechanism, capturing the
complex neighborhood structure in heterogeneous graphs. Extensive experiments
on three benchmark datasets demonstrate HetGPT&apos;s capability to enhance the
performance of state-of-the-art HGNNs on semi-supervised node classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yihong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_N/0/1/0/all/0/1&quot;&gt;Ning Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiayu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mortazavi_M/0/1/0/all/0/1&quot;&gt;Masood Mortazavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chawla_N/0/1/0/all/0/1&quot;&gt;Nitesh V. Chawla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18304">
<title>A Stability Principle for Learning under Non-Stationarity. (arXiv:2310.18304v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18304</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a versatile framework for statistical learning in non-stationary
environments. In each time period, our approach applies a stability principle
to select a look-back window that maximizes the utilization of historical data
while keeping the cumulative bias within an acceptable range relative to the
stochastic error. Our theory showcases the adaptability of this approach to
unknown non-stationarity. The regret bound is minimax optimal up to logarithmic
factors when the population losses are strongly convex, or Lipschitz only. At
the heart of our analysis lie two novel components: a measure of similarity
between functions and a segmentation technique for dividing the non-stationary
data sequence into quasi-stationary pieces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengpiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kaizheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18382">
<title>From Generative AI to Generative Internet of Things: Fundamentals, Framework, and Outlooks. (arXiv:2310.18382v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18382</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Artificial Intelligence (GAI) possesses the capabilities of
generating realistic data and facilitating advanced decision-making. By
integrating GAI into modern Internet of Things (IoT), Generative Internet of
Things (GIoT) is emerging and holds immense potential to revolutionize various
aspects of society, enabling more efficient and intelligent IoT applications,
such as smart surveillance and voice assistants. In this article, we present
the concept of GIoT and conduct an exploration of its potential prospects.
Specifically, we first overview four GAI techniques and investigate promising
GIoT applications. Then, we elaborate on the main challenges in enabling GIoT
and propose a general GAI-based secure incentive mechanism framework to address
them, in which we adopt Generative Diffusion Models (GDMs) for incentive
mechanism designs and apply blockchain technologies for secure GIoT management.
Moreover, we conduct a case study on modern Internet of Vehicle traffic
monitoring, which utilizes GDMs to generate effective contracts for
incentivizing users to contribute sensing data with high quality. Finally, we
suggest several open directions worth investigating for the future popularity
of GIoT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Jinbo Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_J/0/1/0/all/0/1&quot;&gt;Jiangtian Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jiawen Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niyato_D/0/1/0/all/0/1&quot;&gt;Dusit Niyato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_H/0/1/0/all/0/1&quot;&gt;Hongyang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guizani_M/0/1/0/all/0/1&quot;&gt;Mohsen Guizani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03131">
<title>Reservoir-Computing Model for Mapping and Forecasting Neuronal Interactions from Electrophysiological Data. (arXiv:2311.03131v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03131</link>
<description rdf:parseType="Literal">&lt;p&gt;Electrophysiological nature of neuronal networks allows to reveal various
interactions between different cell units at a very short time-scales. One of
the many challenges in analyzing these signals is to retrieve the morphology
and functionality of a given network. In this work we developed a computational
model, based on Reservoir Computing Network (RCN) architecture, which decodes
the spatio-temporal data from electro-physiological measurements of neuronal
cultures and reconstructs the network structure on a macroscopic domain,
representing the connectivity between neuronal units. We demonstrate that the
model can predict the connectivity map of the network with higher accuracy than
the common methods such as Cross-Correlation and Transfer-Entropy. In addition,
we experimentally demonstrate the ability of the model to predict a network
response to a specific input, such as localized stimulus.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Auslender_I/0/1/0/all/0/1&quot;&gt;Ilya Auslender&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Letti_G/0/1/0/all/0/1&quot;&gt;Giorgio Letti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Heydari_Y/0/1/0/all/0/1&quot;&gt;Yasaman Heydari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zaccaria_C/0/1/0/all/0/1&quot;&gt;Clara Zaccaria&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Pavesi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Pavesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10309">
<title>Imagination-Augmented Hierarchical Reinforcement Learning for Safe and Interactive Autonomous Driving in Urban Environments. (arXiv:2311.10309v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10309</link>
<description rdf:parseType="Literal">&lt;p&gt;Hierarchical reinforcement learning (HRL) incorporates temporal abstraction
into reinforcement learning (RL) by explicitly taking advantage of hierarchical
structure. Modern HRL typically designs a hierarchical agent composed of a
high-level policy and low-level policies. The high-level policy selects which
low-level policy to activate at a lower frequency and the activated low-level
policy selects an action at each time step. Recent HRL algorithms have achieved
performance gains over standard RL algorithms in synthetic navigation tasks.
However, we cannot apply these HRL algorithms to real-world navigation tasks.
One of the main challenges is that real-world navigation tasks require an agent
to perform safe and interactive behaviors in dynamic environments. In this
paper, we propose imagination-augmented HRL (IAHRL) that efficiently integrates
imagination into HRL to enable an agent to learn safe and interactive behaviors
in real-world navigation tasks. Imagination is to predict the consequences of
actions without interactions with actual environments. The key idea behind
IAHRL is that the low-level policies imagine safe and structured behaviors, and
then the high-level policy infers interactions with surrounding objects by
interpreting the imagined behaviors. We also introduce a new attention
mechanism that allows our high-level policy to be permutation-invariant to the
order of surrounding objects and to prioritize our agent over them. To evaluate
IAHRL, we introduce five complex urban driving tasks, which are among the most
challenging real-world navigation tasks. The experimental results indicate that
IAHRL enables an agent to perform safe and interactive behaviors, achieving
higher success rates and lower average episode steps than baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang-Hyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1&quot;&gt;Yoonjae Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Seung-Woo Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16536">
<title>Personalized Predictions of Glioblastoma Infiltration: Mathematical Models, Physics-Informed Neural Networks and Multimodal Scans. (arXiv:2311.16536v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16536</link>
<description rdf:parseType="Literal">&lt;p&gt;Predicting the infiltration of Glioblastoma (GBM) from medical MRI scans is
crucial for understanding tumor growth dynamics and designing personalized
radiotherapy treatment plans.Mathematical models of GBM growth can complement
the data in the prediction of spatial distributions of tumor cells. However,
this requires estimating patient-specific parameters of the model from clinical
data, which is a challenging inverse problem due to limited temporal data and
the limited time between imaging and diagnosis. This work proposes a method
that uses Physics-Informed Neural Networks (PINNs) to estimate patient-specific
parameters of a reaction-diffusion PDE model of GBM growth from a single 3D
structural MRI snapshot. PINNs embed both the data and the PDE into a loss
function, thus integrating theory and data. Key innovations include the
identification and estimation of characteristic non-dimensional parameters, a
pre-training step that utilizes the non-dimensional parameters and a
fine-tuning step to determine the patient specific parameters. Additionally,
the diffuse domain method is employed to handle the complex brain geometry
within the PINN framework. Our method is validated both on synthetic and
patient datasets, and shows promise for real-time parametric inference in the
clinical setting for personalized GBM treatment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ray Zirui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ezhov_I/0/1/0/all/0/1&quot;&gt;Ivan Ezhov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balcerak_M/0/1/0/all/0/1&quot;&gt;Michal Balcerak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_A/0/1/0/all/0/1&quot;&gt;Andy Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Benedikt Wiestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lowengrub_J/0/1/0/all/0/1&quot;&gt;John Lowengrub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01185">
<title>A ripple in time: a discontinuity in American history. (arXiv:2312.01185v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01185</link>
<description rdf:parseType="Literal">&lt;p&gt;In this note we use the State of the Union Address (SOTU) dataset from Kaggle
to make some surprising (and some not so surprising) observations pertaining to
the general timeline of American history, and the character and nature of the
addresses themselves. Our main approach is using vector embeddings, such as
BERT (DistilBERT) and GPT-2.
&lt;/p&gt;
&lt;p&gt;While it is widely believed that BERT (and its variations) is most suitable
for NLP classification tasks, we find out that GPT-2 in conjunction with
nonlinear dimension reduction methods such as UMAP provide better separation
and stronger clustering. This makes GPT-2 + UMAP an interesting alternative. In
our case, no model fine-tuning is required, and the pre-trained out-of-the-box
GPT-2 model is enough.
&lt;/p&gt;
&lt;p&gt;We also used a fine-tuned DistilBERT model for classification detecting which
President delivered which address, with very good results (accuracy 93% - 95%
depending on the run). An analogous task was performed to determine the year of
writing, and we were able to pin it down to about 4 years (which is a single
presidential term).
&lt;/p&gt;
&lt;p&gt;It is worth noting that SOTU addresses provide relatively small writing
samples (with about 8&apos;000 words on average, and varying widely from under 2&apos;000
words to more than 20&apos;000), and that the number of authors is relatively large
(we used SOTU addresses of 42 US presidents). This shows that the techniques
employed turn out to be rather efficient, while all the computations described
in this note can be performed using a single GPU instance of Google Colab.
&lt;/p&gt;
&lt;p&gt;The accompanying code is available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolpakov_A/0/1/0/all/0/1&quot;&gt;Alexander Kolpakov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivin_I/0/1/0/all/0/1&quot;&gt;Igor Rivin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02246">
<title>Conditional Variational Diffusion Models. (arXiv:2312.02246v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02246</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse problems aim to determine parameters from observations, a crucial
task in engineering and science. Lately, generative models, especially
diffusion models, have gained popularity in this area for their ability to
produce realistic solutions and their good mathematical properties. Despite
their success, an important drawback of diffusion models is their sensitivity
to the choice of variance schedule, which controls the dynamics of the
diffusion process. Fine-tuning this schedule for specific applications is
crucial but time-costly and does not guarantee an optimal result. We propose a
novel approach for learning the schedule as part of the training process. Our
method supports probabilistic conditioning on data, provides high-quality
solutions, and is flexible, proving able to adapt to different applications
with minimum overhead. This approach is tested in two unrelated inverse
problems: super-resolution microscopy and quantitative phase imaging, yielding
comparable or superior results to previous methods and fine-tuned diffusion
models. We conclude that fine-tuning the schedule by experimentation should be
avoided because it can be learned during training in a stable way that yields
better results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maggiora_G/0/1/0/all/0/1&quot;&gt;Gabriel della Maggiora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Croquevielle_L/0/1/0/all/0/1&quot;&gt;Luis Alberto Croquevielle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_N/0/1/0/all/0/1&quot;&gt;Nikita Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horsley_H/0/1/0/all/0/1&quot;&gt;Harry Horsley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinis_T/0/1/0/all/0/1&quot;&gt;Thomas Heinis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yakimovich_A/0/1/0/all/0/1&quot;&gt;Artur Yakimovich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03311">
<title>On the Nystrom Approximation for Preconditioning in Kernel Machines. (arXiv:2312.03311v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03311</link>
<description rdf:parseType="Literal">&lt;p&gt;Kernel methods are a popular class of nonlinear predictive models in machine
learning. Scalable algorithms for learning kernel models need to be iterative
in nature, but convergence can be slow due to poor conditioning. Spectral
preconditioning is an important tool to speed-up the convergence of such
iterative algorithms for training kernel models. However computing and storing
a spectral preconditioner can be expensive which can lead to large
computational and storage overheads, precluding the application of kernel
methods to problems with large datasets. A Nystrom approximation of the
spectral preconditioner is often cheaper to compute and store, and has
demonstrated success in practical applications. In this paper we analyze the
trade-offs of using such an approximated preconditioner. Specifically, we show
that a sample of logarithmic size (as a function of the size of the dataset)
enables the Nystrom-based approximated preconditioner to accelerate gradient
descent nearly as well as the exact preconditioner, while also reducing the
computational and storage overheads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Abedsoltan_A/0/1/0/all/0/1&quot;&gt;Amirhesam Abedsoltan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pandit_P/0/1/0/all/0/1&quot;&gt;Parthe Pandit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rademacher_L/0/1/0/all/0/1&quot;&gt;Luis Rademacher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Belkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Belkin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.09126">
<title>Towards Trustworthy AI Software Development Assistance. (arXiv:2312.09126v2 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2312.09126</link>
<description rdf:parseType="Literal">&lt;p&gt;It is expected that in the near future, AI software development assistants
will play an important role in the software industry. However, current software
development assistants tend to be unreliable, often producing incorrect,
unsafe, or low-quality code. We seek to resolve these issues by introducing a
holistic architecture for constructing, training, and using trustworthy AI
software development assistants. In the center of the architecture, there is a
foundational LLM trained on datasets representative of real-world coding
scenarios and complex software architectures, and fine-tuned on code quality
criteria beyond correctness. The LLM will make use of graph-based code
representations for advanced semantic comprehension. We envision a knowledge
graph integrated into the system to provide up-to-date background knowledge and
to enable the assistant to provide appropriate explanations. Finally, a modular
framework for constrained decoding will ensure that certain guarantees (e.g.,
for correctness and security) hold for the generated code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maninger_D/0/1/0/all/0/1&quot;&gt;Daniel Maninger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1&quot;&gt;Krishna Narasimhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mezini_M/0/1/0/all/0/1&quot;&gt;Mira Mezini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11486">
<title>Preference and Concurrence Aware Bayesian Graph Neural Networks for Recommender Systems. (arXiv:2312.11486v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11486</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph-based collaborative filtering methods have prevailing performance for
recommender systems since they can capture high-order information between users
and items, in which the graphs are constructed from the observed user-item
interactions that might miss links or contain spurious positive interactions in
industrial scenarios. The Bayesian Graph Neural Network framework approaches
this issue with generative models for the interaction graphs. The critical
problem is to devise a proper family of graph generative models tailored to
recommender systems. We propose an efficient generative model that jointly
considers the preferences of users, the concurrence of items and some important
graph structure information. Experiments on four popular benchmark datasets
demonstrate the effectiveness of our proposed graph generative methods for
recommender systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hongjian Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yaochen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingxue Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12433">
<title>Tracking Any Object Amodally. (arXiv:2312.12433v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12433</link>
<description rdf:parseType="Literal">&lt;p&gt;Amodal perception, the ability to comprehend complete object structures from
partial visibility, is a fundamental skill, even for infants. Its significance
extends to applications like autonomous driving, where a clear understanding of
heavily occluded objects is essential. However, modern detection and tracking
algorithms often overlook this critical capability, perhaps due to the
prevalence of modal annotations in most datasets. To address the scarcity of
amodal data, we introduce the TAO-Amodal benchmark, featuring 880 diverse
categories in thousands of video sequences. Our dataset includes amodal and
modal bounding boxes for visible and occluded objects, including objects that
are partially out-of-frame. To enhance amodal tracking with object permanence,
we leverage a lightweight plug-in module, the amodal expander, to transform
standard, modal trackers into amodal ones through fine-tuning on a few hundred
video sequences with data augmentation. We achieve a 3.3\% and 1.6\%
improvement on the detection and tracking of occluded objects on TAO-Amodal.
When evaluated on people, our method produces dramatic improvements of 2x
compared to state-of-the-art modal baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hsieh_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Hsieh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurana_T/0/1/0/all/0/1&quot;&gt;Tarasha Khurana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1&quot;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramanan_D/0/1/0/all/0/1&quot;&gt;Deva Ramanan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12937">
<title>Robust Loss Functions for Training Decision Trees with Noisy Labels. (arXiv:2312.12937v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12937</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider training decision trees using noisily labeled data, focusing on
loss functions that can lead to robust learning algorithms. Our contributions
are threefold. First, we offer novel theoretical insights on the robustness of
many existing loss functions in the context of decision tree learning. We show
that some of the losses belong to a class of what we call conservative losses,
and the conservative losses lead to an early stopping behavior during training
and noise-tolerant predictions during testing. Second, we introduce a framework
for constructing robust loss functions, called distribution losses. These
losses apply percentile-based penalties based on an assumed margin
distribution, and they naturally allow adapting to different noise rates via a
robustness parameter. In particular, we introduce a new loss called the
negative exponential loss, which leads to an efficient greedy
impurity-reduction learning algorithm. Lastly, our experiments on multiple
datasets and noise settings validate our theoretical insight and the
effectiveness of our adaptive negative exponential loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilton_J/0/1/0/all/0/1&quot;&gt;Jonathan Wilton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_N/0/1/0/all/0/1&quot;&gt;Nan Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15282">
<title>Causal Forecasting for Pricing. (arXiv:2312.15282v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15282</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a novel method for demand forecasting in a pricing
context. Here, modeling the causal relationship between price as an input
variable to demand is crucial because retailers aim to set prices in a (profit)
optimal manner in a downstream decision making problem. Our methods bring
together the Double Machine Learning methodology for causal inference and
state-of-the-art transformer-based forecasting models. In extensive empirical
experiments, we show on the one hand that our method estimates the causal
effect better in a fully controlled setting via synthetic, yet realistic data.
On the other hand, we demonstrate on real-world data that our method
outperforms forecasting methods in off-policy settings (i.e., when there&apos;s a
change in the pricing policy) while only slightly trailing in the on-policy
setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Schultz_D/0/1/0/all/0/1&quot;&gt;Douglas Schultz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Stephan_J/0/1/0/all/0/1&quot;&gt;Johannes Stephan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sieber_J/0/1/0/all/0/1&quot;&gt;Julian Sieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yeh_T/0/1/0/all/0/1&quot;&gt;Trudie Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kunz_M/0/1/0/all/0/1&quot;&gt;Manuel Kunz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Doupe_P/0/1/0/all/0/1&quot;&gt;Patrick Doupe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Januschowski_T/0/1/0/all/0/1&quot;&gt;Tim Januschowski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16613">
<title>Self-supervised Pretraining for Robust Personalized Voice Activity Detection in Adverse Conditions. (arXiv:2312.16613v2 [cs.SD] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16613</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose the use of self-supervised pretraining on a large
unlabelled data set to improve the performance of a personalized voice activity
detection (VAD) model in adverse conditions. We pretrain a long short-term
memory (LSTM)-encoder using the autoregressive predictive coding (APC)
framework and fine-tune it for personalized VAD. We also propose a denoising
variant of APC, with the goal of improving the robustness of personalized VAD.
The trained models are systematically evaluated on both clean speech and speech
contaminated by various types of noise at different SNR-levels and compared to
a purely supervised model. Our experiments show that self-supervised
pretraining not only improves performance in clean conditions, but also yields
models which are more robust to adverse conditions compared to purely
supervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bovbjerg_H/0/1/0/all/0/1&quot;&gt;Holger Severin Bovbjerg&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_J/0/1/0/all/0/1&quot;&gt;Jesper Jensen&lt;/a&gt; (1, 2), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ostergaard_J/0/1/0/all/0/1&quot;&gt;Jan &amp;#xd8;stergaard&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zheng-Hua Tan&lt;/a&gt; (1, 3) ((1) Aalborg University, (2) Oticon, (3) Pioneer Centre for AI, Denmark)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00334">
<title>Explainability-Driven Leaf Disease Classification Using Adversarial Training and Knowledge Distillation. (arXiv:2401.00334v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00334</link>
<description rdf:parseType="Literal">&lt;p&gt;This work focuses on plant leaf disease classification and explores three
crucial aspects: adversarial training, model explainability, and model
compression. The models&apos; robustness against adversarial attacks is enhanced
through adversarial training, ensuring accurate classification even in the
presence of threats. Leveraging explainability techniques, we gain insights
into the model&apos;s decision-making process, improving trust and transparency.
Additionally, we explore model compression techniques to optimize computational
efficiency while maintaining classification performance. Through our
experiments, we determine that on a benchmark dataset, the robustness can be
the price of the classification accuracy with performance reductions of 3%-20%
for regular tests and gains of 50%-70% for adversarial attack tests. We also
demonstrate that a student model can be 15-25 times more computationally
efficient for a slight performance reduction, distilling the knowledge of more
complex models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Echim_S/0/1/0/all/0/1&quot;&gt;Sebastian-Vasile Echim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taiatu_I/0/1/0/all/0/1&quot;&gt;Iulian-Marius T&amp;#x103;iatu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cercel_D/0/1/0/all/0/1&quot;&gt;Dumitru-Clementin Cercel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pop_F/0/1/0/all/0/1&quot;&gt;Florin Pop&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01520">
<title>S$^{2}$-DMs:Skip-Step Diffusion Models. (arXiv:2401.01520v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01520</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have emerged as powerful generative tools, rivaling GANs in
sample quality and mirroring the likelihood scores of autoregressive models. A
subset of these models, exemplified by DDIMs, exhibit an inherent asymmetry:
they are trained over $T$ steps but only sample from a subset of $T$ during
generation. This selective sampling approach, though optimized for speed,
inadvertently misses out on vital information from the unsampled steps, leading
to potential compromises in sample quality. To address this issue, we present
the S$^{2}$-DMs, which is a new training method by using an innovative
$L_{skip}$, meticulously designed to reintegrate the information omitted during
the selective sampling phase. The benefits of this approach are manifold: it
notably enhances sample quality, is exceptionally simple to implement, requires
minimal code modifications, and is flexible enough to be compatible with
various sampling algorithms. On the CIFAR10 dataset, models trained using our
algorithm showed an improvement of 3.27% to 14.06% over models trained with
traditional methods across various sampling algorithms (DDIMs, PNDMs, DEIS) and
different numbers of sampling steps (10, 20, ..., 1000). On the CELEBA dataset,
the improvement ranged from 8.97% to 27.08%. Access to the code and additional
resources is provided in the github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yixuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuangyin Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04079">
<title>RudolfV: A Foundation Model by Pathologists for Pathologists. (arXiv:2401.04079v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.04079</link>
<description rdf:parseType="Literal">&lt;p&gt;Histopathology plays a central role in clinical medicine and biomedical
research. While artificial intelligence shows promising results on many
pathological tasks, generalization and dealing with rare diseases, where
training data is scarce, remains a challenge. Distilling knowledge from
unlabeled data into a foundation model before learning from, potentially
limited, labeled data provides a viable path to address these challenges. In
this work, we extend the state of the art of foundation models for digital
pathology whole slide images by semi-automated data curation and incorporating
pathologist domain knowledge. Specifically, we combine computational and
pathologist domain knowledge (1) to curate a diverse dataset of 103k slides
corresponding to 750 million image patches covering data from different
fixation, staining, and scanning protocols as well as data from different
indications and labs across the EU and US, (2) for grouping semantically
similar slides and tissue patches, and (3) to augment the input images during
training. We evaluate the resulting model on a set of public and internal
benchmarks and show that although our foundation model is trained with an order
of magnitude less slides, it performs on par or better than competing models.
We expect that scaling our approach to more data and larger models will further
increase its performance and capacity to deal with increasingly complex real
world tasks in diagnostics and biomedical research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dippel_J/0/1/0/all/0/1&quot;&gt;Jonas Dippel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feulner_B/0/1/0/all/0/1&quot;&gt;Barbara Feulner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Winterhoff_T/0/1/0/all/0/1&quot;&gt;Tobias Winterhoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schallenberg_S/0/1/0/all/0/1&quot;&gt;Simon Schallenberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dernbach_G/0/1/0/all/0/1&quot;&gt;Gabriel Dernbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kunft_A/0/1/0/all/0/1&quot;&gt;Andreas Kunft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tietz_S/0/1/0/all/0/1&quot;&gt;Stephan Tietz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jurmeister_P/0/1/0/all/0/1&quot;&gt;Philipp Jurmeister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Horst_D/0/1/0/all/0/1&quot;&gt;David Horst&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ruff_L/0/1/0/all/0/1&quot;&gt;Lukas Ruff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Muller_K/0/1/0/all/0/1&quot;&gt;Klaus-Robert M&amp;#xfc;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Klauschen_F/0/1/0/all/0/1&quot;&gt;Frederick Klauschen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alber_M/0/1/0/all/0/1&quot;&gt;Maximilian Alber&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08119">
<title>SpecSTG: A Fast Spectral Diffusion Framework for Probabilistic Spatio-Temporal Traffic Forecasting. (arXiv:2401.08119v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08119</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic forecasting, a crucial application of spatio-temporal graph (STG)
learning, has traditionally relied on deterministic models for accurate point
estimations. Yet, these models fall short of identifying latent risks of
unexpected volatility in future observations. To address this gap,
probabilistic methods, especially variants of diffusion models, have emerged as
uncertainty-aware solutions. However, existing diffusion methods typically
focus on generating separate future time series for individual sensors in the
traffic network, resulting in insufficient involvement of spatial network
characteristics in the probabilistic learning process. To better leverage
spatial dependencies and systematic patterns inherent in traffic data, we
propose SpecSTG, a novel spectral diffusion framework. Our method generates the
Fourier representation of future time series, transforming the learning process
into the spectral domain enriched with spatial information. Additionally, our
approach incorporates a fast spectral graph convolution designed for Fourier
input, alleviating the computational burden associated with existing models.
Numerical experiments show that SpecSTG achieves outstanding performance with
traffic flow and traffic speed datasets compared to state-of-the-art baselines.
The source code for SpecSTG is available at
https://anonymous.4open.science/r/SpecSTG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lequan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_D/0/1/0/all/0/1&quot;&gt;Dai Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_A/0/1/0/all/0/1&quot;&gt;Andi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junbin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.08919">
<title>Partial Diacritization: A Context-Contrastive Inference Approach. (arXiv:2401.08919v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.08919</link>
<description rdf:parseType="Literal">&lt;p&gt;Diacritization plays a pivotal role in improving readability and
disambiguating the meaning of Arabic texts. Efforts have so far focused on
marking every eligible character (Full Diacritization). Comparatively
overlooked, Partial Diacritzation (PD) is the selection of a subset of
characters to be marked to aid comprehension where needed. Research has
indicated that excessive diacritic marks can hinder skilled readers--reducing
reading speed and accuracy. We conduct a behavioral experiment and show that
partially marked text is often easier to read than fully marked text, and
sometimes easier than plain text. In this light, we introduce
Context-Contrastive Partial Diacritization (CCPD)--a novel approach to PD which
integrates seamlessly with existing Arabic diacritization systems. CCPD
processes each word twice, once with context and once without, and diacritizes
only the characters with disparities between the two inferences. Further, we
introduce novel indicators for measuring partial diacritization quality (SR,
PDER, HDER, ERE), essential for establishing this as a machine learning task.
Lastly, we introduce TD2, a Transformer-variant of an established model which
offers a markedly different performance profile on our proposed indicators
compared to all other known systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+ElNokrashy_M/0/1/0/all/0/1&quot;&gt;Muhammad ElNokrashy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AlKhamissi_B/0/1/0/all/0/1&quot;&gt;Badr AlKhamissi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09479">
<title>Uncertainty-Aware Hardware Trojan Detection Using Multimodal Deep Learning. (arXiv:2401.09479v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09479</link>
<description rdf:parseType="Literal">&lt;p&gt;The risk of hardware Trojans being inserted at various stages of chip
production has increased in a zero-trust fabless era. To counter this, various
machine learning solutions have been developed for the detection of hardware
Trojans. While most of the focus has been on either a statistical or deep
learning approach, the limited number of Trojan-infected benchmarks affects the
detection accuracy and restricts the possibility of detecting zero-day Trojans.
To close the gap, we first employ generative adversarial networks to amplify
our data in two alternative representation modalities, a graph and a tabular,
ensuring that the dataset is distributed in a representative manner. Further,
we propose a multimodal deep learning approach to detect hardware Trojans and
evaluate the results from both early fusion and late fusion strategies. We also
estimate the uncertainty quantification metrics of each prediction for
risk-aware decision-making. The outcomes not only confirms the efficacy of our
proposed hardware Trojan detection method but also opens a new door for future
studies employing multimodality and uncertainty quantification to address other
hardware security challenges.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwakarma_R/0/1/0/all/0/1&quot;&gt;Rahul Vishwakarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezaei_A/0/1/0/all/0/1&quot;&gt;Amin Rezaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09943">
<title>Infinite-Horizon Graph Filters: Leveraging Power Series to Enhance Sparse Information Aggregation. (arXiv:2401.09943v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09943</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) have shown considerable effectiveness in a
variety of graph learning tasks, particularly those based on the
message-passing approach in recent years. However, their performance is often
constrained by a limited receptive field, a challenge that becomes more acute
in the presence of sparse graphs. In light of the power series, which possesses
infinite expansion capabilities, we propose a novel Graph Power Filter Neural
Network (GPFN) that enhances node classification by employing a power series
graph filter to augment the receptive field. Concretely, our GPFN designs a new
way to build a graph filter with an infinite receptive field based on the
convergence power series, which can be analyzed in the spectral and spatial
domains. Besides, we theoretically prove that our GPFN is a general framework
that can integrate any power series and capture long-range dependencies.
Finally, experimental results on three datasets demonstrate the superiority of
our GPFN over state-of-the-art baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruizhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xinke Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yuchen Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiayuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yongxin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xu Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junfeng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yasha Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10134">
<title>Spatial-Temporal Large Language Model for Traffic Prediction. (arXiv:2401.10134v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10134</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction, a critical component for intelligent transportation
systems, endeavors to foresee future traffic at specific locations using
historical data. Although existing traffic prediction models often emphasize
developing complex neural network structures, their accuracy has not seen
improvements accordingly. Recently, Large Language Models (LLMs) have shown
outstanding capabilities in time series analysis. Differing from existing
models, LLMs progress mainly through parameter expansion and extensive
pre-training while maintaining their fundamental structures. In this paper, we
propose a Spatial-Temporal Large Language Model (ST-LLM) for traffic
prediction. Specifically, ST-LLM redefines the timesteps at each location as
tokens and incorporates a spatial-temporal embedding module to learn the
spatial location and global temporal representations of tokens. Then these
representations are fused to provide each token with unified spatial and
temporal information. Furthermore, we propose a novel partially frozen
attention strategy of the LLM, which is designed to capture spatial-temporal
dependencies for traffic prediction. Comprehensive experiments on real traffic
datasets offer evidence that ST-LLM outperforms state-of-the-art models.
Notably, the ST-LLM also exhibits robust performance in both few-shot and
zero-shot prediction scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chenxi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_S/0/1/0/all/0/1&quot;&gt;Sun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qianxiong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhishuai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_C/0/1/0/all/0/1&quot;&gt;Cheng Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10225">
<title>ChatQA: Building GPT-4 Level Conversational QA Models. (arXiv:2401.10225v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10225</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce ChatQA, a family of conversational question
answering (QA) models that obtain GPT-4 level accuracies. Specifically, we
propose a two-stage instruction tuning method that can significantly improve
the zero-shot conversational QA results from large language models (LLMs). To
handle retrieval-augmented generation in conversational QA, we fine-tune a
dense retriever on a multi-turn QA dataset, which provides comparable results
to using the state-of-the-art query rewriting model while largely reducing
deployment cost. Notably, our ChatQA-70B can outperform GPT-4 in terms of
average score on 10 conversational QA datasets (54.14 vs. 53.90), without
relying on any synthetic data from OpenAI GPT models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zihan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ping_W/0/1/0/all/0/1&quot;&gt;Wei Ping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roy_R/0/1/0/all/0/1&quot;&gt;Rajarshi Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;Chankyu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shoeybi_M/0/1/0/all/0/1&quot;&gt;Mohammad Shoeybi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Catanzaro_B/0/1/0/all/0/1&quot;&gt;Bryan Catanzaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10754">
<title>Data Augmentation for Traffic Classification. (arXiv:2401.10754v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10754</link>
<description rdf:parseType="Literal">&lt;p&gt;Data Augmentation (DA) -- enriching training data by adding synthetic samples
-- is a technique widely adopted in Computer Vision (CV) and Natural Language
Processing (NLP) tasks to improve models performance. Yet, DA has struggled to
gain traction in networking contexts, particularly in Traffic Classification
(TC) tasks. In this work, we fulfill this gap by benchmarking 18 augmentation
functions applied to 3 TC datasets using packet time series as input
representation and considering a variety of training conditions. Our results
show that (i) DA can reap benefits previously unexplored, (ii) augmentations
acting on time series sequence order and masking are better suited for TC than
amplitude augmentations and (iii) basic models latent space analysis can help
understanding the positive/negative effects of augmentations on classification
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finamore_A/0/1/0/all/0/1&quot;&gt;Alessandro Finamore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michiardi_P/0/1/0/all/0/1&quot;&gt;Pietro Michiardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallo_M/0/1/0/all/0/1&quot;&gt;Massimo Gallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_D/0/1/0/all/0/1&quot;&gt;Dario Rossi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11202">
<title>PartIR: Composing SPMD Partitioning Strategies for Machine Learning. (arXiv:2401.11202v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11202</link>
<description rdf:parseType="Literal">&lt;p&gt;Training of modern large neural networks (NN) requires a combination of
parallelization strategies encompassing data, model, or optimizer sharding.
When strategies increase in complexity, it becomes necessary for partitioning
tools to be 1) expressive, allowing the composition of simpler strategies, and
2) predictable to estimate performance analytically. We present PartIR, our
design for a NN partitioning system. PartIR is focused on an incremental
approach to rewriting and is hardware-and-runtime agnostic. We present a simple
but powerful API for composing sharding strategies and a simulator to validate
them. The process is driven by high-level programmer-issued partitioning
tactics, which can be both manual and automatic. Importantly, the tactics are
specified separately from the model code, making them easy to change. We
evaluate PartIR on several different models to demonstrate its predictability,
expressibility, and ability to reach peak performance..
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabed_S/0/1/0/all/0/1&quot;&gt;Sami Alabed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chrzaszcz_B/0/1/0/all/0/1&quot;&gt;Bart Chrzaszcz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franco_J/0/1/0/all/0/1&quot;&gt;Juliana Franco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grewe_D/0/1/0/all/0/1&quot;&gt;Dominik Grewe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maclaurin_D/0/1/0/all/0/1&quot;&gt;Dougal Maclaurin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_J/0/1/0/all/0/1&quot;&gt;James Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natan_T/0/1/0/all/0/1&quot;&gt;Tom Natan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norman_T/0/1/0/all/0/1&quot;&gt;Tamara Norman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paszke_A/0/1/0/all/0/1&quot;&gt;Adam Paszke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rink_N/0/1/0/all/0/1&quot;&gt;Norman A. Rink&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaarschmidt_M/0/1/0/all/0/1&quot;&gt;Michael Schaarschmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sitdikov_T/0/1/0/all/0/1&quot;&gt;Timur Sitdikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swietlik_A/0/1/0/all/0/1&quot;&gt;Agnieszka Swietlik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vytiniotis_D/0/1/0/all/0/1&quot;&gt;Dimitrios Vytiniotis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wee_J/0/1/0/all/0/1&quot;&gt;Joel Wee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11447">
<title>Sequential Model for Predicting Patient Adherence in Subcutaneous Immunotherapy for Allergic Rhinitis. (arXiv:2401.11447v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11447</link>
<description rdf:parseType="Literal">&lt;p&gt;Objective: Subcutaneous Immunotherapy (SCIT) is the long-lasting causal
treatment of allergic rhinitis. How to enhance the adherence of patients to
maximize the benefit of allergen immunotherapy (AIT) plays a crucial role in
the management of AIT. This study aims to leverage novel machine learning
models to precisely predict the risk of non-adherence of patients and related
systematic symptom scores, to provide a novel approach in the management of
long-term AIT.
&lt;/p&gt;
&lt;p&gt;Methods: The research develops and analyzes two models, Sequential Latent
Actor-Critic (SLAC) and Long Short-Term Memory (LSTM), evaluating them based on
scoring and adherence prediction capabilities.
&lt;/p&gt;
&lt;p&gt;Results: Excluding the biased samples at the first time step, the predictive
adherence accuracy of the SLAC models is from $60\,\%$ to $72\%$, and for LSTM
models, it is $66\,\%$ to $84\,\%$, varying according to the time steps. The
range of Root Mean Square Error (RMSE) for SLAC models is between $0.93$ and
$2.22$, while for LSTM models it is between $1.09$ and $1.77$. Notably, these
RMSEs are significantly lower than the random prediction error of $4.55$.
&lt;/p&gt;
&lt;p&gt;Conclusion: We creatively apply sequential models in the long-term management
of SCIT with promising accuracy in the prediction of SCIT nonadherence in
Allergic Rhinitis (AR) patients. While LSTM outperforms SLAC in adherence
prediction, SLAC excels in score prediction for patients undergoing SCIT for
AR. The state-action-based SLAC adds flexibility, presenting a novel and
effective approach for managing long-term AIT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Y/0/1/0/all/0/1&quot;&gt;Yu Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_W/0/1/0/all/0/1&quot;&gt;Wenxin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qingqing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Si_L/0/1/0/all/0/1&quot;&gt;Liping Si&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smagt_P/0/1/0/all/0/1&quot;&gt;Patrick van der Smagt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jun Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_N/0/1/0/all/0/1&quot;&gt;Nutan Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11488">
<title>HARDCORE: H-field and power loss estimation for arbitrary waveforms with residual, dilated convolutional neural networks in ferrite cores. (arXiv:2401.11488v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11488</link>
<description rdf:parseType="Literal">&lt;p&gt;The MagNet Challenge 2023 calls upon competitors to develop data-driven
models for the material-specific, waveform-agnostic estimation of steady-state
power losses in toroidal ferrite cores. The following HARDCORE (H-field and
power loss estimation for Arbitrary waveforms with Residual, Dilated
convolutional neural networks in ferrite COREs) approach shows that a residual
convolutional neural network with physics-informed extensions can serve this
task efficiently when trained on observational data beforehand. One key
solution element is an intermediate model layer which first reconstructs the bh
curve and then estimates the power losses based on the curve&apos;s area rendering
the proposed topology physically interpretable. In addition, emphasis was
placed on expert-based feature engineering and information-rich inputs in order
to enable a lean model architecture. A model is trained from scratch for each
material, while the topology remains the same. A Pareto-style trade-off between
model size and estimation accuracy is demonstrated, which yields an optimum at
as low as 1755 parameters and down to below 8\,\% for the 95-th percentile of
the relative error for the worst-case material with sufficient samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kirchgassner_W/0/1/0/all/0/1&quot;&gt;Wilhelm Kirchg&amp;#xe4;ssner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Forster_N/0/1/0/all/0/1&quot;&gt;Nikolas F&amp;#xf6;rster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piepenbrock_T/0/1/0/all/0/1&quot;&gt;Till Piepenbrock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schweins_O/0/1/0/all/0/1&quot;&gt;Oliver Schweins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wallscheid_O/0/1/0/all/0/1&quot;&gt;Oliver Wallscheid&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11687">
<title>TIM: An Efficient Temporal Interaction Module for Spiking Transformer. (arXiv:2401.11687v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11687</link>
<description rdf:parseType="Literal">&lt;p&gt;Spiking Neural Networks (SNNs), as the third generation of neural networks,
have gained prominence for their biological plausibility and computational
efficiency, especially in processing diverse datasets. The integration of
attention mechanisms, inspired by advancements in neural network architectures,
has led to the development of Spiking Transformers. These have shown promise in
enhancing SNNs&apos; capabilities, particularly in the realms of both static and
neuromorphic datasets. Despite their progress, a discernible gap exists in
these systems, specifically in the Spiking Self Attention (SSA) mechanism&apos;s
effectiveness in leveraging the temporal processing potential of SNNs. To
address this, we introduce the Temporal Interaction Module (TIM), a novel,
convolution-based enhancement designed to augment the temporal data processing
abilities within SNN architectures. TIM&apos;s integration into existing SNN
frameworks is seamless and efficient, requiring minimal additional parameters
while significantly boosting their temporal information handling capabilities.
Through rigorous experimentation, TIM has demonstrated its effectiveness in
exploiting temporal information, leading to state-of-the-art performance across
various neuromorphic datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_S/0/1/0/all/0/1&quot;&gt;Sicheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongcheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_G/0/1/0/all/0/1&quot;&gt;Guobin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Y/0/1/0/all/0/1&quot;&gt;Yi Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11748">
<title>GI-PIP: Do We Require Impractical Auxiliary Dataset for Gradient Inversion Attacks?. (arXiv:2401.11748v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11748</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep gradient inversion attacks expose a serious threat to Federated Learning
(FL) by accurately recovering private data from shared gradients. However, the
state-of-the-art heavily relies on impractical assumptions to access excessive
auxiliary data, which violates the basic data partitioning principle of FL. In
this paper, a novel method, Gradient Inversion Attack using Practical Image
Prior (GI-PIP), is proposed under a revised threat model. GI-PIP exploits
anomaly detection models to capture the underlying distribution from fewer
data, while GAN-based methods consume significant more data to synthesize
images. The extracted distribution is then leveraged to regulate the attack
process as Anomaly Score loss. Experimental results show that GI-PIP achieves a
16.12 dB PSNR recovery using only 3.8% data of ImageNet, while GAN-based
methods necessitate over 70%. Moreover, GI-PIP exhibits superior capability on
distribution generalization compared to GAN-based methods. Our approach
significantly alleviates the auxiliary data requirement on both amount and
distribution in gradient inversion attacks, hence posing more substantial
threat to real-world FL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+sun_Y/0/1/0/all/0/1&quot;&gt;Yu sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_G/0/1/0/all/0/1&quot;&gt;Gaojian Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xianxun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kailang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jian Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.11792">
<title>Safe and Generalized end-to-end Autonomous Driving System with Reinforcement Learning and Demonstrations. (arXiv:2401.11792v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2401.11792</link>
<description rdf:parseType="Literal">&lt;p&gt;An intelligent driving system should be capable of dynamically formulating
appropriate driving strategies based on the current environment and vehicle
status, while ensuring the security and reliability of the system. However,
existing methods based on reinforcement learning and imitation learning suffer
from low safety, poor generalization, and inefficient sampling. Additionally,
they cannot accurately predict future driving trajectories, and the accurate
prediction of future driving trajectories is a precondition for making optimal
decisions. To solve these problems, in this paper, we introduce a Safe and
Generalized end-to-end Autonomous Driving System (SGADS) for complex and
various scenarios. Our SGADS incorporates variational inference with
normalizing flows, enabling the intelligent vehicle to accurately predict
future driving trajectories. Moreover, we propose the formulation of robust
safety constraints. Furthermore, we combine reinforcement learning with
demonstrations to augment search process of the agent. The experimental results
demonstrate that our SGADS can significantly improve safety performance,
exhibit strong generalization, and enhance the training efficiency of
intelligent vehicles in complex urban scenarios compared to existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zuojin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;YongQiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianyu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.12890">
<title>Dual Online Stein Variational Inference for Control and Dynamics. (arXiv:2103.12890v1 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2103.12890</link>
<description rdf:parseType="Literal">&lt;p&gt;Model predictive control (MPC) schemes have a proven track record for
delivering aggressive and robust performance in many challenging control tasks,
coping with nonlinear system dynamics, constraints, and observational noise.
Despite their success, these methods often rely on simple control
distributions, which can limit their performance in highly uncertain and
complex environments. MPC frameworks must be able to accommodate changing
distributions over system parameters, based on the most recent measurements. In
this paper, we devise an implicit variational inference algorithm able to
estimate distributions over model parameters and control inputs on-the-fly. The
method incorporates Stein Variational gradient descent to approximate the
target distributions as a collection of particles, and performs updates based
on a Bayesian formulation. This enables the approximation of complex
multi-modal posterior distributions, typically occurring in challenging and
realistic robot navigation tasks. We demonstrate our approach on both simulated
and real-world experiments requiring real-time execution in the face of
dynamically changing environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barcelos_L/0/1/0/all/0/1&quot;&gt;Lucas Barcelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lambert_A/0/1/0/all/0/1&quot;&gt;Alexander Lambert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliveira_R/0/1/0/all/0/1&quot;&gt;Rafael Oliveira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Borges_P/0/1/0/all/0/1&quot;&gt;Paulo Borges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boots_B/0/1/0/all/0/1&quot;&gt;Byron Boots&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramos_F/0/1/0/all/0/1&quot;&gt;Fabio Ramos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09994">
<title>Improving Urban Flood Prediction using LSTM-DeepLabv3+ and Bayesian Optimization with Spatiotemporal feature fusion. (arXiv:2304.09994v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2304.09994</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have become increasingly popular for flood prediction
due to their superior accuracy and efficiency compared to traditional methods.
However, current machine learning methods often rely on separate spatial or
temporal feature analysis and have limitations on the types, number, and
dimensions of input data. This study presented a CNN-RNN hybrid feature fusion
modelling approach for urban flood prediction, which integrated the strengths
of CNNs in processing spatial features and RNNs in analyzing different
dimensions of time sequences. This approach allowed for both static and dynamic
flood predictions. Bayesian optimization was applied to identify the seven most
influential flood-driven factors and determine the best combination strategy.
By combining four CNNs (FCN, UNet, SegNet, DeepLabv3+) and three RNNs (LSTM,
BiLSTM, GRU), the optimal hybrid model was identified as LSTM-DeepLabv3+. This
model achieved the highest prediction accuracy (MAE, RMSE, NSE, and KGE were
0.007, 0.025, 0.973 and 0.755, respectively) under various rainfall input
conditions. Additionally, the processing speed was significantly improved, with
an inference time of 1.158s (approximately 1/125 of the traditional computation
time) compared to the physically-based models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Situ_Z/0/1/0/all/0/1&quot;&gt;Zuxiang Situ&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teng_S/0/1/0/all/0/1&quot;&gt;Shuai Teng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_W/0/1/0/all/0/1&quot;&gt;Wanen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Gongfa Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qianqian Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1&quot;&gt;Guangtao Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.06980">
<title>Joint Unsupervised and Supervised Training for Automatic Speech Recognition via Bilevel Optimization. (arXiv:2401.06980v1 [cs.CL] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.06980</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel bilevel optimization-based training
approach to training acoustic models for automatic speech recognition (ASR)
tasks that we term {bi-level joint unsupervised and supervised training
(BL-JUST)}. {BL-JUST employs a lower and upper level optimization with an
unsupervised loss and a supervised loss respectively, leveraging recent
advances in penalty-based bilevel optimization to solve this challenging ASR
problem with affordable complexity and rigorous convergence guarantees.} To
evaluate BL-JUST, extensive experiments on the LibriSpeech and TED-LIUM v2
datasets have been conducted. BL-JUST achieves superior performance over the
commonly used pre-training followed by fine-tuning strategy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saif_A/0/1/0/all/0/1&quot;&gt;A F M Saif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_X/0/1/0/all/0/1&quot;&gt;Xiaodong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Han Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Songtao Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kingsbury_B/0/1/0/all/0/1&quot;&gt;Brian Kingsbury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tianyi Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.12196">
<title>Learning Dynamics from Multicellular Graphs with Deep Neural Networks. (arXiv:2401.12196v1 [physics.bio-ph] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2401.12196</link>
<description rdf:parseType="Literal">&lt;p&gt;The inference of multicellular self-assembly is the central quest of
understanding morphogenesis, including embryos, organoids, tumors, and many
others. However, it has been tremendously difficult to identify structural
features that can indicate multicellular dynamics. Here we propose to harness
the predictive power of graph-based deep neural networks (GNN) to discover
important graph features that can predict dynamics. To demonstrate, we apply a
physically informed GNN (piGNN) to predict the motility of multicellular
collectives from a snapshot of their positions both in experiments and
simulations. We demonstrate that piGNN is capable of navigating through complex
graph features of multicellular living systems, which otherwise can not be
achieved by classical mechanistic models. With increasing amounts of
multicellular data, we propose that collaborative efforts can be made to create
a multicellular data bank (MDB) from which it is possible to construct a large
multicellular graph model (LMGM) for general-purposed predictions of
multicellular organization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haiqian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Meyer_F/0/1/0/all/0/1&quot;&gt;Florian Meyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shaoxun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Liu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lungu_C/0/1/0/all/0/1&quot;&gt;Cristiana Lungu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Olayioye_M/0/1/0/all/0/1&quot;&gt;Monilola A. Olayioye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Buehler_M/0/1/0/all/0/1&quot;&gt;Markus J. Buehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Ming Guo&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>