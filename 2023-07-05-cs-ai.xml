<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-03T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00004" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00014" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00020" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00028" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00101" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00108" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00112" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00161" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00169" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00184" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00240" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00266" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00306" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00316" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1911.10322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.06258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2007.07703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.07888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.12021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2105.14452" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.03892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.07905" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.06527" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.00667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10540" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.10553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.00884" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.04422" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.02646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.13678" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.05723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.10737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.13507" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.11736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.10511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.13679" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.03147" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.12579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01470" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.03122" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.06354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.09543" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10258" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.10894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11012" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.11563" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04356" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.12040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00962" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.07702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10888" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05392" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06174" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13681" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.15769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16380" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18438" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00942" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02739" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.04376" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05480" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07874" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10509" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.10946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.11667" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.12729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15156" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15656" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15969" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16705" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16817" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17624" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.09325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.00068" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.00004">
<title>PV Fleet Modeling via Smooth Periodic Gaussian Copula. (arXiv:2307.00004v1 [stat.AP])</title>
<link>http://arxiv.org/abs/2307.00004</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a method for jointly modeling power generation from a fleet of
photovoltaic (PV) systems. We propose a white-box method that finds a function
that invertibly maps vector time-series data to independent and identically
distributed standard normal variables. The proposed method, based on a novel
approach for fitting a smooth, periodic copula transform to data, captures many
aspects of the data such as diurnal variation in the distribution of power
output, dependencies among different PV systems, and dependencies across time.
It consists of interpretable steps and is scalable to many systems. The
resulting joint probability model of PV fleet output across systems and time
can be used to generate synthetic data, impute missing data, perform anomaly
detection, and make forecasts. In this paper, we explain the method and
demonstrate these applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ogut_M/0/1/0/all/0/1&quot;&gt;Mehmet G. Ogut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Meyers_B/0/1/0/all/0/1&quot;&gt;Bennet Meyers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Boyd_S/0/1/0/all/0/1&quot;&gt;Stephen P. Boyd&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00008">
<title>Investigating Masking-based Data Generation in Language Models. (arXiv:2307.00008v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.00008</link>
<description rdf:parseType="Literal">&lt;p&gt;The current era of natural language processing (NLP) has been defined by the
prominence of pre-trained language models since the advent of BERT. A feature
of BERT and models with similar architecture is the objective of masked
language modeling, in which part of the input is intentionally masked and the
model is trained to predict this piece of masked information. Data augmentation
is a data-driven technique widely used in machine learning, including research
areas like computer vision and natural language processing, to improve model
performance by artificially augmenting the training data set by designated
techniques. Masked language models (MLM), an essential training feature of
BERT, have introduced a novel approach to perform effective pre-training on
Transformer based models in natural language processing tasks. Recent studies
have utilized masked language model to generate artificially augmented data for
NLP downstream tasks. The experimental results show that Mask based data
augmentation method provides a simple but efficient approach to improve the
model performance. In this paper, we explore and discuss the broader
utilization of these data augmentation methods based on MLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_E/0/1/0/all/0/1&quot;&gt;Ed S. Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00012">
<title>Black-Box Prediction of Flaky Test Fix Categories Using Language Models. (arXiv:2307.00012v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2307.00012</link>
<description rdf:parseType="Literal">&lt;p&gt;Flaky tests are problematic because they non-deterministically pass or fail
for the same software version under test, causing confusion and wasting
developer time. While machine learning models have been used to predict
flakiness and its root causes, there is less work on providing support to fix
the problem. To address this gap, we propose a framework that automatically
generates labeled datasets for 13 fix categories and train models to predict
the fix category of a flaky test by analyzing the test code only. Though it is
unrealistic at this stage to accurately predict the fix itself, the categories
provide precise guidance about what part of the test code to look at. Our
approach is based on language models, namely CodeBERT and UniXcoder, whose
output is fine-tuned with a Feed Forward Neural Network (FNN) or a Siamese
Network-based Few Shot Learning (FSL). Our experimental results show that
UniXcoder outperforms CodeBERT, in correctly predicting most of the categories
of fixes a developer should apply. Furthermore, FSL does not appear to have any
significant effect. Given the high accuracy obtained for most fix categories,
our proposed framework has the potential to help developers to fix flaky tests
quickly and accurately.To aid future research, we make our automated labeling
tool, dataset, prediction models, and experimental infrastructure publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatima_S/0/1/0/all/0/1&quot;&gt;Sakina Fatima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hemmati_H/0/1/0/all/0/1&quot;&gt;Hadi Hemmati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briand_L/0/1/0/all/0/1&quot;&gt;Lionel Briand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00014">
<title>Inertial Navigation Meets Deep Learning: A Survey of Current Trends and Future Directions. (arXiv:2307.00014v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.00014</link>
<description rdf:parseType="Literal">&lt;p&gt;Inertial sensing is used in many applications and platforms, ranging from
day-to-day devices such as smartphones to very complex ones such as autonomous
vehicles. In recent years, the development of machine learning and deep
learning techniques has increased significantly in the field of inertial
sensing. This is due to the development of efficient computing hardware and the
accessibility of publicly available sensor data. These data-driven approaches
are used to empower model-based navigation and sensor fusion algorithms. This
paper provides an in-depth review of those deep learning methods. We examine
separately, each vehicle operation domain including land, air, and sea. Each
domain is divided into pure inertial advances and improvements based on filter
parameters learning. In addition, we review deep learning approaches for
calibrating and denoising inertial sensors. Throughout the paper, we discuss
these trends and future directions. We also provide statistics on the commonly
used approaches to illustrate their efficiency and stimulate further research
in deep learning embedded in inertial navigation and fusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_N/0/1/0/all/0/1&quot;&gt;Nadav Cohen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klein_I/0/1/0/all/0/1&quot;&gt;Itzik Klein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00020">
<title>CASEIN: Cascading Explicit and Implicit Control for Fine-grained Emotion Intensity Regulation. (arXiv:2307.00020v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.00020</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing fine-grained intensity regulation methods rely on explicit control
through predicted emotion probabilities. However, these high-level semantic
probabilities are often inaccurate and unsmooth at the phoneme level, leading
to bias in learning. Especially when we attempt to mix multiple emotion
intensities for specific phonemes, resulting in markedly reduced
controllability and naturalness of the synthesis. To address this issue, we
propose the CAScaded Explicit and Implicit coNtrol framework (CASEIN), which
leverages accurate disentanglement of emotion manifolds from the reference
speech to learn the implicit representation at a lower semantic level. This
representation bridges the semantical gap between explicit probabilities and
the synthesis model, reducing bias in learning. In experiments, our CASEIN
surpasses existing methods in both controllability and naturalness. Notably, we
are the first to achieve fine-grained control over the mixed intensity of
multiple emotions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yuhao Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiongwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhongzhou Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Wei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haiqing Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00028">
<title>Seeing in Words: Learning to Classify through Language Bottlenecks. (arXiv:2307.00028v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00028</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks for computer vision extract uninterpretable features despite
achieving high accuracy on benchmarks. In contrast, humans can explain their
predictions using succinct and intuitive descriptions. To incorporate
explainability into neural networks, we train a vision model whose feature
representations are text. We show that such a model can effectively classify
ImageNet images, and we discuss the challenges we encountered when training it.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saifullah_K/0/1/0/all/0/1&quot;&gt;Khalid Saifullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yuxin Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiping_J/0/1/0/all/0/1&quot;&gt;Jonas Geiping&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldblum_M/0/1/0/all/0/1&quot;&gt;Micah Goldblum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldstein_T/0/1/0/all/0/1&quot;&gt;Tom Goldstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00032">
<title>Uncertainty Informed Optimal Resource Allocation with Gaussian Process based Bayesian Inference. (arXiv:2307.00032v1 [math.OC])</title>
<link>http://arxiv.org/abs/2307.00032</link>
<description rdf:parseType="Literal">&lt;p&gt;We focus on the problem of uncertainty informed allocation of medical
resources (vaccines) to heterogeneous populations for managing epidemic spread.
We tackle two related questions: (1) For a compartmental ordinary differential
equation (ODE) model of epidemic spread, how can we estimate and integrate
parameter uncertainty into resource allocation decisions? (2) How can we
computationally handle both nonlinear ODE constraints and parameter
uncertainties for a generic stochastic optimization problem for resource
allocation? To the best of our knowledge current literature does not fully
resolve these questions. Here, we develop a data-driven approach to represent
parameter uncertainty accurately and tractably in a novel stochastic
optimization problem formulation. We first generate a tractable scenario set by
estimating the distribution on ODE model parameters using Bayesian inference
with Gaussian processes. Next, we develop a parallelized solution algorithm
that accounts for scenario-dependent nonlinear ODE constraints. Our
scenario-set generation procedure and solution approach are flexible in that
they can handle any compartmental epidemiological ODE model. Our computational
experiments on two different non-linear ODE models (SEIR and SEPIHR) indicate
that accounting for uncertainty in key epidemiological parameters can improve
the efficacy of time-critical allocation decisions by 4-8%. This improvement
can be attributed to data-driven and optimal (strategic) nature of vaccine
allocations, especially in the early stages of the epidemic when the allocation
strategy can crucially impact the long-term trajectory of the disease.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Gupta_S/0/1/0/all/0/1&quot;&gt;Samarth Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Amin_S/0/1/0/all/0/1&quot;&gt;Saurabh Amin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00039">
<title>Towards Brain Inspired Design for Addressing the Shortcomings of ANNs. (arXiv:2307.00039v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2307.00039</link>
<description rdf:parseType="Literal">&lt;p&gt;As our understanding of the mechanisms of brain function is enhanced, the
value of insights gained from neuroscience to the development of AI algorithms
deserves further consideration. Here, we draw parallels with an existing
tree-based ANN architecture and a recent neuroscience study[27] arguing that
the error-based organization of neurons in the cerebellum that share a
preference for a personalized view of the entire error space, may account for
several desirable features of behavior and learning. We then analyze the
learning behavior and characteristics of the model under varying scenarios to
gauge the potential benefits of a similar mechanism in ANN. Our empirical
results suggest that having separate populations of neurons with personalized
error views can enable efficient learning under class imbalance and limited
data, and reduce the susceptibility to unintended shortcut strategies, leading
to improved generalization. This work highlights the potential of translating
the learning machinery of the brain into the design of a new generation of ANNs
and provides further credence to the argument that biologically inspired AI may
hold the key to overcoming the shortcomings of ANNs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarfraz_F/0/1/0/all/0/1&quot;&gt;Fahad Sarfraz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arani_E/0/1/0/all/0/1&quot;&gt;Elahe Arani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zonooz_B/0/1/0/all/0/1&quot;&gt;Bahram Zonooz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00040">
<title>DisCo: Disentangled Control for Referring Human Dance Generation in Real World. (arXiv:2307.00040v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00040</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative AI has made significant strides in computer vision, particularly
in image/video synthesis conditioned on text descriptions. Despite the
advancements, it remains challenging especially in the generation of
human-centric content such as dance synthesis. Existing dance synthesis methods
struggle with the gap between synthesized content and real-world dance
scenarios. In this paper, we define a new problem setting: Referring Human
Dance Generation, which focuses on real-world dance scenarios with three
important properties: (i) Faithfulness: the synthesis should retain the
appearance of both human subject foreground and background from the reference
image, and precisely follow the target pose; (ii) Generalizability: the model
should generalize to unseen human subjects, backgrounds, and poses; (iii)
Compositionality: it should allow for composition of seen/unseen subjects,
backgrounds, and poses from different sources. To address these challenges, we
introduce a novel approach, DISCO, which includes a novel model architecture
with disentangled control to improve the faithfulness and compositionality of
dance synthesis, and an effective human attribute pre-training for better
generalizability to unseen humans. Extensive qualitative and quantitative
results demonstrate that DISCO can generate high-quality human dance images and
videos with diverse appearances and flexible motions. Code, demo, video and
visualization are available at: https://disco-dance.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_L/0/1/0/all/0/1&quot;&gt;Linjie Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kevin Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Chung-Ching Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hanwang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lijuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00065">
<title>Qualitative Prediction of Multi-Agent Spatial Interactions. (arXiv:2307.00065v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.00065</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying service robots in our daily life, whether in restaurants,
warehouses or hospitals, calls for the need to reason on the interactions
happening in dense and dynamic scenes. In this paper, we present and benchmark
three new approaches to model and predict multi-agent interactions in dense
scenes, including the use of an intuitive qualitative representation. The
proposed solutions take into account static and dynamic context to predict
individual interactions. They exploit an input- and a temporal-attention
mechanism, and are tested on medium and long-term time horizons. The first two
approaches integrate different relations from the so-called Qualitative
Trajectory Calculus (QTC) within a state-of-the-art deep neural network to
create a symbol-driven neural architecture for predicting spatial interactions.
The third approach implements a purely data-driven network for motion
prediction, the output of which is post-processed to predict QTC spatial
interactions. Experimental results on a popular robot dataset of challenging
crowded scenarios show that the purely data-driven prediction approach
generally outperforms the other two. The three approaches were further
evaluated on a different but related human scenarios to assess their
generalisation capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mghames_S/0/1/0/all/0/1&quot;&gt;Sariah Mghames&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castri_L/0/1/0/all/0/1&quot;&gt;Luca Castri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanheide_M/0/1/0/all/0/1&quot;&gt;Marc Hanheide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellotto_N/0/1/0/all/0/1&quot;&gt;Nicola Bellotto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00067">
<title>Transformers in Healthcare: A Survey. (arXiv:2307.00067v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.00067</link>
<description rdf:parseType="Literal">&lt;p&gt;With Artificial Intelligence (AI) increasingly permeating various aspects of
society, including healthcare, the adoption of the Transformers neural network
architecture is rapidly changing many applications. Transformer is a type of
deep learning architecture initially developed to solve general-purpose Natural
Language Processing (NLP) tasks and has subsequently been adapted in many
fields, including healthcare. In this survey paper, we provide an overview of
how this architecture has been adopted to analyze various forms of data,
including medical imaging, structured and unstructured Electronic Health
Records (EHR), social media, physiological signals, and biomolecular sequences.
Those models could help in clinical diagnosis, report generation, data
reconstruction, and drug/protein synthesis. We identified relevant studies
using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses
(PRISMA) guidelines. We also discuss the benefits and limitations of using
transformers in healthcare and examine issues such as computational cost, model
interpretability, fairness, alignment with human values, ethical implications,
and environmental impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nerella_S/0/1/0/all/0/1&quot;&gt;Subhash Nerella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bandyopadhyay_S/0/1/0/all/0/1&quot;&gt;Sabyasachi Bandyopadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiaqing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Contreras_M/0/1/0/all/0/1&quot;&gt;Miguel Contreras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Siegel_S/0/1/0/all/0/1&quot;&gt;Scott Siegel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bumin_A/0/1/0/all/0/1&quot;&gt;Aysegul Bumin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_B/0/1/0/all/0/1&quot;&gt;Brandon Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sena_J/0/1/0/all/0/1&quot;&gt;Jessica Sena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shickel_B/0/1/0/all/0/1&quot;&gt;Benjamin Shickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bihorac_A/0/1/0/all/0/1&quot;&gt;Azra Bihorac&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khezeli_K/0/1/0/all/0/1&quot;&gt;Kia Khezeli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidi_P/0/1/0/all/0/1&quot;&gt;Parisa Rashidi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00101">
<title>Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models. (arXiv:2307.00101v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.00101</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) are trained primarily on minimally processed web
text, which exhibits the same wide range of social biases held by the humans
who created that content. Consequently, text generated by LLMs can
inadvertently perpetuate stereotypes towards marginalized groups, like the
LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs
generate text describing people with different sexual identities. Analyzing
bias in the text generated by an LLM using regard score shows measurable bias
against queer people. We then show that a post-hoc method based on
chain-of-thought prompting using SHAP analysis can increase the regard of the
sentence, representing a promising approach towards debiasing the output of
LLMs in this setting.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhingra_H/0/1/0/all/0/1&quot;&gt;Harnoor Dhingra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayashanker_P/0/1/0/all/0/1&quot;&gt;Preetiha Jayashanker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moghe_S/0/1/0/all/0/1&quot;&gt;Sayali Moghe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1&quot;&gt;Emma Strubell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00104">
<title>Obscured Wildfire Flame Detection By Temporal Analysis of Smoke Patterns Captured by Unmanned Aerial Systems. (arXiv:2307.00104v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00104</link>
<description rdf:parseType="Literal">&lt;p&gt;This research paper addresses the challenge of detecting obscured wildfires
(when the fire flames are covered by trees, smoke, clouds, and other natural
barriers) in real-time using drones equipped only with RGB cameras. We propose
a novel methodology that employs semantic segmentation based on the temporal
analysis of smoke patterns in video sequences. Our approach utilizes an
encoder-decoder architecture based on deep convolutional neural network
architecture with a pre-trained CNN encoder and 3D convolutions for decoding
while using sequential stacking of features to exploit temporal variations. The
predicted fire locations can assist drones in effectively combating forest
fires and pinpoint fire retardant chemical drop on exact flame locations. We
applied our method to a curated dataset derived from the FLAME2 dataset that
includes RGB video along with IR video to determine the ground truth. Our
proposed method has a unique property of detecting obscured fire and achieves a
Dice score of 85.88%, while achieving a high precision of 92.47% and
classification accuracy of 90.67% on test data showing promising results when
inspected visually. Indeed, our method outperforms other methods by a
significant margin in terms of video-level fire classification as we obtained
about 100% accuracy using MobileNet+CBAM as the encoder backbone.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meleti_U/0/1/0/all/0/1&quot;&gt;Uma Meleti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Razi_A/0/1/0/all/0/1&quot;&gt;Abolfazl Razi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00108">
<title>Ticket-BERT: Labeling Incident Management Tickets with Language Models. (arXiv:2307.00108v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.00108</link>
<description rdf:parseType="Literal">&lt;p&gt;An essential aspect of prioritizing incident tickets for resolution is
efficiently labeling tickets with fine-grained categories. However, ticket data
is often complex and poses several unique challenges for modern machine
learning methods: (1) tickets are created and updated either by machines with
pre-defined algorithms or by engineers with domain expertise that share
different protocols, (2) tickets receive frequent revisions that update ticket
status by modifying all or parts of ticket descriptions, and (3) ticket
labeling is time-sensitive and requires knowledge updates and new labels per
the rapid software and hardware improvement lifecycle. To handle these issues,
we introduce Ticket- BERT which trains a simple yet robust language model for
labeling tickets using our proposed ticket datasets. Experiments demonstrate
the superiority of Ticket-BERT over baselines and state-of-the-art text
classifiers on Azure Cognitive Services. We further encapsulate Ticket-BERT
with an active learning cycle and deploy it on the Microsoft IcM system, which
enables the model to quickly finetune on newly-collected tickets with a few
annotations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhexiong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benge_C/0/1/0/all/0/1&quot;&gt;Cris Benge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Siduo Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00112">
<title>Performance of ChatGPT on USMLE: Unlocking the Potential of Large Language Models for AI-Assisted Medical Education. (arXiv:2307.00112v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2307.00112</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence is gaining traction in more ways than ever before.
The popularity of language models and AI-based businesses has soared since
ChatGPT was made available to the general public via OpenAI. It is becoming
increasingly common for people to use ChatGPT both professionally and
personally. Considering the widespread use of ChatGPT and the reliance people
place on it, this study determined how reliable ChatGPT can be for answering
complex medical and clinical questions. Harvard University gross anatomy along
with the United States Medical Licensing Examination (USMLE) questionnaire were
used to accomplish the objective. The paper evaluated the obtained results
using a 2-way ANOVA and posthoc analysis. Both showed systematic covariation
between format and prompt. Furthermore, the physician adjudicators
independently rated the outcome&apos;s accuracy, concordance, and insight. As a
result of the analysis, ChatGPT-generated answers were found to be more
context-oriented and represented a better model for deductive reasoning than
regular Google search results. Furthermore, ChatGPT obtained 58.8% on logical
questions and 60% on ethical questions. This means that the ChatGPT is
approaching the passing range for logical questions and has crossed the
threshold for ethical questions. The paper believes ChatGPT and other language
learning models can be invaluable tools for e-learners; however, the study
suggests that there is still room to improve their accuracy. In order to
improve ChatGPT&apos;s performance in the future, further research is needed to
better understand how it can answer different types of questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_P/0/1/0/all/0/1&quot;&gt;Prabin Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thapa_K/0/1/0/all/0/1&quot;&gt;Kisan Thapa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhakal_P/0/1/0/all/0/1&quot;&gt;Prastab Dhakal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhaya_M/0/1/0/all/0/1&quot;&gt;Mala Deep Upadhaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhikari_S/0/1/0/all/0/1&quot;&gt;Santosh Adhikari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khanal_S/0/1/0/all/0/1&quot;&gt;Salik Ram Khanal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00114">
<title>A Personalized Household Assistive Robot that Learns and Creates New Breakfast Options through Human-Robot Interaction. (arXiv:2307.00114v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.00114</link>
<description rdf:parseType="Literal">&lt;p&gt;For robots to assist users with household tasks, they must first learn about
the tasks from the users. Further, performing the same task every day, in the
same way, can become boring for the robot&apos;s user(s), therefore, assistive
robots must find creative ways to perform tasks in the household. In this
paper, we present a cognitive architecture for a household assistive robot that
can learn personalized breakfast options from its users and then use the
learned knowledge to set up a table for breakfast. The architecture can also
use the learned knowledge to create new breakfast options over a longer period
of time. The proposed cognitive architecture combines state-of-the-art
perceptual learning algorithms, computational implementation of cognitive
models of memory encoding and learning, a task planner for picking and placing
objects in the household, a graphical user interface (GUI) to interact with the
user and a novel approach for creating new breakfast options using the learned
knowledge. The architecture is integrated with the Fetch mobile manipulator
robot and validated, as a proof-of-concept system evaluation in a large indoor
environment with multiple kitchen objects. Experimental results demonstrate the
effectiveness of our architecture to learn personalized breakfast options from
the user and generate new breakfast options never learned by the robot.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ayub_A/0/1/0/all/0/1&quot;&gt;Ali Ayub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nehaniv_C/0/1/0/all/0/1&quot;&gt;Chrystopher L. Nehaniv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dautenhahn_K/0/1/0/all/0/1&quot;&gt;Kerstin Dautenhahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00150">
<title>Large Language Models (GPT) for automating feedback on programming assignments. (arXiv:2307.00150v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2307.00150</link>
<description rdf:parseType="Literal">&lt;p&gt;Addressing the challenge of generating personalized feedback for programming
assignments is demanding due to several factors, like the complexity of code
syntax or different ways to correctly solve a task. In this experimental study,
we automated the process of feedback generation by employing OpenAI&apos;s GPT-3.5
model to generate personalized hints for students solving programming
assignments on an automated assessment platform. Students rated the usefulness
of GPT-generated hints positively. The experimental group (with GPT hints
enabled) relied less on the platform&apos;s regular feedback but performed better in
terms of percentage of successful submissions across consecutive attempts for
tasks, where GPT hints were enabled. For tasks where the GPT feedback was made
unavailable, the experimental group needed significantly less time to solve
assignments. Furthermore, when GPT hints were unavailable, students in the
experimental condition were initially less likely to solve the assignment
correctly. This suggests potential over-reliance on GPT-generated feedback.
However, students in the experimental condition were able to correct reasonably
rapidly, reaching the same percentage correct after seven submission attempts.
The availability of GPT hints did not significantly impact students&apos; affective
state.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pankiewicz_M/0/1/0/all/0/1&quot;&gt;Maciej Pankiewicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baker_R/0/1/0/all/0/1&quot;&gt;Ryan S. Baker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00154">
<title>Stitched ViTs are Flexible Vision Backbones. (arXiv:2307.00154v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00154</link>
<description rdf:parseType="Literal">&lt;p&gt;Large pretrained plain vision Transformers (ViTs) have been the workhorse for
many downstream tasks. However, existing works utilizing off-the-shelf ViTs are
inefficient in terms of training and deployment, because adopting ViTs with
individual sizes requires separate training and is restricted by fixed
performance-efficiency trade-offs. In this paper, we are inspired by stitchable
neural networks, which is a new framework that cheaply produces a single model
that covers rich subnetworks by stitching pretrained model families, supporting
diverse performance-efficiency trade-offs at runtime. Building upon this
foundation, we introduce SN-Netv2, a systematically improved model stitching
framework to facilitate downstream task adaptation. Specifically, we first
propose a Two-way stitching scheme to enlarge the stitching space. We then
design a resource-constrained sampling strategy that takes into account the
underlying FLOPs distributions in the space for improved sampling. Finally, we
observe that learning stitching layers is a low-rank update, which plays an
essential role on downstream tasks to stabilize training and ensure a good
Pareto frontier. With extensive experiments on ImageNet-1K, ADE20K,
COCO-Stuff-10K, NYUv2 and COCO-2017, SN-Netv2 demonstrates strong ability to
serve as a flexible vision backbone, achieving great advantages in both
training efficiency and adaptation. Code will be released at
https://github.com/ziplab/SN-Netv2.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zizheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haoyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00161">
<title>FFPDG: Fast, Fair and Private Data Generation. (arXiv:2307.00161v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00161</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative modeling has been used frequently in synthetic data generation.
Fairness and privacy are two big concerns for synthetic data. Although Recent
GAN [\cite{goodfellow2014generative}] based methods show good results in
preserving privacy, the generated data may be more biased. At the same time,
these methods require high computation resources. In this work, we design a
fast, fair, flexible and private data generation method. We show the
effectiveness of our method theoretically and empirically. We show that models
trained on data generated by the proposed method can perform well (in inference
stage) on real application scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jinjin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iannacci_F/0/1/0/all/0/1&quot;&gt;Francis Iannacci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00169">
<title>VoxWatch: An open-set speaker recognition benchmark on VoxCeleb. (arXiv:2307.00169v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2307.00169</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite its broad practical applications such as in fraud prevention,
open-set speaker identification (OSI) has received less attention in the
speaker recognition community compared to speaker verification (SV). OSI deals
with determining if a test speech sample belongs to a speaker from a set of
pre-enrolled individuals (in-set) or if it is from an out-of-set speaker. In
addition to the typical challenges associated with speech variability, OSI is
prone to the &quot;false-alarm problem&quot;; as the size of the in-set speaker
population (a.k.a watchlist) grows, the out-of-set scores become larger,
leading to increased false alarm rates. This is in particular challenging for
applications in financial institutions and border security where the watchlist
size is typically of the order of several thousand speakers. Therefore, it is
important to systematically quantify the false-alarm problem, and develop
techniques that alleviate the impact of watchlist size on detection
performance. Prior studies on this problem are sparse, and lack a common
benchmark for systematic evaluations. In this paper, we present the first
public benchmark for OSI, developed using the VoxCeleb dataset. We quantify the
effect of the watchlist size and speech duration on the watchlist-based speaker
detection task using three strong neural network based systems. In contrast to
the findings from prior research, we show that the commonly adopted adaptive
score normalization is not guaranteed to improve the performance for this task.
On the other hand, we show that score calibration and score fusion, two other
commonly used techniques in SV, result in significant improvements in OSI
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peri_R/0/1/0/all/0/1&quot;&gt;Raghuveer Peri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sadjadi_S/0/1/0/all/0/1&quot;&gt;Seyed Omid Sadjadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Garcia_Romero_D/0/1/0/all/0/1&quot;&gt;Daniel Garcia-Romero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00171">
<title>The Integer Linear Programming Inference Cookbook. (arXiv:2307.00171v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.00171</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the years, integer linear programs have been employed to model inference
in many natural language processing problems. This survey is meant to guide the
reader through the process of framing a new inference problem as an instance of
an integer linear program and is structured as a collection of recipes. At the
end, we will see two worked examples to illustrate the use of these recipes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srikumar_V/0/1/0/all/0/1&quot;&gt;Vivek Srikumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_D/0/1/0/all/0/1&quot;&gt;Dan Roth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00184">
<title>Personality Traits in Large Language Models. (arXiv:2307.00184v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.00184</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of large language models (LLMs) has revolutionized natural
language processing, enabling the generation of coherent and contextually
relevant text. As LLMs increasingly power conversational agents, the
synthesized personality embedded in these models by virtue of their training on
large amounts of human-generated data draws attention. Since personality is an
important factor determining the effectiveness of communication, we present a
comprehensive method for administering validated psychometric tests and
quantifying, analyzing, and shaping personality traits exhibited in text
generated from widely-used LLMs. We find that: 1) personality simulated in the
outputs of some LLMs (under specific prompting configurations) is reliable and
valid; 2) evidence of reliability and validity of LLM-simulated personality is
stronger for larger and instruction fine-tuned models; and 3) personality in
LLM outputs can be shaped along desired dimensions to mimic specific
personality profiles. We also discuss potential applications and ethical
implications of our measurement and shaping framework, especially regarding
responsible use of LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safdari_M/0/1/0/all/0/1&quot;&gt;Mustafa Safdari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serapio_Garcia_G/0/1/0/all/0/1&quot;&gt;Greg Serapio-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crepy_C/0/1/0/all/0/1&quot;&gt;Cl&amp;#xe9;ment Crepy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fitz_S/0/1/0/all/0/1&quot;&gt;Stephen Fitz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_P/0/1/0/all/0/1&quot;&gt;Peter Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_L/0/1/0/all/0/1&quot;&gt;Luning Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdulhai_M/0/1/0/all/0/1&quot;&gt;Marwa Abdulhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Faust_A/0/1/0/all/0/1&quot;&gt;Aleksandra Faust&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mataric_M/0/1/0/all/0/1&quot;&gt;Maja Matari&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00185">
<title>An Interpretable Constructive Algorithm for Incremental Random Weight Neural Networks and Its Application. (arXiv:2307.00185v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00185</link>
<description rdf:parseType="Literal">&lt;p&gt;Incremental random weight neural networks (IRWNNs) have gained attention in
view of its easy implementation and fast learning. However, a significant
drawback of IRWNNs is that the elationship between the hidden parameters
(node)and the residual error (model performance) is difficult to be
interpreted. To address the above issue, this article proposes an interpretable
constructive algorithm (ICA) with geometric information constraint. First,
based on the geometric relationship between the hidden parameters and the
residual error, an interpretable geometric information constraint is proposed
to randomly assign the hidden parameters. Meanwhile, a node pool strategy is
employed to obtain hidden parameters that is more conducive to convergence from
hidden parameters satisfying the proposed constraint. Furthermore, the
universal approximation property of the ICA is proved. Finally, a lightweight
version of ICA is presented for large-scale data modeling tasks. Experimental
results on six benchmark datasets and a numerical simulation dataset
demonstrate that the ICA outperforms other constructive algorithms in terms of
modeling speed, model accuracy, and model network structure. Besides, two
practical industrial application case are used to validate the effectiveness of
ICA in practical applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_J/0/1/0/all/0/1&quot;&gt;Jing Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_W/0/1/0/all/0/1&quot;&gt;Wei Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_G/0/1/0/all/0/1&quot;&gt;Guan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Ping Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00206">
<title>General Part Assembly Planning. (arXiv:2307.00206v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.00206</link>
<description rdf:parseType="Literal">&lt;p&gt;Most successes in autonomous robotic assembly have been restricted to single
target or category. We propose to investigate general part assembly, the task
of creating novel target assemblies with unseen part shapes. To tackle the
planning of general part assembly, we present General Part Assembly Transformer
(GPAT), a transformer based model architecture that accurately predicts part
poses by inferring how each part shape corresponds to the target shape. Our
experiments on both 3D CAD models and real-world scans demonstrate GPAT&apos;s
generalization abilities to novel and diverse target and part shapes. Project
website: https://general-part-assembly.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yulong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuran Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00209">
<title>Image Matters: A New Dataset and Empirical Study for Multimodal Hyperbole Detection. (arXiv:2307.00209v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00209</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperbole, or exaggeration, is a common linguistic phenomenon. The detection
of hyperbole is an important part of understanding human expression. There have
been several studies on hyperbole detection, but most of which focus on text
modality only. However, with the development of social media, people can create
hyperbolic expressions with various modalities, including text, images, videos,
etc. In this paper, we focus on multimodal hyperbole detection. We create a
multimodal detection dataset\footnote{The dataset will be released to the
community.} from Weibo (a Chinese social media) and carry out some studies on
it. We treat the text and image from a piece of weibo as two modalities and
explore the role of text and image for hyperbole detection. Different
pre-trained multimodal encoders are also evaluated on this downstream task to
show their performance. Besides, since this dataset is constructed from five
different topics, we also evaluate the cross-domain performance of different
models. These studies can serve as a benchmark and point out the direction of
further study on multimodal hyperbole detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiaojun Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00231">
<title>Forward-Forward Algorithm for Hyperspectral Image Classification: A Preliminary Study. (arXiv:2307.00231v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00231</link>
<description rdf:parseType="Literal">&lt;p&gt;The back-propagation algorithm has long been the de-facto standard in
optimizing weights and biases in neural networks, particularly in cutting-edge
deep learning models. Its widespread adoption in fields like natural language
processing, computer vision, and remote sensing has revolutionized automation
in various tasks. The popularity of back-propagation stems from its ability to
achieve outstanding performance in tasks such as classification, detection, and
segmentation. Nevertheless, back-propagation is not without its limitations,
encompassing sensitivity to initial conditions, vanishing gradients,
overfitting, and computational complexity. The recent introduction of a
forward-forward algorithm (FFA), which computes local goodness functions to
optimize network parameters, alleviates the dependence on substantial
computational resources and the constant need for architectural scaling. This
study investigates the application of FFA for hyperspectral image
classification. Experimental results and comparative analysis are provided with
the use of the traditional back-propagation algorithm. Preliminary results show
the potential behind FFA and its promises.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paheding_S/0/1/0/all/0/1&quot;&gt;Sidike Paheding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reyes_Angulo_A/0/1/0/all/0/1&quot;&gt;Abel A. Reyes-Angulo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00240">
<title>VesselMorph: Domain-Generalized Retinal Vessel Segmentation via Shape-Aware Representation. (arXiv:2307.00240v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00240</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the absence of a single standardized imaging protocol, domain shift
between data acquired from different sites is an inherent property of medical
images and has become a major obstacle for large-scale deployment of
learning-based algorithms. For retinal vessel images, domain shift usually
presents as the variation of intensity, contrast and resolution, while the
basic tubular shape of vessels remains unaffected. Thus, taking advantage of
such domain-invariant morphological features can greatly improve the
generalizability of deep models. In this study, we propose a method named
VesselMorph which generalizes the 2D retinal vessel segmentation task by
synthesizing a shape-aware representation. Inspired by the traditional Frangi
filter and the diffusion tensor imaging literature, we introduce a
Hessian-based bipolar tensor field to depict the morphology of the vessels so
that the shape information is taken into account. We map the intensity image
and the tensor field to a latent space for feature extraction. Then we fuse the
two latent representations via a weight-balancing trick and feed the result to
a segmentation network. We evaluate on six public datasets of fundus and OCT
angiography images from diverse patient populations. VesselMorph achieves
superior generalization performance compared with competing methods in
different domain shift scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Han Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_X/0/1/0/all/0/1&quot;&gt;Xing Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiacheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oguz_I/0/1/0/all/0/1&quot;&gt;Ipek Oguz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00250">
<title>THUIR2 at NTCIR-16 Session Search (SS) Task. (arXiv:2307.00250v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2307.00250</link>
<description rdf:parseType="Literal">&lt;p&gt;Our team(THUIR2) participated in both FOSS and POSS subtasks of the NTCIR-161
Session Search (SS) Task. This paper describes our approaches and results. In
the FOSS subtask, we submit five runs using learning-to-rank and fine-tuned
pre-trained language models. We fine-tuned the pre-trained language model with
ad-hoc data and session information and assembled them by a learning-to-rank
method. The assembled model achieves the best performance among all
participants in the preliminary evaluation. In the POSS subtask, we used an
assembled model which also achieves the best performance in the preliminary
evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_W/0/1/0/all/0/1&quot;&gt;Weihang Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiangsheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yiqun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Min Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Shaoping Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00252">
<title>An ML approach to resolution of singularities. (arXiv:2307.00252v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00252</link>
<description rdf:parseType="Literal">&lt;p&gt;The solution set of a system of polynomial equations typically contains
ill-behaved, singular points. Resolution is a fundamental process in geometry
in which we replace singular points with smooth points, while keeping the rest
of the solution set unchanged. Resolutions are not unique: the usual way to
describe them involves repeatedly performing a fundamental operation known as
&quot;blowing-up&quot;, and the complexity of the resolution highly depends on certain
choices. The process can be translated into various versions of a 2-player
game, the so-called Hironaka game, and a winning strategy for the first player
provides a solution to the resolution problem. In this paper we introduce a new
approach to the Hironaka game that uses reinforcement learning agents to find
optimal resolutions of singularities. In certain domains, the trained model
outperforms state-of-the-art selection heuristics in total number of polynomial
additions performed, which provides a proof-of-concept that recent developments
in machine learning have the potential to improve performance of algorithms in
symbolic computation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berczi_G/0/1/0/all/0/1&quot;&gt;Gergely B&amp;#xe9;rczi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Honglu Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_M/0/1/0/all/0/1&quot;&gt;Mingcong Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00257">
<title>Efficient Subclass Segmentation in Medical Images. (arXiv:2307.00257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00257</link>
<description rdf:parseType="Literal">&lt;p&gt;As research interests in medical image analysis become increasingly
fine-grained, the cost for extensive annotation also rises. One feasible way to
reduce the cost is to annotate with coarse-grained superclass labels while
using limited fine-grained annotations as a complement. In this way,
fine-grained data learning is assisted by ample coarse annotations. Recent
studies in classification tasks have adopted this method to achieve
satisfactory results. However, there is a lack of research on efficient
learning of fine-grained subclasses in semantic segmentation tasks. In this
paper, we propose a novel approach that leverages the hierarchical structure of
categories to design network architecture. Meanwhile, a task-driven data
generation method is presented to make it easier for the network to recognize
different subclass categories. Specifically, we introduce a Prior Concatenation
module that enhances confidence in subclass segmentation by concatenating
predicted logits from the superclass classifier, a Separate Normalization
module that stretches the intra-class distance within the same superclass to
facilitate subclass segmentation, and a HierarchicalMix model that generates
high-quality pseudo labels for unlabeled samples by fusing only similar
superclass regions from labeled and unlabeled images. Our experiments on the
BraTS2021 and ACDC datasets demonstrate that our approach achieves comparable
accuracy to a model trained with full subclass annotations, with limited
subclass annotations and sufficient superclass annotations. Our approach offers
a promising solution for efficient fine-grained subclass segmentation in
medical images. Our code is publicly available here.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_L/0/1/0/all/0/1&quot;&gt;Linrui Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_W/0/1/0/all/0/1&quot;&gt;Wenhui Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaofan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00259">
<title>InstructEval: Systematic Evaluation of Instruction Selection Methods. (arXiv:2307.00259v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.00259</link>
<description rdf:parseType="Literal">&lt;p&gt;In-context learning (ICL) performs tasks by prompting a large language model
(LLM) using an instruction and a small set of annotated examples called
demonstrations. Recent work has shown that the precise details of the inputs
used in the prompt significantly impacts ICL, which has incentivized
instruction selection algorithms. The effect of instruction-choice however is
severely underexplored, with existing analyses being restricted to shallow
subsets of models and tasks, which limits the generalizability of their
insights. We develop an ICL evaluation suite to conduct a thorough assessment
of these techniques. The suite includes 13 open-sourced LLMs of varying scales
from 4 distinct model families and covers 9 different tasks, representing a
range of task types across 3 categories. In this work, we evaluate the relative
performance of 7 popular instruction selection methods using our benchmark over
five desiderata relevant to ICL. We discover that using curated
manually-written instructions and simple instructions without any task-specific
descriptions often elicits superior ICL performance than that of automatic
instruction-induction methods, pointing to a lack of generalizability among the
latter. We release our evaluation suite for benchmarking instruction selection
approaches, and call for more rigorous and generalizable methods in this space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ajith_A/0/1/0/all/0/1&quot;&gt;Anirudh Ajith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chris Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_M/0/1/0/all/0/1&quot;&gt;Mengzhou Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_A/0/1/0/all/0/1&quot;&gt;Ameet Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narasimhan_K/0/1/0/all/0/1&quot;&gt;Karthik Narasimhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00266">
<title>Hierarchical Pretraining for Biomedical Term Embeddings. (arXiv:2307.00266v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.00266</link>
<description rdf:parseType="Literal">&lt;p&gt;Electronic health records (EHR) contain narrative notes that provide
extensive details on the medical condition and management of patients. Natural
language processing (NLP) of clinical notes can use observed frequencies of
clinical terms as predictive features for downstream applications such as
clinical decision making and patient trajectory prediction. However, due to the
vast number of highly similar and related clinical concepts, a more effective
modeling strategy is to represent clinical terms as semantic embeddings via
representation learning and use the low dimensional embeddings as feature
vectors for predictive modeling. To achieve efficient representation,
fine-tuning pretrained language models with biomedical knowledge graphs may
generate better embeddings for biomedical terms than those from standard
language models alone. These embeddings can effectively discriminate synonymous
pairs of from those that are unrelated. However, they often fail to capture
different degrees of similarity or relatedness for concepts that are
hierarchical in nature. To overcome this limitation, we propose HiPrBERT, a
novel biomedical term representation model trained on additionally complied
data that contains hierarchical structures for various biomedical terms. We
modify an existing contrastive loss function to extract information from these
hierarchies. Our numerical experiments demonstrate that HiPrBERT effectively
learns the pair-wise distance from hierarchical information, resulting in a
substantially more informative embeddings for further biomedical applications
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_B/0/1/0/all/0/1&quot;&gt;Bryan Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_S/0/1/0/all/0/1&quot;&gt;Sihang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yucong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Doudou Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lu Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00280">
<title>SysNoise: Exploring and Benchmarking Training-Deployment System Inconsistency. (arXiv:2307.00280v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00280</link>
<description rdf:parseType="Literal">&lt;p&gt;Extensive studies have shown that deep learning models are vulnerable to
adversarial and natural noises, yet little is known about model robustness on
noises caused by different system implementations. In this paper, we for the
first time introduce SysNoise, a frequently occurred but often overlooked noise
in the deep learning training-deployment cycle. In particular, SysNoise happens
when the source training system switches to a disparate target system in
deployments, where various tiny system mismatch adds up to a non-negligible
difference. We first identify and classify SysNoise into three categories based
on the inference stage; we then build a holistic benchmark to quantitatively
measure the impact of SysNoise on 20+ models, comprehending image
classification, object detection, instance segmentation and natural language
processing tasks. Our extensive experiments revealed that SysNoise could bring
certain impacts on model robustness across different tasks and common
mitigations like data augmentation and adversarial training show limited
effects on it. Together, our findings open a new research topic and we hope
this work will raise research attention to deep learning deployment systems
accounting for model performance. We have open-sourced the benchmark and
framework at https://modeltc.github.io/systemnoise_web.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_R/0/1/0/all/0/1&quot;&gt;Ruihao Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Aishan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jian Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yongqiang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunchen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tianzi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fengwei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianglong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00306">
<title>SyMFM6D: Symmetry-aware Multi-directional Fusion for Multi-View 6D Object Pose Estimation. (arXiv:2307.00306v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.00306</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting objects and estimating their 6D poses is essential for automated
systems to interact safely with the environment. Most 6D pose estimators,
however, rely on a single camera frame and suffer from occlusions and
ambiguities due to object symmetries. We overcome this issue by presenting a
novel symmetry-aware multi-view 6D pose estimator called SyMFM6D. Our approach
efficiently fuses the RGB-D frames from multiple perspectives in a deep
multi-directional fusion network and predicts predefined keypoints for all
objects in the scene simultaneously. Based on the keypoints and an instance
semantic segmentation, we efficiently compute the 6D poses by least-squares
fitting. To address the ambiguity issues for symmetric objects, we propose a
novel training procedure for symmetry-aware keypoint detection including a new
objective function. Our SyMFM6D network significantly outperforms the
state-of-the-art in both single-view and multi-view 6D pose estimation. We
furthermore show the effectiveness of our symmetry-aware training procedure and
demonstrate that our approach is robust towards inaccurate camera calibration
and dynamic camera setups.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duffhauss_F/0/1/0/all/0/1&quot;&gt;Fabian Duffhauss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_S/0/1/0/all/0/1&quot;&gt;Sebastian Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziesche_H/0/1/0/all/0/1&quot;&gt;Hanna Ziesche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vien_N/0/1/0/all/0/1&quot;&gt;Ngo Anh Vien&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00310">
<title>Gradients Look Alike: Sensitivity is Often Overestimated in DP-SGD. (arXiv:2307.00310v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00310</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentially private stochastic gradient descent (DP-SGD) is the canonical
algorithm for private deep learning. While it is known that its privacy
analysis is tight in the worst-case, several empirical results suggest that
when training on common benchmark datasets, the models obtained leak
significantly less privacy for many datapoints. In this paper, we develop a new
analysis for DP-SGD that captures the intuition that points with similar
neighbors in the dataset enjoy better privacy than outliers. Formally, this is
done by modifying the per-step privacy analysis of DP-SGD to introduce a
dependence on the distribution of model updates computed from a training
dataset. We further develop a new composition theorem to effectively use this
new per-step analysis to reason about an entire training run. Put all together,
our evaluation shows that this novel DP-SGD analysis allows us to now formally
show that DP-SGD leaks significantly less privacy for many datapoints. In
particular, we observe that correctly classified points obtain better privacy
guarantees than misclassified points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thudi_A/0/1/0/all/0/1&quot;&gt;Anvith Thudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1&quot;&gt;Hengrui Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meehan_C/0/1/0/all/0/1&quot;&gt;Casey Meehan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shumailov_I/0/1/0/all/0/1&quot;&gt;Ilia Shumailov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papernot_N/0/1/0/all/0/1&quot;&gt;Nicolas Papernot&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00316">
<title>SHARCS: Shared Concept Space for Explainable Multimodal Learning. (arXiv:2307.00316v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.00316</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal learning is an essential paradigm for addressing complex
real-world problems, where individual data modalities are typically
insufficient to accurately solve a given modelling task. While various deep
learning approaches have successfully addressed these challenges, their
reasoning process is often opaque; limiting the capabilities for a principled
explainable cross-modal analysis and any domain-expert intervention. In this
paper, we introduce SHARCS (SHARed Concept Space) -- a novel concept-based
approach for explainable multimodal learning. SHARCS learns and maps
interpretable concepts from different heterogeneous modalities into a single
unified concept-manifold, which leads to an intuitive projection of
semantically similar cross-modal concepts. We demonstrate that such an approach
can lead to inherently explainable task predictions while also improving
downstream predictive performance. Moreover, we show that SHARCS can operate
and significantly outperform other approaches in practically significant
scenarios, such as retrieval of missing modalities and cross-modal
explanations. Our approach is model-agnostic and easily applicable to different
types (and number) of modalities, thus advancing the development of effective,
interpretable, and trustworthy multimodal approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominici_G/0/1/0/all/0/1&quot;&gt;Gabriele Dominici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barbiero_P/0/1/0/all/0/1&quot;&gt;Pietro Barbiero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magister_L/0/1/0/all/0/1&quot;&gt;Lucie Charlotte Magister&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simidjievski_N/0/1/0/all/0/1&quot;&gt;Nikola Simidjievski&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1911.10322">
<title>Meta Adaptation using Importance Weighted Demonstrations. (arXiv:1911.10322v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/1911.10322</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning has gained immense popularity because of its high
sample-efficiency. However, in real-world scenarios, where the trajectory
distribution of most of the tasks dynamically shifts, model fitting on
continuously aggregated data alone would be futile. In some cases, the
distribution shifts, so much, that it is difficult for an agent to infer the
new task. We propose a novel algorithm to generalize on any related task by
leveraging prior knowledge on a set of specific tasks, which involves assigning
importance weights to each past demonstration. We show experiments where the
robot is trained from a diversity of environmental tasks and is also able to
adapt to an unseen environment, using few-shot learning. We also developed a
prototype robot system to test our approach on the task of visual navigation,
and experimental results obtained were able to confirm these suppositions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lekkala_K/0/1/0/all/0/1&quot;&gt;Kiran Lekkala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abu_El_Haija_S/0/1/0/all/0/1&quot;&gt;Sami Abu-El-Haija&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Itti_L/0/1/0/all/0/1&quot;&gt;Laurent Itti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.06258">
<title>A model of interaction semantics. (arXiv:2007.06258v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2007.06258</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: The purpose of this article is to propose, based on a model of an
interaction semantics, a certain understanding of the &apos;&apos;meaning&apos;&apos; of the
exchanged characters within an interaction.
&lt;/p&gt;
&lt;p&gt;Methodology: Based on a model of system interaction, I structure the model of
interaction semantics similar to the semantics of a formal language: first, I
identify adequate variables in my interaction model to assign values to, and
second, I identify the interpretation function to provide meaning. Thereby I
arrive at a model of interaction semantics which, in the sense of the late
Ludwig Wittgenstein, can do without a &apos;mental&apos; mapping from characters to
concepts.
&lt;/p&gt;
&lt;p&gt;Findings: The key findings are a better understanding of the tight relation
between the informatical approach to model interactions and game theory; of the
central &apos;chicken and egg&apos; problem, any natural language has to solve, namely
that to interact sensibly, we have to understand each other and to acquire a
common understanding, we have to interact with each other, which I call the
&apos;simultaneous interaction and understanding (SIAU)&apos; problem; why ontologies are
less &apos;semantic&apos; then their proponents suggest; and how &apos;semantic&apos;
interoperability is to be achieved.
&lt;/p&gt;
&lt;p&gt;Value: The main value of the proposed model of interaction semantics is that
it could be applied in many different disciplines and therefore could serve as
a basis for scientists of natural sciences and humanities as well as engineers
to understand each other more easily talking about semantics, especially with
the advent of cyber-physical systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reich_J/0/1/0/all/0/1&quot;&gt;Johannes Reich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2007.07703">
<title>Failures of Contingent Thinking. (arXiv:2007.07703v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2007.07703</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we provide a theoretical framework to analyze an agent who
misinterprets or misperceives the true decision problem she faces. We show that
a wide range of behavior observed in experimental settings manifest as failures
to perceive implications, in other words, to properly account for the logical
relationships between various payoff relevant contingencies. We present a
behavioral definition of perceived implication, thereby providing an
elicitation technique, and show that an agent&apos;s account of implication
identifies a subjective state-space that underlies her behavior. By analyzing
this state-space, we characterize distinct benchmarks of logical sophistication
that drive empirical phenomena. We disentangle static and dynamic rationality.
Thus, our framework delivers both a methodology for assessing an agent&apos;s level
of contingent thinking and a strategy for identifying her beliefs in the
absence full rationality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piermont_E/0/1/0/all/0/1&quot;&gt;Evan Piermont&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuazo_Garin_P/0/1/0/all/0/1&quot;&gt;Peio Zuazo-Garin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.07888">
<title>Transfer Learning in Deep Reinforcement Learning: A Survey. (arXiv:2009.07888v6 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2009.07888</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning is a learning paradigm for solving sequential
decision-making problems. Recent years have witnessed remarkable progress in
reinforcement learning upon the fast development of deep neural networks. Along
with the promising prospects of reinforcement learning in numerous domains such
as robotics and game-playing, transfer learning has arisen to tackle various
challenges faced by reinforcement learning, by transferring knowledge from
external expertise to facilitate the efficiency and effectiveness of the
learning process. In this survey, we systematically investigate the recent
progress of transfer learning approaches in the context of deep reinforcement
learning. Specifically, we provide a framework for categorizing the
state-of-the-art transfer learning approaches, under which we analyze their
goals, methodologies, compatible reinforcement learning backbones, and
practical applications. We also draw connections between transfer learning and
other relevant topics from the reinforcement learning perspective and explore
their potential challenges that await future research progress.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhuangdi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kaixiang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Anil K. Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayu Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.12021">
<title>Bridging Offline Reinforcement Learning and Imitation Learning: A Tale of Pessimism. (arXiv:2103.12021v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2103.12021</link>
<description rdf:parseType="Literal">&lt;p&gt;Offline (or batch) reinforcement learning (RL) algorithms seek to learn an
optimal policy from a fixed dataset without active data collection. Based on
the composition of the offline dataset, two main categories of methods are
used: imitation learning which is suitable for expert datasets and vanilla
offline RL which often requires uniform coverage datasets. From a practical
standpoint, datasets often deviate from these two extremes and the exact data
composition is usually unknown a priori. To bridge this gap, we present a new
offline RL framework that smoothly interpolates between the two extremes of
data composition, hence unifying imitation learning and vanilla offline RL. The
new framework is centered around a weak version of the concentrability
coefficient that measures the deviation from the behavior policy to the expert
policy alone.
&lt;/p&gt;
&lt;p&gt;Under this new framework, we further investigate the question on algorithm
design: can one develop an algorithm that achieves a minimax optimal rate and
also adapts to unknown data composition? To address this question, we consider
a lower confidence bound (LCB) algorithm developed based on pessimism in the
face of uncertainty in offline RL. We study finite-sample properties of LCB as
well as information-theoretic limits in multi-armed bandits, contextual
bandits, and Markov decision processes (MDPs). Our analysis reveals surprising
facts about optimality rates. In particular, in all three settings, LCB
achieves a faster rate of $1/N$ for nearly-expert datasets compared to the
usual rate of $1/\sqrt{N}$ in offline RL, where $N$ is the number of samples in
the batch dataset. In the case of contextual bandits with at least two
contexts, we prove that LCB is adaptively optimal for the entire data
composition range, achieving a smooth transition from imitation learning to
offline RL. We further show that LCB is almost adaptively optimal in MDPs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rashidinejad_P/0/1/0/all/0/1&quot;&gt;Paria Rashidinejad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_B/0/1/0/all/0/1&quot;&gt;Banghua Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Cong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiao_J/0/1/0/all/0/1&quot;&gt;Jiantao Jiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russell_S/0/1/0/all/0/1&quot;&gt;Stuart Russell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2105.14452">
<title>A unified logical framework for explanations in classifier systems. (arXiv:2105.14452v6 [cs.LO] UPDATED)</title>
<link>http://arxiv.org/abs/2105.14452</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed a renewed interest in Boolean function in
explaining binary classifiers in the field of explainable AI (XAI). The
standard approach of Boolean function is propositional logic. We present a
modal language of a ceteris paribus nature which supports reasoning about
binary input classifiers and their properties. We study a family of classifier
models, axiomatize it as two proof systems regarding the cardinality of the
language and show completeness of our axiomatics. Moreover, we prove that
satisfiability checking problem for our modal language is NEXPTIME-complete in
the infinite-variable case, while it becomes polynomial in the finite-variable
case. We furthermore identify an interesting NP fragment of our language in the
infinite-variable case. We leverage the language to formalize counterfactual
conditional as well as a variety of notions of explanation including abductive,
contrastive and counterfactual explanations, and biases. Finally, we present
two extensions of our language: a dynamic extension by the notion of assignment
enabling classifier change and an epistemic extension in which the classifier&apos;s
uncertainty about the actual input can be represented.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinghan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorini_E/0/1/0/all/0/1&quot;&gt;Emiliano Lorini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.03892">
<title>TND-NAS: Towards Non-differentiable Objectives in Progressive Differentiable NAS Framework. (arXiv:2111.03892v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2111.03892</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentiable architecture search has gradually become the mainstream
research topic in the field of Neural Architecture Search (NAS) for its high
efficiency compared with the early NAS methods. Recent differentiable NAS also
aims at further improving the search performance and reducing the GPU-memory
consumption. However, these methods are no longer naturally capable of tackling
the non-differentiable objectives, e.g., energy, resource-constrained
efficiency, and other metrics, let alone the multi-objective search demands.
Researches in the multi-objective NAS field target this but requires vast
computational resources cause of the sole optimization of each candidate
architecture. In light of this discrepancy, we propose the TND-NAS, which is
with the merits of the high efficiency in differentiable NAS framework and the
compatibility among non-differentiable metrics in Multi-objective NAS. Under
the differentiable NAS framework, with the continuous relaxation of the search
space, TND-NAS has the architecture parameters been optimized in discrete
space, while resorting to the progressive search space shrinking by
architecture parameters. Our representative experiment takes two objectives
(Parameters, Accuracy) as an example, we achieve a series of high-performance
compact architectures on CIFAR10 (1.09M/3.3%, 2.4M/2.95%, 9.57M/2.54%) and
CIFAR100 (2.46M/18.3%, 5.46/16.73%, 12.88/15.20%) datasets. Favorably, compared
with other multi-objective NAS methods, TND-NAS is less time-consuming (1.3
GPU-days on NVIDIA 1080Ti, 1/6 of that in NSGA-Net), and can be conveniently
adapted to real-world NAS scenarios (resource-constrained,
platform-specialized).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_B/0/1/0/all/0/1&quot;&gt;Bo Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_S/0/1/0/all/0/1&quot;&gt;Shiping Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.07905">
<title>CPTAM: Constituency Parse Tree Aggregation Method. (arXiv:2201.07905v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2201.07905</link>
<description rdf:parseType="Literal">&lt;p&gt;Diverse Natural Language Processing tasks employ constituency parsing to
understand the syntactic structure of a sentence according to a phrase
structure grammar. Many state-of-the-art constituency parsers are proposed, but
they may provide different results for the same sentences, especially for
corpora outside their training domains. This paper adopts the truth discovery
idea to aggregate constituency parse trees from different parsers by estimating
their reliability in the absence of ground truth. Our goal is to consistently
obtain high-quality aggregated constituency parse trees. We formulate the
constituency parse tree aggregation problem in two steps, structure aggregation
and constituent label aggregation. Specifically, we propose the first truth
discovery solution for tree structures by minimizing the weighted sum of
Robinson-Foulds (RF) distances, a classic symmetric distance metric between two
trees. Extensive experiments are conducted on benchmark datasets in different
languages and domains. The experimental results show that our method, CPTAM,
outperforms the state-of-the-art aggregation baselines. We also demonstrate
that the weights estimated by CPTAM can adequately evaluate constituency
parsers in the absence of ground truth.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Adithya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabetpour_N/0/1/0/all/0/1&quot;&gt;Nasim Sabetpour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markin_A/0/1/0/all/0/1&quot;&gt;Alexey Markin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eulenstein_O/0/1/0/all/0/1&quot;&gt;Oliver Eulenstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.06527">
<title>Learning Hidden Markov Models When the Locations of Missing Observations are Unknown. (arXiv:2203.06527v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2203.06527</link>
<description rdf:parseType="Literal">&lt;p&gt;The Hidden Markov Model (HMM) is one of the most widely used statistical
models for sequential data analysis. One of the key reasons for this
versatility is the ability of HMM to deal with missing data. However, standard
HMM learning algorithms rely crucially on the assumption that the positions of
the missing observations \emph{within the observation sequence} are known. In
the natural sciences, where this assumption is often violated, special variants
of HMM, commonly known as Silent-state HMMs (SHMMs), are used. Despite their
widespread use, these algorithms strongly rely on specific structural
assumptions of the underlying chain, such as acyclicity, thus limiting the
applicability of these methods. Moreover, even in the acyclic case, it has been
shown that these methods can lead to poor reconstruction. In this paper we
consider the general problem of learning an HMM from data with unknown missing
observation locations. We provide reconstruction algorithms that do not require
any assumptions about the structure of the underlying chain, and can also be
used with limited prior knowledge, unlike SHMM. We evaluate and compare the
algorithms in a variety of scenarios, measuring their reconstruction precision,
and robustness under model miss-specification. Notably, we show that under
proper specifications one can reconstruct the process dynamics as well as if
the missing observations positions were known.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Perets_B/0/1/0/all/0/1&quot;&gt;Binyamin Perets&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kozdoba_M/0/1/0/all/0/1&quot;&gt;Mark Kozdoba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Mannor_S/0/1/0/all/0/1&quot;&gt;Shie Mannor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.00667">
<title>How Biased are Your Features?: Computing Fairness Influence Functions with Global Sensitivity Analysis. (arXiv:2206.00667v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.00667</link>
<description rdf:parseType="Literal">&lt;p&gt;Fairness in machine learning has attained significant focus due to the
widespread application in high-stake decision-making tasks. Unregulated machine
learning classifiers can exhibit bias towards certain demographic groups in
data, thus the quantification and mitigation of classifier bias is a central
concern in fairness in machine learning. In this paper, we aim to quantify the
influence of different features in a dataset on the bias of a classifier. To do
this, we introduce the Fairness Influence Function (FIF). This function breaks
down bias into its components among individual features and the intersection of
multiple features. The key idea is to represent existing group fairness metrics
as the difference of the scaled conditional variances in the classifier&apos;s
prediction and apply a decomposition of variance according to global
sensitivity analysis. To estimate FIFs, we instantiate an algorithm
FairXplainer that applies variance decomposition of classifier&apos;s prediction
following local regression. Experiments demonstrate that FairXplainer captures
FIFs of individual feature and intersectional features, provides a better
approximation of bias based on FIFs, demonstrates higher correlation of FIFs
with fairness interventions, and detects changes in bias due to fairness
affirmative/punitive actions in the classifier.
&lt;/p&gt;
&lt;p&gt;The code is available at https://github.com/ReAILe/bias-explainer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_B/0/1/0/all/0/1&quot;&gt;Bishwamittra Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basu_D/0/1/0/all/0/1&quot;&gt;Debabrota Basu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meel_K/0/1/0/all/0/1&quot;&gt;Kuldeep S. Meel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10540">
<title>Rethinking Symbolic Regression Datasets and Benchmarks for Scientific Discovery. (arXiv:2206.10540v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10540</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper revisits datasets and evaluation criteria for Symbolic Regression
(SR), specifically focused on its potential for scientific discovery. Focused
on a set of formulas used in the existing datasets based on Feynman Lectures on
Physics, we recreate 120 datasets to discuss the performance of symbolic
regression for scientific discovery (SRSD). For each of the 120 SRSD datasets,
we carefully review the properties of the formula and its variables to design
reasonably realistic sampling ranges of values so that our new SRSD datasets
can be used for evaluating the potential of SRSD such as whether or not an SR
method can (re)discover physical laws from such datasets. We also create
another 120 datasets that contain dummy variables to examine whether SR methods
can choose necessary variables only. Besides, we propose to use normalized edit
distances (NED) between a predicted equation and the true equation trees for
addressing a critical issue that existing SR metrics are either binary or
errors between the target values and an SR model&apos;s predicted values for a given
input. We conduct experiments on our new SRSD datasets using six SR methods.
The experimental results show that we provide a more realistic performance
evaluation, and our user study shows that the NED correlates with human judges
significantly more than an existing SR metric.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matsubara_Y/0/1/0/all/0/1&quot;&gt;Yoshitomo Matsubara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiba_N/0/1/0/all/0/1&quot;&gt;Naoya Chiba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Igarashi_R/0/1/0/all/0/1&quot;&gt;Ryo Igarashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ushiku_Y/0/1/0/all/0/1&quot;&gt;Yoshitaka Ushiku&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.10553">
<title>MABe22: A Multi-Species Multi-Task Benchmark for Learned Representations of Behavior. (arXiv:2207.10553v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2207.10553</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce MABe22, a large-scale, multi-agent video and trajectory
benchmark to assess the quality of learned behavior representations. This
dataset is collected from a variety of biology experiments, and includes
triplets of interacting mice (4.7 million frames video+pose tracking data, 10
million frames pose only), symbiotic beetle-ant interactions (10 million frames
video data), and groups of interacting flies (4.4 million frames of pose
tracking data). Accompanying these data, we introduce a panel of real-life
downstream analysis tasks to assess the quality of learned representations by
evaluating how well they preserve information about the experimental conditions
(e.g. strain, time of day, optogenetic stimulation) and animal behavior. We
test multiple state-of-the-art self-supervised video and trajectory
representation learning methods to demonstrate the use of our benchmark,
revealing that methods developed using human action datasets do not fully
translate to animal datasets. We hope that our benchmark and dataset encourage
a broader exploration of behavior representation learning methods across
species and settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jennifer J. Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marks_M/0/1/0/all/0/1&quot;&gt;Markus Marks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulmer_A/0/1/0/all/0/1&quot;&gt;Andrew Ulmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_D/0/1/0/all/0/1&quot;&gt;Dipam Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geuther_B/0/1/0/all/0/1&quot;&gt;Brian Geuther&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_E/0/1/0/all/0/1&quot;&gt;Edward Hayes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_H/0/1/0/all/0/1&quot;&gt;Heng Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vivek Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oleszko_S/0/1/0/all/0/1&quot;&gt;Sebastian Oleszko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Partridge_Z/0/1/0/all/0/1&quot;&gt;Zachary Partridge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peelman_M/0/1/0/all/0/1&quot;&gt;Milan Peelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robie_A/0/1/0/all/0/1&quot;&gt;Alice Robie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schretter_C/0/1/0/all/0/1&quot;&gt;Catherine E. Schretter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheppard_K/0/1/0/all/0/1&quot;&gt;Keith Sheppard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Chao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uttarwar_P/0/1/0/all/0/1&quot;&gt;Param Uttarwar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_J/0/1/0/all/0/1&quot;&gt;Julian M. Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_E/0/1/0/all/0/1&quot;&gt;Eric Werner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parker_J/0/1/0/all/0/1&quot;&gt;Joseph Parker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perona_P/0/1/0/all/0/1&quot;&gt;Pietro Perona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yisong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branson_K/0/1/0/all/0/1&quot;&gt;Kristin Branson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kennedy_A/0/1/0/all/0/1&quot;&gt;Ann Kennedy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.00884">
<title>Infant movement classification through pressure distribution analysis. (arXiv:2208.00884v3 [eess.SP] UPDATED)</title>
<link>http://arxiv.org/abs/2208.00884</link>
<description rdf:parseType="Literal">&lt;p&gt;Aiming at objective early detection of neuromotor disorders such as cerebral
palsy, we proposed an innovative non-intrusive approach using a pressure
sensing device to classify infant general movements (GMs). Here, we tested the
feasibility of using pressure data to differentiate typical GM patterns of the
&apos;&apos;fidgety period&apos;&apos; (i.e., fidgety movements) vs. the &apos;&apos;pre-fidgety period&apos;&apos;
(i.e., writhing movements). Participants (N = 45) were sampled from a
typically-developing infant cohort. Multi-modal sensor data, including pressure
data from a 32x32-grid pressure sensing mat with 1024 sensors, were
prospectively recorded for each infant in seven succeeding laboratory sessions
in biweekly intervals from 4-16 weeks of post-term age. For proof-of-concept,
1776 pressure data snippets, each 5s long, from the two targeted age periods
were taken for movement classification. Each snippet was pre-annotated based on
corresponding synchronised video data by human assessors as either fidgety
present (FM+) or absent (FM-). Multiple neural network architectures were
tested to distinguish the FM+ vs. FM- classes, including support vector
machines (SVM), feed-forward networks (FFNs), convolutional neural networks
(CNNs), and long short-term memory (LSTM) networks. The CNN achieved the
highest average classification accuracy (81.4%) for classes FM+ vs. FM-.
Comparing the pros and cons of other methods aiming at automated GMA to the
pressure sensing approach, we concluded that the pressure sensing approach has
great potential for efficient large-scale motion data acquisition and sharing.
This will in return enable improvement of the approach that may prove scalable
for daily clinical application for evaluating infant neuromotor functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kulvicius_T/0/1/0/all/0/1&quot;&gt;Tomas Kulvicius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dajie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nielsen_Saines_K/0/1/0/all/0/1&quot;&gt;Karin Nielsen-Saines&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bolte_S/0/1/0/all/0/1&quot;&gt;Sven B&amp;#xf6;lte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kraft_M/0/1/0/all/0/1&quot;&gt;Marc Kraft&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Einspieler_C/0/1/0/all/0/1&quot;&gt;Christa Einspieler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Poustka_L/0/1/0/all/0/1&quot;&gt;Luise Poustka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Worgotter_F/0/1/0/all/0/1&quot;&gt;Florentin W&amp;#xf6;rg&amp;#xf6;tter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marschik_P/0/1/0/all/0/1&quot;&gt;Peter B Marschik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.04422">
<title>Truth Set Algebra: A New Way to Prove Undefinability. (arXiv:2208.04422v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.04422</link>
<description rdf:parseType="Literal">&lt;p&gt;The article proposes a new technique for proving the undefinability of
logical connectives through each other and illustrates the technique with
several examples. Some of the obtained results are new proofs of the existing
theorems, others are original to this work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knight_S/0/1/0/all/0/1&quot;&gt;Sophia Knight&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naumov_P/0/1/0/all/0/1&quot;&gt;Pavel Naumov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Q/0/1/0/all/0/1&quot;&gt;Qi Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suntharraj_V/0/1/0/all/0/1&quot;&gt;Vigasan Suntharraj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06648">
<title>Imputation Strategies Under Clinical Presence: Impact on Algorithmic Fairness. (arXiv:2208.06648v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06648</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning risks reinforcing biases present in data, and, as we argue
in this work, in what is absent from data. In healthcare, biases have marked
medical history, leading to unequal care affecting marginalised groups.
Patterns in missing data often reflect these group discrepancies, but the
algorithmic fairness implications of group-specific missingness are not well
understood. Despite its potential impact, imputation is often an overlooked
preprocessing step, with attention placed on the reduction of reconstruction
error and overall performance, ignoring how imputation can affect groups
differently. Our work studies how imputation choices affect reconstruction
errors across groups and algorithmic fairness properties of downstream
predictions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeanselme_V/0/1/0/all/0/1&quot;&gt;Vincent Jeanselme&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_Arteaga_M/0/1/0/all/0/1&quot;&gt;Maria De-Arteaga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barrett_J/0/1/0/all/0/1&quot;&gt;Jessica Barrett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tom_B/0/1/0/all/0/1&quot;&gt;Brian Tom&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.02646">
<title>A Survey on Generative Diffusion Model. (arXiv:2209.02646v9 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.02646</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models are a prominent approach for data generation, and have
been used to produce high quality samples in various domains. Diffusion models,
an emerging class of deep generative models, have attracted considerable
attention owing to their exceptional generative quality. Despite this, they
have certain limitations, including a time-consuming iterative generation
process and confinement to high-dimensional Euclidean space. This survey
presents a plethora of advanced techniques aimed at enhancing diffusion models,
including sampling acceleration and the design of new diffusion processes. In
addition, we delve into strategies for implementing diffusion models in
manifold and discrete spaces, maximum likelihood training for diffusion models,
and methods for creating bridges between two arbitrary distributions. The
innovations we discuss represent the efforts for improving the functionality
and efficiency of diffusion models in recent years. To examine the efficacy of
existing models, a benchmark of FID score, IS, and NLL is presented in a
specific NFE. Furthermore, diffusion models are found to be useful in various
domains such as computer vision, audio, sequence modeling, and AI for science.
The paper concludes with a summary of this field, along with existing
limitations and future directions. Summation of existing well-classified
methods is in our Github:
https://github.com/chq1155/A-Survey-on-Generative-Diffusion-Model
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Hanqun Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Z/0/1/0/all/0/1&quot;&gt;Zhangyang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yilun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_G/0/1/0/all/0/1&quot;&gt;Guangyong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng-Ann Heng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13578">
<title>Learning When to Advise Human Decision Makers. (arXiv:2209.13578v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13578</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial intelligence (AI) systems are increasingly used for providing
advice to facilitate human decision making in a wide range of domains, such as
healthcare, criminal justice, and finance. Motivated by limitations of the
current practice where algorithmic advice is provided to human users as a
constant element in the decision-making pipeline, in this paper we raise the
question of when should algorithms provide advice? We propose a novel design of
AI systems in which the algorithm interacts with the human user in a two-sided
manner and aims to provide advice only when it is likely to be beneficial for
the user in making their decision. The results of a large-scale experiment show
that our advising approach manages to provide advice at times of need and to
significantly improve human decision making compared to fixed, non-interactive,
advising approaches. This approach has additional advantages in facilitating
human learning, preserving complementary strengths of human decision makers,
and leading to more positive responsiveness to the advice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noti_G/0/1/0/all/0/1&quot;&gt;Gali Noti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiling Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.13678">
<title>FAIR-FATE: Fair Federated Learning with Momentum. (arXiv:2209.13678v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2209.13678</link>
<description rdf:parseType="Literal">&lt;p&gt;While fairness-aware machine learning algorithms have been receiving
increasing attention, the focus has been on centralized machine learning,
leaving decentralized methods underexplored. Federated Learning is a
decentralized form of machine learning where clients train local models with a
server aggregating them to obtain a shared global model. Data heterogeneity
amongst clients is a common characteristic of Federated Learning, which may
induce or exacerbate discrimination of unprivileged groups defined by sensitive
attributes such as race or gender. In this work we propose FAIR-FATE: a novel
FAIR FederATEd Learning algorithm that aims to achieve group fairness while
maintaining high utility via a fairness-aware aggregation method that computes
the global model by taking into account the fairness of the clients. To achieve
that, the global model update is computed by estimating a fair model update
using a Momentum term that helps to overcome the oscillations of non-fair
gradients. To the best of our knowledge, this is the first approach in machine
learning that aims to achieve fairness using a fair Momentum estimate.
Experimental results on real-world datasets demonstrate that FAIR-FATE
outperforms state-of-the-art fair Federated Learning algorithms under different
levels of data heterogeneity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salazar_T/0/1/0/all/0/1&quot;&gt;Teresa Salazar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandes_M/0/1/0/all/0/1&quot;&gt;Miguel Fernandes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_H/0/1/0/all/0/1&quot;&gt;Helder Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abreu_P/0/1/0/all/0/1&quot;&gt;Pedro Henriques Abreu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.05723">
<title>Embeddings as Epistemic States: Limitations on the Use of Pooling Operators for Accumulating Knowledge. (arXiv:2210.05723v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.05723</link>
<description rdf:parseType="Literal">&lt;p&gt;Various neural network architectures rely on pooling operators to aggregate
information coming from different sources. It is often implicitly assumed in
such contexts that vectors encode epistemic states, i.e. that vectors capture
the evidence that has been obtained about some properties of interest, and that
pooling these vectors yields a vector that combines this evidence. We study,
for a number of standard pooling operators, under what conditions they are
compatible with this idea, which we call the epistemic pooling principle. While
we find that all the considered pooling operators can satisfy the epistemic
pooling principle, this only holds when embeddings are sufficiently
high-dimensional and, for most pooling operators, when the embeddings satisfy
particular constraints (e.g. having non-negative coordinates). We furthermore
show that these constraints have important implications on how the embeddings
can be used in practice. In particular, we find that when the epistemic pooling
principle is satisfied, in most cases it is impossible to verify the
satisfaction of propositional formulas using linear scoring functions, with two
exceptions: (i) max-pooling with embeddings that are upper-bounded and (ii)
Hadamard pooling with non-negative embeddings. This finding helps to clarify,
among others, why Graph Neural Networks sometimes under-perform in reasoning
tasks. Finally, we also study an extension of the epistemic pooling principle
to weighted epistemic states, which are important in the context of
non-monotonic reasoning, where max-pooling emerges as the most suitable
operator.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schockaert_S/0/1/0/all/0/1&quot;&gt;Steven Schockaert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.10737">
<title>RSC: Accelerating Graph Neural Networks Training via Randomized Sparse Computations. (arXiv:2210.10737v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.10737</link>
<description rdf:parseType="Literal">&lt;p&gt;The training of graph neural networks (GNNs) is extremely time consuming
because sparse graph-based operations are hard to be accelerated by hardware.
Prior art explores trading off the computational precision to reduce the time
complexity via sampling-based approximation. Based on the idea, previous works
successfully accelerate the dense matrix based operations (e.g., convolution
and linear) with negligible accuracy drop. However, unlike dense matrices,
sparse matrices are stored in the irregular data format such that each
row/column may have different number of non-zero entries. Thus, compared to the
dense counterpart, approximating sparse operations has two unique challenges
(1) we cannot directly control the efficiency of approximated sparse operation
since the computation is only executed on non-zero entries; (2) sub-sampling
sparse matrices is much more inefficient due to the irregular data format. To
address the issues, our key idea is to control the accuracy-efficiency trade
off by optimizing computation resource allocation layer-wisely and
epoch-wisely. Specifically, for the first challenge, we customize the
computation resource to different sparse operations, while limit the total used
resource below a certain budget. For the second challenge, we cache previous
sampled sparse matrices to reduce the epoch-wise sampling overhead. Finally, we
propose a switching mechanisms to improve the generalization of GNNs trained
with approximated operations. To this end, we propose Randomized Sparse
Computation, which for the first time demonstrate the potential of training
GNNs with approximated operations. In practice, rsc can achieve up to
$11.6\times$ speedup for a single sparse operation and a $1.6\times$ end-to-end
wall-clock time speedup with negligible accuracy drop.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zirui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shengyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaixiong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_D/0/1/0/all/0/1&quot;&gt;Daochen Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xiao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xia Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.13507">
<title>Causal Explanation for Reinforcement Learning: Quantifying State and Temporal Importance. (arXiv:2210.13507v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2210.13507</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainability plays an increasingly important role in machine learning.
Furthermore, humans view the world through a causal lens and thus prefer causal
explanations over associational ones. Therefore, in this paper, we develop a
causal explanation mechanism that quantifies the causal importance of states on
actions and such importance over time. We also demonstrate the advantages of
our mechanism over state-of-the-art associational methods in terms of RL policy
explanation through a series of simulation studies, including crop irrigation,
Blackjack, collision avoidance, and lunar lander.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fanyu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1&quot;&gt;Zhaodan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xin Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05039">
<title>Active Acquisition for Multimodal Temporal Data: A Challenging Decision-Making Task. (arXiv:2211.05039v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05039</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a challenging decision-making task that we call active
acquisition for multimodal temporal data (A2MT). In many real-world scenarios,
input features are not readily available at test time and must instead be
acquired at significant cost. With A2MT, we aim to learn agents that actively
select which modalities of an input to acquire, trading off acquisition cost
and predictive performance. A2MT extends a previous task called active feature
acquisition to temporal decision making about high-dimensional inputs. We
propose a method based on the Perceiver IO architecture to address A2MT in
practice. Our agents are able to solve a novel synthetic scenario requiring
practically relevant cross-modal reasoning skills. On two large-scale,
real-world datasets, Kinetics-700 and AudioSet, our agents successfully learn
cost-reactive acquisition behavior. However, an ablation reveals they are
unable to learn adaptive acquisition strategies, emphasizing the difficulty of
the task even for state-of-the-art models. Applications of A2MT may be
impactful in domains like medicine, robotics, or finance, where modalities
differ in acquisition cost and informativeness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kossen_J/0/1/0/all/0/1&quot;&gt;Jannik Kossen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cangea_C/0/1/0/all/0/1&quot;&gt;C&amp;#x103;t&amp;#x103;lina Cangea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vertes_E/0/1/0/all/0/1&quot;&gt;Eszter V&amp;#xe9;rtes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaegle_A/0/1/0/all/0/1&quot;&gt;Andrew Jaegle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patraucean_V/0/1/0/all/0/1&quot;&gt;Viorica Patraucean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ktena_I/0/1/0/all/0/1&quot;&gt;Ira Ktena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomasev_N/0/1/0/all/0/1&quot;&gt;Nenad Tomasev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Belgrave_D/0/1/0/all/0/1&quot;&gt;Danielle Belgrave&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05862">
<title>MixUp-MIL: Novel Data Augmentation for Multiple Instance Learning and a Study on Thyroid Cancer Diagnosis. (arXiv:2211.05862v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05862</link>
<description rdf:parseType="Literal">&lt;p&gt;Multiple instance learning exhibits a powerful approach for whole slide
image-based diagnosis in the absence of pixel- or patch-level annotations. In
spite of the huge size of hole slide images, the number of individual slides is
often rather small, leading to a small number of labeled samples. To improve
training, we propose and investigate different data augmentation strategies for
multiple instance learning based on the idea of linear interpolations of
feature vectors (known as MixUp). Based on state-of-the-art multiple instance
learning architectures and two thyroid cancer data sets, an exhaustive study is
conducted considering a range of common data augmentation strategies. Whereas a
strategy based on to the original MixUp approach showed decreases in accuracy,
the use of a novel intra-slide interpolation method led to consistent increases
in accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gadermayr_M/0/1/0/all/0/1&quot;&gt;Michael Gadermayr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koller_L/0/1/0/all/0/1&quot;&gt;Lukas Koller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tschuchnig_M/0/1/0/all/0/1&quot;&gt;Maximilian Tschuchnig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stangassinger_L/0/1/0/all/0/1&quot;&gt;Lea Maria Stangassinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kreutzer_C/0/1/0/all/0/1&quot;&gt;Christina Kreutzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couillard_Despres_S/0/1/0/all/0/1&quot;&gt;Sebastien Couillard-Despres&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostingh_G/0/1/0/all/0/1&quot;&gt;Gertie Janneke Oostingh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hittmair_A/0/1/0/all/0/1&quot;&gt;Anton Hittmair&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.11736">
<title>Robotic Skill Acquisition via Instruction Augmentation with Vision-Language Models. (arXiv:2211.11736v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2211.11736</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, much progress has been made in learning robotic manipulation
policies that follow natural language instructions. Such methods typically
learn from corpora of robot-language data that was either collected with
specific tasks in mind or expensively re-labelled by humans with rich language
descriptions in hindsight. Recently, large-scale pretrained vision-language
models (VLMs) like CLIP or ViLD have been applied to robotics for learning
representations and scene descriptors. Can these pretrained models serve as
automatic labelers for robot data, effectively importing Internet-scale
knowledge into existing datasets to make them useful even for tasks that are
not reflected in their ground truth annotations? To accomplish this, we
introduce Data-driven Instruction Augmentation for Language-conditioned control
(DIAL): we utilize semi-supervised language labels leveraging the semantic
understanding of CLIP to propagate knowledge onto large datasets of unlabelled
demonstration data and then train language-conditioned policies on the
augmented datasets. This method enables cheaper acquisition of useful language
descriptions compared to expensive human labels, allowing for more efficient
label coverage of large-scale datasets. We apply DIAL to a challenging
real-world robotic manipulation domain where 96.5% of the 80,000 demonstrations
do not contain crowd-sourced language annotations. DIAL enables imitation
learning policies to acquire new capabilities and generalize to 60 novel
instructions unseen in the original dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Ted Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_H/0/1/0/all/0/1&quot;&gt;Harris Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sermanet_P/0/1/0/all/0/1&quot;&gt;Pierre Sermanet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wahid_A/0/1/0/all/0/1&quot;&gt;Ayzaan Wahid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brohan_A/0/1/0/all/0/1&quot;&gt;Anthony Brohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tompson_J/0/1/0/all/0/1&quot;&gt;Jonathan Tompson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.10511">
<title>When Not to Trust Language Models: Investigating Effectiveness of Parametric and Non-Parametric Memories. (arXiv:2212.10511v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.10511</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their impressive performance on diverse tasks, large language models
(LMs) still struggle with tasks requiring rich world knowledge, implying the
limitations of relying solely on their parameters to encode a wealth of world
knowledge. This paper aims to understand LMs&apos; strengths and limitations in
memorizing factual knowledge, by conducting large-scale knowledge probing
experiments of 10 models and 4 augmentation methods on PopQA, our new
open-domain QA dataset with 14k questions. We find that LMs struggle with less
popular factual knowledge, and that scaling fails to appreciably improve
memorization of factual knowledge in the long tail. We then show that
retrieval-augmented LMs largely outperform orders of magnitude larger LMs,
while unassisted LMs remain competitive in questions about high-popularity
entities. Based on those findings, we devise a simple, yet effective, method
for powerful and efficient retrieval-augmented LMs, which retrieves
non-parametric memories only when necessary. Experimental results show that
this significantly improves models&apos; performance while reducing the inference
costs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallen_A/0/1/0/all/0/1&quot;&gt;Alex Mallen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asai_A/0/1/0/all/0/1&quot;&gt;Akari Asai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_V/0/1/0/all/0/1&quot;&gt;Victor Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_R/0/1/0/all/0/1&quot;&gt;Rajarshi Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khashabi_D/0/1/0/all/0/1&quot;&gt;Daniel Khashabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajishirzi_H/0/1/0/all/0/1&quot;&gt;Hannaneh Hajishirzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.13679">
<title>CC-FedAvg: Computationally Customized Federated Averaging. (arXiv:2212.13679v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2212.13679</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL) is an emerging paradigm to train model with
distributed data from numerous Internet of Things (IoT) devices. It inherently
assumes a uniform capacity among participants. However, due to different
conditions such as differing energy budgets or executing parallel unrelated
tasks, participants have diverse computational resources in practice.
Participants with insufficient computation budgets must plan for the use of
restricted computational resources appropriately, otherwise they would be
unable to complete the entire training procedure, resulting in model
performance decline. To address this issue, we propose a strategy for
estimating local models without computationally intensive iterations. Based on
it, we propose Computationally Customized Federated Averaging (CC-FedAvg),
which allows participants to determine whether to perform traditional local
training or model estimation in each round based on their current computational
budgets. Both theoretical analysis and exhaustive experiments indicate that
CC-FedAvg has the same convergence rate and comparable performance as FedAvg
without resource constraints. Furthermore, CC-FedAvg can be viewed as a
computation-efficient version of FedAvg that retains model performance while
considerably lowering computation overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tingting Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyao Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.03147">
<title>Finding Lookalike Customers for E-Commerce Marketing. (arXiv:2301.03147v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.03147</link>
<description rdf:parseType="Literal">&lt;p&gt;Customer-centric marketing campaigns generate a large portion of e-commerce
website traffic for Walmart. As the scale of customer data grows larger,
expanding the marketing audience to reach more customers is becoming more
critical for e-commerce companies to drive business growth and bring more value
to customers. In this paper, we present a scalable and efficient system to
expand targeted audience of marketing campaigns, which can handle hundreds of
millions of customers. We use a deep learning based embedding model to
represent customers and an approximate nearest neighbor search method to
quickly find lookalike customers of interest. The model can deal with various
business interests by constructing interpretable and meaningful customer
similarity metrics. We conduct extensive experiments to demonstrate the great
performance of our system and customer embedding model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changzheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_W/0/1/0/all/0/1&quot;&gt;Wei Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10813">
<title>Increasing Fairness via Combination with Learning Guarantees. (arXiv:2301.10813v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10813</link>
<description rdf:parseType="Literal">&lt;p&gt;The concern about underlying discrimination hidden in ML models is
increasing, as ML systems have been widely applied in more and more real-world
scenarios and any discrimination hidden in them will directly affect human
life. Many techniques have been developed to enhance fairness including
commonly-used group fairness measures and several fairness-aware methods
combining ensemble learning. However, existing fairness measures can only focus
on one aspect -- either group or individual fairness, and the hard
compatibility among them indicates a possibility of remaining biases even if
one of them is satisfied. Moreover, existing mechanisms to boost fairness
usually present empirical results to show validity, yet few of them discuss
whether fairness can be boosted with certain theoretical guarantees. To address
these issues, we propose a fairness quality measure named discriminative risk
in this paper to reflect both individual and group fairness aspects.
Furthermore, we investigate the properties of the proposed measure and propose
first- and second-order oracle bounds to show that fairness can be boosted via
ensemble combination with theoretical learning guarantees. Note that the
analysis is suitable for both binary and multi-class classification. A pruning
method is also proposed to utilise our proposed measure and comprehensive
experiments are conducted to evaluate the effectiveness of the proposed methods
in this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_Y/0/1/0/all/0/1&quot;&gt;Yijun Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_A/0/1/0/all/0/1&quot;&gt;Anqi Qiu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.12579">
<title>Sample Efficient Deep Reinforcement Learning via Local Planning. (arXiv:2301.12579v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.12579</link>
<description rdf:parseType="Literal">&lt;p&gt;The focus of this work is sample-efficient deep reinforcement learning (RL)
with a simulator. One useful property of simulators is that it is typically
easy to reset the environment to a previously observed state. We propose an
algorithmic framework, named uncertainty-first local planning (UFLP), that
takes advantage of this property. Concretely, in each data collection
iteration, with some probability, our meta-algorithm resets the environment to
an observed state which has high uncertainty, instead of sampling according to
the initial-state distribution. The agent-environment interaction then proceeds
as in the standard online RL setting. We demonstrate that this simple procedure
can dramatically improve the sample cost of several baseline RL algorithms on
difficult exploration tasks. Notably, with our framework, we can achieve
super-human performance on the notoriously hard Atari game, Montezuma&apos;s
Revenge, with a simple (distributional) double DQN. Our work can be seen as an
efficient approximate implementation of an existing algorithm with theoretical
guarantees, which offers an interpretation of the positive empirical results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dong Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiagarajan_S/0/1/0/all/0/1&quot;&gt;Sridhar Thiagarajan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazic_N/0/1/0/all/0/1&quot;&gt;Nevena Lazic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajaraman_N/0/1/0/all/0/1&quot;&gt;Nived Rajaraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_B/0/1/0/all/0/1&quot;&gt;Botao Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Szepesvari_C/0/1/0/all/0/1&quot;&gt;Csaba Szepesvari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01470">
<title>Learning to Optimize for Reinforcement Learning. (arXiv:2302.01470v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01470</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, by leveraging more data, computation, and diverse tasks,
learned optimizers have achieved remarkable success in supervised learning,
outperforming classical hand-designed optimizers. Reinforcement learning (RL)
is essentially different from supervised learning and in practice these learned
optimizers do not work well even in simple RL tasks. We investigate this
phenomenon and identity three issues. First, the gradients of an RL agent vary
across a wide range in logarithms while their absolute values are in a small
range, making neural networks hard to obtain accurate parameter updates.
Second, the agent-gradient distribution is non-independent and identically
distributed, leading to inefficient meta-training. Finally, due to highly
stochastic agent-environment interactions, the agent-gradients have high bias
and variance, which increase the difficulty of learning an optimizer for RL. We
propose gradient processing, pipeline training, and a novel optimizer structure
with good inductive bias to address these issues. By applying these techniques,
for the first time, we show that learning an optimizer for RL from scratch is
possible. Although only trained in toy tasks, our learned optimizer can
generalize to unseen complex tasks in Brax.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_Q/0/1/0/all/0/1&quot;&gt;Qingfeng Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_A/0/1/0/all/0/1&quot;&gt;A. Rupam Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_S/0/1/0/all/0/1&quot;&gt;Shuicheng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhongwen Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02119">
<title>Diversity Induced Environment Design via Self-Play. (arXiv:2302.02119v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02119</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on designing an appropriate distribution of environments has
shown promise for training effective generally capable agents. Its success is
partly because of a form of adaptive curriculum learning that generates
environment instances (or levels) at the frontier of the agent&apos;s capabilities.
However, such an environment design framework often struggles to find effective
levels in challenging design spaces and requires costly interactions with the
environment. In this paper, we aim to introduce diversity in the Unsupervised
Environment Design (UED) framework. Specifically, we propose a task-agnostic
method to identify observed/hidden states that are representative of a given
level. The outcome of this method is then utilized to characterize the
diversity between two levels, which as we show can be crucial to effective
performance. In addition, to improve sampling efficiency, we incorporate the
self-play technique that allows the environment generator to automatically
generate environments that are of great benefit to the training agent.
Quantitatively, our approach, Diversity-induced Environment Design via
Self-Play (DivSP), shows compelling performance over existing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Dexun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenjun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Varakantham_P/0/1/0/all/0/1&quot;&gt;Pradeep Varakantham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.03122">
<title>State-wise Safe Reinforcement Learning: A Survey. (arXiv:2302.03122v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.03122</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the tremendous success of Reinforcement Learning (RL) algorithms in
simulation environments, applying RL to real-world applications still faces
many challenges. A major concern is safety, in another word, constraint
satisfaction. State-wise constraints are one of the most common constraints in
real-world applications and one of the most challenging constraints in Safe RL.
Enforcing state-wise constraints is necessary and essential to many challenging
tasks such as autonomous driving, robot manipulation. This paper provides a
comprehensive review of existing approaches that address state-wise constraints
in RL. Under the framework of State-wise Constrained Markov Decision Process
(SCMDP), we will discuss the connections, differences, and trade-offs of
existing approaches in terms of (i) safety guarantee and scalability, (ii)
safety and reward performance, and (iii) safety after convergence and during
training. We also summarize limitations of current methods and discuss
potential future directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiye Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tairan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tianhao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.06354">
<title>Less is More: Selective Layer Finetuning with SubTuning. (arXiv:2302.06354v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.06354</link>
<description rdf:parseType="Literal">&lt;p&gt;Finetuning a pretrained model has become a standard approach for training
neural networks on novel tasks, resulting in fast convergence and improved
performance. In this work, we study an alternative finetuning method, where
instead of finetuning all the weights of the network, we only train a carefully
chosen subset of layers, keeping the rest of the weights frozen at their
initial (pretrained) values. We demonstrate that \emph{subset finetuning} (or
SubTuning) often achieves accuracy comparable to full finetuning of the model,
and even surpasses the performance of full finetuning when training data is
scarce. Therefore, SubTuning allows deploying new tasks at minimal
computational cost, while enjoying the benefits of finetuning the entire model.
This yields a simple and effective method for multi-task learning, where
different tasks do not interfere with one another, and yet share most of the
resources at inference time. We demonstrate the efficiency of SubTuning across
multiple tasks, using different network architectures and pretraining methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaplun_G/0/1/0/all/0/1&quot;&gt;Gal Kaplun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gurevich_A/0/1/0/all/0/1&quot;&gt;Andrey Gurevich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Swisa_T/0/1/0/all/0/1&quot;&gt;Tal Swisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+David_M/0/1/0/all/0/1&quot;&gt;Mazor David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shalev_Shwartz_S/0/1/0/all/0/1&quot;&gt;Shai Shalev-Shwartz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1&quot;&gt;Eran Malach&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.09543">
<title>Topological Feature Selection. (arXiv:2302.09543v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.09543</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce a novel unsupervised, graph-based filter feature
selection technique which exploits the power of topologically constrained
network representations. We model dependency structures among features using a
family of chordal graphs (the Triangulated Maximally Filtered Graph), and we
maximise the likelihood of features&apos; relevance by studying their relative
position inside the network. Such an approach presents three aspects that are
particularly satisfactory compared to its alternatives: (i) it is highly
tunable and easily adaptable to the nature of input data; (ii) it is fully
explainable, maintaining, at the same time, a remarkable level of simplicity;
(iii) it is computationally cheaper compared to its alternatives. We test our
algorithm on 16 benchmark datasets from different applicative domains showing
that it outperforms or matches the current state-of-the-art under heterogeneous
evaluation conditions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Briola_A/0/1/0/all/0/1&quot;&gt;Antonio Briola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aste_T/0/1/0/all/0/1&quot;&gt;Tomaso Aste&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10258">
<title>Neural Algorithmic Reasoning with Causal Regularisation. (arXiv:2302.10258v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10258</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work on neural algorithmic reasoning has investigated the reasoning
capabilities of neural networks, effectively demonstrating they can learn to
execute classical algorithms on unseen data coming from the train distribution.
However, the performance of existing neural reasoners significantly degrades on
out-of-distribution (OOD) test data, where inputs have larger sizes. In this
work, we make an important observation: there are many different inputs for
which an algorithm will perform certain intermediate computations identically.
This insight allows us to develop data augmentation procedures that, given an
algorithm&apos;s intermediate trajectory, produce inputs for which the target
algorithm would have exactly the same next trajectory step. We ensure
invariance in the next-step prediction across such inputs, by employing a
self-supervised objective derived by our observation, formalised in a causal
graph. We prove that the resulting method, which we call Hint-ReLIC, improves
the OOD generalisation capabilities of the reasoner. We evaluate our method on
the CLRS algorithmic reasoning benchmark, where we show up to 3$\times$
improvements on the OOD test data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bevilacqua_B/0/1/0/all/0/1&quot;&gt;Beatrice Bevilacqua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikiforou_K/0/1/0/all/0/1&quot;&gt;Kyriacos Nikiforou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ibarz_B/0/1/0/all/0/1&quot;&gt;Borja Ibarz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bica_I/0/1/0/all/0/1&quot;&gt;Ioana Bica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paganini_M/0/1/0/all/0/1&quot;&gt;Michela Paganini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blundell_C/0/1/0/all/0/1&quot;&gt;Charles Blundell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitrovic_J/0/1/0/all/0/1&quot;&gt;Jovana Mitrovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velickovic_P/0/1/0/all/0/1&quot;&gt;Petar Veli&amp;#x10d;kovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10681">
<title>FrankenSplit: Efficient Neural Feature Compression with Shallow Variational Bottleneck Injection for Mobile Edge Computing. (arXiv:2302.10681v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10681</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of mobile AI accelerators allows latency-sensitive applications to
execute lightweight Deep Neural Networks (DNNs) on the client side. However,
critical applications require powerful models that edge devices cannot host and
must therefore offload requests, where the high-dimensional data will compete
for limited bandwidth. This work proposes shifting away from focusing on
executing shallow layers of partitioned DNNs. Instead, it advocates
concentrating the local resources on variational compression optimized for
machine interpretability. We introduce a novel framework for resource-conscious
compression models and extensively evaluate our method in an environment
reflecting the asymmetric resource distribution between edge devices and
servers. Our method achieves 60% lower bitrate than a state-of-the-art SC
method without decreasing accuracy and is up to 16x faster than offloading with
existing codec standards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Furutanpey_A/0/1/0/all/0/1&quot;&gt;Alireza Furutanpey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Raith_P/0/1/0/all/0/1&quot;&gt;Philipp Raith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dustdar_S/0/1/0/all/0/1&quot;&gt;Schahram Dustdar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.10894">
<title>Red Teaming Deep Neural Networks with Feature Synthesis Tools. (arXiv:2302.10894v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.10894</link>
<description rdf:parseType="Literal">&lt;p&gt;Interpretable AI tools are often motivated by the goal of understanding model
behavior in out-of-distribution (OOD) contexts. Despite the attention this area
of study receives, there are comparatively few cases where these tools have
identified novel, previously unknown, bugs in models. We argue that this is
due, in part, to a common feature of many interpretability methods: they
analyze and explain the behavior of a model using a particular dataset. While
this is useful, such tools can only analyze behaviors induced by features that
the user can sample or identify in advance. To address this, a growing body of
research involves interpreting models using feature synthesis methods which do
not depend on a dataset.
&lt;/p&gt;
&lt;p&gt;In this paper, our primary contribution is a benchmark to evaluate
interpretability tools. Our key insight is that we can train models that
respond to specific triggers (e.g., a specific patch inserted into an image)
with specific outputs (i.e. a label) and then evaluate interpretability tools
based on whether they help humans identify these triggers. We make four
contributions. (1) We propose trojan discovery as an evaluation task for
interpretability tools and introduce a trojan-discovery benchmark with 12
trojans of 3 different types. (2) We demonstrate the difficulty of this
benchmark with a preliminary evaluation of 16 feature attribution/saliency
tools. Even with access to data with a trojan&apos;s trigger, these methods
regularly fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on
our benchmark. (4) We introduce and evaluate 2 variants of the best-performing
method from the previous evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Casper_S/0/1/0/all/0/1&quot;&gt;Stephen Casper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuxiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_T/0/1/0/all/0/1&quot;&gt;Tong Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kevin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hariharan_K/0/1/0/all/0/1&quot;&gt;Kaivalya Hariharan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hadfield_Menell_D/0/1/0/all/0/1&quot;&gt;Dylan Hadfield-Menell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11012">
<title>Likelihood Annealing: Fast Calibrated Uncertainty for Regression. (arXiv:2302.11012v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11012</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning have shown that uncertainty estimation is
becoming increasingly important in applications such as medical imaging,
natural language processing, and autonomous systems. However, accurately
quantifying uncertainty remains a challenging problem, especially in regression
tasks where the output space is continuous. Deep learning approaches that allow
uncertainty estimation for regression problems often converge slowly and yield
poorly calibrated uncertainty estimates that can not be effectively used for
quantification. Recently proposed post hoc calibration techniques are seldom
applicable to regression problems and often add overhead to an already slow
model training phase. This work presents a fast calibrated uncertainty
estimation method for regression tasks called Likelihood Annealing, that
consistently improves the convergence of deep regression models and yields
calibrated uncertainty without any post hoc calibration phase. Unlike previous
methods for calibrated uncertainty in regression that focus only on
low-dimensional regression problems, our method works well on a broad spectrum
of regression problems, including high-dimensional regression.Our empirical
analysis shows that our approach is generalizable to various network
architectures, including multilayer perceptrons, 1D/2D convolutional networks,
and graph neural networks, on five vastly diverse tasks, i.e., chaotic particle
trajectory denoising, physical property prediction of molecules using 3D
atomistic representation, natural image super-resolution, and medical image
translation using MRI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Upadhyay_U/0/1/0/all/0/1&quot;&gt;Uddeshya Upadhyay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jae Myung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_C/0/1/0/all/0/1&quot;&gt;Cordelia Schmidt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akata_Z/0/1/0/all/0/1&quot;&gt;Zeynep Akata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.11563">
<title>Exploration by self-supervised exploitation. (arXiv:2302.11563v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2302.11563</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning can solve decision-making problems and train an agent
to behave in an environment according to a predesigned reward function.
However, such an approach becomes very problematic if the reward is too sparse
and the agent does not come across the reward during the environmental
exploration. The solution to such a problem may be in equipping the agent with
an intrinsic motivation, which will provide informed exploration, during which
the agent is likely to also encounter external reward. Novelty detection is one
of the promising branches of intrinsic motivation research. We present
Self-supervised Network Distillation (SND), a class of internal motivation
algorithms based on the distillation error as a novelty indicator, where the
target model is trained using self-supervised learning. We adapted three
existing self-supervised methods for this purpose and experimentally tested
them on a set of ten environments that are considered difficult to explore. The
results show that our approach achieves faster growth and higher external
reward for the same training time compared to the baseline models, which
implies improved exploration in a very sparse reward environment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pechac_M/0/1/0/all/0/1&quot;&gt;Matej Pech&amp;#xe1;&amp;#x10d;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chovanec_M/0/1/0/all/0/1&quot;&gt;Michal Chovanec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farkas_I/0/1/0/all/0/1&quot;&gt;Igor Farka&amp;#x161;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12432">
<title>Graph Neural Networks with Learnable and Optimal Polynomial Bases. (arXiv:2302.12432v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12432</link>
<description rdf:parseType="Literal">&lt;p&gt;Polynomial filters, a kind of Graph Neural Networks, typically use a
predetermined polynomial basis and learn the coefficients from the training
data. It has been observed that the effectiveness of the model is highly
dependent on the property of the polynomial basis. Consequently, two natural
and fundamental questions arise: Can we learn a suitable polynomial basis from
the training data? Can we determine the optimal polynomial basis for a given
graph and node features?
&lt;/p&gt;
&lt;p&gt;In this paper, we propose two spectral GNN models that provide positive
answers to the questions posed above. First, inspired by Favard&apos;s Theorem, we
propose the FavardGNN model, which learns a polynomial basis from the space of
all possible orthonormal bases. Second, we examine the supposedly unsolvable
definition of optimal polynomial basis from Wang &amp;amp; Zhang (2022) and propose a
simple model, OptBasisGNN, which computes the optimal basis for a given graph
structure and graph signal. Extensive experiments are conducted to demonstrate
the effectiveness of our proposed models. Our code is available at
https://github.com/yuziGuo/FarOptBasis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuhe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14229">
<title>Zero-Shot Cross-Lingual Summarization via Large Language Models. (arXiv:2302.14229v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14229</link>
<description rdf:parseType="Literal">&lt;p&gt;Given a document in a source language, cross-lingual summarization (CLS) aims
to generate a summary in a different target language. Recently, the emergence
of Large Language Models (LLMs), such as GPT-3.5, ChatGPT and GPT-4, has
attracted wide attention from the computational linguistics community. However,
it is not yet known the performance of LLMs on CLS. In this report, we
empirically use various prompts to guide LLMs to perform zero-shot CLS from
different paradigms (i.e., end-to-end and pipeline), and provide a preliminary
evaluation on the generated summaries. We find that ChatGPT and GPT-4
originally prefer to produce lengthy summaries with detailed information. These
two LLMs can further balance informativeness and conciseness with the help of
an interactive prompt, significantly improving their CLS performance.
Experimental results on three widely-used CLS datasets show that GPT-4 achieves
state-of-the-art zero-shot CLS performance, and performs competitively compared
with the fine-tuned mBART-50. Moreover, we also find some multi-lingual and
bilingual LLMs (i.e., BLOOMZ, ChatGLM-6B, Vicuna-13B and ChatYuan) have limited
zero-shot CLS ability. Due to the composite nature of CLS, which requires
models to perform summarization and translation simultaneously, accomplishing
this task in a zero-shot manner is even a challenge for LLMs. Therefore, we
sincerely hope and recommend future LLM research could use CLS as a testbed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yunlong Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_F/0/1/0/all/0/1&quot;&gt;Fandong Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_B/0/1/0/all/0/1&quot;&gt;Beiqi Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhixu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_J/0/1/0/all/0/1&quot;&gt;Jianfeng Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04356">
<title>Soft Actor-Critic Algorithm with Truly-satisfied Inequality Constraint. (arXiv:2303.04356v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04356</link>
<description rdf:parseType="Literal">&lt;p&gt;Soft actor-critic (SAC) in reinforcement learning is expected to be one of
the next-generation robot control schemes. Its ability to maximize policy
entropy would make a robotic controller robust to noise and perturbation, which
is useful for real-world robot applications. However, the priority of
maximizing the policy entropy is automatically tuned in the current
implementation, the rule of which can be interpreted as one for equality
constraint, binding the policy entropy into its specified lower bound. The
current SAC is therefore no longer maximize the policy entropy, contrary to our
expectation. To resolve this issue in SAC, this paper improves its
implementation with a learnable state-dependent slack variable for
appropriately handling the inequality constraint to maximize the policy entropy
by reformulating it as the corresponding equality constraint. The introduced
slack variable is optimized by a switching-type loss function that takes into
account the dual objectives of satisfying the equality constraint and checking
the lower bound. In Mujoco and Pybullet simulators, the modified SAC
statistically achieved the higher robustness for adversarial attacks than
before while regularizing the norm of action. A real-robot variable impedance
task was demonstrated for showing the applicability of the modified SAC to
real-world robot control. In particular, the modified SAC maintained adaptive
behaviors for physical human-robot interaction, which had no experience at all
during training. https://youtu.be/EH3xVtlVaJw
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1&quot;&gt;Taisuke Kobayashi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.12040">
<title>Roots and Requirements for Collaborative AI. (arXiv:2303.12040v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.12040</link>
<description rdf:parseType="Literal">&lt;p&gt;The vision of AI collaborators has long been a staple of science fiction,
where artificial agents understand nuances of collaboration and human
communication. They assist their human partners and teams and contribute
special talents. Government advisory groups and leaders in AI have advocated
for years that AIs should be human compatible and effective collaborators.
Nonetheless, robust AIs that collaborate like talented people remain out of
reach. This position paper draws on psychology and social sciences for an
analysis of what effective and robust collaboration requires. It sketches a
history of public and AI visions for artificial collaborators, starting with
early visions of intelligence augmentation (IA) and artificial intelligence
(AI). It is intended as motivation and context for a second position paper that
recommends how to bootstrap AIs and AI collaborators (Stefik &amp;amp; Price, 2023).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stefik_M/0/1/0/all/0/1&quot;&gt;Mark Stefik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00962">
<title>RegionPLC: Regional Point-Language Contrastive Learning for Open-World 3D Scene Understanding. (arXiv:2304.00962v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00962</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing 3D scene understanding tasks have achieved high performance on
close-set benchmarks but fail to handle novel categories in real-world
applications. To this end, we propose a Regional Point-Language Contrastive
learning framework, namely RegionPLC, for open-world 3D scene understanding,
which equips models trained on closed-set datasets with open-vocabulary
recognition capabilities. We propose dense visual prompts to elicit
region-level visual-language knowledge from 2D foundation models via
captioning, which further allows us to build dense regional point-language
associations. Then, we design a point-discriminative contrastive learning
objective to enable point-independent learning from captions for dense scene
understanding. We conduct extensive experiments on ScanNet, ScanNet200, and
nuScenes datasets. Our RegionPLC significantly outperforms previous
base-annotated 3D open-world scene understanding approaches by an average of
11.6\% and 6.6\% for semantic and instance segmentation, respectively. It also
shows promising open-world results in absence of any human annotation with low
training and inference costs. Code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jihan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Runyu Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01117">
<title>Interpretable Symbolic Regression for Data Science: Analysis of the 2022 Competition. (arXiv:2304.01117v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01117</link>
<description rdf:parseType="Literal">&lt;p&gt;Symbolic regression searches for analytic expressions that accurately
describe studied phenomena. The main attraction of this approach is that it
returns an interpretable model that can be insightful to users. Historically,
the majority of algorithms for symbolic regression have been based on
evolutionary algorithms. However, there has been a recent surge of new
proposals that instead utilize approaches such as enumeration algorithms, mixed
linear integer programming, neural networks, and Bayesian optimization. In
order to assess how well these new approaches behave on a set of common
challenges often faced in real-world data, we hosted a competition at the 2022
Genetic and Evolutionary Computation Conference consisting of different
synthetic and real-world datasets which were blind to entrants. For the
real-world track, we assessed interpretability in a realistic way by using a
domain expert to judge the trustworthiness of candidate models.We present an
in-depth analysis of the results obtained in this competition, discuss current
challenges of symbolic regression algorithms and highlight possible
improvements for future competitions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franca_F/0/1/0/all/0/1&quot;&gt;F. O. de Franca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virgolin_M/0/1/0/all/0/1&quot;&gt;M. Virgolin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kommenda_M/0/1/0/all/0/1&quot;&gt;M. Kommenda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Majumder_M/0/1/0/all/0/1&quot;&gt;M. S. Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cranmer_M/0/1/0/all/0/1&quot;&gt;M. Cranmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Espada_G/0/1/0/all/0/1&quot;&gt;G. Espada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ingelse_L/0/1/0/all/0/1&quot;&gt;L. Ingelse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fonseca_A/0/1/0/all/0/1&quot;&gt;A. Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landajuela_M/0/1/0/all/0/1&quot;&gt;M. Landajuela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petersen_B/0/1/0/all/0/1&quot;&gt;B. Petersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glatt_R/0/1/0/all/0/1&quot;&gt;R. Glatt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mundhenk_N/0/1/0/all/0/1&quot;&gt;N. Mundhenk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_C/0/1/0/all/0/1&quot;&gt;C. S. Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hochhalter_J/0/1/0/all/0/1&quot;&gt;J. D. Hochhalter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Randall_D/0/1/0/all/0/1&quot;&gt;D. L. Randall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamienny_P/0/1/0/all/0/1&quot;&gt;P. Kamienny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;H. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dick_G/0/1/0/all/0/1&quot;&gt;G. Dick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Simon_A/0/1/0/all/0/1&quot;&gt;A. Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burlacu_B/0/1/0/all/0/1&quot;&gt;B. Burlacu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kasak_J/0/1/0/all/0/1&quot;&gt;Jaan Kasak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Machado_M/0/1/0/all/0/1&quot;&gt;Meera Machado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilstrup_C/0/1/0/all/0/1&quot;&gt;Casper Wilstrup&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cava_W/0/1/0/all/0/1&quot;&gt;W. G. La Cava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.07702">
<title>Towards Better Evaluation of GNN Expressiveness with BREC Dataset. (arXiv:2304.07702v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.07702</link>
<description rdf:parseType="Literal">&lt;p&gt;Research on the theoretical expressiveness of Graph Neural Networks (GNNs)
has developed rapidly, and many methods have been proposed to enhance the
expressiveness. However, most methods do not have a uniform expressiveness
measure except for a few that strictly follow the $k$-dimensional
Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often
limited to distinguishing certain families of non-isomorphic graphs, leading to
difficulties in quantitatively comparing their expressiveness. In contrast to
theoretical analysis, another way to measure expressiveness is by evaluating
model performance on certain datasets containing 1-WL-indistinguishable graphs.
Previous datasets specifically designed for this purpose, however, face
problems with difficulty (any model surpassing 1-WL has nearly 100% accuracy),
granularity (models tend to be either 100% correct or near random guess), and
scale (only a few essentially different graphs in each dataset). To address
these limitations, we propose a new expressiveness dataset, $\textbf{BREC}$,
which includes 400 pairs of non-isomorphic graphs carefully selected from four
primary categories (Basic, Regular, Extension, and CFI). These graphs have
higher difficulty (up to 4-WL-indistinguishable), finer granularity (able to
compare models between 1-WL and 3-WL), and a larger scale (400 pairs). Further,
we synthetically test 23 models with higher-than-1-WL expressiveness on our
BREC dataset. Our experiment gives the first thorough comparison of the
expressiveness of those state-of-the-art beyond-1-WL GNN models. We expect this
dataset to serve as a benchmark for testing the expressiveness of future GNNs.
Our dataset and evaluation code are released at:
https://github.com/GraphPKU/BREC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yanbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Muhan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10888">
<title>Learning Robust, Agile, Natural Legged Locomotion Skills in the Wild. (arXiv:2304.10888v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10888</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, reinforcement learning has become a promising and polular solution
for robot legged locomotion. Compared to model-based control, reinforcement
learning based controllers can achieve better robustness against uncertainties
of environments through sim-to-real learning. However, the corresponding
learned gaits are in general overly conservative and unatural. In this paper,
we propose a new framework for learning robust, agile and natural legged
locomotion skills over challenging terrain. We incorporate an adversarial
training branch based on real animal locomotion data upon a teacher-student
training pipeline for robust sim-to-real transfer. Empirical results on both
simulation and real world of a quadruped robot demonstrate that our proposed
algorithm enables robustly traversing challenging terrains such as stairs,
rocky ground and slippery floor with only proprioceptive perception. Meanwhile,
the gaits are more agile, natural, and energy efficient compared to the
baselines. Both qualitative and quantitative results are presented in this
paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jianyu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11436">
<title>Breaching FedMD: Image Recovery via Paired-Logits Inversion Attack. (arXiv:2304.11436v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11436</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning with Model Distillation (FedMD) is a nascent collaborative
learning paradigm, where only output logits of public datasets are transmitted
as distilled knowledge, instead of passing on private model parameters that are
susceptible to gradient inversion attacks, a known privacy risk in federated
learning. In this paper, we found that even though sharing output logits of
public datasets is safer than directly sharing gradients, there still exists a
substantial risk of data exposure caused by carefully designed malicious
attacks. Our study shows that a malicious server can inject a PLI
(Paired-Logits Inversion) attack against FedMD and its variants by training an
inversion neural network that exploits the confidence gap between the server
and client models. Experiments on multiple facial recognition datasets validate
that under FedMD-like schemes, by using paired server-client logits of public
datasets only, the malicious server is able to reconstruct private images on
all tested benchmarks with a high success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Takahashi_H/0/1/0/all/0/1&quot;&gt;Hideaki Takahashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jingjing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14630">
<title>Let the Chart Spark: Embedding Semantic Context into Chart with Text-to-Image Generative Model. (arXiv:2304.14630v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14630</link>
<description rdf:parseType="Literal">&lt;p&gt;Pictorial visualization seamlessly integrates data and semantic context into
visual representation, conveying complex information in a manner that is both
engaging and informative. Extensive studies have been devoted to developing
authoring tools to simplify the creation of pictorial visualizations. However,
mainstream works mostly follow a retrieving-and-editing pipeline that heavily
relies on retrieved visual elements from a dedicated corpus, which often
compromise the data integrity. Text-guided generation methods are emerging, but
may have limited applicability due to its predefined recognized entities. In
this work, we propose ChartSpark, a novel system that embeds semantic context
into chart based on text-to-image generative model. ChartSpark generates
pictorial visualizations conditioned on both semantic context conveyed in
textual inputs and data information embedded in plain charts. The method is
generic for both foreground and background pictorial generation, satisfying the
design practices identified from an empirical research into existing pictorial
visualizations. We further develop an interactive visual interface that
integrates a text analyzer, editing module, and evaluation module to enable
users to generate, modify, and assess pictorial visualizations. We
experimentally demonstrate the usability of our tool, and conclude with a
discussion of the potential of using text-to-image generative model combined
with interactive interface for visualization design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shishi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Suizi Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yue Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yilin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_W/0/1/0/all/0/1&quot;&gt;Wei Zeng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05392">
<title>Sharpness-Aware Minimization Alone can Improve Adversarial Robustness. (arXiv:2305.05392v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05392</link>
<description rdf:parseType="Literal">&lt;p&gt;Sharpness-Aware Minimization (SAM) is an effective method for improving
generalization ability by regularizing loss sharpness. In this paper, we
explore SAM in the context of adversarial robustness. We find that using only
SAM can achieve superior adversarial robustness without sacrificing clean
accuracy compared to standard training, which is an unexpected benefit. We also
discuss the relation between SAM and adversarial training (AT), a popular
method for improving the adversarial robustness of DNNs. In particular, we show
that SAM and AT differ in terms of perturbation strength, leading to different
accuracy and robustness trade-offs. We provide theoretical evidence for these
claims in a simplified model. Finally, while AT suffers from decreased clean
accuracy and computational overhead, we suggest that SAM can be regarded as a
lightweight substitute for AT under certain requirements. Code is available at
https://github.com/weizeming/SAM_AT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zeming Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yihao Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06174">
<title>Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging. (arXiv:2305.06174v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06174</link>
<description rdf:parseType="Literal">&lt;p&gt;Climate change is the defining issue of our time, and we are at a defining
moment. Various interest groups, social movement organizations, and individuals
engage in collective action on this issue on social media. In addition, issue
advocacy campaigns on social media often arise in response to ongoing societal
concerns, especially those faced by energy industries. Our goal in this paper
is to analyze how those industries, their advocacy group, and climate advocacy
group use social media to influence the narrative on climate change. In this
work, we propose a minimally supervised model soup [57] approach combined with
messaging themes to identify the stances of climate ads on Facebook. Finally,
we release our stance dataset, model, and set of themes related to climate
campaigns for future work on opinion mining and the automatic detection of
climate change stances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_T/0/1/0/all/0/1&quot;&gt;Tunazzina Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruqi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goldwasser_D/0/1/0/all/0/1&quot;&gt;Dan Goldwasser&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06569">
<title>How to Index Item IDs for Recommendation Foundation Models. (arXiv:2305.06569v3 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06569</link>
<description rdf:parseType="Literal">&lt;p&gt;Recommendation foundation model utilizes large language models (LLM) for
recommendation by converting recommendation tasks into natural language tasks.
It enables generative recommendation which directly generates the item(s) to
recommend rather than calculating a ranking score for each and every candidate
item in traditional recommendation models, simplifying the recommendation
pipeline from multi-stage filtering to single-stage filtering. To avoid
generating excessively long text when deciding which item(s) to recommend,
creating LLM-compatible item IDs is essential for recommendation foundation
models. In this study, we systematically examine the item indexing problem for
recommendation foundation models, using P5 as the representative backbone model
and replicating its results with various indexing methods. To emphasize the
importance of item indexing, we first discuss the issues of several trivial
item indexing methods, such as independent indexing, title indexing, and random
indexing. We then propose four simple yet effective solutions, including
sequential indexing, collaborative indexing, semantic (content-based) indexing,
and hybrid indexing. Our reproducibility study of P5 highlights the significant
influence of item indexing methods on the model performance, and our results on
real-world datasets validate the effectiveness of our proposed solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_W/0/1/0/all/0/1&quot;&gt;Wenyue Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Shuyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yingqiang Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongfeng Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13093">
<title>Restore Anything Pipeline: Segment Anything Meets Image Restoration. (arXiv:2305.13093v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13093</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent image restoration methods have produced significant advancements using
deep learning. However, existing methods tend to treat the whole image as a
single entity, failing to account for the distinct objects in the image that
exhibit individual texture properties. Existing methods also typically generate
a single result, which may not suit the preferences of different users. In this
paper, we introduce the Restore Anything Pipeline (RAP), a novel interactive
and per-object level image restoration approach that incorporates a
controllable model to generate different results that users may choose from.
RAP incorporates image segmentation through the recent Segment Anything Model
(SAM) into a controllable image restoration model to create a user-friendly
pipeline for several image restoration tasks. We demonstrate the versatility of
RAP by applying it to three common image restoration tasks: image deblurring,
image denoising, and JPEG artifact removal. Our experiments show that RAP
produces superior visual results compared to state-of-the-art methods. RAP
represents a promising direction for image restoration, providing users with
greater control, and enabling image restoration at an object level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiaxi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holz_C/0/1/0/all/0/1&quot;&gt;Christian Holz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13681">
<title>GUARD: A Safe Reinforcement Learning Benchmark. (arXiv:2305.13681v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13681</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the trial-and-error nature, it is typically challenging to apply RL
algorithms to safety-critical real-world applications, such as autonomous
driving, human-robot interaction, robot manipulation, etc, where such errors
are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly
in the literature, in which the agents explore the environment while satisfying
constraints. Due to the diversity of algorithms and tasks, it remains difficult
to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a
Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD
has several advantages compared to existing benchmarks. First, GUARD is a
generalized benchmark with a wide variety of RL agents, tasks, and safety
constraint specifications. Second, GUARD comprehensively covers
state-of-the-art safe RL algorithms with self-contained implementations. Third,
GUARD is highly customizable in tasks and algorithms. We present a comparison
of state-of-the-art safe RL algorithms in various task settings using GUARD and
establish baselines that future work can build on.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Weiye Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Rui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yifan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruixuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tianhao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Changliu Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15689">
<title>Zero-shot Approach to Overcome Perturbation Sensitivity of Prompts. (arXiv:2305.15689v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15689</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have demonstrated that natural-language prompts can help to
leverage the knowledge learned by pre-trained language models for the binary
sentence-level sentiment classification task. Specifically, these methods
utilize few-shot learning settings to fine-tune the sentiment classification
model using manual or automatically generated prompts. However, the performance
of these methods is sensitive to the perturbations of the utilized prompts.
Furthermore, these methods depend on a few labeled instances for automatic
prompt generation and prompt ranking. This study aims to find high-quality
prompts for the given task in a zero-shot setting. Given a base prompt, our
proposed approach automatically generates multiple prompts similar to the base
prompt employing positional, reasoning, and paraphrasing techniques and then
ranks the prompts using a novel metric. We empirically demonstrate that the
top-ranked prompts are high-quality and significantly outperform the base
prompt and the prompts generated using few-shot learning for the binary
sentence-level sentiment classification task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_M/0/1/0/all/0/1&quot;&gt;Mohna Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulkarni_A/0/1/0/all/0/1&quot;&gt;Adithya Kulkarni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.15769">
<title>MERGE: Fast Private Text Generation. (arXiv:2305.15769v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.15769</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have seen increasing concerns about the private inference of NLP
services and Transformer models. However, existing two-party privacy-preserving
methods solely consider NLU scenarios, while the private inference of text
generation such as translation, dialogue, and code completion remains unsolved.
Besides, while migrated to NLG models, existing privacy-preserving methods
perform poorly in terms of inference speed, and suffer from the convergence
problem during the training stage. To address these issues, we propose MERGE, a
fast private text generation framework for Transformer-based language models.
Specifically, MERGE reuse the output hidden state as the word embedding to
bypass the embedding computation, and reorganize the linear operations in the
Transformer module to accelerate the forward procedure. Based on these two
optimizations, extensive experiments show that MERGE can achieve a 26.5x
speedup under the sequence length 512, and reduce 80\% communication bytes,
with an up to 10x speedup to existing state-of-art models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zi Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pinghui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_L/0/1/0/all/0/1&quot;&gt;Lifeng Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_N/0/1/0/all/0/1&quot;&gt;Nuo Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16380">
<title>Scan and Snap: Understanding Training Dynamics and Token Composition in 1-layer Transformer. (arXiv:2305.16380v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16380</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer architecture has shown impressive performance in multiple
research domains and has become the backbone of many neural network models.
However, there is limited understanding on how it works. In particular, with a
simple predictive loss, how the representation emerges from the gradient
\emph{training dynamics} remains a mystery. In this paper, for 1-layer
transformer with one self-attention layer plus one decoder layer, we analyze
its SGD training dynamics for the task of next token prediction in a
mathematically rigorous manner. We open the black box of the dynamic process of
how the self-attention layer combines input tokens, and reveal the nature of
underlying inductive bias. More specifically, with the assumption (a) no
positional encoding, (b) long input sequence, and (c) the decoder layer learns
faster than the self-attention layer, we prove that self-attention acts as a
\emph{discriminative scanning algorithm}: starting from uniform attention, it
gradually attends more to distinct key tokens for a specific next token to be
predicted, and pays less attention to common key tokens that occur across
different next tokens. Among distinct tokens, it progressively drops attention
weights, following the order of low to high co-occurrence between the key and
the query token in the training set. Interestingly, this procedure does not
lead to winner-takes-all, but decelerates due to a \emph{phase transition} that
is controllable by the learning rates of the two layers, leaving (almost) fixed
token combination. We verify this \textbf{\emph{scan and snap}} dynamics on
synthetic and real-world data (WikiText).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuandong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Beidi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Simon Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18243">
<title>Practical PCG Through Large Language Models. (arXiv:2305.18243v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18243</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have proven to be useful tools in various
domains outside of the field of their inception, which was natural language
processing. In this study, we provide practical directions on how to use LLMs
to generate 2D-game rooms for an under-development game, named Metavoidal. Our
technique can harness the power of GPT-3 by Human-in-the-loop fine-tuning which
allows our method to create 37% Playable-Novel levels from as scarce data as
only 60 hand-designed rooms under a scenario of the non-trivial game, with
respect to (Procedural Content Generation) PCG, that has a good amount of local
and global constraints.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nasir_M/0/1/0/all/0/1&quot;&gt;Muhammad U Nasir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Togelius_J/0/1/0/all/0/1&quot;&gt;Julian Togelius&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18326">
<title>BigVideo: A Large-scale Video Subtitle Translation Dataset for Multimodal Machine Translation. (arXiv:2305.18326v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18326</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a large-scale video subtitle translation dataset, BigVideo, to
facilitate the study of multi-modality machine translation. Compared with the
widely used How2 and VaTeX datasets, BigVideo is more than 10 times larger,
consisting of 4.5 million sentence pairs and 9,981 hours of videos. We also
introduce two deliberately designed test sets to verify the necessity of visual
information: Ambiguous with the presence of ambiguous words, and Unambiguous in
which the text context is self-contained for translation. To better model the
common semantics shared across texts and videos, we introduce a contrastive
learning method in the cross-modal encoder. Extensive experiments on the
BigVideo show that: a) Visual information consistently improves the NMT model
in terms of BLEU, BLEURT, and COMET on both Ambiguous and Unambiguous test
sets. b) Visual information helps disambiguation, compared to the strong text
baseline on terminology-targeted scores and human evaluation. Dataset and our
implementations are available at https://github.com/DeepLearnXMU/BigVideo-VMT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_L/0/1/0/all/0/1&quot;&gt;Liyan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Luyang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_N/0/1/0/all/0/1&quot;&gt;Ningxin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_P/0/1/0/all/0/1&quot;&gt;Peihao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Z/0/1/0/all/0/1&quot;&gt;Zewei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Shanbo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mingxuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Degen Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jinsong Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18438">
<title>Reinforcement Learning with Human Feedback: Learning Dynamic Choices via Pessimism. (arXiv:2305.18438v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18438</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study offline Reinforcement Learning with Human Feedback
(RLHF) where we aim to learn the human&apos;s underlying reward and the MDP&apos;s
optimal policy from a set of trajectories induced by human choices. RLHF is
challenging for multiple reasons: large state space but limited human feedback,
the bounded rationality of human decisions, and the off-policy distribution
shift. In this paper, we focus on the Dynamic Discrete Choice (DDC) model for
modeling and understanding human choices. DCC, rooted in econometrics and
decision theory, is widely used to model a human decision-making process with
forward-looking and bounded rationality. We propose a
\underline{D}ynamic-\underline{C}hoice-\underline{P}essimistic-\underline{P}olicy-\underline{O}ptimization
(DCPPO) method. \ The method involves a three-stage process: The first step is
to estimate the human behavior policy and the state-action value function via
maximum likelihood estimation (MLE); the second step recovers the human reward
function via minimizing Bellman mean squared error using the learned value
functions; the third step is to plug in the learned reward and invoke
pessimistic value iteration for finding a near-optimal policy. With only
single-policy coverage (i.e., optimal policy) of the dataset, we prove that the
suboptimality of DCPPO almost matches the classical pessimistic offline RL
algorithm in terms of suboptimality&apos;s dependency on distribution shift and
dimension. To the best of our knowledge, this paper presents the first
theoretical guarantees for off-policy offline RLHF with dynamic discrete choice
model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zihao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengdi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00942">
<title>Train Offline, Test Online: A Real Robot Learning Benchmark. (arXiv:2306.00942v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00942</link>
<description rdf:parseType="Literal">&lt;p&gt;Three challenges limit the progress of robot learning research: robots are
expensive (few labs can participate), everyone uses different robots (findings
do not generalize across labs), and we lack internet-scale robotics data. We
take on these challenges via a new benchmark: Train Offline, Test Online
(TOTO). TOTO provides remote users with access to shared robotic hardware for
evaluating methods on common tasks and an open-source dataset of these tasks
for offline training. Its manipulation task suite requires challenging
generalization to unseen objects, positions, and lighting. We present initial
results on TOTO comparing five pretrained visual representations and four
offline policy learning baselines, remotely contributed by five institutions.
The real promise of TOTO, however, lies in the future: we release the benchmark
for additional submissions from any user, enabling easy, direct comparison to
several methods without the need to obtain hardware or collect data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Gaoyue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dean_V/0/1/0/all/0/1&quot;&gt;Victoria Dean&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srirama_M/0/1/0/all/0/1&quot;&gt;Mohan Kumar Srirama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajeswaran_A/0/1/0/all/0/1&quot;&gt;Aravind Rajeswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pari_J/0/1/0/all/0/1&quot;&gt;Jyothish Pari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatch_K/0/1/0/all/0/1&quot;&gt;Kyle Hatch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Aryan Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tianhe Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbeel_P/0/1/0/all/0/1&quot;&gt;Pieter Abbeel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinto_L/0/1/0/all/0/1&quot;&gt;Lerrel Pinto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finn_C/0/1/0/all/0/1&quot;&gt;Chelsea Finn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhinav Gupta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01763">
<title>Optimization for truss design using Bayesian optimization. (arXiv:2306.01763v2 [stat.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01763</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, geometry optimization of mechanical truss using computer-aided
finite element analysis is presented. The shape of the truss is a dominant
factor in determining the capacity of load it can bear. At a given parameter
space, our goal is to find the parameters of a hull that maximize the
load-bearing capacity and also don&apos;t yield to the induced stress. We rely on
finite element analysis, which is a computationally costly design analysis tool
for design evaluation. For such expensive to-evaluate functions, we chose
Bayesian optimization as our optimization framework which has empirically
proven sample efficient than other simulation-based optimization methods.
&lt;/p&gt;
&lt;p&gt;By utilizing Bayesian optimization algorithms, the truss design involves
iteratively evaluating a set of candidate truss designs and updating a
probabilistic model of the design space based on the results. The model is used
to predict the performance of each candidate design, and the next candidate
design is selected based on the prediction and an acquisition function that
balances exploration and exploitation of the design space. Our result can be
used as a baseline for future study on AI-based optimization in expensive
engineering domains especially in finite element Analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sandeep_B/0/1/0/all/0/1&quot;&gt;Bhawani Sandeep&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Surjeet Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sumit Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02561">
<title>LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and Generative Fusion. (arXiv:2306.02561v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02561</link>
<description rdf:parseType="Literal">&lt;p&gt;We present LLM-Blender, an ensembling framework designed to attain
consistently superior performance by leveraging the diverse strengths of
multiple open-source large language models (LLMs). Our framework consists of
two modules: PairRanker and GenFuser, addressing the observation that optimal
LLMs for different examples can significantly vary. PairRanker employs a
specialized pairwise comparison method to distinguish subtle differences
between candidate outputs. It jointly encodes the input text and a pair of
candidates, using cross-attention encoders to determine the superior one. Our
results demonstrate that PairRanker exhibits the highest correlation with
ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,
generating an improved output by capitalizing on their strengths and mitigating
their weaknesses. To facilitate large-scale evaluation, we introduce a
benchmark dataset, MixInstruct, which is a mixture of multiple instruction
datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly
outperform individual LLMs and baseline methods across various metrics,
establishing a substantial performance gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_D/0/1/0/all/0/1&quot;&gt;Dongfu Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_B/0/1/0/all/0/1&quot;&gt;Bill Yuchen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02739">
<title>Knowledge-Driven Robot Program Synthesis from Human VR Demonstrations. (arXiv:2306.02739v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02739</link>
<description rdf:parseType="Literal">&lt;p&gt;Aging societies, labor shortages and increasing wage costs call for
assistance robots capable of autonomously performing a wide array of real-world
tasks. Such open-ended robotic manipulation requires not only powerful
knowledge representations and reasoning (KR&amp;amp;R) algorithms, but also methods for
humans to instruct robots what tasks to perform and how to perform them. In
this paper, we present a system for automatically generating executable robot
control programs from human task demonstrations in virtual reality (VR). We
leverage common-sense knowledge and game engine-based physics to semantically
interpret human VR demonstrations, as well as an expressive and general task
representation and automatic path planning and code generation, embedded into a
state-of-the-art cognitive architecture. We demonstrate our approach in the
context of force-sensitive fetch-and-place for a robotic shopping assistant.
The source code is available at
https://github.com/ease-crc/vr-program-synthesis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alt_B/0/1/0/all/0/1&quot;&gt;Benjamin Alt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenfack_F/0/1/0/all/0/1&quot;&gt;Franklin Kenghagho Kenfack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haidu_A/0/1/0/all/0/1&quot;&gt;Andrei Haidu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katic_D/0/1/0/all/0/1&quot;&gt;Darko Katic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jakel_R/0/1/0/all/0/1&quot;&gt;Rainer J&amp;#xe4;kel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beetz_M/0/1/0/all/0/1&quot;&gt;Michael Beetz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.04376">
<title>Label Shift Quantification with Robustness Guarantees via Distribution Feature Matching. (arXiv:2306.04376v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2306.04376</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantification learning deals with the task of estimating the target label
distribution under label shift. In this paper, we first present a unifying
framework, distribution feature matching (DFM), that recovers as particular
instances various estimators introduced in previous literature. We derive a
general performance bound for DFM procedures, improving in several key aspects
upon previous bounds derived in particular cases. We then extend this analysis
to study robustness of DFM procedures in the misspecified setting under
departure from the exact label shift hypothesis, in particular in the case of
contamination of the target by an unknown distribution. These theoretical
findings are confirmed by a detailed numerical study on simulated and
real-world datasets. We also introduce an efficient, scalable and robust
version of kernel-based DFM using the Random Fourier Feature principle.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Dussap_B/0/1/0/all/0/1&quot;&gt;Bastien Dussap&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Blanchard_G/0/1/0/all/0/1&quot;&gt;Gilles Blanchard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Cherief_Abdellatif_B/0/1/0/all/0/1&quot;&gt;Badr-Eddine Ch&amp;#xe9;rief-Abdellatif&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05480">
<title>Artificial General Intelligence for Medical Imaging. (arXiv:2306.05480v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05480</link>
<description rdf:parseType="Literal">&lt;p&gt;In this review, we explore the potential applications of Artificial General
Intelligence (AGI) models in healthcare, focusing on foundational Large
Language Models (LLMs), Large Vision Models, and Large Multimodal Models. We
emphasize the importance of integrating clinical expertise, domain knowledge,
and multimodal capabilities into AGI models. In addition, we lay out key
roadmaps that guide the development and deployment of healthcare AGI models.
Throughout the review, we provide critical perspectives on the potential
challenges and pitfalls associated with deploying large-scale AGI models in the
medical field. This comprehensive review aims to offer insights into the future
implications of AGI in medical imaging, healthcare and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zihao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhengliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yixuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Gang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dajiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Pingkun Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Quanzheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_D/0/1/0/all/0/1&quot;&gt;Dinggang Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07743">
<title>V-LoL: A Diagnostic Dataset for Visual Logical Learning. (arXiv:2306.07743v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07743</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the successes of recent developments in visual AI, different
shortcomings still exist; from missing exact logical reasoning, to abstract
generalization abilities, to understanding complex and noisy scenes.
Unfortunately, existing benchmarks, were not designed to capture more than a
few of these aspects. Whereas deep learning datasets focus on visually complex
data but simple visual reasoning tasks, inductive logic datasets involve
complex logical learning tasks, however, lack the visual component. To address
this, we propose the visual logical learning dataset, V-LoL, that seamlessly
combines visual and logical challenges. Notably, we introduce the first
instantiation of V-LoL, V-LoL-Trains, -- a visual rendition of a classic
benchmark in symbolic AI, the Michalski train problem. By incorporating
intricate visual scenes and flexible logical reasoning tasks within a versatile
framework, V-LoL-Trains provides a platform for investigating a wide range of
visual logical learning challenges. We evaluate a variety of AI systems
including traditional symbolic AI, neural AI, as well as neuro-symbolic AI. Our
evaluations demonstrate that even state-of-the-art AI faces difficulties in
dealing with visual logical learning challenges, highlighting unique advantages
and limitations specific to each methodology. Overall, V-LoL opens up new
avenues for understanding and enhancing current abilities in visual logical
learning for AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helff_L/0/1/0/all/0/1&quot;&gt;Lukas Helff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stammer_W/0/1/0/all/0/1&quot;&gt;Wolfgang Stammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shindo_H/0/1/0/all/0/1&quot;&gt;Hikaru Shindo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhami_D/0/1/0/all/0/1&quot;&gt;Devendra Singh Dhami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07812">
<title>Automated 3D Pre-Training for Molecular Property Prediction. (arXiv:2306.07812v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07812</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular property prediction is an important problem in drug discovery and
materials science. As geometric structures have been demonstrated necessary for
molecular property prediction, 3D information has been combined with various
graph learning methods to boost prediction performance. However, obtaining the
geometric structure of molecules is not feasible in many real-world
applications due to the high computational cost. In this work, we propose a
novel 3D pre-training framework (dubbed 3D PGT), which pre-trains a model on 3D
molecular graphs, and then fine-tunes it on molecular graphs without 3D
structures. Based on fact that bond length, bond angle, and dihedral angle are
three basic geometric descriptors corresponding to a complete molecular 3D
conformer, we first develop a multi-task generative pre-train framework based
on these three attributes. Next, to automatically fuse these three generative
tasks, we design a surrogate metric using the \textit{total energy} to search
for weight distribution of the three pretext task since total energy
corresponding to the quality of 3D conformer.Extensive experiments on 2D
molecular graphs are conducted to demonstrate the accuracy, efficiency and
generalization ability of the proposed 3D PGT compared to various pre-training
baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Huan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tu_W/0/1/0/all/0/1&quot;&gt;Weiwei Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Yao_Q/0/1/0/all/0/1&quot;&gt;Quanming Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07874">
<title>Taxonomy-Structured Domain Adaptation. (arXiv:2306.07874v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07874</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain adaptation aims to mitigate distribution shifts among different
domains. However, traditional formulations are mostly limited to categorical
domains, greatly simplifying nuanced domain relationships in the real world. In
this work, we tackle a generalization with taxonomy-structured domains, which
formalizes domains with nested, hierarchical similarity structures such as
animal species and product catalogs. We build on the classic adversarial
framework and introduce a novel taxonomist, which competes with the adversarial
discriminator to preserve the taxonomy information. The equilibrium recovers
the classic adversarial domain adaptation&apos;s solution if given a non-informative
domain taxonomy (e.g., a flat taxonomy where all leaf nodes connect to the root
node) while yielding non-trivial results with other taxonomies. Empirically,
our method achieves state-of-the-art performance on both synthetic and
real-world datasets with successful adaptation. Code is available at
https://github.com/Wang-ML-Lab/TSDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zihao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Hao He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_G/0/1/0/all/0/1&quot;&gt;Guang-Yuan Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Guang-He Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10509">
<title>Can We Trust AI-Generated Educational Content? Comparative Analysis of Human and AI-Generated Learning Resources. (arXiv:2306.10509v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10509</link>
<description rdf:parseType="Literal">&lt;p&gt;As an increasing number of students move to online learning platforms that
deliver personalized learning experiences, there is a great need for the
production of high-quality educational content. Large language models (LLMs)
appear to offer a promising solution to the rapid creation of learning
materials at scale, reducing the burden on instructors. In this study, we
investigated the potential for LLMs to produce learning resources in an
introductory programming context, by comparing the quality of the resources
generated by an LLM with those created by students as part of a learnersourcing
activity. Using a blind evaluation, students rated the correctness and
helpfulness of resources generated by AI and their peers, after both were
initially provided with identical exemplars. Our results show that the quality
of AI-generated resources, as perceived by students, is equivalent to the
quality of resources generated by their peers. This suggests that AI-generated
resources may serve as viable supplementary material in certain contexts.
Resources generated by LLMs tend to closely mirror the given exemplars, whereas
student-generated resources exhibit greater variety in terms of content length
and specific syntax features used. The study highlights the need for further
research exploring different types of learning resources and a broader range of
subject areas, and understanding the long-term impact of AI-generated resources
on learning outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denny_P/0/1/0/all/0/1&quot;&gt;Paul Denny&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khosravi_H/0/1/0/all/0/1&quot;&gt;Hassan Khosravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hellas_A/0/1/0/all/0/1&quot;&gt;Arto Hellas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leinonen_J/0/1/0/all/0/1&quot;&gt;Juho Leinonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarsa_S/0/1/0/all/0/1&quot;&gt;Sami Sarsa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10640">
<title>Evolving Strategies for Competitive Multi-Agent Search. (arXiv:2306.10640v2 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10640</link>
<description rdf:parseType="Literal">&lt;p&gt;While evolutionary computation is well suited for automatic discovery in
engineering, it can also be used to gain insight into how humans and
organizations could perform more effectively. Using a real-world problem of
innovation search in organizations as the motivating example, this article
first formalizes human creative problem solving as competitive multi-agent
search (CMAS). CMAS is different from existing single-agent and team search
problems in that the agents interact through knowledge of other agents&apos;
searches and through the dynamic changes in the search landscape that result
from these searches. The main hypothesis is that evolutionary computation can
be used to discover effective strategies for CMAS; this hypothesis is verified
in a series of experiments on the NK model, i.e.\ partially correlated and
tunably rugged fitness landscapes. Different specialized strategies are evolved
for each different competitive environment, and also general strategies that
perform well across environments. These strategies are more effective and more
complex than hand-designed strategies and a strategy based on traditional tree
search. Using a novel spherical visualization of such landscapes, insight is
gained about how successful strategies work, e.g.\ by tracking positive changes
in the landscape. The article thus provides a possible framework for studying
various human creative activities as competitive multi-agent search in the
future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bahceci_E/0/1/0/all/0/1&quot;&gt;Erkin Bahceci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katila_R/0/1/0/all/0/1&quot;&gt;Riitta Katila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10756">
<title>A HRNet-based Rehabilitation Monitoring System. (arXiv:2306.10756v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10756</link>
<description rdf:parseType="Literal">&lt;p&gt;The rehabilitation treatment helps to heal minor sports and occupational
injuries. In a traditional rehabilitation process, a therapist will assign
certain actions to a patient to perform in between hospital visits, and it will
rely on the patient to remember actions correctly and the schedule to perform
them. Unfortunately, many patients forget to perform actions or fail to recall
actions in detail. As a consequence, the rehabilitation treatment is hampered
or, in the worst case, the patient may suffer from additional injury caused by
performing incorrect actions. To resolve these issues, we propose a HRNet-based
rehabilitation monitoring system, which can remind a patient when to perform
the actions and display the actions for the patient to follow via the patient&apos;s
smartphone. In addition, it helps the therapist to monitor the progress of the
rehabilitation for the patient. Our system consists of an iOS app and several
components at the server side. The app is in charge of displaying and
collecting action videos. The server computes the similarity score between the
therapist&apos;s actions and the patient&apos;s in the videos to keep track of the number
of repetitions of each action. Theses stats will be shown to both of the
patient and therapist. The extensive experiments show that the F1-Score of the
similarity calculation is as high as 0.9 and the soft accuracy of the number of
repetitions is higher than 90%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hung_Y/0/1/0/all/0/1&quot;&gt;Yi-Ching Hung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liou_F/0/1/0/all/0/1&quot;&gt;Fong-Syuan Liou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsao_Y/0/1/0/all/0/1&quot;&gt;Yu-Hsuan Tsao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_Z/0/1/0/all/0/1&quot;&gt;Zi-Cing Chiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;MIn-Te Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.10946">
<title>Att-KGCN: Tourist Attractions Recommendation System by using Attention mechanism and Knowledge Graph Convolution Network. (arXiv:2306.10946v4 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.10946</link>
<description rdf:parseType="Literal">&lt;p&gt;The recommendation algorithm based on knowledge graphs is at a relatively
mature stage. However, there are still some problems in the recommendation of
specific areas. For example, in the tourism field, selecting suitable tourist
attraction attributes process is complicated as the recommendation basis for
tourist attractions. In this paper, we propose the improved Attention Knowledge
Graph Convolution Network model, named ($Att-KGCN$), which automatically
discovers the neighboring entities of the target scenic spot semantically. The
attention layer aggregates relatively similar locations and represents them
with an adjacent vector. Then, according to the tourist&apos;s preferred choices,
the model predicts the probability of similar spots as a recommendation system.
A knowledge graph dataset of tourist attractions used based on tourism data on
Socotra Island-Yemen. Through experiments, it is verified that the Attention
Knowledge Graph Convolution Network has a good effect on the recommendation of
tourist attractions and can make more recommendations for tourists&apos; choices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mubarak_A/0/1/0/all/0/1&quot;&gt;Ahmad A. Mubarak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;JingJing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_H/0/1/0/all/0/1&quot;&gt;Han Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.11667">
<title>G-NM: A Group of Numerical Time Series Prediction Models. (arXiv:2306.11667v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.11667</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we focus on the development and implementation of a
comprehensive ensemble of numerical time series forecasting models,
collectively referred to as the Group of Numerical Time Series Prediction Model
(G-NM). This inclusive set comprises traditional models such as Autoregressive
Integrated Moving Average (ARIMA), Holt-Winters&apos; method, and Support Vector
Regression (SVR), in addition to modern neural network models including
Recurrent Neural Network (RNN) and Long Short-Term Memory (LSTM). G-NM is
explicitly constructed to augment our predictive capabilities related to
patterns and trends inherent in complex natural phenomena. By utilizing time
series data relevant to these events, G-NM facilitates the prediction of such
phenomena over extended periods. The primary objective of this research is to
both advance our understanding of such occurrences and to significantly enhance
the accuracy of our forecasts. G-NM encapsulates both linear and non-linear
dependencies, seasonalities, and trends present in time series data. Each of
these models contributes distinct strengths, from ARIMA&apos;s resilience in
handling linear trends and seasonality, SVR&apos;s proficiency in capturing
non-linear patterns, to LSTM&apos;s adaptability in modeling various components of
time series data. Through the exploitation of the G-NM potential, we strive to
advance the state-of-the-art in large-scale time series forecasting models. We
anticipate that this research will represent a significant stepping stone in
our ongoing endeavor to comprehend and forecast the complex events that
constitute the natural world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_J/0/1/0/all/0/1&quot;&gt;Juyoung Yun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.12729">
<title>MP3: Movement Primitive-Based (Re-)Planning Policy. (arXiv:2306.12729v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.12729</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a novel deep reinforcement learning (RL) approach called
Movement Primitive-based Planning Policy (MP3). By integrating movement
primitives (MPs) into the deep RL framework, MP3 enables the generation of
smooth trajectories throughout the whole learning process while effectively
learning from sparse and non-Markovian rewards. Additionally, MP3 maintains the
capability to adapt to changes in the environment during execution. Although
many early successes in robot RL have been achieved by combining RL with MPs,
these approaches are often limited to learning single stroke-based motions,
lacking the ability to adapt to task variations or adjust motions during
execution. Building upon our previous work, which introduced an episode-based
RL method for the non-linear adaptation of MP parameters to different task
variations, this paper extends the approach to incorporating replanning
strategies. This allows adaptation of the MP parameters throughout motion
execution, addressing the lack of online motion adaptation in stochastic
domains requiring feedback. We compared our approach against state-of-the-art
deep RL and RL with MPs methods. The results demonstrated improved performance
in sophisticated, sparse reward settings and in domains requiring replanning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Otto_F/0/1/0/all/0/1&quot;&gt;Fabian Otto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hongyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celik_O/0/1/0/all/0/1&quot;&gt;Onur Celik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lioutikov_R/0/1/0/all/0/1&quot;&gt;Rudolf Lioutikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neumann_G/0/1/0/all/0/1&quot;&gt;Gerhard Neumann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15156">
<title>Learning non-Markovian Decision-Making from State-only Sequences. (arXiv:2306.15156v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15156</link>
<description rdf:parseType="Literal">&lt;p&gt;Conventional imitation learning assumes access to the actions of
demonstrators, but these motor signals are often non-observable in naturalistic
settings. Additionally, sequential decision-making behaviors in these settings
can deviate from the assumptions of a standard Markov Decision Process (MDP).
To address these challenges, we explore deep generative modeling of state-only
sequences with non-Markov Decision Process (nMDP), where the policy is an
energy-based prior in the latent space of the state transition generator. We
develop maximum likelihood estimation to achieve model-based imitation, which
involves short-run MCMC sampling from the prior and importance sampling for the
posterior. The learned model enables \textit{decision-making as inference}:
model-free policy execution is equivalent to prior sampling, model-based
planning is posterior sampling initialized from the policy. We demonstrate the
efficacy of the proposed method in a prototypical path planning task with
non-Markovian constraints and show that the learned model exhibits strong
performances in challenging domains from the MuJoCo suite.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1&quot;&gt;Aoyang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_F/0/1/0/all/0/1&quot;&gt;Feng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Song-Chun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Sirui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15656">
<title>SparseOptimizer: Sparsify Language Models through Moreau-Yosida Regularization and Accelerate via Compiler Co-design. (arXiv:2306.15656v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15656</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces SparseOptimizer, a novel deep learning optimizer that
exploits Moreau-Yosida regularization to naturally induce sparsity in large
language models such as BERT, ALBERT and GPT. Key to the design of
SparseOptimizer is an embedded shrinkage operator, which imparts sparsity
directly within the optimization process. This operator, backed by a sound
theoretical framework, includes an analytical solution, thereby reinforcing the
optimizer&apos;s robustness and efficacy. Crucially, SparseOptimizer&apos;s plug-and-play
functionality eradicates the need for code modifications, making it a
universally adaptable tool for a wide array of large language models. Empirical
evaluations on benchmark datasets such as GLUE, RACE, SQuAD1, and SQuAD2
confirm that SparseBERT and SparseALBERT, when sparsified using
SparseOptimizer, achieve performance comparable to their dense counterparts,
BERT and ALBERT, while significantly reducing their parameter count. Further,
this work proposes an innovative optimizer-compiler co-design strategy,
demonstrating the potential of inference acceleration (\textbf{3.37x},
\textbf{6.30x}, and \textbf{7.15x} in comparison with Pytorch, TensorFlow, and
LLVM generic compile, respectively) in SparseBERT when paired with an
appropriately designed compiler. This study represents a significant step
forward in the evolution of efficient, scalable, and high-performing large
language models, setting a precedent for future exploration and optimization in
this domain. The SparseOptimizer code and SparseALBERT model will be publicly
available upon paper acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_F/0/1/0/all/0/1&quot;&gt;Fu-Ming Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15969">
<title>Separable Physics-Informed Neural Networks. (arXiv:2306.15969v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15969</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural networks (PINNs) have recently emerged as promising
data-driven PDE solvers showing encouraging results on various PDEs. However,
there is a fundamental limitation of training PINNs to solve multi-dimensional
PDEs and approximate highly complex solution functions. The number of training
points (collocation points) required on these challenging PDEs grows
substantially, but it is severely limited due to the expensive computational
costs and heavy memory overhead. To overcome this issue, we propose a network
architecture and training algorithm for PINNs. The proposed method, separable
PINN (SPINN), operates on a per-axis basis to significantly reduce the number
of network propagations in multi-dimensional PDEs unlike point-wise processing
in conventional PINNs. We also propose using forward-mode automatic
differentiation to reduce the computational cost of computing PDE residuals,
enabling a large number of collocation points (&amp;gt;10^7) on a single commodity
GPU. The experimental results show drastically reduced computational costs (62x
in wall-clock time, 1,394x in FLOPs given the same number of collocation
points) in multi-dimensional PDEs while achieving better accuracy. Furthermore,
we present that SPINN can solve a chaotic (2+1)-d Navier-Stokes equation
significantly faster than the best-performing prior method (9 minutes vs 10
hours in a single GPU), maintaining accuracy. Finally, we showcase that SPINN
can accurately obtain the solution of a highly nonlinear and multi-dimensional
PDE, a (3+1)-d Navier-Stokes equation. For visualized results and code, please
see https://jwcho5576.github.io/spinn.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_J/0/1/0/all/0/1&quot;&gt;Junwoo Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_S/0/1/0/all/0/1&quot;&gt;Seungtae Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hyunmo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Seok-Bae Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_Y/0/1/0/all/0/1&quot;&gt;Youngjoon Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_E/0/1/0/all/0/1&quot;&gt;Eunbyung Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16705">
<title>NNQS-Transformer: an Efficient and Scalable Neural Network Quantum States Approach for Ab initio Quantum Chemistry. (arXiv:2306.16705v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16705</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural network quantum state (NNQS) has emerged as a promising candidate for
quantum many-body problems, but its practical applications are often hindered
by the high cost of sampling and local energy calculation. We develop a
high-performance NNQS method for \textit{ab initio} electronic structure
calculations. The major innovations include: (1) A transformer based
architecture as the quantum wave function ansatz; (2) A data-centric
parallelization scheme for the variational Monte Carlo (VMC) algorithm which
preserves data locality and well adapts for different computing architectures;
(3) A parallel batch sampling strategy which reduces the sampling cost and
achieves good load balance; (4) A parallel local energy evaluation scheme which
is both memory and computationally efficient; (5) Study of real chemical
systems demonstrates both the superior accuracy of our method compared to
state-of-the-art and the strong and weak scalability for large molecular
systems with up to $120$ spin orbitals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yangjun Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fan_Y/0/1/0/all/0/1&quot;&gt;Yi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shang_H/0/1/0/all/0/1&quot;&gt;Honghui Shang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16736">
<title>GraMMaR: Ground-aware Motion Model for 3D Human Motion Reconstruction. (arXiv:2306.16736v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16736</link>
<description rdf:parseType="Literal">&lt;p&gt;Demystifying complex human-ground interactions is essential for accurate and
realistic 3D human motion reconstruction from RGB videos, as it ensures
consistency between the humans and the ground plane. Prior methods have modeled
human-ground interactions either implicitly or in a sparse manner, often
resulting in unrealistic and incorrect motions when faced with noise and
uncertainty. In contrast, our approach explicitly represents these interactions
in a dense and continuous manner. To this end, we propose a novel Ground-aware
Motion Model for 3D Human Motion Reconstruction, named GraMMaR, which jointly
learns the distribution of transitions in both pose and interaction between
every joint and ground plane at each time step of a motion sequence. It is
trained to explicitly promote consistency between the motion and distance
change towards the ground. After training, we establish a joint optimization
strategy that utilizes GraMMaR as a dual-prior, regularizing the optimization
towards the space of plausible ground-aware motions. This leads to realistic
and coherent motion reconstruction, irrespective of the assumed or learned
ground plane. Through extensive evaluation on the AMASS and AIST++ datasets,
our model demonstrates good generalization and discriminating abilities in
challenging cases including complex and ambiguous human-ground interactions.
The code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Sihan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qiong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16817">
<title>Improving Online Continual Learning Performance and Stability with Temporal Ensembles. (arXiv:2306.16817v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16817</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural networks are very effective when trained on large datasets for a large
number of iterations. However, when they are trained on non-stationary streams
of data and in an online fashion, their performance is reduced (1) by the
online setup, which limits the availability of data, (2) due to catastrophic
forgetting because of the non-stationary nature of the data. Furthermore,
several recent works (Caccia et al., 2022; Lange et al., 2023) &lt;a href=&quot;/abs/2205.13452&quot;&gt;arXiv:2205.13452&lt;/a&gt;
showed that replay methods used in continual learning suffer from the stability
gap, encountered when evaluating the model continually (rather than only on
task boundaries). In this article, we study the effect of model ensembling as a
way to improve performance and stability in online continual learning. We
notice that naively ensembling models coming from a variety of training tasks
increases the performance in online continual learning considerably. Starting
from this observation, and drawing inspirations from semi-supervised learning
ensembling methods, we use a lightweight temporal ensemble that computes the
exponential moving average of the weights (EMA) at test time, and show that it
can drastically increase the performance and stability when used in combination
with several methods from the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soutif__Cormerais_A/0/1/0/all/0/1&quot;&gt;Albin Soutif--Cormerais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carta_A/0/1/0/all/0/1&quot;&gt;Antonio Carta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weijer_J/0/1/0/all/0/1&quot;&gt;Joost Van de Weijer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17068">
<title>weighted CapsuleNet networks for Persian multi-domain sentiment analysis. (arXiv:2306.17068v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17068</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentiment classification is a fundamental task in natural language
processing, assigning one of the three classes, positive, negative, or neutral,
to free texts. However, sentiment classification models are highly domain
dependent; the classifier may perform classification with reasonable accuracy
in one domain but not in another due to the Semantic multiplicity of words
getting poor accuracy. This article presents a new Persian/Arabic multi-domain
sentiment analysis method using the cumulative weighted capsule networks
approach. Weighted capsule ensemble consists of training separate capsule
networks for each domain and a weighting measure called domain belonging degree
(DBD). This criterion consists of TF and IDF, which calculates the dependency
of each document for each domain separately; this value is multiplied by the
possible output that each capsule creates. In the end, the sum of these
multiplications is the title of the final output, and is used to determine the
polarity. And the most dependent domain is considered the final output for each
domain. The proposed method was evaluated using the Digikala dataset and
obtained acceptable accuracy compared to the existing approaches. It achieved
an accuracy of 0.89 on detecting the domain of belonging and 0.99 on detecting
the polarity. Also, for the problem of dealing with unbalanced classes, a
cost-sensitive function was used. This function was able to achieve 0.0162
improvements in accuracy for sentiment classification. This approach on Amazon
Arabic data can achieve 0.9695 accuracies in domain classification.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobari_M/0/1/0/all/0/1&quot;&gt;Mahboobeh Sadat Kobari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karimi_N/0/1/0/all/0/1&quot;&gt;Nima Karimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pourhosseini_B/0/1/0/all/0/1&quot;&gt;Benyamin Pourhosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mousa_R/0/1/0/all/0/1&quot;&gt;Ramin Mousa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17408">
<title>LMBot: Distilling Graph Knowledge into Language Model for Graph-less Deployment in Twitter Bot Detection. (arXiv:2306.17408v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17408</link>
<description rdf:parseType="Literal">&lt;p&gt;As malicious actors employ increasingly advanced and widespread bots to
disseminate misinformation and manipulate public opinion, the detection of
Twitter bots has become a crucial task. Though graph-based Twitter bot
detection methods achieve state-of-the-art performance, we find that their
inference depends on the neighbor users multi-hop away from the targets, and
fetching neighbors is time-consuming and may introduce bias. At the same time,
we find that after finetuning on Twitter bot detection, pretrained language
models achieve competitive performance and do not require a graph structure
during deployment. Inspired by this finding, we propose a novel bot detection
framework LMBot that distills the knowledge of graph neural networks (GNNs)
into language models (LMs) for graph-less deployment in Twitter bot detection
to combat the challenge of data dependency. Moreover, LMBot is compatible with
graph-based and graph-less datasets. Specifically, we first represent each user
as a textual sequence and feed them into the LM for domain adaptation. For
graph-based datasets, the output of LMs provides input features for the GNN,
enabling it to optimize for bot detection and distill knowledge back to the LM
in an iterative, mutually enhancing process. Armed with the LM, we can perform
graph-less inference, which resolves the graph data dependency and sampling
bias issues. For datasets without graph structure, we simply replace the GNN
with an MLP, which has also shown strong performance. Our experiments
demonstrate that LMBot achieves state-of-the-art performance on four Twitter
bot detection benchmarks. Extensive studies also show that LMBot is more
robust, versatile, and efficient compared to graph-based Twitter bot detection
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zijian Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxuan Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zifeng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongrui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Minnan Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17624">
<title>Sphere2Vec: A General-Purpose Location Representation Learning over a Spherical Surface for Large-Scale Geospatial Predictions. (arXiv:2306.17624v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17624</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating learning-friendly representations for points in space is a
fundamental and long-standing problem in ML. Recently, multi-scale encoding
schemes (such as Space2Vec and NeRF) were proposed to directly encode any point
in 2D/3D Euclidean space as a high-dimensional vector, and has been
successfully applied to various geospatial prediction and generative tasks.
However, all current 2D and 3D location encoders are designed to model point
distances in Euclidean space. So when applied to large-scale real-world GPS
coordinate datasets, which require distance metric learning on the spherical
surface, both types of models can fail due to the map projection distortion
problem (2D) and the spherical-to-Euclidean distance approximation error (3D).
To solve these problems, we propose a multi-scale location encoder called
Sphere2Vec which can preserve spherical distances when encoding point
coordinates on a spherical surface. We developed a unified view of
distance-reserving encoding on spheres based on the DFS. We also provide
theoretical proof that the Sphere2Vec preserves the spherical surface distance
between any two points, while existing encoding schemes do not. Experiments on
20 synthetic datasets show that Sphere2Vec can outperform all baseline models
on all these datasets with up to 30.8% error rate reduction. We then apply
Sphere2Vec to three geo-aware image classification tasks - fine-grained species
recognition, Flickr image recognition, and remote sensing image classification.
Results on 7 real-world datasets show the superiority of Sphere2Vec over
multiple location encoders on all three tasks. Further analysis shows that
Sphere2Vec outperforms other location encoder models, especially in the polar
regions and data-sparse areas because of its nature for spherical surface
distance preservation. Code and data are available at
https://gengchenmai.github.io/sphere2vec-website/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_G/0/1/0/all/0/1&quot;&gt;Gengchen Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_Y/0/1/0/all/0/1&quot;&gt;Yao Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wenyun Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yutong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jiaming Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ermon_S/0/1/0/all/0/1&quot;&gt;Stefano Ermon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janowicz_K/0/1/0/all/0/1&quot;&gt;Krzysztof Janowicz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lao_N/0/1/0/all/0/1&quot;&gt;Ni Lao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.09325">
<title>TAX-Pose: Task-Specific Cross-Pose Estimation for Robot Manipulation. (arXiv:2211.09325v2 [cs.RO] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2211.09325</link>
<description rdf:parseType="Literal">&lt;p&gt;How do we imbue robots with the ability to efficiently manipulate unseen
objects and transfer relevant skills based on demonstrations? End-to-end
learning methods often fail to generalize to novel objects or unseen
configurations. Instead, we focus on the task-specific pose relationship
between relevant parts of interacting objects. We conjecture that this
relationship is a generalizable notion of a manipulation task that can transfer
to new objects in the same category; examples include the relationship between
the pose of a pan relative to an oven or the pose of a mug relative to a mug
rack. We call this task-specific pose relationship &quot;cross-pose&quot; and provide a
mathematical definition of this concept. We propose a vision-based system that
learns to estimate the cross-pose between two objects for a given manipulation
task using learned cross-object correspondences. The estimated cross-pose is
then used to guide a downstream motion planner to manipulate the objects into
the desired pose relationship (placing a pan into the oven or the mug onto the
mug rack). We demonstrate our method&apos;s capability to generalize to unseen
objects, in some cases after training on only 10 demonstrations in the real
world. Results show that our system achieves state-of-the-art performance in
both simulated and real-world experiments across a number of tasks.
Supplementary information and videos can be found at
https://sites.google.com/view/tax-pose/home.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_C/0/1/0/all/0/1&quot;&gt;Chuer Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Okorn_B/0/1/0/all/0/1&quot;&gt;Brian Okorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Harry Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eisner_B/0/1/0/all/0/1&quot;&gt;Ben Eisner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Held_D/0/1/0/all/0/1&quot;&gt;David Held&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.00068">
<title>Wearing face mask detection using deep learning through COVID-19 pandemic. (arXiv:2305.00068v1 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2305.00068</link>
<description rdf:parseType="Literal">&lt;p&gt;During the COVID-19 pandemic, wearing a face mask has been known to be an
effective way to prevent the spread of COVID-19. In lots of monitoring tasks,
humans have been replaced with computers thanks to the outstanding performance
of the deep learning models. Monitoring the wearing of a face mask is another
task that can be done by deep learning models with acceptable accuracy. The
main challenge of this task is the limited amount of data because of the
quarantine. In this paper, we did an investigation on the capability of three
state-of-the-art object detection neural networks on face mask detection for
real-time applications. As mentioned, here are three models used, Single Shot
Detector (SSD), two versions of You Only Look Once (YOLO) i.e., YOLOv4-tiny,
and YOLOv4-tiny-3l from which the best was selected. In the proposed method,
according to the performance of different models, the best model that can be
suitable for use in real-world and mobile device applications in comparison to
other recent studies was the YOLOv4-tiny model, with 85.31% and 50.66 for mean
Average Precision (mAP) and Frames Per Second (FPS), respectively. These
acceptable values were achieved using two datasets with only 1531 images in
three separate classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khoramdel_J/0/1/0/all/0/1&quot;&gt;Javad Khoramdel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatami_S/0/1/0/all/0/1&quot;&gt;Soheila Hatami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadedel_M/0/1/0/all/0/1&quot;&gt;Majid Sadedel&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>