<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-26T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13765" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13770" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13850" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13851" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13901" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13908" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13925" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13927" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13953" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13957" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13958" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13978" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13985" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13991" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14009" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14016" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14025" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14030" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14052" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14058" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14063" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14071" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14127" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14142" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14177" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14187" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14273" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14278" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14288" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14294" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14331" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14334" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14336" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2008.07073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.05423" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.09559" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.01482" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.04317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02070" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.13803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05783" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.08544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01226" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.12247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.04068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.08870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09630" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.05511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.06710" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.08000" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.11990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18120" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01188" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.01415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02115" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.17723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02100" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04192" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04956" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06281" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.12917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10711" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.13698">
<title>Exploring the Lottery Ticket Hypothesis with Explainability Methods: Insights into Sparse Network Performance. (arXiv:2307.13698v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13698</link>
<description rdf:parseType="Literal">&lt;p&gt;Discovering a high-performing sparse network within a massive neural network
is advantageous for deploying them on devices with limited storage, such as
mobile phones. Additionally, model explainability is essential to fostering
trust in AI. The Lottery Ticket Hypothesis (LTH) finds a network within a deep
network with comparable or superior performance to the original model. However,
limited study has been conducted on the success or failure of LTH in terms of
explainability. In this work, we examine why the performance of the pruned
networks gradually increases or decreases. Using Grad-CAM and Post-hoc concept
bottleneck models (PCBMs), respectively, we investigate the explainability of
pruned networks in terms of pixels and high-level concepts. We perform
extensive experiments across vision and medical imaging datasets. As more
weights are pruned, the performance of the network degrades. The discovered
concepts and pixels from the pruned networks are inconsistent with the original
network -- a possible reason for the drop in performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_S/0/1/0/all/0/1&quot;&gt;Shantanu Ghosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Batmanghelich_K/0/1/0/all/0/1&quot;&gt;Kayhan Batmanghelich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13717">
<title>A Comprehensive Analysis on the Leakage of Fuzzy Matchers. (arXiv:2307.13717v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.13717</link>
<description rdf:parseType="Literal">&lt;p&gt;The present paper presents a comprehensive analysis of potential information
leakage in distance evaluation, with a specific emphasis on threshold-based
obfuscated distance (i.e. Fuzzy Matcher). It includes detailed descriptions of
various situations related to potential information leakage and specific
attention is given to their consequences on security. Generic attacks
corresponding to each scenario are outlined, and their complexities are
assessed. The main contribution of this work lies in providing an upper bound
on the security of a fuzzy matcher in scenarios where there is additional
information leakage from the matcher, providing a straightforward understanding
of the maximum level of achievable security and its potential implications for
data privacy and security.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Durbet_A/0/1/0/all/0/1&quot;&gt;Axel Durbet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grollemund_P/0/1/0/all/0/1&quot;&gt;Paul-Marie Grollemund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiry_Atighehchi_K/0/1/0/all/0/1&quot;&gt;Kevin Thiry-Atighehchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13720">
<title>Composite Diffusion | whole &gt;= \Sigma parts. (arXiv:2307.13720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13720</link>
<description rdf:parseType="Literal">&lt;p&gt;For an artist or a graphic designer, the spatial layout of a scene is a
critical design choice. However, existing text-to-image diffusion models
provide limited support for incorporating spatial information. This paper
introduces Composite Diffusion as a means for artists to generate high-quality
images by composing from the sub-scenes. The artists can specify the
arrangement of these sub-scenes through a flexible free-form segment layout.
They can describe the content of each sub-scene primarily using natural text
and additionally by utilizing reference images or control inputs such as line
art, scribbles, human pose, canny edges, and more.
&lt;/p&gt;
&lt;p&gt;We provide a comprehensive and modular method for Composite Diffusion that
enables alternative ways of generating, composing, and harmonizing sub-scenes.
Further, we wish to evaluate the composite image for effectiveness in both
image quality and achieving the artist&apos;s intent. We argue that existing image
quality metrics lack a holistic evaluation of image composites. To address
this, we propose novel quality criteria especially relevant to composite
generation.
&lt;/p&gt;
&lt;p&gt;We believe that our approach provides an intuitive method of art creation.
Through extensive user surveys, quantitative and qualitative analysis, we show
how it achieves greater spatial, semantic, and creative control over image
generation. In addition, our methods do not need to retrain or modify the
architecture of the base diffusion models and can work in a plug-and-play
manner with the fine-tuned models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jamwal_V/0/1/0/all/0/1&quot;&gt;Vikram Jamwal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+S_R/0/1/0/all/0/1&quot;&gt;Ramaneswaran S&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13721">
<title>Foundational Models Defining a New Era in Vision: A Survey and Outlook. (arXiv:2307.13721v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13721</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision systems to see and reason about the compositional nature of visual
scenes are fundamental to understanding our world. The complex relations
between objects and their locations, ambiguities, and variations in the
real-world environment can be better described in human language, naturally
governed by grammatical rules and other modalities such as audio and depth. The
models learned to bridge the gap between such modalities coupled with
large-scale training data facilitate contextual reasoning, generalization, and
prompt capabilities at test time. These models are referred to as foundational
models. The output of such models can be modified through human-provided
prompts without retraining, e.g., segmenting a particular object by providing a
bounding box, having interactive dialogues by asking questions about an image
or video scene or manipulating the robot&apos;s behavior through language
instructions. In this survey, we provide a comprehensive review of such
emerging foundational models, including typical architecture designs to combine
different modalities (vision, text, audio, etc), training objectives
(contrastive, generative), pre-training datasets, fine-tuning mechanisms, and
the common prompting patterns; textual, visual, and heterogeneous. We discuss
the open challenges and research directions for foundational models in computer
vision, including difficulties in their evaluations and benchmarking, gaps in
their real-world understanding, limitations of their contextual understanding,
biases, vulnerability to adversarial attacks, and interpretability issues. We
review recent developments in this field, covering a wide range of applications
of foundation models systematically and comprehensively. A comprehensive list
of foundational models studied in this work is available at
\url{https://github.com/awaisrauf/Awesome-CV-Foundational-Models}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Awais_M/0/1/0/all/0/1&quot;&gt;Muhammad Awais&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naseer_M/0/1/0/all/0/1&quot;&gt;Muzammal Naseer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anwer_R/0/1/0/all/0/1&quot;&gt;Rao Muhammad Anwer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cholakkal_H/0/1/0/all/0/1&quot;&gt;Hisham Cholakkal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13746">
<title>ChildGAN: Large Scale Synthetic Child Facial Data Using Domain Adaptation in StyleGAN. (arXiv:2307.13746v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13746</link>
<description rdf:parseType="Literal">&lt;p&gt;In this research work, we proposed a novel ChildGAN, a pair of GAN networks
for generating synthetic boys and girls facial data derived from StyleGAN2.
ChildGAN is built by performing smooth domain transfer using transfer learning.
It provides photo-realistic, high-quality data samples. A large-scale dataset
is rendered with a variety of smart facial transformations: facial expressions,
age progression, eye blink effects, head pose, skin and hair color variations,
and variable lighting conditions. The dataset comprises more than 300k distinct
data samples. Further, the uniqueness and characteristics of the rendered
facial features are validated by running different computer vision application
tests which include CNN-based child gender classifier, face localization and
facial landmarks detection test, identity similarity evaluation using ArcFace,
and lastly running eye detection and eye aspect ratio tests. The results
demonstrate that synthetic child facial data of high quality offers an
alternative to the cost and complexity of collecting a large-scale dataset from
real children.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farooq_M/0/1/0/all/0/1&quot;&gt;Muhammad Ali Farooq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_W/0/1/0/all/0/1&quot;&gt;Wang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costache_G/0/1/0/all/0/1&quot;&gt;Gabriel Costache&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corcoran_P/0/1/0/all/0/1&quot;&gt;Peter Corcoran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13755">
<title>TMR-RD: Training-based Model Refinement and Representation Disagreement for Semi-Supervised Object Detection. (arXiv:2307.13755v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13755</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised object detection (SSOD) can incorporate limited labeled data
and large amounts of unlabeled data to improve the performance and
generalization of existing object detectors. Despite many advances, recent SSOD
methods are still challenged by noisy/misleading pseudo-labels, classical
exponential moving average (EMA) strategy, and the consensus of Teacher-Student
models in the latter stages of training. This paper proposes a novel
training-based model refinement (TMR) stage and a simple yet effective
representation disagreement (RD) strategy to address the limitations of
classical EMA and the consensus problem. The TMR stage of Teacher-Student
models optimizes the lightweight scaling operation to refine the model&apos;s
weights and prevent overfitting or forgetting learned patterns from unlabeled
data. Meanwhile, the RD strategy helps keep these models diverged to encourage
the student model to explore complementary representations. In addition, we use
cascade regression to generate more reliable pseudo-labels for supervising the
student model. Extensive experiments demonstrate the superior performance of
our approach over state-of-the-art SSOD methods. Specifically, the proposed
approach outperforms the Unbiased-Teacher method by an average mAP margin of
4.6% and 5.3% when using partially-labeled and fully-labeled data on the
MS-COCO dataset, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1&quot;&gt;Nilanjan Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1&quot;&gt;Nadir Erbilgin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13756">
<title>PlaneRecTR: Unified Query learning for 3D Plane Recovery from a Single View. (arXiv:2307.13756v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13756</link>
<description rdf:parseType="Literal">&lt;p&gt;3D plane recovery from a single image can usually be divided into several
subtasks of plane detection, segmentation, parameter estimation and possibly
depth estimation. Previous works tend to solve this task by either extending
the RCNN-based segmentation network or the dense pixel embedding-based
clustering framework. However, none of them tried to integrate above related
subtasks into a unified framework but treat them separately and sequentially,
which we suspect is potentially a main source of performance limitation for
existing approaches. Motivated by this finding and the success of query-based
learning in enriching reasoning among semantic entities, in this paper, we
propose PlaneRecTR, a Transformer-based architecture, which for the first time
unifies all subtasks related to single-view plane recovery with a single
compact model. Extensive quantitative and qualitative experiments demonstrate
that our proposed unified learning achieves mutual benefits across subtasks,
obtaining a new state-of-the-art performance on public ScanNet and NYUv2-Plane
datasets. Codes are available at https://github.com/SJingjia/PlaneRecTR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jingjia Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_S/0/1/0/all/0/1&quot;&gt;Shuaifeng Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13762">
<title>Implementing and Benchmarking the Locally Competitive Algorithm on the Loihi 2 Neuromorphic Processor. (arXiv:2307.13762v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13762</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic processors have garnered considerable interest in recent years
for their potential in energy-efficient and high-speed computing. The Locally
Competitive Algorithm (LCA) has been utilized for power efficient sparse coding
on neuromorphic processors, including the first Loihi processor. With the Loihi
2 processor enabling custom neuron models and graded spike communication, more
complex implementations of LCA are possible. We present a new implementation of
LCA designed for the Loihi 2 processor and perform an initial set of benchmarks
comparing it to LCA on CPU and GPU devices. In these experiments LCA on Loihi 2
is orders of magnitude more efficient and faster for large sparsity penalties,
while maintaining similar reconstruction quality. We find this performance
improvement increases as the LCA parameters are tuned towards greater
representation sparsity.
&lt;/p&gt;
&lt;p&gt;Our study highlights the potential of neuromorphic processors, particularly
Loihi 2, in enabling intelligent, autonomous, real-time processing on small
robots, satellites where there are strict SWaP (small, lightweight, and low
power) requirements. By demonstrating the superior performance of LCA on Loihi
2 compared to conventional computing device, our study suggests that Loihi 2
could be a valuable tool in advancing these types of applications. Overall, our
study highlights the potential of neuromorphic processors for efficient and
accurate data processing on resource-constrained devices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parpart_G/0/1/0/all/0/1&quot;&gt;Gavin Parpart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Risbud_S/0/1/0/all/0/1&quot;&gt;Sumedh R. Risbud&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kenyon_G/0/1/0/all/0/1&quot;&gt;Garrett T. Kenyon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watkins_Y/0/1/0/all/0/1&quot;&gt;Yijing Watkins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13765">
<title>A real-time material breakage detection for offshore wind turbines based on improved neural network algorithm. (arXiv:2307.13765v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13765</link>
<description rdf:parseType="Literal">&lt;p&gt;The integrity of offshore wind turbines, pivotal for sustainable energy
generation, is often compromised by surface material defects. Despite the
availability of various detection techniques, limitations persist regarding
cost-effectiveness, efficiency, and applicability. Addressing these
shortcomings, this study introduces a novel approach leveraging an advanced
version of the YOLOv8 object detection model, supplemented with a Convolutional
Block Attention Module (CBAM) for improved feature recognition. The optimized
loss function further refines the learning process. Employing a dataset of
5,432 images from the Saemangeum offshore wind farm and a publicly available
dataset, our method underwent rigorous testing. The findings reveal a
substantial enhancement in defect detection stability, marking a significant
stride towards efficient turbine maintenance. This study&apos;s contributions
illuminate the path for future research, potentially revolutionizing
sustainable energy practices.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yantong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13770">
<title>E^2VPT: An Effective and Efficient Approach for Visual Prompt Tuning. (arXiv:2307.13770v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13770</link>
<description rdf:parseType="Literal">&lt;p&gt;As the size of transformer-based models continues to grow, fine-tuning these
large-scale pretrained vision models for new tasks has become increasingly
parameter-intensive. Parameter-efficient learning has been developed to reduce
the number of tunable parameters during fine-tuning. Although these methods
show promising results, there is still a significant performance gap compared
to full fine-tuning. To address this challenge, we propose an Effective and
Efficient Visual Prompt Tuning (E^2VPT) approach for large-scale
transformer-based model adaptation. Specifically, we introduce a set of
learnable key-value prompts and visual prompts into self-attention and input
layers, respectively, to improve the effectiveness of model fine-tuning.
Moreover, we design a prompt pruning procedure to systematically prune low
importance prompts while preserving model performance, which largely enhances
the model&apos;s efficiency. Empirical results demonstrate that our approach
outperforms several state-of-the-art baselines on two benchmarks, with
considerably low parameter usage (e.g., 0.32% of model parameters on VTAB-1k).
Our code is available at https://github.com/ChengHan111/E2VPT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Cheng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yiming Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zhiwen Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_S/0/1/0/all/0/1&quot;&gt;Siyuan Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dongfang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13842">
<title>CosSIF: Cosine similarity-based image filtering to overcome low inter-class variation in synthetic medical image datasets. (arXiv:2307.13842v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13842</link>
<description rdf:parseType="Literal">&lt;p&gt;Crafting effective deep learning models for medical image analysis is a
complex task, particularly in cases where the medical image dataset lacks
significant inter-class variation. This challenge is further aggravated when
employing such datasets to generate synthetic images using generative
adversarial networks (GANs), as the output of GANs heavily relies on the input
data. In this research, we propose a novel filtering algorithm called Cosine
Similarity-based Image Filtering (CosSIF). We leverage CosSIF to develop two
distinct filtering methods: Filtering Before GAN Training (FBGT) and Filtering
After GAN Training (FAGT). FBGT involves the removal of real images that
exhibit similarities to images of other classes before utilizing them as the
training dataset for a GAN. On the other hand, FAGT focuses on eliminating
synthetic images with less discriminative features compared to real images used
for training the GAN. Experimental results reveal that employing either the
FAGT or FBGT method with modern transformer and convolutional-based networks
leads to substantial performance gains in various evaluation metrics. FAGT
implementation on the ISIC-2016 dataset surpasses the baseline method in terms
of sensitivity by 1.59\% and AUC by 1.88\%. Furthermore, for the HAM10000
dataset, applying FABT outperforms the baseline approach in terms of recall by
13.75\%, and with the sole implementation of FAGT, achieves a maximum accuracy
of 94.44\%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Mominul Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zunair_H/0/1/0/all/0/1&quot;&gt;Hasib Zunair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammed_N/0/1/0/all/0/1&quot;&gt;Nabeel Mohammed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13850">
<title>MAEA: Multimodal Attribution for Embodied AI. (arXiv:2307.13850v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13850</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding multimodal perception for embodied AI is an open question
because such inputs may contain highly complementary as well as redundant
information for the task. A relevant direction for multimodal policies is
understanding the global trends of each modality at the fusion layer. To this
end, we disentangle the attributions for visual, language, and previous action
inputs across different policies trained on the ALFRED dataset. Attribution
analysis can be utilized to rank and group the failure scenarios, investigate
modeling and dataset biases, and critically analyze multimodal EAI policies for
robustness and user trust before deployment. We present MAEA, a framework to
compute global attributions per modality of any differentiable policy. In
addition, we show how attributions enable lower-level behavior analysis in EAI
policies for language and visual attributions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_V/0/1/0/all/0/1&quot;&gt;Vidhi Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamarapalli_J/0/1/0/all/0/1&quot;&gt;Jayant Sravan Tamarapalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yerramilli_S/0/1/0/all/0/1&quot;&gt;Sahiti Yerramilli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bisk_Y/0/1/0/all/0/1&quot;&gt;Yonatan Bisk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13851">
<title>SplitFed resilience to packet loss: Where to split, that is the question. (arXiv:2307.13851v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13851</link>
<description rdf:parseType="Literal">&lt;p&gt;Decentralized machine learning has broadened its scope recently with the
invention of Federated Learning (FL), Split Learning (SL), and their hybrids
like Split Federated Learning (SplitFed or SFL). The goal of SFL is to reduce
the computational power required by each client in FL and parallelize SL while
maintaining privacy. This paper investigates the robustness of SFL against
packet loss on communication links. The performance of various SFL aggregation
strategies is examined by splitting the model at two points -- shallow split
and deep split -- and testing whether the split point makes a statistically
significant difference to the accuracy of the final model. Experiments are
carried out on a segmentation model for human embryo images and indicate the
statistically significant advantage of a deeper split point.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiranthika_C/0/1/0/all/0/1&quot;&gt;Chamani Shiranthika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kafshgari_Z/0/1/0/all/0/1&quot;&gt;Zahra Hafezi Kafshgari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeedi_P/0/1/0/all/0/1&quot;&gt;Parvaneh Saeedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajic_I/0/1/0/all/0/1&quot;&gt;Ivan V. Baji&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13855">
<title>Exploring the Sharpened Cosine Similarity. (arXiv:2307.13855v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13855</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional layers have long served as the primary workhorse for image
classification. Recently, an alternative to convolution was proposed using the
Sharpened Cosine Similarity (SCS), which in theory may serve as a better
feature detector. While multiple sources report promising results, there has
not been to date a full-scale empirical analysis of neural network performance
using these new layers. In our work, we explore SCS&apos;s parameter behavior and
potential as a drop-in replacement for convolutions in multiple CNN
architectures benchmarked on CIFAR-10. We find that while SCS may not yield
significant increases in accuracy, it may learn more interpretable
representations. We also find that, in some circumstances, SCS may confer a
slight increase in adversarial robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Skyler Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_F/0/1/0/all/0/1&quot;&gt;Fred Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raff_E/0/1/0/all/0/1&quot;&gt;Edward Raff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Holt_J/0/1/0/all/0/1&quot;&gt;James Holt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13856">
<title>On the unreasonable vulnerability of transformers for image restoration -- and an easy fix. (arXiv:2307.13856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13856</link>
<description rdf:parseType="Literal">&lt;p&gt;Following their success in visual recognition tasks, Vision
Transformers(ViTs) are being increasingly employed for image restoration. As a
few recent works claim that ViTs for image classification also have better
robustness properties, we investigate whether the improved adversarial
robustness of ViTs extends to image restoration. We consider the recently
proposed Restormer model, as well as NAFNet and the &quot;Baseline network&quot; which
are both simplified versions of a Restormer. We use Projected Gradient Descent
(PGD) and CosPGD, a recently proposed adversarial attack tailored to pixel-wise
prediction tasks for our robustness evaluation. Our experiments are performed
on real-world images from the GoPro dataset for image deblurring. Our analysis
indicates that contrary to as advocated by ViTs in image classification works,
these models are highly susceptible to adversarial attacks. We attempt to
improve their robustness through adversarial training. While this yields a
significant increase in robustness for Restormer, results on other networks are
less promising. Interestingly, the design choices in NAFNet and Baselines,
which were based on iid performance, and not on robust generalization, seem to
be at odds with the model robustness. Thus, we investigate this further and
find a fix.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnihotri_S/0/1/0/all/0/1&quot;&gt;Shashank Agnihotri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandikota_K/0/1/0/all/0/1&quot;&gt;Kanchana Vaishnavi Gandikota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabinski_J/0/1/0/all/0/1&quot;&gt;Julia Grabinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandramouli_P/0/1/0/all/0/1&quot;&gt;Paramanand Chandramouli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13865">
<title>Pretrained Deep 2.5D Models for Efficient Predictive Modeling from Retinal OCT. (arXiv:2307.13865v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13865</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of medical imaging, 3D deep learning models play a crucial role
in building powerful predictive models of disease progression. However, the
size of these models presents significant challenges, both in terms of
computational resources and data requirements. Moreover, achieving high-quality
pretraining of 3D models proves to be even more challenging. To address these
issues, hybrid 2.5D approaches provide an effective solution for utilizing 3D
volumetric data efficiently using 2D models. Combining 2D and 3D techniques
offers a promising avenue for optimizing performance while minimizing memory
requirements. In this paper, we explore 2.5D architectures based on a
combination of convolutional neural networks (CNNs), long short-term memory
(LSTM), and Transformers. In addition, leveraging the benefits of recent
non-contrastive pretraining approaches in 2D, we enhanced the performance and
data efficiency of 2.5D techniques even further. We demonstrate the
effectiveness of architectures and associated pretraining on a task of
predicting progression to wet age-related macular degeneration (AMD) within a
six-month period on two large longitudinal OCT datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Emre_T/0/1/0/all/0/1&quot;&gt;Taha Emre&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oghbaie_M/0/1/0/all/0/1&quot;&gt;Marzieh Oghbaie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakravarty_A/0/1/0/all/0/1&quot;&gt;Arunava Chakravarty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivail_A/0/1/0/all/0/1&quot;&gt;Antoine Rivail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riedl_S/0/1/0/all/0/1&quot;&gt;Sophie Riedl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_J/0/1/0/all/0/1&quot;&gt;Julia Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholl_H/0/1/0/all/0/1&quot;&gt;Hendrik P.N. Scholl&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivaprasad_S/0/1/0/all/0/1&quot;&gt;Sobha Sivaprasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lotery_A/0/1/0/all/0/1&quot;&gt;Andrew Lotery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schmidt_Erfurth_U/0/1/0/all/0/1&quot;&gt;Ursula Schmidt-Erfurth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogunovic_H/0/1/0/all/0/1&quot;&gt;Hrvoje Bogunovi&amp;#x107;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13897">
<title>AViT: Adapting Vision Transformers for Small Skin Lesion Segmentation Datasets. (arXiv:2307.13897v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13897</link>
<description rdf:parseType="Literal">&lt;p&gt;Skin lesion segmentation (SLS) plays an important role in skin lesion
analysis. Vision transformers (ViTs) are considered an auspicious solution for
SLS, but they require more training data compared to convolutional neural
networks (CNNs) due to their inherent parameter-heavy structure and lack of
some inductive biases. To alleviate this issue, current approaches fine-tune
pre-trained ViT backbones on SLS datasets, aiming to leverage the knowledge
learned from a larger set of natural images to lower the amount of skin
training data needed. However, fully fine-tuning all parameters of large
backbones is computationally expensive and memory intensive. In this paper, we
propose AViT, a novel efficient strategy to mitigate ViTs&apos; data-hunger by
transferring any pre-trained ViTs to the SLS task. Specifically, we integrate
lightweight modules (adapters) within the transformer layers, which modulate
the feature representation of a ViT without updating its pre-trained weights.
In addition, we employ a shallow CNN as a prompt generator to create a prompt
embedding from the input image, which grasps fine-grained information and CNN&apos;s
inductive biases to guide the segmentation task on small datasets. Our
quantitative experiments on 4 skin lesion datasets demonstrate that AViT
achieves competitive, and at times superior, performance to SOTA but with
significantly fewer trainable parameters. Our code is available at
https://github.com/siyi-wind/AViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Siyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayasi_N/0/1/0/all/0/1&quot;&gt;Nourhan Bayasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harmarneh_G/0/1/0/all/0/1&quot;&gt;Ghassan Harmarneh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garbi_R/0/1/0/all/0/1&quot;&gt;Rafeef Garbi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13899">
<title>Regularizing Neural Networks with Meta-Learning Generative Models. (arXiv:2307.13899v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13899</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates methods for improving generative data augmentation
for deep learning. Generative data augmentation leverages the synthetic samples
produced by generative models as an additional dataset for classification with
small dataset settings. A key challenge of generative data augmentation is that
the synthetic data contain uninformative samples that degrade accuracy. This is
because the synthetic samples do not perfectly represent class categories in
real data and uniform sampling does not necessarily provide useful samples for
tasks. In this paper, we present a novel strategy for generative data
augmentation called meta generative regularization (MGR). To avoid the
degradation of generative data augmentation, MGR utilizes synthetic samples in
the regularization term for feature extractors instead of in the loss function,
e.g., cross-entropy. These synthetic samples are dynamically determined to
minimize the validation losses through meta-learning. We observed that MGR can
avoid the performance degradation of na\&quot;ive generative data augmentation and
boost the baselines. Experiments on six datasets showed that MGR is effective
particularly when datasets are smaller and stably outperforms baselines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yamaguchi_S/0/1/0/all/0/1&quot;&gt;Shin&amp;#x27;ya Yamaguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chijiwa_D/0/1/0/all/0/1&quot;&gt;Daiki Chijiwa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanai_S/0/1/0/all/0/1&quot;&gt;Sekitoshi Kanai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumagai_A/0/1/0/all/0/1&quot;&gt;Atsutoshi Kumagai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kashima_H/0/1/0/all/0/1&quot;&gt;Hisashi Kashima&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13901">
<title>YOLOBench: Benchmarking Efficient Object Detectors on Embedded Systems. (arXiv:2307.13901v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13901</link>
<description rdf:parseType="Literal">&lt;p&gt;We present YOLOBench, a benchmark comprised of 550+ YOLO-based object
detection models on 4 different datasets and 4 different embedded hardware
platforms (x86 CPU, ARM CPU, Nvidia GPU, NPU). We collect accuracy and latency
numbers for a variety of YOLO-based one-stage detectors at different model
scales by performing a fair, controlled comparison of these detectors with a
fixed training environment (code and training hyperparameters).
Pareto-optimality analysis of the collected data reveals that, if modern
detection heads and training techniques are incorporated into the learning
process, multiple architectures of the YOLO series achieve a good
accuracy-latency trade-off, including older models like YOLOv3 and YOLOv4. We
also evaluate training-free accuracy estimators used in neural architecture
search on YOLOBench and demonstrate that, while most state-of-the-art zero-cost
accuracy estimators are outperformed by a simple baseline like MAC count, some
of them can be effectively used to predict Pareto-optimal detection models. We
showcase that by using a zero-cost proxy to identify a YOLO architecture
competitive against a state-of-the-art YOLOv8 model on a Raspberry Pi 4 CPU.
The code and data are available at
https://github.com/Deeplite/deeplite-torch-zoo
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazarevich_I/0/1/0/all/0/1&quot;&gt;Ivan Lazarevich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grimaldi_M/0/1/0/all/0/1&quot;&gt;Matteo Grimaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_R/0/1/0/all/0/1&quot;&gt;Ravish Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_S/0/1/0/all/0/1&quot;&gt;Saptarshi Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Shahrukh Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sah_S/0/1/0/all/0/1&quot;&gt;Sudhakar Sah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13908">
<title>Points-to-3D: Bridging the Gap between Sparse Points and Shape-Controllable Text-to-3D Generation. (arXiv:2307.13908v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13908</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-3D generation has recently garnered significant attention, fueled by
2D diffusion models trained on billions of image-text pairs. Existing methods
primarily rely on score distillation to leverage the 2D diffusion priors to
supervise the generation of 3D models, e.g., NeRF. However, score distillation
is prone to suffer the view inconsistency problem, and implicit NeRF modeling
can also lead to an arbitrary shape, thus leading to less realistic and
uncontrollable 3D generation. In this work, we propose a flexible framework of
Points-to-3D to bridge the gap between sparse yet freely available 3D points
and realistic shape-controllable 3D generation by distilling the knowledge from
both 2D and 3D diffusion models. The core idea of Points-to-3D is to introduce
controllable sparse 3D points to guide the text-to-3D generation. Specifically,
we use the sparse point cloud generated from the 3D diffusion model, Point-E,
as the geometric prior, conditioned on a single reference image. To better
utilize the sparse 3D points, we propose an efficient point cloud guidance loss
to adaptively drive the NeRF&apos;s geometry to align with the shape of the sparse
3D points. In addition to controlling the geometry, we propose to optimize the
NeRF for a more view-consistent appearance. To be specific, we perform score
distillation to the publicly available 2D image diffusion model ControlNet,
conditioned on text as well as depth map of the learned compact geometry.
Qualitative and quantitative comparisons demonstrate that Points-to-3D improves
view consistency and achieves good shape controllability for text-to-3D
generation. Points-to-3D provides users with a new way to improve and control
text-to-3D generation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_C/0/1/0/all/0/1&quot;&gt;Chaohui Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qiang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhe Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13924">
<title>trajdata: A Unified Interface to Multiple Human Trajectory Datasets. (arXiv:2307.13924v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13924</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of trajectory forecasting has grown significantly in recent years,
partially owing to the release of numerous large-scale, real-world human
trajectory datasets for autonomous vehicles (AVs) and pedestrian motion
tracking. While such datasets have been a boon for the community, they each use
custom and unique data formats and APIs, making it cumbersome for researchers
to train and evaluate methods across multiple datasets. To remedy this, we
present trajdata: a unified interface to multiple human trajectory datasets. At
its core, trajdata provides a simple, uniform, and efficient representation and
API for trajectory and map data. As a demonstration of its capabilities, in
this work we conduct a comprehensive empirical evaluation of existing
trajectory datasets, providing users with a rich understanding of the data
underpinning much of current pedestrian and AV motion forecasting research, and
proposing suggestions for future datasets from these insights. trajdata is
permissively licensed (Apache 2.0) and can be accessed online at
https://github.com/NVlabs/trajdata
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ivanovic_B/0/1/0/all/0/1&quot;&gt;Boris Ivanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guanyu Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilitschenski_I/0/1/0/all/0/1&quot;&gt;Igor Gilitschenski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavone_M/0/1/0/all/0/1&quot;&gt;Marco Pavone&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13925">
<title>EasyNet: An Easy Network for 3D Industrial Anomaly Detection. (arXiv:2307.13925v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13925</link>
<description rdf:parseType="Literal">&lt;p&gt;3D anomaly detection is an emerging and vital computer vision task in
industrial manufacturing (IM). Recently many advanced algorithms have been
published, but most of them cannot meet the needs of IM. There are several
disadvantages: i) difficult to deploy on production lines since their
algorithms heavily rely on large pre-trained models; ii) hugely increase
storage overhead due to overuse of memory banks; iii) the inference speed
cannot be achieved in real-time. To overcome these issues, we propose an easy
and deployment-friendly network (called EasyNet) without using pre-trained
models and memory banks: firstly, we design a multi-scale multi-modality
feature encoder-decoder to accurately reconstruct the segmentation maps of
anomalous regions and encourage the interaction between RGB images and depth
images; secondly, we adopt a multi-modality anomaly segmentation network to
achieve a precise anomaly map; thirdly, we propose an attention-based
information entropy fusion module for feature fusion during inference, making
it suitable for real-time deployment. Extensive experiments show that EasyNet
achieves an anomaly detection AUROC of 92.6% without using pre-trained models
and memory banks. In addition, EasyNet is faster than existing methods, with a
high frame rate of 94.55 FPS on a Tesla V100 GPU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Ziqi Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13927">
<title>DFR-Net: Density Feature Refinement Network for Image Dehazing Utilizing Haze Density Difference. (arXiv:2307.13927v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13927</link>
<description rdf:parseType="Literal">&lt;p&gt;In image dehazing task, haze density is a key feature and affects the
performance of dehazing methods. However, some of the existing methods lack a
comparative image to measure densities, and others create intermediate results
but lack the exploitation of their density differences, which can facilitate
perception of density. To address these deficiencies, we propose a
density-aware dehazing method named Density Feature Refinement Network
(DFR-Net) that extracts haze density features from density differences and
leverages density differences to refine density features. In DFR-Net, we first
generate a proposal image that has lower overall density than the hazy input,
bringing in global density differences. Additionally, the dehazing residual of
the proposal image reflects the level of dehazing performance and provides
local density differences that indicate localized hard dehazing or high density
areas. Subsequently, we introduce a Global Branch (GB) and a Local Branch (LB)
to achieve density-awareness. In GB, we use Siamese networks for feature
extraction of hazy inputs and proposal images, and we propose a Global Density
Feature Refinement (GDFR) module that can refine features by pushing features
with different global densities further away. In LB, we explore local density
features from the dehazing residuals between hazy inputs and proposal images
and introduce an Intermediate Dehazing Residual Feedforward (IDRF) module to
update local features and pull them closer to clear image features. Sufficient
experiments demonstrate that the proposed method achieves results beyond the
state-of-the-art methods on various datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongze Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haitao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lujian Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_J/0/1/0/all/0/1&quot;&gt;Jingchao Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kaijie Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13929">
<title>Spatio-Temporal Domain Awareness for Multi-Agent Collaborative Perception. (arXiv:2307.13929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13929</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent collaborative perception as a potential application for
vehicle-to-everything communication could significantly improve the perception
performance of autonomous vehicles over single-agent perception. However,
several challenges remain in achieving pragmatic information sharing in this
emerging research. In this paper, we propose SCOPE, a novel collaborative
perception framework that aggregates the spatio-temporal awareness
characteristics across on-road agents in an end-to-end manner. Specifically,
SCOPE has three distinct strengths: i) it considers effective semantic cues of
the temporal context to enhance current representations of the target agent;
ii) it aggregates perceptually critical spatial information from heterogeneous
agents and overcomes localization errors via multi-scale feature interactions;
iii) it integrates multi-source representations of the target agent based on
their complementary contributions by an adaptive fusion paradigm. To thoroughly
evaluate SCOPE, we consider both real-world and simulated scenarios of
collaborative 3D object detection tasks on three datasets. Extensive
experiments demonstrate the superiority of our approach and the necessity of
the proposed components.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_P/0/1/0/all/0/1&quot;&gt;Peng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Liang Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13933">
<title>AIDE: A Vision-Driven Multi-View, Multi-Modal, Multi-Tasking Dataset for Assistive Driving Perception. (arXiv:2307.13933v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13933</link>
<description rdf:parseType="Literal">&lt;p&gt;Driver distraction has become a significant cause of severe traffic accidents
over the past decade. Despite the growing development of vision-driven driver
monitoring systems, the lack of comprehensive perception datasets restricts
road safety and traffic security. In this paper, we present an AssIstive
Driving pErception dataset (AIDE) that considers context information both
inside and outside the vehicle in naturalistic scenarios. AIDE facilitates
holistic driver monitoring through three distinctive characteristics, including
multi-view settings of driver and scene, multi-modal annotations of face, body,
posture, and gesture, and four pragmatic task designs for driving
understanding. To thoroughly explore AIDE, we provide experimental benchmarks
on three kinds of baseline frameworks via extensive methods. Moreover, two
fusion strategies are introduced to give new insights into learning effective
multi-stream/modal representations. We also systematically investigate the
importance and rationality of the key components in AIDE and benchmarks. The
project link is https://github.com/ydk122024/AIDE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dingkang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_S/0/1/0/all/0/1&quot;&gt;Shuai Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenpeng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shunli Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuzheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhaoyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peixuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_P/0/1/0/all/0/1&quot;&gt;Peng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lihua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13938">
<title>Improving Semi-Supervised Semantic Segmentation with Dual-Level Siamese Structure Network. (arXiv:2307.13938v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13938</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-supervised semantic segmentation (SSS) is an important task that
utilizes both labeled and unlabeled data to reduce expenses on labeling
training examples. However, the effectiveness of SSS algorithms is limited by
the difficulty of fully exploiting the potential of unlabeled data. To address
this, we propose a dual-level Siamese structure network (DSSN) for pixel-wise
contrastive learning. By aligning positive pairs with a pixel-wise contrastive
loss using strong augmented views in both low-level image space and high-level
feature space, the proposed DSSN is designed to maximize the utilization of
available unlabeled data. Additionally, we introduce a novel class-aware
pseudo-label selection strategy for weak-to-strong supervision, which addresses
the limitations of most existing methods that do not perform selection or apply
a predefined threshold for all classes. Specifically, our strategy selects the
top high-confidence prediction of the weak view for each class to generate
pseudo labels that supervise the strong augmented views. This strategy is
capable of taking into account the class imbalance and improving the
performance of long-tailed classes. Our proposed method achieves
state-of-the-art results on two datasets, PASCAL VOC 2012 and Cityscapes,
outperforming other SSS algorithms by a significant margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tain_Z/0/1/0/all/0/1&quot;&gt;Zhibo Tain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaolin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_K/0/1/0/all/0/1&quot;&gt;Kun Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13947">
<title>Centroid-aware feature recalibration for cancer grading in pathology images. (arXiv:2307.13947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13947</link>
<description rdf:parseType="Literal">&lt;p&gt;Cancer grading is an essential task in pathology. The recent developments of
artificial neural networks in computational pathology have shown that these
methods hold great potential for improving the accuracy and quality of cancer
diagnosis. However, the issues with the robustness and reliability of such
methods have not been fully resolved yet. Herein, we propose a centroid-aware
feature recalibration network that can conduct cancer grading in an accurate
and robust manner. The proposed network maps an input pathology image into an
embedding space and adjusts it by using centroids embedding vectors of
different cancer grades via attention mechanism. Equipped with the recalibrated
embedding vector, the proposed network classifiers the input pathology image
into a pertinent class label, i.e., cancer grade. We evaluate the proposed
network using colorectal cancer datasets that were collected under different
environments. The experimental results confirm that the proposed network is
able to conduct cancer grading in pathology images with high accuracy
regardless of the environmental changes in the datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byeon_K/0/1/0/all/0/1&quot;&gt;Keunho Byeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_J/0/1/0/all/0/1&quot;&gt;Jin Tae Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13948">
<title>Rethinking Voice-Face Correlation: A Geometry View. (arXiv:2307.13948v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13948</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous works on voice-face matching and voice-guided face synthesis
demonstrate strong correlations between voice and face, but mainly rely on
coarse semantic cues such as gender, age, and emotion. In this paper, we aim to
investigate the capability of reconstructing the 3D facial shape from voice
from a geometry perspective without any semantic information. We propose a
voice-anthropometric measurement (AM)-face paradigm, which identifies
predictable facial AMs from the voice and uses them to guide 3D face
reconstruction. By leveraging AMs as a proxy to link the voice and face
geometry, we can eliminate the influence of unpredictable AMs and make the face
geometry tractable. Our approach is evaluated on our proposed dataset with
ground-truth 3D face scans and corresponding voice recordings, and we find
significant correlations between voice and specific parts of the face geometry,
such as the nasal cavity and cranium. Our work offers a new perspective on
voice-face correlation and can serve as a good empirical study for
anthropometry science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yandong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Muqiao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinglu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13953">
<title>The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features. (arXiv:2307.13953v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13953</link>
<description rdf:parseType="Literal">&lt;p&gt;This work unveils the enigmatic link between phonemes and facial features.
Traditional studies on voice-face correlations typically involve using a long
period of voice input, including generating face images from voices and
reconstructing 3D face meshes from voices. However, in situations like
voice-based crimes, the available voice evidence may be short and limited.
Additionally, from a physiological perspective, each segment of speech --
phoneme -- corresponds to different types of airflow and movements in the face.
Therefore, it is advantageous to discover the hidden link between phonemes and
face attributes. In this paper, we propose an analysis pipeline to help us
explore the voice-face relationship in a fine-grained manner, i.e., phonemes
v.s. facial anthropometric measurements (AM). We build an estimator for each
phoneme-AM pair and evaluate the correlation through hypothesis testing. Our
results indicate that AMs are more predictable from vowels compared to
consonants, particularly with plosives. Additionally, we observe that if a
specific AM exhibits more movement during phoneme pronunciation, it is more
predictable. Our findings support those in physiology regarding correlation and
lay the groundwork for future research on speech-face multimodal learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_L/0/1/0/all/0/1&quot;&gt;Liao Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_X/0/1/0/all/0/1&quot;&gt;Xianwei Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_Y/0/1/0/all/0/1&quot;&gt;Yandong Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_R/0/1/0/all/0/1&quot;&gt;Rita Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raj_B/0/1/0/all/0/1&quot;&gt;Bhiksha Raj&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13957">
<title>Heterogeneous Embodied Multi-Agent Collaboration. (arXiv:2307.13957v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13957</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent embodied tasks have recently been studied in complex indoor
visual environments. Collaboration among multiple agents can improve work
efficiency and has significant practical value. However, most of the existing
research focuses on homogeneous multi-agent tasks. Compared with homogeneous
agents, heterogeneous agents can leverage their different capabilities to
allocate corresponding sub-tasks and cooperate to complete complex tasks.
Heterogeneous multi-agent tasks are common in real-world scenarios, and the
collaboration strategy among heterogeneous agents is a challenging and
important problem to be solved. To study collaboration among heterogeneous
agents, we propose the heterogeneous multi-agent tidying-up task, in which
multiple heterogeneous agents with different capabilities collaborate with each
other to detect misplaced objects and place them in reasonable locations. This
is a demanding task since it requires agents to make the best use of their
different capabilities to conduct reasonable task planning and complete the
whole task. To solve this task, we build a heterogeneous multi-agent tidying-up
benchmark dataset in a large number of houses with multiple rooms based on
ProcTHOR-10K. We propose the hierarchical decision model based on misplaced
object detection, reasonable receptacle prediction, as well as the
handshake-based group communication mechanism. Extensive experiments are
conducted to demonstrate the effectiveness of the proposed model. The project&apos;s
website and videos of experiments can be found at https://hetercol.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinzhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_D/0/1/0/all/0/1&quot;&gt;Di Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Huaping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13958">
<title>Visual Prompt Flexible-Modal Face Anti-Spoofing. (arXiv:2307.13958v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13958</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, vision transformer based multimodal learning methods have been
proposed to improve the robustness of face anti-spoofing (FAS) systems.
However, multimodal face data collected from the real world is often imperfect
due to missing modalities from various imaging sensors. Recently,
flexible-modal FAS~\cite{yu2023flexible} has attracted more attention, which
aims to develop a unified multimodal FAS model using complete multimodal face
data but is insensitive to test-time missing modalities. In this paper, we
tackle one main challenge in flexible-modal FAS, i.e., when missing modality
occurs either during training or testing in real-world situations. Inspired by
the recent success of the prompt learning in language models, we propose
\textbf{V}isual \textbf{P}rompt flexible-modal \textbf{FAS} (VP-FAS), which
learns the modal-relevant prompts to adapt the frozen pre-trained foundation
model to downstream flexible-modal FAS task. Specifically, both vanilla visual
prompts and residual contextual prompts are plugged into multimodal
transformers to handle general missing-modality cases, while only requiring
less than 4\% learnable parameters compared to training the entire model.
Furthermore, missing-modality regularization is proposed to force models to
learn consistent multimodal feature embeddings when missing partial modalities.
Extensive experiments conducted on two multimodal FAS benchmark datasets
demonstrate the effectiveness of our VP-FAS framework that improves the
performance under various missing-modality cases while alleviating the
requirement of heavy model re-training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zitong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_R/0/1/0/all/0/1&quot;&gt;Rizhao Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Y/0/1/0/all/0/1&quot;&gt;Yawen Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Ajian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changsheng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13974">
<title>Tracking Anything in High Quality. (arXiv:2307.13974v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13974</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual object tracking is a fundamental video task in computer vision.
Recently, the notably increasing power of perception algorithms allows the
unification of single/multiobject and box/mask-based tracking. Among them, the
Segment Anything Model (SAM) attracts much attention. In this report, we
propose HQTrack, a framework for High Quality Tracking anything in videos.
HQTrack mainly consists of a video multi-object segmenter (VMOS) and a mask
refiner (MR). Given the object to be tracked in the initial frame of a video,
VMOS propagates the object masks to the current frame. The mask results at this
stage are not accurate enough since VMOS is trained on several closeset video
object segmentation (VOS) datasets, which has limited ability to generalize to
complex and corner scenes. To further improve the quality of tracking masks, a
pretrained MR model is employed to refine the tracking results. As a compelling
testament to the effectiveness of our paradigm, without employing any tricks
such as test-time data augmentations and model ensemble, HQTrack ranks the 2nd
place in the Visual Object Tracking and Segmentation (VOTS2023) challenge. Code
and models are available at https://github.com/jiawen-zhu/HQTrack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jiawen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Z/0/1/0/all/0/1&quot;&gt;Zeqi Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shijie Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Huchuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jun-Yan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_J/0/1/0/all/0/1&quot;&gt;Jin-Peng Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenyang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13978">
<title>Controlling the Latent Space of GANs through Reinforcement Learning: A Case Study on Task-based Image-to-Image Translation. (arXiv:2307.13978v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.13978</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GAN) have emerged as a formidable AI tool to
generate realistic outputs based on training datasets. However, the challenge
of exerting control over the generation process of GANs remains a significant
hurdle. In this paper, we propose a novel methodology to address this issue by
integrating a reinforcement learning (RL) agent with a latent-space GAN
(l-GAN), thereby facilitating the generation of desired outputs. More
specifically, we have developed an actor-critic RL agent with a meticulously
designed reward policy, enabling it to acquire proficiency in navigating the
latent space of the l-GAN and generating outputs based on specified tasks. To
substantiate the efficacy of our approach, we have conducted a series of
experiments employing the MNIST dataset, including arithmetic addition as an
illustrative task. The outcomes of these experiments serve to validate our
methodology. Our pioneering integration of an RL agent with a GAN model
represents a novel advancement, holding great potential for enhancing
generative networks in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbasian_M/0/1/0/all/0/1&quot;&gt;Mahyar Abbasian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajabzadeh_T/0/1/0/all/0/1&quot;&gt;Taha Rajabzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moradipari_A/0/1/0/all/0/1&quot;&gt;Ahmadreza Moradipari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aqajari_S/0/1/0/all/0/1&quot;&gt;Seyed Amir Hossein Aqajari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongsheng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahmani_A/0/1/0/all/0/1&quot;&gt;Amir Rahmani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13981">
<title>Analysis of Video Quality Datasets via Design of Minimalistic Video Quality Models. (arXiv:2307.13981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13981</link>
<description rdf:parseType="Literal">&lt;p&gt;Blind video quality assessment (BVQA) plays an indispensable role in
monitoring and improving the end-users&apos; viewing experience in various
real-world video-enabled media applications. As an experimental field, the
improvements of BVQA models have been measured primarily on a few human-rated
VQA datasets. Thus, it is crucial to gain a better understanding of existing
VQA datasets in order to properly evaluate the current progress in BVQA.
Towards this goal, we conduct a first-of-its-kind computational analysis of VQA
datasets via designing minimalistic BVQA models. By minimalistic, we restrict
our family of BVQA models to build only upon basic blocks: a video preprocessor
(for aggressive spatiotemporal downsampling), a spatial quality analyzer, an
optional temporal quality analyzer, and a quality regressor, all with the
simplest possible instantiations. By comparing the quality prediction
performance of different model variants on eight VQA datasets with realistic
distortions, we find that nearly all datasets suffer from the easy dataset
problem of varying severity, some of which even admit blind image quality
assessment (BIQA) solutions. We additionally justify our claims by contrasting
our model generalizability on these VQA datasets, and by ablating a dizzying
set of BVQA design choices related to the basic building blocks. Our results
cast doubt on the current progress in BVQA, and meanwhile shed light on good
practices of constructing next-generation VQA datasets and models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_W/0/1/0/all/0/1&quot;&gt;Wen Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Long Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Kede Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13985">
<title>Enhanced Security against Adversarial Examples Using a Random Ensemble of Encrypted Vision Transformer Models. (arXiv:2307.13985v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2307.13985</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are well known to be vulnerable to adversarial
examples (AEs). In addition, AEs have adversarial transferability, which means
AEs generated for a source model can fool another black-box model (target
model) with a non-trivial probability. In previous studies, it was confirmed
that the vision transformer (ViT) is more robust against the property of
adversarial transferability than convolutional neural network (CNN) models such
as ConvMixer, and moreover encrypted ViT is more robust than ViT without any
encryption. In this article, we propose a random ensemble of encrypted ViT
models to achieve much more robust models. In experiments, the proposed scheme
is verified to be more robust against not only black-box attacks but also
white-box ones than convention methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iijima_R/0/1/0/all/0/1&quot;&gt;Ryota Iijima&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanaka_M/0/1/0/all/0/1&quot;&gt;Miki Tanaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shiota_S/0/1/0/all/0/1&quot;&gt;Sayaka Shiota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kiya_H/0/1/0/all/0/1&quot;&gt;Hitoshi Kiya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13986">
<title>Hybrid Representation-Enhanced Sampling for Bayesian Active Learning in Musculoskeletal Segmentation of Lower Extremities. (arXiv:2307.13986v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.13986</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Obtaining manual annotations to train deep learning (DL) models for
auto-segmentation is often time-consuming. Uncertainty-based Bayesian active
learning (BAL) is a widely-adopted method to reduce annotation efforts. Based
on BAL, this study introduces a hybrid representation-enhanced sampling
strategy that integrates density and diversity criteria to save manual
annotation costs by efficiently selecting the most informative samples.
&lt;/p&gt;
&lt;p&gt;Methods: The experiments are performed on two lower extremity (LE) datasets
of MRI and CT images by a BAL framework based on Bayesian U-net. Our method
selects uncertain samples with high density and diversity for manual revision,
optimizing for maximal similarity to unlabeled instances and minimal similarity
to existing training data. We assess the accuracy and efficiency using Dice and
a proposed metric called reduced annotation cost (RAC), respectively. We
further evaluate the impact of various acquisition rules on BAL performance and
design an ablation study for effectiveness estimation.
&lt;/p&gt;
&lt;p&gt;Results: The proposed method showed superiority or non-inferiority to other
methods on both datasets across two acquisition rules, and quantitative results
reveal the pros and cons of the acquisition rules. Our ablation study in
volume-wise acquisition shows that the combination of density and diversity
criteria outperforms solely using either of them in musculoskeletal
segmentation.
&lt;/p&gt;
&lt;p&gt;Conclusion: Our sampling method is proven efficient in reducing annotation
costs in image segmentation tasks. The combination of the proposed method and
our BAL framework provides a semi-automatic way for efficient annotation of
medical image datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ganping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Otake_Y/0/1/0/all/0/1&quot;&gt;Yoshito Otake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Soufi_M/0/1/0/all/0/1&quot;&gt;Mazen Soufi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Taniguchi_M/0/1/0/all/0/1&quot;&gt;Masashi Taniguchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yagi_M/0/1/0/all/0/1&quot;&gt;Masahide Yagi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ichihashi_N/0/1/0/all/0/1&quot;&gt;Noriaki Ichihashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Uemura_K/0/1/0/all/0/1&quot;&gt;Keisuke Uemura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Takao_M/0/1/0/all/0/1&quot;&gt;Masaki Takao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sugano_N/0/1/0/all/0/1&quot;&gt;Nobuhiko Sugano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sato_Y/0/1/0/all/0/1&quot;&gt;Yoshinobu Sato&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13991">
<title>METAVerse: Meta-Learning Traversability Cost Map for Off-Road Navigation. (arXiv:2307.13991v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.13991</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous navigation in off-road conditions requires an accurate estimation
of terrain traversability. However, traversability estimation in unstructured
environments is subject to high uncertainty due to the variability of numerous
factors that influence vehicle-terrain interaction. Consequently, it is
challenging to obtain a generalizable model that can accurately predict
traversability in a variety of environments. This paper presents METAVerse, a
meta-learning framework for learning a global model that accurately and
reliably predicts terrain traversability across diverse environments. We train
the traversability prediction network to generate a dense and continuous-valued
cost map from a sparse LiDAR point cloud, leveraging vehicle-terrain
interaction feedback in a self-supervised manner. Meta-learning is utilized to
train a global model with driving data collected from multiple environments,
effectively minimizing estimation uncertainty. During deployment, online
adaptation is performed to rapidly adapt the network to the local environment
by exploiting recent interaction experiences. To conduct a comprehensive
evaluation, we collect driving data from various terrains and demonstrate that
our method can obtain a global model that minimizes uncertainty. Moreover, by
integrating our model with a model predictive controller, we demonstrate that
the reduced uncertainty results in safe and stable navigation in unstructured
and unknown terrains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Junwon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taekyung Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_S/0/1/0/all/0/1&quot;&gt;Seongyong Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_K/0/1/0/all/0/1&quot;&gt;Kiho Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13992">
<title>Causal reasoning in typical computer vision tasks. (arXiv:2307.13992v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.13992</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has revolutionized the field of artificial intelligence. Based
on the statistical correlations uncovered by deep learning-based methods,
computer vision technology has contributed to tremendous growth in areas such
as autonomous driving and robotics. Despite being the basis of deep learning,
such correlation is not stable and is susceptible to uncontrolled factors. In
the absence of the guidance of prior knowledge, statistical correlations can
easily turn into spurious correlations and cause confounders. As a result,
researchers are beginning to refine deep learning-based methods with causal
theory. Causal theory models the intrinsic causal structure unaffected by data
bias and is effective in avoiding spurious correlations. This paper aims to
comprehensively review the existing causal methods in typical vision and
vision-language tasks such as semantic segmentation, object detection, and
image captioning. The advantages of causality and the approaches for building
causal paradigms will be summarized. Future roadmaps are also proposed,
including facilitating the development of causal theory and its application in
other complex scenes and systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang/0/1/0/all/0/1&quot;&gt;Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kexuan/0/1/0/all/0/1&quot;&gt;Kexuan&lt;/a&gt;, Sun, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiyu/0/1/0/all/0/1&quot;&gt;Qiyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao/0/1/0/all/0/1&quot;&gt;Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaoqiang/0/1/0/all/0/1&quot;&gt;Chaoqiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang/0/1/0/all/0/1&quot;&gt;Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang/0/1/0/all/0/1&quot;&gt;Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14006">
<title>Learning Snippet-to-Motion Progression for Skeleton-based Human Motion Prediction. (arXiv:2307.14006v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14006</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing Graph Convolutional Networks to achieve human motion prediction
largely adopt a one-step scheme, which output the prediction straight from
history input, failing to exploit human motion patterns. We observe that human
motions have transitional patterns and can be split into snippets
representative of each transition. Each snippet can be reconstructed from its
starting and ending poses referred to as the transitional poses. We propose a
snippet-to-motion multi-stage framework that breaks motion prediction into
sub-tasks easier to accomplish. Each sub-task integrates three modules:
transitional pose prediction, snippet reconstruction, and snippet-to-motion
prediction. Specifically, we propose to first predict only the transitional
poses. Then we use them to reconstruct the corresponding snippets, obtaining a
close approximation to the true motion sequence. Finally we refine them to
produce the final prediction output. To implement the network, we propose a
novel unified graph modeling, which allows for direct and effective feature
propagation compared to existing approaches which rely on separate space-time
modeling. Extensive experiments on Human 3.6M, CMU Mocap and 3DPW datasets
verify the effectiveness of our method which achieves state-of-the-art
performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinshun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Q/0/1/0/all/0/1&quot;&gt;Qiongjie Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shen Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mengyuan Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14008">
<title>Adaptive Frequency Filters As Efficient Global Token Mixers. (arXiv:2307.14008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14008</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent vision transformers, large-kernel CNNs and MLPs have attained
remarkable successes in broad vision tasks thanks to their effective
information fusion in the global scope. However, their efficient deployments,
especially on mobile devices, still suffer from noteworthy challenges due to
the heavy computational costs of self-attention mechanisms, large kernels, or
fully connected layers. In this work, we apply conventional convolution theorem
to deep learning for addressing this and reveal that adaptive frequency filters
can serve as efficient global token mixers. With this insight, we propose
Adaptive Frequency Filtering (AFF) token mixer. This neural operator transfers
a latent representation to the frequency domain via a Fourier transform and
performs semantic-adaptive frequency filtering via an elementwise
multiplication, which mathematically equals to a token mixing operation in the
original latent space with a dynamic convolution kernel as large as the spatial
resolution of this latent representation. We take AFF token mixers as primary
neural operators to build a lightweight neural network, dubbed AFFNet.
Extensive experiments demonstrate the effectiveness of our proposed AFF token
mixer and show that AFFNet achieve superior accuracy and efficiency trade-offs
compared to other lightweight network designs on broad visual tasks, including
visual recognition and dense prediction tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhizheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_C/0/1/0/all/0/1&quot;&gt;Cuiling Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zha_Z/0/1/0/all/0/1&quot;&gt;Zheng-Jun Zha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_B/0/1/0/all/0/1&quot;&gt;Baining Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14009">
<title>Car-Studio: Learning Car Radiance Fields from Single-View and Endless In-the-wild Images. (arXiv:2307.14009v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14009</link>
<description rdf:parseType="Literal">&lt;p&gt;Compositional neural scene graph studies have shown that radiance fields can
be an efficient tool in an editable autonomous driving simulator. However,
previous studies learned within a sequence of autonomous driving datasets,
resulting in unsatisfactory blurring when rotating the car in the simulator. In
this letter, we propose a pipeline for learning unconstrained images and
building a dataset from processed images. To meet the requirements of the
simulator, which demands that the vehicle maintain clarity when the perspective
changes and that the contour remains sharp from the background to avoid
artifacts when editing, we design a radiation field of the vehicle, a crucial
part of the urban scene foreground. Through experiments, we demonstrate that
our model achieves competitive performance compared to baselines. Using the
datasets built from in-the-wild images, our method gradually presents a
controllable appearance editing function. We will release the dataset and code
on https://lty2226262.github.io/car-studio/ to facilitate further research in
the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_G/0/1/0/all/0/1&quot;&gt;Guyue Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14010">
<title>ESSAformer: Efficient Transformer for Hyperspectral Image Super-resolution. (arXiv:2307.14010v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14010</link>
<description rdf:parseType="Literal">&lt;p&gt;Single hyperspectral image super-resolution (single-HSI-SR) aims to restore a
high-resolution hyperspectral image from a low-resolution observation. However,
the prevailing CNN-based approaches have shown limitations in building
long-range dependencies and capturing interaction information between spectral
features. This results in inadequate utilization of spectral information and
artifacts after upsampling. To address this issue, we propose ESSAformer, an
ESSA attention-embedded Transformer network for single-HSI-SR with an iterative
refining structure. Specifically, we first introduce a robust and
spectral-friendly similarity metric, \ie, the spectral correlation coefficient
of the spectrum (SCC), to replace the original attention matrix and
incorporates inductive biases into the model to facilitate training. Built upon
it, we further utilize the kernelizable attention technique with theoretical
support to form a novel efficient SCC-kernel-based self-attention (ESSA) and
reduce attention computation to linear complexity. ESSA enlarges the receptive
field for features after upsampling without bringing much computation and
allows the model to effectively utilize spatial-spectral information from
different scales, resulting in the generation of more natural high-resolution
images. Without the need for pretraining on large-scale datasets, our
experiments demonstrate ESSA&apos;s effectiveness in both visual quality and
quantitative results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingjin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jie Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xinbo Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14016">
<title>RPG-Palm: Realistic Pseudo-data Generation for Palmprint Recognition. (arXiv:2307.14016v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14016</link>
<description rdf:parseType="Literal">&lt;p&gt;Palmprint recently shows great potential in recognition applications as it is
a privacy-friendly and stable biometric. However, the lack of large-scale
public palmprint datasets limits further research and development of palmprint
recognition. In this paper, we propose a novel realistic pseudo-palmprint
generation (RPG) model to synthesize palmprints with massive identities. We
first introduce a conditional modulation generator to improve the intra-class
diversity. Then an identity-aware loss is proposed to ensure identity
consistency against unpaired training. We further improve the B\&apos;ezier palm
creases generation strategy to guarantee identity independence. Extensive
experimental results demonstrate that synthetic pretraining significantly
boosts the recognition model performance. For example, our model improves the
state-of-the-art B\&apos;ezierPalm by more than $5\%$ and $14\%$ in terms of
TAR@FAR=1e-6 under the $1:1$ and $1:3$ Open-set protocol. When accessing only
$10\%$ of the real training data, our method still outperforms ArcFace with
$100\%$ real training data, indicating that we are closer to real-data-free
palmprint recognition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lei Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jianlong Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruixin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huaen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yingyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_S/0/1/0/all/0/1&quot;&gt;Shouhong Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_W/0/1/0/all/0/1&quot;&gt;Wei Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14019">
<title>One-Nearest Neighborhood Guides Inlier Estimation for Unsupervised Point Cloud Registration. (arXiv:2307.14019v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14019</link>
<description rdf:parseType="Literal">&lt;p&gt;The precision of unsupervised point cloud registration methods is typically
limited by the lack of reliable inlier estimation and self-supervised signal,
especially in partially overlapping scenarios. In this paper, we propose an
effective inlier estimation method for unsupervised point cloud registration by
capturing geometric structure consistency between the source point cloud and
its corresponding reference point cloud copy. Specifically, to obtain a high
quality reference point cloud copy, an One-Nearest Neighborhood (1-NN) point
cloud is generated by input point cloud. This facilitates matching map
construction and allows for integrating dual neighborhood matching scores of
1-NN point cloud and input point cloud to improve matching confidence.
Benefiting from the high quality reference copy, we argue that the neighborhood
graph formed by inlier and its neighborhood should have consistency between
source point cloud and its corresponding reference copy. Based on this
observation, we construct transformation-invariant geometric structure
representations and capture geometric structure consistency to score the inlier
confidence for estimated correspondences between source point cloud and its
reference copy. This strategy can simultaneously provide the reliable
self-supervised signal for model optimization. Finally, we further calculate
transformation estimation by the weighted SVD algorithm with the estimated
correspondences and corresponding inlier confidence. We train the proposed
model in an unsupervised manner, and extensive experiments on synthetic and
real-world datasets illustrate the effectiveness of the proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yongzhe Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_M/0/1/0/all/0/1&quot;&gt;Maoguo Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Q/0/1/0/all/0/1&quot;&gt;Qiguang Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_A/0/1/0/all/0/1&quot;&gt;A. K. Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14021">
<title>Retinotopy Inspired Brain Encoding Model and the All-for-One Training Recipe. (arXiv:2307.14021v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14021</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain encoding models aim to predict brain voxel-wise responses to stimuli
images, replicating brain signals captured by neuroimaging techniques. There is
a large volume of publicly available data, but training a comprehensive brain
encoding model is challenging. The main difficulties stem from a) diversity
within individual brain, with functional heterogeneous brain regions; b)
diversity of brains from different subjects, due to genetic and developmental
differences; c) diversity of imaging modalities and processing pipelines. We
use this diversity to our advantage by introducing the All-for-One training
recipe, which divides the challenging one-big-model problem into multiple small
models, with the small models aggregating the knowledge while preserving the
distinction between the different functional regions. Agnostic of the training
recipe, we use biological knowledge of the brain, specifically retinotopy, to
introduce inductive bias to learn a 3D brain-to-image mapping that ensures a)
each neuron knows which image regions and semantic levels to gather
information, and b) no neurons are left behind in the model.
&lt;/p&gt;
&lt;p&gt;We pre-trained a brain encoding model using over one million data points from
five public datasets spanning three imaging modalities. To the best of our
knowledge, this is the most comprehensive brain encoding model to the date. We
demonstrate the effectiveness of the pre-trained model as a drop-in replacement
for commonly used vision backbone models. Furthermore, we demonstrate the
application of the model to brain decoding. Code and the model checkpoint will
be made available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huzheng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jianbo Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gee_J/0/1/0/all/0/1&quot;&gt;James Gee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14025">
<title>Topologically-Regularized Multiple Instance Learning for Red Blood Cell Disease Classification. (arXiv:2307.14025v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14025</link>
<description rdf:parseType="Literal">&lt;p&gt;Diagnosing rare anemia disorders using microscopic images is challenging for
skilled specialists and machine-learning methods alike. Due to thousands of
disease-relevant cells in a single blood sample, this constitutes a complex
multiple-instance learning (MIL) problem. While the spatial neighborhood of red
blood cells is not meaningful per se, the topology, i.e., the geometry of blood
samples as a whole, contains informative features to remedy typical MIL issues,
such as vanishing gradients and overfitting when training on limited data. We
thus develop a topology-based approach that extracts multi-scale topological
features from bags of single red blood cell images. The topological features
are used to regularize the model, enforcing the preservation of characteristic
topological properties of the data. Applied to a dataset of 71 patients
suffering from rare anemia disorders with 521 microscopic images of red blood
cells, our experiments show that topological regularization is an effective
method that leads to more than 3% performance improvements for the automated
classification of rare anemia disorders based on single-cell images. This is
the first approach that uses topological properties for regularizing the MIL
process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazeminia_S/0/1/0/all/0/1&quot;&gt;Salome Kazeminia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadafi_A/0/1/0/all/0/1&quot;&gt;Ario Sadafi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makhro_A/0/1/0/all/0/1&quot;&gt;Asya Makhro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bogdanova_A/0/1/0/all/0/1&quot;&gt;Anna Bogdanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1&quot;&gt;Carsten Marr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rieck_B/0/1/0/all/0/1&quot;&gt;Bastian Rieck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14030">
<title>Consensus-Adaptive RANSAC. (arXiv:2307.14030v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14030</link>
<description rdf:parseType="Literal">&lt;p&gt;RANSAC and its variants are widely used for robust estimation, however, they
commonly follow a greedy approach to finding the highest scoring model while
ignoring other model hypotheses. In contrast, Iteratively Reweighted Least
Squares (IRLS) techniques gradually approach the model by iteratively updating
the weight of each correspondence based on the residuals from previous
iterations. Inspired by these methods, we propose a new RANSAC framework that
learns to explore the parameter space by considering the residuals seen so far
via a novel attention layer. The attention mechanism operates on a batch of
point-to-model residuals, and updates a per-point estimation state to take into
account the consensus found through a lightweight one-step transformer. This
rich state then guides the minimal sampling between iterations as well as the
model refinement. We evaluate the proposed approach on essential and
fundamental matrix estimation on a number of indoor and outdoor datasets. It
outperforms state-of-the-art estimators by a significant margin adding only a
small runtime overhead. Moreover, we demonstrate good generalization properties
of our trained model, indicating its effectiveness across different datasets
and tasks. The proposed attention mechanism and one-step transformer provide an
adaptive behavior that enhances the performance of RANSAC, making it a more
effective tool for robust estimation. Code is available at
https://github.com/cavalli1234/CA-RANSAC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavalli_L/0/1/0/all/0/1&quot;&gt;Luca Cavalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barath_D/0/1/0/all/0/1&quot;&gt;Daniel Barath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Larsson_V/0/1/0/all/0/1&quot;&gt;Viktor Larsson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14039">
<title>Controllable Guide-Space for Generalizable Face Forgery Detection. (arXiv:2307.14039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14039</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies on face forgery detection have shown satisfactory performance
for methods involved in training datasets, but are not ideal enough for unknown
domains. This motivates many works to improve the generalization, but
forgery-irrelevant information, such as image background and identity, still
exists in different domain features and causes unexpected clustering, limiting
the generalization. In this paper, we propose a controllable guide-space (GS)
method to enhance the discrimination of different forgery domains, so as to
increase the forgery relevance of features and thereby improve the
generalization. The well-designed guide-space can simultaneously achieve both
the proper separation of forgery domains and the large distance between
real-forgery domains in an explicit and controllable manner. Moreover, for
better discrimination, we use a decoupling module to weaken the interference of
forgery-irrelevant correlations between domains. Furthermore, we make
adjustments to the decision boundary manifold according to the clustering
degree of the same domain features within the neighborhood. Extensive
experiments in multiple in-domain and cross-domain settings confirm that our
method can achieve state-of-the-art generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Ying Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhen_C/0/1/0/all/0/1&quot;&gt;Cheng Zhen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_P/0/1/0/all/0/1&quot;&gt;Pengfei Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14051">
<title>3D Semantic Subspace Traverser: Empowering 3D Generative Model with Shape Editing Capability. (arXiv:2307.14051v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14051</link>
<description rdf:parseType="Literal">&lt;p&gt;Shape generation is the practice of producing 3D shapes as various
representations for 3D content creation. Previous studies on 3D shape
generation have focused on shape quality and structure, without or less
considering the importance of semantic information. Consequently, such
generative models often fail to preserve the semantic consistency of shape
structure or enable manipulation of the semantic attributes of shapes during
generation. In this paper, we proposed a novel semantic generative model named
3D Semantic Subspace Traverser that utilizes semantic attributes for
category-specific 3D shape generation and editing. Our method utilizes implicit
functions as the 3D shape representation and combines a novel latent-space GAN
with a linear subspace model to discover semantic dimensions in the local
latent space of 3D shapes. Each dimension of the subspace corresponds to a
particular semantic attribute, and we can edit the attributes of generated
shapes by traversing the coefficients of those dimensions. Experimental results
demonstrate that our method can produce plausible shapes with complex
structures and enable the editing of semantic attributes. The code and trained
models are available at
https://github.com/TrepangCat/3D_Semantic_Subspace_Traverser
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruowei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_P/0/1/0/all/0/1&quot;&gt;Pei Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qijun Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14052">
<title>Unite-Divide-Unite: Joint Boosting Trunk and Structure for High-accuracy Dichotomous Image Segmentation. (arXiv:2307.14052v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14052</link>
<description rdf:parseType="Literal">&lt;p&gt;High-accuracy Dichotomous Image Segmentation (DIS) aims to pinpoint
category-agnostic foreground objects from natural scenes. The main challenge
for DIS involves identifying the highly accurate dominant area while rendering
detailed object structure. However, directly using a general encoder-decoder
architecture may result in an oversupply of high-level features and neglect the
shallow spatial information necessary for partitioning meticulous structures.
To fill this gap, we introduce a novel Unite-Divide-Unite Network (UDUN} that
restructures and bipartitely arranges complementary features to simultaneously
boost the effectiveness of trunk and structure identification. The proposed
UDUN proceeds from several strengths. First, a dual-size input feeds into the
shared backbone to produce more holistic and detailed features while keeping
the model lightweight. Second, a simple Divide-and-Conquer Module (DCM) is
proposed to decouple multiscale low- and high-level features into our structure
decoder and trunk decoder to obtain structure and trunk information
respectively. Moreover, we design a Trunk-Structure Aggregation module (TSA) in
our union decoder that performs cascade integration for uniform high-accuracy
segmentation. As a result, UDUN performs favorably against state-of-the-art
competitors in all six evaluation metrics on overall DIS-TE, i.e., achieving
0.772 weighted F-measure and 977 HCE. Using 1024*1024 input, our model enables
real-time inference at 65.3 fps with ResNet-18.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1&quot;&gt;Jialun Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhangjun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yueming Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_H/0/1/0/all/0/1&quot;&gt;He Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heng_P/0/1/0/all/0/1&quot;&gt;Pheng-Ann Heng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14058">
<title>Towards Establishing Systematic Classification Requirements for Automated Driving. (arXiv:2307.14058v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14058</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the presence of the classification task in many different benchmark
datasets for perception in the automotive domain, few efforts have been
undertaken to define consistent classification requirements. This work
addresses the topic by proposing a structured method to generate a
classification structure. First, legal categories are identified based on
behavioral requirements for the vehicle. This structure is further
substantiated by considering the two aspects of collision safety for objects as
well as perceptual categories. A classification hierarchy is obtained by
applying the method to an exemplary legal text. A comparison of the results
with benchmark dataset categories shows limited agreement. This indicates the
necessity for explicit consideration of legal requirements regarding
perception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mori_K/0/1/0/all/0/1&quot;&gt;Ken T. Mori&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_T/0/1/0/all/0/1&quot;&gt;Trent Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peters_S/0/1/0/all/0/1&quot;&gt;Steven Peters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14061">
<title>Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models. (arXiv:2307.14061v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14061</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-language pre-training (VLP) models have shown vulnerability to
adversarial examples in multimodal tasks. Furthermore, malicious adversaries
can be deliberately transferred to attack other black-box models. However,
existing work has mainly focused on investigating white-box attacks. In this
paper, we present the first study to investigate the adversarial
transferability of recent VLP models. We observe that existing methods exhibit
much lower transferability, compared to the strong attack performance in
white-box settings. The transferability degradation is partly caused by the
under-utilization of cross-modal interactions. Particularly, unlike unimodal
learning, VLP models rely heavily on cross-modal interactions and the
multimodal alignments are many-to-many, e.g., an image can be described in
various natural languages. To this end, we propose a highly transferable
Set-level Guidance Attack (SGA) that thoroughly leverages modality interactions
and incorporates alignment-preserving augmentation with cross-modal guidance.
Experimental results demonstrate that SGA could generate adversarial examples
that can strongly transfer across different VLP models on multiple downstream
vision-language tasks. On image-text retrieval, SGA significantly enhances the
attack success rate for transfer attacks from ALBEF to TCL by a large margin
(at least 9.78% and up to 30.21%), compared to the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_D/0/1/0/all/0/1&quot;&gt;Dong Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Teng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_W/0/1/0/all/0/1&quot;&gt;Weili Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_H/0/1/0/all/0/1&quot;&gt;Hongchang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14063">
<title>ECO: Ensembling Context Optimization for Vision-Language Models. (arXiv:2307.14063v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14063</link>
<description rdf:parseType="Literal">&lt;p&gt;Image recognition has recently witnessed a paradigm shift, where
vision-language models are now used to perform few-shot classification based on
textual prompts. Among these, the CLIP model has shown remarkable capabilities
for zero-shot transfer by matching an image and a custom textual prompt in its
latent space. This has paved the way for several works that focus on
engineering or learning textual contexts for maximizing CLIP&apos;s classification
capabilities. In this paper, we follow this trend by learning an ensemble of
prompts for image classification. We show that learning diverse and possibly
shorter contexts improves considerably and consistently the results rather than
relying on a single trainable prompt. In particular, we report better few-shot
capabilities with no additional cost at inference time. We demonstrate the
capabilities of our approach on 11 different benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agnolucci_L/0/1/0/all/0/1&quot;&gt;Lorenzo Agnolucci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baldrati_A/0/1/0/all/0/1&quot;&gt;Alberto Baldrati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Todino_F/0/1/0/all/0/1&quot;&gt;Francesco Todino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Becattini_F/0/1/0/all/0/1&quot;&gt;Federico Becattini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertini_M/0/1/0/all/0/1&quot;&gt;Marco Bertini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bimbo_A/0/1/0/all/0/1&quot;&gt;Alberto Del Bimbo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14066">
<title>Pre-Training with Diffusion models for Dental Radiography segmentation. (arXiv:2307.14066v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14066</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical radiography segmentation, and specifically dental radiography, is
highly limited by the cost of labeling which requires specific expertise and
labor-intensive annotations. In this work, we propose a straightforward
pre-training method for semantic segmentation leveraging Denoising Diffusion
Probabilistic Models (DDPM), which have shown impressive results for generative
modeling. Our straightforward approach achieves remarkable performance in terms
of label efficiency and does not require architectural modifications between
pre-training and downstream tasks. We propose to first pre-train a Unet by
exploiting the DDPM training objective, and then fine-tune the resulting model
on a segmentation task. Our experimental results on the segmentation of dental
radiographs demonstrate that the proposed method is competitive with
state-of-the-art pre-training methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rousseau_J/0/1/0/all/0/1&quot;&gt;J&amp;#xe9;r&amp;#xe9;my Rousseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alaka_C/0/1/0/all/0/1&quot;&gt;Christian Alaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Covili_E/0/1/0/all/0/1&quot;&gt;Emma Covili&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mayard_H/0/1/0/all/0/1&quot;&gt;Hippolyte Mayard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misrachi_L/0/1/0/all/0/1&quot;&gt;Laura Misrachi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Au_W/0/1/0/all/0/1&quot;&gt;Willy Au&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14070">
<title>PNT-Edge: Towards Robust Edge Detection with Noisy Labels by Learning Pixel-level Noise Transitions. (arXiv:2307.14070v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14070</link>
<description rdf:parseType="Literal">&lt;p&gt;Relying on large-scale training data with pixel-level labels, previous edge
detection methods have achieved high performance. However, it is hard to
manually label edges accurately, especially for large datasets, and thus the
datasets inevitably contain noisy labels. This label-noise issue has been
studied extensively for classification, while still remaining under-explored
for edge detection. To address the label-noise issue for edge detection, this
paper proposes to learn Pixel-level NoiseTransitions to model the
label-corruption process. To achieve it, we develop a novel Pixel-wise Shift
Learning (PSL) module to estimate the transition from clean to noisy labels as
a displacement field. Exploiting the estimated noise transitions, our model,
named PNT-Edge, is able to fit the prediction to clean labels. In addition, a
local edge density regularization term is devised to exploit local structure
information for better transition learning. This term encourages learning large
shifts for the edges with complex local structures. Experiments on SBD and
Cityscapes demonstrate the effectiveness of our method in relieving the impact
of label noise. Codes will be available at github.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan_W/0/1/0/all/0/1&quot;&gt;Wenjie Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shanshan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Juhua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tongliang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yixin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14071">
<title>Uncertainty Guided Adaptive Warping for Robust and Efficient Stereo Matching. (arXiv:2307.14071v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14071</link>
<description rdf:parseType="Literal">&lt;p&gt;Correlation based stereo matching has achieved outstanding performance, which
pursues cost volume between two feature maps. Unfortunately, current methods
with a fixed model do not work uniformly well across various datasets, greatly
limiting their real-world applicability. To tackle this issue, this paper
proposes a new perspective to dynamically calculate correlation for robust
stereo matching. A novel Uncertainty Guided Adaptive Correlation (UGAC) module
is introduced to robustly adapt the same model for different scenarios.
Specifically, a variance-based uncertainty estimation is employed to adaptively
adjust the sampling area during warping operation. Additionally, we improve the
traditional non-parametric warping with learnable parameters, such that the
position-specific weights can be learned. We show that by empowering the
recurrent network with the UGAC module, stereo matching can be exploited more
robustly and effectively. Extensive experiments demonstrate that our method
achieves state-of-the-art performance over the ETH3D, KITTI, and Middlebury
datasets when employing the same fixed model over these datasets without any
retraining procedure. To target real-time applications, we further design a
lightweight model based on UGAC, which also outperforms other methods over
KITTI benchmarks with only 0.6 M parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_J/0/1/0/all/0/1&quot;&gt;Junpeng Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiankun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_P/0/1/0/all/0/1&quot;&gt;Pengfei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiangyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shuaicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yichen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_X/0/1/0/all/0/1&quot;&gt;Xin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Mai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_L/0/1/0/all/0/1&quot;&gt;Lai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1&quot;&gt;Leonid Sigal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14073">
<title>VideoControlNet: A Motion-Guided Video-to-Video Translation Framework by Using Diffusion Model with ControlNet. (arXiv:2307.14073v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14073</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, diffusion models like StableDiffusion have achieved impressive
image generation results. However, the generation process of such diffusion
models is uncontrollable, which makes it hard to generate videos with
continuous and consistent content. In this work, by using the diffusion model
with ControlNet, we proposed a new motion-guided video-to-video translation
framework called VideoControlNet to generate various videos based on the given
prompts and the condition from the input video. Inspired by the video codecs
that use motion information for reducing temporal redundancy, our framework
uses motion information to prevent the regeneration of the redundant areas for
content consistency. Specifically, we generate the first frame (i.e., the
I-frame) by using the diffusion model with ControlNet. Then we generate other
key frames (i.e., the P-frame) based on the previous I/P-frame by using our
newly proposed motion-guided P-frame generation (MgPG) method, in which the
P-frames are generated based on the motion information and the occlusion areas
are inpainted by using the diffusion model. Finally, the rest frames (i.e., the
B-frame) are generated by using our motion-guided B-frame interpolation (MgBI)
module. Our experiments demonstrate that our proposed VideoControlNet inherits
the generation capability of the pre-trained large diffusion model and extends
the image diffusion model to the video diffusion model by using motion
information. More results are provided at our project page.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhihao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14111">
<title>Periocular biometrics: databases, algorithms and directions. (arXiv:2307.14111v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14111</link>
<description rdf:parseType="Literal">&lt;p&gt;Periocular biometrics has been established as an independent modality due to
concerns on the performance of iris or face systems in uncontrolled conditions.
Periocular refers to the facial region in the eye vicinity, including eyelids,
lashes and eyebrows. It is available over a wide range of acquisition
distances, representing a trade-off between the whole face (which can be
occluded at close distances) and the iris texture (which do not have enough
resolution at long distances). Since the periocular region appears in face or
iris images, it can be used also in conjunction with these modalities. Features
extracted from the periocular region have been also used successfully for
gender classification and ethnicity classification, and to study the impact of
gender transformation or plastic surgery in the recognition performance. This
paper presents a review of the state of the art in periocular biometric
research, providing an insight of the most relevant issues and giving a
thorough coverage of the existing literature. Future research trends are also
briefly discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bigun_J/0/1/0/all/0/1&quot;&gt;Josef Bigun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14119">
<title>A semantics-driven methodology for high-quality image annotation. (arXiv:2307.14119v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14119</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work in Machine Learning and Computer Vision has highlighted the
presence of various types of systematic flaws inside ground truth object
recognition benchmark datasets. Our basic tenet is that these flaws are rooted
in the many-to-many mappings which exist between the visual information encoded
in images and the intended semantics of the labels annotating them. The net
consequence is that the current annotation process is largely under-specified,
thus leaving too much freedom to the subjective judgment of annotators. In this
paper, we propose vTelos, an integrated Natural Language Processing, Knowledge
Representation, and Computer Vision methodology whose main goal is to make
explicit the (otherwise implicit) intended annotation semantics, thus
minimizing the number and role of subjective choices. A key element of vTelos
is the exploitation of the WordNet lexico-semantic hierarchy as the main means
for providing the meaning of natural language labels and, as a consequence, for
driving the annotation of images based on the objects and the visual properties
they depict. The methodology is validated on images populating a subset of the
ImageNet hierarchy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giunchiglia_F/0/1/0/all/0/1&quot;&gt;Fausto Giunchiglia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bagchi_M/0/1/0/all/0/1&quot;&gt;Mayukh Bagchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diao_X/0/1/0/all/0/1&quot;&gt;Xiaolei Diao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14124">
<title>Memory-Efficient Graph Convolutional Networks for Object Classification and Detection with Event Cameras. (arXiv:2307.14124v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14124</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in event camera research emphasize processing data in its
original sparse form, which allows the use of its unique features such as high
temporal resolution, high dynamic range, low latency, and resistance to image
blur. One promising approach for analyzing event data is through graph
convolutional networks (GCNs). However, current research in this domain
primarily focuses on optimizing computational costs, neglecting the associated
memory costs. In this paper, we consider both factors together in order to
achieve satisfying results and relatively low model complexity. For this
purpose, we performed a comparative analysis of different graph convolution
operations, considering factors such as execution time, the number of trainable
model parameters, data format requirements, and training outcomes. Our results
show a 450-fold reduction in the number of parameters for the feature
extraction module and a 4.5-fold reduction in the size of the data
representation while maintaining a classification accuracy of 52.3%, which is
6.3% higher compared to the operation used in state-of-the-art approaches. To
further evaluate performance, we implemented the object detection architecture
and evaluated its performance on the N-Caltech101 dataset. The results showed
an accuracy of 53.7 % mAP@0.5 and reached an execution rate of 82 graphs per
second.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeziorek_K/0/1/0/all/0/1&quot;&gt;Kamil Jeziorek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pinna_A/0/1/0/all/0/1&quot;&gt;Andrea Pinna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1&quot;&gt;Tomasz Kryjak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14126">
<title>Multi-modal Learning with Missing Modality via Shared-Specific Feature Modelling. (arXiv:2307.14126v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14126</link>
<description rdf:parseType="Literal">&lt;p&gt;The missing modality issue is critical but non-trivial to be solved by
multi-modal models. Current methods aiming to handle the missing modality
problem in multi-modal tasks, either deal with missing modalities only during
evaluation or train separate models to handle specific missing modality
settings. In addition, these models are designed for specific tasks, so for
example, classification models are not easily adapted to segmentation tasks and
vice versa. In this paper, we propose the Shared-Specific Feature Modelling
(ShaSpec) method that is considerably simpler and more effective than competing
approaches that address the issues above. ShaSpec is designed to take advantage
of all available input modalities during training and evaluation by learning
shared and specific features to better represent the input data. This is
achieved from a strategy that relies on auxiliary tasks based on distribution
alignment and domain classification, in addition to a residual feature fusion
procedure. Also, the design simplicity of ShaSpec enables its easy adaptation
to multiple tasks, such as classification and segmentation. Experiments are
conducted on both medical image segmentation and computer vision
classification, with results indicating that ShaSpec outperforms competing
methods by a large margin. For instance, on BraTS2018, ShaSpec improves the
SOTA by more than 3% for enhancing tumour, 5% for tumour core and 3% for whole
tumour.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Congbo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Avery_J/0/1/0/all/0/1&quot;&gt;Jodie Avery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hull_L/0/1/0/all/0/1&quot;&gt;Louise Hull&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carneiro_G/0/1/0/all/0/1&quot;&gt;Gustavo Carneiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14127">
<title>Creative Birds: Self-Supervised Single-View 3D Style Transfer. (arXiv:2307.14127v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14127</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel method for single-view 3D style transfer
that generates a unique 3D object with both shape and texture transfer. Our
focus lies primarily on birds, a popular subject in 3D reconstruction, for
which no existing single-view 3D transfer methods have been developed.The
method we propose seeks to generate a 3D mesh shape and texture of a bird from
two single-view images. To achieve this, we introduce a novel shape transfer
generator that comprises a dual residual gated network (DRGNet), and a
multi-layer perceptron (MLP). DRGNet extracts the features of source and target
images using a shared coordinate gate unit, while the MLP generates spatial
coordinates for building a 3D mesh. We also introduce a semantic UV texture
transfer module that implements textural style transfer using semantic UV
segmentation, which ensures consistency in the semantic meaning of the
transferred regions. This module can be widely adapted to many existing
approaches. Finally, our method constructs a novel 3D bird using a
differentiable renderer. Experimental results on the CUB dataset verify that
our method achieves state-of-the-art performance on the single-view 3D style
transfer task. Code is available in
https://github.com/wrk226/2D-to-3D-Evolution-Transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Renke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Que_G/0/1/0/all/0/1&quot;&gt;Guimin Que&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14142">
<title>LOIS: Looking Out of Instance Semantics for Visual Question Answering. (arXiv:2307.14142v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14142</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual question answering (VQA) has been intensively studied as a multimodal
task that requires effort in bridging vision and language to infer answers
correctly. Recent attempts have developed various attention-based modules for
solving VQA tasks. However, the performance of model inference is largely
bottlenecked by visual processing for semantics understanding. Most existing
detection methods rely on bounding boxes, remaining a serious challenge for VQA
models to understand the causal nexus of object semantics in images and
correctly infer contextual information. To this end, we propose a finer model
framework without bounding boxes in this work, termed Looking Out of Instance
Semantics (LOIS) to tackle this important issue. LOIS enables more fine-grained
feature descriptions to produce visual facts. Furthermore, to overcome the
label ambiguity caused by instance masks, two types of relation attention
modules: 1) intra-modality and 2) inter-modality, are devised to infer the
correct answers from the different multi-view features. Specifically, we
implement a mutual relation attention module to model sophisticated and deeper
visual semantic relations between instance objects and background information.
In addition, our proposed attention model can further analyze salient image
regions by focusing on important word-related questions. Experimental results
on four benchmark VQA datasets prove that our proposed method has favorable
performance in improving visual reasoning capability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Siyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yeming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yaoru Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Haibo Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haoran Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14177">
<title>High-definition event frame generation using SoC FPGA devices. (arXiv:2307.14177v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14177</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we have addressed the implementation of the accumulation and
projection of high-resolution event data stream (HD -1280 x 720 pixels) onto
the image plane in FPGA devices. The results confirm the feasibility of this
approach, but there are a number of challenges, limitations and trade-offs to
be considered. The required hardware resources of selected data
representations, such as binary frame, event frame, exponentially decaying time
surface and event frequency, were compared with those available on several
popular platforms from AMD Xilinx. The resulting event frames can be used for
typical vision algorithms, such as object classification and detection, using
both classical and deep neural network methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blachut_K/0/1/0/all/0/1&quot;&gt;Krzysztof Blachut&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kryjak_T/0/1/0/all/0/1&quot;&gt;Tomasz Kryjak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14179">
<title>Resolution-Aware Design of Atrous Rates for Semantic Segmentation Networks. (arXiv:2307.14179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14179</link>
<description rdf:parseType="Literal">&lt;p&gt;DeepLab is a widely used deep neural network for semantic segmentation, whose
success is attributed to its parallel architecture called atrous spatial
pyramid pooling (ASPP). ASPP uses multiple atrous convolutions with different
atrous rates to extract both local and global information. However, fixed
values of atrous rates are used for the ASPP module, which restricts the size
of its field of view. In principle, atrous rate should be a hyperparameter to
change the field of view size according to the target task or dataset. However,
the manipulation of atrous rate is not governed by any guidelines. This study
proposes practical guidelines for obtaining an optimal atrous rate. First, an
effective receptive field for semantic segmentation is introduced to analyze
the inner behavior of segmentation networks. We observed that the use of ASPP
module yielded a specific pattern in the effective receptive field, which was
traced to reveal the module&apos;s underlying mechanism. Accordingly, we derive
practical guidelines for obtaining the optimal atrous rate, which should be
controlled based on the size of input image. Compared to other values, using
the optimal atrous rate consistently improved the segmentation results across
multiple datasets, including the STARE, CHASE_DB1, HRF, Cityscapes, and iSAID
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_B/0/1/0/all/0/1&quot;&gt;Bum Jun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_H/0/1/0/all/0/1&quot;&gt;Hyeyeon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_H/0/1/0/all/0/1&quot;&gt;Hyeonah Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sang Woo Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14187">
<title>ADAPT: Efficient Multi-Agent Trajectory Prediction with Adaptation. (arXiv:2307.14187v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14187</link>
<description rdf:parseType="Literal">&lt;p&gt;Forecasting future trajectories of agents in complex traffic scenes requires
reliable and efficient predictions for all agents in the scene. However,
existing methods for trajectory prediction are either inefficient or sacrifice
accuracy. To address this challenge, we propose ADAPT, a novel approach for
jointly predicting the trajectories of all agents in the scene with dynamic
weight learning. Our approach outperforms state-of-the-art methods in both
single-agent and multi-agent settings on the Argoverse and Interaction
datasets, with a fraction of their computational overhead. We attribute the
improvement in our performance: first, to the adaptive head augmenting the
model capacity without increasing the model size; second, to our design choices
in the endpoint-conditioned prediction, reinforced by gradient stopping. Our
analyses show that ADAPT can focus on each agent with adaptive prediction,
allowing for accurate predictions efficiently. https://KUIS-AI.github.io/adapt
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aydemir_G/0/1/0/all/0/1&quot;&gt;G&amp;#xf6;rkay Aydemir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akan_A/0/1/0/all/0/1&quot;&gt;Adil Kaan Akan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guney_F/0/1/0/all/0/1&quot;&gt;Fatma G&amp;#xfc;ney&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14227">
<title>Computational Approaches for Traditional Chinese Painting: From the &quot;Six Principles of Painting&quot; Perspective. (arXiv:2307.14227v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14227</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional Chinese Painting (TCP) is an invaluable cultural heritage
resource and a unique visual art style. In recent years, increasing interest
has been placed on digitalizing TCPs to preserve and revive the culture. The
resulting digital copies have enabled the advancement of computational methods
for structured and systematic understanding of TCPs. To explore this topic, we
conducted an in-depth analysis of 92 pieces of literature. We examined the
current use of computer technologies on TCPs from three perspectives, based on
numerous conversations with specialists. First, in light of the &quot;Six Principles
of Painting&quot; theory, we categorized the articles according to their research
focus on artistic elements. Second, we created a four-stage framework to
illustrate the purposes of TCP applications. Third, we summarized the popular
computational techniques applied to TCPs. The framework also provides insights
into potential applications and future prospects, with professional opinion.
The list of surveyed publications and related information is available online
at https://ca4tcp.com.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian-Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_K/0/1/0/all/0/1&quot;&gt;Kam Kwai Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yifang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_Y/0/1/0/all/0/1&quot;&gt;Yingchaojie Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Luwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14241">
<title>DisguisOR: Holistic Face Anonymization for the Operating Room. (arXiv:2307.14241v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14241</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Recent advances in Surgical Data Science (SDS) have contributed to
an increase in video recordings from hospital environments. While methods such
as surgical workflow recognition show potential in increasing the quality of
patient care, the quantity of video data has surpassed the scale at which
images can be manually anonymized. Existing automated 2D anonymization methods
under-perform in Operating Rooms (OR), due to occlusions and obstructions. We
propose to anonymize multi-view OR recordings using 3D data from multiple
camera streams. Methods: RGB and depth images from multiple cameras are fused
into a 3D point cloud representation of the scene. We then detect each
individual&apos;s face in 3D by regressing a parametric human mesh model onto
detected 3D human keypoints and aligning the face mesh with the fused 3D point
cloud. The mesh model is rendered into every acquired camera view, replacing
each individual&apos;s face. Results: Our method shows promise in locating faces at
a higher rate than existing approaches. DisguisOR produces geometrically
consistent anonymizations for each camera view, enabling more realistic
anonymization that is less detrimental to downstream tasks. Conclusion:
Frequent obstructions and crowding in operating rooms leaves significant room
for improvement for off-the-shelf anonymization methods. DisguisOR addresses
privacy on a scene level and has the potential to facilitate further research
in SDS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastian_L/0/1/0/all/0/1&quot;&gt;Lennart Bastian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tony Danjun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czempiel_T/0/1/0/all/0/1&quot;&gt;Tobias Czempiel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Busam_B/0/1/0/all/0/1&quot;&gt;Benjamin Busam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14242">
<title>Defending Adversarial Patches via Joint Region Localizing and Inpainting. (arXiv:2307.14242v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14242</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks are successfully used in various applications, but show
their vulnerability to adversarial examples. With the development of
adversarial patches, the feasibility of attacks in physical scenes increases,
and the defenses against patch attacks are urgently needed. However, defending
such adversarial patch attacks is still an unsolved problem. In this paper, we
analyse the properties of adversarial patches, and find that: on the one hand,
adversarial patches will lead to the appearance or contextual inconsistency in
the target objects; on the other hand, the patch region will show abnormal
changes on the high-level feature maps of the objects extracted by a backbone
network. Considering the above two points, we propose a novel defense method
based on a ``localizing and inpainting&quot; mechanism to pre-process the input
examples. Specifically, we design an unified framework, where the ``localizing&quot;
sub-network utilizes a two-branch structure to represent the above two aspects
to accurately detect the adversarial patch region in the image. For the
``inpainting&quot; sub-network, it utilizes the surrounding contextual cues to
recover the original content covered by the adversarial patch. The quality of
inpainted images is also evaluated by measuring the appearance consistency and
the effects of adversarial attacks. These two sub-networks are then jointly
trained via an iterative optimization manner. In this way, the ``localizing&quot;
and ``inpainting&quot; modules can interact closely with each other, and thus learn
a better solution. A series of experiments versus traffic sign classification
and detection tasks are conducted to defend against various adversarial patch
attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Junwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14243">
<title>Fluorescent Neuronal Cells v2: Multi-Task, Multi-Format Annotations for Deep Learning in Microscopy. (arXiv:2307.14243v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14243</link>
<description rdf:parseType="Literal">&lt;p&gt;Fluorescent Neuronal Cells v2 is a collection of fluorescence microscopy
images and the corresponding ground-truth annotations, designed to foster
innovative research in the domains of Life Sciences and Deep Learning. This
dataset encompasses three image collections in which rodent neuronal cells&apos;
nuclei and cytoplasm are stained with diverse markers to highlight their
anatomical or functional characteristics. Alongside the images, we provide
ground-truth annotations for several learning tasks, including semantic
segmentation, object detection, and counting. The contribution is two-fold.
First, given the variety of annotations and their accessible formats, we
envision our work facilitating methodological advancements in computer vision
approaches for segmentation, detection, feature learning, unsupervised and
self-supervised learning, transfer learning, and related areas. Second, by
enabling extensive exploration and benchmarking, we hope Fluorescent Neuronal
Cells v2 will catalyze breakthroughs in fluorescence microscopy analysis and
promote cutting-edge discoveries in life sciences. The data are available at:
https://amsacta.unibo.it/id/eprint/7347
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clissa_L/0/1/0/all/0/1&quot;&gt;Luca Clissa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macaluso_A/0/1/0/all/0/1&quot;&gt;Antonio Macaluso&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morelli_R/0/1/0/all/0/1&quot;&gt;Roberto Morelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Occhinegro_A/0/1/0/all/0/1&quot;&gt;Alessandra Occhinegro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piscitiello_E/0/1/0/all/0/1&quot;&gt;Emiliana Piscitiello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taddei_L/0/1/0/all/0/1&quot;&gt;Ludovico Taddei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luppi_M/0/1/0/all/0/1&quot;&gt;Marco Luppi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amici_R/0/1/0/all/0/1&quot;&gt;Roberto Amici&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cerri_M/0/1/0/all/0/1&quot;&gt;Matteo Cerri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hitrec_T/0/1/0/all/0/1&quot;&gt;Timna Hitrec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rinaldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Rinaldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zoccoli_A/0/1/0/all/0/1&quot;&gt;Antonio Zoccoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14253">
<title>Sparse Double Descent in Vision Transformers: real or phantom threat?. (arXiv:2307.14253v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14253</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision transformers (ViT) have been of broad interest in recent theoretical
and empirical works. They are state-of-the-art thanks to their attention-based
approach, which boosts the identification of key features and patterns within
images thanks to the capability of avoiding inductive bias, resulting in highly
accurate image analysis. Meanwhile, neoteric studies have reported a ``sparse
double descent&apos;&apos; phenomenon that can occur in modern deep-learning models,
where extremely over-parametrized models can generalize well. This raises
practical questions about the optimal size of the model and the quest over
finding the best trade-off between sparsity and performance is launched: are
Vision Transformers also prone to sparse double descent? Can we find a way to
avoid such a phenomenon? Our work tackles the occurrence of sparse double
descent on ViTs. Despite some works that have shown that traditional
architectures, like Resnet, are condemned to the sparse double descent
phenomenon, for ViTs we observe that an optimally-tuned $\ell_2$ regularization
relieves such a phenomenon. However, everything comes at a cost: optimal lambda
will sacrifice the potential compression of the ViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quetu_V/0/1/0/all/0/1&quot;&gt;Victor Qu&amp;#xe9;tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Milovanovic_M/0/1/0/all/0/1&quot;&gt;Marta Milovanovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tartaglione_E/0/1/0/all/0/1&quot;&gt;Enzo Tartaglione&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14262">
<title>Artifact Restoration in Histology Images with Diffusion Probabilistic Models. (arXiv:2307.14262v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14262</link>
<description rdf:parseType="Literal">&lt;p&gt;Histological whole slide images (WSIs) can be usually compromised by
artifacts, such as tissue folding and bubbles, which will increase the
examination difficulty for both pathologists and Computer-Aided Diagnosis (CAD)
systems. Existing approaches to restoring artifact images are confined to
Generative Adversarial Networks (GANs), where the restoration process is
formulated as an image-to-image transfer. Those methods are prone to suffer
from mode collapse and unexpected mistransfer in the stain style, leading to
unsatisfied and unrealistic restored images. Innovatively, we make the first
attempt at a denoising diffusion probabilistic model for histological artifact
restoration, namely ArtiFusion.Specifically, ArtiFusion formulates the artifact
region restoration as a gradual denoising process, and its training relies
solely on artifact-free images to simplify the training complexity.Furthermore,
to capture local-global correlations in the regional artifact restoration, a
novel Swin-Transformer denoising architecture is designed, along with a time
token scheme. Our extensive evaluations demonstrate the effectiveness of
ArtiFusion as a pre-processing method for histology analysis, which can
successfully preserve the tissue structures and stain style in artifact-free
regions during the restoration. Code is available at
https://github.com/zhenqi-he/ArtiFusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zhenqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junjun He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ye_J/0/1/0/all/0/1&quot;&gt;Jin Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yiqing Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14273">
<title>Deepfake Image Generation for Improved Brain Tumor Segmentation. (arXiv:2307.14273v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.14273</link>
<description rdf:parseType="Literal">&lt;p&gt;As the world progresses in technology and health, awareness of disease by
revealing asymptomatic signs improves. It is important to detect and treat
tumors in early stage as it can be life-threatening. Computer-aided
technologies are used to overcome lingering limitations facing disease
diagnosis, while brain tumor segmentation remains a difficult process,
especially when multi-modality data is involved. This is mainly attributed to
ineffective training due to lack of data and corresponding labelling. This work
investigates the feasibility of employing deep-fake image generation for
effective brain tumor segmentation. To this end, a Generative Adversarial
Network was used for image-to-image translation for increasing dataset size,
followed by image segmentation using a U-Net-based convolutional neural network
trained with deepfake images. Performance of the proposed approach is compared
with ground truth of four publicly available datasets. Results show improved
performance in terms of image segmentation quality metrics, and could
potentially assist when training with limited data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Emaryeen_R/0/1/0/all/0/1&quot;&gt;Roa&amp;#x27;a Al-Emaryeen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Nahhas_S/0/1/0/all/0/1&quot;&gt;Sara Al-Nahhas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Himour_F/0/1/0/all/0/1&quot;&gt;Fatima Himour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahafza_W/0/1/0/all/0/1&quot;&gt;Waleed Mahafza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Al_Kadi_O/0/1/0/all/0/1&quot;&gt;Omar Al-Kadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14277">
<title>G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory. (arXiv:2307.14277v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14277</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent video grounding works attempt to introduce vanilla contrastive
learning into video grounding. However, we claim that this naive solution is
suboptimal. Contrastive learning requires two key properties: (1)
\emph{alignment} of features of similar samples, and (2) \emph{uniformity} of
the induced distribution of the normalized features on the hypersphere. Due to
two annoying issues in video grounding: (1) the co-existence of some visual
entities in both ground truth and other moments, \ie semantic overlapping; (2)
only a few moments in the video are annotated, \ie sparse annotation dilemma,
vanilla contrastive learning is unable to model the correlations between
temporally distant moments and learned inconsistent video representations. Both
characteristics lead to vanilla contrastive learning being unsuitable for video
grounding. In this paper, we introduce Geodesic and Game Localization (G2L), a
semantically aligned and uniform video grounding framework via geodesic and
game theory. We quantify the correlations among moments leveraging the geodesic
distance that guides the model to learn the correct cross-modal
representations. Furthermore, from the novel perspective of game theory, we
propose semantic Shapley interaction based on geodesic distance sampling to
learn fine-grained semantic alignment in similar moments. Experiments on three
benchmarks demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Meng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xuxin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuexian Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14278">
<title>Large-scale Fully-Unsupervised Re-Identification. (arXiv:2307.14278v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14278</link>
<description rdf:parseType="Literal">&lt;p&gt;Fully-unsupervised Person and Vehicle Re-Identification have received
increasing attention due to their broad applicability in surveillance,
forensics, event understanding, and smart cities, without requiring any manual
annotation. However, most of the prior art has been evaluated in datasets that
have just a couple thousand samples. Such small-data setups often allow the use
of costly techniques in time and memory footprints, such as Re-Ranking, to
improve clustering results. Moreover, some previous work even pre-selects the
best clustering hyper-parameters for each dataset, which is unrealistic in a
large-scale fully-unsupervised scenario. In this context, this work tackles a
more realistic scenario and proposes two strategies to learn from large-scale
unlabeled data. The first strategy performs a local neighborhood sampling to
reduce the dataset size in each iteration without violating neighborhood
relationships. A second strategy leverages a novel Re-Ranking technique, which
has a lower time upper bound complexity and reduces the memory complexity from
O(n^2) to O(kn) with k &amp;lt;&amp;lt; n. To avoid the pre-selection of specific
hyper-parameter values for the clustering algorithm, we also present a novel
scheduling algorithm that adjusts the density parameter during training, to
leverage the diversity of samples and keep the learning robust to noisy
labeling. Finally, due to the complementary knowledge learned by different
models, we also introduce a co-training strategy that relies upon the
permutation of predicted pseudo-labels, among the backbones, with no need for
any hyper-parameters or weighting optimization. The proposed methodology
outperforms the state-of-the-art methods in well-known benchmarks and in the
challenging large-scale Veri-Wild dataset, with a faster and memory-efficient
Re-Ranking strategy, and a large-scale, noisy-robust, and ensemble-based
learning approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertocco_G/0/1/0/all/0/1&quot;&gt;Gabriel Bertocco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andalo_F/0/1/0/all/0/1&quot;&gt;Fernanda Andal&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boult_T/0/1/0/all/0/1&quot;&gt;Terrance E. Boult&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rocha_A/0/1/0/all/0/1&quot;&gt;Anderson Rocha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14288">
<title>US &amp; MR Image-Fusion Based on Skin Co-Registration. (arXiv:2307.14288v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14288</link>
<description rdf:parseType="Literal">&lt;p&gt;The study and development of innovative solutions for the advanced
visualisation, representation and analysis of medical images offer different
research directions. Current practice in medical imaging consists in combining
real-time US with imaging modalities that allow internal anatomy acquisitions,
such as CT, MRI, PET or similar. Application of image-fusion approaches can be
found in tracking surgical tools and/or needles, in real-time during
interventions. Thus, this work proposes a fusion imaging system for the
registration of CT and MRI images with real-time US acquisition leveraging a 3D
camera sensor. The main focus of the work is the portability of the system and
its applicability to different anatomical districts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paccini_M/0/1/0/all/0/1&quot;&gt;Martina Paccini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paschina_G/0/1/0/all/0/1&quot;&gt;Giacomo Paschina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beni_S/0/1/0/all/0/1&quot;&gt;Stefano De Beni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patane_G/0/1/0/all/0/1&quot;&gt;Giuseppe Patan&amp;#xe8;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14294">
<title>Unraveling the Complexity of Splitting Sequential Data: Tackling Challenges in Video and Time Series Analysis. (arXiv:2307.14294v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.14294</link>
<description rdf:parseType="Literal">&lt;p&gt;Splitting of sequential data, such as videos and time series, is an essential
step in various data analysis tasks, including object tracking and anomaly
detection. However, splitting sequential data presents a variety of challenges
that can impact the accuracy and reliability of subsequent analyses. This
concept article examines the challenges associated with splitting sequential
data, including data acquisition, data representation, split ratio selection,
setting up quality criteria, and choosing suitable selection strategies. We
explore these challenges through two real-world examples: motor test benches
and particle tracking in liquids.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Botache_D/0/1/0/all/0/1&quot;&gt;Diego Botache&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dingel_K/0/1/0/all/0/1&quot;&gt;Kristina Dingel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huhnstock_R/0/1/0/all/0/1&quot;&gt;Rico Huhnstock&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ehresmann_A/0/1/0/all/0/1&quot;&gt;Arno Ehresmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sick_B/0/1/0/all/0/1&quot;&gt;Bernhard Sick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14331">
<title>Visual Instruction Inversion: Image Editing via Visual Prompting. (arXiv:2307.14331v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14331</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-conditioned image editing has emerged as a powerful tool for editing
images. However, in many situations, language can be ambiguous and ineffective
in describing specific image edits. When faced with such challenges, visual
prompts can be a more informative and intuitive way to convey ideas. We present
a method for image editing via visual prompting. Given pairs of example that
represent the &quot;before&quot; and &quot;after&quot; images of an edit, our goal is to learn a
text-based editing direction that can be used to perform the same edit on new
images. We leverage the rich, pretrained editing capabilities of text-to-image
diffusion models by inverting visual prompts into editing instructions. Our
results show that with just one example pair, we can achieve competitive
results compared to state-of-the-art text-conditioned image editing frameworks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thao Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ojha_U/0/1/0/all/0/1&quot;&gt;Utkarsh Ojha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_Y/0/1/0/all/0/1&quot;&gt;Yong Jae Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14332">
<title>Event-based Vision for Early Prediction of Manipulation Actions. (arXiv:2307.14332v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14332</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuromorphic visual sensors are artificial retinas that output sequences of
asynchronous events when brightness changes occur in the scene. These sensors
offer many advantages including very high temporal resolution, no motion blur
and smart data compression ideal for real-time processing. In this study, we
introduce an event-based dataset on fine-grained manipulation actions and
perform an experimental study on the use of transformers for action prediction
with events. There is enormous interest in the fields of cognitive robotics and
human-robot interaction on understanding and predicting human actions as early
as possible. Early prediction allows anticipating complex stages for planning,
enabling effective and real-time interaction. Our Transformer network uses
events to predict manipulation actions as they occur, using online inference.
The model succeeds at predicting actions early on, building up confidence over
time and achieving state-of-the-art classification. Moreover, the
attention-based transformer architecture allows us to study the role of the
spatio-temporal patterns selected by the model. Our experiments show that the
Transformer network captures action dynamic features outperforming video-based
approaches and succeeding with scenarios where the differences between actions
lie in very subtle cues. Finally, we release the new event dataset, which is
the first in the literature for manipulation action recognition. Code will be
available at https://github.com/DaniDeniz/EventVisionTransformer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deniz_D/0/1/0/all/0/1&quot;&gt;Daniel Deniz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fermuller_C/0/1/0/all/0/1&quot;&gt;Cornelia Fermuller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ros_E/0/1/0/all/0/1&quot;&gt;Eduardo Ros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rodriguez_Alvarez_M/0/1/0/all/0/1&quot;&gt;Manuel Rodriguez-Alvarez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barranco_F/0/1/0/all/0/1&quot;&gt;Francisco Barranco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14334">
<title>Towards Generalist Biomedical AI. (arXiv:2307.14334v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2307.14334</link>
<description rdf:parseType="Literal">&lt;p&gt;Medicine is inherently multimodal, with rich data modalities spanning text,
imaging, genomics, and more. Generalist biomedical artificial intelligence (AI)
systems that flexibly encode, integrate, and interpret this data at scale can
potentially enable impactful applications ranging from scientific discovery to
care delivery. To enable the development of these models, we first curate
MultiMedBench, a new multimodal biomedical benchmark. MultiMedBench encompasses
14 diverse tasks such as medical question answering, mammography and
dermatology image interpretation, radiology report generation and
summarization, and genomic variant calling. We then introduce Med-PaLM
Multimodal (Med-PaLM M), our proof of concept for a generalist biomedical AI
system. Med-PaLM M is a large multimodal generative model that flexibly encodes
and interprets biomedical data including clinical language, imaging, and
genomics with the same set of model weights. Med-PaLM M reaches performance
competitive with or exceeding the state of the art on all MultiMedBench tasks,
often surpassing specialist models by a wide margin. We also report examples of
zero-shot generalization to novel medical concepts and tasks, positive transfer
learning across tasks, and emergent zero-shot medical reasoning. To further
probe the capabilities and limitations of Med-PaLM M, we conduct a radiologist
evaluation of model-generated (and human) chest X-ray reports and observe
encouraging performance across model scales. In a side-by-side ranking on 246
retrospective chest X-rays, clinicians express a pairwise preference for
Med-PaLM M reports over those produced by radiologists in up to 40.50% of
cases, suggesting potential clinical utility. While considerable work is needed
to validate these models in real-world use cases, our results represent a
milestone towards the development of generalist biomedical AI systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azizi_S/0/1/0/all/0/1&quot;&gt;Shekoofeh Azizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driess_D/0/1/0/all/0/1&quot;&gt;Danny Driess&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaekermann_M/0/1/0/all/0/1&quot;&gt;Mike Schaekermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amin_M/0/1/0/all/0/1&quot;&gt;Mohamed Amin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_P/0/1/0/all/0/1&quot;&gt;Pi-Chuan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carroll_A/0/1/0/all/0/1&quot;&gt;Andrew Carroll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lau_C/0/1/0/all/0/1&quot;&gt;Chuck Lau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tanno_R/0/1/0/all/0/1&quot;&gt;Ryutaro Tanno&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ktena_I/0/1/0/all/0/1&quot;&gt;Ira Ktena&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_B/0/1/0/all/0/1&quot;&gt;Basil Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhery_A/0/1/0/all/0/1&quot;&gt;Aakanksha Chowdhery&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornblith_S/0/1/0/all/0/1&quot;&gt;Simon Kornblith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fleet_D/0/1/0/all/0/1&quot;&gt;David Fleet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mansfield_P/0/1/0/all/0/1&quot;&gt;Philip Mansfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prakash_S/0/1/0/all/0/1&quot;&gt;Sushant Prakash&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_R/0/1/0/all/0/1&quot;&gt;Renee Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virmani_S/0/1/0/all/0/1&quot;&gt;Sunny Virmani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Semturs_C/0/1/0/all/0/1&quot;&gt;Christopher Semturs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdavi_S/0/1/0/all/0/1&quot;&gt;S Sara Mahdavi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Green_B/0/1/0/all/0/1&quot;&gt;Bradley Green&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dominowska_E/0/1/0/all/0/1&quot;&gt;Ewa Dominowska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcas_B/0/1/0/all/0/1&quot;&gt;Blaise Aguera y Arcas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barral_J/0/1/0/all/0/1&quot;&gt;Joelle Barral&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Webster_D/0/1/0/all/0/1&quot;&gt;Dale Webster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Corrado_G/0/1/0/all/0/1&quot;&gt;Greg S. Corrado&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matias_Y/0/1/0/all/0/1&quot;&gt;Yossi Matias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singhal_K/0/1/0/all/0/1&quot;&gt;Karan Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Florence_P/0/1/0/all/0/1&quot;&gt;Pete Florence&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karthikesalingam_A/0/1/0/all/0/1&quot;&gt;Alan Karthikesalingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Natarajan_V/0/1/0/all/0/1&quot;&gt;Vivek Natarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14336">
<title>MAMo: Leveraging Memory and Attention for Monocular Video Depth Estimation. (arXiv:2307.14336v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14336</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose MAMo, a novel memory and attention frame-work for monocular video
depth estimation. MAMo can augment and improve any single-image depth
estimation networks into video depth estimation models, enabling them to take
advantage of the temporal information to predict more accurate depth. In MAMo,
we augment model with memory which aids the depth prediction as the model
streams through the video. Specifically, the memory stores learned visual and
displacement tokens of the previous time instances. This allows the depth
network to cross-reference relevant features from the past when predicting
depth on the current frame. We introduce a novel scheme to continuously update
the memory, optimizing it to keep tokens that correspond with both the past and
the present visual information. We adopt attention-based approach to process
memory features where we first learn the spatio-temporal relation among the
resultant visual and displacement memory tokens using self-attention module.
Further, the output features of self-attention are aggregated with the current
visual features through cross-attention. The cross-attended features are
finally given to a decoder to predict depth on the current frame. Through
extensive experiments on several benchmarks, including KITTI, NYU-Depth V2, and
DDAD, we show that MAMo consistently improves monocular depth estimation
networks and sets new state-of-the-art (SOTA) accuracy. Notably, our MAMo video
depth estimation provides higher accuracy with lower latency, when omparing to
SOTA cost-volume-based video depth models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yasarla_R/0/1/0/all/0/1&quot;&gt;Rajeev Yasarla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hong Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeong_J/0/1/0/all/0/1&quot;&gt;Jisoo Jeong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yunxiao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garrepalli_R/0/1/0/all/0/1&quot;&gt;Risheek Garrepalli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Porikli_F/0/1/0/all/0/1&quot;&gt;Fatih Porikli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14341">
<title>Virtual Mirrors: Non-Line-of-Sight Imaging Beyond the Third Bounce. (arXiv:2307.14341v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.14341</link>
<description rdf:parseType="Literal">&lt;p&gt;Non-line-of-sight (NLOS) imaging methods are capable of reconstructing
complex scenes that are not visible to an observer using indirect illumination.
However, they assume only third-bounce illumination, so they are currently
limited to single-corner configurations, and present limited visibility when
imaging surfaces at certain orientations. To reason about and tackle these
limitations, we make the key observation that planar diffuse surfaces behave
specularly at wavelengths used in the computational wave-based NLOS imaging
domain. We call such surfaces virtual mirrors. We leverage this observation to
expand the capabilities of NLOS imaging using illumination beyond the third
bounce, addressing two problems: imaging single-corner objects at limited
visibility angles, and imaging objects hidden behind two corners. To image
objects at limited visibility angles, we first analyze the reflections of the
known illuminated point on surfaces of the scene as an estimator of the
position and orientation of objects with limited visibility. We then image
those limited visibility objects by computationally building secondary
apertures at other surfaces that observe the target object from a direct
visibility perspective. Beyond single-corner NLOS imaging, we exploit the
specular behavior of virtual mirrors to image objects hidden behind a second
corner by imaging the space behind such virtual mirrors, where the mirror image
of objects hidden around two corners is formed. No specular surfaces were
involved in the making of this paper.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Royo_D/0/1/0/all/0/1&quot;&gt;Diego Royo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sultan_T/0/1/0/all/0/1&quot;&gt;Talha Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Munoz_A/0/1/0/all/0/1&quot;&gt;Adolfo Mu&amp;#xf1;oz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masumnia_Bisheh_K/0/1/0/all/0/1&quot;&gt;Khadijeh Masumnia-Bisheh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandt_E/0/1/0/all/0/1&quot;&gt;Eric Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gutierrez_D/0/1/0/all/0/1&quot;&gt;Diego Gutierrez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Velten_A/0/1/0/all/0/1&quot;&gt;Andreas Velten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marco_J/0/1/0/all/0/1&quot;&gt;Julio Marco&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2008.07073">
<title>AlphaNet: Improving Long-Tail Classification By Combining Classifiers. (arXiv:2008.07073v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2008.07073</link>
<description rdf:parseType="Literal">&lt;p&gt;Methods in long-tail learning focus on improving performance for data-poor
(rare) classes; however, performance for such classes remains much lower than
performance for more data-rich (frequent) classes. Analyzing the predictions of
long-tail methods for rare classes reveals that a large number of errors are
due to misclassification of rare items as visually similar frequent classes. To
address this problem, we introduce AlphaNet, a method that can be applied to
existing models, performing post hoc correction on classifiers of rare classes.
Starting with a pre-trained model, we find frequent classes that are closest to
rare classes in the model&apos;s representation space and learn weights to update
rare class classifiers with a linear combination of frequent class classifiers.
AlphaNet, applied to several models, greatly improves test accuracy for rare
classes in multiple long-tailed datasets, with very little change to overall
accuracy. Our method also provides a way to control the trade-off between rare
class and overall accuracy, making it practical for long-tail classification in
the wild.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_N/0/1/0/all/0/1&quot;&gt;Nadine Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushik_J/0/1/0/all/0/1&quot;&gt;Jayanth Koushik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_A/0/1/0/all/0/1&quot;&gt;Aarti Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hebert_M/0/1/0/all/0/1&quot;&gt;Martial Hebert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Xiong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarr_M/0/1/0/all/0/1&quot;&gt;Michael J. Tarr&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.05423">
<title>Deep Learning Based 3D Segmentation: A Survey. (arXiv:2103.05423v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.05423</link>
<description rdf:parseType="Literal">&lt;p&gt;3D segmentation is a fundamental and challenging problem in computer vision
with applications in autonomous driving, robotics, augmented reality and
medical image analysis. It has received significant attention from the computer
vision, graphics and machine learning communities. Conventional methods for 3D
segmentation, based on hand-crafted features and machine learning classifiers,
lack generalization ability. Driven by their success in 2D computer vision,
deep learning techniques have recently become the tool of choice for 3D
segmentation tasks. This has led to an influx of a large number of methods in
the literature that have been evaluated on different benchmark datasets.
Whereas survey papers on RGB-D and point cloud segmentation exist, there is a
lack of an in-depth and recent survey that covers all 3D data modalities and
application domains. This paper fills the gap and provides a comprehensive
survey of the recent progress made in deep learning based 3D segmentation. It
covers over 180 works, analyzes their strengths and limitations and discusses
their competitive results on benchmark datasets. The survey provides a summary
of the most commonly used pipelines and finally highlights promising research
directions for the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Yong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongshan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhengeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mian_A/0/1/0/all/0/1&quot;&gt;Ajmal Mian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03906">
<title>Cross-modal Manifold Cutmix for Self-supervised Video Representation Learning. (arXiv:2112.03906v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03906</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the challenge of obtaining large-scale unlabelled
video datasets for contrastive representation learning in real-world
applications. We present a novel video augmentation technique for
self-supervised learning, called Cross-Modal Manifold Cutmix (CMMC), which
generates augmented samples by combining different modalities in videos. By
embedding a video tesseract into another across two modalities in the feature
space, our method enhances the quality of learned video representations. We
perform extensive experiments on two small-scale video datasets, UCF101 and
HMDB51, for action recognition and video retrieval tasks. Our approach is also
shown to be effective on the NTU dataset with limited domain knowledge. Our
CMMC achieves comparable performance to other self-supervised methods while
using less training data for both downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_S/0/1/0/all/0/1&quot;&gt;Srijan Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryoo_M/0/1/0/all/0/1&quot;&gt;Michael S. Ryoo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.09559">
<title>Priming Cross-Session Motor Imagery Classification with A Universal Deep Domain Adaptation Framework. (arXiv:2202.09559v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2202.09559</link>
<description rdf:parseType="Literal">&lt;p&gt;Motor imagery (MI) is a common brain computer interface (BCI) paradigm. EEG
is non-stationary with low signal-to-noise, classifying motor imagery tasks of
the same participant from different EEG recording sessions is generally
challenging, as EEG data distribution may vary tremendously among different
acquisition sessions. Although it is intuitive to consider the cross-session MI
classification as a domain adaptation problem, the rationale and feasible
approach is not elucidated. In this paper, we propose a Siamese deep domain
adaptation (SDDA) framework for cross-session MI classification based on
mathematical models in domain adaptation theory. The proposed framework can be
easily applied to most existing artificial neural networks without altering the
network structure, which facilitates our method with great flexibility and
transferability. In the proposed framework, domain invariants were firstly
constructed jointly with channel normalization and Euclidean alignment. Then,
embedding features from source and target domain were mapped into the
Reproducing Kernel Hilbert Space (RKHS) and aligned accordingly. A cosine-based
center loss was also integrated into the framework to improve the
generalizability of the SDDA. The proposed framework was validated with two
classic and popular convolutional neural networks from BCI research field
(EEGNet and ConvNet) in two MI-EEG public datasets (BCI Competition IV IIA,
IIB). Compared to the vanilla EEGNet and ConvNet, the proposed SDDA framework
was able to boost the MI classification accuracy by 15.2%, 10.2% respectively
in IIA dataset, and 5.5%, 4.2% in IIB dataset. The final MI classification
accuracy reached 82.01% in IIA dataset and 87.52% in IIB, which outperformed
the state-of-the-art methods in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_Z/0/1/0/all/0/1&quot;&gt;Zhengqing Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menon_C/0/1/0/all/0/1&quot;&gt;Carlo Menon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yelong Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_M/0/1/0/all/0/1&quot;&gt;Meirong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_D/0/1/0/all/0/1&quot;&gt;Dong Ming&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.01482">
<title>MetaDT: Meta Decision Tree with Class Hierarchy for Interpretable Few-Shot Learning. (arXiv:2203.01482v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.01482</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-Shot Learning (FSL) is a challenging task, which aims to recognize novel
classes with few examples. Recently, lots of methods have been proposed from
the perspective of meta-learning and representation learning. However, few
works focus on the interpretability of FSL decision process. In this paper, we
take a step towards the interpretable FSL by proposing a novel meta-learning
based decision tree framework, namely, MetaDT. In particular, the FSL
interpretability is achieved from two aspects, i.e., a concept aspect and a
visual aspect. On the concept aspect, we first introduce a tree-like concept
hierarchy as FSL prior. Then, resorting to the prior, we split each few-shot
task to a set of subtasks with different concept levels and then perform class
prediction via a model of decision tree. The advantage of such design is that a
sequence of high-level concept decisions that lead up to a final class
prediction can be obtained, which clarifies the FSL decision process. On the
visual aspect, a set of subtask-specific classifiers with visual attention
mechanism is designed to perform decision at each node of the decision tree. As
a result, a subtask-specific heatmap visualization can be obtained to achieve
the decision interpretability of each tree node. At last, to alleviate the data
scarcity issue of FSL, we regard the prior of concept hierarchy as an
undirected graph, and then design a graph convolution-based decision tree
inference network as our meta-learner to infer parameters of the decision tree.
Extensive experiments on performance comparison and interpretability analysis
show superiority of our MetaDT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Baoquan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hao Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xutao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_S/0/1/0/all/0/1&quot;&gt;Shanshan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunming Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_R/0/1/0/all/0/1&quot;&gt;Rui Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.04317">
<title>MICDIR: Multi-scale Inverse-consistent Deformable Image Registration using UNetMSS with Self-Constructing Graph Latent. (arXiv:2203.04317v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.04317</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration is the process of bringing different images into a common
coordinate system - a technique widely used in various applications of computer
vision, such as remote sensing, image retrieval, and, most commonly, medical
imaging. Deep learning based techniques have been applied successfully to
tackle various complex medical image processing problems, including medical
image registration. Over the years, several image registration techniques have
been proposed using deep learning. Deformable image registration techniques
such as Voxelmorph have been successful in capturing finer changes and
providing smoother deformations. However, Voxelmorph, as well as ICNet and
FIRE, do not explicitly encode global dependencies (i.e. the overall anatomical
view of the supplied image) and, therefore, cannot track large deformations. In
order to tackle the aforementioned problems, this paper extends the Voxelmorph
approach in three different ways. To improve the performance in case of small
as well as large deformations, supervision of the model at different
resolutions has been integrated using a multi-scale UNet. To support the
network to learn and encode the minute structural co-relations of the given
image-pairs, a self-constructing graph network (SCGNet) has been used as the
latent of the multi-scale UNet - which can improve the learning process of the
model and help the model to generalise better. And finally, to make the
deformations inverse-consistent, cycle consistency loss has been employed. On
the task of registration of brain MRIs, the proposed method achieved
significant improvements over ANTs and VoxelMorph, obtaining a Dice score of
0.8013 \pm 0.0243 for intramodal and 0.6211 \pm 0.0309 for intermodal, while
VoxelMorph achieved 0.7747 \pm 0.0260 and 0.6071 \pm 0.0510, respectively
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chatterjee_S/0/1/0/all/0/1&quot;&gt;Soumick Chatterjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bajaj_H/0/1/0/all/0/1&quot;&gt;Himanshi Bajaj&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Siddiquee_I/0/1/0/all/0/1&quot;&gt;Istiyak H. Siddiquee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Subbarayappa_N/0/1/0/all/0/1&quot;&gt;Nandish Bandi Subbarayappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Simon_S/0/1/0/all/0/1&quot;&gt;Steve Simon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Shashidhar_S/0/1/0/all/0/1&quot;&gt;Suraj Bangalore Shashidhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Speck_O/0/1/0/all/0/1&quot;&gt;Oliver Speck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nurnberge_A/0/1/0/all/0/1&quot;&gt;Andreas N&amp;#xfc;rnberge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02070">
<title>Priors in Deep Image Restoration and Enhancement: A Survey. (arXiv:2206.02070v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02070</link>
<description rdf:parseType="Literal">&lt;p&gt;Image restoration and enhancement is a process of improving the image quality
by removing degradations, such as noise, blur, and resolution degradation. Deep
learning (DL) has recently been applied to image restoration and enhancement.
Due to its ill-posed property, plenty of works have been explored priors to
facilitate training deep neural networks (DNNs). However, the importance of
priors has not been systematically studied and analyzed by far in the research
community. Therefore, this paper serves as the first study that provides a
comprehensive overview of recent advancements in priors for deep image
restoration and enhancement. Our work covers five primary contents: (1) A
theoretical analysis of priors for deep image restoration and enhancement; (2)
A hierarchical and structural taxonomy of priors commonly used in the DL-based
methods; (3) An insightful discussion on each prior regarding its principle,
potential, and applications; (4) A summary of crucial problems by highlighting
the potential future directions, especially adopting the large-scale foundation
models as prior, to spark more research in the community; (5) An open-source
repository that provides a taxonomy of all mentioned works and code links.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yunfan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yiqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Hao Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yunhao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_H/0/1/0/all/0/1&quot;&gt;Hui Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.13803">
<title>FedIIC: Towards Robust Federated Learning for Class-Imbalanced Medical Image Classification. (arXiv:2206.13803v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2206.13803</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning (FL), training deep models from decentralized data without
privacy leakage, has shown great potential in medical image computing recently.
However, considering the ubiquitous class imbalance in medical data, FL can
exhibit performance degradation, especially for minority classes (e.g. rare
diseases). Existing methods towards this problem mainly focus on training a
balanced classifier to eliminate class prior bias among classes, but neglect to
explore better representation to facilitate classification performance. In this
paper, we present a privacy-preserving FL method named FedIIC to combat class
imbalance from two perspectives: feature learning and classifier learning. In
feature learning, two levels of contrastive learning are designed to extract
better class-specific features with imbalanced data in FL. In classifier
learning, per-class margins are dynamically set according to real-time
difficulty and class priors, which helps the model learn classes equally.
Experimental results on publicly-available datasets demonstrate the superior
performance of FedIIC in dealing with both real-world and simulated
multi-source medical imaging data under class imbalance. Code is available at
https://github.com/wnn2000/FedIIC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_N/0/1/0/all/0/1&quot;&gt;Nannan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Li Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_K/0/1/0/all/0/1&quot;&gt;Kwang-Ting Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zengqiang Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03829">
<title>Early Detection of Bark Beetle Attack Using Remote Sensing and Machine Learning: A Review. (arXiv:2210.03829v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03829</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper provides a comprehensive review of past and current advances in
the early detection of bark beetle-induced tree mortality from three primary
perspectives: bark beetle &amp;amp; host interactions, RS, and ML/DL. In contrast to
prior efforts, this review encompasses all RS systems and emphasizes ML/DL
methods to investigate their strengths and weaknesses. We parse existing
literature based on multi- or hyper-spectral analyses and distill their
knowledge based on: bark beetle species &amp;amp; attack phases with a primary emphasis
on early stages of attacks, host trees, study regions, RS platforms &amp;amp; sensors,
spectral/spatial/temporal resolutions, spectral signatures, spectral vegetation
indices (SVIs), ML approaches, learning schemes, task categories, models,
algorithms, classes/clusters, features, and DL networks &amp;amp; architectures.
Although DL-based methods and the random forest (RF) algorithm showed promising
results, highlighting their potential to detect subtle changes across visible,
thermal, and short-wave infrared (SWIR) spectral regions, they still have
limited effectiveness and high uncertainties. To inspire novel solutions to
these shortcomings, we delve into the principal challenges &amp;amp; opportunities from
different perspectives, enabling a deeper understanding of the current state of
research and guiding future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marvasti_Zadeh_S/0/1/0/all/0/1&quot;&gt;Seyed Mojtaba Marvasti-Zadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodsman_D/0/1/0/all/0/1&quot;&gt;Devin Goodsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ray_N/0/1/0/all/0/1&quot;&gt;Nilanjan Ray&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erbilgin_N/0/1/0/all/0/1&quot;&gt;Nadir Erbilgin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05783">
<title>Unifying Flow, Stereo and Depth Estimation. (arXiv:2211.05783v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05783</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a unified formulation and model for three motion and 3D perception
tasks: optical flow, rectified stereo matching and unrectified stereo depth
estimation from posed images. Unlike previous specialized architectures for
each specific task, we formulate all three tasks as a unified dense
correspondence matching problem, which can be solved with a single model by
directly comparing feature similarities. Such a formulation calls for
discriminative feature representations, which we achieve using a Transformer,
in particular the cross-attention mechanism. We demonstrate that
cross-attention enables integration of knowledge from another image via
cross-view interactions, which greatly improves the quality of the extracted
features. Our unified model naturally enables cross-task transfer since the
model architecture and parameters are shared across tasks. We outperform RAFT
with our unified model on the challenging Sintel dataset, and our final model
that uses a few additional task-specific refinement steps outperforms or
compares favorably to recent state-of-the-art methods on 10 popular flow,
stereo and depth datasets, while being simpler and more efficient in terms of
model design and inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haofei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rezatofighi_H/0/1/0/all/0/1&quot;&gt;Hamid Rezatofighi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fisher Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.08544">
<title>Exploiting the Partly Scratch-off Lottery Ticket for Quantization-Aware Training. (arXiv:2211.08544v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.08544</link>
<description rdf:parseType="Literal">&lt;p&gt;Quantization-aware training (QAT) receives extensive popularity as it well
retains the performance of quantized networks. In QAT, the contemporary
experience is that all quantized weights are updated for an entire training
process. In this paper, this experience is challenged based on an interesting
phenomenon we observed. Specifically, a large portion of quantized weights
reaches the optimal quantization level after a few training epochs, which we
refer to as the partly scratch-off lottery ticket. This
straightforward-yet-valuable observation naturally inspires us to zero out
gradient calculations of these weights in the remaining training period to
avoid meaningless updating. To effectively find the ticket, we develop a
heuristic method, dubbed lottery ticket scratcher (LTS), which freezes a weight
once the distance between the full-precision one and its quantization level is
smaller than a controllable threshold. Surprisingly, the proposed LTS typically
eliminates 50%-70% weight updating and 25%-35% FLOPs of the backward pass,
while still resulting on par with or even better performance than the compared
baseline. For example, compared with the baseline, LTS improves 2-bit
MobileNetV2 by 5.05%, eliminating 46% weight updating and 23% FLOPs of the
backward pass. Code is at url{https://github.com/zysxmu/LTS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yunshan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nan_G/0/1/0/all/0/1&quot;&gt;Gongrui Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_F/0/1/0/all/0/1&quot;&gt;Fei Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15595">
<title>FsaNet: Frequency Self-attention for Semantic Segmentation. (arXiv:2211.15595v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15595</link>
<description rdf:parseType="Literal">&lt;p&gt;Considering the spectral properties of images, we propose a new
self-attention mechanism with highly reduced computational complexity, up to a
linear rate. To better preserve edges while promoting similarity within
objects, we propose individualized processes over different frequency bands. In
particular, we study a case where the process is merely over low-frequency
components. By ablation study, we show that low frequency self-attention can
achieve very close or better performance relative to full frequency even
without retraining the network. Accordingly, we design and embed novel
plug-and-play modules to the head of a CNN network that we refer to as FsaNet.
The frequency self-attention 1) requires only a few low frequency coefficients
as input, 2) can be mathematically equivalent to spatial domain self-attention
with linear structures, 3) simplifies token mapping ($1\times1$ convolution)
stage and token mixing stage simultaneously. We show that frequency
self-attention requires $87.29\% \sim 90.04\%$ less memory, $96.13\% \sim
98.07\%$ less FLOPs, and $97.56\% \sim 98.18\%$ in run time than the regular
self-attention. Compared to other ResNet101-based self-attention networks,
\ourM achieves a new \sArt result ($83.0\%$ mIoU) on Cityscape test dataset and
competitive results on ADE20k and VOCaug. \ourM can also enhance MASK R-CNN for
instance segmentation on COCO. In addition, utilizing the proposed module,
Segformer can be boosted on a series of models with different scales, and
Segformer-B5 can be improved even without retraining. Code is accessible at
\url{https://github.com/zfy-csu/FsaNet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fengyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Panahi_A/0/1/0/all/0/1&quot;&gt;Ashkan Panahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_G/0/1/0/all/0/1&quot;&gt;Guangjun Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01226">
<title>Factor Fields: A Unified Framework for Neural Fields and Beyond. (arXiv:2302.01226v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01226</link>
<description rdf:parseType="Literal">&lt;p&gt;We present Factor Fields, a novel framework for modeling and representing
signals. Factor Fields decomposes a signal into a product of factors, each of
which is represented by a neural or regular field representation operating on a
coordinate transformed input signal. We show that this decomposition yields a
unified framework that generalizes several recent signal representations
including NeRF, PlenOxels, EG3D, Instant-NGP, and TensoRF. Moreover, the
framework allows for the creation of powerful new signal representations, such
as the Coefficient-Basis Factorization (CoBaFa) which we propose in this paper.
As evidenced by our experiments, CoBaFa leads to improvements over previous
fast reconstruction methods in terms of the three critical goals in neural
signal representation: approximation quality, compactness and efficiency.
Experimentally, we demonstrate that our representation achieves better image
approximation quality on 2D image regression tasks, higher geometric quality
when reconstructing 3D signed distance fields and higher compactness for
radiance field reconstruction tasks compared to previous fast reconstruction
methods. Besides, our CoBaFa representation enables generalization by sharing
the basis across signals during training, enabling generalization tasks such as
image regression with sparse observations and few-shot radiance field
reconstruction. Project Page: https://apchenstu.github.io/FactorFields/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Anpei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zexiang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xinyue Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geiger_A/0/1/0/all/0/1&quot;&gt;Andreas Geiger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.12247">
<title>Quantifying &amp; Modeling Multimodal Interactions: An Information Decomposition Framework. (arXiv:2302.12247v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.12247</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent explosion of interest in multimodal applications has resulted in a
wide selection of datasets and methods for representing and integrating
information from different modalities. Despite these empirical advances, there
remain fundamental research questions: How can we quantify the interactions
that are necessary to solve a multimodal task? Subsequently, what are the most
suitable multimodal models to capture these interactions? To answer these
questions, we propose an information-theoretic approach to quantify the degree
of redundancy, uniqueness, and synergy relating input modalities with an output
task. We term these three measures as the PID statistics of a multimodal
distribution (or PID for short), and introduce two new estimators for these PID
statistics that scale to high-dimensional distributions. To validate PID
estimation, we conduct extensive experiments on both synthetic datasets where
the PID is known and on large-scale multimodal benchmarks where PID estimations
are compared with human annotations. Finally, we demonstrate their usefulness
in (1) quantifying interactions within multimodal datasets, (2) quantifying
interactions captured by multimodal models, (3) principled approaches for model
selection, and (4) three real-world case studies engaging with domain experts
in pathology, mood prediction, and robotic perception where our framework helps
to recommend strong multimodal models for each application.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_X/0/1/0/all/0/1&quot;&gt;Xiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_C/0/1/0/all/0/1&quot;&gt;Chun Kai Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nie_S/0/1/0/all/0/1&quot;&gt;Suzanne Nie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Richard Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zihao Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_N/0/1/0/all/0/1&quot;&gt;Nicholas Allen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Auerbach_R/0/1/0/all/0/1&quot;&gt;Randy Auerbach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahmood_F/0/1/0/all/0/1&quot;&gt;Faisal Mahmood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salakhutdinov_R/0/1/0/all/0/1&quot;&gt;Ruslan Salakhutdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morency_L/0/1/0/all/0/1&quot;&gt;Louis-Philippe Morency&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14831">
<title>FacEDiM: A Face Embedding Distribution Model for Few-Shot Biometric Authentication of Cattle. (arXiv:2302.14831v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14831</link>
<description rdf:parseType="Literal">&lt;p&gt;This work proposes to solve the problem of few-shot biometric authentication
by computing the Mahalanobis distance between testing embeddings and a
multivariate Gaussian distribution of training embeddings obtained using
pre-trained CNNs. Experimental results show that models pre-trained on the
ImageNet dataset significantly outperform models pre-trained on human faces.
With a VGG16 model, we obtain a FRR of 1.25% for a FAR of 1.18% on a dataset of
20 cattle identities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oveneke_M/0/1/0/all/0/1&quot;&gt;Meshia C&amp;#xe9;dric Oveneke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaishampayan_R/0/1/0/all/0/1&quot;&gt;Rucha Vaishampayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nsadisa_D/0/1/0/all/0/1&quot;&gt;Deogratias Lukamba Nsadisa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Onya_J/0/1/0/all/0/1&quot;&gt;Jenny Ambukiyenyi Onya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.04068">
<title>VOCALExplore: Pay-as-You-Go Video Data Exploration and Model Building [Technical Report]. (arXiv:2303.04068v3 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2303.04068</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce VOCALExplore, a system designed to support users in building
domain-specific models over video datasets. VOCALExplore supports interactive
labeling sessions and trains models using user-supplied labels. VOCALExplore
maximizes model quality by automatically deciding how to select samples based
on observed skew in the collected labels. It also selects the optimal video
representations to use when training models by casting feature selection as a
rising bandit problem. Finally, VOCALExplore implements optimizations to
achieve low latency without sacrificing model performance. We demonstrate that
VOCALExplore achieves close to the best possible model quality given candidate
acquisition functions and feature extractors, and it does so with low visible
latency (~1 second per iteration) and no expensive preprocessing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Daum_M/0/1/0/all/0/1&quot;&gt;Maureen Daum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_E/0/1/0/all/0/1&quot;&gt;Enhao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_D/0/1/0/all/0/1&quot;&gt;Dong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mussmann_S/0/1/0/all/0/1&quot;&gt;Stephen Mussmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haynes_B/0/1/0/all/0/1&quot;&gt;Brandon Haynes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balazinska_M/0/1/0/all/0/1&quot;&gt;Magdalena Balazinska&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05123">
<title>Dominating Set Database Selection for Visual Place Recognition. (arXiv:2303.05123v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05123</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents an approach for creating a visual place recognition (VPR)
database for localization in indoor environments from RGBD scanning sequences.
The proposed approach is formulated as a minimization problem in terms of
dominating set algorithm for graph, constructed from spatial information, and
referred as DominatingSet. Our algorithm shows better scene coverage in
comparison to other methodologies that are used for database creation. Also, we
demonstrate that using DominatingSet, a database size could be up to 250-1400
times smaller than the original scanning sequence while maintaining a recall
rate of more than 80% on testing sequences. We evaluated our algorithm on
7-scenes and BundleFusion datasets and an additionally recorded sequence in a
highly repetitive office setting. In addition, the database selection can
produce weakly-supervised labels for fine-tuning neural place recognition
algorithms to particular settings, improving even more their accuracy. The
paper also presents a fully automated pipeline for VPR database creation from
RGBD scanning sequences, as well as a set of metrics for VPR database
evaluation. The code and released data are available on our web-page~ --
https://prime-slam.github.io/place-recognition-db/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kornilova_A/0/1/0/all/0/1&quot;&gt;Anastasiia Kornilova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskalenko_I/0/1/0/all/0/1&quot;&gt;Ivan Moskalenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pushkin_T/0/1/0/all/0/1&quot;&gt;Timofei Pushkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tojiboev_F/0/1/0/all/0/1&quot;&gt;Fakhriddin Tojiboev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tariverdizadeh_R/0/1/0/all/0/1&quot;&gt;Rahim Tariverdizadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrer_G/0/1/0/all/0/1&quot;&gt;Gonzalo Ferrer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06040">
<title>Importance of Aligning Training Strategy with Evaluation for Diffusion Models in 3D Multiclass Segmentation. (arXiv:2303.06040v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06040</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, denoising diffusion probabilistic models (DDPM) have been applied
to image segmentation by generating segmentation masks conditioned on images,
while the applications were mainly limited to 2D networks without exploiting
potential benefits from the 3D formulation. In this work, we studied the
DDPM-based segmentation model for 3D multiclass segmentation on two large
multiclass data sets (prostate MR and abdominal CT). We observed that the
difference between training and test methods led to inferior performance for
existing DDPM methods. To mitigate the inconsistency, we proposed a recycling
method which generated corrupted masks based on the model&apos;s prediction at a
previous time step instead of using ground truth. The proposed method achieved
statistically significantly improved performance compared to existing DDPMs,
independent of a number of other techniques for reducing train-test
discrepancy, including performing mask prediction, using Dice loss, and
reducing the number of diffusion time steps during training. The performance of
diffusion models was also competitive and visually similar to
non-diffusion-based U-net, within the same compute budget. The JAX-based
diffusion framework has been released at
https://github.com/mathpluscode/ImgX-DiffSeg.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yunguan Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saeed_S/0/1/0/all/0/1&quot;&gt;Shaheer U. Saeed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Clarkson_M/0/1/0/all/0/1&quot;&gt;Matthew J. Clarkson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yipeng Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06842">
<title>Scene Graph Generation from Hierarchical Relationship Reasoning. (arXiv:2303.06842v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06842</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach for inferring relationships between
objects in visual scenes. It explicitly exploits an informative hierarchical
structure that can be imposed to divide the object and relationship categories
into disjoint super-categories. Specifically, our proposed method incorporates
a Bayes prediction head, enabling joint predictions of the super-category as
the type of relationship between the two objects, along with the detailed
relationship within that super-category. This design reduces the impact of
class imbalance problems. Furthermore, we also modify the supervised
contrastive learning to adapt our hierarchical classification scheme.
Experimental evaluations on the Visual Genome and OpenImage V6 datasets
demonstrate that this factorized approach allows a relatively simple model to
achieve competitive performance, particularly in predicate classification and
zero-shot tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_B/0/1/0/all/0/1&quot;&gt;Bowen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_C/0/1/0/all/0/1&quot;&gt;Camillo J. Taylor&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15435">
<title>The Stable Signature: Rooting Watermarks in Latent Diffusion Models. (arXiv:2303.15435v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15435</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative image modeling enables a wide range of applications but raises
ethical concerns about responsible deployment. This paper introduces an active
strategy combining image watermarking and Latent Diffusion Models. The goal is
for all generated images to conceal an invisible watermark allowing for future
detection and/or identification. The method quickly fine-tunes the latent
decoder of the image generator, conditioned on a binary signature. A
pre-trained watermark extractor recovers the hidden signature from any
generated image and a statistical test then determines whether it comes from
the generative model. We evaluate the invisibility and robustness of the
watermarks on a variety of generation tasks, showing that Stable Signature
works even after the images are modified. For instance, it detects the origin
of an image generated from a text prompt, then cropped to keep $10\%$ of the
content, with $90$+$\%$ accuracy at a false positive rate below 10$^{-6}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernandez_P/0/1/0/all/0/1&quot;&gt;Pierre Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couairon_G/0/1/0/all/0/1&quot;&gt;Guillaume Couairon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegou_H/0/1/0/all/0/1&quot;&gt;Herv&amp;#xe9; J&amp;#xe9;gou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Douze_M/0/1/0/all/0/1&quot;&gt;Matthijs Douze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Furon_T/0/1/0/all/0/1&quot;&gt;Teddy Furon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15446">
<title>SwiftFormer: Efficient Additive Attention for Transformer-based Real-time Mobile Vision Applications. (arXiv:2303.15446v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15446</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-attention has become a defacto choice for capturing global context in
various vision applications. However, its quadratic computational complexity
with respect to image resolution limits its use in real-time applications,
especially for deployment on resource-constrained mobile devices. Although
hybrid approaches have been proposed to combine the advantages of convolutions
and self-attention for a better speed-accuracy trade-off, the expensive matrix
multiplication operations in self-attention remain a bottleneck. In this work,
we introduce a novel efficient additive attention mechanism that effectively
replaces the quadratic matrix multiplication operations with linear
element-wise multiplications. Our design shows that the key-value interaction
can be replaced with a linear layer without sacrificing any accuracy. Unlike
previous state-of-the-art methods, our efficient formulation of self-attention
enables its usage at all stages of the network. Using our proposed efficient
additive attention, we build a series of models called &quot;SwiftFormer&quot; which
achieves state-of-the-art performance in terms of both accuracy and mobile
inference speed. Our small variant achieves 78.5% top-1 ImageNet-1K accuracy
with only 0.8 ms latency on iPhone 14, which is more accurate and 2x faster
compared to MobileViT-v2. Code: https://github.com/Amshaker/SwiftFormer
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaker_A/0/1/0/all/0/1&quot;&gt;Abdelrahman Shaker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maaz_M/0/1/0/all/0/1&quot;&gt;Muhammad Maaz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasheed_H/0/1/0/all/0/1&quot;&gt;Hanoona Rasheed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_S/0/1/0/all/0/1&quot;&gt;Salman Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Ming-Hsuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_F/0/1/0/all/0/1&quot;&gt;Fahad Shahbaz Khan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15786">
<title>HOICLIP: Efficient Knowledge Transfer for HOI Detection with Vision-Language Models. (arXiv:2303.15786v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15786</link>
<description rdf:parseType="Literal">&lt;p&gt;Human-Object Interaction (HOI) detection aims to localize human-object pairs
and recognize their interactions. Recently, Contrastive Language-Image
Pre-training (CLIP) has shown great potential in providing interaction prior
for HOI detectors via knowledge distillation. However, such approaches often
rely on large-scale training data and suffer from inferior performance under
few/zero-shot scenarios. In this paper, we propose a novel HOI detection
framework that efficiently extracts prior knowledge from CLIP and achieves
better generalization. In detail, we first introduce a novel interaction
decoder to extract informative regions in the visual feature map of CLIP via a
cross-attention mechanism, which is then fused with the detection backbone by a
knowledge integration block for more accurate human-object pair detection. In
addition, prior knowledge in CLIP text encoder is leveraged to generate a
classifier by embedding HOI descriptions. To distinguish fine-grained
interactions, we build a verb classifier from training data via visual semantic
arithmetic and a lightweight verb representation adapter. Furthermore, we
propose a training-free enhancement to exploit global HOI predictions from
CLIP. Extensive experiments demonstrate that our method outperforms the state
of the art by a large margin on various settings, e.g. +4.04 mAP on HICO-Det.
The source code is available in https://github.com/Artanic30/HOICLIP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_S/0/1/0/all/0/1&quot;&gt;Shan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_L/0/1/0/all/0/1&quot;&gt;Longtian Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yongfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_X/0/1/0/all/0/1&quot;&gt;Xuming He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17595">
<title>Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts. (arXiv:2303.17595v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17595</link>
<description rdf:parseType="Literal">&lt;p&gt;Supervised learning of image classifiers distills human knowledge into a
parametric model through pairs of images and corresponding labels (X,Y). We
argue that this simple and widely used representation of human knowledge
neglects rich auxiliary information from the annotation procedure, such as the
time-series of mouse traces and clicks left after image selection. Our insight
is that such annotation byproducts Z provide approximate human attention that
weakly guides the model to focus on the foreground cues, reducing spurious
correlations and discouraging shortcut learning. To verify this, we create
ImageNet-AB and COCO-AB. They are ImageNet and COCO training sets enriched with
sample-wise annotation byproducts, collected by replicating the respective
original annotation tasks. We refer to the new paradigm of training models with
annotation byproducts as learning using annotation byproducts (LUAB). We show
that a simple multitask loss for regressing Z together with Y already improves
the generalisability and robustness of the learned models. Compared to the
original supervised learning, LUAB does not require extra annotation costs.
ImageNet-AB and COCO-AB are at https://github.com/naver-ai/NeglectedFreeLunch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_D/0/1/0/all/0/1&quot;&gt;Dongyoon Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choe_J/0/1/0/all/0/1&quot;&gt;Junsuk Choe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Seonghyeok Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;John Joon Young Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_M/0/1/0/all/0/1&quot;&gt;Minsuk Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jean Y. Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_S/0/1/0/all/0/1&quot;&gt;Seong Joon Oh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.08870">
<title>UPGPT: Universal Diffusion Model for Person Image Generation, Editing and Pose Transfer. (arXiv:2304.08870v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.08870</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image models (T2I) such as StableDiffusion have been used to generate
high quality images of people. However, due to the random nature of the
generation process, the person has a different appearance e.g. pose, face, and
clothing, despite using the same text prompt. The appearance inconsistency
makes T2I unsuitable for pose transfer. We address this by proposing a
multimodal diffusion model that accepts text, pose, and visual prompting. Our
model is the first unified method to perform all person image tasks -
generation, pose transfer, and mask-less edit. We also pioneer using small
dimensional 3D body model parameters directly to demonstrate new capability -
simultaneous pose and camera view interpolation while maintaining the person&apos;s
appearance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheong_S/0/1/0/all/0/1&quot;&gt;Soon Yau Cheong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mustafa_A/0/1/0/all/0/1&quot;&gt;Armin Mustafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gilbert_A/0/1/0/all/0/1&quot;&gt;Andrew Gilbert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09630">
<title>Few-shot Medical Image Segmentation via Cross-Reference Transformer. (arXiv:2304.09630v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09630</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models have become the mainstream method for medical image
segmentation, but they require a large manually labeled dataset for training
and are difficult to extend to unseen categories. Few-shot segmentation(FSS)
has the potential to address these challenges by learning new categories from a
small number of labeled samples. The majority of the current methods employ a
prototype learning architecture, which involves expanding support prototype
vectors and concatenating them with query features to conduct conditional
segmentation. However, such framework potentially focuses more on query
features while may neglect the correlation between support and query features.
In this paper, we propose a novel self-supervised few shot medical image
segmentation network with Cross-Reference Transformer, which addresses the lack
of interaction between the support image and the query image. We first enhance
the correlation features between the support set image and the query image
using a bidirectional cross-attention module. Then, we employ a cross-reference
mechanism to mine and enhance the similar parts of support features and query
features in high-dimensional channels. Experimental results show that the
proposed model achieves good results on both CT dataset and MRI dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.05511">
<title>Self-supervised dense representation learning for live-cell microscopy with time arrow prediction. (arXiv:2305.05511v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.05511</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art object detection and segmentation methods for microscopy
images rely on supervised machine learning, which requires laborious manual
annotation of training data. Here we present a self-supervised method based on
time arrow prediction pre-training that learns dense image representations from
raw, unlabeled live-cell microscopy videos. Our method builds upon the task of
predicting the correct order of time-flipped image regions via a single-image
feature extractor followed by a time arrow prediction head that operates on the
fused features. We show that the resulting dense representations capture
inherently time-asymmetric biological processes such as cell divisions on a
pixel-level. We furthermore demonstrate the utility of these representations on
several live-cell microscopy datasets for detection and segmentation of
dividing cells, as well as for cell state classification. Our method
outperforms supervised methods, particularly when only limited ground truth
annotations are available as is commonly the case in practice. We provide code
at https://github.com/weigertlab/tarrow.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallusser_B/0/1/0/all/0/1&quot;&gt;Benjamin Gallusser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stieber_M/0/1/0/all/0/1&quot;&gt;Max Stieber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weigert_M/0/1/0/all/0/1&quot;&gt;Martin Weigert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.06710">
<title>Null-text Guidance in Diffusion Models is Secretly a Cartoon-style Creator. (arXiv:2305.06710v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.06710</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifier-free guidance is an effective sampling technique in diffusion
models that has been widely adopted. The main idea is to extrapolate the model
in the direction of text guidance and away from null-text guidance. In this
paper, we demonstrate that null-text guidance in diffusion models is secretly a
cartoon-style creator, i.e., the generated images can be efficiently
transformed into cartoons by simply perturbing the null-text guidance.
Specifically, we proposed two disturbance methods, i.e., Rollback disturbance
(Back-D) and Image disturbance (Image-D), to construct misalignment between the
noisy images used for predicting null-text guidance and text guidance
(subsequently referred to as \textbf{null-text noisy image} and \textbf{text
noisy image} respectively) in the sampling process. Back-D achieves
cartoonization by altering the noise level of null-text noisy image via
replacing $x_t$ with $x_{t+\Delta t}$. Image-D, alternatively, produces
high-fidelity, diverse cartoons by defining $x_t$ as a clean input image, which
further improves the incorporation of finer image details. Through
comprehensive experiments, we delved into the principle of noise disturbing for
null-text and uncovered that the efficacy of disturbance depends on the
correlation between the null-text noisy image and the source image. Moreover,
our proposed techniques, which can generate cartoon images and cartoonize
specific ones, are training-free and easily integrated as a plug-and-play
component in any classifier-free guided diffusion model. Project page is
available at \url{https://nulltextforcartoon.github.io/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Heliang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Long Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wanrong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenjing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.08000">
<title>DNN-Compressed Domain Visual Recognition with Feature Adaptation. (arXiv:2305.08000v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.08000</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning-based image compression was shown to achieve a competitive
performance with state-of-the-art transform-based codecs. This motivated the
development of new learning-based visual compression standards such as JPEG-AI.
Of particular interest to these emerging standards is the development of
learning-based image compression systems targeting both humans and machines.
This paper is concerned with learning-based compression schemes whose
compressed-domain representations can be utilized to perform visual processing
and computer vision tasks directly in the compressed domain. In our work, we
adopt a learning-based compressed-domain classification framework for
performing visual recognition using the compressed-domain latent representation
at varying bit-rates. We propose a novel feature adaptation module integrating
a lightweight attention model to adaptively emphasize and enhance the key
features within the extracted channel-wise information. Also, we design an
adaptation training strategy to utilize the pretrained pixel-domain weights.
For comparison, in addition to the performance results that are obtained using
our proposed latent-based compressed-domain method, we also present performance
results using compressed but fully decoded images in the pixel domain as well
as original uncompressed images. The obtained performance results show that our
proposed compressed-domain classification model can distinctly outperform the
existing compressed-domain classification models, and that it can also yield
similar accuracy results with a much higher computational efficiency as
compared to the pixel-domain models that are trained using fully decoded
images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Y/0/1/0/all/0/1&quot;&gt;Yingpeng Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karam_L/0/1/0/all/0/1&quot;&gt;Lina J. Karam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11467">
<title>Learning Sequence Descriptor based on Spatio-Temporal Attention for Visual Place Recognition. (arXiv:2305.11467v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11467</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual Place Recognition (VPR) aims to retrieve frames from a geotagged
database that are located at the same place as the query frame. To improve the
robustness of VPR in perceptually aliasing scenarios, sequence-based VPR
methods are proposed. These methods are either based on matching between frame
sequences or extracting sequence descriptors for direct retrieval. However, the
former is usually based on the assumption of constant velocity, which is
difficult to hold in practice, and is computationally expensive and subject to
sequence length. Although the latter overcomes these problems, existing
sequence descriptors are constructed by aggregating features of multiple frames
only, without interaction on temporal information, and thus cannot obtain
descriptors with spatio-temporal discrimination. In this paper, we propose a
sequence descriptor that effectively incorporates spatio-temporal information.
Specifically, spatial attention within the same frame is utilized to learn
spatial feature patterns, while attention in corresponding local regions of
different frames is utilized to learn the persistence or change of features
over time. We use a sliding window to control the temporal range of attention
and use relative position encoding to construct sequential relationships
between different features. This allows our descriptors to capture the
intrinsic dynamics in a sequence of frames. Comprehensive experiments on
challenging benchmark datasets show that the proposed approach outperforms
recent state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Fenglin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Junqiao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Y/0/1/0/all/0/1&quot;&gt;Yingfeng Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_G/0/1/0/all/0/1&quot;&gt;Gengxuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mu_W/0/1/0/all/0/1&quot;&gt;Wenjie Mu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_C/0/1/0/all/0/1&quot;&gt;Chen Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.11990">
<title>Productive Crop Field Detection: A New Dataset and Deep Learning Benchmark Results. (arXiv:2305.11990v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.11990</link>
<description rdf:parseType="Literal">&lt;p&gt;In precision agriculture, detecting productive crop fields is an essential
practice that allows the farmer to evaluate operating performance separately
and compare different seed varieties, pesticides, and fertilizers. However,
manually identifying productive fields is often a time-consuming and
error-prone task. Previous studies explore different methods to detect crop
fields using advanced machine learning algorithms, but they often lack good
quality labeled data. In this context, we propose a high-quality dataset
generated by machine operation combined with Sentinel-2 images tracked over
time. As far as we know, it is the first one to overcome the lack of labeled
samples by using this technique. In sequence, we apply a semi-supervised
classification of unlabeled data and state-of-the-art supervised and
self-supervised deep learning methods to detect productive crop fields
automatically. Finally, the results demonstrate high accuracy in Positive
Unlabeled learning, which perfectly fits the problem where we have high
confidence in the positive samples. Best performances have been found in
Triplet Loss Siamese given the existence of an accurate dataset and Contrastive
Learning considering situations where we do not have a comprehensive labeled
dataset available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nascimento_E/0/1/0/all/0/1&quot;&gt;Eduardo Nascimento&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Just_J/0/1/0/all/0/1&quot;&gt;John Just&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_J/0/1/0/all/0/1&quot;&gt;Jurandy Almeida&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almeida_T/0/1/0/all/0/1&quot;&gt;Tiago Almeida&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14708">
<title>EgoVSR: Towards High-Quality Egocentric Video Super-Resolution. (arXiv:2305.14708v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14708</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the limitations of capture devices and scenarios, egocentric videos
frequently have low visual quality, mainly caused by high compression and
severe motion blur. With the increasing application of egocentric videos, there
is an urgent need to enhance the quality of these videos through
super-resolution. However, existing Video Super-Resolution (VSR) works,
focusing on third-person view videos, are actually unsuitable for handling
blurring artifacts caused by rapid ego-motion and object motion in egocentric
videos. To this end, we propose EgoVSR, a VSR framework specifically designed
for egocentric videos. We explicitly tackle motion blurs in egocentric videos
using a Dual Branch Deblur Network (DB$^2$Net) in the VSR framework. Meanwhile,
a blurring mask is introduced to guide the DB$^2$Net learning, and can be used
to localize blurred areas in video frames. We also design a MaskNet to predict
the mask, as well as a mask loss to optimize the mask estimation. Additionally,
an online motion blur synthesis model for common VSR training data is proposed
to simulate motion blurs as in egocentric videos. In order to validate the
effectiveness of our proposed method, we introduce an EgoVSR dataset containing
a large amount of fast-motion egocentric video sequences. Extensive experiments
demonstrate that our EgoVSR model can efficiently super-resolve low-quality
egocentric videos and outperform strong comparison baselines. Our code,
pre-trained models and data can be found at https://github.com/chiyich/EGOVSR/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yichen Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Junhao Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiamiao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenming Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yapeng Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18120">
<title>TD-GEM: Text-Driven Garment Editing Mapper. (arXiv:2305.18120v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18120</link>
<description rdf:parseType="Literal">&lt;p&gt;Language-based fashion image editing allows users to try out variations of
desired garments through provided text prompts. Inspired by research on
manipulating latent representations in StyleCLIP and HairCLIP, we focus on
these latent spaces for editing fashion items of full-body human datasets.
Currently, there is a gap in handling fashion image editing due to the
complexity of garment shapes and textures and the diversity of human poses. In
this paper, we propose an editing optimizer scheme method called Text-Driven
Garment Editing Mapper (TD-GEM), aiming to edit fashion items in a disentangled
way. To this end, we initially obtain a latent representation of an image
through generative adversarial network inversions such as Encoder for Editing
(e4e) or Pivotal Tuning Inversion (PTI) for more accurate results. An
optimization-based Contrastive Language-Image Pre-training (CLIP) is then
utilized to guide the latent representation of a fashion image in the direction
of a target attribute expressed in terms of a text prompt. Our TD-GEM
manipulates the image accurately according to the target attribute, while other
parts of the image are kept untouched. In the experiments, we evaluate TD-GEM
on two different attributes (i.e., &quot;color&quot; and &quot;sleeve length&quot;), which
effectively generates realistic images compared to the recent manipulation
schemes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dadfar_R/0/1/0/all/0/1&quot;&gt;Reza Dadfar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabzevari_S/0/1/0/all/0/1&quot;&gt;Sanaz Sabzevari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bjorkman_M/0/1/0/all/0/1&quot;&gt;M&amp;#xe5;rten Bj&amp;#xf6;rkman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kragic_D/0/1/0/all/0/1&quot;&gt;Danica Kragic&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01188">
<title>Event-based Stereo Visual Odometry with Native Temporal Resolution via Continuous-time Gaussian Process Regression. (arXiv:2306.01188v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01188</link>
<description rdf:parseType="Literal">&lt;p&gt;Event-based cameras asynchronously capture individual visual changes in a
scene. This makes them more robust than traditional frame-based cameras to
highly dynamic motions and poor illumination. It also means that every
measurement in a scene can occur at a unique time.
&lt;/p&gt;
&lt;p&gt;Handling these different measurement times is a major challenge of using
event-based cameras. It is often addressed in visual odometry (VO) pipelines by
approximating temporally close measurements as occurring at one common time.
This grouping simplifies the estimation problem but, absent additional sensors,
sacrifices the inherent temporal resolution of event-based cameras.
&lt;/p&gt;
&lt;p&gt;This paper instead presents a complete stereo VO pipeline that estimates
directly with individual event-measurement times without requiring any grouping
or approximation in the estimation state. It uses continuous-time trajectory
estimation to maintain the temporal fidelity and asynchronous nature of
event-based cameras through Gaussian process regression with a physically
motivated prior. Its performance is evaluated on the MVSEC dataset, where it
achieves 7.9e-3 and 5.9e-3 RMS relative error on two independent sequences,
outperforming the existing publicly available event-based stereo VO pipeline by
two and four times, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jianeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gammell_J/0/1/0/all/0/1&quot;&gt;Jonathan D. Gammell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.01415">
<title>Learning Landmarks Motion from Speech for Speaker-Agnostic 3D Talking Heads Generation. (arXiv:2306.01415v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.01415</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach for generating 3D talking heads from raw
audio inputs. Our method grounds on the idea that speech related movements can
be comprehensively and efficiently described by the motion of a few control
points located on the movable parts of the face, i.e., landmarks. The
underlying musculoskeletal structure then allows us to learn how their motion
influences the geometrical deformations of the whole face. The proposed method
employs two distinct models to this aim: the first one learns to generate the
motion of a sparse set of landmarks from the given audio. The second model
expands such landmarks motion to a dense motion field, which is utilized to
animate a given 3D mesh in neutral state. Additionally, we introduce a novel
loss function, named Cosine Loss, which minimizes the angle between the
generated motion vectors and the ground truth ones. Using landmarks in 3D
talking head generation offers various advantages such as consistency,
reliability, and obviating the need for manual-annotation. Our approach is
designed to be identity-agnostic, enabling high-quality facial animations for
any users without additional data or training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nocentini_F/0/1/0/all/0/1&quot;&gt;Federico Nocentini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrari_C/0/1/0/all/0/1&quot;&gt;Claudio Ferrari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Berretti_S/0/1/0/all/0/1&quot;&gt;Stefano Berretti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02115">
<title>Table and Image Generation for Investigating Knowledge of Entities in Pre-trained Vision and Language Models. (arXiv:2306.02115v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02115</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a table and image generation task to verify how the
knowledge about entities acquired from natural language is retained in Vision &amp;amp;
Language (V&amp;amp;L) models. This task consists of two parts: the first is to
generate a table containing knowledge about an entity and its related image,
and the second is to generate an image from an entity with a caption and a
table containing related knowledge of the entity. In both tasks, the model must
know the entities used to perform the generation properly. We created the
Wikipedia Table and Image Generation (WikiTIG) dataset from about 200,000
infoboxes in English Wikipedia articles to perform the proposed tasks. We
evaluated the performance on the tasks with respect to the above research
question using the V&amp;amp;L model OFA, which has achieved state-of-the-art results
in multiple tasks. Experimental results show that OFA forgets part of its
entity knowledge by pre-training as a complement to improve the performance of
image related tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamigaito_H/0/1/0/all/0/1&quot;&gt;Hidetaka Kamigaito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayashi_K/0/1/0/all/0/1&quot;&gt;Katsuhiko Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_T/0/1/0/all/0/1&quot;&gt;Taro Watanabe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.17723">
<title>FlipNeRF: Flipped Reflection Rays for Few-shot Novel View Synthesis. (arXiv:2306.17723v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.17723</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) has been a mainstream in novel view synthesis
with its remarkable quality of rendered images and simple architecture.
Although NeRF has been developed in various directions improving continuously
its performance, the necessity of a dense set of multi-view images still exists
as a stumbling block to progress for practical application. In this work, we
propose FlipNeRF, a novel regularization method for few-shot novel view
synthesis by utilizing our proposed flipped reflection rays. The flipped
reflection rays are explicitly derived from the input ray directions and
estimated normal vectors, and play a role of effective additional training rays
while enabling to estimate more accurate surface normals and learn the 3D
geometry effectively. Since the surface normal and the scene depth are both
derived from the estimated densities along a ray, the accurate surface normal
leads to more exact depth estimation, which is a key factor for few-shot novel
view synthesis. Furthermore, with our proposed Uncertainty-aware Emptiness Loss
and Bottleneck Feature Consistency Loss, FlipNeRF is able to estimate more
reliable outputs with reducing floating artifacts effectively across the
different scene structures, and enhance the feature-level consistency between
the pair of the rays cast toward the photo-consistent pixels without any
additional feature extractor, respectively. Our FlipNeRF achieves the SOTA
performance on the multiple benchmarks across all the scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_S/0/1/0/all/0/1&quot;&gt;Seunghyeon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Y/0/1/0/all/0/1&quot;&gt;Yeonjin Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwak_N/0/1/0/all/0/1&quot;&gt;Nojun Kwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00773">
<title>DifFSS: Diffusion Model for Few-Shot Semantic Segmentation. (arXiv:2307.00773v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00773</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have demonstrated excellent performance in image generation.
Although various few-shot semantic segmentation (FSS) models with different
network structures have been proposed, performance improvement has reached a
bottleneck. This paper presents the first work to leverage the diffusion model
for FSS task, called DifFSS. DifFSS, a novel FSS paradigm, can further improve
the performance of the state-of-the-art FSS models by a large margin without
modifying their network structure. Specifically, we utilize the powerful
generation ability of diffusion models to generate diverse auxiliary support
images by using the semantic mask, scribble or soft HED boundary of the support
image as control conditions. This generation process simulates the variety
within the class of the query image, such as color, texture variation,
lighting, $etc$. As a result, FSS models can refer to more diverse support
images, yielding more robust representations, thereby achieving a consistent
improvement in segmentation performance. Extensive experiments on three
publicly available datasets based on existing advanced FSS models demonstrate
the effectiveness of the diffusion model for FSS task. Furthermore, we explore
in detail the impact of different input settings of the diffusion model on
segmentation performance. Hopefully, this completely new paradigm will bring
inspiration to the study of FSS task integrated with AI-generated content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_W/0/1/0/all/0/1&quot;&gt;Weimin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Bo Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02100">
<title>MDViT: Multi-domain Vision Transformer for Small Medical Image Segmentation Datasets. (arXiv:2307.02100v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02100</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite its clinical utility, medical image segmentation (MIS) remains a
daunting task due to images&apos; inherent complexity and variability. Vision
transformers (ViTs) have recently emerged as a promising solution to improve
MIS; however, they require larger training datasets than convolutional neural
networks. To overcome this obstacle, data-efficient ViTs were proposed, but
they are typically trained using a single source of data, which overlooks the
valuable knowledge that could be leveraged from other available datasets.
Naivly combining datasets from different domains can result in negative
knowledge transfer (NKT), i.e., a decrease in model performance on some domains
with non-negligible inter-domain heterogeneity. In this paper, we propose
MDViT, the first multi-domain ViT that includes domain adapters to mitigate
data-hunger and combat NKT by adaptively exploiting knowledge in multiple small
data resources (domains). Further, to enhance representation learning across
domains, we integrate a mutual knowledge distillation paradigm that transfers
knowledge between a universal network (spanning all the domains) and auxiliary
domain-specific branches. Experiments on 4 skin lesion segmentation datasets
show that MDViT outperforms state-of-the-art algorithms, with superior
segmentation performance and a fixed model size, at inference time, even as
more domains are added. Our code is available at
https://github.com/siyi-wind/MDViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_S/0/1/0/all/0/1&quot;&gt;Siyi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bayasi_N/0/1/0/all/0/1&quot;&gt;Nourhan Bayasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harmarneh_G/0/1/0/all/0/1&quot;&gt;Ghassan Harmarneh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garbi_R/0/1/0/all/0/1&quot;&gt;Rafeef Garbi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04192">
<title>SAS Video-QA: Self-Adaptive Sampling for Efficient Video Question-Answering. (arXiv:2307.04192v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04192</link>
<description rdf:parseType="Literal">&lt;p&gt;Video question--answering is a fundamental task in the field of video
understanding. Although current vision--language models (VLMs) equipped with
Video Transformers have enabled temporal modeling and yielded superior results,
they are at the cost of huge computational power and thus too expensive to
deploy in real-time application scenarios. An economical workaround only
samples a small portion of frames to represent the main content of that video
and tune an image--text model on these sampled frames. Recent video
understanding models usually randomly sample a set of frames or clips,
regardless of internal correlations between their visual contents, nor their
relevance to the problem. We argue that such kinds of aimless sampling may omit
the key frames from which the correct answer can be deduced, and the situation
gets worse when the sampling sparsity increases, which always happens as the
video lengths increase. To mitigate this issue, we propose two frame sampling
strategies, namely the most domain frames (MDF) and most implied frames (MIF),
to maximally preserve those frames that are most likely vital to the given
questions. MDF passively minimizes the risk of key frame omission in a
bootstrap manner, while MIS actively searches key frames customized for each
video--question pair with the assistance of auxiliary models. The experimental
results on three public datasets from three advanced VLMs (CLIP, GIT and
All-in-one) demonstrate that our proposed strategies can boost the performance
for image--text pretrained models. The source codes pertaining to the method
proposed in this paper are publicly available at
https://github.com/declare-lab/sas-vqa.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_W/0/1/0/all/0/1&quot;&gt;Wei Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kan_M/0/1/0/all/0/1&quot;&gt;Min-Yen Kan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poria_S/0/1/0/all/0/1&quot;&gt;Soujanya Poria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04956">
<title>PKU-GoodsAD: A Supermarket Goods Dataset for Unsupervised Anomaly Detection and Segmentation. (arXiv:2307.04956v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04956</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual anomaly detection is essential and commonly used for many tasks in the
field of computer vision. Recent anomaly detection datasets mainly focus on
industrial automated inspection, medical image analysis and video surveillance.
In order to broaden the application and research of anomaly detection in
unmanned supermarkets and smart manufacturing, we introduce the supermarket
goods anomaly detection (GoodsAD) dataset. It contains 6124 high-resolution
images of 484 different appearance goods divided into 6 categories. Each
category contains several common different types of anomalies such as
deformation, surface damage and opened. Anomalies contain both texture changes
and structural changes. It follows the unsupervised setting and only normal
(defect-free) images are used for training. Pixel-precise ground truth regions
are provided for all anomalies. Moreover, we also conduct a thorough evaluation
of current state-of-the-art unsupervised anomaly detection methods. This
initial benchmark indicates that some methods which perform well on the
industrial anomaly detection dataset (e.g., MVTec AD), show poor performance on
our dataset. This is a comprehensive, multi-object dataset for supermarket
goods anomaly detection that focuses on real-world applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_R/0/1/0/all/0/1&quot;&gt;Runwei Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ban_M/0/1/0/all/0/1&quot;&gt;Miaoju Ban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Ge Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06281">
<title>MMBench: Is Your Multi-modal Model an All-around Player?. (arXiv:2307.06281v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06281</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models have recently achieved remarkable progress,
exhibiting great perception and reasoning abilities concerning visual
information. However, how to effectively evaluate these large vision-language
models remains a major obstacle, hindering future model development.
Traditional benchmarks like VQAv2 or COCO Caption provide quantitative
performance measurements but suffer from a lack of fine-grained ability
assessment and non-robust evaluation metrics. Recent subjective benchmarks,
such as OwlEval, offer comprehensive evaluations of a model&apos;s abilities by
incorporating human labor, but they are not scalable and display significant
bias. In response to these challenges, we propose MMBench, a novel
multi-modality benchmark. MMBench methodically develops a comprehensive
evaluation pipeline, primarily comprised of two elements. The first element is
a meticulously curated dataset that surpasses existing similar benchmarks in
terms of the number and variety of evaluation questions and abilities. The
second element introduces a novel CircularEval strategy and incorporates the
use of ChatGPT. This implementation is designed to convert free-form
predictions into pre-defined choices, thereby facilitating a more robust
evaluation of the model&apos;s predictions. MMBench is a systematically-designed
objective benchmark for robustly evaluating the various abilities of
vision-language models. We hope MMBench will assist the research community in
better evaluating their models and encourage future advancements in this
domain. Project page: https://opencompass.org.cn/mmbench.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_H/0/1/0/all/0/1&quot;&gt;Haodong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanhan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Songyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wangbo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yike Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiaqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Conghui He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kai Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07928">
<title>Reinforced Disentanglement for Face Swapping without Skip Connection. (arXiv:2307.07928v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07928</link>
<description rdf:parseType="Literal">&lt;p&gt;The SOTA face swap models still suffer the problem of either target identity
(i.e., shape) being leaked or the target non-identity attributes (i.e.,
background, hair) failing to be fully preserved in the final results. We show
that this insufficient disentanglement is caused by two flawed designs that
were commonly adopted in prior models: (1) counting on only one compressed
encoder to represent both the semantic-level non-identity facial
attributes(i.e., pose) and the pixel-level non-facial region details, which is
contradictory to satisfy at the same time; (2) highly relying on long
skip-connections between the encoder and the final generator, leaking a certain
amount of target face identity into the result. To fix them, we introduce a new
face swap framework called &apos;WSC-swap&apos; that gets rid of skip connections and
uses two target encoders to respectively capture the pixel-level non-facial
region attributes and the semantic non-identity attributes in the face region.
To further reinforce the disentanglement learning for the target encoder, we
employ both identity removal loss via adversarial training (i.e., GAN) and the
non-identity preservation loss via prior 3DMM models like [11]. Extensive
experiments on both FaceForensics++ and CelebA-HQ show that our results
significantly outperform previous works on a rich set of metrics, including one
novel metric for measuring identity consistency that was completely neglected
before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaohang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1&quot;&gt;Pengfei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Heung-Yeung Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08579">
<title>Scale-Aware Modulation Meet Transformer. (arXiv:2307.08579v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08579</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new vision Transformer, Scale-Aware Modulation
Transformer (SMT), that can handle various downstream tasks efficiently by
combining the convolutional network and vision Transformer. The proposed
Scale-Aware Modulation (SAM) in the SMT includes two primary novel designs.
Firstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which can
capture multi-scale features and expand the receptive field. Secondly, we
propose the Scale-Aware Aggregation (SAA) module, which is lightweight but
effective, enabling information fusion across different heads. By leveraging
these two modules, convolutional modulation is further enhanced. Furthermore,
in contrast to prior works that utilized modulations throughout all stages to
build an attention-free network, we propose an Evolutionary Hybrid Network
(EHN), which can effectively simulate the shift from capturing local to global
dependencies as the network becomes deeper, resulting in superior performance.
Extensive experiments demonstrate that SMT significantly outperforms existing
state-of-the-art models across a wide range of visual tasks. Specifically, SMT
with 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1
accuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in
224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned with
resolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN,
the SMT base trained with 1x and 3x schedule outperforms the Swin Transformer
counterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentation
with UPerNet, the SMT base test at single- and multi-scale surpasses Swin by
2.0 and 1.1 mIoU respectively on the ADE20K.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_W/0/1/0/all/0/1&quot;&gt;Weifeng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Ziheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiayu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lianwen Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12721">
<title>AMAE: Adaptation of Pre-Trained Masked Autoencoder for Dual-Distribution Anomaly Detection in Chest X-Rays. (arXiv:2307.12721v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12721</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised anomaly detection in medical images such as chest radiographs is
stepping into the spotlight as it mitigates the scarcity of the labor-intensive
and costly expert annotation of anomaly data. However, nearly all existing
methods are formulated as a one-class classification trained only on
representations from the normal class and discard a potentially significant
portion of the unlabeled data. This paper focuses on a more practical setting,
dual distribution anomaly detection for chest X-rays, using the entire training
data, including both normal and unlabeled images. Inspired by a modern
self-supervised vision transformer model trained using partial image inputs to
reconstruct missing image regions -- we propose AMAE, a two-stage algorithm for
adaptation of the pre-trained masked autoencoder (MAE). Starting from MAE
initialization, AMAE first creates synthetic anomalies from only normal
training images and trains a lightweight classifier on frozen transformer
features. Subsequently, we propose an adaptation strategy to leverage unlabeled
images containing anomalies. The adaptation scheme is accomplished by assigning
pseudo-labels to unlabeled images and using two separate MAE based modules to
model the normative and anomalous distributions of pseudo-labeled images. The
effectiveness of the proposed adaptation strategy is evaluated with different
anomaly ratios in an unlabeled training set. AMAE leads to consistent
performance gains over competing self-supervised and dual distribution anomaly
detection methods, setting the new state-of-the-art on three public chest X-ray
benchmarks: RSNA, NIH-CXR, and VinDr-CXR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozorgtabar_B/0/1/0/all/0/1&quot;&gt;Behzad Bozorgtabar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_D/0/1/0/all/0/1&quot;&gt;Dwarikanath Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiran_J/0/1/0/all/0/1&quot;&gt;Jean-Philippe Thiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.12917">
<title>Hierarchical Skeleton Meta-Prototype Contrastive Learning with Hard Skeleton Mining for Unsupervised Person Re-Identification. (arXiv:2307.12917v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.12917</link>
<description rdf:parseType="Literal">&lt;p&gt;With rapid advancements in depth sensors and deep learning, skeleton-based
person re-identification (re-ID) models have recently achieved remarkable
progress with many advantages. Most existing solutions learn single-level
skeleton features from body joints with the assumption of equal skeleton
importance, while they typically lack the ability to exploit more informative
skeleton features from various levels such as limb level with more global body
patterns. The label dependency of these methods also limits their flexibility
in learning more general skeleton representations. This paper proposes a
generic unsupervised Hierarchical skeleton Meta-Prototype Contrastive learning
(Hi-MPC) approach with Hard Skeleton Mining (HSM) for person re-ID with
unlabeled 3D skeletons. Firstly, we construct hierarchical representations of
skeletons to model coarse-to-fine body and motion features from the levels of
body joints, components, and limbs. Then a hierarchical meta-prototype
contrastive learning model is proposed to cluster and contrast the most typical
skeleton features (&quot;prototypes&quot;) from different-level skeletons. By converting
original prototypes into meta-prototypes with multiple homogeneous
transformations, we induce the model to learn the inherent consistency of
prototypes to capture more effective skeleton features for person re-ID.
Furthermore, we devise a hard skeleton mining mechanism to adaptively infer the
informative importance of each skeleton, so as to focus on harder skeletons to
learn more discriminative skeleton representations. Extensive evaluations on
five datasets demonstrate that our approach outperforms a wide variety of
state-of-the-art skeleton-based methods. We further show the general
applicability of our method to cross-view person re-ID and RGB-based scenarios
with estimated skeletons.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_H/0/1/0/all/0/1&quot;&gt;Haocong Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leung_C/0/1/0/all/0/1&quot;&gt;Cyril Leung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_C/0/1/0/all/0/1&quot;&gt;Chunyan Miao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10711">
<title>AdjointDPM: Adjoint Sensitivity Method for Gradient Backpropagation of Diffusion Probabilistic Models. (arXiv:2307.10711v2 [cs.CV] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2307.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing customization methods require access to multiple reference examples
to align pre-trained diffusion probabilistic models (DPMs) with user-provided
concepts. This paper aims to address the challenge of DPM customization when
the only available supervision is a differentiable metric defined on the
generated contents. Since the sampling procedure of DPMs involves recursive
calls to the denoising UNet, na\&quot;ive gradient backpropagation requires storing
the intermediate states of all iterations, resulting in extremely high memory
consumption. To overcome this issue, we propose a novel method AdjointDPM,
which first generates new samples from diffusion models by solving the
corresponding probability-flow ODEs. It then uses the adjoint sensitivity
method to backpropagate the gradients of the loss to the models&apos; parameters
(including conditioning signals, network weights, and initial noises) by
solving another augmented ODE. To reduce numerical errors in both the forward
generation and gradient backpropagation processes, we further reparameterize
the probability-flow ODE and augmented ODE as simple non-stiff ODEs using
exponential integration. Finally, we demonstrate the effectiveness of
AdjointDPM on three interesting tasks: converting visual effects into
identification text embeddings, finetuning DPMs for specific types of
stylization, and optimizing initial noise to generate adversarial samples for
security auditing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiachun Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_V/0/1/0/all/0/1&quot;&gt;Vincent Y. F. Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>