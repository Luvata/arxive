<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-12-10T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04584" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04595" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04610" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04654" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04670" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04690" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04691" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04736" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04762" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04792" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04808" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04820" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04828" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04837" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04839" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04843" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04854" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04862" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04864" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04881" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04902" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04916" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04917" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04945" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04963" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04986" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05019" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05021" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05034" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05043" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05171" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05185" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05195" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.05253" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.07246" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2108.02924" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.04688" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.00385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.01842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.03408" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.09744" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02407" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05066" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.09424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.11090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14938" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00006" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.02204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05415" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.05668" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15657" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.16680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11471" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.14348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.16620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01557" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.04852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.06089" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.13032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.03393" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.07361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13443" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16111" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17098" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17961" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00844" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00858" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00966" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.02366" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.03733" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04474" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2312.04574">
<title>Differentiable Visual Computing for Inverse Problems and Machine Learning. (arXiv:2312.04574v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04574</link>
<description rdf:parseType="Literal">&lt;p&gt;Originally designed for applications in computer graphics, visual computing
(VC) methods synthesize information about physical and virtual worlds, using
prescribed algorithms optimized for spatial computing. VC is used to analyze
geometry, physically simulate solids, fluids, and other media, and render the
world via optical techniques. These fine-tuned computations that operate
explicitly on a given input solve so-called forward problems, VC excels at. By
contrast, deep learning (DL) allows for the construction of general algorithmic
models, side stepping the need for a purely first principles-based approach to
problem solving. DL is powered by highly parameterized neural network
architectures -- universal function approximators -- and gradient-based search
algorithms which can efficiently search that large parameter space for optimal
models. This approach is predicated by neural network differentiability, the
requirement that analytic derivatives of a given problem&apos;s task metric can be
computed with respect to neural network&apos;s parameters. Neural networks excel
when an explicit model is not known, and neural network training solves an
inverse problem in which a model is computed from data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spielberg_A/0/1/0/all/0/1&quot;&gt;Andrew Spielberg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1&quot;&gt;Fangcheng Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rematas_K/0/1/0/all/0/1&quot;&gt;Konstantinos Rematas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jatavallabhula_K/0/1/0/all/0/1&quot;&gt;Krishna Murthy Jatavallabhula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oztireli_C/0/1/0/all/0/1&quot;&gt;Cengiz Oztireli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tzu-Mao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04578">
<title>Towards a Psychological Generalist AI: A Survey of Current Applications of Large Language Models and Future Prospects. (arXiv:2312.04578v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.04578</link>
<description rdf:parseType="Literal">&lt;p&gt;The complexity of psychological principles underscore a significant societal
challenge, given the vast social implications of psychological problems.
Bridging the gap between understanding these principles and their actual
clinical and real-world applications demands rigorous exploration and adept
implementation. In recent times, the swift advancement of highly adaptive and
reusable artificial intelligence (AI) models has emerged as a promising way to
unlock unprecedented capabilities in the realm of psychology. This paper
emphasizes the importance of performance validation for these large-scale AI
models, emphasizing the need to offer a comprehensive assessment of their
verification from diverse perspectives. Moreover, we review the cutting-edge
advancements and practical implementations of these expansive models in
psychology, highlighting pivotal work spanning areas such as social media
analytics, clinical nursing insights, vigilant community monitoring, and the
nuanced exploration of psychological theories. Based on our review, we project
an acceleration in the progress of psychological fields, driven by these
large-scale AI models. These future generalist AI models harbor the potential
to substantially curtail labor costs and alleviate social stress. However, this
forward momentum will not be without its set of challenges, especially when
considering the paradigm changes and upgrades required for medical
instrumentation and related applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_T/0/1/0/all/0/1&quot;&gt;Tianyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_G/0/1/0/all/0/1&quot;&gt;Guanghui Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yijing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_C/0/1/0/all/0/1&quot;&gt;Changwei Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_H/0/1/0/all/0/1&quot;&gt;Hongzhi Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_D/0/1/0/all/0/1&quot;&gt;Dan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Huijing Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bing Xiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04579">
<title>zkFDL: An efficient and privacy-preserving decentralized federated learning with zero knowledge proof. (arXiv:2312.04579v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04579</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated leaning (FL) has been frequently used in various field of studies
and businesses. Traditional centralized FL systems suffer from serious issues.
To address these concerns, decentralized federated learning (DFL) systems have
been introduced in recent years in which with the help of blockchains, try to
achieve more integrity and efficiency. On the other hand, privacy-preserving is
an uncovered part of these systems. To address this, and also scaling the
blockchain-based computations, we propose a zero knowledge proof (ZKP) based
aggregator (zkDFL) that allows clients to share their large-scale model
parameters with a trusted centralized server without revealing their individual
data to other clients. We utilize blockchain technology to manage the
aggregation algorithm via smart contracts. The server performs a ZKP algorithm
to prove to the clients that the aggregation is done according to the accepted
algorithm. The server can also prove that all inputs of clients have been used.
We evaluate our measure through a public dataset about wearable internet of
things. As demonstrated by numerical evaluations, zkDFL introduces
verifiability of correctness of aggregation process and enhances the privacy
protection and scalability of DFL systems, while the gas cost has declined
significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmadi_M/0/1/0/all/0/1&quot;&gt;Mojtaba Ahmadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nourmohammadi_R/0/1/0/all/0/1&quot;&gt;Reza Nourmohammadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04584">
<title>Towards Sample-specific Backdoor Attack with Clean Labels via Attribute Trigger. (arXiv:2312.04584v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04584</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, sample-specific backdoor attacks (SSBAs) are the most advanced and
malicious methods since they can easily circumvent most of the current backdoor
defenses. In this paper, we reveal that SSBAs are not sufficiently stealthy due
to their poisoned-label nature, where users can discover anomalies if they
check the image-label relationship. In particular, we demonstrate that it is
ineffective to directly generalize existing SSBAs to their clean-label variants
by poisoning samples solely from the target class. We reveal that it is
primarily due to two reasons, including \textbf{(1)} the `antagonistic effects&apos;
of ground-truth features and \textbf{(2)} the learning difficulty of
sample-specific features. Accordingly, trigger-related features of existing
SSBAs cannot be effectively learned under the clean-label setting due to their
mild trigger intensity required for ensuring stealthiness. We argue that the
intensity constraint of existing SSBAs is mostly because their trigger patterns
are `content-irrelevant&apos; and therefore act as `noises&apos; for both humans and
DNNs. Motivated by this understanding, we propose to exploit content-relevant
features, $a.k.a.$ (human-relied) attributes, as the trigger patterns to design
clean-label SSBAs. This new attack paradigm is dubbed backdoor attack with
attribute trigger (BAAT). Extensive experiments are conducted on benchmark
datasets, which verify the effectiveness of our BAAT and its resistance to
existing defenses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yiming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Mingyan Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Junfeng Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_T/0/1/0/all/0/1&quot;&gt;Tao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Z/0/1/0/all/0/1&quot;&gt;Zhan Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04590">
<title>Reconciling AI Performance and Data Reconstruction Resilience for Medical Imaging. (arXiv:2312.04590v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04590</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) models are vulnerable to information leakage of
their training data, which can be highly sensitive, for example in medical
imaging. Privacy Enhancing Technologies (PETs), such as Differential Privacy
(DP), aim to circumvent these susceptibilities. DP is the strongest possible
protection for training models while bounding the risks of inferring the
inclusion of training samples or reconstructing the original data. DP achieves
this by setting a quantifiable privacy budget. Although a lower budget
decreases the risk of information leakage, it typically also reduces the
performance of such models. This imposes a trade-off between robust performance
and stringent privacy. Additionally, the interpretation of a privacy budget
remains abstract and challenging to contextualize. In this study, we contrast
the performance of AI models at various privacy budgets against both,
theoretical risk bounds and empirical success of reconstruction attacks. We
show that using very large privacy budgets can render reconstruction attacks
impossible, while drops in performance are negligible. We thus conclude that
not using DP -- at all -- is negligent when applying AI models to sensitive
data. We deem those results to lie a foundation for further debates on striking
a balance between privacy risks and model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ziller_A/0/1/0/all/0/1&quot;&gt;Alexander Ziller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mueller_T/0/1/0/all/0/1&quot;&gt;Tamara T. Mueller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stieger_S/0/1/0/all/0/1&quot;&gt;Simon Stieger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feiner_L/0/1/0/all/0/1&quot;&gt;Leonhard Feiner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brandt_J/0/1/0/all/0/1&quot;&gt;Johannes Brandt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braren_R/0/1/0/all/0/1&quot;&gt;Rickmer Braren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaissis_G/0/1/0/all/0/1&quot;&gt;Georgios Kaissis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04594">
<title>FedGeo: Privacy-Preserving User Next Location Prediction with Federated Learning. (arXiv:2312.04594v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04594</link>
<description rdf:parseType="Literal">&lt;p&gt;A User Next Location Prediction (UNLP) task, which predicts the next location
that a user will move to given his/her trajectory, is an indispensable task for
a wide range of applications. Previous studies using large-scale trajectory
datasets in a single server have achieved remarkable performance in UNLP task.
However, in real-world applications, legal and ethical issues have been raised
regarding privacy concerns leading to restrictions against sharing human
trajectory datasets to any other server. In response, Federated Learning (FL)
has emerged to address the personal privacy issue by collaboratively training
multiple clients (i.e., users) and then aggregating them. While previous
studies employed FL for UNLP, they are still unable to achieve reliable
performance because of the heterogeneity of clients&apos; mobility. To tackle this
problem, we propose the Federated Learning for Geographic Information (FedGeo),
a FL framework specialized for UNLP, which alleviates the heterogeneity of
clients&apos; mobility and guarantees personal privacy protection. Firstly, we
incorporate prior global geographic adjacency information to the local client
model, since the spatial correlation between locations is trained partially in
each client who has only a heterogeneous subset of the overall trajectories in
FL. We also introduce a novel aggregation method that minimizes the gap between
client models to solve the problem of client drift caused by differences
between client models when learning with their heterogeneous data. Lastly, we
probabilistically exclude clients with extremely heterogeneous data from the FL
process by focusing on clients who visit relatively diverse locations. We show
that FedGeo is superior to other FL methods for model performance in UNLP task.
We also validated our model in a real-world application using our own
customers&apos; mobile phones and the FL agent system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_C/0/1/0/all/0/1&quot;&gt;Chung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_T/0/1/0/all/0/1&quot;&gt;Taekyoon Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taesan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_M/0/1/0/all/0/1&quot;&gt;Mincheol Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_J/0/1/0/all/0/1&quot;&gt;Junui Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minsung Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choo_J/0/1/0/all/0/1&quot;&gt;Jaegul Choo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04595">
<title>Evaluating The Accuracy of Classification Algorithms for Detecting Heart Disease Risk. (arXiv:2312.04595v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04595</link>
<description rdf:parseType="Literal">&lt;p&gt;The healthcare industry generates enormous amounts of complex clinical data
that make the prediction of disease detection a complicated process. In medical
informatics, making effective and efficient decisions is very important. Data
Mining (DM) techniques are mainly used to identify and extract hidden patterns
and interesting knowledge to diagnose and predict diseases in medical datasets.
Nowadays, heart disease is considered one of the most important problems in the
healthcare field. Therefore, early diagnosis leads to a reduction in deaths. DM
techniques have proven highly effective for predicting and diagnosing heart
diseases. This work utilizes the classification algorithms with a medical
dataset of heart disease; namely, J48, Random Forest, and Na\&quot;ive Bayes to
discover the accuracy of their performance. We also examine the impact of the
feature selection method. A comparative and analysis study was performed to
determine the best technique using Waikato Environment for Knowledge Analysis
(Weka) software, version 3.8.6. The performance of the utilized algorithms was
evaluated using standard metrics such as accuracy, sensitivity and specificity.
The importance of using classification techniques for heart disease diagnosis
has been highlighted. We also reduced the number of attributes in the dataset,
which showed a significant improvement in prediction accuracy. The results
indicate that the best algorithm for predicting heart disease was Random Forest
with an accuracy of 99.24%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alariyibi_A/0/1/0/all/0/1&quot;&gt;Alhaam Alariyibi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+El_Jarai_M/0/1/0/all/0/1&quot;&gt;Mohamed El-Jarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maatuk_A/0/1/0/all/0/1&quot;&gt;Abdelsalam Maatuk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04601">
<title>Estimating Fr\&apos;echet bounds for validating programmatic weak supervision. (arXiv:2312.04601v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2312.04601</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop methods for estimating Fr\&apos;echet bounds on (possibly
high-dimensional) distribution classes in which some variables are
continuous-valued. We establish the statistical correctness of the computed
bounds under uncertainty in the marginal constraints and demonstrate the
usefulness of our algorithms by evaluating the performance of machine learning
(ML) models trained with programmatic weak supervision (PWS). PWS is a
framework for principled learning from weak supervision inputs (e.g.,
crowdsourced labels, knowledge bases, pre-trained models on related tasks,
etc), and it has achieved remarkable success in many areas of science and
engineering. Unfortunately, it is generally difficult to validate the
performance of ML models trained with PWS due to the absence of labeled data.
Our algorithms address this issue by estimating sharp lower and upper bounds
for performance metrics such as accuracy/recall/precision/F1 score.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Polo_F/0/1/0/all/0/1&quot;&gt;Felipe Maia Polo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Yurochkin_M/0/1/0/all/0/1&quot;&gt;Mikhail Yurochkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Banerjee_M/0/1/0/all/0/1&quot;&gt;Moulinath Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Maity_S/0/1/0/all/0/1&quot;&gt;Subha Maity&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuekai Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04609">
<title>Short-term prediction of construction waste transport activities using AI-Truck. (arXiv:2312.04609v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04609</link>
<description rdf:parseType="Literal">&lt;p&gt;Construction waste hauling trucks (or `slag trucks&apos;) are among the most
commonly seen heavy-duty vehicles in urban streets, which not only produce
significant NOx and PM emissions but are also a major source of on-road and
on-site fugitive dust. Slag trucks are subject to a series of spatial and
temporal access restrictions by local traffic and environmental policies. This
paper addresses the practical problem of predicting slag truck activity at a
city scale during heavy pollution episodes, such that environmental law
enforcement units can take timely and proactive measures against localized
truck aggregation. A deep ensemble learning framework (coined AI-Truck) is
designed, which employs a soft vote integrator that utilizes BI-LSTM, TCN,
STGCN, and PDFormer as base classifiers to predict the level of slag truck
activities at a resolution of 1km$\times$1km, in a 193 km$^2$ area in Chengdu,
China. As a classifier, AI-Truck yields a Macro f1 close to 80\% for 0.5h- and
1h-prediction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Meng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_K/0/1/0/all/0/1&quot;&gt;Ke Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04610">
<title>Data-driven Semi-supervised Machine Learning with Surrogate Safety Measures for Abnormal Driving Behavior Detection. (arXiv:2312.04610v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04610</link>
<description rdf:parseType="Literal">&lt;p&gt;Detecting abnormal driving behavior is critical for road traffic safety and
the evaluation of drivers&apos; behavior. With the advancement of machine learning
(ML) algorithms and the accumulation of naturalistic driving data, many ML
models have been adopted for abnormal driving behavior detection. Most existing
ML-based detectors rely on (fully) supervised ML methods, which require
substantial labeled data. However, ground truth labels are not always available
in the real world, and labeling large amounts of data is tedious. Thus, there
is a need to explore unsupervised or semi-supervised methods to make the
anomaly detection process more feasible and efficient. To fill this research
gap, this study analyzes large-scale real-world data revealing several abnormal
driving behaviors (e.g., sudden acceleration, rapid lane-changing) and develops
a Hierarchical Extreme Learning Machines (HELM) based semi-supervised ML method
using partly labeled data to accurately detect the identified abnormal driving
behaviors. Moreover, previous ML-based approaches predominantly utilize basic
vehicle motion features (such as velocity and acceleration) to label and detect
abnormal driving behaviors, while this study seeks to introduce Surrogate
Safety Measures (SSMs) as the input features for ML models to improve the
detection performance. Results from extensive experiments demonstrate the
effectiveness of the proposed semi-supervised ML model with the introduced SSMs
serving as important features. The proposed semi-supervised ML method
outperforms other baseline semi-supervised or unsupervised methods regarding
various metrics, e.g., delivering the best accuracy at 99.58% and the best F-1
measure at 0.9913. The ablation study further highlights the significance of
SSMs for advancing detection performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lanxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yongqi Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farah_H/0/1/0/all/0/1&quot;&gt;Haneen Farah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zgonnikov_A/0/1/0/all/0/1&quot;&gt;Arkady Zgonnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arem_B/0/1/0/all/0/1&quot;&gt;Bart van Arem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04640">
<title>Autoencoding Labeled Interpolator, Inferring Parameters From Image, And Image From Parameters. (arXiv:2312.04640v1 [astro-ph.HE])</title>
<link>http://arxiv.org/abs/2312.04640</link>
<description rdf:parseType="Literal">&lt;p&gt;The Event Horizon Telescope (EHT) provides an avenue to study black hole
accretion flows on event-horizon scales. Fitting a semi-analytical model to EHT
observations requires the construction of synthetic images, which is
computationally expensive. This study presents an image generation tool in the
form of a generative machine learning model, which extends the capabilities of
a variational autoencoder. This tool can rapidly and continuously interpolate
between a training set of images and can retrieve the defining parameters of
those images. Trained on a set of synthetic black hole images, our tool
showcases success in both interpolating black hole images and their associated
physical parameters. By reducing the computational cost of generating an image,
this tool facilitates parameter estimation and model validation for
observations of black hole system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+SaraerToosi_A/0/1/0/all/0/1&quot;&gt;Ali SaraerToosi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Broderick_A/0/1/0/all/0/1&quot;&gt;Avery Broderick&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04654">
<title>NeuSD: Surface Completion with Multi-View Text-to-Image Diffusion. (arXiv:2312.04654v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04654</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method for 3D surface reconstruction from multiple images
where only a part of the object of interest is captured. Our approach builds on
two recent developments: surface reconstruction using neural radiance fields
for the reconstruction of the visible parts of the surface, and guidance of
pre-trained 2D diffusion models in the form of Score Distillation Sampling
(SDS) to complete the shape in unobserved regions in a plausible manner. We
introduce three components. First, we suggest employing normal maps as a pure
geometric representation for SDS instead of color renderings which are
entangled with the appearance information. Second, we introduce the freezing of
the SDS noise during training which results in more coherent gradients and
better convergence. Third, we propose Multi-View SDS as a way to condition the
generation of the non-observable part of the surface without fine-tuning or
making changes to the underlying 2D Stable Diffusion model. We evaluate our
approach on the BlendedMVS dataset demonstrating significant qualitative and
quantitative improvements over competing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ignatyev_S/0/1/0/all/0/1&quot;&gt;Savva Ignatyev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Selikhanovych_D/0/1/0/all/0/1&quot;&gt;Daniil Selikhanovych&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voynov_O/0/1/0/all/0/1&quot;&gt;Oleg Voynov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yiqun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wonka_P/0/1/0/all/0/1&quot;&gt;Peter Wonka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lefkimmiatis_S/0/1/0/all/0/1&quot;&gt;Stamatios Lefkimmiatis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burnaev_E/0/1/0/all/0/1&quot;&gt;Evgeny Burnaev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04657">
<title>Self-Supervised Behavior Cloned Transformers are Path Crawlers for Text Games. (arXiv:2312.04657v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04657</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce a self-supervised behavior cloning transformer for
text games, which are challenging benchmarks for multi-step reasoning in
virtual environments. Traditionally, Behavior Cloning Transformers excel in
such tasks but rely on supervised training data. Our approach auto-generates
training data by exploring trajectories (defined by common macro-action
sequences) that lead to reward within the games, while determining the
generality and utility of these trajectories by rapidly training small models
then evaluating their performance on unseen development games. Through
empirical analysis, we show our method consistently uncovers generalizable
training data, achieving about 90\% performance of supervised systems across
three benchmark text games.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruoyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jansen_P/0/1/0/all/0/1&quot;&gt;Peter Jansen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04668">
<title>TOD-Flow: Modeling the Structure of Task-Oriented Dialogues. (arXiv:2312.04668v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04668</link>
<description rdf:parseType="Literal">&lt;p&gt;Task-Oriented Dialogue (TOD) systems have become crucial components in
interactive artificial intelligence applications. While recent advances have
capitalized on pre-trained language models (PLMs), they exhibit limitations
regarding transparency and controllability. To address these challenges, we
propose a novel approach focusing on inferring the TOD-Flow graph from dialogue
data annotated with dialog acts, uncovering the underlying task structure in
the form of a graph. The inferred TOD-Flow graph can be easily integrated with
any dialogue model to improve its prediction performance, transparency, and
controllability. Our TOD-Flow graph learns what a model can, should, and should
not predict, effectively reducing the search space and providing a rationale
for the model&apos;s prediction. We show that the proposed TOD-Flow graph better
resembles human-annotated graphs compared to prior approaches. Furthermore,
when combined with several dialogue policies and end-to-end dialogue models, we
demonstrate that our approach significantly improves dialog act classification
and end-to-end response generation performance in the MultiWOZ and SGD
benchmarks. Code available at: https://github.com/srsohn/TOD-Flow
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Sungryull Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_Y/0/1/0/all/0/1&quot;&gt;Yiwei Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anthony Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Logeswaran_L/0/1/0/all/0/1&quot;&gt;Lajanugen Logeswaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dong-Ki Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_D/0/1/0/all/0/1&quot;&gt;Dongsub Shim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Honglak Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04670">
<title>Rapid Motor Adaptation for Robotic Manipulator Arms. (arXiv:2312.04670v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.04670</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing generalizable manipulation skills is a core challenge in embodied
AI. This includes generalization across diverse task configurations,
encompassing variations in object shape, density, friction coefficient, and
external disturbances such as forces applied to the robot. Rapid Motor
Adaptation (RMA) offers a promising solution to this challenge. It posits that
essential hidden variables influencing an agent&apos;s task performance, such as
object mass and shape, can be effectively inferred from the agent&apos;s action and
proprioceptive history. Drawing inspiration from RMA in locomotion and in-hand
rotation, we use depth perception to develop agents tailored for rapid motor
adaptation in a variety of manipulation tasks. We evaluated our agents on four
challenging tasks from the Maniskill2 benchmark, namely pick-and-place
operations with hundreds of objects from the YCB and EGAD datasets, peg
insertion with precise position and orientation, and operating a variety of
faucets and handles, with customized environment variations. Empirical results
demonstrate that our agents surpass state-of-the-art methods like automatic
domain randomization and vision-based policies, obtaining better generalization
performance and sample efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yichao Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ellis_K/0/1/0/all/0/1&quot;&gt;Kevin Ellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o Henriques&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04684">
<title>Latent Skill Discovery for Chain-of-Thought Reasoning. (arXiv:2312.04684v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04684</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Large Language Models (LLMs) have led to an emergent
ability of chain-of-thought (CoT) prompting, a prompt reasoning strategy that
adds intermediate rationale steps between questions and answers to construct
prompts. Conditioned on these prompts, LLMs can effectively learn in context to
generate rationales that lead to more accurate answers than when answering the
same question directly. To design LLM prompts, one important setting, called
demonstration selection, considers selecting demonstrations from an example
bank. Existing methods use various heuristics for this selection, but for CoT
prompting, which involves unique rationales, it is essential to base the
selection upon the intrinsic skills that CoT rationales need, for instance, the
skills of addition or subtraction for math word problems.
&lt;/p&gt;
&lt;p&gt;To address this requirement, we introduce a novel approach named Reasoning
Skill Discovery (RSD) that use unsupervised learning to create a latent space
representation of rationales, called a reasoning skill. Simultaneously, RSD
learns a reasoning policy to determine the required reasoning skill for a given
question. This can then guide the selection of examples that demonstrate the
required reasoning skills. Our approach offers several desirable properties: it
is (1) theoretically grounded, (2) sample-efficient, requiring no LLM inference
or manual prompt design, and (3) LLM-agnostic. Empirically, RSD outperforms
existing methods by up to 6% in terms of the answer accuracy across multiple
reasoning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zifan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haozhu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bespalov_D/0/1/0/all/0/1&quot;&gt;Dmitriy Bespalov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yanjun Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04688">
<title>Federated Learning for 6G: Paradigms, Taxonomy, Recent Advances and Insights. (arXiv:2312.04688v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04688</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) is expected to play an instrumental role in the
next generation of wireless systems, such as sixth-generation (6G) mobile
network. However, massive data, energy consumption, training complexity, and
sensitive data protection in wireless systems are all crucial challenges that
must be addressed for training AI models and gathering intelligence and
knowledge from distributed devices. Federated Learning (FL) is a recent
framework that has emerged as a promising approach for multiple learning agents
to build an accurate and robust machine learning models without sharing raw
data. By allowing mobile handsets and devices to collaboratively learn a global
model without explicit sharing of training data, FL exhibits high privacy and
efficient spectrum utilization. While there are a lot of survey papers
exploring FL paradigms and usability in 6G privacy, none of them has clearly
addressed how FL can be used to improve the protocol stack and wireless
operations. The main goal of this survey is to provide a comprehensive overview
on FL usability to enhance mobile services and enable smart ecosystems to
support novel use-cases. This paper examines the added-value of implementing FL
throughout all levels of the protocol stack. Furthermore, it presents important
FL applications, addresses hot topics, provides valuable insights and explicits
guidance for future research and developments. Our concluding remarks aim to
leverage the synergy between FL and future 6G, while highlighting FL&apos;s
potential to revolutionize wireless industry and sustain the development of
cutting-edge mobile services.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Driss_M/0/1/0/all/0/1&quot;&gt;Maryam Ben Driss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sabir_E/0/1/0/all/0/1&quot;&gt;Essaid Sabir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elbiaze_H/0/1/0/all/0/1&quot;&gt;Halima Elbiaze&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04690">
<title>SynthScribe: Deep Multimodal Tools for Synthesizer Sound Retrieval and Exploration. (arXiv:2312.04690v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.04690</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthesizers are powerful tools that allow musicians to create dynamic and
original sounds. Existing commercial interfaces for synthesizers typically
require musicians to interact with complex low-level parameters or to manage
large libraries of premade sounds. To address these challenges, we implement
SynthScribe -- a fullstack system that uses multimodal deep learning to let
users express their intentions at a much higher level. We implement features
which address a number of difficulties, namely 1) searching through existing
sounds, 2) creating completely new sounds, 3) making meaningful modifications
to a given sound. This is achieved with three main features: a multimodal
search engine for a large library of synthesizer sounds; a user centered
genetic algorithm by which completely new sounds can be created and selected
given the users preferences; a sound editing support feature which highlights
and gives examples for key control parameters with respect to a text or audio
based query. The results of our user studies show SynthScribe is capable of
reliably retrieving and modifying sounds while also affording the ability to
create completely new sounds that expand a musicians creative horizon.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brade_S/0/1/0/all/0/1&quot;&gt;Stephen Brade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bryan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sousa_M/0/1/0/all/0/1&quot;&gt;Mauricio Sousa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newsome_G/0/1/0/all/0/1&quot;&gt;Gregory Lee Newsome&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oore_S/0/1/0/all/0/1&quot;&gt;Sageev Oore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grossman_T/0/1/0/all/0/1&quot;&gt;Tovi Grossman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04691">
<title>Simul-LLM: A Framework for Exploring High-Quality Simultaneous Translation with Large Language Models. (arXiv:2312.04691v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04691</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) with billions of parameters and pretrained on
massive amounts of data are now capable of near or better than state-of-the-art
performance in a variety of downstream natural language processing tasks.
Neural machine translation (NMT) is one such task that LLMs have been applied
to with great success. However, little research has focused on applying LLMs to
the more difficult subset of NMT called simultaneous translation (SimulMT),
where translation begins before the entire source context is available to the
model. In this paper, we address key challenges facing LLMs fine-tuned for
SimulMT, validate classical SimulMT concepts and practices in the context of
LLMs, explore adapting LLMs that are fine-tuned for NMT to the task of SimulMT,
and introduce Simul-LLM, the first open-source fine-tuning and evaluation
pipeline development framework for LLMs focused on SimulMT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agostinelli_V/0/1/0/all/0/1&quot;&gt;Victor Agostinelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wild_M/0/1/0/all/0/1&quot;&gt;Max Wild&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raffel_M/0/1/0/all/0/1&quot;&gt;Matthew Raffel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fuad_K/0/1/0/all/0/1&quot;&gt;Kazi Asif Fuad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lizhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04713">
<title>gcDLSeg: Integrating Graph-cut into Deep Learning for Binary Semantic Segmentation. (arXiv:2312.04713v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04713</link>
<description rdf:parseType="Literal">&lt;p&gt;Binary semantic segmentation in computer vision is a fundamental problem. As
a model-based segmentation method, the graph-cut approach was one of the most
successful binary segmentation methods thanks to its global optimality
guarantee of the solutions and its practical polynomial-time complexity.
Recently, many deep learning (DL) based methods have been developed for this
task and yielded remarkable performance, resulting in a paradigm shift in this
field. To combine the strengths of both approaches, we propose in this study to
integrate the graph-cut approach into a deep learning network for end-to-end
learning. Unfortunately, backward propagation through the graph-cut module in
the DL network is challenging due to the combinatorial nature of the graph-cut
algorithm. To tackle this challenge, we propose a novel residual graph-cut loss
and a quasi-residual connection, enabling the backward propagation of the
gradients of the residual graph-cut loss for effective feature learning guided
by the graph-cut segmentation model. In the inference phase, globally optimal
segmentation is achieved with respect to the graph-cut energy defined on the
optimized image features learned from DL networks. Experiments on the public
AZH chronic wound data set and the pancreas cancer data set from the medical
segmentation decathlon (MSD) demonstrated promising segmentation accuracy, and
improved robustness against adversarial attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Hui Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weiyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ya Xing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Buatti_J/0/1/0/all/0/1&quot;&gt;John Buatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04715">
<title>Deep Emotions Across Languages: A Novel Approach for Sentiment Propagation in Multilingual WordNets. (arXiv:2312.04715v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04715</link>
<description rdf:parseType="Literal">&lt;p&gt;Sentiment analysis involves using WordNets enriched with emotional metadata,
which are valuable resources. However, manual annotation is time-consuming and
expensive, resulting in only a few WordNet Lexical Units being annotated. This
paper introduces two new techniques for automatically propagating sentiment
annotations from a partially annotated WordNet to its entirety and to a WordNet
in a different language: Multilingual Structured Synset Embeddings (MSSE) and
Cross-Lingual Deep Neural Sentiment Propagation (CLDNS). We evaluated the
proposed MSSE+CLDNS method extensively using Princeton WordNet and Polish
WordNet, which have many inter-lingual relations. Our results show that the
MSSE+CLDNS method outperforms existing propagation methods, indicating its
effectiveness in enriching WordNets with emotional metadata across multiple
languages. This work provides a solid foundation for large-scale, multilingual
sentiment analysis and is valuable for academic research and practical
applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1&quot;&gt;Jan Koco&amp;#x144;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04720">
<title>From Big to Small Without Losing It All: Text Augmentation with ChatGPT for Efficient Sentiment Analysis. (arXiv:2312.04720v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04720</link>
<description rdf:parseType="Literal">&lt;p&gt;In the era of artificial intelligence, data is gold but costly to annotate.
The paper demonstrates a groundbreaking solution to this dilemma using ChatGPT
for text augmentation in sentiment analysis. We leverage ChatGPT&apos;s generative
capabilities to create synthetic training data that significantly improves the
performance of smaller models, making them competitive with, or even
outperforming, their larger counterparts. This innovation enables models to be
both efficient and effective, thereby reducing computational cost, inference
time, and memory usage without compromising on quality. Our work marks a key
advancement in the cost-effective development and deployment of robust
sentiment analysis models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozniak_S/0/1/0/all/0/1&quot;&gt;Stanis&amp;#x142;aw Wo&amp;#x17a;niak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kocon_J/0/1/0/all/0/1&quot;&gt;Jan Koco&amp;#x144;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04729">
<title>The Internet of Responsibilities-Connecting Human Responsibilities using Big Data and Blockchain. (arXiv:2312.04729v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2312.04729</link>
<description rdf:parseType="Literal">&lt;p&gt;Accountability in the workplace is critically important and remains a
challenging problem, especially with respect to workplace safety management. In
this paper, we introduce a novel notion, the Internet of Responsibilities, for
accountability management. Our method sorts through the list of
responsibilities with respect to hazardous positions. The positions are
interconnected using directed acyclic graphs (DAGs) indicating the hierarchy of
responsibilities in the organization. In addition, the system detects and
collects responsibilities, and represents risk areas in terms of the positions
of the responsibility nodes. Finally, an automatic reminder and assignment
system is used to enforce a strict responsibility control without human
intervention. Using blockchain technology, we further extend our system with
the capability to store, recover and encrypt responsibility data. We show that
through the application of the Internet of Responsibility network model driven
by Big Data, enterprise and government agencies can attain a highly secured and
safe workplace. Therefore, our model offers a combination of interconnected
responsibilities, accountability, monitoring, and safety which is crucial for
the protection of employees and the success of organizations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xuejiao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jiong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toure_I/0/1/0/all/0/1&quot;&gt;Ibrahim Toure&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messina_E/0/1/0/all/0/1&quot;&gt;Enza Messina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xueping Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuebing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Sheng Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04730">
<title>DeceptPrompt: Exploiting LLM-driven Code Generation via Adversarial Natural Language Instructions. (arXiv:2312.04730v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04730</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of Large Language Models (LLMs), significant progress
has been made in code generation, enabling LLMs to transform natural language
into programming code. These Code LLMs have been widely accepted by massive
users and organizations. However, a dangerous nature is hidden in the code,
which is the existence of fatal vulnerabilities. While some LLM providers have
attempted to address these issues by aligning with human guidance, these
efforts fall short of making Code LLMs practical and robust. Without a deep
understanding of the performance of the LLMs under the practical worst cases,
it would be concerning to apply them to various real-world applications. In
this paper, we answer the critical issue: Are existing Code LLMs immune to
generating vulnerable code? If not, what is the possible maximum severity of
this issue in practical deployment scenarios? In this paper, we introduce
DeceptPrompt, a novel algorithm that can generate adversarial natural language
instructions that drive the Code LLMs to generate functionality correct code
with vulnerabilities. DeceptPrompt is achieved through a systematic
evolution-based algorithm with a fine grain loss design. The unique advantage
of DeceptPrompt enables us to find natural prefix/suffix with totally benign
and non-directional semantic meaning, meanwhile, having great power in inducing
the Code LLMs to generate vulnerable code. This feature can enable us to
conduct the almost-worstcase red-teaming on these LLMs in a real scenario,
where users are using natural language. Our extensive experiments and analyses
on DeceptPrompt not only validate the effectiveness of our approach but also
shed light on the huge weakness of LLMs in the code generation task. When
applying the optimized prefix/suffix, the attack success rate (ASR) will
improve by average 50% compared with no prefix/suffix applying.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fangzhou Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaogeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04731">
<title>STraceBERT: Source Code Retrieval using Semantic Application Traces. (arXiv:2312.04731v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.04731</link>
<description rdf:parseType="Literal">&lt;p&gt;Software reverse engineering is an essential task in software engineering and
security, but it can be a challenging process, especially for adversarial
artifacts. To address this challenge, we present STraceBERT, a novel approach
that utilizes a Java dynamic analysis tool to record calls to core Java
libraries, and pretrain a BERT-style model on the recorded application traces
for effective method source code retrieval from a candidate set. Our
experiments demonstrate the effectiveness of STraceBERT in retrieving the
source code compared to existing approaches. Our proposed approach offers a
promising solution to the problem of code retrieval in software reverse
engineering and opens up new avenues for further research in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spiess_C/0/1/0/all/0/1&quot;&gt;Claudio Spiess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04736">
<title>Is Feedback All You Need? Leveraging Natural Language Feedback in Goal-Conditioned Reinforcement Learning. (arXiv:2312.04736v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04736</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite numerous successes, the field of reinforcement learning (RL) remains
far from matching the impressive generalisation power of human behaviour
learning. One possible way to help bridge this gap be to provide RL agents with
richer, more human-like feedback expressed in natural language. To investigate
this idea, we first extend BabyAI to automatically generate language feedback
from the environment dynamics and goal condition success. Then, we modify the
Decision Transformer architecture to take advantage of this additional signal.
We find that training with language feedback either in place of or in addition
to the return-to-go or goal descriptions improves agents&apos; generalisation
performance, and that agents can benefit from feedback even when this is only
available during training, but not at inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCallum_S/0/1/0/all/0/1&quot;&gt;Sabrina McCallum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_Davies_M/0/1/0/all/0/1&quot;&gt;Max Taylor-Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Albrecht_S/0/1/0/all/0/1&quot;&gt;Stefano V. Albrecht&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Suglia_A/0/1/0/all/0/1&quot;&gt;Alessandro Suglia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04737">
<title>Efficient Large Language Models Fine-Tuning On Graphs. (arXiv:2312.04737v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04737</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning from Text-Attributed Graphs (TAGs) has attracted significant
attention due to its wide range of real-world applications. The rapid evolution
of large language models (LLMs) has revolutionized the way we process textual
data, which indicates a strong potential to replace shallow text embedding
generally used in Graph Neural Networks (GNNs). However, we find that existing
LLM approaches that exploit text information in graphs suffer from inferior
computation and data efficiency. In this work, we introduce a novel and
efficient approach for the end-to-end fine-tuning of Large Language Models
(LLMs) on TAGs, named LEADING. The proposed approach maintains computation cost
and memory overhead comparable to the graph-less fine-tuning of LLMs. Moreover,
it transfers the rick knowledge in LLMs to downstream graph learning tasks
effectively with limited labeled data in semi-supervised learning. Its superior
computation and data efficiency are demonstrated through comprehensive
experiments, offering a promising solution for a wide range of LLMs and graph
learning tasks on TAGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_R/0/1/0/all/0/1&quot;&gt;Rui Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_X/0/1/0/all/0/1&quot;&gt;Xipeng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_R/0/1/0/all/0/1&quot;&gt;Ruozhou Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaorui Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04740">
<title>Train &apos;n Trade: Foundations of Parameter Markets. (arXiv:2312.04740v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04740</link>
<description rdf:parseType="Literal">&lt;p&gt;Organizations typically train large models individually. This is costly and
time-consuming, particularly for large-scale foundation models. Such vertical
production is known to be suboptimal. Inspired by this economic insight, we ask
whether it is possible to leverage others&apos; expertise by trading the constituent
parts in models, i.e., sets of weights, as if they were market commodities.
While recent advances in aligning and interpolating models suggest that doing
so may be possible, a number of fundamental questions must be answered to
create viable parameter markets. In this work, we address these basic
questions, propose a framework containing the infrastructure necessary for
market operations to take place, study strategies for exchanging parameters,
and offer means for agents to monetize parameters. Excitingly, compared to
agents who train siloed models from scratch, we show that it is possible to
mutually gain by using the market, even in competitive settings. This suggests
that the notion of parameter markets may be a useful paradigm for improving
large-scale model training in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tzu-Heng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwakarma_H/0/1/0/all/0/1&quot;&gt;Harit Vishwakarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04746">
<title>Quilt-LLaVA: Visual Instruction Tuning by Extracting Localized Narratives from Open-Source Histopathology Videos. (arXiv:2312.04746v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04746</link>
<description rdf:parseType="Literal">&lt;p&gt;The gigapixel scale of whole slide images (WSIs) poses a challenge for
histopathology multi-modal chatbots, requiring a global WSI analysis for
diagnosis, compounding evidence from different WSI patches. Current visual
instruction datasets, generated through large language models, focus on
creating question/answer pairs for individual image patches, which may lack
diagnostic capacity on their own in histopathology, further complicated by the
absence of spatial grounding in histopathology image captions. To bridge this
gap, we introduce Quilt-Instruct, a large-scale dataset of 107,131
histopathology-specific instruction question/answer pairs, that is collected by
leveraging educational histopathology videos from YouTube, which provides
spatial localization of captions by automatically extracting narrators&apos; cursor
movements. In addition, we provide contextual reasoning by extracting diagnosis
and supporting facts from the entire video content to guide the extrapolative
reasoning of GPT-4. Using Quilt-Instruct, we train Quilt-LLaVA, which can
reason beyond the given single image patch, enabling diagnostic reasoning and
the capability of spatial awareness. To evaluate Quilt-LLaVA, we propose a
comprehensive evaluation dataset created from 985 images and 1283
human-generated question-answers. We also thoroughly evaluate Quilt-LLaVA using
public histopathology datasets, where Quilt-LLaVA significantly outperforms
SOTA by over 10% on relative GPT-4 score and 4% and 9% on open and closed set
VQA. Our code, data, and model are publicly available at quilt-llava.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seyfioglu_M/0/1/0/all/0/1&quot;&gt;Mehmet Saygin Seyfioglu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ikezogwo_W/0/1/0/all/0/1&quot;&gt;Wisdom O. Ikezogwo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghezloo_F/0/1/0/all/0/1&quot;&gt;Fatemeh Ghezloo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishna_R/0/1/0/all/0/1&quot;&gt;Ranjay Krishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapiro_L/0/1/0/all/0/1&quot;&gt;Linda Shapiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04748">
<title>Forcing Generative Models to Degenerate Ones: The Power of Data Poisoning Attacks. (arXiv:2312.04748v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04748</link>
<description rdf:parseType="Literal">&lt;p&gt;Growing applications of large language models (LLMs) trained by a third party
raise serious concerns on the security vulnerability of LLMs.It has been
demonstrated that malicious actors can covertly exploit these vulnerabilities
in LLMs through poisoning attacks aimed at generating undesirable outputs.
While poisoning attacks have received significant attention in the image domain
(e.g., object detection), and classification tasks, their implications for
generative models, particularly in the realm of natural language generation
(NLG) tasks, remain poorly understood. To bridge this gap, we perform a
comprehensive exploration of various poisoning techniques to assess their
effectiveness across a range of generative tasks. Furthermore, we introduce a
range of metrics designed to quantify the success and stealthiness of poisoning
attacks specifically tailored to NLG tasks. Through extensive experiments on
multiple NLG tasks, LLMs and datasets, we show that it is possible to
successfully poison an LLM during the fine-tuning stage using as little as 1\%
of the total tuning data samples. Our paper presents the first systematic
approach to comprehend poisoning attacks targeting NLG tasks considering a wide
range of triggers and attack settings. We hope our findings will assist the AI
security community in devising appropriate defenses against such threats.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Shuli Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kadhe_S/0/1/0/all/0/1&quot;&gt;Swanand Ravindra Kadhe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_L/0/1/0/all/0/1&quot;&gt;Ling Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baracaldo_N/0/1/0/all/0/1&quot;&gt;Nathalie Baracaldo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04762">
<title>The Graph Lottery Ticket Hypothesis: Finding Sparse, Informative Graph Structure. (arXiv:2312.04762v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04762</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph learning methods help utilize implicit relationships among data items,
thereby reducing training label requirements and improving task performance.
However, determining the optimal graph structure for a particular learning task
remains a challenging research problem.
&lt;/p&gt;
&lt;p&gt;In this work, we introduce the Graph Lottery Ticket (GLT) Hypothesis - that
there is an extremely sparse backbone for every graph, and that graph learning
algorithms attain comparable performance when trained on that subgraph as on
the full graph. We identify and systematically study 8 key metrics of interest
that directly influence the performance of graph learning algorithms.
Subsequently, we define the notion of a &quot;winning ticket&quot; for graph structure -
an extremely sparse subset of edges that can deliver a robust approximation of
the entire graph&apos;s performance. We propose a straightforward and efficient
algorithm for finding these GLTs in arbitrary graphs. Empirically, we observe
that performance of different graph learning algorithms can be matched or even
exceeded on graphs with the average degree as low as 5.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsitsulin_A/0/1/0/all/0/1&quot;&gt;Anton Tsitsulin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perozzi_B/0/1/0/all/0/1&quot;&gt;Bryan Perozzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04772">
<title>Remembering to Be Fair: On Non-Markovian Fairness in Sequential DecisionMaking (Preliminary Report). (arXiv:2312.04772v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.04772</link>
<description rdf:parseType="Literal">&lt;p&gt;Fair decision making has largely been studied with respect to a single
decision. In this paper we investigate the notion of fairness in the context of
sequential decision making where multiple stakeholders can be affected by the
outcomes of decisions, and where decision making may be informed by additional
constraints and criteria beyond the requirement of fairness. In this setting,
we observe that fairness often depends on the history of the sequential
decision-making process and not just on the current state. To advance our
understanding of this class of fairness problems, we define the notion of
non-Markovian fairness in the context of sequential decision making. We
identify properties of non-Markovian fairness, including notions of long-term,
anytime, periodic, and bounded fairness. We further explore the interplay
between non-Markovian fairness and memory, and how this can support
construction of fair policies in sequential decision-making settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alamdari_P/0/1/0/all/0/1&quot;&gt;Parand A. Alamdari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klassen_T/0/1/0/all/0/1&quot;&gt;Toryn Q. Klassen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Creager_E/0/1/0/all/0/1&quot;&gt;Elliot Creager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McIlraith_S/0/1/0/all/0/1&quot;&gt;Sheila A. McIlraith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04780">
<title>Fine-Tuning InstructPix2Pix for Advanced Image Colorization. (arXiv:2312.04780v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04780</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel approach to human image colorization by
fine-tuning the InstructPix2Pix model, which integrates a language model
(GPT-3) with a text-to-image model (Stable Diffusion). Despite the original
InstructPix2Pix model&apos;s proficiency in editing images based on textual
instructions, it exhibits limitations in the focused domain of colorization. To
address this, we fine-tuned the model using the IMDB-WIKI dataset, pairing
black-and-white images with a diverse set of colorization prompts generated by
ChatGPT. This paper contributes by (1) applying fine-tuning techniques to
stable diffusion models specifically for colorization tasks, and (2) employing
generative models to create varied conditioning prompts. After finetuning, our
model outperforms the original InstructPix2Pix model on multiple metrics
quantitatively, and we produce more realistically colored images qualitatively.
The code for this project is provided on the GitHub Repository
https://github.com/AllenAnZifeng/DeepLearning282.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_Z/0/1/0/all/0/1&quot;&gt;Zifeng An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zijing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_E/0/1/0/all/0/1&quot;&gt;Eric Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Q/0/1/0/all/0/1&quot;&gt;Qi Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04792">
<title>AI safety by debate via regret minimization. (arXiv:2312.04792v1 [cs.GT])</title>
<link>http://arxiv.org/abs/2312.04792</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the setting of AI safety by debate as a repeated game. We
consider the question of efficient regret minimization in this setting, when
the players are either AIs or humans, equipped with access to computationally
superior AIs. In such a setting, we characterize when internal and external
regret can be minimized efficiently. We conclude with conditions in which a
sequence of strategies converges to a correlated equilibrium.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_A/0/1/0/all/0/1&quot;&gt;Angelica Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_D/0/1/0/all/0/1&quot;&gt;Dean Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazan_E/0/1/0/all/0/1&quot;&gt;Elad Hazan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04799">
<title>An Overview of MLCommons Cloud Mask Benchmark: Related Research and Data. (arXiv:2312.04799v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2312.04799</link>
<description rdf:parseType="Literal">&lt;p&gt;Cloud masking is a crucial task that is well-motivated for meteorology and
its applications in environmental and atmospheric sciences. Its goal is, given
satellite images, to accurately generate cloud masks that identify each pixel
in image to contain either cloud or clear sky. In this paper, we summarize some
of the ongoing research activities in cloud masking, with a focus on the
research and benchmark currently conducted in MLCommons Science Working Group.
This overview is produced with the hope that others will have an easier time
getting started and collaborate on the activities related to MLCommons Cloud
Mask Benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laszewski_G/0/1/0/all/0/1&quot;&gt;Gregor von Laszewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_R/0/1/0/all/0/1&quot;&gt;Ruochen Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04805">
<title>Development and Assessment of Autonomous Vehicles in Both Fully Automated and Mixed Traffic Conditions. (arXiv:2312.04805v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2312.04805</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Vehicle (AV) technology is advancing rapidly, promising a
significant shift in road transportation safety and potentially resolving
various complex transportation issues. With the increasing deployment of AVs by
various companies, questions emerge about how AVs interact with each other and
with human drivers, especially when AVs are prevalent on the roads. Ensuring
cooperative interaction between AVs and between AVs and human drivers is
critical, though there are concerns about possible negative competitive
behaviors. This paper presents a multi-stage approach, starting with the
development of a single AV and progressing to connected AVs, incorporating
sharing and caring V2V communication strategy to enhance mutual coordination. A
survey is conducted to validate the driving performance of the AV and will be
utilized for a mixed traffic case study, which focuses on how the human drivers
will react to the AV driving alongside them on the same road. Results show that
using deep reinforcement learning, the AV acquired driving behavior that
reached human driving performance. The adoption of sharing and caring based V2V
communication within AV networks enhances their driving behavior, aids in more
effective action planning, and promotes collaborative behavior amongst the AVs.
The survey shows that safety in mixed traffic cannot be guaranteed, as we
cannot control human ego-driven actions if they decide to compete with AV.
Consequently, this paper advocates for enhanced research into the safe
incorporation of AVs on public roads.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelrahman_A/0/1/0/all/0/1&quot;&gt;Ahmed Abdelrahman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04808">
<title>A Review On Table Recognition Based On Deep Learning. (arXiv:2312.04808v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04808</link>
<description rdf:parseType="Literal">&lt;p&gt;Table recognition is using the computer to automatically understand the
table, to detect the position of the table from the document or picture, and to
correctly extract and identify the internal structure and content of the table.
After earlier mainstream approaches based on heuristic rules and machine
learning, the development of deep learning techniques has brought a new
paradigm to this field. This review mainly discusses the table recognition
problem from five aspects. The first part introduces data sets, benchmarks, and
commonly used evaluation indicators. This section selects representative data
sets, benchmarks, and evaluation indicators that are frequently used by
researchers. The second part introduces the table recognition model. This
survey introduces the development of the table recognition model, especially
the table recognition model based on deep learning. It is generally accepted
that table recognition is divided into two stages: table detection and table
structure recognition. This section introduces the models that follow this
paradigm (TD and TSR). The third part is the End-to-End method, this section
introduces some scholars&apos; attempts to use an end-to-end approach to solve the
table recognition problem once and for all and the part are Data-centric
methods, such as data augmentation, aligning benchmarks, and other methods. The
fourth part is the data-centric approach, such as data enhancement, alignment
benchmark, and so on. The fifth part summarizes and compares the experimental
data in the field of form recognition, and analyzes the mainstream and more
advantageous methods. Finally, this paper also discusses the possible
development direction and trend of form processing in the future, to provide
some ideas for researchers in the field of table recognition. (Resource will be
released at https://github.com/Wa1den-jy/Topic-on-Table-Recognition .)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiyuan_S/0/1/0/all/0/1&quot;&gt;Shi Jiyuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+chunqi_S/0/1/0/all/0/1&quot;&gt;Shi chunqi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04820">
<title>Learn to Optimize Denoising Scores for 3D Generation: A Unified and Improved Diffusion Prior on NeRF and 3D Gaussian Splatting. (arXiv:2312.04820v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04820</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a unified framework aimed at enhancing the diffusion priors for 3D
generation tasks. Despite the critical importance of these tasks, existing
methodologies often struggle to generate high-caliber results. We begin by
examining the inherent limitations in previous diffusion priors. We identify a
divergence between the diffusion priors and the training procedures of
diffusion models that substantially impairs the quality of 3D generation. To
address this issue, we propose a novel, unified framework that iteratively
optimizes both the 3D model and the diffusion prior. Leveraging the different
learnable parameters of the diffusion prior, our approach offers multiple
configurations, affording various trade-offs between performance and
implementation complexity. Notably, our experimental results demonstrate that
our method markedly surpasses existing techniques, establishing new
state-of-the-art in the realm of text-to-3D generation. Furthermore, our
approach exhibits impressive performance on both NeRF and the newly introduced
3D Gaussian Splatting backbones. Additionally, our framework yields insightful
contributions to the understanding of recent score distillation methods, such
as the VSD and DDS loss.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaofeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yiwen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cheng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yi Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xulei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_F/0/1/0/all/0/1&quot;&gt;Fayao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guosheng Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04821">
<title>Unify Change Point Detection and Segment Classification in a Regression Task for Transportation Mode Identification. (arXiv:2312.04821v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04821</link>
<description rdf:parseType="Literal">&lt;p&gt;Identifying travelers&apos; transportation modes is important in transportation
science and location-based services. It&apos;s appealing for researchers to leverage
GPS trajectory data to infer transportation modes with the popularity of
GPS-enabled devices, e.g., smart phones. Existing studies frame this problem as
classification task. The dominant two-stage studies divide the trip into
single-one mode segments first and then categorize these segments. The over
segmentation strategy and inevitable error propagation bring difficulties to
classification stage and make optimizing the whole system hard. The recent
one-stage works throw out trajectory segmentation entirely to avoid these by
directly conducting point-wise classification for the trip, whereas leaving
predictions dis-continuous. To solve above-mentioned problems, inspired by YOLO
and SSD in object detection, we propose to reframe change point detection and
segment classification as a unified regression task instead of the existing
classification task. We directly regress coordinates of change points and
classify associated segments. In this way, our method divides the trip into
segments under a supervised manner and leverage more contextual information,
obtaining predictions with high accuracy and continuity. Two frameworks,
TrajYOLO and TrajSSD, are proposed to solve the regression task and various
feature extraction backbones are exploited. Exhaustive experiments on GeoLife
dataset show that the proposed method has competitive overall identification
accuracy of 0.853 when distinguishing five modes: walk, bike, bus, car, train.
As for change point detection, our method increases precision at the cost of
drop in recall. All codes are available at
https://github.com/RadetzkyLi/TrajYOLO-SSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rongsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_X/0/1/0/all/0/1&quot;&gt;Xin Pei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04823">
<title>Assessing Neural Network Representations During Training Using Noise-Resilient Diffusion Spectral Entropy. (arXiv:2312.04823v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04823</link>
<description rdf:parseType="Literal">&lt;p&gt;Entropy and mutual information in neural networks provide rich information on
the learning process, but they have proven difficult to compute reliably in
high dimensions. Indeed, in noisy and high-dimensional data, traditional
estimates in ambient dimensions approach a fixed entropy and are prohibitively
hard to compute. To address these issues, we leverage data geometry to access
the underlying manifold and reliably compute these information-theoretic
measures. Specifically, we define diffusion spectral entropy (DSE) in neural
representations of a dataset as well as diffusion spectral mutual information
(DSMI) between different variables representing data. First, we show that they
form noise-resistant measures of intrinsic dimensionality and relationship
strength in high-dimensional simulated data that outperform classic Shannon
entropy, nonparametric estimation, and mutual information neural estimation
(MINE). We then study the evolution of representations in classification
networks with supervised learning, self-supervision, or overfitting. We observe
that (1) DSE of neural representations increases during training; (2) DSMI with
the class label increases during generalizable learning but stays stagnant
during overfitting; (3) DSMI with the input signal shows differing trends: on
MNIST it increases, while on CIFAR-10 and STL-10 it decreases. Finally, we show
that DSE can be used to guide better network initialization and that DSMI can
be used to predict downstream classification accuracy across 962 models on
ImageNet. The official implementation is available at
https://github.com/ChenLiu-1996/DiffusionSpectralEntropy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liao_D/0/1/0/all/0/1&quot;&gt;Danqi Liao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Christensen_B/0/1/0/all/0/1&quot;&gt;Benjamin W. Christensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_A/0/1/0/all/0/1&quot;&gt;Alexander Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huguet_G/0/1/0/all/0/1&quot;&gt;Guillaume Huguet&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1&quot;&gt;Guy Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nickel_M/0/1/0/all/0/1&quot;&gt;Maximilian Nickel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adelstein_I/0/1/0/all/0/1&quot;&gt;Ian Adelstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krishnaswamy_S/0/1/0/all/0/1&quot;&gt;Smita Krishnaswamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04828">
<title>HuRef: HUman-REadable Fingerprint for Large Language Models. (arXiv:2312.04828v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04828</link>
<description rdf:parseType="Literal">&lt;p&gt;Protecting the copyright of large language models (LLMs) has become crucial
due to their resource-intensive training and accompanying carefully designed
licenses. However, identifying the original base model of an LLM is challenging
due to potential parameter alterations through fine-tuning or continued
pretraining. In this study, we introduce HuRef, a human-readable fingerprint
for LLMs that uniquely identifies the base model without exposing model
parameters or interfering with training. We first observe that the vector
direction of LLM parameters remains stable after the model has converged during
pretraining, showing negligible perturbations through subsequent training
steps, including continued pretraining, supervised fine-tuning (SFT), and RLHF,
which makes it a sufficient condition to identify the base model. The necessity
is validated by continuing to train an LLM with an extra term to drive away the
model parameters&apos; direction and the model becomes damaged. However, this
direction is vulnerable to simple attacks like dimension permutation or matrix
rotation, which significantly change it without affecting performance. To
address this, leveraging the Transformer structure, we systematically analyze
potential attacks and define three invariant terms that identify an LLM&apos;s base
model. We make these invariant terms human-readable by mapping them to a
Gaussian vector using a convolutional encoder and then converting it into a
natural image with StyleGAN2. Our method generates a dog image as an identity
fingerprint for an LLM, where the dog&apos;s appearance strongly indicates the LLM&apos;s
base model. Experimental results across various LLMs demonstrate the
effectiveness of our method, the generated dog image remains invariant to
different training steps, including SFT, RLHF, or even continued pretraining
with augmented vocabulary in a new language.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_B/0/1/0/all/0/1&quot;&gt;Boyi Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chenghu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinbing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhouhan Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04836">
<title>Thermodynamic Computing System for AI Applications. (arXiv:2312.04836v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2312.04836</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent breakthroughs in artificial intelligence (AI) algorithms have
highlighted the need for novel computing hardware in order to truly unlock the
potential for AI. Physics-based hardware, such as thermodynamic computing, has
the potential to provide a fast, low-power means to accelerate AI primitives,
especially generative AI and probabilistic AI. In this work, we present the
first continuous-variable thermodynamic computer, which we call the stochastic
processing unit (SPU). Our SPU is composed of RLC circuits, as unit cells, on a
printed circuit board, with 8 unit cells that are all-to-all coupled via
switched capacitances. It can be used for either sampling or linear algebra
primitives, and we demonstrate Gaussian sampling and matrix inversion on our
hardware. The latter represents the first thermodynamic linear algebra
experiment. We also illustrate the applicability of the SPU to uncertainty
quantification for neural network classification. We envision that this
hardware, when scaled up in size, will have significant impact on accelerating
various probabilistic AI applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Melanson_D/0/1/0/all/0/1&quot;&gt;Denis Melanson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khater_M/0/1/0/all/0/1&quot;&gt;Mohammad Abu Khater&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aifer_M/0/1/0/all/0/1&quot;&gt;Maxwell Aifer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Donatella_K/0/1/0/all/0/1&quot;&gt;Kaelan Donatella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gordon_M/0/1/0/all/0/1&quot;&gt;Max Hunter Gordon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahle_T/0/1/0/all/0/1&quot;&gt;Thomas Ahle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crooks_G/0/1/0/all/0/1&quot;&gt;Gavin Crooks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martinez_A/0/1/0/all/0/1&quot;&gt;Antonio J. Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sbahi_F/0/1/0/all/0/1&quot;&gt;Faris Sbahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coles_P/0/1/0/all/0/1&quot;&gt;Patrick J. Coles&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04837">
<title>Localized Symbolic Knowledge Distillation for Visual Commonsense Models. (arXiv:2312.04837v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.04837</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction following vision-language (VL) models offer a flexible interface
that supports a broad range of multimodal tasks in a zero-shot fashion.
However, interfaces that operate on full images do not directly enable the user
to &quot;point to&quot; and access specific regions within images. This capability is
important not only to support reference-grounded VL benchmarks, but also, for
practical applications that require precise within-image reasoning. We build
Localized Visual Commonsense models, which allow users to specify (multiple)
regions as input. We train our model by sampling localized commonsense
knowledge from a large language model (LLM): specifically, we prompt an LLM to
collect commonsense knowledge given a global literal image description and a
local literal region description automatically generated by a set of VL models.
With a separately trained critic model that selects high-quality examples, we
find that training on the localized commonsense corpus can successfully distill
existing VL models to support a reference-as-input interface. Empirical results
and human evaluations in a zero-shot setup demonstrate that our distillation
method results in more precise VL models of reasoning compared to a baseline of
passing a generated referring expression to an LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jae Sung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hessel_J/0/1/0/all/0/1&quot;&gt;Jack Hessel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandu_K/0/1/0/all/0/1&quot;&gt;Khyathi Raghavi Chandu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_P/0/1/0/all/0/1&quot;&gt;Paul Pu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Ximing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+West_P/0/1/0/all/0/1&quot;&gt;Peter West&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Youngjae Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiuyuan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Jianfeng Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farhadi_A/0/1/0/all/0/1&quot;&gt;Ali Farhadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Yejin Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04839">
<title>Understanding Teacher Perspectives and Experiences after Deployment of AI Literacy Curriculum in Middle-school Classrooms. (arXiv:2312.04839v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2312.04839</link>
<description rdf:parseType="Literal">&lt;p&gt;Artificial Intelligence (AI) and its associated applications are ubiquitous
in today&apos;s world, making it imperative that students and their teachers
understand how it works and the ramifications arising from its usage. In this
study, we investigate the experiences of seven teachers following their
implementation of modules from the MIT RAICA (Responsible AI for Computational
Action) curriculum. Through semi-structured interviews, we investigated their
instructional strategies as they engaged with the AI curriculum in their
classroom, how their teaching and learning beliefs about AI evolved with the
curriculum as well as how those beliefs impacted their implementation of the
curriculum. Our analysis suggests that the AI modules not only expanded our
teachers&apos; knowledge in the field, but also prompted them to recognize its daily
applications and their ethical and societal implications, so that they could
better engage with the content they deliver to students. Teachers were able to
leverage their own interdisciplinary backgrounds to creatively introduce
foundational AI topics to students to maximize engagement and playful learning.
Our teachers advocated their need for better external support when navigating
technological resources, additional time for preparation given the novelty of
the curriculum, more flexibility within curriculum timelines, and additional
accommodations for students of determination. Our findings provide valuable
insights for enhancing future iterations of AI literacy curricula and teacher
professional development (PD) resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ravi_P/0/1/0/all/0/1&quot;&gt;Prerna Ravi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Broski_A/0/1/0/all/0/1&quot;&gt;Annalisa Broski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stump_G/0/1/0/all/0/1&quot;&gt;Glenda Stump&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abelson_H/0/1/0/all/0/1&quot;&gt;Hal Abelson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klopfer_E/0/1/0/all/0/1&quot;&gt;Eric Klopfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breazeal_C/0/1/0/all/0/1&quot;&gt;Cynthia Breazeal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04840">
<title>Analysis on Effects of Fault Elements in Memristive Neuromorphic Systems. (arXiv:2312.04840v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2312.04840</link>
<description rdf:parseType="Literal">&lt;p&gt;Nowadays, neuromorphic systems based on Spiking Neural Networks (SNNs)
attract attentions of many researchers. There are many studies to improve
performances of neuromorphic systems. These studies have been showing
satisfactory results. To magnify performances of neuromorphic systems,
developing actual neuromorphic systems is essential. For developing them,
memristors play key role due to their useful characteristics. Although
memristors are essential for actual neuromorphic systems, they are vulnerable
to faults. However, there are few studies analyzing effects of fault elements
in neuromorphic systems using memristors. To solve this problem, we analyze
performance of a memristive neuromorphic system with fault elements changing
fault ratios, types, and positions. We choose neurons and synapses to inject
faults. We inject two types of faults to synapses: SA0 and SA1 faults. The
fault synapses appear in random and important positions. Through our analysis,
we discover the following four interesting points. First, memristive
characteristics increase vulnerability of neuromorphic systems to fault
elements. Second, fault neuron ratios reducing performance sharply exist.
Third, performance degradation by fault synapses depends on fault types.
Finally, SA1 fault synapses improve performance when they appear in important
positions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hyun-Jong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_J/0/1/0/all/0/1&quot;&gt;Jae-Han Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04843">
<title>FREDSum: A Dialogue Summarization Corpus for French Political Debates. (arXiv:2312.04843v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04843</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep learning, and especially the invention of
encoder-decoder architectures, has significantly improved the performance of
abstractive summarization systems. The majority of research has focused on
written documents, however, neglecting the problem of multi-party dialogue
summarization. In this paper, we present a dataset of French political debates
for the purpose of enhancing resources for multi-lingual dialogue
summarization. Our dataset consists of manually transcribed and annotated
political debates, covering a range of topics and perspectives. We highlight
the importance of high quality transcription and annotations for training
accurate and effective dialogue summarization models, and emphasize the need
for multilingual resources to support dialogue summarization in non-English
languages. We also provide baseline experiments using state-of-the-art methods,
and encourage further research in this area to advance the field of dialogue
summarization. Our dataset will be made publicly available for use by the
research community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rennard_V/0/1/0/all/0/1&quot;&gt;Virgile Rennard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shang_G/0/1/0/all/0/1&quot;&gt;Guokan Shang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grari_D/0/1/0/all/0/1&quot;&gt;Damien Grari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hunter_J/0/1/0/all/0/1&quot;&gt;Julie Hunter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vazirgiannis_M/0/1/0/all/0/1&quot;&gt;Michalis Vazirgiannis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04854">
<title>Apollo&apos;s Oracle: Retrieval-Augmented Reasoning in Multi-Agent Debates. (arXiv:2312.04854v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04854</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent debate systems are designed to derive accurate and consistent
conclusions through adversarial interactions among agents. However, these
systems often encounter challenges due to cognitive constraints, manifesting as
(1) agents&apos; obstinate adherence to incorrect viewpoints and (2) their
propensity to abandon correct viewpoints. These issues are primarily
responsible for the ineffectiveness of such debates. Addressing the challenge
of cognitive constraints, we introduce a novel framework, the Multi-Agent
Debate with Retrieval Augmented (MADRA). MADRA incorporates retrieval of prior
knowledge into the debate process, effectively breaking cognitive constraints
and enhancing the agents&apos; reasoning capabilities. Furthermore, we have
developed a self-selection module within this framework, enabling agents to
autonomously select pertinent evidence, thereby minimizing the impact of
irrelevant or noisy data. We have comprehensively tested and analyzed MADRA
across six diverse datasets. The experimental results demonstrate that our
approach significantly enhances performance across various tasks, proving the
effectiveness of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haotian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiyuan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Weijiang Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qianglong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Kun Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_Z/0/1/0/all/0/1&quot;&gt;Zheng Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_L/0/1/0/all/0/1&quot;&gt;Lian Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_Y/0/1/0/all/0/1&quot;&gt;Yi Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04861">
<title>Radar Perception in Autonomous Driving: Exploring Different Data Representations. (arXiv:2312.04861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04861</link>
<description rdf:parseType="Literal">&lt;p&gt;With the rapid advancements of sensor technology and deep learning,
autonomous driving systems are providing safe and efficient access to
intelligent vehicles as well as intelligent transportation. Among these
equipped sensors, the radar sensor plays a crucial role in providing robust
perception information in diverse environmental conditions. This review focuses
on exploring different radar data representations utilized in autonomous
driving systems. Firstly, we introduce the capabilities and limitations of the
radar sensor by examining the working principles of radar perception and signal
processing of radar measurements. Then, we delve into the generation process of
five radar representations, including the ADC signal, radar tensor, point
cloud, grid map, and micro-Doppler signature. For each radar representation, we
examine the related datasets, methods, advantages and limitations. Furthermore,
we discuss the challenges faced in these data representations and propose
potential research directions. Above all, this comprehensive review offers an
in-depth insight into how these representations enhance autonomous system
capabilities, providing guidance for radar perception researchers. To
facilitate retrieval and comparison of different data representations, datasets
and methods, we provide an interactive website at
https://radar-camera-fusion.github.io/radar.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_S/0/1/0/all/0/1&quot;&gt;Shanliang Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_R/0/1/0/all/0/1&quot;&gt;Runwei Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zitian Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_C/0/1/0/all/0/1&quot;&gt;Chenhang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yilu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yong Yue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1&quot;&gt;Eng Gee Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_H/0/1/0/all/0/1&quot;&gt;Hyungjoon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Man_K/0/1/0/all/0/1&quot;&gt;Ka Lok Man&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaohui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_Y/0/1/0/all/0/1&quot;&gt;Yutao Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04862">
<title>Damage GAN: A Generative Model for Imbalanced Data. (arXiv:2312.04862v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04862</link>
<description rdf:parseType="Literal">&lt;p&gt;This study delves into the application of Generative Adversarial Networks
(GANs) within the context of imbalanced datasets. Our primary aim is to enhance
the performance and stability of GANs in such datasets. In pursuit of this
objective, we introduce a novel network architecture known as Damage GAN,
building upon the ContraD GAN framework which seamlessly integrates GANs and
contrastive learning. Through the utilization of contrastive learning, the
discriminator is trained to develop an unsupervised representation capable of
distinguishing all provided samples. Our approach draws inspiration from the
straightforward framework for contrastive learning of visual representations
(SimCLR), leading to the formulation of a distinctive loss function. We also
explore the implementation of self-damaging contrastive learning (SDCLR) to
further enhance the optimization of the ContraD GAN model. Comparative
evaluations against baseline models including the deep convolutional GAN
(DCGAN) and ContraD GAN demonstrate the evident superiority of our proposed
model, Damage GAN, in terms of generated image distribution, model stability,
and image quality when applied to imbalanced datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anaissi_A/0/1/0/all/0/1&quot;&gt;Ali Anaissi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yuanzhe Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Braytee_A/0/1/0/all/0/1&quot;&gt;Ali Braytee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Naji_M/0/1/0/all/0/1&quot;&gt;Mohamad Naji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alyassine_W/0/1/0/all/0/1&quot;&gt;Widad Alyassine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04864">
<title>Critical Analysis of 5G Networks Traffic Intrusion using PCA, t-SNE and UMAP Visualization and Classifying Attacks. (arXiv:2312.04864v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04864</link>
<description rdf:parseType="Literal">&lt;p&gt;Networks, threat models, and malicious actors are advancing quickly. With the
increased deployment of the 5G networks, the security issues of the attached 5G
physical devices have also increased. Therefore, artificial intelligence based
autonomous end-to-end security design is needed that can deal with incoming
threats by detecting network traffic anomalies. To address this requirement, in
this research, we used a recently published 5G traffic dataset, 5G-NIDD, to
detect network traffic anomalies using machine and deep learning approaches.
First, we analyzed the dataset using three visualization techniques:
t-Distributed Stochastic Neighbor Embedding (t-SNE), Uniform Manifold
Approximation and Projection (UMAP), and Principal Component Analysis (PCA).
Second, we reduced the data dimensionality using mutual information and PCA
techniques. Third, we solve the class imbalance issue by inserting synthetic
records of minority classes. Last, we performed classification using six
different classifiers and presented the evaluation metrics. We received the
best results when K-Nearest Neighbors classifier was used: accuracy (97.2%),
detection rate (96.7%), and false positive rate (2.2%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghani_H/0/1/0/all/0/1&quot;&gt;Humera Ghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salekzamankhani_S/0/1/0/all/0/1&quot;&gt;Shahram Salekzamankhani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Virdee_B/0/1/0/all/0/1&quot;&gt;Bal Virdee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04881">
<title>Predictive Chemistry Augmented with Text Retrieval. (arXiv:2312.04881v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04881</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on using natural language descriptions to enhance
predictive models in the chemistry field. Conventionally, chemoinformatics
models are trained with extensive structured data manually extracted from the
literature. In this paper, we introduce TextReact, a novel method that directly
augments predictive chemistry with texts retrieved from the literature.
TextReact retrieves text descriptions relevant for a given chemical reaction,
and then aligns them with the molecular representation of the reaction. This
alignment is enhanced via an auxiliary masked LM objective incorporated in the
predictor training. We empirically validate the framework on two chemistry
tasks: reaction condition recommendation and one-step retrosynthesis. By
leveraging text retrieval, TextReact significantly outperforms state-of-the-art
chemoinformatics models trained solely on molecular data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yujie Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhening Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhengkai Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coley_C/0/1/0/all/0/1&quot;&gt;Connor W. Coley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barzilay_R/0/1/0/all/0/1&quot;&gt;Regina Barzilay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04882">
<title>Classification of Human- and AI-Generated Texts for English, French, German, and Spanish. (arXiv:2312.04882v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04882</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we analyze features to classify human- and AI-generated text
for English, French, German and Spanish and compare them across languages. We
investigate two scenarios: (1) The detection of text generated by AI from
scratch, and (2) the detection of text rephrased by AI. For training and
testing the classifiers in this multilingual setting, we created a new text
corpus covering 10 topics for each language. For the detection of AI-generated
text, the combination of all proposed features performs best, indicating that
our features are portable to other related languages: The F1-scores are close
with 99% for Spanish, 98% for English, 97% for German and 95% for French. For
the detection of AI-rephrased text, the systems with all features outperform
systems with other features in many cases, but using only document features
performs best for German (72%) and Spanish (86%) and only text vector features
leads to best results for English (78%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaaff_K/0/1/0/all/0/1&quot;&gt;Kristina Schaaff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schlippe_T/0/1/0/all/0/1&quot;&gt;Tim Schlippe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mindner_L/0/1/0/all/0/1&quot;&gt;Lorenz Mindner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04889">
<title>KwaiAgents: Generalized Information-seeking Agent System with Large Language Models. (arXiv:2312.04889v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.04889</link>
<description rdf:parseType="Literal">&lt;p&gt;Driven by curiosity, humans have continually sought to explore and understand
the world around them, leading to the invention of various tools to satiate
this inquisitiveness. Despite not having the capacity to process and memorize
vast amounts of information in their brains, humans excel in critical thinking,
planning, reflection, and harnessing available tools to interact with and
interpret the world, enabling them to find answers efficiently. The recent
advancements in large language models (LLMs) suggest that machines might also
possess the aforementioned human-like capabilities, allowing them to exhibit
powerful abilities even with a constrained parameter count. In this paper, we
introduce KwaiAgents, a generalized information-seeking agent system based on
LLMs. Within KwaiAgents, we propose an agent system that employs LLMs as its
cognitive core, which is capable of understanding a user&apos;s query, behavior
guidelines, and referencing external documents. The agent can also update and
retrieve information from its internal memory, plan and execute actions using a
time-aware search-browse toolkit, and ultimately provide a comprehensive
response. We further investigate the system&apos;s performance when powered by LLMs
less advanced than GPT-4, and introduce the Meta-Agent Tuning (MAT) framework,
designed to ensure even an open-sourced 7B or 13B model performs well among
many agent systems. We exploit both benchmark and human evaluations to
systematically validate these capabilities. Extensive experiments show the
superiority of our agent system compared to other autonomous agents and
highlight the enhanced generalized agent-abilities of our fine-tuned LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_H/0/1/0/all/0/1&quot;&gt;Haojie Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_Z/0/1/0/all/0/1&quot;&gt;Zepeng Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_H/0/1/0/all/0/1&quot;&gt;Hao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yaojia Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_R/0/1/0/all/0/1&quot;&gt;Ruiji Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Ming Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhongyuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bing Qin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04902">
<title>BELT: Old-School Backdoor Attacks can Evade the State-of-the-Art Defense with Backdoor Exclusivity Lifting. (arXiv:2312.04902v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04902</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks (DNNs) are susceptible to backdoor attacks, where
malicious functionality is embedded to allow attackers to trigger incorrect
classifications. Old-school backdoor attacks use strong trigger features that
can easily be learned by victim models. Despite robustness against input
variation, the robustness however increases the likelihood of unintentional
trigger activations. This leaves traces to existing defenses, which find
approximate replacements for the original triggers that can activate the
backdoor without being identical to the original trigger via, e.g., reverse
engineering and sample overlay.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose and investigate a new characteristic of backdoor
attacks, namely, backdoor exclusivity, which measures the ability of backdoor
triggers to remain effective in the presence of input variation. Building upon
the concept of backdoor exclusivity, we propose Backdoor Exclusivity LifTing
(BELT), a novel technique which suppresses the association between the backdoor
and fuzzy triggers to enhance backdoor exclusivity for defense evasion.
Extensive evaluation on three popular backdoor benchmarks validate, our
approach substantially enhances the stealthiness of four old-school backdoor
attacks, which, after backdoor exclusivity lifting, is able to evade six
state-of-the-art backdoor countermeasures, at almost no cost of the attack
success rate and normal utility. For example, one of the earliest backdoor
attacks BadNet, enhanced by BELT, evades most of the state-of-the-art defenses
including ABS and MOTH which would otherwise recognize the backdoored model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Huming Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Junjie Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xudong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04913">
<title>SA-Attack: Improving Adversarial Transferability of Vision-Language Pre-training Models via Self-Augmentation. (arXiv:2312.04913v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04913</link>
<description rdf:parseType="Literal">&lt;p&gt;Current Visual-Language Pre-training (VLP) models are vulnerable to
adversarial examples. These adversarial examples present substantial security
risks to VLP models, as they can leverage inherent weaknesses in the models,
resulting in incorrect predictions. In contrast to white-box adversarial
attacks, transfer attacks (where the adversary crafts adversarial examples on a
white-box model to fool another black-box model) are more reflective of
real-world scenarios, thus making them more meaningful for research. By
summarizing and analyzing existing research, we identified two factors that can
influence the efficacy of transfer attacks on VLP models: inter-modal
interaction and data diversity. Based on these insights, we propose a
self-augment-based transfer attack method, termed SA-Attack. Specifically,
during the generation of adversarial images and adversarial texts, we apply
different data augmentation methods to the image modality and text modality,
respectively, with the aim of improving the adversarial transferability of the
generated adversarial images and texts. Experiments conducted on the FLickr30K
and COCO datasets have validated the effectiveness of our method. Our code will
be available after this paper is accepted.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_B/0/1/0/all/0/1&quot;&gt;Bangyan He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaojun Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Siyuan Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_T/0/1/0/all/0/1&quot;&gt;Tianrui Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_X/0/1/0/all/0/1&quot;&gt;Xiaochun Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04916">
<title>EE-LLM: Large-Scale Training and Inference of Early-Exit Large Language Models with 3D Parallelism. (arXiv:2312.04916v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.04916</link>
<description rdf:parseType="Literal">&lt;p&gt;We present EE-LLM, a framework for large-scale training and inference of
early-exit large language models (LLMs). While recent works have shown
preliminary evidence for the efficacy of early exiting in accelerating LLM
inference, EE-LLM makes a foundational step towards scaling up early-exit LLMs
by supporting their training and inference with massive 3D parallelism. Built
upon Megatron-LM, EE-LLM implements a variety of algorithmic innovations and
performance optimizations tailored to early exiting, including a lightweight
method that facilitates backpropagation for the early-exit training objective
with pipeline parallelism, techniques of leveraging idle resources in the
original pipeline schedule for computation related to early-exit layers, and
two approaches of early-exit inference that are compatible with KV caching for
autoregressive generation. Our analytical and empirical study shows that EE-LLM
achieves great training efficiency with negligible computational overhead
compared to standard LLM training, as well as outstanding inference speedup
without compromising output quality. To facilitate further research and
adoption, we release EE-LLM at https://github.com/pan-x-c/EE-LLM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanxi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xuchen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_B/0/1/0/all/0/1&quot;&gt;Bolin Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04917">
<title>Operationalizing Assurance Cases for Data Scientists: A Showcase of Concepts and Tooling in the Context of Test Data Quality for Machine Learning. (arXiv:2312.04917v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.04917</link>
<description rdf:parseType="Literal">&lt;p&gt;Assurance Cases (ACs) are an established approach in safety engineering to
argue quality claims in a structured way. In the context of quality assurance
for Machine Learning (ML)-based software components, ACs are also being
discussed and appear promising. Tools for operationalizing ACs do exist, yet
mainly focus on supporting safety engineers on the system level. However,
assuring the quality of an ML component within the system is commonly the
responsibility of data scientists, who are usually less familiar with these
tools. To address this gap, we propose a framework to support the
operationalization of ACs for ML components based on technologies that data
scientists use on a daily basis: Python and Jupyter Notebook. Our aim is to
make the process of creating ML-related evidence in ACs more effective. Results
from the application of the framework, documented through notebooks, can be
integrated into existing AC tools. We illustrate the application of the
framework on an example excerpt concerned with the quality of the test data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jockel_L/0/1/0/all/0/1&quot;&gt;Lisa J&amp;#xf6;ckel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klas_M/0/1/0/all/0/1&quot;&gt;Michael Kl&amp;#xe4;s&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gross_J/0/1/0/all/0/1&quot;&gt;Janek Gro&amp;#xdf;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerber_P/0/1/0/all/0/1&quot;&gt;Pascal Gerber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholz_M/0/1/0/all/0/1&quot;&gt;Markus Scholz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eberle_J/0/1/0/all/0/1&quot;&gt;Jonathan Eberle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teschner_M/0/1/0/all/0/1&quot;&gt;Marc Teschner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seifert_D/0/1/0/all/0/1&quot;&gt;Daniel Seifert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hawkins_R/0/1/0/all/0/1&quot;&gt;Richard Hawkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molloy_J/0/1/0/all/0/1&quot;&gt;John Molloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ottnad_J/0/1/0/all/0/1&quot;&gt;Jens Ottnad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04940">
<title>Canaries and Whistles: Resilient Drone Communication Networks with (or without) Deep Reinforcement Learning. (arXiv:2312.04940v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.04940</link>
<description rdf:parseType="Literal">&lt;p&gt;Communication networks able to withstand hostile environments are critically
important for disaster relief operations. In this paper, we consider a
challenging scenario where drones have been compromised in the supply chain,
during their manufacture, and harbour malicious software capable of
wide-ranging and infectious disruption. We investigate multi-agent deep
reinforcement learning as a tool for learning defensive strategies that
maximise communications bandwidth despite continual adversarial interference.
Using a public challenge for learning network resilience strategies, we propose
a state-of-the-art expert technique and study its superiority over deep
reinforcement learning agents. Correspondingly, we identify three specific
methods for improving the performance of our learning-based agents: (1)
ensuring each observation contains the necessary information, (2) using expert
agents to provide a curriculum for learning, and (3) paying close attention to
reward. We apply our methods and present a new mixed strategy enabling expert
and learning-based agents to work together and improve on all prior results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hicks_C/0/1/0/all/0/1&quot;&gt;Chris Hicks&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mavroudis_V/0/1/0/all/0/1&quot;&gt;Vasilios Mavroudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foley_M/0/1/0/all/0/1&quot;&gt;Myles Foley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Davies_T/0/1/0/all/0/1&quot;&gt;Thomas Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Highnam_K/0/1/0/all/0/1&quot;&gt;Kate Highnam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1&quot;&gt;Tim Watson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04945">
<title>The ICL Consistency Test. (arXiv:2312.04945v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2312.04945</link>
<description rdf:parseType="Literal">&lt;p&gt;Just like the previous generation of task-tuned models, large language models
(LLMs) that are adapted to tasks via prompt-based methods like
in-context-learning (ICL) perform well in some setups but not in others. This
lack of consistency in prompt-based learning hints at a lack of robust
generalisation. We here introduce the ICL consistency test -- a contribution to
the GenBench collaborative benchmark task (CBT) -- which evaluates how
consistent a model makes predictions across many different setups while using
the same data. The test is based on different established natural language
inference tasks. We provide preprocessed data constituting 96 different
&apos;setups&apos; and a metric that estimates model consistency across these setups. The
metric is provided on a fine-grained level to understand what properties of a
setup render predictions unstable and on an aggregated level to compare overall
model consistency. We conduct an empirical analysis of eight state-of-the-art
models, and our consistency metric reveals how all tested LLMs lack robust
generalisation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weber_L/0/1/0/all/0/1&quot;&gt;Lucas Weber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bruni_E/0/1/0/all/0/1&quot;&gt;Elia Bruni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hupkes_D/0/1/0/all/0/1&quot;&gt;Dieuwke Hupkes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04947">
<title>Benchmarking and Analysis of Unsupervised Object Segmentation from Real-world Single Images. (arXiv:2312.04947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04947</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study the problem of unsupervised object segmentation from
single images. We do not introduce a new algorithm, but systematically
investigate the effectiveness of existing unsupervised models on challenging
real-world images. We first introduce seven complexity factors to
quantitatively measure the distributions of background and foreground object
biases in appearance and geometry for datasets with human annotations. With the
aid of these factors, we empirically find that, not surprisingly, existing
unsupervised models fail to segment generic objects in real-world images,
although they can easily achieve excellent performance on numerous simple
synthetic datasets, due to the vast gap in objectness biases between synthetic
and real images. By conducting extensive experiments on multiple groups of
ablated real-world datasets, we ultimately find that the key factors underlying
the failure of existing unsupervised models on real-world images are the
challenging distributions of background and foreground object biases in
appearance and geometry. Because of this, the inductive biases introduced in
existing unsupervised models can hardly capture the diverse object
distributions. Our research results suggest that future work should exploit
more explicit objectness biases in the network design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yafei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_B/0/1/0/all/0/1&quot;&gt;Bo Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04960">
<title>MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness. (arXiv:2312.04960v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04960</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision Transformers (ViTs) achieve superior performance on various tasks
compared to convolutional neural networks (CNNs), but ViTs are also vulnerable
to adversarial attacks. Adversarial training is one of the most successful
methods to build robust CNN models. Thus, recent works explored new
methodologies for adversarial training of ViTs based on the differences between
ViTs and CNNs, such as better training strategies, preventing attention from
focusing on a single block, or discarding low-attention embeddings. However,
these methods still follow the design of traditional supervised adversarial
training, limiting the potential of adversarial training on ViTs. This paper
proposes a novel defense method, MIMIR, which aims to build a different
adversarial training methodology by utilizing Masked Image Modeling at
pre-training. We create an autoencoder that accepts adversarial examples as
input but takes the clean examples as the modeling target. Then, we create a
mutual information (MI) penalty following the idea of the Information
Bottleneck. Among the two information source inputs and corresponding
adversarial perturbation, the perturbation information is eliminated due to the
constraint of the modeling target. Next, we provide a theoretical analysis of
MIMIR using the bounds of the MI penalty. We also design two adaptive attacks
when the adversary is aware of the MIMIR defense and show that MIMIR still
performs well. The experimental results show that MIMIR improves (natural and
adversarial) accuracy on average by 4.19\% on CIFAR-10 and 5.52\% on
ImageNet-1K, compared to baselines. On Tiny-ImageNet, we obtained improved
natural accuracy of 2.99\% on average and comparable adversarial accuracy. Our
code and trained models are publicly
available\footnote{\url{https://anonymous.4open.science/r/MIMIR-5444/README.md}}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_S/0/1/0/all/0/1&quot;&gt;Shujian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jingzheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picek_S/0/1/0/all/0/1&quot;&gt;Stjepan Picek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04963">
<title>Text-to-3D Generation with Bidirectional Diffusion using both 2D and 3D priors. (arXiv:2312.04963v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04963</link>
<description rdf:parseType="Literal">&lt;p&gt;Most 3D generation research focuses on up-projecting 2D foundation models
into the 3D space, either by minimizing 2D Score Distillation Sampling (SDS)
loss or fine-tuning on multi-view datasets. Without explicit 3D priors, these
methods often lead to geometric anomalies and multi-view inconsistency.
Recently, researchers have attempted to improve the genuineness of 3D objects
by directly training on 3D datasets, albeit at the cost of low-quality texture
generation due to the limited texture diversity in 3D datasets. To harness the
advantages of both approaches, we propose Bidirectional Diffusion(BiDiff), a
unified framework that incorporates both a 3D and a 2D diffusion process, to
preserve both 3D fidelity and 2D texture richness, respectively. Moreover, as a
simple combination may yield inconsistent generation results, we further bridge
them with novel bidirectional guidance. In addition, our method can be used as
an initialization of optimization-based models to further improve the quality
of 3D model and efficiency of optimization, reducing the generation process
from 3.4 hours to 20 minutes. Experimental results have shown that our model
achieves high-quality, diverse, and scalable 3D generation. Project website:
https://bidiff.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_L/0/1/0/all/0/1&quot;&gt;Lihe Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Shaocong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhanpeng Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zibin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_K/0/1/0/all/0/1&quot;&gt;Kaixiong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_T/0/1/0/all/0/1&quot;&gt;Tianfan Xue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04965">
<title>Inversion-Free Image Editing with Natural Language. (arXiv:2312.04965v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.04965</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite recent advances in inversion-based editing, text-guided image
manipulation remains challenging for diffusion models. The primary bottlenecks
include 1) the time-consuming nature of the inversion process; 2) the struggle
to balance consistency with accuracy; 3) the lack of compatibility with
efficient consistency sampling methods used in consistency models. To address
the above issues, we start by asking ourselves if the inversion process can be
eliminated for editing. We show that when the initial sample is known, a
special variance schedule reduces the denoising step to the same form as the
multi-step consistency sampling. We name this Denoising Diffusion Consistent
Model (DDCM), and note that it implies a virtual inversion strategy without
explicit inversion in sampling. We further unify the attention control
mechanisms in a tuning-free framework for text-guided editing. Combining them,
we present inversion-free editing (InfEdit), which allows for consistent and
faithful editing for both rigid and non-rigid semantic changes, catering to
intricate modifications without compromising on the image&apos;s integrity and
explicit inversion. Through extensive experiments, InfEdit shows strong
performance in various editing tasks and also maintains a seamless workflow
(less than 3 seconds on one single A40), demonstrating the potential for
real-time applications. Project Page: https://sled-group.github.io/InfEdit/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Sihan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yidong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_J/0/1/0/all/0/1&quot;&gt;Jiayi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Ziqiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chai_J/0/1/0/all/0/1&quot;&gt;Joyce Chai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04986">
<title>Out of Context: How important is Local Context in Neural Program Repair?. (arXiv:2312.04986v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2312.04986</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning source code models have been applied very successfully to the
problem of automated program repair. One of the standing issues is the small
input window of current models which often cannot fully fit the context code
required for a bug fix (e.g., method or class declarations of a project).
Instead, input is often restricted to the local context, that is, the lines
below and above the bug location. In this work we study the importance of this
local context on repair success: how much local context is needed?; is context
before or after the bug location more important? how is local context tied to
the bug type? To answer these questions we train and evaluate Transformer
models in many different local context configurations on three datasets and two
programming languages. Our results indicate that overall repair success
increases with the size of the local context (albeit not for all bug types) and
confirm the common practice that roughly 50-60% of the input window should be
used for context leading the bug. Our results are not only relevant for
researchers working on Transformer-based APR tools but also for benchmark and
dataset creators who must decide what and how much context to include in their
datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prenner_J/0/1/0/all/0/1&quot;&gt;Julian Aron Prenner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robbes_R/0/1/0/all/0/1&quot;&gt;Romain Robbes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04997">
<title>Beyond Transduction: A Survey on Inductive, Few Shot, and Zero Shot Link Prediction in Knowledge Graphs. (arXiv:2312.04997v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.04997</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs (KGs) comprise entities interconnected by relations of
different semantic meanings. KGs are being used in a wide range of
applications. However, they inherently suffer from incompleteness, i.e.
entities or facts about entities are missing. Consequently, a larger body of
works focuses on the completion of missing information in KGs, which is
commonly referred to as link prediction (LP). This task has traditionally and
extensively been studied in the transductive setting, where all entities and
relations in the testing set are observed during training. Recently, several
works have tackled the LP task under more challenging settings, where entities
and relations in the test set may be unobserved during training, or appear in
only a few facts. These works are known as inductive, few-shot, and zero-shot
link prediction. In this work, we conduct a systematic review of existing works
in this area. A thorough analysis leads us to point out the undesirable
existence of diverging terminologies and task definitions for the
aforementioned settings, which further limits the possibility of comparison
between recent works. We consequently aim at dissecting each setting
thoroughly, attempting to reveal its intrinsic characteristics. A unifying
nomenclature is ultimately proposed to refer to each of them in a simple and
consistent manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubert_N/0/1/0/all/0/1&quot;&gt;Nicolas Hubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1&quot;&gt;Pierre Monnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paulheim_H/0/1/0/all/0/1&quot;&gt;Heiko Paulheim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05019">
<title>Vision-based Learning for Drones: A Survey. (arXiv:2312.05019v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.05019</link>
<description rdf:parseType="Literal">&lt;p&gt;Drones as advanced cyber-physical systems are undergoing a transformative
shift with the advent of vision-based learning, a field that is rapidly gaining
prominence due to its profound impact on drone autonomy and functionality.
Different from existing task-specific surveys, this review offers a
comprehensive overview of vision-based learning in drones, emphasizing its
pivotal role in enhancing their operational capabilities. We start by
elucidating the fundamental principles of vision-based learning, highlighting
how it significantly improves drones&apos; visual perception and decision-making
processes. We then categorize vision-based control methods into indirect,
semi-direct, and end-to-end approaches from the perception-control perspective.
We further explore various applications of vision-based drones with learning
capabilities, ranging from single-agent systems to more complex multi-agent and
heterogeneous system scenarios, and underscore the challenges and innovations
characterizing each area. Finally, we explore open questions and potential
solutions, paving the way for ongoing research and development in this dynamic
and rapidly evolving field. With growing large language models (LLMs) and
embodied intelligence, vision-based learning for drones provides a promising
but challenging road towards artificial general intelligence (AGI) in 3D
physical world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jiaping Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rangya Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feroskhan_M/0/1/0/all/0/1&quot;&gt;Mir Feroskhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05021">
<title>A Negative Result on Gradient Matching for Selective Backprop. (arXiv:2312.05021v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05021</link>
<description rdf:parseType="Literal">&lt;p&gt;With increasing scale in model and dataset size, the training of deep neural
networks becomes a massive computational burden. One approach to speed up the
training process is Selective Backprop. For this approach, we perform a forward
pass to obtain a loss value for each data point in a minibatch. The backward
pass is then restricted to a subset of that minibatch, prioritizing high-loss
examples. We build on this approach, but seek to improve the subset selection
mechanism by choosing the (weighted) subset which best matches the mean
gradient over the entire minibatch. We use the gradients w.r.t. the model&apos;s
last layer as a cheap proxy, resulting in virtually no overhead in addition to
the forward pass. At the same time, for our experiments we add a simple random
selection baseline which has been absent from prior work. Surprisingly, we find
that both the loss-based as well as the gradient-matching strategy fail to
consistently outperform the random baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balles_L/0/1/0/all/0/1&quot;&gt;Lukas Balles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Archambeau_C/0/1/0/all/0/1&quot;&gt;Cedric Archambeau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1&quot;&gt;Giovanni Zappella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05023">
<title>Reinforcement Learning-Based Bionic Reflex Control for Anthropomorphic Robotic Grasping exploiting Domain Randomization. (arXiv:2312.05023v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.05023</link>
<description rdf:parseType="Literal">&lt;p&gt;Achieving human-level dexterity in robotic grasping remains a challenging
endeavor. Robotic hands frequently encounter slippage and deformation during
object manipulation, issues rarely encountered by humans due to their sensory
receptors, experiential learning, and motor memory. The emulation of the human
grasping reflex within robotic hands is referred to as the ``bionic reflex&quot;.
Past endeavors in the realm of bionic reflex control predominantly relied on
model-based and supervised learning approaches, necessitating human
intervention during thresholding and labeling tasks. In this study, we
introduce an innovative bionic reflex control pipeline, leveraging
reinforcement learning (RL); thereby eliminating the need for human
intervention during control design. Our proposed bionic reflex controller has
been designed and tested on an anthropomorphic hand, manipulating deformable
objects in the PyBullet physics simulator, incorporating domain randomization
(DR) for enhanced Sim2Real transferability. Our findings underscore the promise
of RL as a potent tool for advancing bionic reflex control within
anthropomorphic robotic hands. We anticipate that this autonomous, RL-based
bionic reflex controller will catalyze the development of dependable and highly
efficient robotic and prosthetic hands, revolutionizing human-robot interaction
and assistive technologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basumatary_H/0/1/0/all/0/1&quot;&gt;Hirakjyoti Basumatary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhar_D/0/1/0/all/0/1&quot;&gt;Daksh Adhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shrawge_A/0/1/0/all/0/1&quot;&gt;Atharva Shrawge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kanbaskar_P/0/1/0/all/0/1&quot;&gt;Prathamesh Kanbaskar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazarika_S/0/1/0/all/0/1&quot;&gt;Shyamanta M. Hazarika&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05034">
<title>Grasp Force Optimization as a Bilinear Matrix Inequality Problem: A Deep Learning Approach. (arXiv:2312.05034v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2312.05034</link>
<description rdf:parseType="Literal">&lt;p&gt;Grasp force synthesis is a non-convex optimization problem involving
constraints that are bilinear. Traditional approaches to this problem involve
general-purpose gradient-based nonlinear optimization and semi-definite
programming. With a view towards dealing with postural synergies and non-smooth
but convex positive semidefinite constraints, we look beyond gradient-based
optimization. The focus of this paper is to undertake a grasp analysis of
biomimetic grasping in multi-fingered robotic hands as a bilinear matrix
inequality (BMI) problem. Our analysis is to solve it using a deep learning
approach to make the algorithm efficiently generate force closure grasps with
optimal grasp quality on untrained/unseen objects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basumatary_H/0/1/0/all/0/1&quot;&gt;Hirakjyoti Basumatary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adhar_D/0/1/0/all/0/1&quot;&gt;Daksh Adhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaw_R/0/1/0/all/0/1&quot;&gt;Riddhiman Shaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hazarika_S/0/1/0/all/0/1&quot;&gt;Shyamanta M. Hazarika&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05039">
<title>SmartMask: Context Aware High-Fidelity Mask Generation for Fine-grained Object Insertion and Layout Control. (arXiv:2312.05039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2312.05039</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of generative image inpainting and object insertion has made
significant progress with the recent advent of latent diffusion models.
Utilizing a precise object mask can greatly enhance these applications.
However, due to the challenges users encounter in creating high-fidelity masks,
there is a tendency for these methods to rely on more coarse masks (e.g.,
bounding box) for these applications. This results in limited control and
compromised background content preservation. To overcome these limitations, we
introduce SmartMask, which allows any novice user to create detailed masks for
precise object insertion. Combined with a ControlNet-Inpaint model, our
experiments demonstrate that SmartMask achieves superior object insertion
quality, preserving the background content more effectively than previous
methods. Notably, unlike prior works the proposed approach can also be used
even without user-mask guidance, which allows it to perform mask-free object
insertion at diverse positions and scales. Furthermore, we find that when used
iteratively with a novel instruction-tuning based planning model, SmartMask can
be used to design detailed layouts from scratch. As compared with user-scribble
based layout design, we observe that SmartMask allows for better quality
outputs with layout-to-image generation methods. Project page is available at
https://smartmask-gen.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_J/0/1/0/all/0/1&quot;&gt;Jaskirat Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_C/0/1/0/all/0/1&quot;&gt;Cameron Smith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhe Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_L/0/1/0/all/0/1&quot;&gt;Liang Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05043">
<title>Physical-Layer Semantic-Aware Network for Zero-Shot Wireless Sensing. (arXiv:2312.05043v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2312.05043</link>
<description rdf:parseType="Literal">&lt;p&gt;Device-free wireless sensing has recently attracted significant interest due
to its potential to support a wide range of immersive human-machine interactive
applications. However, data heterogeneity in wireless signals and data privacy
regulation of distributed sensing have been considered as the major challenges
that hinder the wide applications of wireless sensing in large area networking
systems. Motivated by the observation that signals recorded by wireless
receivers are closely related to a set of physical-layer semantic features, in
this paper we propose a novel zero-shot wireless sensing solution that allows
models constructed in one or a limited number of locations to be directly
transferred to other locations without any labeled data. We develop a novel
physical-layer semantic-aware network (pSAN) framework to characterize the
correlation between physical-layer semantic features and the sensing data
distributions across different receivers. We then propose a pSAN-based
zero-shot learning solution in which each receiver can obtain a
location-specific gesture recognition model by directly aggregating the already
constructed models of other receivers. We theoretically prove that models
obtained by our proposed solution can approach the optimal model without
requiring any local model training. Experimental results once again verify that
the accuracy of models derived by our proposed solution matches that of the
models trained by the real labeled data based on supervised learning approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Huixiang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_Y/0/1/0/all/0/1&quot;&gt;Yong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_G/0/1/0/all/0/1&quot;&gt;Guangming Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saad_W/0/1/0/all/0/1&quot;&gt;Walid Saad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05044">
<title>Backward Learning for Goal-Conditioned Policies. (arXiv:2312.05044v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05044</link>
<description rdf:parseType="Literal">&lt;p&gt;Can we learn policies in reinforcement learning without rewards? Can we learn
a policy just by trying to reach a goal state? We answer these questions
positively by proposing a multi-step procedure that first learns a world model
that goes backward in time, secondly generates goal-reaching backward
trajectories, thirdly improves those sequences using shortest path finding
algorithms, and finally trains a neural network policy by imitation learning.
We evaluate our method on a deterministic maze environment where the
observations are $64\times 64$ pixel bird&apos;s eye images and can show that it
consistently reaches several goals.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoftmann_M/0/1/0/all/0/1&quot;&gt;Marc H&amp;#xf6;ftmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robine_J/0/1/0/all/0/1&quot;&gt;Jan Robine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harmeling_S/0/1/0/all/0/1&quot;&gt;Stefan Harmeling&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05114">
<title>On the Inadequacy of Similarity-based Privacy Metrics: Reconstruction Attacks against &quot;Truly Anonymous Synthetic Data&apos;&apos;. (arXiv:2312.05114v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2312.05114</link>
<description rdf:parseType="Literal">&lt;p&gt;Training generative models to produce synthetic data is meant to provide a
privacy-friendly approach to data release. However, we get robust guarantees
only when models are trained to satisfy Differential Privacy (DP). Alas, this
is not the standard in industry as many companies use ad-hoc strategies to
empirically evaluate privacy based on the statistical similarity between
synthetic and real data. In this paper, we review the privacy metrics offered
by leading companies in this space and shed light on a few critical flaws in
reasoning about privacy entirely via empirical evaluations. We analyze the
undesirable properties of the most popular metrics and filters and demonstrate
their unreliability and inconsistency through counter-examples. We then present
a reconstruction attack, ReconSyn, which successfully recovers (i.e., leaks all
attributes of) at least 78% of the low-density train records (or outliers) with
only black-box access to a single fitted generative model and the privacy
metrics. Finally, we show that applying DP only to the model or using
low-utility generators does not mitigate ReconSyn as the privacy leakage
predominantly comes from the metrics. Overall, our work serves as a warning to
practitioners not to deviate from established privacy-preserving mechanisms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganev_G/0/1/0/all/0/1&quot;&gt;Georgi Ganev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cristofaro_E/0/1/0/all/0/1&quot;&gt;Emiliano De Cristofaro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05162">
<title>A Review of Cooperation in Multi-agent Learning. (arXiv:2312.05162v1 [cs.MA])</title>
<link>http://arxiv.org/abs/2312.05162</link>
<description rdf:parseType="Literal">&lt;p&gt;Cooperation in multi-agent learning (MAL) is a topic at the intersection of
numerous disciplines, including game theory, economics, social sciences, and
evolutionary biology. Research in this area aims to understand both how agents
can coordinate effectively when goals are aligned and how they may cooperate in
settings where gains from working together are possible but possibilities for
conflict abound. In this paper we provide an overview of the fundamental
concepts, problem settings and algorithms of multi-agent learning. This
encompasses reinforcement learning, multi-agent sequential decision-making,
challenges associated with multi-agent cooperation, and a comprehensive review
of recent progress, along with an evaluation of relevant metrics. Finally we
discuss open challenges in the field with the aim of inspiring new avenues for
research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yali Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leibo_J/0/1/0/all/0/1&quot;&gt;Joel Z. Leibo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_U/0/1/0/all/0/1&quot;&gt;Usman Islam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Willis_R/0/1/0/all/0/1&quot;&gt;Richard Willis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunehag_P/0/1/0/all/0/1&quot;&gt;Peter Sunehag&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05171">
<title>DARLEI: Deep Accelerated Reinforcement Learning with Evolutionary Intelligence. (arXiv:2312.05171v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05171</link>
<description rdf:parseType="Literal">&lt;p&gt;We present DARLEI, a framework that combines evolutionary algorithms with
parallelized reinforcement learning for efficiently training and evolving
populations of UNIMAL agents. Our approach utilizes Proximal Policy
Optimization (PPO) for individual agent learning and pairs it with a tournament
selection-based generational learning mechanism to foster morphological
evolution. By building on Nvidia&apos;s Isaac Gym, DARLEI leverages GPU accelerated
simulation to achieve over 20x speedup using just a single workstation,
compared to previous work which required large distributed CPU clusters. We
systematically characterize DARLEI&apos;s performance under various conditions,
revealing factors impacting diversity of evolved morphologies. For example, by
enabling inter-agent collisions within the simulator, we find that we can
simulate some multi-agent interactions between the same morphology, and see how
it influences individual agent capabilities and long-term evolutionary
adaptation. While current results demonstrate limited diversity across
generations, we hope to extend DARLEI in future work to include interactions
between diverse morphologies in richer environments, and create a platform that
allows for coevolving populations and investigating emergent behaviours in
them. Our source code is also made publicly at
https://saeejithnair.github.io/darlei.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nair_S/0/1/0/all/0/1&quot;&gt;Saeejith Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shafiee_M/0/1/0/all/0/1&quot;&gt;Mohammad Javad Shafiee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_A/0/1/0/all/0/1&quot;&gt;Alexander Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05181">
<title>TENPLEX: Changing Resources of Deep Learning Jobs using Parallelizable Tensor Collections. (arXiv:2312.05181v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2312.05181</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) jobs use multi-dimensional parallelism, i.e they combine
data, model, and pipeline parallelism, to use large GPU clusters efficiently.
This couples jobs tightly to a set of GPU devices, but jobs may experience
changes to the device allocation: (i) resource elasticity during training adds
or removes devices; (ii) hardware maintenance may require redeployment on
different devices; and (iii) device failures force jobs to run with fewer
devices. Current DL frameworks lack support for these scenarios, as they cannot
change the multi-dimensional parallelism of an already-running job in an
efficient and model-independent way.
&lt;/p&gt;
&lt;p&gt;We describe Tenplex, a state management library for DL frameworks that
enables jobs to change the GPU allocation and job parallelism at runtime.
Tenplex achieves this by externalizing the DL job state during training as a
parallelizable tensor collection (PTC). When the GPU allocation for the DL job
changes, Tenplex uses the PTC to transform the DL job state: for the dataset
state, Tenplex repartitions it under data parallelism and exposes it to workers
through a virtual file system; for the model state, Tenplex obtains it as
partitioned checkpoints and transforms them to reflect the new parallelization
configuration. For efficiency, these PTC transformations are executed in
parallel with a minimum amount of data movement between devices and workers.
Our experiments show that Tenplex enables DL jobs to support dynamic
parallelization with low overhead.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagenlander_M/0/1/0/all/0/1&quot;&gt;Marcel Wagenl&amp;#xe4;nder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_L/0/1/0/all/0/1&quot;&gt;Luo Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pietzuch_P/0/1/0/all/0/1&quot;&gt;Peter Pietzuch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05185">
<title>AI Competitions and Benchmarks: Competition platforms. (arXiv:2312.05185v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05185</link>
<description rdf:parseType="Literal">&lt;p&gt;The ecosystem of artificial intelligence competitions is a diverse and
multifaceted landscape, encompassing a variety of platforms that each host
numerous competitions annually, alongside a plethora of specialized websites
dedicated to singular contests. These platforms adeptly manage the overarching
administrative responsibilities inherent in orchestrating competitions, thus
affording organizers the liberty to allocate greater attention to other facets
of their contests. Notably, these platforms exhibit considerable diversity in
their operational functionalities, economic models, and community dynamics.
This chapter conducts an extensive review of the foremost services in this
realm and elucidates several alternative methodologies that facilitate the
independent hosting of such challenges. Keywords: competition platform,
challenge hosting services, comparison.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ustyuzhanin_A/0/1/0/all/0/1&quot;&gt;Andrey Ustyuzhanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carlens_H/0/1/0/all/0/1&quot;&gt;Harald Carlens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05195">
<title>Conformal Prediction in Multi-User Settings: An Evaluation. (arXiv:2312.05195v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05195</link>
<description rdf:parseType="Literal">&lt;p&gt;Typically, machine learning models are trained and evaluated without making
any distinction between users (e.g, using traditional hold-out and
cross-validation). However, this produces inaccurate performance metrics
estimates in multi-user settings. That is, situations where the data were
collected by multiple users with different characteristics (e.g., age, gender,
height, etc.) which is very common in user computer interaction and medical
applications. For these types of scenarios model evaluation strategies that
provide better performance estimates have been proposed such as mixed,
user-independent, user-dependent, and user-adaptive models. Although those
strategies are better suited for multi-user systems, they are typically
assessed with respect to performance metrics that capture the overall behavior
of the models and do not provide any performance guarantees for individual
predictions nor they provide any feedback about the predictions&apos; uncertainty.
In order to overcome those limitations, in this work we evaluated the conformal
prediction framework in several multi-user settings. Conformal prediction is a
model agnostic method that provides confidence guarantees on the predictions,
thus, increasing the trustworthiness and robustness of the models. We conducted
extensive experiments using different evaluation strategies and found
significant differences in terms of conformal performance measures. We also
proposed several visualizations based on matrices, graphs, and charts that
capture different aspects of the resulting prediction sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Ceja_E/0/1/0/all/0/1&quot;&gt;Enrique Garcia-Ceja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcia_Banuelos_L/0/1/0/all/0/1&quot;&gt;Luciano Garcia-Banuelos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jourdan_N/0/1/0/all/0/1&quot;&gt;Nicolas Jourdan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05209">
<title>HALO: An Ontology for Representing Hallucinations in Generative Models. (arXiv:2312.05209v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05209</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent progress in generative AI, including large language models (LLMs) like
ChatGPT, has opened up significant opportunities in fields ranging from natural
language processing to knowledge discovery and data mining. However, there is
also a growing awareness that the models can be prone to problems such as
making information up or `hallucinations&apos;, and faulty reasoning on seemingly
simple problems. Because of the popularity of models like ChatGPT, both
academic scholars and citizen scientists have documented hallucinations of
several different types and severity. Despite this body of work, a formal model
for describing and representing these hallucinations (with relevant meta-data)
at a fine-grained level, is still lacking. In this paper, we address this gap
by presenting the Hallucination Ontology or HALO, a formal, extensible ontology
written in OWL that currently offers support for six different types of
hallucinations known to arise in LLMs, along with support for provenance and
experimental metadata. We also collect and publish a dataset containing
hallucinations that we inductively gathered across multiple independent Web
sources, and show that HALO can be successfully used to model this dataset and
answer competency questions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nananukul_N/0/1/0/all/0/1&quot;&gt;Navapat Nananukul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kejriwal_M/0/1/0/all/0/1&quot;&gt;Mayank Kejriwal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05230">
<title>Language Models, Agent Models, and World Models: The LAW for Machine Reasoning and Planning. (arXiv:2312.05230v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2312.05230</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their tremendous success in many applications, large language models
often fall short of consistent reasoning and planning in various (language,
embodied, and social) scenarios, due to inherent limitations in their
inference, learning, and modeling capabilities. In this position paper, we
present a new perspective of machine reasoning, LAW, that connects the concepts
of Language models, Agent models, and World models, for more robust and
versatile reasoning capabilities. In particular, we propose that world and
agent models are a better abstraction of reasoning, that introduces the crucial
elements of deliberate human-like reasoning, including beliefs about the world
and other agents, anticipation of consequences, goals/rewards, and strategic
planning. Crucially, language models in LAW serve as a backend to implement the
system or its elements and hence provide the computational power and
adaptability. We review the recent studies that have made relevant progress and
discuss future research directions towards operationalizing the LAW framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhiting Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_T/0/1/0/all/0/1&quot;&gt;Tianmin Shu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05250">
<title>TaskMet: Task-Driven Metric Learning for Model Learning. (arXiv:2312.05250v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05250</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are often deployed in downstream tasks that the training
procedure may not be aware of. For example, models solely trained to achieve
accurate predictions may struggle to perform well on downstream tasks because
seemingly small prediction errors may incur drastic task errors. The standard
end-to-end learning approach is to make the task loss differentiable or to
introduce a differentiable surrogate that the model can be trained on. In these
settings, the task loss needs to be carefully balanced with the prediction loss
because they may have conflicting objectives. We propose take the task loss
signal one level deeper than the parameters of the model and use it to learn
the parameters of the loss function the model is trained on, which can be done
by learning a metric in the prediction space. This approach does not alter the
optimal prediction model itself, but rather changes the model learning to
emphasize the information important for the downstream task. This enables us to
achieve the best of both worlds: a prediction model trained in the original
prediction space while also being valuable for the desired downstream task. We
validate our approach through experiments conducted in two main settings: 1)
decision-focused model learning scenarios involving portfolio optimization and
budget allocation, and 2) reinforcement learning in noisy environments with
distracting states. The source code to reproduce our experiments is available
at https://github.com/facebookresearch/taskmet
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_D/0/1/0/all/0/1&quot;&gt;Dishank Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ricky T. Q. Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukadam_M/0/1/0/all/0/1&quot;&gt;Mustafa Mukadam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amos_B/0/1/0/all/0/1&quot;&gt;Brandon Amos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.05253">
<title>KBFormer: A Diffusion Model for Structured Entity Completion. (arXiv:2312.05253v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2312.05253</link>
<description rdf:parseType="Literal">&lt;p&gt;We develop a generative attention-based approach to modeling structured
entities comprising different property types, such as numerical, categorical,
string, and composite. This approach handles such heterogeneous data through a
mixed continuous-discrete diffusion process over the properties. Our flexible
framework can model entities with arbitrary hierarchical properties, enabling
applications to structured Knowledge Base (KB) entities and tabular data. Our
approach obtains state-of-the-art performance on a majority of cases across 15
datasets. In addition, experiments with a device KB and a nuclear physics
dataset demonstrate the model&apos;s ability to learn representations useful for
entity completion in diverse settings. This has many downstream use cases,
including modeling numerical properties with high accuracy - critical for
science applications, which also benefit from the model&apos;s inherent
probabilistic nature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kitouni_O/0/1/0/all/0/1&quot;&gt;Ouail Kitouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nolte_N/0/1/0/all/0/1&quot;&gt;Niklas Nolte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hensman_J/0/1/0/all/0/1&quot;&gt;James Hensman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_B/0/1/0/all/0/1&quot;&gt;Bhaskar Mitra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.07246">
<title>Responsibility Management through Responsibility Networks. (arXiv:2102.07246v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2102.07246</link>
<description rdf:parseType="Literal">&lt;p&gt;The safety management is critically important in the workplace.
Unfortunately, responsibility issues therein such as inefficient supervision,
poor evaluation and inadequate perception have not been properly addressed. To
this end, in this paper, we deploy the Internet of Responsibilities (IoR) for
responsibility management. Through the building of IoR framework, hierarchical
responsibility management, automated responsibility evaluation at all level and
efficient responsibility perception are achieved. The practical deployment of
IoR system showed its effective responsibility management capability in various
workplaces.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruijun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jiong Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xuejiao Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2108.02924">
<title>Interpretable Visual Understanding with Cognitive Attention Network. (arXiv:2108.02924v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2108.02924</link>
<description rdf:parseType="Literal">&lt;p&gt;While image understanding on recognition-level has achieved remarkable
advancements, reliable visual scene understanding requires comprehensive image
understanding on recognition-level but also cognition-level, which calls for
exploiting the multi-source information as well as learning different levels of
understanding and extensive commonsense knowledge. In this paper, we propose a
novel Cognitive Attention Network (CAN) for visual commonsense reasoning to
achieve interpretable visual understanding. Specifically, we first introduce an
image-text fusion module to fuse information from images and text collectively.
Second, a novel inference module is designed to encode commonsense among image,
query and response. Extensive experiments on large-scale Visual Commonsense
Reasoning (VCR) benchmark dataset demonstrate the effectiveness of our
approach. The implementation is publicly available at
https://github.com/tanjatang/CAN
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xuejiao Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wenbin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Turner_K/0/1/0/all/0/1&quot;&gt;Kea Turner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derr_T/0/1/0/all/0/1&quot;&gt;Tyler Derr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ntoutsi_E/0/1/0/all/0/1&quot;&gt;Eirini Ntoutsi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.04688">
<title>BAFFLE: Hiding Backdoors in Offline Reinforcement Learning Datasets. (arXiv:2210.04688v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.04688</link>
<description rdf:parseType="Literal">&lt;p&gt;Reinforcement learning (RL) makes an agent learn from trial-and-error
experiences gathered during the interaction with the environment. Recently,
offline RL has become a popular RL paradigm because it saves the interactions
with environments. In offline RL, data providers share large pre-collected
datasets, and others can train high-quality agents without interacting with the
environments. This paradigm has demonstrated effectiveness in critical tasks
like robot control, autonomous driving, etc. However, less attention is paid to
investigating the security threats to the offline RL system. This paper focuses
on backdoor attacks, where some perturbations are added to the data
(observations) such that given normal observations, the agent takes
high-rewards actions, and low-reward actions on observations injected with
triggers. In this paper, we propose Baffle (Backdoor Attack for Offline
Reinforcement Learning), an approach that automatically implants backdoors to
RL agents by poisoning the offline RL dataset, and evaluate how different
offline RL algorithms react to this attack. Our experiments conducted on four
tasks and four offline RL algorithms expose a disquieting fact: none of the
existing offline RL algorithms is immune to such a backdoor attack. More
specifically, Baffle modifies 10\% of the datasets for four tasks (3 robotic
controls and 1 autonomous driving). Agents trained on the poisoned datasets
perform well in normal settings. However, when triggers are presented, the
agents&apos; performance decreases drastically by 63.2\%, 53.9\%, 64.7\%, and 47.4\%
in the four tasks on average. The backdoor still persists after fine-tuning
poisoned agents on clean datasets. We further show that the inserted backdoor
is also hard to be detected by a popular defensive method. This paper calls
attention to developing more effective protection for the open-source offline
RL dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_C/0/1/0/all/0/1&quot;&gt;Chen Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhou Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Junda He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jieke Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kecen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_A/0/1/0/all/0/1&quot;&gt;Arunesh Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Bowen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_X/0/1/0/all/0/1&quot;&gt;Xinwen Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lo_D/0/1/0/all/0/1&quot;&gt;David Lo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianhao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.00385">
<title>Behavioral Intention Prediction in Driving Scenes: A Survey. (arXiv:2211.00385v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.00385</link>
<description rdf:parseType="Literal">&lt;p&gt;In the driving scene, the road agents usually conduct frequent interactions
and intention understanding of the surroundings. Ego-agent (each road agent
itself) predicts what behavior will be engaged by other road users all the time
and expects a shared and consistent understanding for safe movement. Behavioral
Intention Prediction (BIP) simulates such a human consideration process and
fulfills the early prediction of specific behaviors. Similar to other
prediction tasks, such as trajectory prediction, data-driven deep learning
methods have taken the primary pipeline in research. The rapid development of
BIP inevitably leads to new issues and challenges. To catalyze future research,
this work provides a comprehensive review of BIP from the available datasets,
key factors and challenges, pedestrian-centric and vehicle-centric BIP
approaches, and BIP-aware applications. Based on the investigation, data-driven
deep learning approaches have become the primary pipelines. The behavioral
intention types are still monotonous in most current datasets and methods
(e.g., Crossing (C) and Not Crossing (NC) for pedestrians and Lane Changing
(LC) for vehicles) in this field. In addition, for the safe-critical scenarios
(e.g., near-crashing situations), current research is limited. Through this
investigation, we identify open issues in behavioral intention prediction and
suggest possible insights for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_J/0/1/0/all/0/1&quot;&gt;Jianwu Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_J/0/1/0/all/0/1&quot;&gt;Jianru Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chua_T/0/1/0/all/0/1&quot;&gt;Tat-seng Chua&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.01842">
<title>Construction of Hierarchical Neural Architecture Search Spaces based on Context-free Grammars. (arXiv:2211.01842v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.01842</link>
<description rdf:parseType="Literal">&lt;p&gt;The discovery of neural architectures from simple building blocks is a
long-standing goal of Neural Architecture Search (NAS). Hierarchical search
spaces are a promising step towards this goal but lack a unifying search space
design framework and typically only search over some limited aspect of
architectures. In this work, we introduce a unifying search space design
framework based on context-free grammars that can naturally and compactly
generate expressive hierarchical search spaces that are 100s of orders of
magnitude larger than common spaces from the literature. By enhancing and using
their properties, we effectively enable search over the complete architecture
and can foster regularity. Further, we propose an efficient hierarchical kernel
design for a Bayesian Optimization search strategy to efficiently search over
such huge spaces. We demonstrate the versatility of our search space design
framework and show that our search strategy can be superior to existing NAS
approaches. Code is available at
https://github.com/automl/hierarchical_nas_construction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schrodi_S/0/1/0/all/0/1&quot;&gt;Simon Schrodi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stoll_D/0/1/0/all/0/1&quot;&gt;Danny Stoll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ru_B/0/1/0/all/0/1&quot;&gt;Binxin Ru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sukthanker_R/0/1/0/all/0/1&quot;&gt;Rhea Sukthanker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brox_T/0/1/0/all/0/1&quot;&gt;Thomas Brox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hutter_F/0/1/0/all/0/1&quot;&gt;Frank Hutter&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.03408">
<title>RITA: Boost Driving Simulators with Realistic Interactive Traffic Flow. (arXiv:2211.03408v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2211.03408</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality traffic flow generation is the core module in building
simulators for autonomous driving. However, the majority of available
simulators are incapable of replicating traffic patterns that accurately
reflect the various features of real-world data while also simulating
human-like reactive responses to the tested autopilot driving strategies.
Taking one step forward to addressing such a problem, we propose Realistic
Interactive TrAffic flow (RITA) as an integrated component of existing driving
simulators to provide high-quality traffic flow for the evaluation and
optimization of the tested driving strategies. RITA is developed with
consideration of three key features, i.e., fidelity, diversity, and
controllability, and consists of two core modules called RITABackend and
RITAKit. RITABackend is built to support vehicle-wise control and provide
traffic generation models from real-world datasets, while RITAKit is developed
with easy-to-use interfaces for controllable traffic generation via
RITABackend. We demonstrate RITA&apos;s capacity to create diversified and
high-fidelity traffic simulations in several highly interactive highway
scenarios. The experimental findings demonstrate that our produced RITA traffic
flows exhibit all three key features, hence enhancing the completeness of
driving strategy evaluation. Moreover, we showcase the possibility for further
improvement of baseline strategies through online fine-tuning with RITA traffic
flows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhengbang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shenyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Y/0/1/0/all/0/1&quot;&gt;Yuzheng Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuecheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_L/0/1/0/all/0/1&quot;&gt;Liyuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Z/0/1/0/all/0/1&quot;&gt;Ziqin Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kai_S/0/1/0/all/0/1&quot;&gt;Shixiong Kai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Qiang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yong Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.09744">
<title>DSI++: Updating Transformer Memory with New Documents. (arXiv:2212.09744v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.09744</link>
<description rdf:parseType="Literal">&lt;p&gt;Differentiable Search Indices (DSIs) encode a corpus of documents in model
parameters and use the same model to answer user queries directly. Despite the
strong performance of DSI models, deploying them in situations where the corpus
changes over time is computationally expensive because reindexing the corpus
requires re-training the model. In this work, we introduce DSI++, a continual
learning challenge for DSI to incrementally index new documents while being
able to answer queries related to both previously and newly indexed documents.
Across different model scales and document identifier representations, we show
that continual indexing of new documents leads to considerable forgetting of
previously indexed documents. We also hypothesize and verify that the model
experiences forgetting events during training, leading to unstable learning. To
mitigate these issues, we investigate two approaches. The first focuses on
modifying the training dynamics. Flatter minima implicitly alleviate
forgetting, so we optimize for flatter loss basins and show that the model
stably memorizes more documents ($+12\%$). Next, we introduce a generative
memory to sample pseudo-queries for documents and supplement them during
continual indexing to prevent forgetting for the retrieval task. Extensive
experiments on novel continual indexing benchmarks based on Natural Questions
(NQ) and MS MARCO demonstrate that our proposed solution mitigates forgetting
significantly. Concretely, it improves the average Hits@10 by $+21.1\%$ over
competitive baselines for NQ and requires $6$ times fewer model updates
compared to re-training the DSI model for incrementally indexing five corpora
in a sequence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehta_S/0/1/0/all/0/1&quot;&gt;Sanket Vaibhav Mehta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_J/0/1/0/all/0/1&quot;&gt;Jai Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tay_Y/0/1/0/all/0/1&quot;&gt;Yi Tay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dehghani_M/0/1/0/all/0/1&quot;&gt;Mostafa Dehghani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_V/0/1/0/all/0/1&quot;&gt;Vinh Q. Tran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_J/0/1/0/all/0/1&quot;&gt;Jinfeng Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Najork_M/0/1/0/all/0/1&quot;&gt;Marc Najork&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strubell_E/0/1/0/all/0/1&quot;&gt;Emma Strubell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metzler_D/0/1/0/all/0/1&quot;&gt;Donald Metzler&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01404">
<title>Provably Bounding Neural Network Preimages. (arXiv:2302.01404v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01404</link>
<description rdf:parseType="Literal">&lt;p&gt;Most work on the formal verification of neural networks has focused on
bounding the set of outputs that correspond to a given set of inputs (for
example, bounded perturbations of a nominal input). However, many use cases of
neural network verification require solving the inverse problem, or
over-approximating the set of inputs that lead to certain outputs. We present
the INVPROP algorithm for verifying properties over the preimage of a linearly
constrained output set, which can be combined with branch-and-bound to increase
precision. Contrary to other approaches, our efficient algorithm is
GPU-accelerated and does not require a linear programming solver. We
demonstrate our algorithm for identifying safe control regions for a dynamical
system via backward reachability analysis, verifying adversarial robustness,
and detecting out-of-distribution inputs to a neural network. Our results show
that in certain settings, we find over-approximations over 2500x tighter than
prior work while being 2.5x faster. By strengthening robustness verification
with output constraints, we consistently verify more properties than the
previous state-of-the-art on multiple benchmarks, including a large model with
167k neurons in VNN-COMP 2023. Our algorithm has been incorporated into the
$\alpha,\!\beta$-CROWN verifier, available at https://abcrown.org.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotha_S/0/1/0/all/0/1&quot;&gt;Suhas Kotha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brix_C/0/1/0/all/0/1&quot;&gt;Christopher Brix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_Z/0/1/0/all/0/1&quot;&gt;Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dvijotham_K/0/1/0/all/0/1&quot;&gt;Krishnamurthy Dvijotham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Huan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02407">
<title>HyPHEN: A Hybrid Packing Method and Optimizations for Homomorphic Encryption-Based Neural Networks. (arXiv:2302.02407v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02407</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural network (CNN) inference using fully homomorphic
encryption (FHE) is a promising private inference (PI) solution due to the
capability of FHE that enables offloading the whole computation process to the
server while protecting the privacy of sensitive user data. Prior FHE-based CNN
(HCNN) work has demonstrated the feasibility of constructing deep neural
network architectures such as ResNet using FHE. Despite these advancements,
HCNN still faces significant challenges in practicality due to the high
computational and memory overhead. To overcome these limitations, we present
HyPHEN, a deep HCNN construction that incorporates novel convolution algorithms
(RAConv and CAConv), data packing methods (2D gap packing and PRCR scheme), and
optimization techniques tailored to HCNN construction. Such enhancements enable
HyPHEN to substantially reduce the memory footprint and the number of expensive
homomorphic operations, such as ciphertext rotation and bootstrapping. As a
result, HyPHEN brings the latency of HCNN CIFAR-10 inference down to a
practical level at 1.4 seconds (ResNet-20) and demonstrates HCNN ImageNet
inference for the first time at 14.7 seconds (ResNet-18).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Donghwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jaiyoung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jongmin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangpyo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_J/0/1/0/all/0/1&quot;&gt;Jung Ho Ahn&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05066">
<title>Distortion-Disentangled Contrastive Learning. (arXiv:2303.05066v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05066</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning is well known for its remarkable performance in
representation learning and various downstream computer vision tasks. Recently,
Positive-pair-Only Contrastive Learning (POCL) has achieved reliable
performance without the need to construct positive-negative training sets. It
reduces memory requirements by lessening the dependency on the batch size. The
POCL method typically uses a single loss function to extract the distortion
invariant representation (DIR) which describes the proximity of positive-pair
representations affected by different distortions. This loss function
implicitly enables the model to filter out or ignore the distortion variant
representation (DVR) affected by different distortions. However, existing POCL
methods do not explicitly enforce the disentanglement and exploitation of the
actually valuable DVR. In addition, these POCL methods have been observed to be
sensitive to augmentation strategies. To address these limitations, we propose
a novel POCL framework named Distortion-Disentangled Contrastive Learning
(DDCL) and a Distortion-Disentangled Loss (DDL). Our approach is the first to
explicitly disentangle and exploit the DVR inside the model and feature stream
to improve the overall representation utilization efficiency, robustness and
representation ability. Experiments carried out demonstrate the superiority of
our framework to Barlow Twins and Simsiam in terms of convergence,
representation quality, and robustness on several benchmark datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinfeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Sifan Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_J/0/1/0/all/0/1&quot;&gt;Jionglong Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;S. Kevin Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.09424">
<title>Loss Minimization Yields Multicalibration for Large Neural Networks. (arXiv:2304.09424v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.09424</link>
<description rdf:parseType="Literal">&lt;p&gt;Multicalibration is a notion of fairness for predictors that requires them to
provide calibrated predictions across a large set of protected groups.
Multicalibration is known to be a distinct goal than loss minimization, even
for simple predictors such as linear functions.
&lt;/p&gt;
&lt;p&gt;In this work, we consider the setting where the protected groups can be
represented by neural networks of size $k$, and the predictors are neural
networks of size $n &amp;gt; k$. We show that minimizing the squared loss over all
neural nets of size $n$ implies multicalibration for all but a bounded number
of unlucky values of $n$. We also give evidence that our bound on the number of
unlucky values is tight, given our proof technique. Previously, results of the
flavor that loss minimization yields multicalibration were known only for
predictors that were near the ground truth, hence were rather limited in
applicability. Unlike these, our results rely on the expressivity of neural
nets and utilize the representation of the predictor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blasiok_J/0/1/0/all/0/1&quot;&gt;Jaros&amp;#x142;aw B&amp;#x142;asiok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gopalan_P/0/1/0/all/0/1&quot;&gt;Parikshit Gopalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lunjia Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalai_A/0/1/0/all/0/1&quot;&gt;Adam Tauman Kalai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakkiran_P/0/1/0/all/0/1&quot;&gt;Preetum Nakkiran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.11090">
<title>Towards Responsible AI in the Era of Generative AI: A Reference Architecture for Designing Foundation Model based Systems. (arXiv:2304.11090v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2304.11090</link>
<description rdf:parseType="Literal">&lt;p&gt;The release of ChatGPT has drawn huge interests on foundations models. There
is a broad consensus that foundations models will be the fundamental building
blocks for future AI systems. However, there is a lack of systematic guidance
on the architecture design. Particularly, the the rapidly growing capabilities
of foundations models can eventually absorb other components of AI systems,
posing challenges of moving boundary and interface evolution in architecture
design. Furthermore, incorporating foundations models into AI systems raises
significant concerns about responsible AI due to their opaque nature and
rapidly advancing intelligence. To address these challenges, the paper first
presents an architecture evolution of AI systems in the era of foundation
models, transitioning from &quot;foundation-model-as-a-connector&quot; to
&quot;foundation-model-as-a-monolithic architecture&quot;. The paper then identifies key
design decisions and proposes a pattern-oriented reference architecture for
designing responsible foundation-model-based systems. The patterns can enable
the potential of foundation models while minimising associated risks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Q/0/1/0/all/0/1&quot;&gt;Qinghua Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Liming Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiwei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_Z/0/1/0/all/0/1&quot;&gt;Zhenchang Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Whittle_J/0/1/0/all/0/1&quot;&gt;Jon Whittle&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10406">
<title>Variational Classification. (arXiv:2305.10406v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a latent variable model for classification that provides a novel
probabilistic interpretation of neural network softmax classifiers. We derive a
variational objective to train the model, analogous to the evidence lower bound
(ELBO) used to train variational auto-encoders, that generalises the
cross-entropy loss used to train classification models. Treating inputs to the
softmax layer as samples of a latent variable, our abstracted perspective
reveals a potential inconsistency between their anticipated distribution,
required for accurate label predictions, and the empirical distribution they
follow in practice. We then devise a variational objective to mitigate such
inconsistency and encourage a specified latent distribution, instead of the
implicit assumption in off-the-shelf softmax classifiers. Overall, we provide
new theoretical insight into the inner workings of widely-used softmax
classification; and empirical evaluation on image and text classification
datasets demonstrates that our proposed remedy, variational classification,
maintains classification accuracy while the reshaped latent space improves
other desirable classifier properties, such as calibration, adversarial
robustness, robustness to distribution shift and sample efficiency useful in
low data settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1&quot;&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13877">
<title>NarrativeXL: A Large-scale Dataset For Long-Term Memory Models. (arXiv:2305.13877v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13877</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a new large-scale (nearly a million questions) ultra-long-context
(more than 50,000 words average document length) reading comprehension dataset.
Using GPT 3.5, we summarized each scene in 1,500 hand-curated fiction books
from Project Gutenberg, which resulted in approximately 150 scene-level
summaries per book. After that, we created a number of reading comprehension
questions based on these summaries, including three types of multiple-choice
scene recognition questions, as well as free-form narrative reconstruction
questions. With 990,595 total questions, our dataset is an order of magnitude
larger than the closest alternatives. Crucially, most questions have a known
``retention demand&apos;&apos;, indicating how long-term of a memory is needed to answer
them, which should aid long-term memory performance evaluation. We validate our
data in four small-scale experiments: one with human labelers, and three with
existing language models. We show that our questions 1) adequately represent
the source material 2) can be used to diagnose a model&apos;s memory capacity 3) are
not trivial for modern language models even when the memory demand does not
exceed those models&apos; context lengths. Lastly, we provide our code which can be
used to further expand the dataset with minimal human labor.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moskvichev_A/0/1/0/all/0/1&quot;&gt;Arseny Moskvichev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_K/0/1/0/all/0/1&quot;&gt;Ky-Vinh Mai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14938">
<title>Do LLMs Understand Social Knowledge? Evaluating the Sociability of Large Language Models with SocKET Benchmark. (arXiv:2305.14938v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14938</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have been shown to perform well at a variety of
syntactic, discourse, and reasoning tasks. While LLMs are increasingly deployed
in many forms including conversational agents that interact with humans, we
lack a grounded benchmark to measure how well LLMs understand \textit{social}
language. Here, we introduce a new theory-driven benchmark, SocKET, that
contains 58 NLP tasks testing social knowledge which we group into five
categories: humor &amp;amp; sarcasm, offensiveness, sentiment &amp;amp; emotion, and
trustworthiness. In tests on the benchmark, we demonstrate that current models
attain only moderate performance but reveal significant potential for task
transfer among different types and categories of tasks, which were predicted
from theory. Through zero-shot evaluations, we show that pretrained models
already possess some innate but limited capabilities of social language
understanding and training on one category of tasks can improve zero-shot
testing on others. Our benchmark provides a systematic way to analyze model
performance on an important dimension of language and points to clear room for
improvement to build more socially-aware LLMs. The associated resources are
released at https://github.com/minjechoi/SOCKET.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_M/0/1/0/all/0/1&quot;&gt;Minje Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pei_J/0/1/0/all/0/1&quot;&gt;Jiaxin Pei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_S/0/1/0/all/0/1&quot;&gt;Sagar Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_C/0/1/0/all/0/1&quot;&gt;Chang Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jurgens_D/0/1/0/all/0/1&quot;&gt;David Jurgens&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00006">
<title>Truncated Affinity Maximization: One-class Homophily Modeling for Graph Anomaly Detection. (arXiv:2306.00006v4 [cs.SI] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00006</link>
<description rdf:parseType="Literal">&lt;p&gt;We reveal a one-class homophily phenomenon, which is one prevalent property
we find empirically in real-world graph anomaly detection (GAD) datasets, i.e.,
normal nodes tend to have strong connection/affinity with each other, while the
homophily in abnormal nodes is significantly weaker than normal nodes. However,
this anomaly-discriminative property is ignored by existing GAD methods that
are typically built using a conventional anomaly detection objective, such as
data reconstruction. In this work, we explore this property to introduce a
novel unsupervised anomaly scoring measure for GAD, local node affinity, that
assigns a larger anomaly score to nodes that are less affiliated with their
neighbors, with the affinity defined as similarity on node
attributes/representations. We further propose Truncated Affinity Maximization
(TAM) that learns tailored node representations for our anomaly measure by
maximizing the local affinity of nodes to their neighbors. Optimizing on the
original graph structure can be biased by nonhomophily edges (i.e., edges
connecting normal and abnormal nodes). Thus, TAM is instead optimized on
truncated graphs where non-homophily edges are removed iteratively to mitigate
this bias. The learned representations result in significantly stronger local
affinity for normal nodes than abnormal nodes. Extensive empirical results on
10 real-world GAD datasets show that TAM substantially outperforms seven
competing models, achieving over 10% increase in AUROC/AUPRC compared to the
best contenders on challenging datasets. Our code is available at
https://github.com/mala-lab/TAM-master/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_H/0/1/0/all/0/1&quot;&gt;Hezhe Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_G/0/1/0/all/0/1&quot;&gt;Guansong Pang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.02204">
<title>Cycle Consistency Driven Object Discovery. (arXiv:2306.02204v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.02204</link>
<description rdf:parseType="Literal">&lt;p&gt;Developing deep learning models that effectively learn object-centric
representations, akin to human cognition, remains a challenging task. Existing
approaches facilitate object discovery by representing objects as fixed-size
vectors, called ``slots&apos;&apos; or ``object files&apos;&apos;. While these approaches have
shown promise in certain scenarios, they still exhibit certain limitations.
First, they rely on architectural priors which can be unreliable and usually
require meticulous engineering to identify the correct objects. Second, there
has been a notable gap in investigating the practical utility of these
representations in downstream tasks. To address the first limitation, we
introduce a method that explicitly optimizes the constraint that each object in
a scene should be associated with a distinct slot. We formalize this constraint
by introducing consistency objectives which are cyclic in nature. By
integrating these consistency objectives into various existing slot-based
object-centric methods, we showcase substantial improvements in
object-discovery performance. These enhancements consistently hold true across
both synthetic and real-world scenes, underscoring the effectiveness and
adaptability of the proposed approach. To tackle the second limitation, we
apply the learned object-centric representations from the proposed method to
two downstream reinforcement learning tasks, demonstrating considerable
performance enhancements compared to conventional slot-based and monolithic
representation learning methods. Our results suggest that the proposed approach
not only improves object discovery, but also provides richer features for
downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Didolkar_A/0/1/0/all/0/1&quot;&gt;Aniket Didolkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goyal_A/0/1/0/all/0/1&quot;&gt;Anirudh Goyal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bengio_Y/0/1/0/all/0/1&quot;&gt;Yoshua Bengio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05415">
<title>Causal normalizing flows: from theory to practice. (arXiv:2306.05415v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05415</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we deepen on the use of normalizing flows for causal reasoning.
Specifically, we first leverage recent results on non-linear ICA to show that
causal models are identifiable from observational data given a causal ordering,
and thus can be recovered using autoregressive normalizing flows (NFs). Second,
we analyze different design and learning choices for causal normalizing flows
to capture the underlying causal data-generating process. Third, we describe
how to implement the do-operator in causal NFs, and thus, how to answer
interventional and counterfactual questions. Finally, in our experiments, we
validate our design and training choices through a comprehensive ablation
study; compare causal NFs to other approaches for approximating causal models;
and empirically demonstrate that causal NFs can be used to address real-world
problems, where the presence of mixed discrete-continuous data and partial
knowledge on the causal graph is the norm. The code for this work can be found
at https://github.com/psanch21/causal-flows.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javaloy_A/0/1/0/all/0/1&quot;&gt;Adri&amp;#xe1;n Javaloy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_Martin_P/0/1/0/all/0/1&quot;&gt;Pablo S&amp;#xe1;nchez-Mart&amp;#xed;n&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Valera_I/0/1/0/all/0/1&quot;&gt;Isabel Valera&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.05668">
<title>RePaint-NeRF: NeRF Editting via Semantic Masks and Diffusion Models. (arXiv:2306.05668v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.05668</link>
<description rdf:parseType="Literal">&lt;p&gt;The emergence of Neural Radiance Fields (NeRF) has promoted the development
of synthesized high-fidelity views of the intricate real world. However, it is
still a very demanding task to repaint the content in NeRF. In this paper, we
propose a novel framework that can take RGB images as input and alter the 3D
content in neural scenes. Our work leverages existing diffusion models to guide
changes in the designated 3D content. Specifically, we semantically select the
target object and a pre-trained diffusion model will guide the NeRF model to
generate new 3D objects, which can improve the editability, diversity, and
application range of NeRF. Experiment results show that our algorithm is
effective for editing 3D objects in NeRF under different text prompts,
including editing appearance, shape, and more. We validate our method on both
real-world datasets and synthetic-world datasets for these editing tasks.
Please visit https://starstesla.github.io/repaintnerf for a better view of our
results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xingchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Y/0/1/0/all/0/1&quot;&gt;Ying He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;F. Richard Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jianqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;You Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13596">
<title>Max-Margin Token Selection in Attention Mechanism. (arXiv:2306.13596v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13596</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention mechanism is a central component of the transformer architecture
which led to the phenomenal success of large language models. However, the
theoretical principles underlying the attention mechanism are poorly
understood, especially its nonconvex optimization dynamics. In this work, we
explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle
\boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where
$\boldsymbol{X}$ is the token sequence and
$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are trainable parameters. We
prove that running gradient descent on $\boldsymbol{p}$, or equivalently
$\boldsymbol{W}$, converges in direction to a max-margin solution that
separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly
formalizes attention as an optimal token selection mechanism. Remarkably, our
results are applicable to general data and precisely characterize
$\textit{optimality}$ of tokens in terms of the value embeddings
$\boldsymbol{Xv}$ and problem geometry. We also provide a broader
regularization path analysis that establishes the margin maximizing nature of
attention even for nonlinear prediction heads. When optimizing $\boldsymbol{v}$
and $\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions
under which the regularization paths directionally converge to their respective
hard-margin SVM solutions where $\boldsymbol{v}$ separates the input features
based on their labels. Interestingly, the SVM formulation of $\boldsymbol{p}$
is influenced by the support vector geometry of $\boldsymbol{v}$. Finally, we
verify our theoretical findings via numerical experiments and provide insights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1&quot;&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingcong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuechen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15657">
<title>The Distortion of Binomial Voting Defies Expectation. (arXiv:2306.15657v2 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15657</link>
<description rdf:parseType="Literal">&lt;p&gt;In computational social choice, the distortion of a voting rule quantifies
the degree to which the rule overcomes limited preference information to select
a socially desirable outcome. This concept has been investigated extensively,
but only through a worst-case lens. Instead, we study the expected distortion
of voting rules with respect to an underlying distribution over voter
utilities. Our main contribution is the design and analysis of a novel and
intuitive rule, binomial voting, which provides strong distribution-independent
guarantees for both expected distortion and expected welfare.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gonczarowski_Y/0/1/0/all/0/1&quot;&gt;Yannai A. Gonczarowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kehne_G/0/1/0/all/0/1&quot;&gt;Gregory Kehne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Procaccia_A/0/1/0/all/0/1&quot;&gt;Ariel D. Procaccia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiffer_B/0/1/0/all/0/1&quot;&gt;Ben Schiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shirley Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.16680">
<title>On the Trustworthiness Landscape of State-of-the-art Generative Models: A Survey and Outlook. (arXiv:2307.16680v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.16680</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models and large language models have emerged as leading-edge
generative models, revolutionizing various aspects of human life. However, the
practical implementations of these models have also exposed inherent risks,
bringing to the forefront their evil sides and sparking concerns regarding
their trustworthiness. Despite the wealth of literature on this subject, a
comprehensive survey specifically delving into the intersection of large-scale
generative models and their trustworthiness remains largely absent. To bridge
this gap, this paper investigates both the long-standing and emerging threats
associated with these models across four fundamental dimensions: 1) privacy, 2)
security, 3) fairness, and 4) responsibility. Based on the investigation
results, we develop an extensive map outlining the trustworthiness of large
generative models. After that, we provide practical recommendations and
potential research directions for future secure applications equipped with
large generative models, ultimately promoting the trustworthiness of the models
and benefiting the society as a whole.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_M/0/1/0/all/0/1&quot;&gt;Mingyuan Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11471">
<title>Dynamic Open Vocabulary Enhanced Safe-landing with Intelligence (DOVESEI). (arXiv:2308.11471v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11471</link>
<description rdf:parseType="Literal">&lt;p&gt;This work targets what we consider to be the foundational step for urban
airborne robots, a safe landing. Our attention is directed toward what we deem
the most crucial aspect of the safe landing perception stack: segmentation. We
present a streamlined reactive UAV system that employs visual servoing by
harnessing the capabilities of open vocabulary image segmentation. This
approach can adapt to various scenarios with minimal adjustments, bypassing the
necessity for extensive data accumulation for refining internal models, thanks
to its open vocabulary methodology. Given the limitations imposed by local
authorities, our primary focus centers on operations originating from altitudes
of 100 meters. This choice is deliberate, as numerous preceding works have
dealt with altitudes up to 30 meters, aligning with the capabilities of small
stereo cameras. Consequently, we leave the remaining 20m to be navigated using
conventional 3D path planning methods. Utilizing monocular cameras and image
segmentation, our findings demonstrate the system&apos;s capability to successfully
execute landing maneuvers at altitudes as low as 20 meters. However, this
approach is vulnerable to intermittent and occasionally abrupt fluctuations in
the segmentation between frames in a video stream. To address this challenge,
we enhance the image segmentation output by introducing what we call a dynamic
focus: a masking mechanism that self adjusts according to the current landing
stage. This dynamic focus guides the control system to avoid regions beyond the
drone&apos;s safety radius projected onto the ground, thus mitigating the problems
with fluctuations. Through the implementation of this supplementary layer, our
experiments have reached improvements in the landing success rate of almost
tenfold when compared to global segmentation. All the source code is open
source and available online (github.com/MISTLab/DOVESEI).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bong_H/0/1/0/all/0/1&quot;&gt;Haechan Mark Bong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Azambuja_R/0/1/0/all/0/1&quot;&gt;Ricardo de Azambuja&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beltrame_G/0/1/0/all/0/1&quot;&gt;Giovanni Beltrame&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03251">
<title>Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03251</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph
(KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial
task that aims to predict future facts based on historical occurrences. The key
challenge lies in uncovering structural dependencies within historical
subgraphs and temporal patterns. Most existing approaches model TKGs relying on
entity modeling, as nodes in the graph play a crucial role in knowledge
representation. However, the real-world scenario often involves an extensive
number of entities, with new entities emerging over time. This makes it
challenging for entity-dependent methods to cope with extensive volumes of
entities, and effectively handling newly emerging entities also becomes a
significant challenge. Therefore, we propose Temporal Inductive Path Neural
Network (TiPNN), which models historical information in an entity-independent
perspective. Specifically, TiPNN adopts a unified graph, namely history
temporal graph, to comprehensively capture and encapsulate information from
history. Subsequently, we utilize the defined query-aware temporal paths to
model historical path information related to queries on history temporal graph
for the reasoning. Extensive experiments illustrate that the proposed model not
only attains significant performance enhancements but also handles inductive
settings, while additionally facilitating the provision of reasoning evidence
through history temporal graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Meng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03886">
<title>FIND: A Function Description Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03886</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeling neural network submodules with human-legible descriptions is useful
for many downstream tasks: such descriptions can surface failures, guide
interventions, and perhaps even explain important model behaviors. To date,
most mechanistic descriptions of trained networks have involved small models,
narrowly delimited phenomena, and large amounts of human labor. Labeling all
human-interpretable sub-computations in models of increasing size and
complexity will almost certainly require tools that can generate and validate
descriptions automatically. Recently, techniques that use learned models
in-the-loop for labeling have begun to gain traction, but methods for
evaluating their efficacy are limited and ad-hoc. How should we validate and
compare open-ended labeling tools? This paper introduces FIND (Function
INterpretation and Description), a benchmark suite for evaluating the building
blocks of automated interpretability methods. FIND contains functions that
resemble components of trained neural networks, and accompanying descriptions
of the kind we seek to generate. The functions span textual and numeric
domains, and involve a range of real-world complexities. We evaluate methods
that use pretrained language models (LMs) to produce descriptions of function
behavior in natural language and code. Additionally, we introduce a new
interactive method in which an Automated Interpretability Agent (AIA) generates
function descriptions. We find that an AIA, built from an LM with black-box
access to functions, can infer function structure, acting as a scientist by
forming hypotheses, proposing experiments, and updating descriptions in light
of new data. However, AIA descriptions tend to capture global function behavior
and miss local details. These results suggest that FIND will be useful for
evaluating more sophisticated interpretability methods before they are applied
to real-world models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1&quot;&gt;Sarah Schwettmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1&quot;&gt;Tamar Rott Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1&quot;&gt;Joanna Materzynska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1&quot;&gt;Neil Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.14348">
<title>Defending Against Alignment-Breaking Attacks via Robustly Aligned LLM. (arXiv:2309.14348v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.14348</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, Large Language Models (LLMs) have made significant advancements and
are now widely used across various domains. Unfortunately, there has been a
rising concern that LLMs can be misused to generate harmful or malicious
content. Though a line of research has focused on aligning LLMs with human
values and preventing them from producing inappropriate content, such
alignments are usually vulnerable and can be bypassed by alignment-breaking
attacks via adversarially optimized or handcrafted jailbreaking prompts. In
this work, we introduce a Robustly Aligned LLM (RA-LLM) to defend against
potential alignment-breaking attacks. RA-LLM can be directly constructed upon
an existing aligned LLM with a robust alignment checking function, without
requiring any expensive retraining or fine-tuning process of the original LLM.
Furthermore, we also provide a theoretical analysis for RA-LLM to verify its
effectiveness in defending against alignment-breaking attacks. Through
real-world experiments on open-source large language models, we demonstrate
that RA-LLM can successfully defend against both state-of-the-art adversarial
prompts and popular handcrafted jailbreaking prompts by reducing their attack
success rates from nearly 100% to around 10% or less.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_B/0/1/0/all/0/1&quot;&gt;Bochuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuanpu Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jinghui Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.16620">
<title>Depthwise Hyperparameter Transfer in Residual Networks: Dynamics and Scaling Limit. (arXiv:2309.16620v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2309.16620</link>
<description rdf:parseType="Literal">&lt;p&gt;The cost of hyperparameter tuning in deep learning has been rising with model
sizes, prompting practitioners to find new tuning methods using a proxy of
smaller networks. One such proposal uses $\mu$P parameterized networks, where
the optimal hyperparameters for small width networks transfer to networks with
arbitrarily large width. However, in this scheme, hyperparameters do not
transfer across depths. As a remedy, we study residual networks with a residual
branch scale of $1/\sqrt{\text{depth}}$ in combination with the $\mu$P
parameterization. We provide experiments demonstrating that residual
architectures including convolutional ResNets and Vision Transformers trained
with this parameterization exhibit transfer of optimal hyperparameters across
width and depth on CIFAR-10 and ImageNet. Furthermore, our empirical findings
are supported and motivated by theory. Using recent developments in the
dynamical mean field theory (DMFT) description of neural network learning
dynamics, we show that this parameterization of ResNets admits a well-defined
feature learning joint infinite-width and infinite-depth limit and show
convergence of finite-size network dynamics towards this limit.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bordelon_B/0/1/0/all/0/1&quot;&gt;Blake Bordelon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Noci_L/0/1/0/all/0/1&quot;&gt;Lorenzo Noci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mufan Bill Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Hanin_B/0/1/0/all/0/1&quot;&gt;Boris Hanin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Pehlevan_C/0/1/0/all/0/1&quot;&gt;Cengiz Pehlevan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01557">
<title>SmartPlay: A Benchmark for LLMs as Intelligent Agents. (arXiv:2310.01557v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01557</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent large language models (LLMs) have demonstrated great potential toward
intelligent agents and next-gen automation, but there currently lacks a
systematic benchmark for evaluating LLMs&apos; abilities as agents. We introduce
SmartPlay: both a challenging benchmark and a methodology for evaluating LLMs
as agents. SmartPlay consists of 6 different games, including
Rock-Paper-Scissors, Tower of Hanoi, Minecraft. Each game features a unique
setting, providing up to 20 evaluation settings and infinite environment
variations. Each game in SmartPlay uniquely challenges a subset of 9 important
capabilities of an intelligent LLM agent, including reasoning with object
dependencies, planning ahead, spatial reasoning, learning from history, and
understanding randomness. The distinction between the set of capabilities each
game test allows us to analyze each capability separately. SmartPlay serves not
only as a rigorous testing ground for evaluating the overall performance of LLM
agents but also as a road-map for identifying gaps in current methodologies. We
release our benchmark at github.com/microsoft/SmartPlay
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yue Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_X/0/1/0/all/0/1&quot;&gt;Xuan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitchell_T/0/1/0/all/0/1&quot;&gt;Tom M. Mitchell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yuanzhi Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03234">
<title>Non-Smooth Weakly-Convex Finite-sum Coupled Compositional Optimization. (arXiv:2310.03234v3 [math.OC] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03234</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates new families of compositional optimization problems,
called $\underline{\bf n}$on-$\underline{\bf s}$mooth $\underline{\bf
w}$eakly-$\underline{\bf c}$onvex $\underline{\bf f}$inite-sum $\underline{\bf
c}$oupled $\underline{\bf c}$ompositional $\underline{\bf o}$ptimization (NSWC
FCCO). There has been a growing interest in FCCO due to its wide-ranging
applications in machine learning and AI, as well as its ability to address the
shortcomings of stochastic algorithms based on empirical risk minimization.
However, current research on FCCO presumes that both the inner and outer
functions are smooth, limiting their potential to tackle a more diverse set of
problems. Our research expands on this area by examining non-smooth
weakly-convex FCCO, where the outer function is weakly convex and
non-decreasing, and the inner function is weakly-convex. We analyze a
single-loop algorithm and establish its complexity for finding an
$\epsilon$-stationary point of the Moreau envelop of the objective function.
Additionally, we also extend the algorithm to solving novel non-smooth
weakly-convex tri-level finite-sum coupled compositional optimization problems,
which feature a nested arrangement of three functions. Lastly, we explore the
applications of our algorithms in deep learning for two-way partial AUC
maximization and multi-instance two-way partial AUC maximization, using
empirical studies to showcase the effectiveness of the proposed algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Hu_Q/0/1/0/all/0/1&quot;&gt;Quanqi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dixian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianbao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.04852">
<title>Balancing utility and cognitive cost in social representation. (arXiv:2310.04852v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.04852</link>
<description rdf:parseType="Literal">&lt;p&gt;To successfully navigate its environment, an agent must construct and
maintain representations of the other agents that it encounters. Such
representations are useful for many tasks, but they are not without cost. As a
result, agents must make decisions regarding how much information they choose
to store about the agents in their environment. Using selective social learning
as an example task, we motivate the problem of finding agent representations
that optimally trade off between downstream utility and information cost, and
illustrate two example approaches to resource-constrained social
representation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Taylor_Davies_M/0/1/0/all/0/1&quot;&gt;Max Taylor-Davies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_C/0/1/0/all/0/1&quot;&gt;Christopher G. Lucas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.06089">
<title>Predictive auxiliary objectives in deep RL mimic learning in the brain. (arXiv:2310.06089v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.06089</link>
<description rdf:parseType="Literal">&lt;p&gt;The ability to predict upcoming events has been hypothesized to comprise a
key aspect of natural and machine cognition. This is supported by trends in
deep reinforcement learning (RL), where self-supervised auxiliary objectives
such as prediction are widely used to support representation learning and
improve task performance. Here, we study the effects predictive auxiliary
objectives have on representation learning across different modules of an RL
system and how these mimic representational changes observed in the brain. We
find that predictive objectives improve and stabilize learning particularly in
resource-limited architectures, and we identify settings where longer
predictive horizons better support representational transfer. Furthermore, we
find that representational changes in this RL system bear a striking
resemblance to changes in neural activity observed in the brain across various
experiments. Specifically, we draw a connection between the auxiliary
predictive model of the RL system and hippocampus, an area thought to learn a
predictive model to support memory-guided behavior. We also connect the encoder
network and the value learning network of the RL system to visual cortex and
striatum in the brain, respectively. This work demonstrates how representation
learning in deep RL systems can provide an interpretable framework for modeling
multi-region interactions in the brain. The deep RL perspective taken here also
suggests an additional role of the hippocampus in the brain -- that of an
auxiliary learning system that benefits representation learning in other
regions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_C/0/1/0/all/0/1&quot;&gt;Ching Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stachenfeld_K/0/1/0/all/0/1&quot;&gt;Kimberly L Stachenfeld&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.13032">
<title>Quality-Diversity through AI Feedback. (arXiv:2310.13032v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.13032</link>
<description rdf:parseType="Literal">&lt;p&gt;In many text-generation problems, users may prefer not only a single
response, but a diverse range of high-quality outputs from which to choose.
Quality-diversity (QD) search algorithms aim at such outcomes, by continually
improving and diversifying a population of candidates. However, the
applicability of QD to qualitative domains, like creative writing, has been
limited by the difficulty of algorithmically specifying measures of quality and
diversity. Interestingly, recent developments in language models (LMs) have
enabled guiding search through AI feedback, wherein LMs are prompted in natural
language to evaluate qualitative aspects of text. Leveraging this development,
we introduce Quality-Diversity through AI Feedback (QDAIF), wherein an
evolutionary algorithm applies LMs to both generate variation and evaluate the
quality and diversity of candidate text. When assessed on creative writing
domains, QDAIF covers more of a specified search space with high-quality
samples than do non-QD controls. Further, human evaluation of QDAIF-generated
creative texts validates reasonable agreement between AI and human evaluation.
Our results thus highlight the potential of AI feedback to guide open-ended
search for creative and original solutions, providing a recipe that seemingly
generalizes to many domains and modalities. In this way, QDAIF is a step
towards AI systems that can independently search, diversify, evaluate, and
improve, which are among the core skills underlying human society&apos;s capacity
for innovation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bradley_H/0/1/0/all/0/1&quot;&gt;Herbie Bradley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_A/0/1/0/all/0/1&quot;&gt;Andrew Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teufel_H/0/1/0/all/0/1&quot;&gt;Hannah Teufel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jenny Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oostermeijer_K/0/1/0/all/0/1&quot;&gt;Koen Oostermeijer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bellagente_M/0/1/0/all/0/1&quot;&gt;Marco Bellagente&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clune_J/0/1/0/all/0/1&quot;&gt;Jeff Clune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stanley_K/0/1/0/all/0/1&quot;&gt;Kenneth Stanley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schott_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;gory Schott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lehman_J/0/1/0/all/0/1&quot;&gt;Joel Lehman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.03393">
<title>Sketching Multidimensional Time Series for Fast Discord Mining. (arXiv:2311.03393v3 [cs.DB] UPDATED)</title>
<link>http://arxiv.org/abs/2311.03393</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series discords are a useful primitive for time series anomaly
detection, and the matrix profile is capable of capturing discord effectively.
There exist many research efforts to improve the scalability of discord
discovery with respect to the length of time series. However, there is
surprisingly little work focused on reducing the time complexity of matrix
profile computation associated with dimensionality of a multidimensional time
series. In this work, we propose a sketch for discord mining among
multi-dimensional time series. After an initial pre-processing of the sketch as
fast as reading the data, the discord mining has runtime independent of the
dimensionality of the original data. On several real world examples from water
treatment and transportation, the proposed algorithm improves the throughput by
at least an order of magnitude (50X) and only has minimal impact on the quality
of the approximated solution. Additionally, the proposed method can handle the
dynamic addition or deletion of dimensions inconsequential overhead. This
allows a data analyst to consider &quot;what-if&quot; scenarios in real time while
exploring the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeh_C/0/1/0/all/0/1&quot;&gt;Chin-Chia Michael Yeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yan Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_M/0/1/0/all/0/1&quot;&gt;Menghai Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_Z/0/1/0/all/0/1&quot;&gt;Zhongfang Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Junpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_J/0/1/0/all/0/1&quot;&gt;Jeff M. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keogh_E/0/1/0/all/0/1&quot;&gt;Eamonn Keogh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.07361">
<title>The Impact of Large Language Models on Scientific Discovery: a Preliminary Study using GPT-4. (arXiv:2311.07361v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.07361</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, groundbreaking advancements in natural language processing
have culminated in the emergence of powerful large language models (LLMs),
which have showcased remarkable capabilities across a vast array of domains,
including the understanding, generation, and translation of natural language,
and even tasks that extend beyond language processing. In this report, we delve
into the performance of LLMs within the context of scientific discovery,
focusing on GPT-4, the state-of-the-art language model. Our investigation spans
a diverse range of scientific areas encompassing drug discovery, biology,
computational chemistry (density functional theory (DFT) and molecular dynamics
(MD)), materials design, and partial differential equations (PDE). Evaluating
GPT-4 on scientific tasks is crucial for uncovering its potential across
various research domains, validating its domain-specific expertise,
accelerating scientific progress, optimizing resource allocation, guiding
future model development, and fostering interdisciplinary research. Our
exploration methodology primarily consists of expert-driven case assessments,
which offer qualitative insights into the model&apos;s comprehension of intricate
scientific concepts and relationships, and occasionally benchmark testing,
which quantitatively evaluates the model&apos;s capacity to solve well-defined
domain-specific problems. Our preliminary exploration indicates that GPT-4
exhibits promising potential for a variety of scientific applications,
demonstrating its aptitude for handling complex problem-solving and knowledge
integration tasks. Broadly speaking, we evaluate GPT-4&apos;s knowledge base,
scientific understanding, scientific numerical calculation abilities, and
various scientific prediction capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AI4Science_M/0/1/0/all/0/1&quot;&gt;Microsoft Research AI4Science&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quantum_M/0/1/0/all/0/1&quot;&gt;Microsoft Azure Quantum&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13443">
<title>Guided Flows for Generative Modeling and Decision Making. (arXiv:2311.13443v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13443</link>
<description rdf:parseType="Literal">&lt;p&gt;Classifier-free guidance is a key component for enhancing the performance of
conditional generative models across diverse tasks. While it has previously
demonstrated remarkable improvements for the sample quality, it has only been
exclusively employed for diffusion models. In this paper, we integrate
classifier-free guidance into Flow Matching (FM) models, an alternative
simulation-free approach that trains Continuous Normalizing Flows (CNFs) based
on regressing vector fields. We explore the usage of \emph{Guided Flows} for a
variety of downstream applications. We show that Guided Flows significantly
improves the sample quality in conditional image generation and zero-shot
text-to-speech synthesis, boasting state-of-the-art performance. Notably, we
are the first to apply flow models for plan generation in the offline
reinforcement learning setting, showcasing a 10x speedup in computation
compared to diffusion models while maintaining comparable performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Q/0/1/0/all/0/1&quot;&gt;Qinqing Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_M/0/1/0/all/0/1&quot;&gt;Matt Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaul_N/0/1/0/all/0/1&quot;&gt;Neta Shaul&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipman_Y/0/1/0/all/0/1&quot;&gt;Yaron Lipman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grover_A/0/1/0/all/0/1&quot;&gt;Aditya Grover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ricky T. Q. Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13534">
<title>LM-Cocktail: Resilient Tuning of Language Models via Model Merging. (arXiv:2311.13534v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13534</link>
<description rdf:parseType="Literal">&lt;p&gt;The pre-trained language models are continually fine-tuned to better support
downstream applications. However, this operation may result in significant
performance degeneration on general tasks beyond the targeted domain. To
overcome this problem, we propose LM-Cocktail which enables the fine-tuned
model to stay resilient in general perspectives. Our method is conducted in the
form of model merging, where the fine-tuned language model is merged with the
pre-trained base model or the peer models from other domains through weighted
average. Despite simplicity, LM-Cocktail is surprisingly effective: the
resulted model is able to achieve a strong empirical performance in the whole
scope of general tasks while preserving a superior capacity in its targeted
domain. We conduct comprehensive experiments with LLama and BGE model on
popular benchmarks, including FLAN, MMLU, MTEB, whose results validate the
efficacy of our proposed method. The code and checkpoints are available at
https://github.com/FlagOpen/FlagEmbedding/tree/master/LM_Cocktail.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_S/0/1/0/all/0/1&quot;&gt;Shitao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_P/0/1/0/all/0/1&quot;&gt;Peitian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_X/0/1/0/all/0/1&quot;&gt;Xingrun Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16111">
<title>Neural Crystals. (arXiv:2311.16111v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16111</link>
<description rdf:parseType="Literal">&lt;p&gt;We face up to the challenge of explainability in Multimodal Artificial
Intelligence (MMAI). At the nexus of neuroscience-inspired and quantum
computing, interpretable and transparent spin-geometrical neural architectures
for early fusion of large-scale, heterogeneous, graph-structured data are
envisioned, harnessing recent evidence for relativistic quantum neural coding
of (co-)behavioral states in the self-organizing brain, under competitive,
multidimensional dynamics. The designs draw on a self-dual classical
description - via special Clifford-Lipschitz operations - of spinorial quantum
states within registers of at most 16 qubits for efficient encoding of
exponentially large neural structures. Formally &apos;trained&apos;, Lorentz neural
architectures with precisely one lateral layer of exclusively inhibitory
interneurons accounting for anti-modalities, as well as their co-architectures
with intra-layer connections are highlighted. The approach accommodates the
fusion of up to 16 time-invariant interconnected (anti-)modalities and the
crystallization of latent multidimensional patterns. Comprehensive insights are
expected to be gained through applications to Multimodal Big Data, under
diverse real-world scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Karamintziou_S/0/1/0/all/0/1&quot;&gt;Sofia Karamintziou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Mavropoulos_T/0/1/0/all/0/1&quot;&gt;Thanassis Mavropoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ntioudis_D/0/1/0/all/0/1&quot;&gt;Dimos Ntioudis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Meditskos_G/0/1/0/all/0/1&quot;&gt;Georgios Meditskos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Vrochidis_S/0/1/0/all/0/1&quot;&gt;Stefanos Vrochidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ioannis/0/1/0/all/0/1&quot;&gt;Ioannis&lt;/a&gt; (Yiannis) &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Kompatsiaris/0/1/0/all/0/1&quot;&gt;Kompatsiaris&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17098">
<title>DyRA: Dynamic Resolution Adjustment for Scale-robust Object Detection. (arXiv:2311.17098v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17098</link>
<description rdf:parseType="Literal">&lt;p&gt;In object detection, achieving constant accuracy is challenging due to the
variability of object sizes. One possible solution to this problem is to
optimize the input resolution, known as a multi-resolution strategy. Previous
approaches for optimizing resolution are often based on pre-defined resolutions
or a dynamic neural network, but there is a lack of study for run-time
resolution optimization for existing architecture. In this paper, we propose an
adaptive resolution scaling network called DyRA, which comprises convolutions
and transformer encoder blocks, for existing detectors. Our DyRA returns a
scale factor from an input image, which enables instance-specific scaling. This
network is jointly trained with detectors with specially designed loss
functions, namely ParetoScaleLoss and BalanceLoss. The ParetoScaleLoss produces
an adaptive scale factor from the image, while the BalanceLoss optimizes the
scale factor according to localization power for the dataset. The loss function
is designed to minimize accuracy drop about the contrasting objective of small
and large objects. Our experiments on COCO, RetinaNet, Faster-RCNN, FCOS, and
Mask-RCNN achieved 1.3%, 1.1%, 1.3%, and 0.8% accuracy improvement than a
multi-resolution baseline with solely resolution adjustment. The code is
available at https://github.com/DaEunFullGrace/DyRA.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_D/0/1/0/all/0/1&quot;&gt;Daeun Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hoeseok Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Hyungshin Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17961">
<title>Skilful Precipitation Nowcasting Using NowcastNet. (arXiv:2311.17961v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2311.17961</link>
<description rdf:parseType="Literal">&lt;p&gt;Designing early warning system for precipitation requires accurate short-term
forecasting system. Climate change has led to an increase in frequency of
extreme weather events, and hence such systems can prevent disasters and loss
of life. Managing such events remain a challenge for both public and private
institutions. Precipitation nowcasting can help relevant institutions to better
prepare for such events as they impact agriculture, transport, public health
and safety, etc. Physics-based numerical weather prediction (NWP) is unable to
perform well for nowcasting because of large computational turn-around time.
Deep-learning based models on the other hand are able to give predictions
within seconds. We use recently proposed NowcastNet, a physics-conditioned deep
generative network, to forecast precipitation for different regions of Europe
using satellite images. Both spatial and temporal transfer learning is done by
forecasting for the unseen regions and year. Model makes realistic predictions
and is able to outperform baseline for such a prediction task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Kumar_A/0/1/0/all/0/1&quot;&gt;Ajitabh Kumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00844">
<title>Sparse Beats Dense: Rethinking Supervision in Radar-Camera Depth Completion. (arXiv:2312.00844v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00844</link>
<description rdf:parseType="Literal">&lt;p&gt;It is widely believed that the dense supervision is better than the sparse
supervision in the field of depth completion, but the underlying reasons for
this are rarely discussed. In this paper, we find that the challenge of using
sparse supervision for training Radar-Camera depth prediction models is the
Projection Transformation Collapse (PTC). The PTC implies that sparse
supervision leads the model to learn unexpected collapsed projection
transformations between Image/Radar/LiDAR spaces. Building on this insight, we
propose a novel ``Disruption-Compensation&quot; framework to handle the PTC, thereby
relighting the use of sparse supervision in depth completion tasks. The
disruption part deliberately discards position correspondences among
Image/Radar/LiDAR, while the compensation part leverages 3D spatial and 2D
semantic information to compensate for the discarded beneficial position
correspondence. Extensive experimental results demonstrate that our framework
(sparse supervision) outperforms the state-of-the-art (dense supervision) with
11.6$\%$ improvement in mean absolute error and $1.6 \times$ speedup. The code
is available at ...
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Huadong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_M/0/1/0/all/0/1&quot;&gt;Minhao Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jiajun Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haoqiang Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Renhe Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00858">
<title>DeepCache: Accelerating Diffusion Models for Free. (arXiv:2312.00858v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00858</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have recently gained unprecedented attention in the field of
image synthesis due to their remarkable generative capabilities.
Notwithstanding their prowess, these models often incur substantial
computational costs, primarily attributed to the sequential denoising process
and cumbersome model size. Traditional methods for compressing diffusion models
typically involve extensive retraining, presenting cost and feasibility
challenges. In this paper, we introduce DeepCache, a novel training-free
paradigm that accelerates diffusion models from the perspective of model
architecture. DeepCache capitalizes on the inherent temporal redundancy
observed in the sequential denoising steps of diffusion models, which caches
and retrieves features across adjacent denoising stages, thereby curtailing
redundant computations. Utilizing the property of the U-Net, we reuse the
high-level features while updating the low-level features in a very cheap way.
This innovative strategy, in turn, enables a speedup factor of 2.3$\times$ for
Stable Diffusion v1.5 with only a 0.05 decline in CLIP Score, and 4.1$\times$
for LDM-4-G with a slight decrease of 0.22 in FID on ImageNet. Our experiments
also demonstrate DeepCache&apos;s superiority over existing pruning and distillation
methods that necessitate retraining and its compatibility with current sampling
techniques. Furthermore, we find that under the same throughput, DeepCache
effectively achieves comparable or even marginally improved results with DDIM
or PLMS. The code is available at https://github.com/horseee/DeepCache
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xinyin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_G/0/1/0/all/0/1&quot;&gt;Gongfan Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinchao Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00966">
<title>Spectral Temporal Contrastive Learning. (arXiv:2312.00966v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00966</link>
<description rdf:parseType="Literal">&lt;p&gt;Learning useful data representations without requiring labels is a
cornerstone of modern deep learning. Self-supervised learning methods,
particularly contrastive learning (CL), have proven successful by leveraging
data augmentations to define positive pairs. This success has prompted a number
of theoretical studies to better understand CL and investigate theoretical
bounds for downstream linear probing tasks. This work is concerned with the
temporal contrastive learning (TCL) setting where the sequential structure of
the data is used instead to define positive pairs, which is more commonly used
in RL and robotics contexts. In this paper, we adapt recent work on Spectral CL
to formulate Spectral Temporal Contrastive Learning (STCL). We discuss a
population loss based on a state graph derived from a time-homogeneous
reversible Markov chain with uniform stationary distribution. The STCL loss
enables to connect the linear probing performance to the spectral properties of
the graph, and can be estimated by considering previously observed data
sequences as an ensemble of MCMC chains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morin_S/0/1/0/all/0/1&quot;&gt;Sacha Morin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nath_S/0/1/0/all/0/1&quot;&gt;Somjit Nath&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kahou_S/0/1/0/all/0/1&quot;&gt;Samira Ebrahimi Kahou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_G/0/1/0/all/0/1&quot;&gt;Guy Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.02366">
<title>Towards General Purpose Vision Foundation Models for Medical Image Analysis: An Experimental Study of DINOv2 on Radiology Benchmarks. (arXiv:2312.02366v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.02366</link>
<description rdf:parseType="Literal">&lt;p&gt;The integration of deep learning systems into the medical domain has been
hindered by the resource-intensive process of data annotation and the inability
of these systems to generalize to different data distributions. Foundation
models, which are models pre-trained on large datasets, have emerged as a
solution to reduce reliance on annotated data and enhance model
generalizability and robustness. DINOv2, an open-source foundation model
pre-trained with self-supervised learning on 142 million curated natural
images, excels in extracting general-purpose visual representations, exhibiting
promising capabilities across various vision tasks. Nevertheless, a critical
question remains unanswered regarding DINOv2&apos;s adaptability to radiological
imaging, and the clarity on whether its features are sufficiently general to
benefit radiology image analysis is yet to be established. Therefore, this
study comprehensively evaluates DINOv2 for radiology, conducting over 100
experiments across diverse modalities (X-ray, CT, and MRI). Tasks include
disease classification and organ segmentation on both 2D and 3D images,
evaluated under different settings like kNN, few-shot learning, linear-probing,
end-to-end fine-tuning, and parameter-efficient fine-tuning, to measure the
effectiveness and generalizability of the DINOv2 feature embeddings.
Comparative analyses with established medical image analysis models, U-Net and
TransUnet for segmentation, and CNN and ViT models pre-trained via supervised,
weakly supervised, and self-supervised learning for classification, reveal
DINOv2&apos;s superior performance in segmentation tasks and competitive results in
disease classification. The findings contribute insights to potential avenues
for optimizing pre-training strategies for medical imaging and enhancing the
broader understanding of DINOv2&apos;s role in bridging the gap between natural and
radiological image analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baharoon_M/0/1/0/all/0/1&quot;&gt;Mohammed Baharoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qureshi_W/0/1/0/all/0/1&quot;&gt;Waseem Qureshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_J/0/1/0/all/0/1&quot;&gt;Jiahong Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yanwu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aljouie_A/0/1/0/all/0/1&quot;&gt;Abdulrhman Aljouie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_W/0/1/0/all/0/1&quot;&gt;Wei Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03126">
<title>Learning Curricula in Open-Ended Worlds. (arXiv:2312.03126v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03126</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep reinforcement learning (RL) provides powerful methods for training
optimal sequential decision-making agents. As collecting real-world
interactions can entail additional costs and safety risks, the common paradigm
of sim2real conducts training in a simulator, followed by real-world
deployment. Unfortunately, RL agents easily overfit to the choice of simulated
training environments, and worse still, learning ends when the agent masters
the specific set of simulated environments. In contrast, the real world is
highly open-ended, featuring endlessly evolving environments and challenges,
making such RL approaches unsuitable. Simply randomizing over simulated
environments is insufficient, as it requires making arbitrary distributional
assumptions and can be combinatorially less likely to sample specific
environment instances that are useful for learning. An ideal learning process
should automatically adapt the training environment to maximize the learning
potential of the agent over an open-ended task space that matches or surpasses
the complexity of the real world. This thesis develops a class of methods
called Unsupervised Environment Design (UED), which aim to produce such
open-ended processes. Given an environment design space, UED automatically
generates an infinite sequence or curriculum of training environments at the
frontier of the learning agent&apos;s capabilities. Through extensive empirical
studies and theoretical arguments founded on minimax-regret decision theory and
game theory, the findings in this thesis show that UED autocurricula can
produce RL agents exhibiting significantly improved robustness and
generalization to previously unseen environment instances. Such autocurricula
are promising paths toward open-ended learning systems that achieve more
general intelligence by continually generating and mastering additional
challenges of their own design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Minqi Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03721">
<title>Exploring the Robustness of Model-Graded Evaluations and Automated Interpretability. (arXiv:2312.03721v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03721</link>
<description rdf:parseType="Literal">&lt;p&gt;There has been increasing interest in evaluations of language models for a
variety of risks and characteristics. Evaluations relying on natural language
understanding for grading can often be performed at scale by using other
language models. We test the robustness of these model-graded evaluations to
injections on different datasets including a new Deception Eval. These
injections resemble direct communication between the testee and the evaluator
to change their grading. We extrapolate that future, more intelligent models
might manipulate or cooperate with their evaluation model. We find significant
susceptibility to these injections in state-of-the-art commercial models on all
examined evaluations. Furthermore, similar injections can be used on automated
interpretability frameworks to produce misleading model-written explanations.
The results inspire future work and should caution against unqualified trust in
evaluations and automated interpretability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lermen_S/0/1/0/all/0/1&quot;&gt;Simon Lermen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kvapil_O/0/1/0/all/0/1&quot;&gt;Ond&amp;#x159;ej Kvapil&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.03733">
<title>Methods to Estimate Large Language Model Confidence. (arXiv:2312.03733v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.03733</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models have difficulty communicating uncertainty, which is a
significant obstacle to applying LLMs to complex medical tasks. This study
evaluates methods to measure LLM confidence when suggesting a diagnosis for
challenging clinical vignettes. GPT4 was asked a series of challenging case
questions using Chain of Thought and Self Consistency prompting. Multiple
methods were investigated to assess model confidence and evaluated on their
ability to predict the models observed accuracy. The methods evaluated were
Intrinsic Confidence, SC Agreement Frequency and CoT Response Length. SC
Agreement Frequency correlated with observed accuracy, yielding a higher Area
under the Receiver Operating Characteristic Curve compared to Intrinsic
Confidence and CoT Length analysis. SC agreement is the most useful proxy for
model confidence, especially for medical diagnosis. Model Intrinsic Confidence
and CoT Response Length exhibit a weaker ability to differentiate between
correct and incorrect answers, preventing them from being reliable and
interpretable markers for model confidence. We conclude GPT4 has a limited
ability to assess its own diagnostic accuracy. SC Agreement Frequency is the
most useful method to measure GPT4 confidence.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotelanski_M/0/1/0/all/0/1&quot;&gt;Maia Kotelanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gallo_R/0/1/0/all/0/1&quot;&gt;Robert Gallo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nayak_A/0/1/0/all/0/1&quot;&gt;Ashwin Nayak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savage_T/0/1/0/all/0/1&quot;&gt;Thomas Savage&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04474">
<title>Chain of Code: Reasoning with a Language Model-Augmented Code Emulator. (arXiv:2312.04474v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04474</link>
<description rdf:parseType="Literal">&lt;p&gt;Code provides a general syntactic structure to build complex programs and
perform precise computations when paired with a code interpreter - we
hypothesize that language models (LMs) can leverage code-writing to improve
Chain of Thought reasoning not only for logic and arithmetic tasks, but also
for semantic ones (and in particular, those that are a mix of both). For
example, consider prompting an LM to write code that counts the number of times
it detects sarcasm in an essay: the LM may struggle to write an implementation
for &quot;detect_sarcasm(string)&quot; that can be executed by the interpreter (handling
the edge cases would be insurmountable). However, LMs may still produce a valid
solution if they not only write code, but also selectively &quot;emulate&quot; the
interpreter by generating the expected output of &quot;detect_sarcasm(string)&quot; and
other lines of code that cannot be executed. In this work, we propose Chain of
Code (CoC), a simple yet surprisingly effective extension that improves LM
code-driven reasoning. The key idea is to encourage LMs to format semantic
sub-tasks in a program as flexible pseudocode that the interpreter can
explicitly catch undefined behaviors and hand off to simulate with an LM (as an
&quot;LMulator&quot;). Experiments demonstrate that Chain of Code outperforms Chain of
Thought and other baselines across a variety of benchmarks; on BIG-Bench Hard,
Chain of Code achieves 84%, a gain of 12% over Chain of Thought. CoC scales
well with large and small models alike, and broadens the scope of reasoning
questions that LMs can correctly answer by &quot;thinking in code&quot;. Project webpage:
https://chain-of-code.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengshu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_J/0/1/0/all/0/1&quot;&gt;Jacky Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_A/0/1/0/all/0/1&quot;&gt;Andy Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hausman_K/0/1/0/all/0/1&quot;&gt;Karol Hausman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadigh_D/0/1/0/all/0/1&quot;&gt;Dorsa Sadigh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_Fei_L/0/1/0/all/0/1&quot;&gt;Li Fei-Fei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_F/0/1/0/all/0/1&quot;&gt;Fei Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ichter_B/0/1/0/all/0/1&quot;&gt;Brian Ichter&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>