<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-21T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10252" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10256" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10262" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10272" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10341" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10379" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10402" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10474" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10512" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10525" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10529" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10530" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10537" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10541" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10556" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10561" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10564" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10620" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10637" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10640" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10643" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10709" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10711" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10712" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10752" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10777" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10786" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10790" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10805" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10848" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10877" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10889" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/1812.01243" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.10702" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2106.01061" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.09773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16031" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.06551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.09424" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.11795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.08044" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.10766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13359" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.02506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.05015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06088" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.14501" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.00746" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13310" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.08251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02119" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.09466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.14197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.20685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15420" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15497" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.00067" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06955" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.06999" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.07823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.08010" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.12653" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.16516" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01984" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.05594" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.07567" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09495" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.09895" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10090" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.10191" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.10247">
<title>Resolution Chromatography of Diffusion Models. (arXiv:2401.10247v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10247</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models generate high-resolution images through iterative stochastic
processes. In particular, the denoising method is one of the most popular
approaches that predicts the noise in samples and denoises it at each time
step. It has been commonly observed that the resolution of generated samples
changes over time, starting off blurry and coarse, and becoming sharper and
finer. In this paper, we introduce &quot;resolution chromatography&quot; that indicates
the signal generation rate of each resolution, which is very helpful concept to
mathematically explain this coarse-to-fine behavior in generation process, to
understand the role of noise schedule, and to design time-dependent modulation.
Using resolution chromatography, we determine which resolution level becomes
dominant at a specific time step, and experimentally verify our theory with
text-to-image diffusion models. We also propose some direct applications
utilizing the concept: upscaling pre-trained models to higher resolutions and
time-dependent prompt composing. Our theory not only enables a better
understanding of numerous pre-existing techniques for manipulating image
generation, but also suggests the potential for designing better noise
schedules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Juno Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;Yong-Hyun Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_J/0/1/0/all/0/1&quot;&gt;Junghyo Jo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10252">
<title>A Beam-Segmenting Polar Format Algorithm Based on Double PCS for Video SAR Persistent Imaging. (arXiv:2401.10252v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10252</link>
<description rdf:parseType="Literal">&lt;p&gt;Video synthetic aperture radar (SAR) is attracting more attention in recent
years due to its abilities of high resolution, high frame rate and advantages
in continuous observation. Generally, the polar format algorithm (PFA) is an
efficient algorithm for spotlight mode video SAR. However, in the process of
PFA, the wavefront curvature error (WCE) limits the imaging scene size and the
2-D interpolation affects the efficiency. To solve the aforementioned problems,
a beam-segmenting PFA based on principle of chirp scaling (PCS), called
BS-PCS-PFA, is proposed for video SAR imaging, which has the capability of
persistent imaging for different carrier frequencies video SAR. Firstly, an
improved PCS applicable to video SAR PFA is proposed to replace the 2-D
interpolation and the coarse image in the ground output coordinate system
(GOCS) is obtained. As for the distortion or defocus existing in the coarse
image, a novel sub-block imaging method based on beam-segmenting fast filtering
is proposed to segment the image into multiple sub-beam data, whose distortion
and defocus can be ignored when the equivalent size of sub-block is smaller
than the distortion negligible region. Through processing the sub-beam data and
mosaicking the refocused subimages, the full image in GOCS without distortion
and defocus is obtained. Moreover, a three-step MoCo method is applied to the
algorithm for the adaptability to the actual irregular trajectories. The
proposed method can significantly expand the effective scene size of PFA, and
the better operational efficiency makes it more suitable for video SAR imaging.
The feasibility of the algorithm is verified by the experimental data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Jiawei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yinwei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_S/0/1/0/all/0/1&quot;&gt;Shaowen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yiming Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10254">
<title>Beyond the Frame: Single and mutilple video summarization method with user-defined length. (arXiv:2401.10254v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10254</link>
<description rdf:parseType="Literal">&lt;p&gt;Video smmarization is a crucial method to reduce the time of videos which
reduces the spent time to watch/review a long video. This apporach has became
more important as the amount of publisehed video is increasing everyday. A
single or multiple videos can be summarized into a relatively short video using
various of techniques from multimodal audio-visual techniques, to natural
language processing approaches. Audiovisual techniques may be used to recognize
significant visual events and pick the most important parts, while NLP
techniques can be used to evaluate the audio transcript and extract the main
sentences (timestamps) and corresponding video frames from the original video.
Another approach is to use the best of both domain. Meaning that we can use
audio-visual cues as well as video transcript to extract and summarize the
video. In this paper, we combine a variety of NLP techniques (extractive and
contect-based summarizers) with video processing techniques to convert a long
video into a single relatively short video. We design this toll in a way that
user can specify the relative length of the summarized video. We have also
explored ways of summarizing and concatenating multiple videos into a single
short video which will help having most important concepts from the same
subject in a single short video. Out approach shows that video summarizing is a
difficult but significant work, with substantial potential for further research
and development, and it is possible thanks to the development of NLP models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalkhorani_V/0/1/0/all/0/1&quot;&gt;Vahid Ahmadi Kalkhorani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingquan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_G/0/1/0/all/0/1&quot;&gt;Guanqun Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Ting Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10256">
<title>Active headrest combined with a depth camera-based ear-positioning system. (arXiv:2401.10256v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10256</link>
<description rdf:parseType="Literal">&lt;p&gt;Active headrests can reduce low-frequency noise around ears based on active
noise control (ANC) system. Both the control system using fixed control filters
and the remote microphone-based adaptive control system provide good noise
reduction performance when the head is in the original position. However, their
performance degrades significantly when the head is in motion. In this paper, a
human ear-positioning system based on the depth camera is introduced to address
this problem. The system uses RTMpose model to estimate the two-dimensional
(2D) positions of the ears in the color frame, and then derives the
corresponding three-dimensional (3D) coordinates in the depth frame with a
depth camera. Experimental results show that the ear-positioning system can
effectively track the movement of ears, and the broadband noise reduction
performance of the active headrest combined with the system is significantly
improved when the human head is translating or rotating.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuteng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haowen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_H/0/1/0/all/0/1&quot;&gt;Haishan Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jing Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhibin Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10262">
<title>Null Space Properties of Neural Networks with Applications to Image Steganography. (arXiv:2401.10262v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10262</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper explores the null space properties of neural networks. We extend
the null space definition from linear to nonlinear maps and discuss the
presence of a null space in neural networks. The null space of a given neural
network can tell us the part of the input data that makes no contribution to
the final prediction so that we can use it to trick the neural network. This
reveals an inherent weakness in neural networks that can be exploited. One
application described here leads to a method of image steganography. Through
experiments on image datasets such as MNIST, we show that we can use null space
components to force the neural network to choose a selected hidden image class,
even though the overall image can be made to look like a completely different
image. We conclude by showing comparisons between what a human viewer would
see, and the part of the image that the neural network is actually using to
make predictions and, hence, show that what the neural network ``sees&apos;&apos; is
completely different than what we would expect.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Short_K/0/1/0/all/0/1&quot;&gt;Kevin M. Short&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10272">
<title>Multi-Source Collaborative Gradient Discrepancy Minimization for Federated Domain Generalization. (arXiv:2401.10272v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10272</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Domain Generalization aims to learn a domain-invariant model from
multiple decentralized source domains for deployment on unseen target domain.
Due to privacy concerns, the data from different source domains are kept
isolated, which poses challenges in bridging the domain gap. To address this
issue, we propose a Multi-source Collaborative Gradient Discrepancy
Minimization (MCGDM) method for federated domain generalization. Specifically,
we propose intra-domain gradient matching between the original images and
augmented images to avoid overfitting the domain-specific information within
isolated domains. Additionally, we propose inter-domain gradient matching with
the collaboration of other domains, which can further reduce the domain shift
across decentralized domains. Combining intra-domain and inter-domain gradient
matching, our method enables the learned model to generalize well on unseen
domains. Furthermore, our method can be extended to the federated domain
adaptation task by fine-tuning the target model on the pseudo-labeled target
domain. The extensive experiments on federated domain generalization and
adaptation indicate that our method outperforms the state-of-the-art methods
significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yikang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yahong Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10299">
<title>An attempt to generate new bridge types from latent space of generative flow. (arXiv:2401.10299v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10299</link>
<description rdf:parseType="Literal">&lt;p&gt;Through examples of coordinate and probability transformation between
different distributions, the basic principle of normalizing flow is introduced
in a simple and concise manner. From the perspective of the distribution of
random variable function, the essence of probability transformation is
explained, and the scaling factor Jacobian determinant of probability
transformation is introduced. Treating the dataset as a sample from the
population, obtaining normalizing flow is essentially through sampling surveys
to statistically infer the numerical features of the population, and then the
loss function is established by using the maximum likelihood estimation method.
This article introduces how normalizing flow cleverly solves the two major
application challenges of high-dimensional matrix determinant calculation and
neural network reversible transformation. Using symmetric structured image
dataset of three-span beam bridge, arch bridge, cable-stayed bridge and
suspension bridge, constructing and training normalizing flow based on the Glow
API in the TensorFlow Probability library. The model can smoothly transform the
complex distribution of the bridge dataset into a standard normal distribution,
and from the obtained latent space sampling, it can generate new bridge types
that are different from the training dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hongjun Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10341">
<title>ELRT: Efficient Low-Rank Training for Compact Convolutional Neural Networks. (arXiv:2401.10341v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10341</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-rank compression, a popular model compression technique that produces
compact convolutional neural networks (CNNs) with low rankness, has been
well-studied in the literature. On the other hand, low-rank training, as an
alternative way to train low-rank CNNs from scratch, has been exploited little
yet. Unlike low-rank compression, low-rank training does not need pre-trained
full-rank models, and the entire training phase is always performed on the
low-rank structure, bringing attractive benefits for practical applications.
However, the existing low-rank training solutions still face several
challenges, such as a considerable accuracy drop and/or still needing to update
full-size models during the training. In this paper, we perform a systematic
investigation on low-rank CNN training. By identifying the proper low-rank
format and performance-improving strategy, we propose ELRT, an efficient
low-rank training solution for high-accuracy, high-compactness, low-rank CNN
models. Our extensive evaluation results for training various CNNs on different
datasets demonstrate the effectiveness of ELRT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sui_Y/0/1/0/all/0/1&quot;&gt;Yang Sui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_M/0/1/0/all/0/1&quot;&gt;Miao Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yu Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jinqi Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_H/0/1/0/all/0/1&quot;&gt;Huy Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_B/0/1/0/all/0/1&quot;&gt;Bo Yuan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10373">
<title>Harmonized Spatial and Spectral Learning for Robust and Generalized Medical Image Segmentation. (arXiv:2401.10373v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10373</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning has demonstrated remarkable achievements in medical image
segmentation. However, prevailing deep learning models struggle with poor
generalization due to (i) intra-class variations, where the same class appears
differently in different samples, and (ii) inter-class independence, resulting
in difficulties capturing intricate relationships between distinct objects,
leading to higher false negative cases. This paper presents a novel approach
that synergies spatial and spectral representations to enhance
domain-generalized medical image segmentation. We introduce the innovative
Spectral Correlation Coefficient objective to improve the model&apos;s capacity to
capture middle-order features and contextual long-range dependencies. This
objective complements traditional spatial objectives by incorporating valuable
spectral information. Extensive experiments reveal that optimizing this
objective with existing architectures like UNet and TransUNet significantly
enhances generalization, interpretability, and noise robustness, producing more
confident predictions. For instance, in cardiac segmentation, we observe a 0.81
pp and 1.63 pp (pp = percentage point) improvement in DSC over UNet and
TransUNet, respectively. Our interpretability study demonstrates that, in most
tasks, objectives optimized with UNet outperform even TransUNet by introducing
global contextual information alongside local details. These findings
underscore the versatility and effectiveness of our proposed method across
diverse imaging modalities and medical domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gorade_V/0/1/0/all/0/1&quot;&gt;Vandan Gorade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mittal_S/0/1/0/all/0/1&quot;&gt;Sparsh Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jha_D/0/1/0/all/0/1&quot;&gt;Debesh Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Singhal_R/0/1/0/all/0/1&quot;&gt;Rekha Singhal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagci_U/0/1/0/all/0/1&quot;&gt;Ulas Bagci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10379">
<title>Agricultural Object Detection with You Look Only Once (YOLO) Algorithm: A Bibliometric and Systematic Literature Review. (arXiv:2401.10379v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10379</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision is a major component in several digital technologies and tools used in
agriculture. The object detector, You Look Only Once (YOLO), has gained
popularity in agriculture in a relatively short span due to its
state-of-the-art performance. YOLO offers real-time detection with good
accuracy and is implemented in various agricultural tasks, including
monitoring, surveillance, sensing, automation, and robotics. The research and
application of YOLO in agriculture are accelerating rapidly but are fragmented
and multidisciplinary. Moreover, the performance characteristics (i.e.,
accuracy, speed, computation) of the object detector influence the rate of
technology implementation and adoption in agriculture. Thus, the study aims to
collect extensive literature to document and critically evaluate the advances
and application of YOLO for agricultural object recognition. First, we
conducted a bibliometric review of 257 articles to understand the scholarly
landscape of YOLO in agricultural domain. Secondly, we conducted a systematic
review of 30 articles to identify current knowledge, gaps, and modifications in
YOLO for specific agricultural tasks. The study critically assesses and
summarizes the information on YOLO&apos;s end-to-end learning approach, including
data acquisition, processing, network modification, integration, and
deployment. We also discussed task-specific YOLO algorithm modification and
integration to meet the agricultural object or environment-specific challenges.
In general, YOLO-integrated digital tools and technologies show the potential
for real-time, automated monitoring, surveillance, and object handling to
reduce labor, production cost, and environmental impact while maximizing
resource efficiency. The study provides detailed documentation and
significantly advances the existing knowledge on applying YOLO in agriculture,
which can greatly benefit the scientific community.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badgujar_C/0/1/0/all/0/1&quot;&gt;Chetan M Badgujar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poulose_A/0/1/0/all/0/1&quot;&gt;Alwin Poulose&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_H/0/1/0/all/0/1&quot;&gt;Hao Gan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10397">
<title>Analyzing and Mitigating Bias for Vulnerable Classes: Towards Balanced Representation in Dataset. (arXiv:2401.10397v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10397</link>
<description rdf:parseType="Literal">&lt;p&gt;The accuracy and fairness of perception systems in autonomous driving are
crucial, particularly for vulnerable road users. Mainstream research has looked
into improving the performance metrics for classification accuracy. However,
the hidden traits of bias inheritance in the AI models, class imbalances and
disparities in the datasets are often overlooked. In this context, our study
examines the class imbalances for vulnerable road users by focusing on class
distribution analysis, performance evaluation, and bias impact assessment. We
identify the concern of imbalances in class representation, leading to
potential biases in detection accuracy. Utilizing popular CNN models and Vision
Transformers (ViTs) with the nuScenes dataset, our performance evaluation
reveals detection disparities for underrepresented classes. We propose a
methodology for model optimization and bias mitigation, which includes data
augmentation, resampling, and metric-specific learning. Using the proposed
mitigation approaches, we see improvement in IoU(%) and NDS(%) metrics from
71.3 to 75.6 and 80.6 to 83.7 respectively, for the CNN model. Similarly, for
ViT, we observe improvement in IoU and NDS metrics from 74.9 to 79.2 and 83.8
to 87.1 respectively. This research contributes to developing more reliable
models and datasets, enhancing inclusiveness for minority classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Katare_D/0/1/0/all/0/1&quot;&gt;Dewant Katare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noguero_D/0/1/0/all/0/1&quot;&gt;David Solans Noguero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Souneil Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kourtellis_N/0/1/0/all/0/1&quot;&gt;Nicolas Kourtellis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Janssen_M/0/1/0/all/0/1&quot;&gt;Marijn Janssen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_A/0/1/0/all/0/1&quot;&gt;Aaron Yi Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10402">
<title>Reconstructing the Invisible: Video Frame Restoration through Siamese Masked Conditional Variational Autoencoder. (arXiv:2401.10402v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10402</link>
<description rdf:parseType="Literal">&lt;p&gt;In the domain of computer vision, the restoration of missing information in
video frames is a critical challenge, particularly in applications such as
autonomous driving and surveillance systems. This paper introduces the Siamese
Masked Conditional Variational Autoencoder (SiamMCVAE), leveraging a siamese
architecture with twin encoders based on vision transformers. This innovative
design enhances the model&apos;s ability to comprehend lost content by capturing
intrinsic similarities between paired frames. SiamMCVAE proficiently
reconstructs missing elements in masked frames, effectively addressing issues
arising from camera malfunctions through variational inferences. Experimental
results robustly demonstrate the model&apos;s effectiveness in restoring missing
information, thus enhancing the resilience of computer vision systems. The
incorporation of Siamese Vision Transformer (SiamViT) encoders in SiamMCVAE
exemplifies promising potential for addressing real-world challenges in
computer vision, reinforcing the adaptability of autonomous systems in dynamic
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yongchen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Richard Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10404">
<title>Inflation with Diffusion: Efficient Temporal Adaptation for Text-to-Video Super-Resolution. (arXiv:2401.10404v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10404</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose an efficient diffusion-based text-to-video super-resolution (SR)
tuning approach that leverages the readily learned capacity of pixel level
image diffusion model to capture spatial information for video generation. To
accomplish this goal, we design an efficient architecture by inflating the
weightings of the text-to-image SR model into our video generation framework.
Additionally, we incorporate a temporal adapter to ensure temporal coherence
across video frames. We investigate different tuning approaches based on our
inflated architecture and report trade-offs between computational costs and
super-resolution quality. Empirical evaluation, both quantitative and
qualitative, on the Shutterstock video dataset, demonstrates that our approach
is able to perform text-to-video SR generation with good visual quality and
temporal consistency. To evaluate temporal coherence, we also present
visualizations in video format in
https://drive.google.com/drive/folders/1YVc-KMSJqOrEUdQWVaI-Yfu8Vsfu_1aO?usp=sharing .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_X/0/1/0/all/0/1&quot;&gt;Xin Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_J/0/1/0/all/0/1&quot;&gt;Jinoo Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Keyang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tov_O/0/1/0/all/0/1&quot;&gt;Omer Tov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fei_H/0/1/0/all/0/1&quot;&gt;Hongliang Fei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10416">
<title>DataViz3D: An Novel Method Leveraging Online Holographic Modeling for Extensive Dataset Preprocessing and Visualization. (arXiv:2401.10416v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10416</link>
<description rdf:parseType="Literal">&lt;p&gt;DataViz3D is an innovative online software that transforms complex datasets
into interactive 3D spatial models using holographic technology. This tool
enables users to generate scatter plot within a 3D space, accurately mapped to
the XYZ coordinates of the dataset, providing a vivid and intuitive
understanding of the spatial relationships inherent in the data. DataViz3D&apos;s
user friendly interface makes advanced 3D modeling and holographic
visualization accessible to a wide range of users, fostering new opportunities
for collaborative research and education across various disciplines.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_J/0/1/0/all/0/1&quot;&gt;Jinli Duan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10419">
<title>M3BUNet: Mobile Mean Max UNet for Pancreas Segmentation on CT-Scans. (arXiv:2401.10419v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10419</link>
<description rdf:parseType="Literal">&lt;p&gt;Segmenting organs in CT scan images is a necessary process for multiple
downstream medical image analysis tasks. Currently, manual CT scan segmentation
by radiologists is prevalent, especially for organs like the pancreas, which
requires a high level of domain expertise for reliable segmentation due to
factors like small organ size, occlusion, and varying shapes. When resorting to
automated pancreas segmentation, these factors translate to limited reliable
labeled data to train effective segmentation models. Consequently, the
performance of contemporary pancreas segmentation models is still not within
acceptable ranges. To improve that, we propose M3BUNet, a fusion of MobileNet
and U-Net neural networks, equipped with a novel Mean-Max (MM) attention that
operates in two stages to gradually segment pancreas CT images from coarse to
fine with mask guidance for object detection. This approach empowers the
network to surpass segmentation performance achieved by similar network
architectures and achieve results that are on par with complex state-of-the-art
methods, all while maintaining a low parameter count. Additionally, we
introduce external contour segmentation as a preprocessing step for the coarse
stage to assist in the segmentation process through image standardization. For
the fine segmentation stage, we found that applying a wavelet decomposition
filter to create multi-input images enhances pancreas segmentation performance.
We extensively evaluate our approach on the widely known NIH pancreas dataset
and MSD pancreas dataset. Our approach demonstrates a considerable performance
improvement, achieving an average Dice Similarity Coefficient (DSC) value of up
to 89.53% and an Intersection Over Union (IOU) score of up to 81.16 for the NIH
pancreas dataset, and 88.60% DSC and 79.90% IOU for the MSD Pancreas dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+juwita_J/0/1/0/all/0/1&quot;&gt;Juwita juwita&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hassan_G/0/1/0/all/0/1&quot;&gt;Ghulam Mubashar Hassan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akhtar_N/0/1/0/all/0/1&quot;&gt;Naveed Akhtar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Datta_A/0/1/0/all/0/1&quot;&gt;Amitava Datta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10442">
<title>Path Choice Matters for Clear Attribution in Path Methods. (arXiv:2401.10442v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10442</link>
<description rdf:parseType="Literal">&lt;p&gt;Rigorousness and clarity are both essential for interpretations of DNNs to
engender human trust. Path methods are commonly employed to generate rigorous
attributions that satisfy three axioms. However, the meaning of attributions
remains ambiguous due to distinct path choices. To address the ambiguity, we
introduce \textbf{Concentration Principle}, which centrally allocates high
attributions to indispensable features, thereby endowing aesthetic and
sparsity. We then present \textbf{SAMP}, a model-agnostic interpreter, which
efficiently searches the near-optimal path from a pre-defined set of
manipulation paths. Moreover, we propose the infinitesimal constraint (IC) and
momentum strategy (MS) to improve the rigorousness and optimality.
Visualizations show that SAMP can precisely reveal DNNs by pinpointing salient
image pixels. We also perform quantitative experiments and observe that our
method significantly outperforms the counterparts. Code:
https://github.com/zbr17/SAMP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10461">
<title>Learning to Robustly Reconstruct Low-light Dynamic Scenes from Spike Streams. (arXiv:2401.10461v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10461</link>
<description rdf:parseType="Literal">&lt;p&gt;As a neuromorphic sensor with high temporal resolution, spike camera can
generate continuous binary spike streams to capture per-pixel light intensity.
We can use reconstruction methods to restore scene details in high-speed
scenarios. However, due to limited information in spike streams, low-light
scenes are difficult to effectively reconstruct. In this paper, we propose a
bidirectional recurrent-based reconstruction framework, including a
Light-Robust Representation (LR-Rep) and a fusion module, to better handle such
extreme conditions. LR-Rep is designed to aggregate temporal information in
spike streams, and a fusion module is utilized to extract temporal features.
Additionally, we have developed a reconstruction benchmark for high-speed
low-light scenes. Light sources in the scenes are carefully aligned to
real-world conditions. Experimental results demonstrate the superiority of our
method, which also generalizes well to real spike streams. Related codes and
proposed datasets will be released after publication.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Liwen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_Z/0/1/0/all/0/1&quot;&gt;Ziluo Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Mianzhi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Tiejun Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10474">
<title>LDReg: Local Dimensionality Regularized Self-Supervised Learning. (arXiv:2401.10474v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10474</link>
<description rdf:parseType="Literal">&lt;p&gt;Representations learned via self-supervised learning (SSL) can be susceptible
to dimensional collapse, where the learned representation subspace is of
extremely low dimensionality and thus fails to represent the full data
distribution and modalities. Dimensional collapse also known as the
&quot;underfilling&quot; phenomenon is one of the major causes of degraded performance on
downstream tasks. Previous work has investigated the dimensional collapse
problem of SSL at a global level. In this paper, we demonstrate that
representations can span over high dimensional space globally, but collapse
locally. To address this, we propose a method called $\textit{local
dimensionality regularization (LDReg)}$. Our formulation is based on the
derivation of the Fisher-Rao metric to compare and optimize local distance
distributions at an asymptotically small radius for each data point. By
increasing the local intrinsic dimensionality, we demonstrate through a range
of experiments that LDReg improves the representation quality of SSL. The
results also show that LDReg can regularize dimensionality at both local and
global levels.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hanxun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campello_R/0/1/0/all/0/1&quot;&gt;Ricardo J. G. B. Campello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erfani_S/0/1/0/all/0/1&quot;&gt;Sarah Monazam Erfani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xingjun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houle_M/0/1/0/all/0/1&quot;&gt;Michael E. Houle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bailey_J/0/1/0/all/0/1&quot;&gt;James Bailey&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10475">
<title>CBVS: A Large-Scale Chinese Image-Text Benchmark for Real-World Short Video Search Scenarios. (arXiv:2401.10475v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10475</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision-Language Models pre-trained on large-scale image-text datasets have
shown superior performance in downstream tasks such as image retrieval. Most of
the images for pre-training are presented in the form of open domain
common-sense visual elements. Differently, video covers in short video search
scenarios are presented as user-originated contents that provide important
visual summaries of videos. In addition, a portion of the video covers come
with manually designed cover texts that provide semantic complements. In order
to fill in the gaps in short video cover data, we establish the first
large-scale cover-text benchmark for Chinese short video search scenarios.
Specifically, we release two large-scale datasets CBVS-5M/10M to provide short
video covers, and the manual fine-labeling dataset CBVS-20K to provide real
user queries, which serves as an image-text benchmark test in the Chinese short
video search field. To integrate the semantics of cover text in the case of
modality missing, we propose UniCLIP where cover texts play a guiding role
during training, however are not relied upon by inference. Extensive evaluation
on CBVS-20K demonstrates the excellent performance of our proposal. UniCLIP has
been deployed to Tencent&apos;s online video search systems with hundreds of
millions of visits and achieved significant gains. The complete dataset, code
and checkpoints will be available upon release.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_X/0/1/0/all/0/1&quot;&gt;Xiangshuo Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xiaozhe Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yu Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_C/0/1/0/all/0/1&quot;&gt;Cihang Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jin Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10501">
<title>Enhancing medical vision-language contrastive learning via inter-matching relation modelling. (arXiv:2401.10501v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10501</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical image representations can be learned through medical vision-language
contrastive learning (mVLCL) where medical imaging reports are used as weak
supervision through image-text alignment. These learned image representations
can be transferred to and benefit various downstream medical vision tasks such
as disease classification and segmentation. Recent mVLCL methods attempt to
align image sub-regions and the report keywords as local-matchings. However,
these methods aggregate all local-matchings via simple pooling operations while
ignoring the inherent relations between them. These methods therefore fail to
reason between local-matchings that are semantically related, e.g.,
local-matchings that correspond to the disease word and the location word
(semantic-relations), and also fail to differentiate such clinically important
local-matchings from others that correspond to less meaningful words, e.g.,
conjunction words (importance-relations). Hence, we propose a mVLCL method that
models the inter-matching relations between local-matchings via a
relation-enhanced contrastive learning framework (RECLF). In RECLF, we
introduce a semantic-relation reasoning module (SRM) and an importance-relation
reasoning module (IRM) to enable more fine-grained report supervision for image
representation learning. We evaluated our method using four public benchmark
datasets on four downstream tasks, including segmentation, zero-shot
classification, supervised classification, and cross-modal retrieval. Our
results demonstrated the superiority of our RECLF over the state-of-the-art
mVLCL methods with consistent improvements across single-modal and cross-modal
tasks. These results suggest that our RECLF, by modelling the inter-matching
relations, can learn improved medical image representations with better
generalization capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mingjian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Mingyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fulham_M/0/1/0/all/0/1&quot;&gt;Michael Fulham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;David Dagan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bi_L/0/1/0/all/0/1&quot;&gt;Lei Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinman Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10511">
<title>GMC-IQA: Exploiting Global-correlation and Mean-opinion Consistency for No-reference Image Quality Assessment. (arXiv:2401.10511v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10511</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the subjective nature of image quality assessment (IQA), assessing
which image has better quality among a sequence of images is more reliable than
assigning an absolute mean opinion score for an image. Thus, IQA models are
evaluated by global correlation consistency (GCC) metrics like PLCC and SROCC,
rather than mean opinion consistency (MOC) metrics like MAE and MSE. However,
most existing methods adopt MOC metrics to define their loss functions, due to
the infeasible computation of GCC metrics during training. In this work, we
construct a novel loss function and network to exploit Global-correlation and
Mean-opinion Consistency, forming a GMC-IQA framework. Specifically, we propose
a novel GCC loss by defining a pairwise preference-based rank estimation to
solve the non-differentiable problem of SROCC and introducing a queue mechanism
to reserve previous data to approximate the global results of the whole data.
Moreover, we propose a mean-opinion network, which integrates diverse opinion
features to alleviate the randomness of weight learning and enhance the model
robustness. Experiments indicate that our method outperforms SOTA methods on
multiple authentic datasets with higher accuracy and generalization. We also
adapt the proposed loss to various networks, which brings better performance
and more stable training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zewen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Juan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_C/0/1/0/all/0/1&quot;&gt;Chunfeng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Weiming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Junxian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youqun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Congxuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10512">
<title>Exploring Color Invariance through Image-Level Ensemble Learning. (arXiv:2401.10512v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10512</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of computer vision, the persistent presence of color bias,
resulting from fluctuations in real-world lighting and camera conditions,
presents a substantial challenge to the robustness of models. This issue is
particularly pronounced in complex wide-area surveillance scenarios, such as
person re-identification and industrial dust segmentation, where models often
experience a decline in performance due to overfitting on color information
during training, given the presence of environmental variations. Consequently,
there is a need to effectively adapt models to cope with the complexities of
camera conditions. To address this challenge, this study introduces a learning
strategy named Random Color Erasing, which draws inspiration from ensemble
learning. This strategy selectively erases partial or complete color
information in the training data without disrupting the original image
structure, thereby achieving a balanced weighting of color features and other
features within the neural network. This approach mitigates the risk of
overfitting and enhances the model&apos;s ability to handle color variation, thereby
improving its overall robustness. The approach we propose serves as an ensemble
learning strategy, characterized by robust interpretability. A comprehensive
analysis of this methodology is presented in this paper. Across various tasks
such as person re-identification and semantic segmentation, our approach
consistently improves strong baseline methods. Notably, in comparison to
existing methods that prioritize color robustness, our strategy significantly
enhances performance in cross-domain scenarios. The code available at
\url{https://github.com/layumi/Person\_reID\_baseline\_pytorch/blob/master/random\_erasing.py}
or \url{https://github.com/finger-monkey/Data-Augmentation}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiaquan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lifei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Min Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10525">
<title>Focaler-IoU: More Focused Intersection over Union Loss. (arXiv:2401.10525v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10525</link>
<description rdf:parseType="Literal">&lt;p&gt;Bounding box regression plays a crucial role in the field of object
detection, and the positioning accuracy of object detection largely depends on
the loss function of bounding box regression. Existing researchs improve
regression performance by utilizing the geometric relationship between bounding
boxes, while ignoring the impact of difficult and easy sample distribution on
bounding box regression. In this article, we analyzed the impact of difficult
and easy sample distribution on regression results, and then proposed
Focaler-IoU, which can improve detector performance in different detection
tasks by focusing on different regression samples. Finally, comparative
experiments were conducted using existing advanced detectors and regression
methods for different detection tasks, and the detection performance was
further improved by using the method proposed in this paper.Code is available
at \url{https://github.com/malagoutou/Focaler-IoU}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shuaijie Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10526">
<title>On mitigating stability-plasticity dilemma in CLIP-guided image morphing via geodesic distillation loss. (arXiv:2401.10526v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10526</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale language-vision pre-training models, such as CLIP, have achieved
remarkable text-guided image morphing results by leveraging several
unconditional generative models. However, existing CLIP-guided image morphing
methods encounter difficulties when morphing photorealistic images.
Specifically, existing guidance fails to provide detailed explanations of the
morphing regions within the image, leading to misguidance. In this paper, we
observed that such misguidance could be effectively mitigated by simply using a
proper regularization loss. Our approach comprises two key components: 1) a
geodesic cosine similarity loss that minimizes inter-modality features (i.e.,
image and text) on a projected subspace of CLIP space, and 2) a latent
regularization loss that minimizes intra-modality features (i.e., image and
image) on the image manifold. By replacing the na\&quot;ive directional CLIP loss in
a drop-in replacement manner, our method achieves superior morphing results on
both images and videos for various benchmarks, including CLIP-inversion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_Y/0/1/0/all/0/1&quot;&gt;Yeongtak Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Saehyung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_U/0/1/0/all/0/1&quot;&gt;Uiwon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sungroh Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10529">
<title>Mementos: A Comprehensive Benchmark for Multimodal Large Language Model Reasoning over Image Sequences. (arXiv:2401.10529v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10529</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal Large Language Models (MLLMs) have demonstrated proficiency in
handling a variety of visual-language tasks. However, current MLLM benchmarks
are predominantly designed to evaluate reasoning based on static information
about a single image, and the ability of modern MLLMs to extrapolate from image
sequences, which is essential for understanding our ever-changing world, has
been less investigated. To address this challenge, this paper introduces
Mementos, a new benchmark designed to assess MLLMs&apos; sequential image reasoning
abilities. Mementos features 4,761 diverse image sequences with varying
lengths. We also employ a GPT-4 assisted method to evaluate MLLM reasoning
performance. Through a careful evaluation of nine recent MLLMs on Mementos,
including GPT-4V and Gemini, we find that they struggle to accurately describe
dynamic information about given image sequences, often leading to
hallucinations/misrepresentations of objects and their corresponding behaviors.
Our quantitative analysis and case studies identify three key factors impacting
MLLMs&apos; sequential image reasoning: the correlation between object and
behavioral hallucinations, the influence of cooccurring behaviors, and the
compounding impact of behavioral hallucinations. Our dataset is available at
https://github.com/umd-huang-lab/Mementos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuhang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Hongjin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yuancheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_F/0/1/0/all/0/1&quot;&gt;Feihong He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jaehong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Taixi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertasius_G/0/1/0/all/0/1&quot;&gt;Gedas Bertasius&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bansal_M/0/1/0/all/0/1&quot;&gt;Mohit Bansal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_H/0/1/0/all/0/1&quot;&gt;Huaxiu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Furong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10530">
<title>NWPU-MOC: A Benchmark for Fine-grained Multi-category Object Counting in Aerial Images. (arXiv:2401.10530v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10530</link>
<description rdf:parseType="Literal">&lt;p&gt;Object counting is a hot topic in computer vision, which aims to estimate the
number of objects in a given image. However, most methods only count objects of
a single category for an image, which cannot be applied to scenes that need to
count objects with multiple categories simultaneously, especially in aerial
scenes. To this end, this paper introduces a Multi-category Object Counting
(MOC) task to estimate the numbers of different objects (cars, buildings,
ships, etc.) in an aerial image. Considering the absence of a dataset for this
task, a large-scale Dataset (NWPU-MOC) is collected, consisting of 3,416 scenes
with a resolution of 1024 $\times$ 1024 pixels, and well-annotated using 14
fine-grained object categories. Besides, each scene contains RGB and Near
Infrared (NIR) images, of which the NIR spectrum can provide richer
characterization information compared with only the RGB spectrum. Based on
NWPU-MOC, the paper presents a multi-spectrum, multi-category object counting
framework, which employs a dual-attention module to fuse the features of RGB
and NIR and subsequently regress multi-channel density maps corresponding to
each object category. In addition, to modeling the dependency between different
channels in the density map with each object category, a spatial contrast loss
is designed as a penalty for overlapping predictions at the same spatial
position. Experimental results demonstrate that the proposed method achieves
state-of-the-art performance compared with some mainstream counting algorithms.
The dataset, code and models are publicly available at
https://github.com/lyongo/NWPU-MOC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_J/0/1/0/all/0/1&quot;&gt;Junyu Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Liangliang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuelong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10537">
<title>Learning Position-Aware Implicit Neural Network for Real-World Face Inpainting. (arXiv:2401.10537v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10537</link>
<description rdf:parseType="Literal">&lt;p&gt;Face inpainting requires the model to have a precise global understanding of
the facial position structure. Benefiting from the powerful capabilities of
deep learning backbones, recent works in face inpainting have achieved decent
performance in ideal setting (square shape with $512px$). However, existing
methods often produce a visually unpleasant result, especially in the
position-sensitive details (e.g., eyes and nose), when directly applied to
arbitrary-shaped images in real-world scenarios. The visually unpleasant
position-sensitive details indicate the shortcomings of existing methods in
terms of position information processing capability. In this paper, we propose
an \textbf{I}mplicit \textbf{N}eural \textbf{I}npainting \textbf{N}etwork
(IN$^2$) to handle arbitrary-shape face images in real-world scenarios by
explicit modeling for position information. Specifically, a downsample
processing encoder is proposed to reduce information loss while obtaining the
global semantic feature. A neighbor hybrid attention block is proposed with a
hybrid attention mechanism to improve the facial understanding ability of the
model without restricting the shape of the input. Finally, an implicit neural
pyramid decoder is introduced to explicitly model position information and
bridge the gap between low-resolution features and high-resolution output.
Extensive experiments demonstrate the superiority of the proposed method in
real-world face inpainting task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_B/0/1/0/all/0/1&quot;&gt;Bo Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jianlong Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10541">
<title>I-SplitEE: Image classification in Split Computing DNNs with Early Exits. (arXiv:2401.10541v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10541</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent advances in Deep Neural Networks (DNNs) stem from their
exceptional performance across various domains. However, their inherent large
size hinders deploying these networks on resource-constrained devices like
edge, mobile, and IoT platforms. Strategies have emerged, from partial cloud
computation offloading (split computing) to integrating early exits within DNN
layers. Our work presents an innovative unified approach merging early exits
and split computing. We determine the &apos;splitting layer&apos;, the optimal depth in
the DNN for edge device computations, and whether to infer on edge device or be
offloaded to the cloud for inference considering accuracy, computational
efficiency, and communication costs. Also, Image classification faces diverse
environmental distortions, influenced by factors like time of day, lighting,
and weather. To adapt to these distortions, we introduce I-SplitEE, an online
unsupervised algorithm ideal for scenarios lacking ground truths and with
sequential data. Experimental validation using Caltech-256 and Cifar-10
datasets subjected to varied distortions showcases I-SplitEE&apos;s ability to
reduce costs by a minimum of 55% with marginal performance degradation of at
most 5%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bajpai_D/0/1/0/all/0/1&quot;&gt;Divya Jyoti Bajpai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaiswal_A/0/1/0/all/0/1&quot;&gt;Aastha Jaiswal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hanawal_M/0/1/0/all/0/1&quot;&gt;Manjesh Kumar Hanawal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10556">
<title>Symbol as Points: Panoptic Symbol Spotting via Point-based Representation. (arXiv:2401.10556v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10556</link>
<description rdf:parseType="Literal">&lt;p&gt;This work studies the problem of panoptic symbol spotting, which is to spot
and parse both countable object instances (windows, doors, tables, etc.) and
uncountable stuff (wall, railing, etc.) from computer-aided design (CAD)
drawings. Existing methods typically involve either rasterizing the vector
graphics into images and using image-based methods for symbol spotting, or
directly building graphs and using graph neural networks for symbol
recognition. In this paper, we take a different approach, which treats graphic
primitives as a set of 2D points that are locally connected and use point cloud
segmentation methods to tackle it. Specifically, we utilize a point transformer
to extract the primitive features and append a mask2former-like spotting head
to predict the final output. To better use the local connection information of
primitives and enhance their discriminability, we further propose the attention
with connection module (ACM) and contrastive connection learning scheme (CCL).
Finally, we propose a KNN interpolation mechanism for the mask attention module
of the spotting head to better handle primitive mask downsampling, which is
primitive-level in contrast to pixel-level for the image. Our approach, named
SymPoint, is simple yet effective, outperforming recent state-of-the-art method
GAT-CADNet by an absolute increase of 9.6% PQ and 10.4% RQ on the FloorPlanCAD
dataset. The source code and models will be available at
https://github.com/nicehuster/SymPoint.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wenlong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_T/0/1/0/all/0/1&quot;&gt;Tianyu Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuhan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qizhi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10560">
<title>360ORB-SLAM: A Visual SLAM System for Panoramic Images with Depth Completion Network. (arXiv:2401.10560v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10560</link>
<description rdf:parseType="Literal">&lt;p&gt;To enhance the performance and effect of AR/VR applications and visual
assistance and inspection systems, visual simultaneous localization and mapping
(vSLAM) is a fundamental task in computer vision and robotics. However,
traditional vSLAM systems are limited by the camera&apos;s narrow field-of-view,
resulting in challenges such as sparse feature distribution and lack of dense
depth information. To overcome these limitations, this paper proposes a
360ORB-SLAM system for panoramic images that combines with a depth completion
network. The system extracts feature points from the panoramic image, utilizes
a panoramic triangulation module to generate sparse depth information, and
employs a depth completion network to obtain a dense panoramic depth map.
Experimental results on our novel panoramic dataset constructed based on Carla
demonstrate that the proposed method achieves superior scale accuracy compared
to existing monocular SLAM methods and effectively addresses the challenges of
feature association and scale ambiguity. The integration of the depth
completion network enhances system stability and mitigates the impact of
dynamic elements on SLAM performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yichen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yiqi Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ruyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_G/0/1/0/all/0/1&quot;&gt;Guodao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Bo Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianhua Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10561">
<title>MAEDiff: Masked Autoencoder-enhanced Diffusion Models for Unsupervised Anomaly Detection in Brain Images. (arXiv:2401.10561v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10561</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised anomaly detection has gained significant attention in the field
of medical imaging due to its capability of relieving the costly pixel-level
annotation. To achieve this, modern approaches usually utilize generative
models to produce healthy references of the diseased images and then identify
the abnormalities by comparing the healthy references and the original diseased
images. Recently, diffusion models have exhibited promising potential for
unsupervised anomaly detection in medical images for their good mode coverage
and high sample quality. However, the intrinsic characteristics of the medical
images, e.g. the low contrast, and the intricate anatomical structure of the
human body make the reconstruction challenging. Besides, the global information
of medical images often remain underutilized. To address these two issues, we
propose a novel Masked Autoencoder-enhanced Diffusion Model (MAEDiff) for
unsupervised anomaly detection in brain images. The MAEDiff involves a
hierarchical patch partition. It generates healthy images by overlapping
upper-level patches and implements a mechanism based on the masked autoencoders
operating on the sub-level patches to enhance the condition on the unnoised
regions. Extensive experiments on data of tumors and multiple sclerosis lesions
demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Du_B/0/1/0/all/0/1&quot;&gt;Bo Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10564">
<title>Dream360: Diverse and Immersive Outdoor Virtual Scene Creation via Transformer-Based 360 Image Outpainting. (arXiv:2401.10564v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10564</link>
<description rdf:parseType="Literal">&lt;p&gt;360 images, with a field-of-view (FoV) of 180x360, provide immersive and
realistic environments for emerging virtual reality (VR) applications, such as
virtual tourism, where users desire to create diverse panoramic scenes from a
narrow FoV photo they take from a viewpoint via portable devices. It thus
brings us to a technical challenge: `How to allow the users to freely create
diverse and immersive virtual scenes from a narrow FoV image with a specified
viewport?&apos; To this end, we propose a transformer-based 360 image outpainting
framework called Dream360, which can generate diverse, high-fidelity, and
high-resolution panoramas from user-selected viewports, considering the
spherical properties of 360 images. Compared with existing methods, e.g., [3],
which primarily focus on inputs with rectangular masks and central locations
while overlooking the spherical property of 360 images, our Dream360 offers
higher outpainting flexibility and fidelity based on the spherical
representation. Dream360 comprises two key learning stages: (I) codebook-based
panorama outpainting via Spherical-VQGAN (S-VQGAN), and (II) frequency-aware
refinement with a novel frequency-aware consistency loss. Specifically, S-VQGAN
learns a sphere-specific codebook from spherical harmonic (SH) values,
providing a better representation of spherical data distribution for scene
modeling. The frequency-aware refinement matches the resolution and further
improves the semantic consistency and visual fidelity of the generated results.
Our Dream360 achieves significantly lower Frechet Inception Distance (FID)
scores and better visual fidelity than existing methods. We also conducted a
user study involving 15 participants to interactively evaluate the quality of
the generated results in VR, demonstrating the flexibility and superiority of
our Dream360 framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_H/0/1/0/all/0/1&quot;&gt;Hao Ai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Z/0/1/0/all/0/1&quot;&gt;Zidong Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_H/0/1/0/all/0/1&quot;&gt;Haonan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_P/0/1/0/all/0/1&quot;&gt;Pengyuan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Tae-Kyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hui_P/0/1/0/all/0/1&quot;&gt;Pan Hui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10578">
<title>3D Shape Completion on Unseen Categories:A Weakly-supervised Approach. (arXiv:2401.10578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10578</link>
<description rdf:parseType="Literal">&lt;p&gt;3D shapes captured by scanning devices are often incomplete due to occlusion.
3D shape completion methods have been explored to tackle this limitation.
However, most of these methods are only trained and tested on a subset of
categories, resulting in poor generalization to unseen categories. In this
paper, we introduce a novel weakly-supervised framework to reconstruct the
complete shapes from unseen categories. We first propose an end-to-end
prior-assisted shape learning network that leverages data from the seen
categories to infer a coarse shape. Specifically, we construct a prior bank
consisting of representative shapes from the seen categories. Then, we design a
multi-scale pattern correlation module for learning the complete shape of the
input by analyzing the correlation between local patterns within the input and
the priors at various scales. In addition, we propose a self-supervised shape
refinement model to further refine the coarse shape. Considering the shape
variability of 3D objects across categories, we construct a category-specific
prior bank to facilitate shape refinement. Then, we devise a voxel-based
partial matching loss and leverage the partial scans to drive the refinement
process. Extensive experimental results show that our approach is superior to
state-of-the-art methods by a large margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lintai Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Junhui Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Linqi Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10588">
<title>DGL: Dynamic Global-Local Prompt Tuning for Text-Video Retrieval. (arXiv:2401.10588v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10588</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-video retrieval is a critical multi-modal task to find the most relevant
video for a text query. Although pretrained models like CLIP have demonstrated
impressive potential in this area, the rising cost of fully finetuning these
models due to increasing model size continues to pose a problem. To address
this challenge, prompt tuning has emerged as an alternative. However, existing
works still face two problems when adapting pretrained image-text models to
downstream video-text tasks: (1) The visual encoder could only encode
frame-level features and failed to extract global-level general video
information. (2) Equipping the visual and text encoder with separated prompts
failed to mitigate the visual-text modality gap. To this end, we propose DGL, a
cross-modal Dynamic prompt tuning method with Global-Local video attention. In
contrast to previous prompt tuning methods, we employ the shared latent space
to generate local-level text and frame prompts that encourage inter-modal
interaction. Furthermore, we propose modeling video in a global-local attention
mechanism to capture global video information from the perspective of prompt
tuning. Extensive experiments reveal that when only 0.67% parameters are tuned,
our cross-modal prompt tuning strategy DGL outperforms or is comparable to
fully finetuning methods on MSR-VTT, VATEX, LSMDC, and ActivityNet datasets.
Code will be available at https://github.com/knightyxp/DGL
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiangpeng Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Linchao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaohan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10608">
<title>M2ORT: Many-To-One Regression Transformer for Spatial Transcriptomics Prediction from Histopathology Images. (arXiv:2401.10608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10608</link>
<description rdf:parseType="Literal">&lt;p&gt;The advancement of Spatial Transcriptomics (ST) has facilitated the
spatially-aware profiling of gene expressions based on histopathology images.
Although ST data offers valuable insights into the micro-environment of tumors,
its acquisition cost remains expensive. Therefore, directly predicting the ST
expressions from digital pathology images is desired. Current methods usually
adopt existing regression backbones for this task, which ignore the inherent
multi-scale hierarchical data structure of digital pathology images. To address
this limit, we propose M2ORT, a many-to-one regression Transformer that can
accommodate the hierarchical structure of the pathology images through a
decoupled multi-scale feature extractor. Different from traditional models that
are trained with one-to-one image-label pairs, M2ORT accepts multiple pathology
images of different magnifications at a time to jointly predict the gene
expressions at their corresponding common ST spot, aiming at learning a
many-to-one relationship through training. We have tested M2ORT on three public
ST datasets and the experimental results show that M2ORT can achieve
state-of-the-art performance with fewer parameters and floating-point
operations (FLOPs). The code is available at:
https://github.com/Dootmaan/M2ORT/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiuju Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ouyang_S/0/1/0/all/0/1&quot;&gt;Shuyi Ouyang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yen-Wei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lanfen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10620">
<title>Polytopic Autoencoders with Smooth Clustering for Reduced-order Modelling of Flows. (arXiv:2401.10620v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2401.10620</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advancement of neural networks, there has been a notable increase,
both in terms of quantity and variety, in research publications concerning the
application of autoencoders to reduced-order models. We propose a polytopic
autoencoder architecture that includes a lightweight nonlinear encoder, a
convex combination decoder, and a smooth clustering network. Supported by
several proofs, the model architecture ensures that all reconstructed states
lie within a polytope, accompanied by a metric indicating the quality of the
constructed polytopes, referred to as polytope error. Additionally, it offers a
minimal number of convex coordinates for polytopic linear-parameter varying
systems while achieving acceptable reconstruction errors compared to proper
orthogonal decomposition (POD). To validate our proposed model, we conduct
simulations involving two flow scenarios with the incompressible Navier-Stokes
equation. Numerical results demonstrate the guaranteed properties of the model,
low reconstruction errors compared to POD, and the improvement in error using a
clustering network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heiland_J/0/1/0/all/0/1&quot;&gt;Jan Heiland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yongho Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10637">
<title>Towards Universal Unsupervised Anomaly Detection in Medical Imaging. (arXiv:2401.10637v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10637</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing complexity of medical imaging data underscores the need for
advanced anomaly detection methods to automatically identify diverse
pathologies. Current methods face challenges in capturing the broad spectrum of
anomalies, often limiting their use to specific lesion types in brain scans. To
address this challenge, we introduce a novel unsupervised approach, termed
\textit{Reversed Auto-Encoders (RA)}, designed to create realistic
pseudo-healthy reconstructions that enable the detection of a wider range of
pathologies. We evaluate the proposed method across various imaging modalities,
including magnetic resonance imaging (MRI) of the brain, pediatric wrist X-ray,
and chest X-ray, and demonstrate superior performance in detecting anomalies
compared to existing state-of-the-art methods. Our unsupervised anomaly
detection approach may enhance diagnostic accuracy in medical imaging by
identifying a broader range of unknown pathologies. Our code is publicly
available at: \url{https://github.com/ci-ber/RA}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bercea_C/0/1/0/all/0/1&quot;&gt;Cosmin I. Bercea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wiestler_B/0/1/0/all/0/1&quot;&gt;Benedikt Wiestler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schnabel_J/0/1/0/all/0/1&quot;&gt;Julia A. Schnabel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10640">
<title>A comprehensive study on fidelity metrics for XAI. (arXiv:2401.10640v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10640</link>
<description rdf:parseType="Literal">&lt;p&gt;The use of eXplainable Artificial Intelligence (XAI) systems has introduced a
set of challenges that need resolution. Herein, we focus on how to correctly
select an XAI method, an open questions within the field. The inherent
difficulty of this task is due to the lack of a ground truth. Several authors
have proposed metrics to approximate the fidelity of different XAI methods.
These metrics lack verification and have concerning disagreements. In this
study, we proposed a novel methodology to verify fidelity metrics, using a
well-known transparent model, namely a decision tree. This model allowed us to
obtain explanations with perfect fidelity. Our proposal constitutes the first
objective benchmark for these metrics, facilitating a comparison of existing
proposals, and surpassing existing methods. We applied our benchmark to assess
the existing fidelity metrics in two different experiments, each using public
datasets comprising 52,000 images. The images from these datasets had a size a
128 by 128 pixels and were synthetic data that simplified the training process.
All metric values, indicated a lack of fidelity, with the best one showing a 30
\% deviation from the expected values for perfect explanation. Our
experimentation led us to conclude that the current fidelity metrics are not
reliable enough to be used in real scenarios. From this finding, we deemed it
necessary to development new metrics, to avoid the detected problems, and we
recommend the usage of our proposal as a benchmark within the scientific
community to address these limitations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miro_Nicolau_M/0/1/0/all/0/1&quot;&gt;Miquel Mir&amp;#xf3;-Nicolau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jaume_i_Capo_A/0/1/0/all/0/1&quot;&gt;Antoni Jaume-i-Cap&amp;#xf3;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moya_Alcover_G/0/1/0/all/0/1&quot;&gt;Gabriel Moy&amp;#xe0;-Alcover&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10643">
<title>A Comprehensive Survey on Deep-Learning-based Vehicle Re-Identification: Models, Data Sets and Challenges. (arXiv:2401.10643v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10643</link>
<description rdf:parseType="Literal">&lt;p&gt;Vehicle re-identification (ReID) endeavors to associate vehicle images
collected from a distributed network of cameras spanning diverse traffic
environments. This task assumes paramount importance within the spectrum of
vehicle-centric technologies, playing a pivotal role in deploying Intelligent
Transportation Systems (ITS) and advancing smart city initiatives. Rapid
advancements in deep learning have significantly propelled the evolution of
vehicle ReID technologies in recent years. Consequently, undertaking a
comprehensive survey of methodologies centered on deep learning for vehicle
re-identification has become imperative and inescapable. This paper extensively
explores deep learning techniques applied to vehicle ReID. It outlines the
categorization of these methods, encompassing supervised and unsupervised
approaches, delves into existing research within these categories, introduces
datasets and evaluation criteria, and delineates forthcoming challenges and
potential research directions. This comprehensive assessment examines the
landscape of deep learning in vehicle ReID and establishes a foundation and
starting point for future works. It aims to serve as a complete reference by
highlighting challenges and emerging trends, fostering advancements and
applications in vehicle ReID utilizing deep learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amiri_A/0/1/0/all/0/1&quot;&gt;Ali Amiri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaya_A/0/1/0/all/0/1&quot;&gt;Aydin Kaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keceli_A/0/1/0/all/0/1&quot;&gt;Ali Seydi Keceli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10659">
<title>BadODD: Bangladeshi Autonomous Driving Object Detection Dataset. (arXiv:2401.10659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10659</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a comprehensive dataset for object detection in diverse driving
environments across 9 districts in Bangladesh. The dataset, collected
exclusively from smartphone cameras, provided a realistic representation of
real-world scenarios, including day and night conditions. Most existing
datasets lack suitable classes for autonomous navigation on Bangladeshi roads,
making it challenging for researchers to develop models that can handle the
intricacies of road scenarios. To address this issue, the authors proposed a
new set of classes based on characteristics rather than local vehicle names.
The dataset aims to encourage the development of models that can handle the
unique challenges of Bangladeshi road scenarios for the effective deployment of
autonomous vehicles. The dataset did not consist of any online images to
simulate real-world conditions faced by autonomous vehicles. The classification
of vehicles is challenging because of the diverse range of vehicles on
Bangladeshi roads, including those not found elsewhere in the world. The
proposed classification system is scalable and can accommodate future vehicles,
making it a valuable resource for researchers in the autonomous vehicle sector.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baig_M/0/1/0/all/0/1&quot;&gt;Mirza Nihal Baig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajong_R/0/1/0/all/0/1&quot;&gt;Rony Hajong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patwary_M/0/1/0/all/0/1&quot;&gt;Mahdi Murshed Patwary&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Mohammad Shahidur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_H/0/1/0/all/0/1&quot;&gt;Husne Ara Chowdhury&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10666">
<title>MixNet: Towards Effective and Efficient UHD Low-Light Image Enhancement. (arXiv:2401.10666v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10666</link>
<description rdf:parseType="Literal">&lt;p&gt;With the continuous advancement of imaging devices, the prevalence of
Ultra-High-Definition (UHD) images is rising. Although many image restoration
methods have achieved promising results, they are not directly applicable to
UHD images on devices with limited computational resources due to the
inherently high computational complexity of UHD images. In this paper, we focus
on the task of low-light image enhancement (LLIE) and propose a novel LLIE
method called MixNet, which is designed explicitly for UHD images. To capture
the long-range dependency of features without introducing excessive
computational complexity, we present the Global Feature Modulation Layer
(GFML). GFML associates features from different views by permuting the feature
maps, enabling efficient modeling of long-range dependency. In addition, we
also design the Local Feature Modulation Layer (LFML) and Feed-forward Layer
(FFL) to capture local features and transform features into a compact
representation. This way, our MixNet achieves effective LLIE with few model
parameters and low computational complexity. We conducted extensive experiments
on both synthetic and real-world datasets, and the comprehensive results
demonstrate that our proposed method surpasses the performance of current
state-of-the-art methods. The code will be available at
\url{https://github.com/zzr-idam/MixNet}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chen Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiuyi Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_W/0/1/0/all/0/1&quot;&gt;Wenqi Ren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10709">
<title>Dense 3D Reconstruction Through Lidar: A Comparative Study on Ex-vivo Porcine Tissue. (arXiv:2401.10709v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10709</link>
<description rdf:parseType="Literal">&lt;p&gt;New sensing technologies and more advanced processing algorithms are
transforming computer-integrated surgery. While researchers are actively
investigating depth sensing and 3D reconstruction for vision-based surgical
assistance, it remains difficult to achieve real-time, accurate, and robust 3D
representations of the abdominal cavity for minimally invasive surgery. Thus,
this work uses quantitative testing on fresh ex-vivo porcine tissue to
thoroughly characterize the quality with which a 3D laser-based time-of-flight
sensor (lidar) can perform anatomical surface reconstruction. Ground-truth
surface shapes are captured with a commercial laser scanner, and the resulting
signed error fields are analyzed using rigorous statistical tools. When
compared to modern learning-based stereo matching from endoscopic images,
time-of-flight sensing demonstrates higher precision, lower processing delay,
higher frame rate, and superior robustness against sensor distance and poor
illumination. Furthermore, we report on the potential negative effect of
near-infrared light penetration on the accuracy of lidar measurements across
different tissue samples, identifying a significant measured depth offset for
muscle in contrast to fat and liver. Our findings highlight the potential of
lidar for intraoperative 3D perception and point toward new methods that
combine complementary time-of-flight and spectral imaging.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Caccianiga_G/0/1/0/all/0/1&quot;&gt;Guido Caccianiga&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nubert_J/0/1/0/all/0/1&quot;&gt;Julian Nubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hutter_M/0/1/0/all/0/1&quot;&gt;Marco Hutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kuchenbecker_K/0/1/0/all/0/1&quot;&gt;Katherine J. Kuchenbecker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10711">
<title>Weakly Supervised Gaussian Contrastive Grounding with Large Multimodal Models for Video Question Answering. (arXiv:2401.10711v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10711</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Question Answering (VideoQA) aims to answer natural language questions
based on the information observed in videos. Despite the recent success of
Large Multimodal Models (LMMs) in image-language understanding and reasoning,
they deal with VideoQA insufficiently by simply taking uniformly sampled frames
as visual inputs, which ignores question-relevant visual clues. Moreover, there
are no human annotations for question-critical timestamps in existing VideoQA
datasets. In light of this, we propose a novel weakly supervised framework to
enforce the LMMs to reason out the answers with question-critical moments as
visual inputs. Specifically, we fuse the question and answer pairs as event
descriptions to find multiple keyframes as target moments, which will be
pseudo-labels. With these pseudo-labels as additionally weak supervision, we
devise a lightweight Gaussian-based Contrastive Grounding (GCG) module. GCG
learns multiple Gaussian functions to characterize the temporal structure of
the video, and sample question-critical frames as positive moments to be the
visual inputs of LMMs. Extensive experiments on several VideoQA benchmarks
verify the effectiveness of our framework, and we achieve substantial
improvements compared to previous state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_C/0/1/0/all/0/1&quot;&gt;Chenghang Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yixuan Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Weifeng Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10712">
<title>Q&amp;A Prompts: Discovering Rich Visual Clues through Mining Question-Answer Prompts for VQA requiring Diverse World Knowledge. (arXiv:2401.10712v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10712</link>
<description rdf:parseType="Literal">&lt;p&gt;With the breakthrough of multi-modal large language models, answering complex
visual questions that demand advanced reasoning abilities and world knowledge
has become a much more important testbed for developing AI models than ever.
However, equipping AI models with robust cross-modality reasoning ability
remains challenging since the cognition scheme of humans has not been
understood systematically. In this paper, we believe that if we can collect
visual clues in the given image as much as possible, we will recognize the
image more accurately, understand the question better, recall relevant
knowledge more easily, and finally reason out the answer. We discover these
rich visual clues by mining question-answer pairs in images and sending them
into multi-modal large language models as prompts. We call the proposed method
Q&amp;amp;A Prompts. Specifically, we first use the image-answer pairs and the
corresponding questions in the training set as inputs and outputs to train a
visual question generation model. Then, we use an image tagging model to
identify various instances and send packaged image-tag pairs into the visual
question generation model to generate relevant questions with the extracted
image tags as answers. Finally, we encode these generated question-answer pairs
as prompts with a visual-aware prompting module and send them into pre-trained
multi-modal large language models to reason out the final answers. Experimental
results show that, compared with state-of-the-art methods, our Q&amp;amp;A Prompts
achieves substantial improvements on the challenging visual question answering
datasets requiring reasoning over diverse world knowledge, such as OK-VQA and
A-OKVQA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haibi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_W/0/1/0/all/0/1&quot;&gt;Weifeng Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10727">
<title>Tool-LMM: A Large Multi-Modal Model for Tool Agent Learning. (arXiv:2401.10727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10727</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, the astonishing performance of large language models (LLMs) in
natural language comprehension and generation tasks triggered lots of
exploration of using them as central controllers to build agent systems.
Multiple studies focus on bridging the LLMs to external tools to extend the
application scenarios. However, the current LLMs&apos; perceiving tool-use ability
is limited to a single text query, which may result in ambiguity in
understanding the users&apos; real intentions. LLMs are expected to eliminate that
by perceiving the visual- or auditory-grounded instructions&apos; information.
Therefore, in this paper, we propose Tool-LMM, a system incorporating
open-source LLMs and multi-modal encoders so that the learnt LLMs can be
conscious of multi-modal input instruction and then select the function-matched
tool correctly. To facilitate the evaluation of the model&apos;s capability, we
collect a dataset featured by consisting of multi-modal input tools from
HuggingFace. Another important feature of our dataset is that our dataset also
contains multiple potential choices for the same instruction due to the
existence of identical functions and synonymous functions, which provides more
potential solutions for the same query. The experiments reveal that our LMM is
capable of recommending appropriate tools for multi-modal instructions. Codes
and data are available at https://github.com/Tool-LMM/Tool-LMM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chenyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_W/0/1/0/all/0/1&quot;&gt;Weixin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Q/0/1/0/all/0/1&quot;&gt;Qianyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1&quot;&gt;Haonan Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_J/0/1/0/all/0/1&quot;&gt;Jindi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Sixun Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiaohua/0/1/0/all/0/1&quot;&gt;Xiaohua&lt;/a&gt; (Michael) &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xuan/0/1/0/all/0/1&quot;&gt;Xuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhengxin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_S/0/1/0/all/0/1&quot;&gt;Shenghua Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10731">
<title>Removal and Selection: Improving RGB-Infrared Object Detection via Coarse-to-Fine Fusion. (arXiv:2401.10731v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10731</link>
<description rdf:parseType="Literal">&lt;p&gt;Object detection in visible (RGB) and infrared (IR) images has been widely
applied in recent years. Leveraging the complementary characteristics of RGB
and IR images, the object detector provides reliable and robust object
localization from day to night. Existing fusion strategies directly inject RGB
and IR images into convolution neural networks, leading to inferior detection
performance. Since the RGB and IR features have modality-specific noise, these
strategies will worsen the fused features along with the propagation. Inspired
by the mechanism of human brain processing multimodal information, this work
introduces a new coarse-to-fine perspective to purify and fuse two modality
features. Specifically, following this perspective, we design a Redundant
Spectrum Removal module to coarsely remove interfering information within each
modality and a Dynamic Feature Selection module to finely select the desired
features for feature fusion. To verify the effectiveness of the coarse-to-fine
fusion strategy, we construct a new object detector called Removal and
Selection Detector (RSDet). Extensive experiments on three RGB-IR object
detection datasets verify the superior performance of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianyi Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Maoxun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10732">
<title>Bridging the gap between image coding for machines and humans. (arXiv:2401.10732v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10732</link>
<description rdf:parseType="Literal">&lt;p&gt;Image coding for machines (ICM) aims at reducing the bitrate required to
represent an image while minimizing the drop in machine vision analysis
accuracy. In many use cases, such as surveillance, it is also important that
the visual quality is not drastically deteriorated by the compression process.
Recent works on using neural network (NN) based ICM codecs have shown
significant coding gains against traditional methods; however, the decompressed
images, especially at low bitrates, often contain checkerboard artifacts. We
propose an effective decoder finetuning scheme based on adversarial training to
significantly enhance the visual quality of ICM codecs, while preserving the
machine analysis accuracy, without adding extra bitcost or parameters at the
inference phase. The results show complete removal of the checkerboard
artifacts at the negligible cost of -1.6% relative change in task performance
score. In the cases where some amount of artifacts is tolerable, such as when
machine consumption is the primary target, this technique can enhance both
pixel-fidelity and feature-fidelity scores without losing task performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Nam Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Honglei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cricri_F/0/1/0/all/0/1&quot;&gt;Francesco Cricri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Youvalari_R/0/1/0/all/0/1&quot;&gt;Ramin G. Youvalari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tavakoli_H/0/1/0/all/0/1&quot;&gt;Hamed Rezazadegan Tavakoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Aksu_E/0/1/0/all/0/1&quot;&gt;Emre Aksu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hannuksela_M/0/1/0/all/0/1&quot;&gt;Miska M. Hannuksela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahtu_E/0/1/0/all/0/1&quot;&gt;Esa Rahtu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10741">
<title>Character Recognition in Byzantine Seals with Deep Neural Networks. (arXiv:2401.10741v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10741</link>
<description rdf:parseType="Literal">&lt;p&gt;Seals are small coin-shaped artifacts, mostly made of lead, held with strings
to seal letters. This work presents the first attempt towards automatic reading
of text on Byzantine seal images.Byzantine seals are generally decorated with
iconography on the obverse side and Greek text on the reverse side. Text may
include the sender&apos;s name, position in the Byzantine aristocracy, and elements
of prayers. Both text and iconography are precious literary sources that wait
to be exploited electronically, so the development of computerized systems for
interpreting seals images is of paramount importance. This work&apos;s contribution
is hence a deep, two-stages, character reading pipeline for transcribing
Byzantine seal images. A first deep convolutional neural network (CNN) detects
characters in the seal (character localization). A second convolutional network
reads the localized characters (character classification). Finally, a
diplomatic transcription of the seal is provided by post-processing the two
network outputs. We provide an experimental evaluation of each CNN in isolation
and both CNNs in combination. All performances are evaluated by
cross-validation. Character localization achieves a mean average precision
(mAP@0.5) greater than 0.9. Classification of characters cropped from ground
truth bounding boxes achieves Top-1 accuracy greater than 0.92. End-to-end
evaluation shows the efficiency of the proposed approach when compared to the
SoTA for similar tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rageau_T/0/1/0/all/0/1&quot;&gt;Th&amp;#xe9;ophile Rageau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Likforman_Sulem_L/0/1/0/all/0/1&quot;&gt;Laurence Likforman-Sulem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fiandrotti_A/0/1/0/all/0/1&quot;&gt;Attilio Fiandrotti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eyharabide_V/0/1/0/all/0/1&quot;&gt;Victoria Eyharabide&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caseau_B/0/1/0/all/0/1&quot;&gt;B&amp;#xe9;atrice Caseau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheynet_J/0/1/0/all/0/1&quot;&gt;Jean-Claude Cheynet&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10752">
<title>HiCD: Change Detection in Quality-Varied Images via Hierarchical Correlation Distillation. (arXiv:2401.10752v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10752</link>
<description rdf:parseType="Literal">&lt;p&gt;Advanced change detection techniques primarily target image pairs of equal
and high quality. However, variations in imaging conditions and platforms
frequently lead to image pairs with distinct qualities: one image being
high-quality, while the other being low-quality. These disparities in image
quality present significant challenges for understanding image pairs
semantically and extracting change features, ultimately resulting in a notable
decline in performance. To tackle this challenge, we introduce an innovative
training strategy grounded in knowledge distillation. The core idea revolves
around leveraging task knowledge acquired from high-quality image pairs to
guide the model&apos;s learning process when dealing with image pairs that exhibit
differences in quality. Additionally, we develop a hierarchical correlation
distillation approach (involving self-correlation, cross-correlation, and
global correlation). This approach compels the student model to replicate the
correlations inherent in the teacher model, rather than focusing solely on
individual features. This ensures effective knowledge transfer while
maintaining the student model&apos;s training flexibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_C/0/1/0/all/0/1&quot;&gt;Chao Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_X/0/1/0/all/0/1&quot;&gt;Xingxing Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jiang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_G/0/1/0/all/0/1&quot;&gt;Gui-Song Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10761">
<title>NN-VVC: Versatile Video Coding boosted by self-supervisedly learned image coding for machines. (arXiv:2401.10761v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.10761</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent progress in artificial intelligence has led to an ever-increasing
usage of images and videos by machine analysis algorithms, mainly neural
networks. Nonetheless, compression, storage and transmission of media have
traditionally been designed considering human beings as the viewers of the
content. Recent research on image and video coding for machine analysis has
progressed mainly in two almost orthogonal directions. The first is represented
by end-to-end (E2E) learned codecs which, while offering high performance on
image coding, are not yet on par with state-of-the-art conventional video
codecs and lack interoperability. The second direction considers using the
Versatile Video Coding (VVC) standard or any other conventional video codec
(CVC) together with pre- and post-processing operations targeting machine
analysis. While the CVC-based methods benefit from interoperability and broad
hardware and software support, the machine task performance is often lower than
the desired level, particularly in low bitrates. This paper proposes a hybrid
codec for machines called NN-VVC, which combines the advantages of an
E2E-learned image codec and a CVC to achieve high performance in both image and
video coding for machines. Our experiments show that the proposed system
achieved up to -43.20% and -26.8% Bj{\o}ntegaard Delta rate reduction over VVC
for image and video data, respectively, when evaluated on multiple different
datasets and machine vision tasks. To the best of our knowledge, this is the
first research paper showing a hybrid video codec that outperforms VVC on
multiple datasets and multiple machine vision tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ahonen_J/0/1/0/all/0/1&quot;&gt;Jukka I. Ahonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Nam Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Honglei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hallapuro_A/0/1/0/all/0/1&quot;&gt;Antti Hallapuro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Cricri_F/0/1/0/all/0/1&quot;&gt;Francesco Cricri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tavakoli_H/0/1/0/all/0/1&quot;&gt;Hamed Rezazadegan Tavakoli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hannuksela_M/0/1/0/all/0/1&quot;&gt;Miska M. Hannuksela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rahtu_E/0/1/0/all/0/1&quot;&gt;Esa Rahtu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10777">
<title>Determination of efficiency indicators of the stand for intelligent control of manual operations in industrial production. (arXiv:2401.10777v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10777</link>
<description rdf:parseType="Literal">&lt;p&gt;Systems of intelligent control of manual operations in industrial production
are being implemented in many industries nowadays. Such systems use
high-resolution cameras and computer vision algorithms to automatically track
the operator&apos;s manipulations and prevent technological errors in the assembly
process. At the same time compliance with safety regulations in the workspace
is monitored. As a result, the defect rate of manufactured products and the
number of accidents during the manual assembly of any device are decreased.
Before implementing an intelligent control system into a real production it is
necessary to calculate its efficiency. In order to do it experiments on the
stand for manual operations control systems were carried out. This paper
proposes the methodology for calculating the efficiency indicators. This
mathematical approach is based on the IoU calculation of real- and
predicted-time intervals between assembly stages. The results show high
precision in tracking the validity of manual assembly and do not depend on the
duration of the assembly process.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sergeev_A/0/1/0/all/0/1&quot;&gt;Anton Sergeev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Minchenkov_V/0/1/0/all/0/1&quot;&gt;Victor Minchenkov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soldatov_A/0/1/0/all/0/1&quot;&gt;Aleksei Soldatov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10786">
<title>Sat2Scene: 3D Urban Scene Generation from Satellite Images with Diffusion. (arXiv:2401.10786v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10786</link>
<description rdf:parseType="Literal">&lt;p&gt;Directly generating scenes from satellite imagery offers exciting
possibilities for integration into applications like games and map services.
However, challenges arise from significant view changes and scene scale.
Previous efforts mainly focused on image or video generation, lacking
exploration into the adaptability of scene generation for arbitrary views.
Existing 3D generation works either operate at the object level or are
difficult to utilize the geometry obtained from satellite imagery. To overcome
these limitations, we propose a novel architecture for direct 3D scene
generation by introducing diffusion models into 3D sparse representations and
combining them with neural rendering techniques. Specifically, our approach
generates texture colors at the point level for a given geometry using a 3D
diffusion model first, which is then transformed into a scene representation in
a feed-forward manner. The representation can be utilized to render arbitrary
views which would excel in both single-frame quality and inter-frame
consistency. Experiments in two city-scale datasets show that our model
demonstrates proficiency in generating photo-realistic street-view image
sequences and cross-view urban scenes from satellite imagery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zuoyue Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhenqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_Z/0/1/0/all/0/1&quot;&gt;Zhaopeng Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oswald_M/0/1/0/all/0/1&quot;&gt;Martin R. Oswald&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10790">
<title>Measuring the Impact of Scene Level Objects on Object Detection: Towards Quantitative Explanations of Detection Decisions. (arXiv:2401.10790v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10790</link>
<description rdf:parseType="Literal">&lt;p&gt;Although accuracy and other common metrics can provide a useful window into
the performance of an object detection model, they lack a deeper view of the
model&apos;s decision process. Regardless of the quality of the training data and
process, the features that an object detection model learns cannot be
guaranteed. A model may learn a relationship between certain background
context, i.e., scene level objects, and the presence of the labeled classes.
Furthermore, standard performance verification and metrics would not identify
this phenomenon. This paper presents a new black box explainability method for
additional verification of object detection models by finding the impact of
scene level objects on the identification of the objects within the image. By
comparing the accuracies of a model on test data with and without certain scene
level objects, the contributions of these objects to the model&apos;s performance
becomes clearer. The experiment presented here will assess the impact of
buildings and people in image context on the detection of emergency road
vehicles by a fine-tuned YOLOv8 model. A large increase in accuracy in the
presence of a scene level object will indicate the model&apos;s reliance on that
object to make its detections. The results of this research lead to providing a
quantitative explanation of the object detection model&apos;s decision process,
enabling a deeper understanding of the model&apos;s performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haar_L/0/1/0/all/0/1&quot;&gt;Lynn Vonder Haar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elvira_T/0/1/0/all/0/1&quot;&gt;Timothy Elvira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Newcomb_L/0/1/0/all/0/1&quot;&gt;Luke Newcomb&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ochoa_O/0/1/0/all/0/1&quot;&gt;Omar Ochoa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10805">
<title>Learning to Visually Connect Actions and their Effects. (arXiv:2401.10805v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10805</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we introduce the novel concept of visually Connecting Actions
and Their Effects (CATE) in video understanding. CATE can have applications in
areas like task planning and learning from demonstration. We propose different
CATE-based task formulations, such as action selection and action
specification, where video understanding models connect actions and effects at
semantic and fine-grained levels. We observe that different formulations
produce representations capturing intuitive action properties. We also design
various baseline models for action selection and action specification. Despite
the intuitive nature of the task, we observe that models struggle, and humans
outperform them by a large margin. The study aims to establish a foundation for
future efforts, showcasing the flexibility and versatility of connecting
actions and effects in video understanding, with the hope of inspiring advanced
formulations and models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peh_E/0/1/0/all/0/1&quot;&gt;Eric Peh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parmar_P/0/1/0/all/0/1&quot;&gt;Paritosh Parmar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fernando_B/0/1/0/all/0/1&quot;&gt;Basura Fernando&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10815">
<title>RAD-DINO: Exploring Scalable Medical Image Encoders Beyond Text Supervision. (arXiv:2401.10815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10815</link>
<description rdf:parseType="Literal">&lt;p&gt;Language-supervised pre-training has proven to be a valuable method for
extracting semantically meaningful features from images, serving as a
foundational element in multimodal systems within the computer vision and
medical imaging domains. However, resulting features are limited by the
information contained within the text. This is particularly problematic in
medical imaging, where radiologists&apos; written findings focus on specific
observations; a challenge compounded by the scarcity of paired imaging-text
data due to concerns over leakage of personal health information. In this work,
we fundamentally challenge the prevailing reliance on language supervision for
learning general purpose biomedical imaging encoders. We introduce RAD-DINO, a
biomedical image encoder pre-trained solely on unimodal biomedical imaging data
that obtains similar or greater performance than state-of-the-art biomedical
language supervised models on a diverse range of benchmarks. Specifically, the
quality of learned representations is evaluated on standard imaging tasks
(classification and semantic segmentation), and a vision-language alignment
task (text report generation from images). To further demonstrate the drawback
of language supervision, we show that features from RAD-DINO correlate with
other medical records (e.g., sex or age) better than language-supervised
models, which are generally not mentioned in radiology reports. Finally, we
conduct a series of ablations determining the factors in RAD-DINO&apos;s
performance; notably, we observe that RAD-DINO&apos;s downstream performance scales
well with the quantity and diversity of training data, demonstrating that
image-only supervision is a scalable approach for training a foundational
biomedical image encoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perez_Garcia_F/0/1/0/all/0/1&quot;&gt;Fernando P&amp;#xe9;rez-Garc&amp;#xed;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_H/0/1/0/all/0/1&quot;&gt;Harshita Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bond_Taylor_S/0/1/0/all/0/1&quot;&gt;Sam Bond-Taylor&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bouzid_K/0/1/0/all/0/1&quot;&gt;Kenza Bouzid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salvatelli_V/0/1/0/all/0/1&quot;&gt;Valentina Salvatelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilse_M/0/1/0/all/0/1&quot;&gt;Maximilian Ilse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bannur_S/0/1/0/all/0/1&quot;&gt;Shruthi Bannur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Castro_D/0/1/0/all/0/1&quot;&gt;Daniel C. Castro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwaighofer_A/0/1/0/all/0/1&quot;&gt;Anton Schwaighofer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lungren_M/0/1/0/all/0/1&quot;&gt;Matthew P. Lungren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetscherek_M/0/1/0/all/0/1&quot;&gt;Maria Wetscherek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Codella_N/0/1/0/all/0/1&quot;&gt;Noel Codella&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyland_S/0/1/0/all/0/1&quot;&gt;Stephanie L. Hyland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alvarez_Valle_J/0/1/0/all/0/1&quot;&gt;Javier Alvarez-Valle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oktay_O/0/1/0/all/0/1&quot;&gt;Ozan Oktay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10822">
<title>ActAnywhere: Subject-Aware Video Background Generation. (arXiv:2401.10822v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10822</link>
<description rdf:parseType="Literal">&lt;p&gt;Generating video background that tailors to foreground subject motion is an
important problem for the movie industry and visual effects community. This
task involves synthesizing background that aligns with the motion and
appearance of the foreground subject, while also complies with the artist&apos;s
creative intention. We introduce ActAnywhere, a generative model that automates
this process which traditionally requires tedious manual efforts. Our model
leverages the power of large-scale video diffusion models, and is specifically
tailored for this task. ActAnywhere takes a sequence of foreground subject
segmentation as input and an image that describes the desired scene as
condition, to produce a coherent video with realistic foreground-background
interactions while adhering to the condition frame. We train our model on a
large-scale dataset of human-scene interaction videos. Extensive evaluations
demonstrate the superior performance of our model, significantly outperforming
baselines. Moreover, we show that ActAnywhere generalizes to diverse
out-of-distribution samples, including non-human subjects. Please visit our
project webpage at https://actanywhere.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_B/0/1/0/all/0/1&quot;&gt;Boxiao Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chun-Hao Paul Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_K/0/1/0/all/0/1&quot;&gt;Krishna Kumar Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas J. Guibas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jimei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10831">
<title>Understanding Video Transformers via Universal Concept Discovery. (arXiv:2401.10831v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10831</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies the problem of concept-based interpretability of
transformer representations for videos. Concretely, we seek to explain the
decision-making process of video transformers based on high-level,
spatiotemporal concepts that are automatically discovered. Prior research on
concept-based interpretability has concentrated solely on image-level tasks.
Comparatively, video models deal with the added temporal dimension, increasing
complexity and posing challenges in identifying dynamic concepts over time. In
this work, we systematically address these challenges by introducing the first
Video Transformer Concept Discovery (VTCD) algorithm. To this end, we propose
an efficient approach for unsupervised identification of units of video
transformer representations - concepts, and ranking their importance to the
output of a model. The resulting concepts are highly interpretable, revealing
spatio-temporal reasoning mechanisms and object-centric representations in
unstructured video models. Performing this analysis jointly over a diverse set
of supervised and self-supervised representations, we discover that some of
these mechanism are universal in video transformers. Finally, we demonstrate
that VTCDcan be used to improve model performance for fine-grained tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kowal_M/0/1/0/all/0/1&quot;&gt;Matthew Kowal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dave_A/0/1/0/all/0/1&quot;&gt;Achal Dave&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ambrus_R/0/1/0/all/0/1&quot;&gt;Rares Ambrus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gaidon_A/0/1/0/all/0/1&quot;&gt;Adrien Gaidon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Derpanis_K/0/1/0/all/0/1&quot;&gt;Konstantinos G. Derpanis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tokmakov_P/0/1/0/all/0/1&quot;&gt;Pavel Tokmakov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10848">
<title>Source-Free and Image-Only Unsupervised Domain Adaptation for Category Level Object Pose Estimation. (arXiv:2401.10848v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10848</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of source-free unsupervised category-level pose
estimation from only RGB images to a target domain without any access to source
domain data or 3D annotations during adaptation. Collecting and annotating
real-world 3D data and corresponding images is laborious, expensive, yet
unavoidable process, since even 3D pose domain adaptation methods require 3D
data in the target domain. We introduce 3DUDA, a method capable of adapting to
a nuisance-ridden target domain without 3D or depth data. Our key insight stems
from the observation that specific object subparts remain stable across
out-of-domain (OOD) scenarios, enabling strategic utilization of these
invariant subcomponents for effective model updates. We represent object
categories as simple cuboid meshes, and harness a generative model of neural
feature activations modeled at each mesh vertex learnt using differential
rendering. We focus on individual locally robust mesh vertex features and
iteratively update them based on their proximity to corresponding features in
the target domain even when the global pose is not correct. Our model is then
trained in an EM fashion, alternating between updating the vertex features and
the feature extractor. We show that our method simulates fine-tuning on a
global pseudo-labeled dataset under mild assumptions, which converges to the
target domain asymptotically. Through extensive empirical validation, including
a complex extreme UDA setup which combines real nuisances, synthetic noise, and
occlusion, we demonstrate the potency of our simple approach in addressing the
domain shift challenge and significantly improving pose estimation accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaushik_P/0/1/0/all/0/1&quot;&gt;Prakhar Kaushik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mishra_A/0/1/0/all/0/1&quot;&gt;Aayush Mishra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kortylewski_A/0/1/0/all/0/1&quot;&gt;Adam Kortylewski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10857">
<title>Motion Consistency Loss for Monocular Visual Odometry with Attention-Based Deep Learning. (arXiv:2401.10857v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10857</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning algorithms have driven expressive progress in many complex
tasks. The loss function is a core component of deep learning techniques,
guiding the learning process of neural networks. This paper contributes by
introducing a consistency loss for visual odometry with deep learning-based
approaches. The motion consistency loss explores repeated motions that appear
in consecutive overlapped video clips. Experimental results show that our
approach increased the performance of a model on the KITTI odometry benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francani_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; O. Fran&amp;#xe7;ani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maximo_M/0/1/0/all/0/1&quot;&gt;Marcos R. O. A. Maximo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10877">
<title>The Cadaver in the Machine: The Social Practices of Measurement and Validation in Motion Capture Technology. (arXiv:2401.10877v1 [cs.CY])</title>
<link>http://arxiv.org/abs/2401.10877</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion capture systems, used across various domains, make body
representations concrete through technical processes. We argue that the
measurement of bodies and the validation of measurements for motion capture
systems can be understood as social practices. By analyzing the findings of a
systematic literature review (N=278) through the lens of social practice
theory, we show how these practices, and their varying attention to errors,
become ingrained in motion capture design and innovation over time. Moreover,
we show how contemporary motion capture systems perpetuate assumptions about
human bodies and their movements. We suggest that social practices of
measurement and validation are ubiquitous in the development of data- and
sensor-driven systems more broadly, and provide this work as a basis for
investigating hidden design assumptions and their potential negative
consequences in human-computer interaction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harvey_E/0/1/0/all/0/1&quot;&gt;Emma Harvey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sandhaus_H/0/1/0/all/0/1&quot;&gt;Hauke Sandhaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jacobs_A/0/1/0/all/0/1&quot;&gt;Abigail Z. Jacobs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moss_E/0/1/0/all/0/1&quot;&gt;Emanuel Moss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sloane_M/0/1/0/all/0/1&quot;&gt;Mona Sloane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10886">
<title>SCENES: Subpixel Correspondence Estimation With Epipolar Supervision. (arXiv:2401.10886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10886</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting point correspondences from two or more views of a scene is a
fundamental computer vision problem with particular importance for relative
camera pose estimation and structure-from-motion. Existing local feature
matching approaches, trained with correspondence supervision on large-scale
datasets, obtain highly-accurate matches on the test sets. However, they do not
generalise well to new datasets with different characteristics to those they
were trained on, unlike classic feature extractors. Instead, they require
finetuning, which assumes that ground-truth correspondences or ground-truth
camera poses and 3D structure are available. We relax this assumption by
removing the requirement of 3D structure, e.g., depth maps or point clouds, and
only require camera pose information, which can be obtained from odometry. We
do so by replacing correspondence losses with epipolar losses, which encourage
putative matches to lie on the associated epipolar line. While weaker than
correspondence supervision, we observe that this cue is sufficient for
finetuning existing models on new data. We then further relax the assumption of
known camera poses by using pose estimates in a novel bootstrapping approach.
We evaluate on highly challenging datasets, including an indoor drone dataset
and an outdoor smartphone camera dataset, and obtain state-of-the-art results
without strong supervision.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloepfer_D/0/1/0/all/0/1&quot;&gt;Dominik A. Kloepfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Henriques_J/0/1/0/all/0/1&quot;&gt;Jo&amp;#xe3;o F. Henriques&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Campbell_D/0/1/0/all/0/1&quot;&gt;Dylan Campbell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10889">
<title>Synthesizing Moving People with 3D Control. (arXiv:2401.10889v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10889</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a diffusion model-based framework for animating
people from a single image for a given target 3D motion sequence. Our approach
has two core components: a) learning priors about invisible parts of the human
body and clothing, and b) rendering novel body poses with proper clothing and
texture. For the first part, we learn an in-filling diffusion model to
hallucinate unseen parts of a person given a single image. We train this model
on texture map space, which makes it more sample-efficient since it is
invariant to pose and viewpoint. Second, we develop a diffusion-based rendering
pipeline, which is controlled by 3D human poses. This produces realistic
renderings of novel poses of the person, including clothing, hair, and
plausible in-filling of unseen regions. This disentangled approach allows our
method to generate a sequence of images that are faithful to the target motion
in the 3D pose and, to the input image in terms of visual similarity. In
addition to that, the 3D control allows various synthetic camera trajectories
to render a person. Our experiments show that our method is resilient in
generating prolonged motions and varied challenging and complex poses compared
to prior methods. Please check our website for more details:
https://boyiliee.github.io/3DHM.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Boyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rajasegaran_J/0/1/0/all/0/1&quot;&gt;Jathushan Rajasegaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandelsman_Y/0/1/0/all/0/1&quot;&gt;Yossi Gandelsman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_J/0/1/0/all/0/1&quot;&gt;Jitendra Malik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10890">
<title>Event detection from novel data sources: Leveraging satellite imagery alongside GPS traces. (arXiv:2401.10890v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10890</link>
<description rdf:parseType="Literal">&lt;p&gt;Rapid identification and response to breaking events, particularly those that
pose a threat to human life such as natural disasters or conflicts, is of
paramount importance. The prevalence of mobile devices and the ubiquity of
network connectivity has generated a massive amount of temporally- and
spatially-stamped data. Numerous studies have used mobile data to derive
individual human mobility patterns for various applications. Similarly, the
increasing number of orbital satellites has made it easier to gather
high-resolution images capturing a snapshot of a geographical area in sub-daily
temporal frequency. We propose a novel data fusion methodology integrating
satellite imagery with privacy-enhanced mobile data to augment the event
inference task, whether in real-time or historical. In the absence of boots on
the ground, mobile data is able to give an approximation of human mobility,
proximity to one another, and the built environment. On the other hand,
satellite imagery can provide visual information on physical changes to the
built and natural environment. The expected use cases for our methodology
include small-scale disaster detection (i.e., tornadoes, wildfires, and floods)
in rural regions, search and rescue operation augmentation for lost hikers in
remote wilderness areas, and identification of active conflict areas and
population displacement in war-torn states. Our implementation is open-source
on GitHub: https://github.com/ekinugurel/SatMobFusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ugurel_E/0/1/0/all/0/1&quot;&gt;Ekin Ugurel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coenen_S/0/1/0/all/0/1&quot;&gt;Steffen Coenen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Minda Zhou Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Cynthia Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10891">
<title>Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data. (arXiv:2401.10891v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.10891</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents Depth Anything, a highly practical solution for robust
monocular depth estimation. Without pursuing novel technical modules, we aim to
build a simple yet powerful foundation model dealing with any images under any
circumstances. To this end, we scale up the dataset by designing a data engine
to collect and automatically annotate large-scale unlabeled data (~62M), which
significantly enlarges the data coverage and thus is able to reduce the
generalization error. We investigate two simple yet effective strategies that
make data scaling-up promising. First, a more challenging optimization target
is created by leveraging data augmentation tools. It compels the model to
actively seek extra visual knowledge and acquire robust representations.
Second, an auxiliary supervision is developed to enforce the model to inherit
rich semantic priors from pre-trained encoders. We evaluate its zero-shot
capabilities extensively, including six public datasets and randomly captured
photos. It demonstrates impressive generalization ability. Further, through
fine-tuning it with metric depth information from NYUv2 and KITTI, new SOTAs
are set. Our better depth model also results in a better depth-conditioned
ControlNet. Our models are released at
https://github.com/LiheYoung/Depth-Anything.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lihe Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Bingyi Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zilong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaogang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Hengshuang Zhao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/1812.01243">
<title>Efficient Attention: Attention with Linear Complexities. (arXiv:1812.01243v10 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/1812.01243</link>
<description rdf:parseType="Literal">&lt;p&gt;Dot-product attention has wide applications in computer vision and natural
language processing. However, its memory and computational costs grow
quadratically with the input size. Such growth prohibits its application on
high-resolution inputs. To remedy this drawback, this paper proposes a novel
efficient attention mechanism equivalent to dot-product attention but with
substantially less memory and computational costs. Its resource efficiency
allows more widespread and flexible integration of attention modules into a
network, which leads to better accuracies. Empirical evaluations demonstrated
the effectiveness of its advantages. Efficient attention modules brought
significant performance boosts to object detectors and instance segmenters on
MS-COCO 2017. Further, the resource efficiency democratizes attention to
complex models, where high costs prohibit the use of dot-product attention. As
an exemplar, a model with efficient attention achieved state-of-the-art
accuracies for stereo depth estimation on the Scene Flow dataset. Code is
available at https://github.com/cmsflash/efficient-attention.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Z/0/1/0/all/0/1&quot;&gt;Zhuoran Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_S/0/1/0/all/0/1&quot;&gt;Shuai Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongsheng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.10702">
<title>ClawCraneNet: Leveraging Object-level Relation for Text-based Video Segmentation. (arXiv:2103.10702v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.10702</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-based video segmentation is a challenging task that segments out the
natural language referred objects in videos. It essentially requires semantic
comprehension and fine-grained video understanding. Existing methods introduce
language representation into segmentation models in a bottom-up manner, which
merely conducts vision-language interaction within local receptive fields of
ConvNets. We argue that such interaction is not fulfilled since the model can
barely construct region-level relationships given partial observations, which
is contrary to the description logic of natural language/referring expressions.
In fact, people usually describe a target object using relations with other
objects, which may not be easily understood without seeing the whole video. To
address the issue, we introduce a novel top-down approach by imitating how we
human segment an object with the language guidance. We first figure out all
candidate objects in videos and then choose the refereed one by parsing
relations among those high-level objects. Three kinds of object-level relations
are investigated for precise relationship understanding, i.e., positional
relation, text-guided semantic relation, and temporal relation. Extensive
experiments on A2D Sentences and J-HMDB Sentences show our method outperforms
state-of-the-art methods by a large margin. Qualitative results also show our
results are more explainable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yawei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2106.01061">
<title>Rethinking Cross-modal Interaction from a Top-down Perspective for Referring Video Object Segmentation. (arXiv:2106.01061v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2106.01061</link>
<description rdf:parseType="Literal">&lt;p&gt;Referring video object segmentation (RVOS) aims to segment video objects with
the guidance of natural language reference. Previous methods typically tackle
RVOS through directly grounding linguistic reference over the image lattice.
Such bottom-up strategy fails to explore object-level cues, easily leading to
inferior results. In this work, we instead put forward a two-stage, top-down
RVOS solution. First, an exhaustive set of object tracklets is constructed by
propagating object masks detected from several sampled frames to the entire
video. Second, a Transformer-based tracklet-language grounding module is
proposed, which models instance-level visual relations and cross-modal
interactions simultaneously and efficiently. Our model ranks first place on
CVPR2021 Referring Youtube-VOS challenge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianfei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zongxin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Y/0/1/0/all/0/1&quot;&gt;Yunchao Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.09773">
<title>Local-Global Context Aware Transformer for Language-Guided Video Segmentation. (arXiv:2203.09773v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.09773</link>
<description rdf:parseType="Literal">&lt;p&gt;We explore the task of language-guided video segmentation (LVS). Previous
algorithms mostly adopt 3D CNNs to learn video representation, struggling to
capture long-term context and easily suffering from visual-linguistic
misalignment. In light of this, we present Locater (local-global context aware
Transformer), which augments the Transformer architecture with a finite memory
so as to query the entire video with the language expression in an efficient
manner. The memory is designed to involve two components -- one for
persistently preserving global video content, and one for dynamically gathering
local temporal context and segmentation history. Based on the memorized
local-global context and the particular content of each frame, Locater
holistically and flexibly comprehends the expression as an adaptive query
vector for each frame. The vector is used to query the corresponding frame for
mask generation. The memory also allows Locater to process videos with linear
time complexity and constant size memory, while Transformer-style
self-attention computation scales quadratically with sequence length. To
thoroughly examine the visual grounding capability of LVS models, we contribute
a new LVS dataset, A2D-S+, which is built upon A2D-S dataset but poses
increased challenges in disambiguating among similar objects. Experiments on
three LVS datasets and our A2D-S+ show that Locater outperforms previous
state-of-the-arts. Further, we won the 1st place in the Referring Video Object
Segmentation Track of the 3rd Large-scale Video Object Segmentation Challenge,
where Locater served as the foundation for the winning solution. Our code and
dataset are available at: https://github.com/leonnnop/Locater
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_C/0/1/0/all/0/1&quot;&gt;Chen Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenguan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Tianfei Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miao_J/0/1/0/all/0/1&quot;&gt;Jiaxu Miao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yawei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yi Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16031">
<title>How Deep is Your Art: An Experimental Study on the Limits of Artistic Understanding in a Single-Task, Single-Modality Neural Network. (arXiv:2203.16031v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16031</link>
<description rdf:parseType="Literal">&lt;p&gt;Computational modeling of artwork meaning is complex and difficult. This is
because art interpretation is multidimensional and highly subjective. This
paper experimentally investigated the degree to which a state-of-the-art Deep
Convolutional Neural Network (DCNN), a popular Machine Learning approach, can
correctly distinguish modern conceptual art work into the galleries devised by
art curators. Two hypotheses were proposed to state that the DCNN model uses
Exhibited Properties for classification, like shape and color, but not
Non-Exhibited Properties, such as historical context and artist intention. The
two hypotheses were experimentally validated using a methodology designed for
this purpose. VGG-11 DCNN pre-trained on ImageNet dataset and discriminatively
fine-tuned was trained on handcrafted datasets designed from real-world
conceptual photography galleries. Experimental results supported the two
hypotheses showing that the DCNN model ignores Non-Exhibited Properties and
uses only Exhibited Properties for artwork classification. This work points to
current DCNN limitations, which should be addressed by future DNN models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zahedi_M/0/1/0/all/0/1&quot;&gt;Mahan Agha Zahedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gholamrezaei_N/0/1/0/all/0/1&quot;&gt;Niloofar Gholamrezaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doboli_A/0/1/0/all/0/1&quot;&gt;Alex Doboli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.06551">
<title>Exploiting Multiple Sequence Lengths in Fast End to End Training for Image Captioning. (arXiv:2208.06551v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.06551</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method called the Expansion mechanism that processes the input
unconstrained by the number of elements in the sequence. By doing so, the model
can learn more effectively compared to traditional attention-based approaches.
To support this claim, we design a novel architecture ExpansionNet v2 that
achieved strong results on the MS COCO 2014 Image Captioning challenge and the
State of the Art in its respective category, with a score of 143.7 CIDErD in
the offline test split, 140.8 CIDErD in the online evaluation server and 72.9
AllCIDEr on the nocaps validation set. Additionally, we introduce an End to End
training algorithm up to 2.8 times faster than established alternatives. Source
code available at: https://github.com/jchenghu/ExpansionNet_v2
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_J/0/1/0/all/0/1&quot;&gt;Jia Cheng Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cavicchioli_R/0/1/0/all/0/1&quot;&gt;Roberto Cavicchioli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Capotondi_A/0/1/0/all/0/1&quot;&gt;Alessandro Capotondi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.09424">
<title>Hierarchical Compositional Representations for Few-shot Action Recognition. (arXiv:2208.09424v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.09424</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently action recognition has received more and more attention for its
comprehensive and practical applications in intelligent surveillance and
human-computer interaction. However, few-shot action recognition has not been
well explored and remains challenging because of data scarcity. In this paper,
we propose a novel hierarchical compositional representations (HCR) learning
approach for few-shot action recognition. Specifically, we divide a complicated
action into several sub-actions by carefully designed hierarchical clustering
and further decompose the sub-actions into more fine-grained spatially
attentional sub-actions (SAS-actions). Although there exist large differences
between base classes and novel classes, they can share similar patterns in
sub-actions or SAS-actions. Furthermore, we adopt the Earth Mover&apos;s Distance in
the transportation problem to measure the similarity between video samples in
terms of sub-action representations. It computes the optimal matching flows
between sub-actions as distance metric, which is favorable for comparing
fine-grained patterns. Extensive experiments show our method achieves the
state-of-the-art results on HMDB51, UCF101 and Kinetics datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changzhen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shuzhe Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_X/0/1/0/all/0/1&quot;&gt;Xin Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1&quot;&gt;Shiguang Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.11795">
<title>PoseScript: Linking 3D Human Poses and Natural Language. (arXiv:2210.11795v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.11795</link>
<description rdf:parseType="Literal">&lt;p&gt;Natural language plays a critical role in many computer vision applications,
such as image captioning, visual question answering, and cross-modal retrieval,
to provide fine-grained semantic information. Unfortunately, while human pose
is key to human understanding, current 3D human pose datasets lack detailed
language descriptions. To address this issue, we have introduced the PoseScript
dataset. This dataset pairs more than six thousand 3D human poses from AMASS
with rich human-annotated descriptions of the body parts and their spatial
relationships. Additionally, to increase the size of the dataset to a scale
that is compatible with data-hungry learning algorithms, we have proposed an
elaborate captioning process that generates automatic synthetic descriptions in
natural language from given 3D keypoints. This process extracts low-level pose
information, known as &quot;posecodes&quot;, using a set of simple but generic rules on
the 3D keypoints. These posecodes are then combined into higher level textual
descriptions using syntactic rules. With automatic annotations, the amount of
available data significantly scales up (100k), making it possible to
effectively pretrain deep models for finetuning on human captions. To showcase
the potential of annotated poses, we present three multi-modal learning tasks
that utilize the PoseScript dataset. Firstly, we develop a pipeline that maps
3D poses and textual descriptions into a joint embedding space, allowing for
cross-modal retrieval of relevant poses from large-scale datasets. Secondly, we
establish a baseline for a text-conditioned model generating 3D poses. Thirdly,
we present a learned process for generating pose descriptions. These
applications demonstrate the versatility and usefulness of annotated poses in
various tasks and pave the way for future research in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Delmas_G/0/1/0/all/0/1&quot;&gt;Ginger Delmas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weinzaepfel_P/0/1/0/all/0/1&quot;&gt;Philippe Weinzaepfel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lucas_T/0/1/0/all/0/1&quot;&gt;Thomas Lucas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moreno_Noguer_F/0/1/0/all/0/1&quot;&gt;Francesc Moreno-Noguer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rogez_G/0/1/0/all/0/1&quot;&gt;Gr&amp;#xe9;gory Rogez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.08044">
<title>Benchmarking Robustness of Multimodal Image-Text Models under Distribution Shift. (arXiv:2212.08044v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.08044</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal image-text models have shown remarkable performance in the past
few years. However, evaluating robustness against distribution shifts is
crucial before adopting them in real-world applications. In this work, we
investigate the robustness of 12 popular open-sourced image-text models under
common perturbations on five tasks (image-text retrieval, visual reasoning,
visual entailment, image captioning, and text-to-image generation). In
particular, we propose several new multimodal robustness benchmarks by applying
17 image perturbation and 16 text perturbation techniques on top of existing
datasets. We observe that multimodal models are not robust to image and text
perturbations, especially to image perturbations. Among the tested perturbation
methods, character-level perturbations constitute the most severe distribution
shift for text, and zoom blur is the most severe shift for image data. We also
introduce two new robustness metrics (\textbf{MMI} for MultiModal Impact score
and \textbf{MOR} for Missing Object Rate) for proper evaluations of multimodal
models. We hope our extensive study sheds light on new directions for the
development of robust multimodal models. More details can be found on the
project webpage: \url{https://MMRobustness.github.io}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_J/0/1/0/all/0/1&quot;&gt;Jielin Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_X/0/1/0/all/0/1&quot;&gt;Xingjian Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wenzel_F/0/1/0/all/0/1&quot;&gt;Florian Wenzel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Ding Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Mu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.10766">
<title>On the Adversarial Robustness of Camera-based 3D Object Detection. (arXiv:2301.10766v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.10766</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, camera-based 3D object detection has gained widespread
attention for its ability to achieve high performance with low computational
cost. However, the robustness of these methods to adversarial attacks has not
been thoroughly examined, especially when considering their deployment in
safety-critical domains like autonomous driving. In this study, we conduct the
first comprehensive investigation of the robustness of leading camera-based 3D
object detection approaches under various adversarial conditions. We
systematically analyze the resilience of these models under two attack
settings: white-box and black-box; focusing on two primary objectives:
classification and localization. Additionally, we delve into two types of
adversarial attack techniques: pixel-based and patch-based. Our experiments
yield four interesting findings: (a) bird&apos;s-eye-view-based representations
exhibit stronger robustness against localization attacks; (b)
depth-estimation-free approaches have the potential to show stronger
robustness; (c) accurate depth estimation effectively improves robustness for
depth-estimation-based methods; (d) incorporating multi-frame benign inputs can
effectively mitigate adversarial attacks. We hope our findings can steer the
development of future camera-based object detection models with enhanced
adversarial robustness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_S/0/1/0/all/0/1&quot;&gt;Shaoyuan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zichao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13359">
<title>IM-IAD: Industrial Image Anomaly Detection Benchmark in Manufacturing. (arXiv:2301.13359v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13359</link>
<description rdf:parseType="Literal">&lt;p&gt;Image anomaly detection (IAD) is an emerging and vital computer vision task
in industrial manufacturing (IM). Recently, many advanced algorithms have been
reported, but their performance deviates considerably with various IM settings.
We realize that the lack of a uniform IM benchmark is hindering the development
and usage of IAD methods in real-world applications. In addition, it is
difficult for researchers to analyze IAD algorithms without a uniform
benchmark. To solve this problem, we propose a uniform IM benchmark, for the
first time, to assess how well these algorithms perform, which includes various
levels of supervision (unsupervised versus fully supervised), learning
paradigms (few-shot, continual and noisy label), and efficiency (memory usage
and inference speed). Then, we construct a comprehensive image anomaly
detection benchmark (IM-IAD), which includes 19 algorithms on seven major
datasets with a uniform setting. Extensive experiments (17,017 total) on IM-IAD
provide in-depth insights into IAD algorithm redesign or selection. Moreover,
the proposed IM-IAD benchmark challenges existing algorithms and suggests
future research directions. To foster reproducibility and accessibility, the
source code of IM-IAD is uploaded on the website,
https://github.com/M-3LAB/IM-IAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_G/0/1/0/all/0/1&quot;&gt;Guoyang Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinbao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lyu_J/0/1/0/all/0/1&quot;&gt;Jiayi Lyu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chengjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_F/0/1/0/all/0/1&quot;&gt;Feng Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yaochu Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.02506">
<title>Prismer: A Vision-Language Model with Multi-Task Experts. (arXiv:2303.02506v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.02506</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent vision-language models have shown impressive multi-modal generation
capabilities. However, typically they require training huge models on massive
datasets. As a more scalable alternative, we introduce Prismer, a data- and
parameter-efficient vision-language model that leverages an ensemble of
task-specific experts. Prismer only requires training of a small number of
components, with the majority of network weights inherited from multiple
readily-available, pre-trained experts, and kept frozen during training. By
leveraging experts from a wide range of domains, we show Prismer can
efficiently pool this expert knowledge and adapt it to various vision-language
reasoning tasks. In our experiments, we show that Prismer achieves fine-tuned
and few-shot learning performance which is competitive with current
state-of-the-arts, whilst requiring up to two orders of magnitude less training
data. Code is available at https://github.com/NVlabs/prismer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shikun Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Linxi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Johns_E/0/1/0/all/0/1&quot;&gt;Edward Johns&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhiding Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_C/0/1/0/all/0/1&quot;&gt;Chaowei Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anandkumar_A/0/1/0/all/0/1&quot;&gt;Anima Anandkumar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.05015">
<title>Smooth and Stepwise Self-Distillation for Object Detection. (arXiv:2303.05015v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.05015</link>
<description rdf:parseType="Literal">&lt;p&gt;Distilling the structured information captured in feature maps has
contributed to improved results for object detection tasks, but requires
careful selection of baseline architectures and substantial pre-training.
Self-distillation addresses these limitations and has recently achieved
state-of-the-art performance for object detection despite making several
simplifying architectural assumptions. Building on this work, we propose Smooth
and Stepwise Self-Distillation (SSSD) for object detection. Our SSSD
architecture forms an implicit teacher from object labels and a feature pyramid
network backbone to distill label-annotated feature maps using Jensen-Shannon
distance, which is smoother than distillation losses used in prior work. We
additionally add a distillation coefficient that is adaptively configured based
on the learning rate. We extensively benchmark SSSD against a baseline and two
state-of-the-art object detector architectures on the COCO dataset by varying
the coefficients and backbone and detector networks. We demonstrate that SSSD
achieves higher average precision in most experimental settings, is robust to a
wide range of coefficients, and benefits from our stepwise distillation
procedure.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jieren Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhihong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aguiar_D/0/1/0/all/0/1&quot;&gt;Derek Aguiar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06088">
<title>Towards domain-invariant Self-Supervised Learning with Batch Styles Standardization. (arXiv:2303.06088v6 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06088</link>
<description rdf:parseType="Literal">&lt;p&gt;In Self-Supervised Learning (SSL), models are typically pretrained,
fine-tuned, and evaluated on the same domains. However, they tend to perform
poorly when evaluated on unseen domains, a challenge that Unsupervised Domain
Generalization (UDG) seeks to address. Current UDG methods rely on domain
labels, which are often challenging to collect, and domain-specific
architectures that lack scalability when confronted with numerous domains,
making the current methodology impractical and rigid. Inspired by
contrastive-based UDG methods that mitigate spurious correlations by
restricting comparisons to examples from the same domain, we hypothesize that
eliminating style variability within a batch could provide a more convenient
and flexible way to reduce spurious correlations without requiring domain
labels. To verify this hypothesis, we introduce Batch Styles Standardization
(BSS), a relatively simple yet powerful Fourier-based method to standardize the
style of images in a batch specifically designed for integration with SSL
methods to tackle UDG. Combining BSS with existing SSL methods offers serious
advantages over prior UDG methods: (1) It eliminates the need for domain labels
or domain-specific network components to enhance domain-invariance in SSL
representations, and (2) offers flexibility as BSS can be seamlessly integrated
with diverse contrastive-based but also non-contrastive-based SSL methods.
Experiments on several UDG datasets demonstrate that it significantly improves
downstream task performances on unseen domains, often outperforming or rivaling
with UDG methods. Finally, this work clarifies the underlying mechanisms
contributing to BSS&apos;s effectiveness in improving domain-invariance in SSL
representations and performances on unseen domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scalbert_M/0/1/0/all/0/1&quot;&gt;Marin Scalbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakalopoulou_M/0/1/0/all/0/1&quot;&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Couzinie_Devy_F/0/1/0/all/0/1&quot;&gt;Florent Couzini&amp;#xe9;-Devy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.14501">
<title>Link Prediction for Flow-Driven Spatial Networks. (arXiv:2303.14501v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.14501</link>
<description rdf:parseType="Literal">&lt;p&gt;Link prediction algorithms aim to infer the existence of connections (or
links) between nodes in network-structured data and are typically applied to
refine the connectivity among nodes. In this work, we focus on link prediction
for flow-driven spatial networks, which are embedded in a Euclidean space and
relate to physical exchange and transportation processes (e.g., blood flow in
vessels or traffic flow in road networks). To this end, we propose the Graph
Attentive Vectors (GAV) link prediction framework. GAV models simplified
dynamics of physical flow in spatial networks via an attentive,
neighborhood-aware message-passing paradigm, updating vector embeddings in a
constrained manner. We evaluate GAV on eight flow-driven spatial networks given
by whole-brain vessel graphs and road networks. GAV demonstrates superior
performances across all datasets and metrics and outperformed the
state-of-the-art on the ogbl-vessel benchmark at the time of submission by 12%
(98.38 vs. 87.98 AUC). All code is publicly available on GitHub.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wittmann_B/0/1/0/all/0/1&quot;&gt;Bastian Wittmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Paetzold_J/0/1/0/all/0/1&quot;&gt;Johannes C. Paetzold&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakar_C/0/1/0/all/0/1&quot;&gt;Chinmay Prabhakar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menze_B/0/1/0/all/0/1&quot;&gt;Bjoern Menze&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.00746">
<title>OTS: A One-shot Learning Approach for Text Spotting in Historical Manuscripts. (arXiv:2304.00746v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.00746</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of historical manuscript research, scholars frequently encounter
novel symbols in ancient texts, investing considerable effort in their
identification and documentation. Although some object detection methods have
achieved impressive performance, they primarily excel at detecting categories
included in training datasets, often failing to recognize novel symbols without
retraining. To overcome this limitation, we propose a novel One-shot
learning-based Text Spotting (OTS) approach that accurately and reliably spots
novel characters with just one annotated support sample. Drawing inspiration
from cognitive research, we introduce a spatial alignment module that finds,
focuses on, and learns the most discriminative spatial regions in the query
image based on one support image. Especially, since the low-resource spotting
task often faces the problem of example imbalance, we propose a novel loss
function called torus loss which can make the embedding space of distance
metric more discriminative. Our approach is highly efficient and requires only
a few training samples while exhibiting the remarkable ability to handle novel
characters and symbols. To enhance dataset diversity, a new manuscript dataset
that contains the ancient Dongba hieroglyphics (DBH) is created, a script
associated with China and developed by the ancestors of the Naxi minority. We
conduct experiments on publicly available DBH, EGY, VML-HD, TKH, and NC
datasets. The experimental results demonstrate that OTS outperforms the
state-of-the-art methods in one-shot text spotting. Overall, our proposed
method offers promising applications in text spotting in historical
manuscripts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wenbo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_H/0/1/0/all/0/1&quot;&gt;Hongjian Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Cong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Bing Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yue Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13310">
<title>Matcher: Segment Anything with One Shot Using All-Purpose Feature Matching. (arXiv:2305.13310v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13310</link>
<description rdf:parseType="Literal">&lt;p&gt;Powered by large-scale pre-training, vision foundation models exhibit
significant potential in open-world image understanding. However, unlike large
language models that excel at directly tackling various language tasks, vision
foundation models require a task-specific model structure followed by
fine-tuning on specific tasks. In this work, we present Matcher, a novel
perception paradigm that utilizes off-the-shelf vision foundation models to
address various perception tasks. Matcher can segment anything by using an
in-context example without training. Additionally, we design three effective
components within the Matcher framework to collaborate with these foundation
models and unleash their full potential in diverse perception tasks. Matcher
demonstrates impressive generalization performance across various segmentation
tasks, all without training. For example, it achieves 52.7% mIoU on COCO-20$^i$
with one example, surpassing the state-of-the-art specialist model by 1.6%. In
addition, Matcher achieves 33.0% mIoU on the proposed LVIS-92$^i$ for one-shot
semantic segmentation, outperforming the state-of-the-art generalist model by
14.4%. Our visualization results further showcase the open-world generality and
flexibility of Matcher when applied to images in the wild. Our code can be
found at https://github.com/aim-uofa/Matcher.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Muzhi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hengtao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xinlong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.08251">
<title>GBSD: Generative Bokeh with Stage Diffusion. (arXiv:2306.08251v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.08251</link>
<description rdf:parseType="Literal">&lt;p&gt;The bokeh effect is an artistic technique that blurs out-of-focus areas in a
photograph and has gained interest due to recent developments in text-to-image
synthesis and the ubiquity of smart-phone cameras and photo-sharing apps. Prior
work on rendering bokeh effects have focused on post hoc image manipulation to
produce similar blurring effects in existing photographs using classical
computer graphics or neural rendering techniques, but have either depth
discontinuity artifacts or are restricted to reproducing bokeh effects that are
present in the training data. More recent diffusion based models can synthesize
images with an artistic style, but either require the generation of
high-dimensional masks, expensive fine-tuning, or affect global image
characteristics. In this paper, we present GBSD, the first generative
text-to-image model that synthesizes photorealistic images with a bokeh style.
Motivated by how image synthesis occurs progressively in diffusion models, our
approach combines latent diffusion models with a 2-stage conditioning algorithm
to render bokeh effects on semantically defined objects. Since we can focus the
effect on objects, this semantic bokeh effect is more versatile than classical
rendering techniques. We evaluate GBSD both quantitatively and qualitatively
and demonstrate its ability to be applied in both text-to-image and
image-to-image settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_J/0/1/0/all/0/1&quot;&gt;Jieren Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_X/0/1/0/all/0/1&quot;&gt;Xin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_H/0/1/0/all/0/1&quot;&gt;Hao Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zhihong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aguiar_D/0/1/0/all/0/1&quot;&gt;Derek Aguiar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02119">
<title>Hierarchical Masked 3D Diffusion Model for Video Outpainting. (arXiv:2309.02119v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02119</link>
<description rdf:parseType="Literal">&lt;p&gt;Video outpainting aims to adequately complete missing areas at the edges of
video frames. Compared to image outpainting, it presents an additional
challenge as the model should maintain the temporal consistency of the filled
area. In this paper, we introduce a masked 3D diffusion model for video
outpainting. We use the technique of mask modeling to train the 3D diffusion
model. This allows us to use multiple guide frames to connect the results of
multiple video clip inferences, thus ensuring temporal consistency and reducing
jitter between adjacent frames. Meanwhile, we extract the global frames of the
video as prompts and guide the model to obtain information other than the
current video clip using cross-attention. We also introduce a hybrid
coarse-to-fine inference pipeline to alleviate the artifact accumulation
problem. The existing coarse-to-fine pipeline only uses the infilling strategy,
which brings degradation because the time interval of the sparse frames is too
large. Our pipeline benefits from bidirectional learning of the mask modeling
and thus can employ a hybrid strategy of infilling and interpolation when
generating sparse frames. Experiments show that our method achieves
state-of-the-art results in video outpainting tasks. More results and codes are
provided at our https://fanfanda.github.io/M3DDM/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fanda Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_C/0/1/0/all/0/1&quot;&gt;Chaoxu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_L/0/1/0/all/0/1&quot;&gt;Litong Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Biao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_T/0/1/0/all/0/1&quot;&gt;Tiezheng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yuning Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_C/0/1/0/all/0/1&quot;&gt;Chunjie Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_J/0/1/0/all/0/1&quot;&gt;Jianfeng Zhan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02773">
<title>Diffusion Model is Secretly a Training-free Open Vocabulary Semantic Segmenter. (arXiv:2309.02773v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02773</link>
<description rdf:parseType="Literal">&lt;p&gt;The pre-trained text-image discriminative models, such as CLIP, has been
explored for open-vocabulary semantic segmentation with unsatisfactory results
due to the loss of crucial localization information and awareness of object
shapes. Recently, there has been a growing interest in expanding the
application of generative models from generation tasks to semantic
segmentation. These approaches utilize generative models either for generating
annotated data or extracting features to facilitate semantic segmentation. This
typically involves generating a considerable amount of synthetic data or
requiring additional mask annotations. To this end, we uncover the potential of
generative text-to-image diffusion models (e.g., Stable Diffusion) as highly
efficient open-vocabulary semantic segmenters, and introduce a novel
training-free approach named DiffSegmenter. The insight is that to generate
realistic objects that are semantically faithful to the input text, both the
complete object shapes and the corresponding semantics are implicitly learned
by diffusion models. We discover that the object shapes are characterized by
the self-attention maps while the semantics are indicated through the
cross-attention maps produced by the denoising U-Net, forming the basis of our
segmentation results.Additionally, we carefully design effective textual
prompts and a category filtering mechanism to further enhance the segmentation
results. Extensive experiments on three benchmark datasets show that the
proposed DiffSegmenter achieves impressive results for open-vocabulary semantic
segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinglong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qingyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qian Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_L/0/1/0/all/0/1&quot;&gt;Lu Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dong Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06023">
<title>Learning from History: Task-agnostic Model Contrastive Learning for Image Restoration. (arXiv:2309.06023v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06023</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has emerged as a prevailing paradigm for high-level
vision tasks, which, by introducing properly negative samples, has also been
exploited for low-level vision tasks to achieve a compact optimization space to
account for their ill-posed nature. However, existing methods rely on manually
predefined and task-oriented negatives, which often exhibit pronounced
task-specific biases. To address this challenge, our paper introduces an
innovative method termed &apos;learning from history&apos;, which dynamically generates
negative samples from the target model itself. Our approach, named Model
Contrastive paradigm for Image Restoration (MCIR), rejuvenates latency models
as negative models, making it compatible with diverse image restoration tasks.
We propose the Self-Prior guided Negative loss (SPN) to enable it. This
approach significantly enhances existing models when retrained with the
proposed model contrastive paradigm. The results show significant improvements
in image restoration across various tasks and architectures. For example,
models retrained with SPN outperform the original FFANet and DehazeFormer by
3.41 dB and 0.57 dB on the RESIDE indoor dataset for image dehazing. Similarly,
they achieve notable improvements of 0.47 dB on SPA-Data over IDT for image
deraining and 0.12 dB on Manga109 for a 4x scale super-resolution over
lightweight SwinIR, respectively. Code and retrained models are available at
https://github.com/Aitical/MCIR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_G/0/1/0/all/0/1&quot;&gt;Gang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_J/0/1/0/all/0/1&quot;&gt;Junjun Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kui Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xianming Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.09466">
<title>Progressive Text-to-Image Diffusion with Soft Latent Direction. (arXiv:2309.09466v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.09466</link>
<description rdf:parseType="Literal">&lt;p&gt;In spite of the rapidly evolving landscape of text-to-image generation, the
synthesis and manipulation of multiple entities while adhering to specific
relational constraints pose enduring challenges. This paper introduces an
innovative progressive synthesis and editing operation that systematically
incorporates entities into the target image, ensuring their adherence to
spatial and relational constraints at each sequential step. Our key insight
stems from the observation that while a pre-trained text-to-image diffusion
model adeptly handles one or two entities, it often falters when dealing with a
greater number. To address this limitation, we propose harnessing the
capabilities of a Large Language Model (LLM) to decompose intricate and
protracted text descriptions into coherent directives adhering to stringent
formats. To facilitate the execution of directives involving distinct semantic
operations-namely insertion, editing, and erasing-we formulate the Stimulus,
Response, and Fusion (SRF) framework. Within this framework, latent regions are
gently stimulated in alignment with each operation, followed by the fusion of
the responsive latent components to achieve cohesive entity manipulation. Our
proposed framework yields notable advancements in object synthesis,
particularly when confronted with intricate and lengthy textual inputs.
Consequently, it establishes a new benchmark for text-to-image generation
tasks, further elevating the field&apos;s performance standards.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;YuTeng Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jiale Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Hang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanwen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Youjia Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zikai Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_C/0/1/0/all/0/1&quot;&gt;Chenxing Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Junqing Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wei Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.14197">
<title>Diffusion-based Data Augmentation for Nuclei Image Segmentation. (arXiv:2310.14197v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.14197</link>
<description rdf:parseType="Literal">&lt;p&gt;Nuclei segmentation is a fundamental but challenging task in the quantitative
analysis of histopathology images. Although fully-supervised deep
learning-based methods have made significant progress, a large number of
labeled images are required to achieve great segmentation performance.
Considering that manually labeling all nuclei instances for a dataset is
inefficient, obtaining a large-scale human-annotated dataset is time-consuming
and labor-intensive. Therefore, augmenting a dataset with only a few labeled
images to improve the segmentation performance is of significant research and
application value. In this paper, we introduce the first diffusion-based
augmentation method for nuclei segmentation. The idea is to synthesize a large
number of labeled images to facilitate training the segmentation model. To
achieve this, we propose a two-step strategy. In the first step, we train an
unconditional diffusion model to synthesize the Nuclei Structure that is
defined as the representation of pixel-level semantic and distance transform.
Each synthetic nuclei structure will serve as a constraint on histopathology
image synthesis and is further post-processed to be an instance map. In the
second step, we train a conditioned diffusion model to synthesize
histopathology images based on nuclei structures. The synthetic histopathology
images paired with synthetic instance maps will be added to the real dataset
for training the segmentation model. The experimental results show that by
augmenting 10% labeled real dataset with synthetic samples, one can achieve
comparable segmentation results with the fully-supervised baseline. The code is
released in: https://github.com/lhaof/Nudiff
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xinyi Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lou_W/0/1/0/all/0/1&quot;&gt;Wei Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Siqi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wan_X/0/1/0/all/0/1&quot;&gt;Xiang Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haofeng Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18999">
<title>DynPoint: Dynamic Neural Point For View Synthesis. (arXiv:2310.18999v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18999</link>
<description rdf:parseType="Literal">&lt;p&gt;The introduction of neural radiance fields has greatly improved the
effectiveness of view synthesis for monocular videos. However, existing
algorithms face difficulties when dealing with uncontrolled or lengthy
scenarios, and require extensive training time specific to each new scenario.
To tackle these limitations, we propose DynPoint, an algorithm designed to
facilitate the rapid synthesis of novel views for unconstrained monocular
videos. Rather than encoding the entirety of the scenario information into a
latent representation, DynPoint concentrates on predicting the explicit 3D
correspondence between neighboring frames to realize information aggregation.
Specifically, this correspondence prediction is achieved through the estimation
of consistent depth and scene flow information across frames. Subsequently, the
acquired correspondence is utilized to aggregate information from multiple
reference frames to a target frame, by constructing hierarchical neural point
clouds. The resulting framework enables swift and accurate view synthesis for
desired views of target frames. The experimental results obtained demonstrate
the considerable acceleration of training time achieved - typically an order of
magnitude - by our proposed method while yielding comparable outcomes compared
to prior approaches. Furthermore, our method exhibits strong robustness in
handling long-duration videos without learning a canonical representation of
video content.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_K/0/1/0/all/0/1&quot;&gt;Kaichen Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_J/0/1/0/all/0/1&quot;&gt;Jia-Xing Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Sangyun Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_K/0/1/0/all/0/1&quot;&gt;Kai Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yiyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Markham_A/0/1/0/all/0/1&quot;&gt;Andrew Markham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trigoni_N/0/1/0/all/0/1&quot;&gt;Niki Trigoni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.20685">
<title>NeRF Revisited: Fixing Quadrature Instability in Volume Rendering. (arXiv:2310.20685v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.20685</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRF) rely on volume rendering to synthesize novel
views. Volume rendering requires evaluating an integral along each ray, which
is numerically approximated with a finite sum that corresponds to the exact
integral along the ray under piecewise constant volume density. As a
consequence, the rendered result is unstable w.r.t. the choice of samples along
the ray, a phenomenon that we dub quadrature instability. We propose a
mathematically principled solution by reformulating the sample-based rendering
equation so that it corresponds to the exact integral under piecewise linear
volume density. This simultaneously resolves multiple issues: conflicts between
samples along different rays, imprecise hierarchical sampling, and
non-differentiability of quantiles of ray termination distances w.r.t. model
parameters. We demonstrate several benefits over the classical sample-based
rendering equation, such as sharper textures, better geometric reconstruction,
and stronger depth supervision. Our proposed formulation can be also be used as
a drop-in replacement to the volume rendering equation of existing NeRF-based
methods. Our project page can be found at pl-nerf.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uy_M/0/1/0/all/0/1&quot;&gt;Mikaela Angelina Uy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nakayama_K/0/1/0/all/0/1&quot;&gt;Kiyohiro Nakayama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guandao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomas_R/0/1/0/all/0/1&quot;&gt;Rahul Krishna Thomas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas Guibas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Ke Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15420">
<title>Data-Driven Modelling for Harmonic Current Emission in Low-Voltage Grid Using MCReSANet with Interpretability Analysis. (arXiv:2311.15420v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15420</link>
<description rdf:parseType="Literal">&lt;p&gt;Even though the use of power electronics PE loads offers enhanced electrical
energy conversion efficiency and control, they remain the primary sources of
harmonics in grids. When diverse loads are connected in the distribution
system, their interactions complicate establishing analytical models for the
relationship between harmonic voltages and currents. To solve this, our paper
presents a data-driven model using MCReSANet to construct the highly nonlinear
between harmonic voltage and current. Two datasets from PCCs in Finland and
Germany are utilized, which demonstrates that MCReSANet is capable of
establishing accurate nonlinear mappings, even in the presence of various
network characteristics for selected Finland and Germany datasets. The model
built by MCReSANet can improve the MAE by 10% and 14% compared to the CNN, and
by 8% and 17% compared to the MLP for both Finnish and German datasets, also
showing much lower model uncertainty than others. This is a crucial
prerequisite for more precise SHAP value-based feature importance analysis,
which is a method for the model interpretability analysis in this paper. The
results by feature importance analysis show the detailed relationships between
each order of harmonic voltage and current in the distribution system. There is
an interactive impact on each order of harmonic current, but some orders of
harmonic voltages have a dominant influence on harmonic current emissions:
positive sequence and zero sequence harmonics have the dominant importance in
the Finnish and German networks, respectively, which conforms to the pattern of
connected load types in two selected Finnish and German datasets. This paper
enhances the potential for understanding and predicting harmonic current
emissions by diverse PE loads in distribution systems, which is beneficial to
more effective management for optimizing power quality in diverse grid
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yao_J/0/1/0/all/0/1&quot;&gt;Jieyu Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Judge_P/0/1/0/all/0/1&quot;&gt;Paul Judge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jia_J/0/1/0/all/0/1&quot;&gt;Jiabin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Djokic_S/0/1/0/all/0/1&quot;&gt;Sasa Djokic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Puvi_V/0/1/0/all/0/1&quot;&gt;Verner P&amp;#xfc;vi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lehtonen_M/0/1/0/all/0/1&quot;&gt;Matti Lehtonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meyer_J/0/1/0/all/0/1&quot;&gt;Jan Meyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15497">
<title>Adaptive Image Registration: A Hybrid Approach Integrating Deep Learning and Optimization Functions for Enhanced Precision. (arXiv:2311.15497v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15497</link>
<description rdf:parseType="Literal">&lt;p&gt;Image registration has traditionally been done using two distinct approaches:
learning based methods, relying on robust deep neural networks, and
optimization-based methods, applying complex mathematical transformations to
warp images accordingly. Of course, both paradigms offer advantages and
disadvantages, and, in this work, we seek to combine their respective strengths
into a single streamlined framework, using the outputs of the learning based
method as initial parameters for optimization while prioritizing computational
power for the image pairs that offer the greatest loss. Our investigations
showed improvements of up to 1.6% in test data, while maintaining the same
inference time, and a substantial 1.0% points performance gain in deformation
field smoothness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Araujo_G/0/1/0/all/0/1&quot;&gt;Gabriel De Araujo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_S/0/1/0/all/0/1&quot;&gt;Shanlin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xiaohui Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.00067">
<title>Predicting breast cancer with AI for individual risk-adjusted MRI screening and early detection. (arXiv:2312.00067v2 [physics.med-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2312.00067</link>
<description rdf:parseType="Literal">&lt;p&gt;Women with an increased life-time risk of breast cancer undergo supplemental
annual screening MRI. We propose to predict the risk of developing breast
cancer within one year based on the current MRI, with the objective of reducing
screening burden and facilitating early detection. An AI algorithm was
developed on 53,858 breasts from 12,694 patients who underwent screening or
diagnostic MRI and accrued over 12 years, with 2,331 confirmed cancers. A first
U-Net was trained to segment lesions and identify regions of concern. A second
convolutional network was trained to detect malignant cancer using features
extracted by the U-Net. This network was then fine-tuned to estimate the risk
of developing cancer within a year in cases that radiologists considered normal
or likely benign. Risk predictions from this AI were evaluated with a
retrospective analysis of 9,183 breasts from a high-risk screening cohort,
which were not used for training. Statistical analysis focused on the tradeoff
between number of omitted exams versus negative predictive value, and number of
potential early detections versus positive predictive value. The AI algorithm
identified regions of concern that coincided with future tumors in 52% of
screen-detected cancers. Upon directed review, a radiologist found that 71.3%
of cancers had a visible correlate on the MRI prior to diagnosis, 65% of these
correlates were identified by the AI model. Reevaluating these regions in 10%
of all cases with higher AI-predicted risk could have resulted in up to 33%
early detections by a radiologist. Additionally, screening burden could have
been reduced in 16% of lower-risk cases by recommending a later follow-up
without compromising current interval cancer rate. With increasing datasets and
improving image quality we expect this new AI-aided, adaptive screening to
meaningfully reduce screening burden and improve early detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hirsch_L/0/1/0/all/0/1&quot;&gt;Lukas Hirsch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Makse_H/0/1/0/all/0/1&quot;&gt;Hernan A. Makse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Martinez_D/0/1/0/all/0/1&quot;&gt;Danny F. Martinez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Hughes_M/0/1/0/all/0/1&quot;&gt;Mary Hughes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Eskreis_Winkler_S/0/1/0/all/0/1&quot;&gt;Sarah Eskreis-Winkler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Pinker_K/0/1/0/all/0/1&quot;&gt;Katja Pinker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Morris_E/0/1/0/all/0/1&quot;&gt;Elizabeth Morris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Parra_L/0/1/0/all/0/1&quot;&gt;Lucas C. Parra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Sutton_E/0/1/0/all/0/1&quot;&gt;Elizabeth J. Sutton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06946">
<title>WaterHE-NeRF: Water-ray Tracing Neural Radiance Fields for Underwater Scene Reconstruction. (arXiv:2312.06946v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06946</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) technology demonstrates immense potential in
novel viewpoint synthesis tasks, due to its physics-based volumetric rendering
process, which is particularly promising in underwater scenes. Addressing the
limitations of existing underwater NeRF methods in handling light attenuation
caused by the water medium and the lack of real Ground Truth (GT) supervision,
this study proposes WaterHE-NeRF. We develop a new water-ray tracing field by
Retinex theory that precisely encodes color, density, and illuminance
attenuation in three-dimensional space. WaterHE-NeRF, through its illuminance
attenuation mechanism, generates both degraded and clear multi-view images and
optimizes image restoration by combining reconstruction loss with Wasserstein
distance. Additionally, the use of histogram equalization (HE) as pseudo-GT
enhances the network&apos;s accuracy in preserving original details and color
distribution. Extensive experiments on real underwater datasets and synthetic
datasets validate the effectiveness of WaterHE-NeRF. Our code will be made
publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_T/0/1/0/all/0/1&quot;&gt;Tianyu Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dehuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zongxin He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06955">
<title>IA2U: A Transfer Plugin with Multi-Prior for In-Air Model to Underwater. (arXiv:2312.06955v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06955</link>
<description rdf:parseType="Literal">&lt;p&gt;In underwater environments, variations in suspended particle concentration
and turbidity cause severe image degradation, posing significant challenges to
image enhancement (IE) and object detection (OD) tasks. Currently, in-air image
enhancement and detection methods have made notable progress, but their
application in underwater conditions is limited due to the complexity and
variability of these environments. Fine-tuning in-air models saves high
overhead and has more optional reference work than building an underwater model
from scratch. To address these issues, we design a transfer plugin with
multiple priors for converting in-air models to underwater applications, named
IA2U. IA2U enables efficient application in underwater scenarios, thereby
improving performance in Underwater IE and OD. IA2U integrates three types of
underwater priors: the water type prior that characterizes the degree of image
degradation, such as color and visibility; the degradation prior, focusing on
differences in details and textures; and the sample prior, considering the
environmental conditions at the time of capture and the characteristics of the
photographed object. Utilizing a Transformer-like structure, IA2U employs these
priors as query conditions and a joint task loss function to achieve
hierarchical enhancement of task-level underwater image features, therefore
considering the requirements of two different tasks, IE and OD. Experimental
results show that IA2U combined with an in-air model can achieve superior
performance in underwater image enhancement and object detection tasks. The
code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gai_Q/0/1/0/all/0/1&quot;&gt;Qilin Gai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kin-man Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianping Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.06999">
<title>DGNet: Dynamic Gradient-guided Network with Noise Suppression for Underwater Image Enhancement. (arXiv:2312.06999v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.06999</link>
<description rdf:parseType="Literal">&lt;p&gt;Underwater image enhancement (UIE) is a challenging task due to the complex
degradation caused by underwater environments. To solve this issue, previous
methods often idealize the degradation process, and neglect the impact of
medium noise and object motion on the distribution of image features, limiting
the generalization and adaptability of the model. Previous methods use the
reference gradient that is constructed from original images and synthetic
ground-truth images. This may cause the network performance to be influenced by
some low-quality training data. Our approach utilizes predicted images to
dynamically update pseudo-labels, adding a dynamic gradient to optimize the
network&apos;s gradient space. This process improves image quality and avoids local
optima. Moreover, we propose a Feature Restoration and Reconstruction module
(FRR) based on a Channel Combination Inference (CCI) strategy and a Frequency
Domain Smoothing module (FRS). These modules decouple other degradation
features while reducing the impact of various types of noise on network
performance. Experiments on multiple public datasets demonstrate the
superiority of our method over existing state-of-the-art approaches, especially
in achieving performance milestones: PSNR of 25.6dB and SSIM of 0.93 on the
UIEB dataset. Its efficiency in terms of parameter size and inference time
further attests to its broad practicality. The code will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingchun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zongxin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;Dehuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kin-man Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_X/0/1/0/all/0/1&quot;&gt;Xianping Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.07823">
<title>Semantic Lens: Instance-Centric Semantic Alignment for Video Super-Resolution. (arXiv:2312.07823v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.07823</link>
<description rdf:parseType="Literal">&lt;p&gt;As a critical clue of video super-resolution (VSR), inter-frame alignment
significantly impacts overall performance. However, accurate pixel-level
alignment is a challenging task due to the intricate motion interweaving in the
video. In response to this issue, we introduce a novel paradigm for VSR named
Semantic Lens, predicated on semantic priors drawn from degraded videos.
Specifically, video is modeled as instances, events, and scenes via a Semantic
Extractor. Those semantics assist the Pixel Enhancer in understanding the
recovered contents and generating more realistic visual results. The distilled
global semantics embody the scene information of each frame, while the
instance-specific semantics assemble the spatial-temporal contexts related to
each instance. Furthermore, we devise a Semantics-Powered Attention
Cross-Embedding (SPACE) block to bridge the pixel-level features with semantic
knowledge, composed of a Global Perspective Shifter (GPS) and an
Instance-Specific Semantic Embedding Encoder (ISEE). Concretely, the GPS module
generates pairs of affine transformation parameters for pixel-level feature
modulation conditioned on global semantics. After that, the ISEE module
harnesses the attention mechanism to align the adjacent frames in the
instance-centric semantic space. In addition, we incorporate a simple yet
effective pre-alignment module to alleviate the difficulty of model training.
Extensive experiments demonstrate the superiority of our model over existing
state-of-the-art VSR methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_Q/0/1/0/all/0/1&quot;&gt;Qi Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yao Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Meiqin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_J/0/1/0/all/0/1&quot;&gt;Jian Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_C/0/1/0/all/0/1&quot;&gt;Chao Yao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.08010">
<title>EZ-CLIP: Efficient Zeroshot Video Action Recognition. (arXiv:2312.08010v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.08010</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large-scale pre-training of visual-language models on
paired image-text data have demonstrated impressive generalization capabilities
for zero-shot tasks. Building on this success, efforts have been made to adapt
these image-based visual-language models, such as CLIP, for videos extending
their zero-shot capabilities to the video domain. While these adaptations have
shown promising results, they come at a significant computational cost and
struggle with effectively modeling the crucial temporal aspects inherent to the
video domain. In this study, we present EZ-CLIP, a simple and efficient
adaptation of CLIP that addresses these challenges. EZ-CLIP leverages temporal
visual prompting for seamless temporal adaptation, requiring no fundamental
alterations to the core CLIP architecture while preserving its remarkable
generalization abilities. Moreover, we introduce a novel learning objective
that guides the temporal visual prompts to focus on capturing motion, thereby
enhancing its learning capabilities from video data. We conducted extensive
experiments on five different benchmark datasets, thoroughly evaluating EZ-CLIP
for zero-shot learning and base-to-novel video action recognition, and also
demonstrating its potential for few-shot generalization.Impressively, with a
mere 5.2 million learnable parameters (as opposed to the 71.1 million in the
prior best model), EZ-CLIP can be efficiently trained on a single GPU,
outperforming existing approaches in several evaluations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_S/0/1/0/all/0/1&quot;&gt;Shahzad Ahmad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chanda_S/0/1/0/all/0/1&quot;&gt;Sukalpa Chanda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1&quot;&gt;Yogesh S Rawat&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.12653">
<title>Diagnosis Of Takotsubo Syndrome By Robust Feature Selection From The Complex Latent Space Of DL-based Segmentation Network. (arXiv:2312.12653v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.12653</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers have shown significant correlations among segmented objects in
various medical imaging modalities and disease related pathologies. Several
studies showed that using hand crafted features for disease prediction neglects
the immense possibility to use latent features from deep learning (DL) models
which may reduce the overall accuracy of differential diagnosis. However,
directly using classification or segmentation models on medical to learn latent
features opt out robust feature selection and may lead to overfitting. To fill
this gap, we propose a novel feature selection technique using the latent space
of a segmentation model that can aid diagnosis. We evaluated our method in
differentiating a rare cardiac disease: Takotsubo Syndrome (TTS) from the ST
elevation myocardial infarction (STEMI) using echocardiogram videos (echo). TTS
can mimic clinical features of STEMI in echo and extremely hard to distinguish.
Our approach shows promising results in differential diagnosis of TTS with 82%
diagnosis accuracy beating the previous state-of-the-art (SOTA) approach.
Moreover, the robust feature selection technique using LASSO algorithm shows
great potential in reducing the redundant features and creates a robust
pipeline for short- and long-term disease prognoses in the downstream analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zaman_F/0/1/0/all/0/1&quot;&gt;Fahim Ahmed Zaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Alam_W/0/1/0/all/0/1&quot;&gt;Wahidul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Roy_T/0/1/0/all/0/1&quot;&gt;Tarun Kanti Roy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chang_A/0/1/0/all/0/1&quot;&gt;Amanda Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaodong Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16451">
<title>Domain Generalization with Vital Phase Augmentation. (arXiv:2312.16451v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16451</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have shown remarkable performance in image
classification. However, their performance significantly deteriorates with
corrupted input data. Domain generalization methods have been proposed to train
robust models against out-of-distribution data. Data augmentation in the
frequency domain is one of such approaches that enable a model to learn phase
features to establish domain-invariant representations. This approach changes
the amplitudes of the input data while preserving the phases. However, using
fixed phases leads to susceptibility to phase fluctuations because amplitudes
and phase fluctuations commonly occur in out-of-distribution. In this study, to
address this problem, we introduce an approach using finite variation of the
phases of input data rather than maintaining fixed phases. Based on the
assumption that the degree of domain-invariant features varies for each phase,
we propose a method to distinguish phases based on this degree. In addition, we
propose a method called vital phase augmentation (VIPAug) that applies the
variation to the phases differently according to the degree of domain-invariant
features of given phases. The model depends more on the vital phases that
contain more domain-invariant features for attaining robustness to amplitude
and phase fluctuations. We present experimental evaluations of our proposed
approach, which exhibited improved performance for both clean and corrupted
data. VIPAug achieved SOTA performance on the benchmark CIFAR-10 and CIFAR-100
datasets, as well as near-SOTA performance on the ImageNet-100 and ImageNet
datasets. Our code is available at https://github.com/excitedkid/vipaug.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_I/0/1/0/all/0/1&quot;&gt;Ingyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_W/0/1/0/all/0/1&quot;&gt;Wooju Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Myung_H/0/1/0/all/0/1&quot;&gt;Hyun Myung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.16516">
<title>ConstScene: Dataset and Model for Advancing Robust Semantic Segmentation in Construction Environments. (arXiv:2312.16516v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.16516</link>
<description rdf:parseType="Literal">&lt;p&gt;The increasing demand for autonomous machines in construction environments
necessitates the development of robust object detection algorithms that can
perform effectively across various weather and environmental conditions. This
paper introduces a new semantic segmentation dataset specifically tailored for
construction sites, taking into account the diverse challenges posed by adverse
weather and environmental conditions. The dataset is designed to enhance the
training and evaluation of object detection models, fostering their
adaptability and reliability in real-world construction applications. Our
dataset comprises annotated images captured under a wide range of different
weather conditions, including but not limited to sunny days, rainy periods,
foggy atmospheres, and low-light situations. Additionally, environmental
factors such as the existence of dirt/mud on the camera lens are integrated
into the dataset through actual captures and synthetic generation to simulate
the complex conditions prevalent in construction sites. We also generate
synthetic images of the annotations including precise semantic segmentation
masks for various objects commonly found in construction environments, such as
wheel loader machines, personnel, cars, and structural elements. To demonstrate
the dataset&apos;s utility, we evaluate state-of-the-art object detection algorithms
on our proposed benchmark. The results highlight the dataset&apos;s success in
adversarial training models across diverse conditions, showcasing its efficacy
compared to existing datasets that lack such environmental variability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salimi_M/0/1/0/all/0/1&quot;&gt;Maghsood Salimi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loni_M/0/1/0/all/0/1&quot;&gt;Mohammad Loni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Afshar_S/0/1/0/all/0/1&quot;&gt;Sara Afshar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cicchetti_A/0/1/0/all/0/1&quot;&gt;Antonio Cicchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sirjani_M/0/1/0/all/0/1&quot;&gt;Marjan Sirjani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00110">
<title>Diffusion Model with Perceptual Loss. (arXiv:2401.00110v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00110</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models trained with mean squared error loss tend to generate
unrealistic samples. Current state-of-the-art models rely on classifier-free
guidance to improve sample quality, yet its surprising effectiveness is not
fully understood. In this paper, we show that the effectiveness of
classifier-free guidance partly originates from it being a form of implicit
perceptual guidance. As a result, we can directly incorporate perceptual loss
in diffusion training to improve sample quality. Since the score matching
objective used in diffusion training strongly resembles the denoising
autoencoder objective used in unsupervised training of perceptual networks, the
diffusion model itself is a perceptual network and can be used to generate
meaningful perceptual loss. We propose a novel self-perceptual objective that
results in diffusion models capable of generating more realistic samples. For
conditional generation, our method only improves sample quality without
entanglement with the conditional input and therefore does not sacrifice sample
diversity. Our method can also improve sample quality for unconditional
generation, which was not possible with classifier-free guidance before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Shanchuan Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiao Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01984">
<title>AUPIMO: Redefining Visual Anomaly Detection Benchmarks with High Speed and Low Tolerance. (arXiv:2401.01984v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01984</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in visual anomaly detection research have seen AUROC and
AUPRO scores on public benchmark datasets such as MVTec and VisA converge
towards perfect recall, giving the impression that these benchmarks are
near-solved. However, high AUROC and AUPRO scores do not always reflect
qualitative performance, which limits the validity of these metrics in
real-world applications. We argue that the artificial ceiling imposed by the
lack of an adequate evaluation metric restrains progression of the field, and
it is crucial that we revisit the evaluation metrics used to rate our
algorithms. In response, we introduce Per-IMage Overlap (PIMO), a novel metric
that addresses the shortcomings of AUROC and AUPRO. PIMO retains the
recall-based nature of the existing metrics but introduces two distinctions:
the assignment of curves (and respective area under the curve) is per-image,
and its X-axis relies solely on normal images. Measuring recall per image
simplifies instance score indexing and is more robust to noisy annotations. As
we show, it also accelerates computation and enables the usage of statistical
tests to compare models. By imposing low tolerance for false positives on
normal images, PIMO provides an enhanced model validation procedure and
highlights performance variations across datasets. Our experiments demonstrate
that PIMO offers practical advantages and nuanced performance insights that
redefine anomaly detection benchmarks -- notably challenging the perception
that MVTec AD and VisA datasets have been solved by contemporary models.
Available on GitHub: https://github.com/jpcbertoldo/aupimo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bertoldo_J/0/1/0/all/0/1&quot;&gt;Joao P. C. Bertoldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ameln_D/0/1/0/all/0/1&quot;&gt;Dick Ameln&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaidya_A/0/1/0/all/0/1&quot;&gt;Ashwin Vaidya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akcay_S/0/1/0/all/0/1&quot;&gt;Samet Ak&amp;#xe7;ay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.05594">
<title>Wasserstein Distance-based Expansion of Low-Density Latent Regions for Unknown Class Detection. (arXiv:2401.05594v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.05594</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper addresses the significant challenge in open-set object detection
(OSOD): the tendency of state-of-the-art detectors to erroneously classify
unknown objects as known categories with high confidence. We present a novel
approach that effectively identifies unknown objects by distinguishing between
high and low-density regions in latent space. Our method builds upon the
Open-Det (OD) framework, introducing two new elements to the loss function.
These elements enhance the known embedding space&apos;s clustering and expand the
unknown space&apos;s low-density regions. The first addition is the Class
Wasserstein Anchor (CWA), a new function that refines the classification
boundaries. The second is a spectral normalisation step, improving the
robustness of the model. Together, these augmentations to the existing
Contrastive Feature Learner (CFL) and Unknown Probability Learner (UPL) loss
functions significantly improve OSOD performance. Our proposed OpenDet-CWA
(OD-CWA) method demonstrates: a) a reduction in open-set errors by
approximately 17%-22%, b) an enhancement in novelty detection capability by
1.5%-16%, and c) a decrease in the wilderness index by 2%-20% across various
open-set scenarios. These results represent a substantial advancement in the
field, showcasing the potential of our approach in managing the complexities of
open-set object detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mallick_P/0/1/0/all/0/1&quot;&gt;Prakash Mallick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayoub_F/0/1/0/all/0/1&quot;&gt;Feras Dayoub&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sherrah_J/0/1/0/all/0/1&quot;&gt;Jamie Sherrah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.07567">
<title>Bias-Conflict Sample Synthesis and Adversarial Removal Debias Strategy for Temporal Sentence Grounding in Video. (arXiv:2401.07567v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.07567</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Sentence Grounding in Video (TSGV) is troubled by dataset bias
issue, which is caused by the uneven temporal distribution of the target
moments for samples with similar semantic components in input videos or query
texts. Existing methods resort to utilizing prior knowledge about bias to
artificially break this uneven distribution, which only removes a limited
amount of significant language biases. In this work, we propose the
bias-conflict sample synthesis and adversarial removal debias strategy
(BSSARD), which dynamically generates bias-conflict samples by explicitly
leveraging potentially spurious correlations between single-modality features
and the temporal position of the target moments. Through adversarial training,
its bias generators continuously introduce biases and generate bias-conflict
samples to deceive its grounding model. Meanwhile, the grounding model
continuously eliminates the introduced biases, which requires it to model
multi-modality alignment information. BSSARD will cover most kinds of coupling
relationships and disrupt language and visual biases simultaneously. Extensive
experiments on Charades-CD and ActivityNet-CD demonstrate the promising
debiasing capability of BSSARD. Source codes are available at
https://github.com/qzhb/BSSARD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Z/0/1/0/all/0/1&quot;&gt;Zhaobo Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yibo Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruan_X/0/1/0/all/0/1&quot;&gt;Xiaowen Ruan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuhui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weigang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qingming Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09495">
<title>IPR-NeRF: Ownership Verification meets Neural Radiance Field. (arXiv:2401.09495v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09495</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural Radiance Field (NeRF) models have gained significant attention in the
computer vision community in the recent past with state-of-the-art visual
quality and produced impressive demonstrations. Since then, technopreneurs have
sought to leverage NeRF models into a profitable business. Therefore, NeRF
models make it worth the risk of plagiarizers illegally copying,
re-distributing, or misusing those models. This paper proposes a comprehensive
intellectual property (IP) protection framework for the NeRF model in both
black-box and white-box settings, namely IPR-NeRF. In the black-box setting, a
diffusion-based solution is introduced to embed and extract the watermark via a
two-stage optimization process. In the white-box setting, a designated digital
signature is embedded into the weights of the NeRF model by adopting the sign
loss objective. Our extensive experiments demonstrate that not only does our
approach maintain the fidelity (\ie, the rendering quality) of IPR-NeRF models,
but it is also robust against both ambiguity and removal attacks compared to
prior arts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ong_W/0/1/0/all/0/1&quot;&gt;Win Kent Ong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ng_K/0/1/0/all/0/1&quot;&gt;Kam Woh Ng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chan_C/0/1/0/all/0/1&quot;&gt;Chee Seng Chan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Y/0/1/0/all/0/1&quot;&gt;Yi Zhe Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_T/0/1/0/all/0/1&quot;&gt;Tao Xiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09721">
<title>Fast graph-based denoising for point cloud color information. (arXiv:2401.09721v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09721</link>
<description rdf:parseType="Literal">&lt;p&gt;Point clouds are utilized in various 3D applications such as cross-reality
(XR) and realistic 3D displays. In some applications, e.g., for live streaming
using a 3D point cloud, real-time point cloud denoising methods are required to
enhance the visual quality. However, conventional high-precision denoising
methods cannot be executed in real time for large-scale point clouds owing to
the complexity of graph constructions with K nearest neighbors and noise level
estimation. This paper proposes a fast graph-based denoising (FGBD) for a
large-scale point cloud. First, high-speed graph construction is achieved by
scanning a point cloud in various directions and searching adjacent
neighborhoods on the scanning lines. Second, we propose a fast noise level
estimation method using eigenvalues of the covariance matrix on a graph.
Finally, we also propose a new low-cost filter selection method to enhance
denoising accuracy to compensate for the degradation caused by the acceleration
algorithms. In our experiments, we succeeded in reducing the processing time
dramatically while maintaining accuracy relative to conventional denoising
methods. Denoising was performed at 30fps, with frames containing approximately
1 million points.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watanabe_R/0/1/0/all/0/1&quot;&gt;Ryosuke Watanabe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nonaka_K/0/1/0/all/0/1&quot;&gt;Keisuke Nonaka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavez_E/0/1/0/all/0/1&quot;&gt;Eduardo Pavez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kobayashi_T/0/1/0/all/0/1&quot;&gt;Tatsuya Kobayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ortega_A/0/1/0/all/0/1&quot;&gt;Antonio Ortega&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.09895">
<title>Skeleton-Guided Instance Separation for Fine-Grained Segmentation in Microscopy. (arXiv:2401.09895v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.09895</link>
<description rdf:parseType="Literal">&lt;p&gt;One of the fundamental challenges in microscopy (MS) image analysis is
instance segmentation (IS), particularly when segmenting cluster regions where
multiple objects of varying sizes and shapes may be connected or even
overlapped in arbitrary orientations. Existing IS methods usually fail in
handling such scenarios, as they rely on coarse instance representations such
as keypoints and horizontal bounding boxes (h-bboxes). In this paper, we
propose a novel one-stage framework named A2B-IS to address this challenge and
enhance the accuracy of IS in MS images. Our approach represents each instance
with a pixel-level mask map and a rotated bounding box (r-bbox). Unlike
two-stage methods that use box proposals for segmentations, our method
decouples mask and box predictions, enabling simultaneous processing to
streamline the model pipeline. Additionally, we introduce a Gaussian skeleton
map to aid the IS task in two key ways: (1) It guides anchor placement,
reducing computational costs while improving the model&apos;s capacity to learn
RoI-aware features by filtering out noise from background regions. (2) It
ensures accurate isolation of densely packed instances by rectifying erroneous
box predictions near instance boundaries. To further enhance the performance,
we integrate two modules into the framework: (1) An Atrous Attention Block
(A2B) designed to extract high-resolution feature maps with fine-grained
multiscale information, and (2) A Semi-Supervised Learning (SSL) strategy that
leverages both labeled and unlabeled images for model training. Our method has
been thoroughly validated on two large-scale MS datasets, demonstrating its
superiority over most state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengfeng Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ming_Z/0/1/0/all/0/1&quot;&gt;Zhaoyan Ming&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_L/0/1/0/all/0/1&quot;&gt;Lina Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xudong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_D/0/1/0/all/0/1&quot;&gt;Dahong Qian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10090">
<title>Cross-Modality Perturbation Synergy Attack for Person Re-identification. (arXiv:2401.10090v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10090</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there has been significant research focusing on addressing
security concerns in single-modal person re-identification (ReID) systems that
are based on RGB images. However, the safety of cross-modality scenarios, which
are more commonly encountered in practical applications involving images
captured by infrared cameras, has not received adequate attention. The main
challenge in cross-modality ReID lies in effectively dealing with visual
differences between different modalities. For instance, infrared images are
typically grayscale, unlike visible images that contain color information.
Existing attack methods have primarily focused on the characteristics of the
visible image modality, overlooking the features of other modalities and the
variations in data distribution among different modalities. This oversight can
potentially undermine the effectiveness of these methods in image retrieval
across diverse modalities. This study represents the first exploration into the
security of cross-modality ReID models and proposes a universal perturbation
attack specifically designed for cross-modality ReID. This attack optimizes
perturbations by leveraging gradients from diverse modality data, thereby
disrupting the discriminator and reinforcing the differences between
modalities. We conducted experiments on two widely used cross-modality
datasets, namely RegDB and SYSU, which not only demonstrated the effectiveness
of our method but also provided insights for future enhancements in the
robustness of cross-modality ReID systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yunpeng Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Z/0/1/0/all/0/1&quot;&gt;Zhun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Z/0/1/0/all/0/1&quot;&gt;Zhiming Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yansong Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_M/0/1/0/all/0/1&quot;&gt;Min Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10110">
<title>VIPTR: A Vision Permutable Extractor for Fast and Efficient Scene Text Recognition. (arXiv:2401.10110v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10110</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene Text Recognition (STR) is a challenging task that involves recognizing
text within images of natural scenes. Although current state-of-the-art models
for STR exhibit high performance, they typically suffer from low inference
efficiency due to their reliance on hybrid architectures comprised of visual
encoders and sequence decoders. In this work, we propose the VIsion Permutable
extractor for fast and efficient scene Text Recognition (VIPTR), which achieves
an impressive balance between high performance and rapid inference speeds in
the domain of STR. Specifically, VIPTR leverages a visual-semantic extractor
with a pyramid structure, characterized by multiple self-attention layers,
while eschewing the traditional sequence decoder. This design choice results in
a lightweight and efficient model capable of handling inputs of varying sizes.
Extensive experimental results on various standard datasets for both Chinese
and English scene text recognition validate the superiority of VIPTR. Notably,
the VIPTR-T (Tiny) variant delivers highly competitive accuracy on par with
other lightweight models and achieves SOTA inference speeds. Meanwhile, the
VIPTR-L (Large) variant attains greater recognition accuracy, while maintaining
a low parameter count and favorable inference speed. Our proposed method
provides a compelling solution for the STR challenge, which blends high
accuracy with efficiency and greatly benefits real-world applications requiring
fast and reliable text recognition. The code is publicly available at
https://github.com/cxfyxl/VIPTR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xianfu Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_W/0/1/0/all/0/1&quot;&gt;Weixiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaoming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jian Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tongliang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhoujun Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10150">
<title>Motion-Zero: Zero-Shot Moving Object Control Framework for Diffusion-Based Video Generation. (arXiv:2401.10150v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10150</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent large-scale pre-trained diffusion models have demonstrated a powerful
generative ability to produce high-quality videos from detailed text
descriptions. However, exerting control over the motion of objects in videos
generated by any video diffusion model is a challenging problem. In this paper,
we propose a novel zero-shot moving object trajectory control framework,
Motion-Zero, to enable a bounding-box-trajectories-controlled text-to-video
diffusion model.To this end, an initial noise prior module is designed to
provide a position-based prior to improve the stability of the appearance of
the moving object and the accuracy of position. In addition, based on the
attention map of the U-net, spatial constraints are directly applied to the
denoising process of diffusion models, which further ensures the positional and
spatial consistency of moving objects during the inference. Furthermore,
temporal consistency is guaranteed with a proposed shift temporal attention
mechanism. Our method can be flexibly applied to various state-of-the-art video
diffusion models without any training process. Extensive experiments
demonstrate our proposed method can control the motion trajectories of objects
and generate high-quality videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Changgu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shu_J/0/1/0/all/0/1&quot;&gt;Junwei Shu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lianggangxu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_G/0/1/0/all/0/1&quot;&gt;Gaoqi He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.10191">
<title>Divide and not forget: Ensemble of selectively trained experts in Continual Learning. (arXiv:2401.10191v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2401.10191</link>
<description rdf:parseType="Literal">&lt;p&gt;Class-incremental learning is becoming more popular as it helps models widen
their applicability while not forgetting what they already know. A trend in
this area is to use a mixture-of-expert technique, where different models work
together to solve the task. However, the experts are usually trained all at
once using whole task data, which makes them all prone to forgetting and
increasing computational burden. To address this limitation, we introduce a
novel approach named SEED. SEED selects only one, the most optimal expert for a
considered task, and uses data from this task to fine-tune only this expert.
For this purpose, each expert represents each class with a Gaussian
distribution, and the optimal expert is selected based on the similarity of
those distributions. Consequently, SEED increases diversity and heterogeneity
within the experts while maintaining the high stability of this ensemble
method. The extensive experiments demonstrate that SEED achieves
state-of-the-art performance in exemplar-free settings across various
scenarios, showing the potential of expert diversification through data in
continual learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rypesc_G/0/1/0/all/0/1&quot;&gt;Grzegorz Rype&amp;#x15b;&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cygert_S/0/1/0/all/0/1&quot;&gt;Sebastian Cygert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khan_V/0/1/0/all/0/1&quot;&gt;Valeriya Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trzcinski_T/0/1/0/all/0/1&quot;&gt;Tomasz Trzci&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zielinski_B/0/1/0/all/0/1&quot;&gt;Bartosz Zieli&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Twardowski_B/0/1/0/all/0/1&quot;&gt;Bart&amp;#x142;omiej Twardowski&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>