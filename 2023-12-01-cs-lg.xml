<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.LG updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Machine Learning (cs.LG) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-11-29T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Machine Learning</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17065" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17068" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17072" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17073" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17076" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17103" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17104" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17107" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17110" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17121" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17124" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17133" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17134" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17138" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17141" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17162" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17179" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17190" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17218" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17233" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17250" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17259" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17279" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17287" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17299" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17301" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17303" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17323" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17326" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17327" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17352" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17353" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17373" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17400" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17401" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17431" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17434" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17446" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17451" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17508" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17539" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17565" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17593" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17601" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17607" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17609" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17631" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17633" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17673" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17717" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17740" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17750" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17778" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17801" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17833" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17840" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17842" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17853" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17855" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.17856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2006.09017" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2102.07737" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.04442" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.12403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.14053" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.03865" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2202.03349" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.00948" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11952" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.10586" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.04869" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.07626" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.03803" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.05368" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.10649" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.11695" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.01714" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06753" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07321" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.10426" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01552" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01628" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.07598" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10361" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.12534" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14912" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18381" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.19891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.03436" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.15832" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.13903" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.01050" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.03818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.06703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.09791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.01029" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.12931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00489" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.00965" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01768" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01870" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.01929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03605" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.03684" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05703" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.05866" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08235" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.08812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.09949" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11518" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.12819" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17462" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.18348" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.19583" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.00290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08569" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08745" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.08972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.09511" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.10642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.11772" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.12754" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14056" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14698" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.14743" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.15940" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16519" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16834" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16410" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.16459" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2311.17065">
<title>Efficient Deep Speech Understanding at the Edge. (arXiv:2311.17065v1 [eess.AS])</title>
<link>http://arxiv.org/abs/2311.17065</link>
<description rdf:parseType="Literal">&lt;p&gt;Contemporary Speech Understanding (SU) involves a sophisticated pipeline:
capturing real-time voice input, the pipeline encompasses a deep neural network
with an encoder-decoder architecture enhanced by beam search. This network
periodically assesses attention and Connectionist Temporal Classification (CTC)
scores in its autoregressive output.
&lt;/p&gt;
&lt;p&gt;This paper aims to enhance SU performance on edge devices with limited
resources. It pursues two intertwined goals: accelerating on-device execution
and efficiently handling inputs that surpass the on-device model&apos;s capacity.
While these objectives are well-established, we introduce innovative solutions
that specifically address SU&apos;s distinctive challenges: 1. Late
contextualization: Enables the parallel execution of a model&apos;s attentive
encoder during input ingestion. 2. Pilot decoding: Alleviates temporal load
imbalances. 3. Autoregression offramps: Facilitate offloading decisions based
on partial output sequences.
&lt;/p&gt;
&lt;p&gt;Our techniques seamlessly integrate with existing SU models, pipelines, and
frameworks, allowing for independent or combined application. Together, they
constitute a hybrid solution for edge SU, exemplified by our prototype, XYZ.
Evaluated on platforms equipped with 6-8 Arm cores, our system achieves
State-of-the-Art (SOTA) accuracy, reducing end-to-end latency by 2x and halving
offloading requirements.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rongxiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Felix Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17068">
<title>Deep convolutional encoder-decoder hierarchical neural networks for conjugate heat transfer surrogate modeling. (arXiv:2311.17068v1 [cs.CE])</title>
<link>http://arxiv.org/abs/2311.17068</link>
<description rdf:parseType="Literal">&lt;p&gt;Conjugate heat transfer (CHT) models are vital for the design of many
engineering systems. However, high-fidelity CHT models are computationally
intensive, which limits their use in applications such as design optimization,
where hundreds to thousands of model evaluations are required. In this work, we
develop a modular deep convolutional encoder-decoder hierarchical (DeepEDH)
neural network, a novel deep-learning-based surrogate modeling methodology for
computationally intensive CHT models. Leveraging convective temperature
dependencies, we propose a two-stage temperature prediction architecture that
couples velocity and temperature models. The proposed DeepEDH methodology is
demonstrated by modeling the pressure, velocity, and temperature fields for a
liquid-cooled cold-plate-based battery thermal management system with variable
channel geometry. A computational model of the cold plate is developed and
solved using the finite element method (FEM), generating a dataset of 1,500
simulations. The FEM results are transformed and scaled from unstructured to
structured, image-like meshes to create training and test datasets. The DeepEDH
methodology&apos;s performance is examined in relation to data scaling, training
dataset size, and network depth. Our performance analysis covers the impact of
the novel architecture, separate field models, output geometry masks,
multi-stage temperature models, and optimizations of the hyperparameters and
architecture. Furthermore, we quantify the influence of the CHT thermal
boundary condition on surrogate model performance, highlighting improved
temperature model performance with higher heat fluxes. Compared to other deep
learning neural network surrogate models, such as U-Net and DenseED, the
proposed DeepEDH methodology for CHT models exhibits up to a 65% enhancement in
the coefficient of determination ($R^{2}$).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebbs_Picken_T/0/1/0/all/0/1&quot;&gt;Takiah Ebbs-Picken&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_D/0/1/0/all/0/1&quot;&gt;David A. Romero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_C/0/1/0/all/0/1&quot;&gt;Carlos M. Da Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amon_C/0/1/0/all/0/1&quot;&gt;Cristina H. Amon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17072">
<title>IG Captioner: Information Gain Captioners are Strong Zero-shot Classifiers. (arXiv:2311.17072v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17072</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative training has been demonstrated to be powerful for building
visual-language models. However, on zero-shot discriminative benchmarks, there
is still a performance gap between models trained with generative and
discriminative objectives. In this paper, we aim to narrow this gap by
improving the efficacy of generative training on classification tasks, without
any finetuning processes or additional modules.
&lt;/p&gt;
&lt;p&gt;Specifically, we focus on narrowing the gap between the generative captioner
and the CLIP classifier. We begin by analysing the predictions made by the
captioner and classifier and observe that the caption generation inherits the
distribution bias from the language model trained with pure text modality,
making it less grounded on the visual signal. To tackle this problem, we
redesign the scoring objective for the captioner to alleviate the
distributional bias and focus on measuring the gain of information brought by
the visual inputs. We further design a generative training objective to match
the evaluation objective. We name our model trained and evaluated from the
novel procedures as Information Gain (IG) captioner. We pretrain the models on
the public Laion-5B dataset and perform a series of discriminative evaluations.
For the zero-shot classification on ImageNet, IG captioner achieves $&amp;gt; 18\%$
improvements over the standard captioner, achieving comparable performances
with the CLIP classifier. IG captioner also demonstrated strong performance on
zero-shot image-text retrieval tasks on MSCOCO and Flickr30K. We hope this
paper inspires further research towards unifying generative and discriminative
training procedures for visual-language models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenglin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_S/0/1/0/all/0/1&quot;&gt;Siyuan Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_T/0/1/0/all/0/1&quot;&gt;Tao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuille_A/0/1/0/all/0/1&quot;&gt;Alan Yuille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jiahui Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17073">
<title>Practical Layout-Aware Analog/Mixed-Signal Design Automation with Bayesian Neural Networks. (arXiv:2311.17073v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17073</link>
<description rdf:parseType="Literal">&lt;p&gt;The high simulation cost has been a bottleneck of practical
analog/mixed-signal design automation. Many learning-based algorithms require
thousands of simulated data points, which is impractical for expensive to
simulate circuits. We propose a learning-based algorithm that can be trained
using a small amount of data and, therefore, scalable to tasks with expensive
simulations. Our efficient algorithm solves the post-layout performance
optimization problem where simulations are known to be expensive. Our
comprehensive study also solves the schematic-level sizing problem. For
efficient optimization, we utilize Bayesian Neural Networks as a regression
model to approximate circuit performance. For layout-aware optimization, we
handle the problem as a multi-fidelity optimization problem and improve
efficiency by exploiting the correlations from cheaper evaluations. We present
three test cases to demonstrate the efficiency of our algorithms. Our tests
prove that the proposed approach is more efficient than conventional baselines
and state-of-the-art algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Budak_A/0/1/0/all/0/1&quot;&gt;Ahmet F. Budak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_K/0/1/0/all/0/1&quot;&gt;Keren Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_D/0/1/0/all/0/1&quot;&gt;David Z. Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17076">
<title>Compositional Chain-of-Thought Prompting for Large Multimodal Models. (arXiv:2311.17076v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17076</link>
<description rdf:parseType="Literal">&lt;p&gt;The combination of strong visual backbones and Large Language Model (LLM)
reasoning has led to Large Multimodal Models (LMMs) becoming the current
standard for a wide range of vision and language (VL) tasks. However, recent
research has shown that even the most advanced LMMs still struggle to capture
aspects of compositional visual reasoning, such as attributes and relationships
between objects. One solution is to utilize scene graphs (SGs)--a formalization
of objects and their relations and attributes that has been extensively used as
a bridge between the visual and textual domains. Yet, scene graph data requires
scene graph annotations, which are expensive to collect and thus not easily
scalable. Moreover, finetuning an LMM based on SG data can lead to catastrophic
forgetting of the pretraining objective. To overcome this, inspired by
chain-of-thought methods, we propose Compositional Chain-of-Thought (CCoT), a
novel zero-shot Chain-of-Thought prompting method that utilizes SG
representations in order to extract compositional knowledge from an LMM.
Specifically, we first generate an SG using the LMM, and then use that SG in
the prompt to produce a response. Through extensive experiments, we find that
the proposed CCoT approach not only improves LMM performance on several vision
and language VL compositional benchmarks but also improves the performance of
several popular LMMs on general multimodal benchmarks, without the need for
fine-tuning or annotated ground-truth SGs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mitra_C/0/1/0/all/0/1&quot;&gt;Chancharik Mitra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Brandon Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Darrell_T/0/1/0/all/0/1&quot;&gt;Trevor Darrell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herzig_R/0/1/0/all/0/1&quot;&gt;Roei Herzig&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17093">
<title>Improved Prototypical Semi-Supervised Learning with Foundation Models: Prototype Selection, Parametric vMF-SNE Pretraining and Multi-view Pseudolabelling. (arXiv:2311.17093v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17093</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper we present an improved approach to prototypical semi-supervised
learning for computer vision, in the context of leveraging a frozen foundation
model as the backbone of our neural network. As a general tool, we propose
parametric von-Mises Fisher Stochastic Neighbour Embedding (vMF-SNE) to create
mappings with neural networks between high-dimensional latent spaces that
preserve local structure. This enables us to pretrain the projection head of
our network using the high-quality embeddings of the foundation model with
vMF-SNE. We also propose soft multi-view pseudolabels, where predictions across
multiple views are combined to provide a more reliable supervision signal
compared to a consistency or swapped assignment approach. We demonstrate that
these ideas improve upon P}redicting View-Assignments with Support Samples
(PAWS), a current state-of-the-art semi-supervised learning method, as well as
Robust PAWS (RoPAWS), over a range of benchmarking datasets. We also introduce
simple $k$-means prototype selection, a technique that provides superior
performance to other unsupervised label selection approaches in this context.
These changes improve upon PAWS by an average of +2.9% for CIFAR-10 and +5.7%
for CIFAR-100 with four labels per class, and by +15.2% for DeepWeeds, a
particularly challenging dataset for semi-supervised learning. We also achieve
new state-of-the-art results in semi-supervised learning in this small label
regime for CIFAR-10 - 95.8% (+0.7%) and CIFAR-100 - 76.6% (+12.0%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mannix_E/0/1/0/all/0/1&quot;&gt;Evelyn Mannix&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bondell_H/0/1/0/all/0/1&quot;&gt;Howard Bondell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17094">
<title>In Search of a Data Transformation That Accelerates Neural Field Training. (arXiv:2311.17094v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17094</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural field is an emerging paradigm in data representation that trains a
neural network to approximate the given signal. A key obstacle that prevents
its widespread adoption is the encoding speed-generating neural fields requires
an overfitting of a neural network, which can take a significant number of SGD
steps to reach the desired fidelity level. In this paper, we delve into the
impacts of data transformations on the speed of neural field training,
specifically focusing on how permuting pixel locations affect the convergence
speed of SGD. Counterintuitively, we find that randomly permuting the pixel
locations can considerably accelerate the training. To explain this phenomenon,
we examine the neural field training through the lens of PSNR curves, loss
landscapes, and error patterns. Our analyses suggest that the random pixel
permutations remove the easy-to-fit patterns, which facilitate easy
optimization in the early stage but hinder capturing fine details of the
signal.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Junwon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangyoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kwang In Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17097">
<title>Anonymous Jamming Detection in 5G with Bayesian Network Model Based Inference Analysis. (arXiv:2311.17097v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17097</link>
<description rdf:parseType="Literal">&lt;p&gt;Jamming and intrusion detection are critical in 5G research, aiming to
maintain reliability, prevent user experience degradation, and avoid
infrastructure failure. This paper introduces an anonymous jamming detection
model for 5G based on signal parameters from the protocol stacks. The system
uses supervised and unsupervised learning for real-time, high-accuracy
detection of jamming, including unknown types. Supervised models reach an AUC
of 0.964 to 1, compared to LSTM models with an AUC of 0.923 to 1. However, the
need for data annotation limits the supervised approach. To address this, an
unsupervised auto-encoder-based anomaly detection is presented with an AUC of
0.987. The approach is resistant to adversarial training samples. For
transparency and domain knowledge injection, a Bayesian network-based causation
analysis is introduced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Ying Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jere_S/0/1/0/all/0/1&quot;&gt;Shashank Jere&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Banerjee_S/0/1/0/all/0/1&quot;&gt;Soumya Banerjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjia Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shetty_S/0/1/0/all/0/1&quot;&gt;Sachin Shetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dayekh_S/0/1/0/all/0/1&quot;&gt;Shehadi Dayekh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17103">
<title>Single-cell Multi-view Clustering via Community Detection with Unknown Number of Clusters. (arXiv:2311.17103v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2311.17103</link>
<description rdf:parseType="Literal">&lt;p&gt;Single-cell multi-view clustering enables the exploration of cellular
heterogeneity within the same cell from different views. Despite the
development of several multi-view clustering methods, two primary challenges
persist. Firstly, most existing methods treat the information from both
single-cell RNA (scRNA) and single-cell Assay of Transposase Accessible
Chromatin (scATAC) views as equally significant, overlooking the substantial
disparity in data richness between the two views. This oversight frequently
leads to a degradation in overall performance. Additionally, the majority of
clustering methods necessitate manual specification of the number of clusters
by users. However, for biologists dealing with cell data, precisely determining
the number of distinct cell types poses a formidable challenge. To this end, we
introduce scUNC, an innovative multi-view clustering approach tailored for
single-cell data, which seamlessly integrates information from different views
without the need for a predefined number of clusters. The scUNC method
comprises several steps: initially, it employs a cross-view fusion network to
create an effective embedding, which is then utilized to generate initial
clusters via community detection. Subsequently, the clusters are automatically
merged and optimized until no further clusters can be merged. We conducted a
comprehensive evaluation of scUNC using three distinct single-cell datasets.
The results underscored that scUNC outperforms the other baseline methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dayu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhibin Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Siwei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17104">
<title>Single-Cell Clustering via Dual-Graph Alignment. (arXiv:2311.17104v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17104</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, the field of single-cell RNA sequencing has seen a surge in
the development of clustering methods. These methods enable the identification
of cell subpopulations, thereby facilitating the understanding of tumor
microenvironments. Despite their utility, most existing clustering algorithms
primarily focus on the attribute information provided by the cell matrix or the
network structure between cells, often neglecting the network between genes.
This oversight could lead to loss of information and clustering results that
lack clinical significance. To address this limitation, we develop an advanced
single-cell clustering model incorporating dual-graph alignment, which
integrates gene network information into the clustering process based on
self-supervised and unsupervised optimization. Specifically, we designed a
graph-based autoencoder enhanced by an attention mechanism to effectively
capture relationships between cells. Moreover, we performed the node2vec method
on Protein-Protein Interaction (PPI) networks to derive the gene network
structure and maintained this structure throughout the clustering process. Our
proposed method has been demonstrated to be effective through experimental
results, showcasing its ability to optimize clustering outcomes while
preserving the original associations between cells and genes. This research
contributes to obtaining accurate cell subpopulations and generates clustering
results that more closely resemble real-world biological scenarios. It provides
better insights into the characteristics and distribution of diseased cells,
ultimately building a foundation for early disease diagnosis and treatment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_D/0/1/0/all/0/1&quot;&gt;Dayu Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_K/0/1/0/all/0/1&quot;&gt;Ke Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinwang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17107">
<title>ClimateX: Do LLMs Accurately Assess Human Expert Confidence in Climate Statements?. (arXiv:2311.17107v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17107</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating the accuracy of outputs generated by Large Language Models (LLMs)
is especially important in the climate science and policy domain. We introduce
the Expert Confidence in Climate Statements (ClimateX) dataset, a novel,
curated, expert-labeled dataset consisting of 8094 climate statements collected
from the latest Intergovernmental Panel on Climate Change (IPCC) reports,
labeled with their associated confidence levels. Using this dataset, we show
that recent LLMs can classify human expert confidence in climate-related
statements, especially in a few-shot learning setting, but with limited (up to
47%) accuracy. Overall, models exhibit consistent and significant
over-confidence on low and medium confidence statements. We highlight
implications of our results for climate communication, LLMs evaluation
strategies, and the use of LLMs in information retrieval systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lacombe_R/0/1/0/all/0/1&quot;&gt;Romain Lacombe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_K/0/1/0/all/0/1&quot;&gt;Kerrie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dilworth_E/0/1/0/all/0/1&quot;&gt;Eddie Dilworth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17110">
<title>XAI for time-series classification leveraging image highlight methods. (arXiv:2311.17110v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17110</link>
<description rdf:parseType="Literal">&lt;p&gt;Although much work has been done on explainability in the computer vision and
natural language processing (NLP) fields, there is still much work to be done
to explain methods applied to time series as time series by nature can not be
understood at first sight. In this paper, we present a Deep Neural Network
(DNN) in a teacher-student architecture (distillation model) that offers
interpretability in time-series classification tasks. The explainability of our
approach is based on transforming the time series to 2D plots and applying
image highlight methods (such as LIME and GradCam), making the predictions
interpretable. At the same time, the proposed approach offers increased
accuracy competing with the baseline model with the trade-off of increasing the
training time.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Makridis_G/0/1/0/all/0/1&quot;&gt;Georgios Makridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fatouros_G/0/1/0/all/0/1&quot;&gt;Georgios Fatouros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koukos_V/0/1/0/all/0/1&quot;&gt;Vasileios Koukos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kotios_D/0/1/0/all/0/1&quot;&gt;Dimitrios Kotios&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyriazis_D/0/1/0/all/0/1&quot;&gt;Dimosthenis Kyriazis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soldatos_I/0/1/0/all/0/1&quot;&gt;Ioannis Soldatos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17121">
<title>Generative Data Augmentation Improves Scribble-supervised Semantic Segmentation. (arXiv:2311.17121v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17121</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative models, such as diffusion models, have made
generating high-quality synthetic images widely accessible. Prior works have
shown that training on synthetic images improves many perception tasks, such as
image classification, object detection, and semantic segmentation. We are the
first to explore generative data augmentations for scribble-supervised semantic
segmentation. We propose a generative data augmentation method that leverages a
ControlNet diffusion model conditioned on semantic scribbles to produce
high-quality training data. However, naive implementations of generative data
augmentations may inadvertently harm the performance of the downstream
segmentor rather than improve it. We leverage classifier-free diffusion
guidance to enforce class consistency and introduce encode ratios to trade off
data diversity for data realism. Using the guidance scale and encode ratio, we
are able to generate a spectrum of high-quality training images. We propose
multiple augmentation schemes and find that these schemes significantly impact
model performance, especially in the low-data regime. Our framework further
reduces the gap between the performance of scribble-supervised segmentation and
that of fully-supervised segmentation. We also show that our framework
significantly improves segmentation performance on small datasets, even
surpassing fully-supervised segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnell_J/0/1/0/all/0/1&quot;&gt;Jacob Schnell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jieke Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_L/0/1/0/all/0/1&quot;&gt;Lu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_V/0/1/0/all/0/1&quot;&gt;Vincent Tao Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_M/0/1/0/all/0/1&quot;&gt;Meng Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17124">
<title>A knowledge-driven AutoML architecture. (arXiv:2311.17124v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17124</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper proposes a knowledge-driven AutoML architecture for pipeline and
deep feature synthesis. The main goal is to render the AutoML process
explainable and to leverage domain knowledge in the synthesis of pipelines and
features. The architecture explores several novel ideas: first, the
construction of pipelines and deep features is approached in an unified way.
Next, synthesis is driven by a shared knowledge system, interactively queried
as to what pipeline operations to use or features to compute. Lastly, the
synthesis processes takes decisions at runtime using partial solutions and
results of their application on data. Two experiments are conducted to
demonstrate the functionality of a na\&quot;{\i}ve implementation of the proposed
architecture and to discuss its advantages, trade-offs as well as future
potential for AutoML.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cofaru_C/0/1/0/all/0/1&quot;&gt;Corneliu Cofaru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loeckx_J/0/1/0/all/0/1&quot;&gt;Johan Loeckx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17129">
<title>Feedback RoI Features Improve Aerial Object Detection. (arXiv:2311.17129v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17129</link>
<description rdf:parseType="Literal">&lt;p&gt;Neuroscience studies have shown that the human visual system utilizes
high-level feedback information to guide lower-level perception, enabling
adaptation to signals of different characteristics. In light of this, we
propose Feedback multi-Level feature Extractor (Flex) to incorporate a similar
mechanism for object detection. Flex refines feature selection based on
image-wise and instance-level feedback information in response to image quality
variation and classification uncertainty. Experimental results show that Flex
offers consistent improvement to a range of existing SOTA methods on the
challenging aerial object detection datasets including DOTA-v1.0, DOTA-v1.5,
and HRSC2016. Although the design originates in aerial image detection, further
experiments on MS COCO also reveal our module&apos;s efficacy in general detection
models. Quantitative and qualitative analyses indicate that the improvements
are closely related to image qualities, which match our motivation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_B/0/1/0/all/0/1&quot;&gt;Botao Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_B/0/1/0/all/0/1&quot;&gt;Botian Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tengyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_Z/0/1/0/all/0/1&quot;&gt;Zhidong Deng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17133">
<title>Deployment of a Robust and Explainable Mortality Prediction Model: The COVID-19 Pandemic and Beyond. (arXiv:2311.17133v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17133</link>
<description rdf:parseType="Literal">&lt;p&gt;This study investigated the performance, explainability, and robustness of
deployed artificial intelligence (AI) models in predicting mortality during the
COVID-19 pandemic and beyond. The first study of its kind, we found that
Bayesian Neural Networks (BNNs) and intelligent training techniques allowed our
models to maintain performance amidst significant data shifts. Our results
emphasize the importance of developing robust AI models capable of matching or
surpassing clinician predictions, even under challenging conditions. Our
exploration of model explainability revealed that stochastic models generate
more diverse and personalized explanations thereby highlighting the need for AI
models that provide detailed and individualized insights in real-world clinical
settings. Furthermore, we underscored the importance of quantifying uncertainty
in AI models which enables clinicians to make better-informed decisions based
on reliable predictions. Our study advocates for prioritizing implementation
science in AI research for healthcare and ensuring that AI solutions are
practical, beneficial, and sustainable in real-world clinical environments. By
addressing unique challenges and complexities in healthcare settings,
researchers can develop AI models that effectively improve clinical practice
and patient outcomes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epifano_J/0/1/0/all/0/1&quot;&gt;Jacob R. Epifano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_S/0/1/0/all/0/1&quot;&gt;Stephen Glass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramachandran_R/0/1/0/all/0/1&quot;&gt;Ravi P. Ramachandran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_S/0/1/0/all/0/1&quot;&gt;Sharad Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masino_A/0/1/0/all/0/1&quot;&gt;Aaron J. Masino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rasool_G/0/1/0/all/0/1&quot;&gt;Ghulam Rasool&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17134">
<title>\texttt{GlycoNMR}: Dataset and benchmarks for NMR chemical shift prediction of carbohydrates with graph neural networks. (arXiv:2311.17134v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17134</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular representation learning (MRL) is a powerful tool for bridging the
gap between machine learning and chemical sciences, as it converts molecules
into numerical representations while preserving their chemical features. These
encoded representations serve as a foundation for various downstream
biochemical studies, including property prediction and drug design. MRL has had
great success with proteins and general biomolecule datasets. Yet, in the
growing sub-field of glycoscience (the study of carbohydrates, where longer
carbohydrates are also called glycans), MRL methods have been barely explored.
This under-exploration can be primarily attributed to the limited availability
of comprehensive and well-curated carbohydrate-specific datasets and a lack of
Machine learning (ML) pipelines specifically tailored to meet the unique
problems presented by carbohydrate data. Since interpreting and annotating
carbohydrate-specific data is generally more complicated than protein data,
domain experts are usually required to get involved. The existing MRL methods,
predominately optimized for proteins and small biomolecules, also cannot be
directly used in carbohydrate applications without special modifications. To
address this challenge, accelerate progress in glycoscience, and enrich the
data resources of the MRL community, we introduce GlycoNMR. GlycoNMR contains
two laboriously curated datasets with 2,609 carbohydrate structures and 211,543
annotated nuclear magnetic resonance (NMR) chemical shifts for precise
atomic-level prediction. We tailored carbohydrate-specific features and adapted
existing MRL models to tackle this problem effectively. For illustration, we
benchmark four modified MRL models on our new datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zizhang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Badman_R/0/1/0/all/0/1&quot;&gt;Ryan Paul Badman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foley_L/0/1/0/all/0/1&quot;&gt;Lachele Foley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woods_R/0/1/0/all/0/1&quot;&gt;Robert Woods&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_P/0/1/0/all/0/1&quot;&gt;Pengyu Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17137">
<title>Generative Models: What do they know? Do they know things? Let&apos;s find out!. (arXiv:2311.17137v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17137</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models have been shown to be capable of synthesizing highly
detailed and realistic images. It is natural to suspect that they implicitly
learn to model some image intrinsics such as surface normals, depth, or
shadows. In this paper, we present compelling evidence that generative models
indeed internally produce high-quality scene intrinsic maps. We introduce
Intrinsic LoRA (I LoRA), a universal, plug-and-play approach that transforms
any generative model into a scene intrinsic predictor, capable of extracting
intrinsic scene maps directly from the original generator network without
needing additional decoders or fully fine-tuning the original network. Our
method employs a Low-Rank Adaptation (LoRA) of key feature maps, with newly
learned parameters that make up less than 0.6% of the total parameters in the
generative model. Optimized with a small set of labeled images, our
model-agnostic approach adapts to various generative architectures, including
Diffusion models, GANs, and Autoregressive models. We show that the scene
intrinsic maps produced by our method compare well with, and in some cases
surpass those generated by leading supervised techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_X/0/1/0/all/0/1&quot;&gt;Xiaodan Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolkin_N/0/1/0/all/0/1&quot;&gt;Nicholas Kolkin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shakhnarovich_G/0/1/0/all/0/1&quot;&gt;Greg Shakhnarovich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1&quot;&gt;Anand Bhattad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17138">
<title>Shadows Don&apos;t Lie and Lines Can&apos;t Bend! Generative Models don&apos;t know Projective Geometry...for now. (arXiv:2311.17138v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17138</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative models can produce impressively realistic images. This paper
demonstrates that generated images have geometric features different from those
of real images. We build a set of collections of generated images, prequalified
to fool simple, signal-based classifiers into believing they are real. We then
show that prequalified generated images can be identified reliably by
classifiers that only look at geometric properties. We use three such
classifiers. All three classifiers are denied access to image pixels, and look
only at derived geometric features. The first classifier looks at the
perspective field of the image, the second looks at lines detected in the
image, and the third looks at relations between detected objects and shadows.
Our procedure detects generated images more reliably than SOTA local signal
based detectors, for images from a number of distinct generators. Saliency maps
suggest that the classifiers can identify geometric problems reliably. We
conclude that current generators cannot reliably reproduce geometric properties
of real images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarkar_A/0/1/0/all/0/1&quot;&gt;Ayush Sarkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mai_H/0/1/0/all/0/1&quot;&gt;Hanlin Mai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahapatra_A/0/1/0/all/0/1&quot;&gt;Amitabh Mahapatra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lazebnik_S/0/1/0/all/0/1&quot;&gt;Svetlana Lazebnik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forsyth_D/0/1/0/all/0/1&quot;&gt;D.A. Forsyth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhattad_A/0/1/0/all/0/1&quot;&gt;Anand Bhattad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17141">
<title>A point cloud approach to generative modeling for galaxy surveys at the field level. (arXiv:2311.17141v1 [astro-ph.CO])</title>
<link>http://arxiv.org/abs/2311.17141</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a diffusion-based generative model to describe the distribution
of galaxies in our Universe directly as a collection of points in 3-D space
(coordinates) optionally with associated attributes (e.g., velocities and
masses), without resorting to binning or voxelization. The custom diffusion
model can be used both for emulation, reproducing essential summary statistics
of the galaxy distribution, as well as inference, by computing the conditional
likelihood of a galaxy field. We demonstrate a first application to massive
dark matter haloes in the Quijote simulation suite. This approach can be
extended to enable a comprehensive analysis of cosmological data, circumventing
limitations inherent to summary statistic -- as well as neural simulation-based
inference methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Cuesta_Lazaro_C/0/1/0/all/0/1&quot;&gt;Carolina Cuesta-Lazaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Mishra_Sharma_S/0/1/0/all/0/1&quot;&gt;Siddharth Mishra-Sharma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17143">
<title>Predicting the Age of Astronomical Transients from Real-Time Multivariate Time Series. (arXiv:2311.17143v1 [astro-ph.IM])</title>
<link>http://arxiv.org/abs/2311.17143</link>
<description rdf:parseType="Literal">&lt;p&gt;Astronomical transients, such as supernovae and other rare stellar
explosions, have been instrumental in some of the most significant discoveries
in astronomy. New astronomical sky surveys will soon record unprecedented
numbers of transients as sparsely and irregularly sampled multivariate time
series. To improve our understanding of the physical mechanisms of transients
and their progenitor systems, early-time measurements are necessary.
Prioritizing the follow-up of transients based on their age along with their
class is crucial for new surveys. To meet this demand, we present the first
method of predicting the age of transients in real-time from multi-wavelength
time-series observations. We build a Bayesian probabilistic recurrent neural
network. Our method can accurately predict the age of a transient with robust
uncertainties as soon as it is initially triggered by a survey telescope. This
work will be essential for the advancement of our understanding of the numerous
young transients being detected by ongoing and upcoming astronomical surveys.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hali Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Muthukrishna_D/0/1/0/all/0/1&quot;&gt;Daniel Muthukrishna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Nair_P/0/1/0/all/0/1&quot;&gt;Prajna Nair&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zimi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Fausnaugh_M/0/1/0/all/0/1&quot;&gt;Michael Fausnaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Majumder_T/0/1/0/all/0/1&quot;&gt;Torsha Majumder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Foley_R/0/1/0/all/0/1&quot;&gt;Ryan J. Foley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/astro-ph/1/au:+Ricker_G/0/1/0/all/0/1&quot;&gt;George R. Ricker&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17154">
<title>Pragmatic Radiology Report Generation. (arXiv:2311.17154v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17154</link>
<description rdf:parseType="Literal">&lt;p&gt;When pneumonia is not found on a chest X-ray, should the report describe this
negative observation or omit it? We argue that this question cannot be answered
from the X-ray alone and requires a pragmatic perspective, which captures the
communicative goal that radiology reports serve between radiologists and
patients. However, the standard image-to-text formulation for radiology report
generation fails to incorporate such pragmatic intents. Following this
pragmatic perspective, we demonstrate that the indication, which describes why
a patient comes for an X-ray, drives the mentions of negative observations and
introduce indications as additional input to report generation. With respect to
the output, we develop a framework to identify uninferable information from the
image as a source of model hallucinations, and limit them by cleaning
groundtruth reports. Finally, we use indications and cleaned groundtruth
reports to develop pragmatic models, and show that they outperform existing
methods not only in new pragmatics-inspired metrics (+4.3 Negative F1) but also
in standard metrics (+6.3 Positive F1 and +11.0 BLEU-2).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Dang Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chacha Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;He He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Chenhao Tan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17162">
<title>Fast Particle-based Anomaly Detection Algorithm with Variational Autoencoder. (arXiv:2311.17162v1 [hep-ex])</title>
<link>http://arxiv.org/abs/2311.17162</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-agnostic anomaly detection is one of the promising approaches in the
search for new beyond the standard model physics. In this paper, we present
Set-VAE, a particle-based variational autoencoder (VAE) anomaly detection
algorithm. We demonstrate a 2x signal efficiency gain compared with traditional
subjettiness-based jet selection. Furthermore, with an eye to the future
deployment to trigger systems, we propose the CLIP-VAE, which reduces the
inference-time cost of anomaly detection by using the KL-divergence loss as the
anomaly score, resulting in a 2x acceleration in latency and reducing the
caching requirement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Liu_R/0/1/0/all/0/1&quot;&gt;Ryan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Gandrakota_A/0/1/0/all/0/1&quot;&gt;Abhijith Gandrakota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Ngadiuba_J/0/1/0/all/0/1&quot;&gt;Jennifer Ngadiuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Spiropulu_M/0/1/0/all/0/1&quot;&gt;Maria Spiropulu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/hep-ex/1/au:+Vlimant_J/0/1/0/all/0/1&quot;&gt;Jean-Roch Vlimant&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17165">
<title>(Ir)rationality in AI: State of the Art, Research Challenges and Open Questions. (arXiv:2311.17165v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2311.17165</link>
<description rdf:parseType="Literal">&lt;p&gt;The concept of rationality is central to the field of artificial
intelligence. Whether we are seeking to simulate human reasoning, or the goal
is to achieve bounded optimality, we generally seek to make artificial agents
as rational as possible. Despite the centrality of the concept within AI, there
is no unified definition of what constitutes a rational agent. This article
provides a survey of rationality and irrationality in artificial intelligence,
and sets out the open questions in this area. The understanding of rationality
in other fields has influenced its conception within artificial intelligence,
in particular work in economics, philosophy and psychology. Focusing on the
behaviour of artificial agents, we consider irrational behaviours that can
prove to be optimal in certain scenarios. Some methods have been developed to
deal with irrational agents, both in terms of identification and interaction,
however work in this area remains limited. Methods that have up to now been
developed for other purposes, namely adversarial scenarios, may be adapted to
suit interactions with artificial agents. We further discuss the interplay
between human and artificial agents, and the role that rationality plays within
this interaction; many questions remain in this area, relating to potentially
irrational behaviour of both humans and artificial agents.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Macmillan_Scott_O/0/1/0/all/0/1&quot;&gt;Olivia Macmillan-Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Musolesi_M/0/1/0/all/0/1&quot;&gt;Mirco Musolesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17173">
<title>A personalized Uncertainty Quantification framework for patient survival models: estimating individual uncertainty of patients with metastatic brain tumors in the absence of ground truth. (arXiv:2311.17173v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17173</link>
<description rdf:parseType="Literal">&lt;p&gt;TodevelopanovelUncertaintyQuantification (UQ) framework to estimate the
uncertainty of patient survival models in the absence of ground truth, we
developed and evaluated our approach based on a dataset of 1383 patients
treated with stereotactic radiosurgery (SRS) for brain metastases between
January 2015 and December 2020. Our motivating hypothesis is that a
time-to-event prediction of a test patient on inference is more certain given a
higher feature-space-similarity to patients in the training set. Therefore, the
uncertainty for a particular patient-of-interest is represented by the
concordance index between a patient similarity rank and a prediction similarity
rank. Model uncertainty was defined as the increased percentage of the max
uncertainty-constrained-AUC compared to the model AUC. We evaluated our method
on multiple clinically-relevant endpoints, including time to intracranial
progression (ICP), progression-free survival (PFS) after SRS, overall survival
(OS), and time to ICP and/or death (ICPD), on a variety of both statistical and
non-statistical models, including CoxPH, conditional survival forest (CSF), and
neural multi-task linear regression (NMTLR). Our results show that all models
had the lowest uncertainty on ICP (2.21%) and the highest uncertainty (17.28%)
on ICPD. OS models demonstrated high variation in uncertainty performance,
where NMTLR had the lowest uncertainty(1.96%)and CSF had the highest
uncertainty (14.29%). In conclusion, our method can estimate the uncertainty of
individual patient survival modeling results. As expected, our data empirically
demonstrate that as model uncertainty measured via our technique increases, the
similarity between a feature-space and its predicted outcome decreases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Aarzu Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carpenter_D/0/1/0/all/0/1&quot;&gt;David Carpenter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mullikin_T/0/1/0/all/0/1&quot;&gt;Trey Mullikin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reitman_Z/0/1/0/all/0/1&quot;&gt;Zachary J. Reitman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Floyd_S/0/1/0/all/0/1&quot;&gt;Scott Floyd&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kirkpatrick_J/0/1/0/all/0/1&quot;&gt;John Kirkpatrick&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salama_J/0/1/0/all/0/1&quot;&gt;Joseph K. Salama&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperduto_P/0/1/0/all/0/1&quot;&gt;Paul W. Sperduto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jian-Guo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bashir_M/0/1/0/all/0/1&quot;&gt;Mustafa R. Bashir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lafata_K/0/1/0/all/0/1&quot;&gt;Kyle J. Lafata&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17179">
<title>SatCLIP: Global, General-Purpose Location Embeddings with Satellite Imagery. (arXiv:2311.17179v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17179</link>
<description rdf:parseType="Literal">&lt;p&gt;Geographic location is essential for modeling tasks in fields ranging from
ecology to epidemiology to the Earth system sciences. However, extracting
relevant and meaningful characteristics of a location can be challenging, often
entailing expensive data fusion or data distillation from global imagery
datasets. To address this challenge, we introduce Satellite Contrastive
Location-Image Pretraining (SatCLIP), a global, general-purpose geographic
location encoder that learns an implicit representation of locations from
openly available satellite imagery. Trained location encoders provide vector
embeddings summarizing the characteristics of any given location for convenient
usage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained
on globally sampled multi-spectral Sentinel-2 satellite data, can be used in
various predictive tasks that depend on location information but not
necessarily satellite imagery, including temperature prediction, animal
recognition in imagery, and population density estimation. Across tasks,
SatCLIP embeddings consistently outperform embeddings from existing pretrained
location encoders, ranging from models trained on natural images to models
trained on semantic context. SatCLIP embeddings also help to improve geographic
generalization. This demonstrates the potential of general-purpose location
encoders and opens the door to learning meaningful representations of our
planet from the vast, varied, and largely untapped modalities of geospatial
data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemmer_K/0/1/0/all/0/1&quot;&gt;Konstantin Klemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolf_E/0/1/0/all/0/1&quot;&gt;Esther Rolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robinson_C/0/1/0/all/0/1&quot;&gt;Caleb Robinson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mackey_L/0/1/0/all/0/1&quot;&gt;Lester Mackey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Russwurm_M/0/1/0/all/0/1&quot;&gt;Marc Ru&amp;#xdf;wurm&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17190">
<title>Minimax Exploiter: A Data Efficient Approach for Competitive Self-Play. (arXiv:2311.17190v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17190</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in Competitive Self-Play (CSP) have achieved, or even
surpassed, human level performance in complex game environments such as Dota 2
and StarCraft II using Distributed Multi-Agent Reinforcement Learning (MARL).
One core component of these methods relies on creating a pool of learning
agents -- consisting of the Main Agent, past versions of this agent, and
Exploiter Agents -- where Exploiter Agents learn counter-strategies to the Main
Agents. A key drawback of these approaches is the large computational cost and
physical time that is required to train the system, making them impractical to
deploy in highly iterative real-life settings such as video game productions.
In this paper, we propose the Minimax Exploiter, a game theoretic approach to
exploiting Main Agents that leverages knowledge of its opponents, leading to
significant increases in data efficiency. We validate our approach in a
diversity of settings, including simple turn based games, the arcade learning
environment, and For Honor, a modern video game. The Minimax Exploiter
consistently outperforms strong baselines, demonstrating improved stability and
data efficiency, leading to a robust CSP-MARL method that is both flexible and
easy to deploy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bairamian_D/0/1/0/all/0/1&quot;&gt;Daniel Bairamian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marcotte_P/0/1/0/all/0/1&quot;&gt;Philippe Marcotte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romoff_J/0/1/0/all/0/1&quot;&gt;Joshua Romoff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robert_G/0/1/0/all/0/1&quot;&gt;Gabriel Robert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nowrouzezahrai_D/0/1/0/all/0/1&quot;&gt;Derek Nowrouzezahrai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17204">
<title>Optimal EEG Electrode Set for Emotion Recognition From Brain Signals: An Empirical Quest. (arXiv:2311.17204v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17204</link>
<description rdf:parseType="Literal">&lt;p&gt;The human brain is a complex organ, still completely undiscovered, that
controls almost all the parts of the body. Apart from survival, the human brain
stimulates emotions. Recent research indicates that brain signals can be very
effective for emotion recognition. However, which parts of the brain exhibit
most of the emotions is still under-explored. In this study, we empirically
analyze the contribution of each part of the brain in exhibiting emotions. We
use the DEAP dataset to find the most optimal electrode set which eventually
leads to the effective brain part associated with emotions. We use Fast Fourier
Transformation for effective feature extraction and a 1D-CNN with residual
connection for classification. Though 32 electrodes from the DEAP dataset got
an accuracy of 97.34%, only 12 electrodes (F7, P8, O1, F8, C4, T7, PO3, Fp1,
Fp2, O2, P3, and Fz) achieve 95.81% accuracy. This study also shows that adding
more than 10 electrodes does not improve performance significantly. Moreover,
the frontal lobe is the most important for recognizing emotion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prodhan_R/0/1/0/all/0/1&quot;&gt;Rumman Ahmed Prodhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akter_S/0/1/0/all/0/1&quot;&gt;Sumya Akter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pias_T/0/1/0/all/0/1&quot;&gt;Tanmoy Sarkar Pias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adnan_M/0/1/0/all/0/1&quot;&gt;Md. Akhtaruzzaman Adnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17218">
<title>BIM: Block-Wise Self-Supervised Learning with Masked Image Modeling. (arXiv:2311.17218v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17218</link>
<description rdf:parseType="Literal">&lt;p&gt;Like masked language modeling (MLM) in natural language processing, masked
image modeling (MIM) aims to extract valuable insights from image patches to
enhance the feature extraction capabilities of the underlying deep neural
network (DNN). Contrasted with other training paradigms like supervised
learning and unsupervised contrastive learning, masked image modeling (MIM)
pretraining typically demands significant computational resources in order to
manage large training data batches (e.g., 4096). The significant memory and
computation requirements pose a considerable challenge to its broad adoption.
To mitigate this, we introduce a novel learning framework,
termed~\textit{Block-Wise Masked Image Modeling} (BIM). This framework involves
decomposing the MIM tasks into several sub-tasks with independent computation
patterns, resulting in block-wise back-propagation operations instead of the
traditional end-to-end approach. Our proposed BIM maintains superior
performance compared to conventional MIM while greatly reducing peak memory
consumption. Moreover, BIM naturally enables the concurrent training of
numerous DNN backbones of varying depths. This leads to the creation of
multiple trained DNN backbones, each tailored to different hardware platforms
with distinct computing capabilities. This approach significantly reduces
computational costs in comparison with training each DNN backbone individually.
Our framework offers a promising solution for resource constrained training of
MIM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yixuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_M/0/1/0/all/0/1&quot;&gt;Mengye Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Sai Qian Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17225">
<title>Invariance assumptions for class distribution estimation. (arXiv:2311.17225v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17225</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of class distribution estimation under dataset shift. On
the training dataset, both features and class labels are observed while on the
test dataset only the features can be observed. The task then is the estimation
of the distribution of the class labels, i.e. the estimation of the class prior
probabilities, in the test dataset. Assumptions of invariance between the
training joint distribution of features and labels and the test distribution
can considerably facilitate this task. We discuss the assumptions of covariate
shift, factorizable joint shift, and sparse joint shift and their implications
for class distribution estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tasche_D/0/1/0/all/0/1&quot;&gt;Dirk Tasche&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17233">
<title>Quantifying the redundancy between prosody and text. (arXiv:2311.17233v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17233</link>
<description rdf:parseType="Literal">&lt;p&gt;Prosody -- the suprasegmental component of speech, including pitch, loudness,
and tempo -- carries critical aspects of meaning. However, the relationship
between the information conveyed by prosody vs. by the words themselves remains
poorly understood. We use large language models (LLMs) to estimate how much
information is redundant between prosody and the words themselves. Using a
large spoken corpus of English audiobooks, we extract prosodic features aligned
to individual words and test how well they can be predicted from LLM
embeddings, compared to non-contextual word embeddings. We find a high degree
of redundancy between the information carried by the words and prosodic
information across several prosodic features, including intensity, duration,
pauses, and pitch contours. Furthermore, a word&apos;s prosodic information is
redundant with both the word itself and the context preceding as well as
following it. Still, we observe that prosodic features can not be fully
predicted from text, suggesting that prosody carries information above and
beyond the words. Along with this paper, we release a general-purpose data
processing pipeline for quantifying the relationship between linguistic
information and extra-linguistic features.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lukas Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pimentel_T/0/1/0/all/0/1&quot;&gt;Tiago Pimentel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedorenko_E/0/1/0/all/0/1&quot;&gt;Evelina Fedorenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cotterell_R/0/1/0/all/0/1&quot;&gt;Ryan Cotterell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warstadt_A/0/1/0/all/0/1&quot;&gt;Alex Warstadt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilcox_E/0/1/0/all/0/1&quot;&gt;Ethan Wilcox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Regev_T/0/1/0/all/0/1&quot;&gt;Tamar Regev&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17250">
<title>Fourier Neural Differential Equations for learning Quantum Field Theories. (arXiv:2311.17250v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17250</link>
<description rdf:parseType="Literal">&lt;p&gt;A Quantum Field Theory is defined by its interaction Hamiltonian, and linked
to experimental data by the scattering matrix. The scattering matrix is
calculated as a perturbative series, and represented succinctly as a first
order differential equation in time. Neural Differential Equations (NDEs) learn
the time derivative of a residual network&apos;s hidden state, and have proven
efficacy in learning differential equations with physical constraints. Hence
using an NDE to learn particle scattering matrices presents a possible
experiment-theory phenomenological connection. In this paper, NDE models are
used to learn $\phi^4$ theory, Scalar-Yukawa theory and Scalar Quantum
Electrodynamics. A new NDE architecture is also introduced, the Fourier Neural
Differential Equation (FNDE), which combines NDE integration and Fourier
network convolution. The FNDE model demonstrates better generalisability than
the non-integrated equivalent FNO model. It is also shown that by training on
scattering data, the interaction Hamiltonian of a theory can be extracted from
network parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brant_I/0/1/0/all/0/1&quot;&gt;Isaac Brant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norcliffe_A/0/1/0/all/0/1&quot;&gt;Alexander Norcliffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Li&amp;#xf2;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17259">
<title>SoUnD Framework: Analyzing (So)cial Representation in (Un)structured (D)ata. (arXiv:2311.17259v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17259</link>
<description rdf:parseType="Literal">&lt;p&gt;The unstructured nature of data used in foundation model development is a
challenge to systematic analyses for making data use and documentation
decisions. From a Responsible AI perspective, these decisions often rely upon
understanding how people are represented in data. We propose a framework
designed to guide analysis of human representation in unstructured data and
identify downstream risks. We apply the framework in two toy examples using the
Common Crawl web text corpus (C4) and LAION-400M. We also propose a set of
hypothetical action steps in service of dataset use, development, and
documentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diaz_M/0/1/0/all/0/1&quot;&gt;Mark D&amp;#xed;az&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dev_S/0/1/0/all/0/1&quot;&gt;Sunipa Dev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reif_E/0/1/0/all/0/1&quot;&gt;Emily Reif&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Denton_R/0/1/0/all/0/1&quot;&gt;Remi Denton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabhakaran_V/0/1/0/all/0/1&quot;&gt;Vinodkumar Prabhakaran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17277">
<title>An Online Optimization-Based Decision Support Tool for Small Farmers in India: Learning in Non-stationary Environments. (arXiv:2311.17277v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17277</link>
<description rdf:parseType="Literal">&lt;p&gt;Crop management decision support systems are specialized tools for farmers
that reduce the riskiness of revenue streams, especially valuable for use under
the current climate changes that impact agricultural productivity.
Unfortunately, small farmers in India, who could greatly benefit from these
tools, do not have access to them. In this paper, we model an individual
greenhouse as a Markov Decision Process (MDP) and adapt Li and Li (2019)&apos;s
Follow the Weighted Leader (FWL) online learning algorithm to offer crop
planning advice. We successfully produce utility-preserving cropping pattern
suggestions in simulations. When we compare against an offline planning
algorithm, we achieve the same cumulative revenue with greatly reduced runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tuxun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prins_A/0/1/0/all/0/1&quot;&gt;Aviva Prins&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17279">
<title>LiveTune: Dynamic Parameter Tuning for Training Deep Neural Networks. (arXiv:2311.17279v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17279</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional machine learning training is a static process that lacks
real-time adaptability of hyperparameters. Popular tuning solutions during
runtime involve checkpoints and schedulers. Adjusting hyper-parameters usually
require the program to be restarted, wasting utilization and time, while
placing unnecessary strain on memory and processors. We present LiveTune, a new
framework allowing real-time parameter tuning during training through
LiveVariables. Live Variables allow for a continuous training session by
storing parameters on designated ports on the system, allowing them to be
dynamically adjusted. Extensive evaluations of our framework show saving up to
60 seconds and 5.4 Kilojoules of energy per hyperparameter change.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabgahi_S/0/1/0/all/0/1&quot;&gt;Soheil Zibakhsh Shabgahi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheybani_N/0/1/0/all/0/1&quot;&gt;Nojan Sheybani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tabrizi_A/0/1/0/all/0/1&quot;&gt;Aiden Tabrizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koushanfar_F/0/1/0/all/0/1&quot;&gt;Farinaz Koushanfar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17287">
<title>Utilizing Model Residuals to Identify Rental Properties of Interest: The Price Anomaly Score (PAS) and Its Application to Real-time Data in Manhattan. (arXiv:2311.17287v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17287</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding whether a property is priced fairly hinders buyers and sellers
since they usually do not have an objective viewpoint of the price distribution
for the overall market of their interest. Drawing from data collected of all
possible available properties for rent in Manhattan as of September 2023, this
paper aims to strengthen our understanding of model residuals; specifically on
machine learning models which generalize for a majority of the distribution of
a well-proportioned dataset. Most models generally perceive deviations from
predicted values as mere inaccuracies, however this paper proposes a different
vantage point: when generalizing to at least 75\% of the data-set, the
remaining deviations reveal significant insights. To harness these insights, we
introduce the Price Anomaly Score (PAS), a metric capable of capturing
boundaries between irregularly predicted prices. By combining relative pricing
discrepancies with statistical significance, the Price Anomaly Score (PAS)
offers a multifaceted view of rental valuations. This metric allows experts to
identify overpriced or underpriced properties within a dataset by aggregating
PAS values, then fine-tuning upper and lower boundaries to any threshold to set
indicators of choice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sultan_Y/0/1/0/all/0/1&quot;&gt;Youssef Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rafter_J/0/1/0/all/0/1&quot;&gt;Jackson C. Rafter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_H/0/1/0/all/0/1&quot;&gt;Huyen T. Nguyen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17299">
<title>Federated Fine-Tuning of Foundation Models via Probabilistic Masking. (arXiv:2311.17299v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17299</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation Models (FMs) have revolutionized machine learning with their
adaptability and high performance across tasks; yet, their integration into
Federated Learning (FL) is challenging due to substantial communication
overhead from their extensive parameterization. Current communication-efficient
FL strategies, such as gradient compression, reduce bitrates to around $1$
bit-per-parameter (bpp). However, these approaches fail to harness the
characteristics of FMs, with their large number of parameters still posing a
challenge to communication efficiency, even at these bitrate regimes. In this
work, we present DeltaMask, a novel method that efficiently fine-tunes FMs in
FL at an ultra-low bitrate, well below 1 bpp. DeltaMask employs stochastic
masking to detect highly effective subnetworks within FMs and leverage
stochasticity and sparsity in client masks to compress updates into a compact
grayscale image using probabilistic filters, deviating from traditional weight
training approaches. Our comprehensive evaluations across various datasets and
architectures demonstrate DeltaMask efficiently achieves bitrates as low as
0.09 bpp, enhancing communication efficiency while maintaining FMs performance,
as measured on 8 datasets and 5 pre-trained models of various network
architectures.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsouvalas_V/0/1/0/all/0/1&quot;&gt;Vasileios Tsouvalas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asano_Y/0/1/0/all/0/1&quot;&gt;Yuki Asano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeed_A/0/1/0/all/0/1&quot;&gt;Aaqib Saeed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17301">
<title>Language Models: A Guide for the Perplexed. (arXiv:2311.17301v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17301</link>
<description rdf:parseType="Literal">&lt;p&gt;Given the growing importance of AI literacy, we decided to write this
tutorial to help narrow the gap between the discourse among those who study
language models -- the core technology underlying ChatGPT and similar products
-- and those who are intrigued and want to learn more about them. In short, we
believe the perspective of researchers and educators can add some clarity to
the public&apos;s understanding of the technologies beyond what&apos;s currently
available, which tends to be either extremely technical or promotional material
generated about products by their purveyors.
&lt;/p&gt;
&lt;p&gt;Our approach teases apart the concept of a language model from products built
on them, from the behaviors attributed to or desired from those products, and
from claims about similarity to human cognition. As a starting point, we (1)
offer a scientific viewpoint that focuses on questions amenable to study
through experimentation; (2) situate language models as they are today in the
context of the research that led to their development; and (3) describe the
boundaries of what is known about the models at this writing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serrano_S/0/1/0/all/0/1&quot;&gt;Sofia Serrano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brumbaugh_Z/0/1/0/all/0/1&quot;&gt;Zander Brumbaugh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smith_N/0/1/0/all/0/1&quot;&gt;Noah A. Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17303">
<title>Enhancing the Performance of Neural Networks Through Causal Discovery and Integration of Domain Knowledge. (arXiv:2311.17303v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17303</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we develop a generic methodology to encode hierarchical
causality structure among observed variables into a neural network in order to
improve its predictive performance. The proposed methodology, called
causality-informed neural network (CINN), leverages three coherent steps to
systematically map the structural causal knowledge into the layer-to-layer
design of neural network while strictly preserving the orientation of every
causal relationship. In the first step, CINN discovers causal relationships
from observational data via directed acyclic graph (DAG) learning, where causal
discovery is recast as a continuous optimization problem to avoid the
combinatorial nature. In the second step, the discovered hierarchical causality
structure among observed variables is systematically encoded into neural
network through a dedicated architecture and customized loss function. By
categorizing variables in the causal DAG as root, intermediate, and leaf nodes,
the hierarchical causal DAG is translated into CINN with a one-to-one
correspondence between nodes in the causal DAG and units in the CINN while
maintaining the relative order among these nodes. Regarding the loss function,
both intermediate and leaf nodes in the DAG graph are treated as target outputs
during CINN training so as to drive co-learning of causal relationships among
different types of nodes. As multiple loss components emerge in CINN, we
leverage the projection of conflicting gradients to mitigate gradient
interference among the multiple learning tasks. Computational experiments
across a broad spectrum of UCI data sets demonstrate substantial advantages of
CINN in predictive performance over other state-of-the-art methods. In
addition, an ablation study underscores the value of integrating structural and
quantitative causal knowledge in enhancing the neural network&apos;s predictive
performance incrementally.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiao-Lin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_F/0/1/0/all/0/1&quot;&gt;Fenglei Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheung_Y/0/1/0/all/0/1&quot;&gt;Yiu-Ming Cheung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bose_I/0/1/0/all/0/1&quot;&gt;Indranil Bose&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17323">
<title>Accelerating DNN Training With Photonics: A Residue Number System-Based Design. (arXiv:2311.17323v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2311.17323</link>
<description rdf:parseType="Literal">&lt;p&gt;Photonic computing is a compelling avenue for performing highly efficient
matrix multiplication, a crucial operation in Deep Neural Networks (DNNs).
While this method has shown great success in DNN inference, meeting the high
precision demands of DNN training proves challenging due to the precision
limitations imposed by costly data converters and the analog noise inherent in
photonic hardware. This paper proposes Mirage, a photonic DNN training
accelerator that overcomes the precision challenges in photonic hardware using
the Residue Number System (RNS). RNS is a numeral system based on modular
arithmetic$\unicode{x2014}$allowing us to perform high-precision operations via
multiple low-precision modular operations. In this work, we present a novel
micro-architecture and dataflow for an RNS-based photonic tensor core
performing modular arithmetic in the analog domain. By combining RNS and
photonics, Mirage provides high energy efficiency without compromising
precision and can successfully train state-of-the-art DNNs achieving accuracy
comparable to FP32 training. Our study shows that on average across several
DNNs when compared to systolic arrays, Mirage achieves more than $23.8\times$
faster training and $32.1\times$ lower EDP in an iso-energy scenario and
consumes $42.8\times$ lower power with comparable or better EDP in an iso-area
scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirkiran_C/0/1/0/all/0/1&quot;&gt;Cansu Demirkiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_G/0/1/0/all/0/1&quot;&gt;Guowei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunandar_D/0/1/0/all/0/1&quot;&gt;Darius Bunandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ajay Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17326">
<title>Mostly Beneficial Clustering: Aggregating Data for Operational Decision Making. (arXiv:2311.17326v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17326</link>
<description rdf:parseType="Literal">&lt;p&gt;With increasingly volatile market conditions and rapid product innovations,
operational decision-making for large-scale systems entails solving thousands
of problems with limited data. Data aggregation is proposed to combine the data
across problems to improve the decisions obtained by solving those problems
individually. We propose a novel cluster-based shrunken-SAA approach that can
exploit the cluster structure among problems when implementing the data
aggregation approaches. We prove that, as the number of problems grows,
leveraging the known cluster structure among problems yields additional
benefits over the data aggregation approaches that neglect such structure. When
the cluster structure is unknown, we show that unveiling the cluster structure,
even at the cost of a few data points, can be beneficial, especially when the
distance between clusters of problems is substantial. Our proposed approach can
be extended to general cost functions under mild conditions. When the number of
problems gets large, the optimality gap of our proposed approach decreases
exponentially in the distance between the clusters. We explore the performance
of the proposed approach through the application of managing newsvendor systems
via numerical experiments. We investigate the impacts of distance metrics
between problem instances on the performance of the cluster-based Shrunken-SAA
approach with synthetic data. We further validate our proposed approach with
real data and highlight the advantages of cluster-based data aggregation,
especially in the small-data large-scale regime, compared to the existing
approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengzhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Z/0/1/0/all/0/1&quot;&gt;Zhenkang Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rong_Y/0/1/0/all/0/1&quot;&gt;Ying Rong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17327">
<title>Improving Self-supervised Molecular Representation Learning using Persistent Homology. (arXiv:2311.17327v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17327</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised learning (SSL) has great potential for molecular
representation learning given the complexity of molecular graphs, the large
amounts of unlabelled data available, the considerable cost of obtaining labels
experimentally, and the hence often only small training datasets. The
importance of the topic is reflected in the variety of paradigms and
architectures that have been investigated recently. Yet the differences in
performance seem often minor and are barely understood to date. In this paper,
we study SSL based on persistent homology (PH), a mathematical tool for
modeling topological features of data that persist across multiple scales. It
has several unique features which particularly suit SSL, naturally offering:
different views of the data, stability in terms of distance preservation, and
the opportunity to flexibly incorporate domain knowledge. We (1) investigate an
autoencoder, which shows the general representational power of PH, and (2)
propose a contrastive loss that complements existing approaches. We rigorously
evaluate our approach for molecular property prediction and demonstrate its
particular features in improving the embedding space: after SSL, the
representations are better and offer considerably more predictive power than
the baselines over different probing tasks; our loss increases baseline
performance, sometimes largely; and we often obtain substantial improvements
over very small datasets, a common scenario in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_Y/0/1/0/all/0/1&quot;&gt;Yuankai Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_L/0/1/0/all/0/1&quot;&gt;Lei Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thost_V/0/1/0/all/0/1&quot;&gt;Veronika Thost&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17352">
<title>Efficient Stitchable Task Adaptation. (arXiv:2311.17352v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17352</link>
<description rdf:parseType="Literal">&lt;p&gt;The paradigm of pre-training and fine-tuning has laid the foundation for
deploying deep learning models. However, most fine-tuning methods are designed
to meet a specific resource budget. Recently, considering diverse deployment
scenarios with various resource budgets, stitchable neural network (SN-Net) is
introduced to quickly obtain numerous new networks (stitches) from the
pre-trained models (anchors) in a model family via model stitching. Although
promising, SN-Net confronts new challenges when adapting it to new target
domains, including huge memory and storage requirements and a long and
sub-optimal multistage adaptation process. In this work, we present a novel
framework, Efficient Stitchable Task Adaptation (ESTA), to efficiently produce
a palette of fine-tuned models that adhere to diverse resource constraints.
Specifically, we first tailor parameter-efficient fine-tuning to share low-rank
updates among the stitches while maintaining independent bias terms. In this
way, we largely reduce fine-tuning memory burdens and mitigate the interference
among stitches that arises in task adaptation. Furthermore, we streamline a
simple yet effective one-stage deployment pipeline, which estimates the
important stitches to deploy with training-time gradient statistics. By
assigning higher sampling probabilities to important stitches, we also get a
boosted Pareto frontier. Extensive experiments on 25 downstream visual
recognition tasks demonstrate that our ESTA is capable of generating stitches
with smooth accuracy-efficiency trade-offs and surpasses the direct SN-Net
adaptation by remarkable margins with significantly lower training time and
fewer trainable parameters. Furthermore, we demonstrate the flexibility and
scalability of our ESTA framework by stitching LLMs from LLaMA family,
obtaining chatbot stitches of assorted sizes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Haoyu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Z/0/1/0/all/0/1&quot;&gt;Zizheng Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jing Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jianfei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_B/0/1/0/all/0/1&quot;&gt;Bohan Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17353">
<title>Continuous optimization by quantum adaptive distribution search. (arXiv:2311.17353v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.17353</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we introduce the quantum adaptive distribution search (QuADS),
a quantum continuous optimization algorithm that integrates Grover adaptive
search (GAS) with the covariance matrix adaptation - evolution strategy
(CMA-ES), a classical technique for continuous optimization. QuADS utilizes the
quantum-based search capabilities of GAS and enhances them with the principles
of CMA-ES for more efficient optimization. It employs a multivariate normal
distribution for the initial state of the quantum search and repeatedly updates
it throughout the optimization process. Our numerical experiments show that
QuADS outperforms both GAS and CMA-ES. This is achieved through adaptive
refinement of the initial state distribution rather than consistently using a
uniform state, resulting in fewer oracle calls. This study presents an
important step toward exploiting the potential of quantum computing for
continuous optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Morimoto_K/0/1/0/all/0/1&quot;&gt;Kohei Morimoto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Takase_Y/0/1/0/all/0/1&quot;&gt;Yusuke Takase&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Mitarai_K/0/1/0/all/0/1&quot;&gt;Kosuke Mitarai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Fujii_K/0/1/0/all/0/1&quot;&gt;Keisuke Fujii&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17373">
<title>The Devil is in the Data: Learning Fair Graph Neural Networks via Partial Knowledge Distillation. (arXiv:2311.17373v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17373</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) are being increasingly used in many high-stakes
tasks, and as a result, there is growing attention on their fairness recently.
GNNs have been shown to be unfair as they tend to make discriminatory decisions
toward certain demographic groups, divided by sensitive attributes such as
gender and race. While recent works have been devoted to improving their
fairness performance, they often require accessible demographic information.
This greatly limits their applicability in real-world scenarios due to legal
restrictions. To address this problem, we present a demographic-agnostic method
to learn fair GNNs via knowledge distillation, namely FairGKD. Our work is
motivated by the empirical observation that training GNNs on partial data
(i.e., only node attributes or topology data) can improve their fairness,
albeit at the cost of utility. To make a balanced trade-off between fairness
and utility performance, we employ a set of fairness experts (i.e., GNNs
trained on different partial data) to construct the synthetic teacher, which
distills fairer and informative knowledge to guide the learning of the GNN
student. Experiments on several benchmark datasets demonstrate that FairGKD,
which does not require access to demographic information, significantly
improves the fairness of GNNs by a large margin while maintaining their
utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yuchang Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jintang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Liang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Z/0/1/0/all/0/1&quot;&gt;Zibin Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17400">
<title>Improving the Robustness of Transformer-based Large Language Models with Dynamic Attention. (arXiv:2311.17400v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17400</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformer-based models, such as BERT and GPT, have been widely adopted in
natural language processing (NLP) due to their exceptional performance.
However, recent studies show their vulnerability to textual adversarial attacks
where the model&apos;s output can be misled by intentionally manipulating the text
inputs. Despite various methods that have been proposed to enhance the model&apos;s
robustness and mitigate this vulnerability, many require heavy consumption
resources (e.g., adversarial training) or only provide limited protection
(e.g., defensive dropout). In this paper, we propose a novel method called
dynamic attention, tailored for the transformer architecture, to enhance the
inherent robustness of the model itself against various adversarial attacks.
Our method requires no downstream task knowledge and does not incur additional
costs. The proposed dynamic attention consists of two modules: (I) attention
rectification, which masks or weakens the attention value of the chosen tokens,
and (ii) dynamic modeling, which dynamically builds the set of candidate
tokens. Extensive experiments demonstrate that dynamic attention significantly
mitigates the impact of adversarial attacks, improving up to 33\% better
performance than previous methods against widely-used adversarial attacks. The
model-level design of dynamic attention enables it to be easily combined with
other defense methods (e.g., adversarial training) to further enhance the
model&apos;s robustness. Furthermore, we demonstrate that dynamic attention
preserves the state-of-the-art robustness space of the original model compared
to other dynamic modeling methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_L/0/1/0/all/0/1&quot;&gt;Lujia Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yuwen Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_S/0/1/0/all/0/1&quot;&gt;Shouling Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Changjiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_C/0/1/0/all/0/1&quot;&gt;Chunpeng Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Ting Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17401">
<title>Gene-MOE: A Sparsely-gated Framework for Pan-Cancer Genomic Analysis. (arXiv:2311.17401v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17401</link>
<description rdf:parseType="Literal">&lt;p&gt;Analyzing the genomic information from the Pan-Cancer database can help us
understand cancer-related factors and contribute to the cancer diagnosis and
prognosis. However, existing computational methods and deep learning methods
can not effectively find the deep correlations between tens of thousands of
genes, which leads to precision loss. In this paper, we proposed a novel
pretrained model called Gene-MOE to learn the general feature representations
of the Pan-Cancer dataset and transfer the pretrained weights to the downstream
tasks. The Gene-MOE fully exploits the mixture of expert (MOE) layers to learn
rich feature representations of high-dimensional genes. At the same time, we
build a mixture of attention expert (MOAE) model to learn the deep semantic
relationships within genetic features. Finally, we proposed a new
self-supervised pretraining strategy including loss function design, data
enhancement, and optimization strategy to train the Gene-MOE and further
improve the performance for the downstream analysis. We carried out cancer
classification and survival analysis experiments based on the Gene-MOE.
According to the survival analysis results on 14 cancer types, using Gene-MOE
outperformed state-of-the-art models on 12 cancer types. According to the
classification results, the total accuracy of the classification model for 33
cancer classifications reached 95.2\%. Through detailed feature analysis, we
found the Gene-MOE model can learn rich feature representations of
high-dimensional genes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xiangyu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_T/0/1/0/all/0/1&quot;&gt;Tao Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_H/0/1/0/all/0/1&quot;&gt;Huanhuan Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_L/0/1/0/all/0/1&quot;&gt;Lian Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_H/0/1/0/all/0/1&quot;&gt;Hongzhen Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1&quot;&gt;Long Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xun Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17410">
<title>GNNFlow: A Distributed Framework for Continuous Temporal GNN Learning on Dynamic Graphs. (arXiv:2311.17410v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2311.17410</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph Neural Networks (GNNs) play a crucial role in various fields. However,
most existing deep graph learning frameworks assume pre-stored static graphs
and do not support training on graph streams. In contrast, many real-world
graphs are dynamic and contain time domain information. We introduce GNNFlow, a
distributed framework that enables efficient continuous temporal graph
representation learning on dynamic graphs on multi-GPU machines. GNNFlow
introduces an adaptive time-indexed block-based data structure that effectively
balances memory usage with graph update and sampling operation efficiency. It
features a hybrid GPU-CPU graph data placement for rapid GPU-based temporal
neighborhood sampling and kernel optimizations for enhanced sampling processes.
A dynamic GPU cache for node and edge features is developed to maximize cache
hit rates through reuse and restoration strategies. GNNFlow supports
distributed training across multiple machines with static scheduling to ensure
load balance. We implement GNNFlow based on DGL and PyTorch. Our experimental
results show that GNNFlow provides up to 21.1x faster continuous learning than
existing systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_Y/0/1/0/all/0/1&quot;&gt;Yuchen Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sheng_G/0/1/0/all/0/1&quot;&gt;Guangming Sheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_T/0/1/0/all/0/1&quot;&gt;Tianzuo Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Minjie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_Q/0/1/0/all/0/1&quot;&gt;Quan Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chuan Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17431">
<title>Grounding Foundation Models through Federated Transfer Learning: A General Framework. (arXiv:2311.17431v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17431</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation Models (FMs) such as GPT-4 encoded with vast knowledge and
powerful emergent abilities have achieved remarkable success in various natural
language processing and computer vision tasks. Grounding FMs by adapting them
to domain-specific tasks or augmenting them with domain-specific knowledge
enables us to exploit the full potential of FMs. However, grounding FMs faces
several challenges, stemming primarily from constrained computing resources,
data privacy, model heterogeneity, and model ownership. Federated Transfer
Learning (FTL), the combination of federated learning and transfer learning,
provides promising solutions to address these challenges. In recent years, the
need for grounding FMs leveraging FTL, coined FTL-FM, has arisen strongly in
both academia and industry. Motivated by the strong growth in FTL-FM research
and the potential impact of FTL-FM on industrial applications, we propose an
FTL-FM framework that formulates problems of grounding FMs in the federated
learning setting, construct a detailed taxonomy based on the FTL-FM framework
to categorize state-of-the-art FTL-FM works, and comprehensively overview
FTL-FM works based on the proposed taxonomy. We also establish correspondences
between FTL-FM and conventional phases of adapting FM so that FM practitioners
can align their research works with FTL-FM. In addition, we overview advanced
efficiency-improving and privacy-preserving techniques because efficiency and
privacy are critical concerns in FTL-FM. Last, we discuss opportunities and
future research directions of FTL-FM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yan Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_T/0/1/0/all/0/1&quot;&gt;Tao Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hanlin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_L/0/1/0/all/0/1&quot;&gt;Lixin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qiang Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17434">
<title>Group-wise Sparse and Explainable Adversarial Attacks. (arXiv:2311.17434v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17434</link>
<description rdf:parseType="Literal">&lt;p&gt;Sparse adversarial attacks fool deep neural networks (DNNs) through minimal
pixel perturbations, typically regularized by the $\ell_0$ norm. Recent efforts
have replaced this norm with a structural sparsity regularizer, such as the
nuclear group norm, to craft group-wise sparse adversarial attacks. The
resulting perturbations are thus explainable and hold significant practical
relevance, shedding light on an even greater vulnerability of DNNs than
previously anticipated. However, crafting such attacks poses an optimization
challenge, as it involves computing norms for groups of pixels within a
non-convex objective. In this paper, we tackle this challenge by presenting an
algorithm that simultaneously generates group-wise sparse attacks within
semantically meaningful areas of an image. In each iteration, the core
operation of our algorithm involves the optimization of a quasinorm adversarial
loss. This optimization is achieved by employing the $1/2$-quasinorm proximal
operator for some iterations, a method tailored for nonconvex programming.
Subsequently, the algorithm transitions to a projected Nesterov&apos;s accelerated
gradient descent with $2$-norm regularization applied to perturbation
magnitudes. We rigorously evaluate the efficacy of our novel attack in both
targeted and non-targeted attack scenarios, on CIFAR-10 and ImageNet datasets.
When compared to state-of-the-art methods, our attack consistently results in a
remarkable increase in group-wise sparsity, e.g., an increase of $48.12\%$ on
CIFAR-10 and $40.78\%$ on ImageNet (average case, targeted attack), all while
maintaining lower perturbation magnitudes. Notably, this performance is
complemented by a significantly faster computation time and a $100\%$ attack
success rate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sadiku_S/0/1/0/all/0/1&quot;&gt;Shpresim Sadiku&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_M/0/1/0/all/0/1&quot;&gt;Moritz Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pokutta_S/0/1/0/all/0/1&quot;&gt;Sebastian Pokutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17446">
<title>Uncertainty in Additive Feature Attribution methods. (arXiv:2311.17446v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17446</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we explore various topics that fall under the umbrella of
Uncertainty in post-hoc Explainable AI (XAI) methods. We in particular focus on
the class of additive feature attribution explanation methods. We first
describe our specifications of uncertainty and compare various statistical and
recent methods to quantify the same. Next, for a particular instance, we study
the relationship between a feature&apos;s attribution and its uncertainty and
observe little correlation. As a result, we propose a modification in the
distribution from which perturbations are sampled in LIME-based algorithms such
that the important features have minimal uncertainty without an increase in
computational cost. Next, while studying how the uncertainty in explanations
varies across the feature space of a classifier, we observe that a fraction of
instances show near-zero uncertainty. We coin the term &quot;stable instances&quot; for
such instances and diagnose factors that make an instance stable. Next, we
study how an XAI algorithm&apos;s uncertainty varies with the size and complexity of
the underlying model. We observe that the more complex the model, the more
inherent uncertainty is exhibited by it. As a result, we propose a measure to
quantify the relative complexity of a blackbox classifier. This could be
incorporated, for example, in LIME-based algorithms&apos; sampling densities, to
help different explanation algorithms achieve tighter confidence levels.
Together, the above measures would have a strong impact on making XAI models
relatively trustworthy for the end-user as well as aiding scientific discovery.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Madaan_A/0/1/0/all/0/1&quot;&gt;Abhishek Madaan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_T/0/1/0/all/0/1&quot;&gt;Tanya Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rana_N/0/1/0/all/0/1&quot;&gt;Neha Rana&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allan_J/0/1/0/all/0/1&quot;&gt;James Allan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_T/0/1/0/all/0/1&quot;&gt;Tanmoy Chakraborty&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17451">
<title>Wireless Network Digital Twin for 6G: Generative AI as A Key Enabler. (arXiv:2311.17451v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2311.17451</link>
<description rdf:parseType="Literal">&lt;p&gt;Digital twin, which enables emulation, evaluation, and optimization of
physical entities through synchronized digital replicas, has gained
increasingly attention as a promising technology for intricate wireless
networks. For 6G, numerous innovative wireless technologies and network
architectures have posed new challenges in establishing wireless network
digital twins. To tackle these challenges, artificial intelligence (AI),
particularly the flourishing generative AI, emerges as a potential solution. In
this article, we discuss emerging prerequisites for wireless network digital
twins considering the complicated network architecture, tremendous network
scale, extensive coverage, and diversified application scenarios in the 6G era.
We further explore the applications of generative AI, such as transformer and
diffusion model, to empower the 6G digital twin from multiple perspectives
including implementation, physical-digital synchronization, and slicing
capability. Subsequently, we propose a hierarchical generative AI-enabled
wireless network digital twin at both the message-level and policy-level, and
provide a typical use case with numerical results to validate the effectiveness
and efficiency. Finally, open research issues for wireless network digital
twins in the 6G era are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1&quot;&gt;Zhenyu Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wei Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yongming Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaoyun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_X/0/1/0/all/0/1&quot;&gt;Xiaohu You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17466">
<title>Slot-Mixup with Subsampling: A Simple Regularization for WSI Classification. (arXiv:2311.17466v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17466</link>
<description rdf:parseType="Literal">&lt;p&gt;Whole slide image (WSI) classification requires repetitive zoom-in and out
for pathologists, as only small portions of the slide may be relevant to
detecting cancer. Due to the lack of patch-level labels, multiple instance
learning (MIL) is a common practice for training a WSI classifier. One of the
challenges in MIL for WSIs is the weak supervision coming only from the
slide-level labels, often resulting in severe overfitting. In response,
researchers have considered adopting patch-level augmentation or applying mixup
augmentation, but their applicability remains unverified. Our approach augments
the training dataset by sampling a subset of patches in the WSI without
significantly altering the underlying semantics of the original slides.
Additionally, we introduce an efficient model (Slot-MIL) that organizes patches
into a fixed number of slots, the abstract representation of patches, using an
attention mechanism. We empirically demonstrate that the subsampling
augmentation helps to make more informative slots by restricting the
over-concentration of attention and to improve interpretability. Finally, we
illustrate that combining our attention-based aggregation model with
subsampling and mixup, which has shown limited compatibility in existing MIL
methods, can enhance both generalization and calibration. Our proposed methods
achieve the state-of-the-art performance across various benchmark datasets
including class imbalance and distribution shifts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keum_S/0/1/0/all/0/1&quot;&gt;Seongho Keum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanghyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Soojeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Juho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17508">
<title>Model Performance Prediction for Hyperparameter Optimization of Deep Learning Models Using High Performance Computing and Quantum Annealing. (arXiv:2311.17508v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17508</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperparameter Optimization (HPO) of Deep Learning-based models tends to be a
compute resource intensive process as it usually requires to train the target
model with many different hyperparameter configurations. We show that
integrating model performance prediction with early stopping methods holds
great potential to speed up the HPO process of deep learning models. Moreover,
we propose a novel algorithm called Swift-Hyperband that can use either
classical or quantum support vector regression for performance prediction and
benefit from distributed High Performance Computing environments. This
algorithm is tested not only for the Machine-Learned Particle Flow model used
in High Energy Physics, but also for a wider range of target models from
domains such as computer vision and natural language processing.
Swift-Hyperband is shown to find comparable (or better) hyperparameters as well
as using less computational resources in all test cases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Amboage_J/0/1/0/all/0/1&quot;&gt;Juan Pablo Garc&amp;#xed;a Amboage&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wulff_E/0/1/0/all/0/1&quot;&gt;Eric Wulff&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girone_M/0/1/0/all/0/1&quot;&gt;Maria Girone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pena_T/0/1/0/all/0/1&quot;&gt;Tom&amp;#xe1;s F. Pena&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17518">
<title>The devil is in the fine-grained details: Evaluating open-vocabulary object detectors for fine-grained understanding. (arXiv:2311.17518v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17518</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advancements in large vision-language models enabled visual object
detection in open-vocabulary scenarios, where object classes are defined in
free-text formats during inference. In this paper, we aim to probe the
state-of-the-art methods for open-vocabulary object detection to determine to
what extent they understand fine-grained properties of objects and their parts.
To this end, we introduce an evaluation protocol based on dynamic vocabulary
generation to test whether models detect, discern, and assign the correct
fine-grained description to objects in the presence of hard-negative classes.
We contribute with a benchmark suite of increasing difficulty and probing
different properties like color, pattern, and material. We further enhance our
investigation by evaluating several state-of-the-art open-vocabulary object
detectors using the proposed protocol and find that most existing solutions,
which shine in standard open-vocabulary benchmarks, struggle to accurately
capture and distinguish finer object details. We conclude the paper by
highlighting the limitations of current methodologies and exploring promising
research directions to overcome the discovered drawbacks. Data and code are
available at https://github.com/lorebianchi98/FG-OVD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bianchi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Bianchi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrara_F/0/1/0/all/0/1&quot;&gt;Fabio Carrara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Messina_N/0/1/0/all/0/1&quot;&gt;Nicola Messina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gennaro_C/0/1/0/all/0/1&quot;&gt;Claudio Gennaro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Falchi_F/0/1/0/all/0/1&quot;&gt;Fabrizio Falchi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17539">
<title>The Effects of Overparameterization on Sharpness-aware Minimization: An Empirical and Theoretical Analysis. (arXiv:2311.17539v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17539</link>
<description rdf:parseType="Literal">&lt;p&gt;Training an overparameterized neural network can yield minimizers of the same
level of training loss and yet different generalization capabilities. With
evidence that indicates a correlation between sharpness of minima and their
generalization errors, increasing efforts have been made to develop an
optimization method to explicitly find flat minima as more generalizable
solutions. This sharpness-aware minimization (SAM) strategy, however, has not
been studied much yet as to how overparameterization can actually affect its
behavior. In this work, we analyze SAM under varying degrees of
overparameterization and present both empirical and theoretical results that
suggest a critical influence of overparameterization on SAM. Specifically, we
first use standard techniques in optimization to prove that SAM can achieve a
linear convergence rate under overparameterization in a stochastic setting. We
also show that the linearly stable minima found by SAM are indeed flatter and
have more uniformly distributed Hessian moments compared to those of SGD. These
results are corroborated with our experiments that reveal a consistent trend
that the generalization improvement made by SAM continues to increase as the
model becomes more overparameterized. We further present that sparsity can open
up an avenue for effective overparameterization in practice.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_S/0/1/0/all/0/1&quot;&gt;Sungbin Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dongyeop Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andriushchenko_M/0/1/0/all/0/1&quot;&gt;Maksym Andriushchenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_N/0/1/0/all/0/1&quot;&gt;Namhoon Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17552">
<title>An Efficient Illumination Invariant Tiger Detection Framework for Wildlife Surveillance. (arXiv:2311.17552v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17552</link>
<description rdf:parseType="Literal">&lt;p&gt;Tiger conservation necessitates the strategic deployment of multifaceted
initiatives encompassing the preservation of ecological habitats, anti-poaching
measures, and community involvement for sustainable growth in the tiger
population. With the advent of artificial intelligence, tiger surveillance can
be automated using object detection. In this paper, an accurate illumination
invariant framework is proposed based on EnlightenGAN and YOLOv8 for tiger
detection. The fine-tuned YOLOv8 model achieves a mAP score of 61% without
illumination enhancement. The illumination enhancement improves the mAP by
0.7%. The approaches elevate the state-of-the-art performance on the ATRW
dataset by approximately 6% to 7%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pendharkar_G/0/1/0/all/0/1&quot;&gt;Gaurav Pendharkar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Micheal_A/0/1/0/all/0/1&quot;&gt;A.Ancy Micheal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misquitta_J/0/1/0/all/0/1&quot;&gt;Jason Misquitta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaippada_R/0/1/0/all/0/1&quot;&gt;Ranjeesh Kaippada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17560">
<title>Interpreting Differentiable Latent States for Healthcare Time-series Data. (arXiv:2311.17560v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17560</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning enables extracting clinical insights from large temporal
datasets. The applications of such machine learning models include identifying
disease patterns and predicting patient outcomes. However, limited
interpretability poses challenges for deploying advanced machine learning in
digital healthcare. Understanding the meaning of latent states is crucial for
interpreting machine learning models, assuming they capture underlying
patterns. In this paper, we present a concise algorithm that allows for i)
interpreting latent states using highly related input features; ii)
interpreting predictions using subsets of input features via latent states; and
iii) interpreting changes in latent states over time. The proposed algorithm is
feasible for any model that is differentiable. We demonstrate that this
approach enables the identification of a daytime behavioral pattern for
predicting nocturnal behavior in a real-world healthcare dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bijlani_N/0/1/0/all/0/1&quot;&gt;Nivedita Bijlani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kouchaki_S/0/1/0/all/0/1&quot;&gt;Samaneh Kouchaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnaghi_P/0/1/0/all/0/1&quot;&gt;Payam Barnaghi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17565">
<title>Bias Resilient Multi-Step Off-Policy Goal-Conditioned Reinforcement Learning. (arXiv:2311.17565v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17565</link>
<description rdf:parseType="Literal">&lt;p&gt;In goal-conditioned reinforcement learning (GCRL), sparse rewards present
significant challenges, often obstructing efficient learning. Although
multi-step GCRL can boost this efficiency, it can also lead to off-policy
biases in target values. This paper dives deep into these biases, categorizing
them into two distinct categories: &quot;shooting&quot; and &quot;shifting&quot;. Recognizing that
certain behavior policies can hasten policy refinement, we present solutions
designed to capitalize on the positive aspects of these biases while minimizing
their drawbacks, enabling the use of larger step sizes to speed up GCRL. An
empirical study demonstrates that our approach ensures a resilient and robust
improvement, even in ten-step learning scenarios, leading to superior learning
efficiency and performance that generally surpass the baseline and several
state-of-the-art multi-step GCRL benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lisheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Ke Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17582">
<title>LoCoMotif: Discovering time-warped motifs in time series. (arXiv:2311.17582v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17582</link>
<description rdf:parseType="Literal">&lt;p&gt;Time Series Motif Discovery (TSMD) refers to the task of identifying patterns
that occur multiple times (possibly with minor variations) in a time series.
All existing methods for TSMD have one or more of the following limitations:
they only look for the two most similar occurrences of a pattern; they only
look for patterns of a pre-specified, fixed length; they cannot handle
variability along the time axis; and they only handle univariate time series.
In this paper, we present a new method, LoCoMotif, that has none of these
limitations. The method is motivated by a concrete use case from physiotherapy.
We demonstrate the value of the proposed method on this use case. We also
introduce a new quantitative evaluation metric for motif discovery, and
benchmark data for comparing TSMD methods. LoCoMotif substantially outperforms
the existing methods, on top of being more broadly applicable.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wesenbeeck_D/0/1/0/all/0/1&quot;&gt;Daan Van Wesenbeeck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yurtman_A/0/1/0/all/0/1&quot;&gt;Aras Yurtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meert_W/0/1/0/all/0/1&quot;&gt;Wannes Meert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Blockeel_H/0/1/0/all/0/1&quot;&gt;Hendrik Blockeel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17586">
<title>Federated Online and Bandit Convex Optimization. (arXiv:2311.17586v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17586</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problems of distributed online and bandit convex optimization
against an adaptive adversary. We aim to minimize the average regret on $M$
machines working in parallel over $T$ rounds with $R$ intermittent
communications. Assuming the underlying cost functions are convex and can be
generated adaptively, our results show that collaboration is not beneficial
when the machines have access to the first-order gradient information at the
queried points. This is in contrast to the case for stochastic functions, where
each machine samples the cost functions from a fixed distribution. Furthermore,
we delve into the more challenging setting of federated online optimization
with bandit (zeroth-order) feedback, where the machines can only access values
of the cost functions at the queried points. The key finding here is
identifying the high-dimensional regime where collaboration is beneficial and
may even lead to a linear speedup in the number of machines. We further
illustrate our findings through federated adversarial linear bandits by
developing novel distributed single and two-point feedback algorithms. Our work
is the first attempt towards a systematic understanding of federated online
optimization with limited feedback, and it attains tight regret bounds in the
intermittent communication setting for both first and zeroth-order feedback.
Our results thus bridge the gap between stochastic and adaptive settings in
federated online optimization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1&quot;&gt;Kumar Kshitij Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saha_A/0/1/0/all/0/1&quot;&gt;Aadirupa Saha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebro_N/0/1/0/all/0/1&quot;&gt;Nati Sebro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17593">
<title>LanGWM: Language Grounded World Model. (arXiv:2311.17593v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17593</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in deep reinforcement learning have showcased its potential
in tackling complex tasks. However, experiments on visual control tasks have
revealed that state-of-the-art reinforcement learning models struggle with
out-of-distribution generalization. Conversely, expressing higher-level
concepts and global contexts is relatively easy using language.
&lt;/p&gt;
&lt;p&gt;Building upon recent success of the large language models, our main objective
is to improve the state abstraction technique in reinforcement learning by
leveraging language for robust action selection. Specifically, we focus on
learning language-grounded visual features to enhance the world model learning,
a model-based reinforcement learning technique.
&lt;/p&gt;
&lt;p&gt;To enforce our hypothesis explicitly, we mask out the bounding boxes of a few
objects in the image observation and provide the text prompt as descriptions
for these masked objects. Subsequently, we predict the masked objects along
with the surrounding regions as pixel reconstruction, similar to the
transformer-based masked autoencoder approach.
&lt;/p&gt;
&lt;p&gt;Our proposed LanGWM: Language Grounded World Model achieves state-of-the-art
performance in out-of-distribution test at the 100K interaction steps
benchmarks of iGibson point navigation tasks. Furthermore, our proposed
technique of explicit language-grounded visual representation learning has the
potential to improve models for human-robot interaction because our extracted
visual features are language grounded.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poudel_R/0/1/0/all/0/1&quot;&gt;Rudra P.K. Poudel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pandya_H/0/1/0/all/0/1&quot;&gt;Harit Pandya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cipolla_R/0/1/0/all/0/1&quot;&gt;Roberto Cipolla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17598">
<title>Improving embedding of graphs with missing data by soft manifolds. (arXiv:2311.17598v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17598</link>
<description rdf:parseType="Literal">&lt;p&gt;Embedding graphs in continous spaces is a key factor in designing and
developing algorithms for automatic information extraction to be applied in
diverse tasks (e.g., learning, inferring, predicting). The reliability of graph
embeddings directly depends on how much the geometry of the continuous space
matches the graph structure. Manifolds are mathematical structure that can
enable to incorporate in their topological spaces the graph characteristics,
and in particular nodes distances. State-of-the-art of manifold-based graph
embedding algorithms take advantage of the assumption that the projection on a
tangential space of each point in the manifold (corresponding to a node in the
graph) would locally resemble a Euclidean space. Although this condition helps
in achieving efficient analytical solutions to the embedding problem, it does
not represent an adequate set-up to work with modern real life graphs, that are
characterized by weighted connections across nodes often computed over sparse
datasets with missing records. In this work, we introduce a new class of
manifold, named soft manifold, that can solve this situation. In particular,
soft manifolds are mathematical structures with spherical symmetry where the
tangent spaces to each point are hypocycloids whose shape is defined according
to the velocity of information propagation across the data points. Using soft
manifolds for graph embedding, we can provide continuous spaces to pursue any
task in data analysis over complex datasets. Experimental results on
reconstruction tasks on synthetic and real datasets show how the proposed
approach enable more accurate and reliable characterization of graphs in
continuous spaces with respect to the state-of-the-art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marinoni_A/0/1/0/all/0/1&quot;&gt;Andrea Marinoni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lio_P/0/1/0/all/0/1&quot;&gt;Pietro Lio&amp;#x27;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barp_A/0/1/0/all/0/1&quot;&gt;Alessandro Barp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jutten_C/0/1/0/all/0/1&quot;&gt;Christian Jutten&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girolami_M/0/1/0/all/0/1&quot;&gt;Mark Girolami&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17601">
<title>Continual Learning with Low Rank Adaptation. (arXiv:2311.17601v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17601</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent work using pretrained transformers has shown impressive performance
when fine-tuned with data from the downstream problem of interest. However,
they struggle to retain that performance when the data characteristics changes.
In this paper, we focus on continual learning, where a pre-trained transformer
is updated to perform well on new data, while retaining its performance on data
it was previously trained on. Earlier works have tackled this primarily through
methods inspired from prompt tuning. We question this choice, and investigate
the applicability of Low Rank Adaptation (LoRA) to continual learning. On a
range of domain-incremental learning benchmarks, our LoRA-based solution,
CoLoR, yields state-of-the-art performance, while still being as parameter
efficient as the prompt tuning based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wistuba_M/0/1/0/all/0/1&quot;&gt;Martin Wistuba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sivaprasad_P/0/1/0/all/0/1&quot;&gt;Prabhu Teja Sivaprasad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balles_L/0/1/0/all/0/1&quot;&gt;Lukas Balles&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zappella_G/0/1/0/all/0/1&quot;&gt;Giovanni Zappella&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17607">
<title>Topology-Preserving Adversarial Training. (arXiv:2311.17607v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17607</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the effectiveness in improving the robustness of neural networks,
adversarial training has suffered from the natural accuracy degradation
problem, i.e., accuracy on natural samples has reduced significantly. In this
study, we reveal that natural accuracy degradation is highly related to the
disruption of the natural sample topology in the representation space by
quantitative and qualitative experiments. Based on this observation, we propose
Topology-pReserving Adversarial traINing (TRAIN) to alleviate the problem by
preserving the topology structure of natural samples from a standard model
trained only on natural samples during adversarial training. As an additional
regularization, our method can easily be combined with various popular
adversarial training algorithms in a plug-and-play manner, taking advantage of
both sides. Extensive experiments on CIFAR-10, CIFAR-100, and Tiny ImageNet
show that our proposed method achieves consistent and significant improvements
over various strong baselines in most cases. Specifically, without additional
data, our proposed method achieves up to 8.78% improvement in natural accuracy
and 4.50% improvement in robust accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weng_Y/0/1/0/all/0/1&quot;&gt;Yepeng Weng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Danding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Juan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Sheng Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17608">
<title>Adversarial Robust Memory-Based Continual Learner. (arXiv:2311.17608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17608</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the remarkable advances that have been made in continual learning,
the adversarial vulnerability of such methods has not been fully discussed. We
delve into the adversarial robustness of memory-based continual learning
algorithms and observe limited robustness improvement by directly applying
adversarial training techniques. Preliminary studies reveal the twin challenges
for building adversarial robust continual learners: accelerated forgetting in
continual learning and gradient obfuscation in adversarial robustness. In this
study, we put forward a novel adversarial robust memory-based continual learner
that adjusts data logits to mitigate the forgetting of pasts caused by
adversarial samples. Furthermore, we devise a gradient-based data selection
mechanism to overcome the gradient obfuscation caused by limited stored data.
The proposed approach can widely integrate with existing memory-based continual
learning as well as adversarial training algorithms in a plug-and-play way.
Extensive experiments on Split-CIFAR10/100 and Split-Tiny-ImageNet demonstrate
the effectiveness of our approach, achieving up to 8.13% higher accuracy for
adversarial data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mi_X/0/1/0/all/0/1&quot;&gt;Xiaoyue Mi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_F/0/1/0/all/0/1&quot;&gt;Fan Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zonghan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Danding Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_J/0/1/0/all/0/1&quot;&gt;Juan Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17609">
<title>AnyLens: A Generative Diffusion Model with Any Rendering Lens. (arXiv:2311.17609v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17609</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art diffusion models can generate highly realistic images based
on various conditioning like text, segmentation, and depth. However, an
essential aspect often overlooked is the specific camera geometry used during
image capture. The influence of different optical systems on the final scene
appearance is frequently overlooked. This study introduces a framework that
intimately integrates a text-to-image diffusion model with the particular lens
geometry used in image rendering. Our method is based on a per-pixel coordinate
conditioning method, enabling the control over the rendering geometry. Notably,
we demonstrate the manipulation of curvature properties, achieving diverse
visual effects, such as fish-eye, panoramic views, and spherical texturing
using a single diffusion model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Voynov_A/0/1/0/all/0/1&quot;&gt;Andrey Voynov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hertz_A/0/1/0/all/0/1&quot;&gt;Amir Hertz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arar_M/0/1/0/all/0/1&quot;&gt;Moab Arar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fruchter_S/0/1/0/all/0/1&quot;&gt;Shlomi Fruchter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Or_D/0/1/0/all/0/1&quot;&gt;Daniel Cohen-Or&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17631">
<title>Q-learning Based Optimal False Data Injection Attack on Probabilistic Boolean Control Networks. (arXiv:2311.17631v1 [eess.SY])</title>
<link>http://arxiv.org/abs/2311.17631</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a reinforcement learning (RL) method for solving
optimal false data injection attack problems in probabilistic Boolean control
networks (PBCNs) where the attacker lacks knowledge of the system model.
Specifically, we employ a Q-learning (QL) algorithm to address this problem. We
then propose an improved QL algorithm that not only enhances learning
efficiency but also obtains optimal attack strategies for large-scale PBCNs
that the standard QL algorithm cannot handle. Finally, we verify the
effectiveness of our proposed approach by considering two attacked PBCNs,
including a 10-node network and a 28-node network.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Peng_X/0/1/0/all/0/1&quot;&gt;Xianlun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Fangfei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17633">
<title>Introduction to Transformers: an NLP Perspective. (arXiv:2311.17633v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17633</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers have dominated empirical machine learning models of natural
language processing. In this paper, we introduce basic concepts of Transformers
and present key techniques that form the recent advances of these models. This
includes a description of the standard Transformer architecture, a series of
model refinements, and common applications. Given that Transformers and related
deep learning techniques might be evolving in ways we have never seen, we
cannot dive into all the model details or cover all the technical areas.
Instead, we focus on just those concepts that are helpful for gaining a good
understanding of Transformers and their variants. We also summarize the key
ideas that impact this field, thereby yielding some insights into the strengths
and limitations of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_T/0/1/0/all/0/1&quot;&gt;Tong Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jingbo Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17646">
<title>A novel feature selection method based on quantum support vector machine. (arXiv:2311.17646v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2311.17646</link>
<description rdf:parseType="Literal">&lt;p&gt;Feature selection is critical in machine learning to reduce dimensionality
and improve model accuracy and efficiency. The exponential growth in feature
space dimensionality for modern datasets directly results in ambiguous samples
and redundant features, which can severely degrade classification accuracy.
Quantum machine learning offers potential advantages for addressing this
challenge. In this paper, we propose a novel method, quantum support vector
machine feature selection (QSVMF), integrating quantum support vector machines
with multi-objective genetic algorithm. QSVMF optimizes multiple simultaneous
objectives: maximizing classification accuracy, minimizing selected features
and quantum circuit costs, and reducing feature covariance. We apply QSVMF for
feature selection on a breast cancer dataset, comparing the performance of
QSVMF against classical approaches with the selected features. Experimental
results show that QSVMF achieves superior performance. Furthermore, The Pareto
front solutions of QSVMF enable analysis of accuracy versus feature set size
trade-offs, identifying extremely sparse yet accurate feature subsets. We
contextualize the biological relevance of the selected features in terms of
known breast cancer biomarkers. This work highlights the potential of
quantum-based feature selection to enhance machine learning efficiency and
performance on complex real-world data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Haiyan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17673">
<title>Using Ornstein-Uhlenbeck Process to understand Denoising Diffusion Probabilistic Model and its Noise Schedules. (arXiv:2311.17673v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2311.17673</link>
<description rdf:parseType="Literal">&lt;p&gt;The aim of this short note is to show that Denoising Diffusion Probabilistic
Model DDPM, a non-homogeneous discrete-time Markov process, can be represented
by a time-homogeneous continuous-time Markov process observed at non-uniformly
sampled discrete times. Surprisingly, this continuous-time Markov process is
the well-known and well-studied Ornstein-Ohlenbeck (OU) process, which was
developed in 1930&apos;s for studying Brownian particles in Harmonic potentials. We
establish the formal equivalence between DDPM and the OU process using its
analytical solution. We further demonstrate that the design problem of the
noise scheduler for non-homogeneous DDPM is equivalent to designing observation
times for the OU process. We present several heuristic designs for observation
times based on principled quantities such as auto-variance and Fisher
Information and connect them to ad hoc noise schedules for DDPM. Interestingly,
we show that the Fisher-Information-motivated schedule corresponds exactly the
cosine schedule, which was developed without any theoretical foundation but is
the current state-of-the-art noise schedule.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Santos_J/0/1/0/all/0/1&quot;&gt;Javier E. Santos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yen Ting Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17693">
<title>Toward a Surgeon-in-the-Loop Ophthalmic Robotic Apprentice using Reinforcement and Imitation Learning. (arXiv:2311.17693v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.17693</link>
<description rdf:parseType="Literal">&lt;p&gt;Robotic-assisted surgical systems have demonstrated significant potential in
enhancing surgical precision and minimizing human errors. However, existing
systems lack the ability to accommodate the unique preferences and requirements
of individual surgeons. Additionally, they primarily focus on general surgeries
(e.g., laparoscopy) and are not suitable for highly precise microsurgeries,
such as ophthalmic procedures. Thus, we propose a simulation-based image-guided
approach for surgeon-centered autonomous agents that can adapt to the
individual surgeon&apos;s skill level and preferred surgical techniques during
ophthalmic cataract surgery. Our approach utilizes a simulated environment to
train reinforcement and imitation learning agents guided by image data to
perform all tasks of the incision phase of cataract surgery. By integrating the
surgeon&apos;s actions and preferences into the training process with the
surgeon-in-the-loop, our approach enables the robot to implicitly learn and
adapt to the individual surgeon&apos;s unique approach through demonstrations. This
results in a more intuitive and personalized surgical experience for the
surgeon. Simultaneously, it ensures consistent performance for the autonomous
robotic apprentice. We define and evaluate the effectiveness of our approach
using our proposed metrics; and highlight the trade-off between a generic agent
and a surgeon-centered adapted agent. Moreover, our approach has the potential
to extend to other ophthalmic surgical procedures, opening the door to a new
generation of surgeon-in-the-loop autonomous surgical robots. We provide an
open-source simulation framework for future development and reproducibility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomaa_A/0/1/0/all/0/1&quot;&gt;Amr Gomaa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mahdy_B/0/1/0/all/0/1&quot;&gt;Bilal Mahdy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kleer_N/0/1/0/all/0/1&quot;&gt;Niko Kleer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kruger_A/0/1/0/all/0/1&quot;&gt;Antonio Kr&amp;#xfc;ger&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17695">
<title>Fair Text-to-Image Diffusion via Fair Mapping. (arXiv:2311.17695v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17695</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the limitations of existing text-to-image diffusion
models in generating demographically fair results when given human-related
descriptions. These models often struggle to disentangle the target language
context from sociocultural biases, resulting in biased image generation. To
overcome this challenge, we propose Fair Mapping, a general, model-agnostic,
and lightweight approach that modifies a pre-trained text-to-image model by
controlling the prompt to achieve fair image generation. One key advantage of
our approach is its high efficiency. The training process only requires
updating a small number of parameters in an additional linear mapping network.
This not only reduces the computational cost but also accelerates the
optimization process. We first demonstrate the issue of bias in generated
results caused by language biases in text-guided diffusion models. By
developing a mapping network that projects language embeddings into an unbiased
space, we enable the generation of relatively balanced demographic results
based on a keyword specified in the prompt. With comprehensive experiments on
face image generation, we show that our method significantly improves image
generation performance when prompted with descriptions related to human faces.
By effectively addressing the issue of bias, we produce more fair and diverse
image outputs. This work contributes to the field of text-to-image generation
by enhancing the ability to generate images that accurately reflect the
intended demographic characteristics specified in the text.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_L/0/1/0/all/0/1&quot;&gt;Lijie Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_T/0/1/0/all/0/1&quot;&gt;Tianhang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17717">
<title>Receler: Reliable Concept Erasing of Text-to-Image Diffusion Models via Lightweight Erasers. (arXiv:2311.17717v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17717</link>
<description rdf:parseType="Literal">&lt;p&gt;Concept erasure in text-to-image diffusion models aims to disable pre-trained
diffusion models from generating images related to a target concept. To perform
reliable concept erasure, the properties of robustness and locality are
desirable. The former refrains the model from producing images associated with
the target concept for any paraphrased or learned prompts, while the latter
preserves the model ability in generating images for non-target concepts. In
this paper, we propose Reliable Concept Erasing via Lightweight Erasers
(Receler), which learns a lightweight Eraser to perform concept erasing and
enhances locality and robustness with the proposed concept-localized
regularization and adversarial prompt learning, respectively. Comprehensive
quantitative and qualitative experiments with various concept prompts verify
the superiority of Receler over the previous erasing methods on the above two
desirable properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chi-Pin Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_K/0/1/0/all/0/1&quot;&gt;Kai-Po Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsai_C/0/1/0/all/0/1&quot;&gt;Chung-Ting Tsai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yung-Hsuan Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu-Chiang Frank Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17722">
<title>SenTest: Evaluating Robustness of Sentence Encoders. (arXiv:2311.17722v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2311.17722</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning has proven to be an effective method for pre-training
models using weakly labeled data in the vision domain. Sentence transformers
are the NLP counterparts to this architecture, and have been growing in
popularity due to their rich and effective sentence representations. Having
effective sentence representations is paramount in multiple tasks, such as
information retrieval, retrieval augmented generation (RAG), and sentence
comparison. Keeping in mind the deployability factor of transformers,
evaluating the robustness of sentence transformers is of utmost importance.
This work focuses on evaluating the robustness of the sentence encoders. We
employ several adversarial attacks to evaluate its robustness. This system uses
character-level attacks in the form of random character substitution,
word-level attacks in the form of synonym replacement, and sentence-level
attacks in the form of intra-sentence word order shuffling. The results of the
experiments strongly undermine the robustness of sentence encoders. The models
produce significantly different predictions as well as embeddings on perturbed
datasets. The accuracy of the models can fall up to 15 percent on perturbed
datasets as compared to unperturbed datasets. Furthermore, the experiments
demonstrate that these embeddings does capture the semantic and syntactic
structure (sentence order) of sentences. However, existing supervised
classification strategies fail to leverage this information, and merely
function as n-gram detectors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chavan_T/0/1/0/all/0/1&quot;&gt;Tanmay Chavan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patankar_S/0/1/0/all/0/1&quot;&gt;Shantanu Patankar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kane_A/0/1/0/all/0/1&quot;&gt;Aditya Kane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokhale_O/0/1/0/all/0/1&quot;&gt;Omkar Gokhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kale_G/0/1/0/all/0/1&quot;&gt;Geetanjali Kale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_R/0/1/0/all/0/1&quot;&gt;Raviraj Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17740">
<title>A transductive few-shot learning approach for classification of digital histopathological slides from liver cancer. (arXiv:2311.17740v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2311.17740</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a new approach for classifying 2D histopathology patches
using few-shot learning. The method is designed to tackle a significant
challenge in histopathology, which is the limited availability of labeled data.
By applying a sliding window technique to histopathology slides, we illustrate
the practical benefits of transductive learning (i.e., making joint predictions
on patches) to achieve consistent and accurate classification. Our approach
involves an optimization-based strategy that actively penalizes the prediction
of a large number of distinct classes within each window. We conducted
experiments on histopathological data to classify tissue classes in digital
slides of liver cancer, specifically hepatocellular carcinoma. The initial
results show the effectiveness of our method and its potential to enhance the
process of automated cancer diagnosis and treatment, all while reducing the
time and effort required for expert annotation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sadraoui_A/0/1/0/all/0/1&quot;&gt;Aymen Sadraoui&lt;/a&gt; (OPIS, CVN), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Martin_S/0/1/0/all/0/1&quot;&gt;S&amp;#xe9;gol&amp;#xe8;ne Martin&lt;/a&gt; (OPIS, CVN), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Barbot_E/0/1/0/all/0/1&quot;&gt;Eliott Barbot&lt;/a&gt; (OPIS, CVN), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Laurent_Bellue_A/0/1/0/all/0/1&quot;&gt;Astrid Laurent-Bellue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pesquet_J/0/1/0/all/0/1&quot;&gt;Jean-Christophe Pesquet&lt;/a&gt; (OPIS, CVN), &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Guettier_C/0/1/0/all/0/1&quot;&gt;Catherine Guettier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ayed_I/0/1/0/all/0/1&quot;&gt;Ismail Ben Ayed&lt;/a&gt; (ETS)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17750">
<title>Addressing Membership Inference Attack in Federated Learning with Model Compression. (arXiv:2311.17750v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17750</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has been proposed as a privacy-preserving solution
for machine learning. However, recent works have shown that Federated Learning
can leak private client data through membership attacks. In this paper, we show
that the effectiveness of these attacks on the clients negatively correlates
with the size of the client datasets and model complexity. Based on this
finding, we propose model-agnostic Federated Learning as a privacy-enhancing
solution because it enables the use of models of varying complexity in the
clients. To this end, we present $\texttt{MaPP-FL}$, a novel privacy-aware FL
approach that leverages model compression on the clients while keeping a full
model on the server. We compare the performance of $\texttt{MaPP-FL}$ against
state-of-the-art model-agnostic FL methods on the CIFAR-10, CIFAR-100, and
FEMNIST vision datasets. Our experiments show the effectiveness of
$\texttt{MaPP-FL}$ in preserving the clients&apos; and the server&apos;s privacy while
achieving competitive classification accuracies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nemeth_G/0/1/0/all/0/1&quot;&gt;Gergely D&amp;#xe1;niel N&amp;#xe9;meth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lozano_M/0/1/0/all/0/1&quot;&gt;Miguel &amp;#xc1;ngel Lozano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Quadrianto_N/0/1/0/all/0/1&quot;&gt;Novi Quadrianto&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oliver_N/0/1/0/all/0/1&quot;&gt;Nuria Oliver&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17778">
<title>Unified Binary and Multiclass Margin-Based Classification. (arXiv:2311.17778v1 [stat.ML])</title>
<link>http://arxiv.org/abs/2311.17778</link>
<description rdf:parseType="Literal">&lt;p&gt;The notion of margin loss has been central to the development and analysis of
algorithms for binary classification. To date, however, there remains no
consensus as to the analogue of the margin loss for multiclass classification.
In this work, we show that a broad range of multiclass loss functions,
including many popular ones, can be expressed in the relative margin form, a
generalization of the margin form of binary losses. The relative margin form is
broadly useful for understanding and analyzing multiclass losses as shown by
our prior work (Wang and Scott, 2020, 2021). To further demonstrate the utility
of this way of expressing multiclass losses, we use it to extend the seminal
result of Bartlett et al. (2006) on classification-calibration of binary margin
losses to multiclass. We then analyze the class of Fenchel-Young losses, and
expand the set of these losses that are known to be classification-calibrated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yutong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Scott_C/0/1/0/all/0/1&quot;&gt;Clayton Scott&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17781">
<title>Propagate &amp; Distill: Towards Effective Graph Learners Using Propagation-Embracing MLPs. (arXiv:2311.17781v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17781</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies attempted to utilize multilayer perceptrons (MLPs) to solve
semisupervised node classification on graphs, by training a student MLP by
knowledge distillation from a teacher graph neural network (GNN). While
previous studies have focused mostly on training the student MLP by matching
the output probability distributions between the teacher and student models
during distillation, it has not been systematically studied how to inject the
structural information in an explicit and interpretable manner. Inspired by
GNNs that separate feature transformation $T$ and propagation $\Pi$, we
re-frame the distillation process as making the student MLP learn both $T$ and
$\Pi$. Although this can be achieved by applying the inverse propagation
$\Pi^{-1}$ before distillation from the teacher, it still comes with a high
computational cost from large matrix multiplications during training. To solve
this problem, we propose Propagate &amp;amp; Distill (P&amp;amp;D), which propagates the output
of the teacher before distillation, which can be interpreted as an approximate
process of the inverse propagation. We demonstrate that P&amp;amp;D can readily improve
the performance of the student MLP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_Y/0/1/0/all/0/1&quot;&gt;Yong-Min Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_W/0/1/0/all/0/1&quot;&gt;Won-Yong Shin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17795">
<title>Marginal Laplacian Score. (arXiv:2311.17795v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17795</link>
<description rdf:parseType="Literal">&lt;p&gt;High-dimensional imbalanced data poses a machine learning challenge. In the
absence of sufficient or high-quality labels, unsupervised feature selection
methods are crucial for the success of subsequent algorithms. Therefore, there
is a growing need for unsupervised feature selection algorithms focused on
imbalanced data. Thus, we propose a Marginal Laplacian Score (MLS) a
modification of the well-known Laplacian Score (LS) to be better suited for
imbalance data. We introduce an assumption that the minority class or anomalous
appear more frequently in the margin of the features. Consequently, MLS aims to
preserve the local structure of the data set&apos;s margin. As MLS is better suited
for handling imbalanced data, we propose its integration into modern feature
selection methods that utilize the Laplacian score. We integrate the MLS
algorithm into the Differentiable Unsupervised Feature Selection (DUFS),
resulting in DUFS-MLS. The proposed methods demonstrate robust and improved
performance on synthetic and public data sets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hay_G/0/1/0/all/0/1&quot;&gt;Guy Hay&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Volk_O/0/1/0/all/0/1&quot;&gt;Ohad Volk&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17797">
<title>Learning to Simulate: Generative Metamodeling via Quantile Regression. (arXiv:2311.17797v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17797</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic simulation models, while effective in capturing the dynamics of
complex systems, are often too slow to run for real-time decision-making.
Metamodeling techniques are widely used to learn the relationship between a
summary statistic of the outputs (e.g., the mean or quantile) and the inputs of
the simulator, so that it can be used in real time. However, this methodology
requires the knowledge of an appropriate summary statistic in advance, making
it inflexible for many practical situations. In this paper, we propose a new
metamodeling concept, called generative metamodeling, which aims to construct a
&quot;fast simulator of the simulator&quot;. This technique can generate random outputs
substantially faster than the original simulation model, while retaining an
approximately equal conditional distribution given the same inputs. Once
constructed, a generative metamodel can instantaneously generate a large amount
of random outputs as soon as the inputs are specified, thereby facilitating the
immediate computation of any summary statistic for real-time decision-making.
Furthermore, we propose a new algorithm -- quantile-regression-based generative
metamodeling (QRGMM) -- and study its convergence and rate of convergence.
Extensive numerical experiments are conducted to investigate the empirical
performance of QRGMM, compare it with other state-of-the-art generative
algorithms, and demonstrate its usefulness in practical real-time
decision-making.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_L/0/1/0/all/0/1&quot;&gt;L. Jeff Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_Y/0/1/0/all/0/1&quot;&gt;Yanxi Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qingkai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17801">
<title>Towards Efficient Hyperdimensional Computing Using Photonics. (arXiv:2311.17801v1 [cs.ET])</title>
<link>http://arxiv.org/abs/2311.17801</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past few years, silicon photonics-based computing has emerged as a
promising alternative to CMOS-based computing for Deep Neural Networks (DNN).
Unfortunately, the non-linear operations and the high-precision requirements of
DNNs make it extremely challenging to design efficient silicon photonics-based
systems for DNN inference and training. Hyperdimensional Computing (HDC) is an
emerging, brain-inspired machine learning technique that enjoys several
advantages over existing DNNs, including being lightweight, requiring
low-precision operands, and being robust to noise introduced by the
nonidealities in the hardware. For HDC, computing in-memory (CiM) approaches
have been widely used, as CiM reduces the data transfer cost if the operands
can fit into the memory. However, inefficient multi-bit operations, high write
latency, and low endurance make CiM ill-suited for HDC. On the other hand, the
existing electro-photonic DNN accelerators are inefficient for HDC because they
are specifically optimized for matrix multiplication in DNNs and consume a lot
of power with high-precision data converters.
&lt;/p&gt;
&lt;p&gt;In this paper, we argue that photonic computing and HDC complement each other
better than photonic computing and DNNs, or CiM and HDC. We propose PhotoHDC,
the first-ever electro-photonic accelerator for HDC training and inference,
supporting the basic, record-based, and graph encoding schemes. Evaluating with
popular datasets, we show that our accelerator can achieve two to five orders
of magnitude lower EDP than the state-of-the-art electro-photonic DNN
accelerators for implementing HDC training and inference. PhotoHDC also
achieves four orders of magnitude lower energy-delay product than CiM-based
accelerators for both HDC training and inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fayza_F/0/1/0/all/0/1&quot;&gt;Farbin Fayza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Demirkiran_C/0/1/0/all/0/1&quot;&gt;Cansu Demirkiran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanning Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che-Kai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohan_A/0/1/0/all/0/1&quot;&gt;Avi Mohan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Errahmouni_H/0/1/0/all/0/1&quot;&gt;Hamza Errahmouni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sanggeon Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imani_M/0/1/0/all/0/1&quot;&gt;Mohsen Imani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_D/0/1/0/all/0/1&quot;&gt;David Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bunandar_D/0/1/0/all/0/1&quot;&gt;Darius Bunandar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_A/0/1/0/all/0/1&quot;&gt;Ajay Joshi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17833">
<title>Analyzing and Explaining Image Classifiers via Diffusion Guidance. (arXiv:2311.17833v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2311.17833</link>
<description rdf:parseType="Literal">&lt;p&gt;While deep learning has led to huge progress in complex image classification
tasks like ImageNet, unexpected failure modes, e.g. via spurious features, call
into question how reliably these classifiers work in the wild. Furthermore, for
safety-critical tasks the black-box nature of their decisions is problematic,
and explanations or at least methods which make decisions plausible are needed
urgently. In this paper, we address these problems by generating images that
optimize a classifier-derived objective using a framework for guided image
generation. We analyze the behavior and decisions of image classifiers by
visual counterfactual explanations (VCEs), detection of systematic mistakes by
analyzing images where classifiers maximally disagree, and visualization of
neurons to verify potential spurious features. In this way, we validate
existing observations, e.g. the shape bias of adversarially robust models, as
well as novel failure modes, e.g. systematic errors of zero-shot CLIP
classifiers, or identify harmful spurious features. Moreover, our VCEs
outperform previous work while being more versatile.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Augustin_M/0/1/0/all/0/1&quot;&gt;Maximilian Augustin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuhaus_Y/0/1/0/all/0/1&quot;&gt;Yannic Neuhaus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hein_M/0/1/0/all/0/1&quot;&gt;Matthias Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17840">
<title>A quasi-polynomial time algorithm for Multi-Dimensional Scaling via LP hierarchies. (arXiv:2311.17840v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2311.17840</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-dimensional Scaling (MDS) is a family of methods for embedding
pair-wise dissimilarities between $n$ objects into low-dimensional space. MDS
is widely used as a data visualization tool in the social and biological
sciences, statistics, and machine learning. We study the Kamada-Kawai
formulation of MDS: given a set of non-negative dissimilarities $\{d_{i,j}\}_{i
, j \in [n]}$ over $n$ points, the goal is to find an embedding
$\{x_1,\dots,x_n\} \subset \mathbb{R}^k$ that minimizes \[ \text{OPT} =
\min_{x} \mathbb{E}_{i,j \in [n]} \left[ \left(1-\frac{\|x_i -
x_j\|}{d_{i,j}}\right)^2 \right] \]
&lt;/p&gt;
&lt;p&gt;Despite its popularity, our theoretical understanding of MDS is extremely
limited. Recently, Demaine, Hesterberg, Koehler, Lynch, and Urschel
(&lt;a href=&quot;/abs/2109.11505&quot;&gt;arXiv:2109.11505&lt;/a&gt;) gave the first approximation algorithm with provable
guarantees for Kamada-Kawai, which achieves an embedding with cost $\text{OPT}
+\epsilon$ in $n^2 \cdot 2^{\tilde{\mathcal{O}}(k \Delta^4 / \epsilon^2)}$
time, where $\Delta$ is the aspect ratio of the input dissimilarities. In this
work, we give the first approximation algorithm for MDS with quasi-polynomial
dependency on $\Delta$: for target dimension $k$, we achieve a solution with
cost $\mathcal{O}(\text{OPT}^{ \hspace{0.04in}1/k } \cdot \log(\Delta/\epsilon)
)+ \epsilon$ in time $n^{ \mathcal{O}(1)} \cdot 2^{\tilde{\mathcal{O}}( k^2
(\log(\Delta)/\epsilon)^{k/2 + 1} ) }$.
&lt;/p&gt;
&lt;p&gt;Our approach is based on a novel analysis of a conditioning-based rounding
scheme for the Sherali-Adams LP Hierarchy. Crucially, our analysis exploits the
geometry of low-dimensional Euclidean space, allowing us to avoid an
exponential dependence on the aspect ratio $\Delta$. We believe our
geometry-aware treatment of the Sherali-Adams Hierarchy is an important step
towards developing general-purpose techniques for efficient metric optimization
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakshi_A/0/1/0/all/0/1&quot;&gt;Ainesh Bakshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cohen_Addad_V/0/1/0/all/0/1&quot;&gt;Vincent Cohen-Addad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hopkins_S/0/1/0/all/0/1&quot;&gt;Samuel B. Hopkins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayaram_R/0/1/0/all/0/1&quot;&gt;Rajesh Jayaram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lattanzi_S/0/1/0/all/0/1&quot;&gt;Silvio Lattanzi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17842">
<title>Look Before You Leap: Unveiling the Power of GPT-4V in Robotic Vision-Language Planning. (arXiv:2311.17842v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2311.17842</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we are interested in imbuing robots with the capability of
physically-grounded task planning. Recent advancements have shown that large
language models (LLMs) possess extensive knowledge useful in robotic tasks,
especially in reasoning and planning. However, LLMs are constrained by their
lack of world grounding and dependence on external affordance models to
perceive environmental information, which cannot jointly reason with LLMs. We
argue that a task planner should be an inherently grounded, unified multimodal
system. To this end, we introduce Robotic Vision-Language Planning (ViLa), a
novel approach for long-horizon robotic planning that leverages vision-language
models (VLMs) to generate a sequence of actionable steps. ViLa directly
integrates perceptual data into its reasoning and planning process, enabling a
profound understanding of commonsense knowledge in the visual world, including
spatial layouts and object attributes. It also supports flexible multimodal
goal specification and naturally incorporates visual feedback. Our extensive
evaluation, conducted in both real-robot and simulated environments,
demonstrates ViLa&apos;s superiority over existing LLM-based planners, highlighting
its effectiveness in a wide array of open-world manipulation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yingdong Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_F/0/1/0/all/0/1&quot;&gt;Fanqi Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_L/0/1/0/all/0/1&quot;&gt;Li Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17853">
<title>On the Adversarial Robustness of Graph Contrastive Learning Methods. (arXiv:2311.17853v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17853</link>
<description rdf:parseType="Literal">&lt;p&gt;Contrastive learning (CL) has emerged as a powerful framework for learning
representations of images and text in a self-supervised manner while enhancing
model robustness against adversarial attacks. More recently, researchers have
extended the principles of contrastive learning to graph-structured data,
giving birth to the field of graph contrastive learning (GCL). However, whether
GCL methods can deliver the same advantages in adversarial robustness as their
counterparts in the image and text domains remains an open question. In this
paper, we introduce a comprehensive robustness evaluation protocol tailored to
assess the robustness of GCL models. We subject these models to adaptive
adversarial attacks targeting the graph structure, specifically in the evasion
scenario. We evaluate node and graph classification tasks using diverse
real-world datasets and attack strategies. With our work, we aim to offer
insights into the robustness of GCL methods and hope to open avenues for
potential future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guerranti_F/0/1/0/all/0/1&quot;&gt;Filippo Guerranti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_Z/0/1/0/all/0/1&quot;&gt;Zinuo Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Starovoit_A/0/1/0/all/0/1&quot;&gt;Anna Starovoit&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamel_R/0/1/0/all/0/1&quot;&gt;Rafiq Kamel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geisler_S/0/1/0/all/0/1&quot;&gt;Simon Geisler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gunnemann_S/0/1/0/all/0/1&quot;&gt;Stephan G&amp;#xfc;nnemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17855">
<title>Maximum Entropy Model Correction in Reinforcement Learning. (arXiv:2311.17855v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17855</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose and theoretically analyze an approach for planning with an
approximate model in reinforcement learning that can reduce the adverse impact
of model error. If the model is accurate enough, it accelerates the convergence
to the true value function too. One of its key components is the MaxEnt Model
Correction (MoCo) procedure that corrects the model&apos;s next-state distributions
based on a Maximum Entropy density estimation formulation. Based on MoCo, we
introduce the Model Correcting Value Iteration (MoCoVI) algorithm, and its
sampled-based variant MoCoDyna. We show that MoCoVI and MoCoDyna&apos;s convergence
can be much faster than the conventional model-free algorithms. Unlike
traditional model-based algorithms, MoCoVI and MoCoDyna effectively utilize an
approximate model and still converge to the correct value function.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rakhsha_A/0/1/0/all/0/1&quot;&gt;Amin Rakhsha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kemertas_M/0/1/0/all/0/1&quot;&gt;Mete Kemertas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghavamzadeh_M/0/1/0/all/0/1&quot;&gt;Mohammad Ghavamzadeh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1&quot;&gt;Amir-massoud Farahmand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.17856">
<title>Leveraging Graph Diffusion Models for Network Refinement Tasks. (arXiv:2311.17856v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2311.17856</link>
<description rdf:parseType="Literal">&lt;p&gt;Most real-world networks are noisy and incomplete samples from an unknown
target distribution. Refining them by correcting corruptions or inferring
unobserved regions typically improves downstream performance. Inspired by the
impressive generative capabilities that have been used to correct corruptions
in images, and the similarities between &quot;in-painting&quot; and filling in missing
nodes and edges conditioned on the observed graph, we propose a novel graph
generative framework, SGDM, which is based on subgraph diffusion. Our framework
not only improves the scalability and fidelity of graph diffusion models, but
also leverages the reverse process to perform novel, conditional generation
tasks. In particular, through extensive empirical analysis and a set of novel
metrics, we demonstrate that our proposed model effectively supports the
following refinement tasks for partially observable networks: T1: denoising
extraneous subgraphs, T2: expanding existing subgraphs and T3: performing
&quot;style&quot; transfer by regenerating a particular subgraph to match the
characteristics of a different node or subgraph.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trivedi_P/0/1/0/all/0/1&quot;&gt;Puja Trivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_R/0/1/0/all/0/1&quot;&gt;Ryan Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arbour_D/0/1/0/all/0/1&quot;&gt;David Arbour&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_T/0/1/0/all/0/1&quot;&gt;Tong Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dernoncourt_F/0/1/0/all/0/1&quot;&gt;Franck Dernoncourt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sungchul Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipka_N/0/1/0/all/0/1&quot;&gt;Nedim Lipka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_N/0/1/0/all/0/1&quot;&gt;Namyong Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nesreen K. Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutra_D/0/1/0/all/0/1&quot;&gt;Danai Koutra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2006.09017">
<title>Estimates on Learning Rates for Multi-Penalty Distribution Regression. (arXiv:2006.09017v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2006.09017</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper is concerned with functional learning by utilizing two-stage
sampled distribution regression. We study a multi-penalty regularization
algorithm for distribution regression under the framework of learning theory.
The algorithm aims at regressing to real valued outputs from probability
measures. The theoretical analysis on distribution regression is far from
maturity and quite challenging, since only second stage samples are observable
in practical setting. In the algorithm, to transform information from samples,
we embed the distributions to a reproducing kernel Hilbert space
$\mathcal{H}_K$ associated with Mercer kernel $K$ via mean embedding technique.
The main contribution of the paper is to present a novel multi-penalty
regularization algorithm to capture more features of distribution regression
and derive optimal learning rates for the algorithm. The work also derives
learning rates for distribution regression in the nonstandard setting
$f_{\rho}\notin\mathcal{H}_K$, which is not explored in existing literature.
Moreover, we propose a distribution regression-based distributed learning
algorithm to face large-scale data or information challenge. The optimal
learning rates are derived for the distributed learning algorithm. By providing
new algorithms and showing their learning rates, we improve the existing work
in different aspects in the literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhan Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ho_D/0/1/0/all/0/1&quot;&gt;Daniel W. C. Ho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2102.07737">
<title>Zero-Shot Self-Supervised Learning for MRI Reconstruction. (arXiv:2102.07737v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2102.07737</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) has emerged as a powerful tool for accelerated MRI
reconstruction, but often necessitates a database of fully-sampled measurements
for training. Recent self-supervised and unsupervised learning approaches
enable training without fully-sampled data. However, a database of undersampled
measurements may not be available in many scenarios, especially for scans
involving contrast or translational acquisitions in development. Moreover,
recent studies show that database-trained models may not generalize well when
the unseen measurements differ in terms of sampling pattern, acceleration rate,
SNR, image contrast, and anatomy. Such challenges necessitate a new methodology
to enable subject-specific DL MRI reconstruction without external training
datasets, since it is clinically imperative to provide high-quality
reconstructions that can be used to identify lesions/disease for \emph{every
individual}. In this work, we propose a zero-shot self-supervised learning
approach to perform subject-specific accelerated DL MRI reconstruction to
tackle these issues. The proposed approach partitions the available
measurements from a single scan into three disjoint sets. Two of these sets are
used to enforce data consistency and define loss during training for
self-supervision, while the last set serves to self-validate, establishing an
early stopping criterion. In the presence of models pre-trained on a database
with different image characteristics, we show that the proposed approach can be
combined with transfer learning for faster convergence time and reduced
computational complexity. The code is available at
\url{https://github.com/byaman14/ZS-SSL}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yaman_B/0/1/0/all/0/1&quot;&gt;Burhaneddin Yaman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hosseini_S/0/1/0/all/0/1&quot;&gt;Seyed Amir Hossein Hosseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Akcakaya_M/0/1/0/all/0/1&quot;&gt;Mehmet Ak&amp;#xe7;akaya&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.04442">
<title>A Primer on Deep Learning for Causal Inference. (arXiv:2110.04442v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.04442</link>
<description rdf:parseType="Literal">&lt;p&gt;This review systematizes the emerging literature for causal inference using
deep neural networks under the potential outcomes framework. It provides an
intuitive introduction on how deep learning can be used to estimate/predict
heterogeneous treatment effects and extend causal inference to settings where
confounding is non-linear, time varying, or encoded in text, networks, and
images. To maximize accessibility, we also introduce prerequisite concepts from
causal inference and deep learning. The survey differs from other treatments of
deep learning and causal inference in its sharp focus on observational causal
estimation, its extended exposition of key algorithms, and its detailed
tutorials for implementing, training, and selecting among deep estimators in
Tensorflow 2 available at github.com/kochbj/Deep-Learning-for-Causal-Inference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_B/0/1/0/all/0/1&quot;&gt;Bernard Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sainburg_T/0/1/0/all/0/1&quot;&gt;Tim Sainburg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Geraldo_P/0/1/0/all/0/1&quot;&gt;Pablo Geraldo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_S/0/1/0/all/0/1&quot;&gt;Song Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yizhou Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foster_J/0/1/0/all/0/1&quot;&gt;Jacob Gates Foster&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.12403">
<title>Learning to Estimate Without Bias. (arXiv:2110.12403v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2110.12403</link>
<description rdf:parseType="Literal">&lt;p&gt;The Gauss Markov theorem states that the weighted least squares estimator is
a linear minimum variance unbiased estimation (MVUE) in linear models. In this
paper, we take a first step towards extending this result to non linear
settings via deep learning with bias constraints. The classical approach to
designing non-linear MVUEs is through maximum likelihood estimation (MLE) which
often involves computationally challenging optimizations. On the other hand,
deep learning methods allow for non-linear estimators with fixed computational
complexity. Learning based estimators perform optimally on average with respect
to their training set but may suffer from significant bias in other parameters.
To avoid this, we propose to add a simple bias constraint to the loss function,
resulting in an estimator we refer to as Bias Constrained Estimator (BCE). We
prove that this yields asymptotic MVUEs that behave similarly to the classical
MLEs and asymptotically attain the Cramer Rao bound. We demonstrate the
advantages of our approach in the context of signal to noise ratio estimation
as well as covariance estimation. A second motivation to BCE is in applications
where multiple estimates of the same unknown are averaged for improved
performance. Examples include distributed sensor networks and data augmentation
in test-time. In such applications, we show that BCE leads to asymptotically
consistent estimators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diskin_T/0/1/0/all/0/1&quot;&gt;Tzvi Diskin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eldar_Y/0/1/0/all/0/1&quot;&gt;Yonina C. Eldar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wiesel_A/0/1/0/all/0/1&quot;&gt;Ami Wiesel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.14053">
<title>NeuroBack: Improving CDCL SAT Solving using Graph Neural Networks. (arXiv:2110.14053v6 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2110.14053</link>
<description rdf:parseType="Literal">&lt;p&gt;Propositional satisfiability (SAT) is an NP-complete problem that impacts
many research fields, such as planning, verification, and security. Mainstream
modern SAT solvers are based on the Conflict-Driven Clause Learning (CDCL)
algorithm. Recent work aimed to enhance CDCL SAT solvers using Graph Neural
Networks (GNNs). However, so far this approach either has not made solving more
effective, or required substantial GPU resources for frequent online model
inferences. Aiming to make GNN improvements practical, this paper proposes an
approach called NeuroBack, which builds on two insights: (1) predicting phases
(i.e., values) of variables appearing in the majority (or even all) of the
satisfying assignments are essential for CDCL SAT solving, and (2) it is
sufficient to query the neural model only once for the predictions before the
SAT solving starts. Once trained, the offline model inference allows NeuroBack
to execute exclusively on the CPU, removing its reliance on GPU resources. To
train NeuroBack, a new dataset called DataBack containing 120,286 data samples
is created. Finally, NeuroBack is implemented as an enhancement to a
state-of-the-art SAT solver called Kissat. As a result, it allowed Kissat to
solve 5.2% more problems on the recent SAT competition problem set,
SATCOMP-2022. NeuroBack therefore shows how machine learning can be harnessed
to improve SAT solving in an effective and practical manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenxi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yang Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tiwari_M/0/1/0/all/0/1&quot;&gt;Mohit Tiwari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khurshid_S/0/1/0/all/0/1&quot;&gt;Sarfraz Khurshid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McMillan_K/0/1/0/all/0/1&quot;&gt;Kenneth McMillan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Miikkulainen_R/0/1/0/all/0/1&quot;&gt;Risto Miikkulainen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.03865">
<title>Universalizing Weak Supervision. (arXiv:2112.03865v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2112.03865</link>
<description rdf:parseType="Literal">&lt;p&gt;Weak supervision (WS) frameworks are a popular way to bypass hand-labeling
large datasets for training data-hungry models. These approaches synthesize
multiple noisy but cheaply-acquired estimates of labels into a set of
high-quality pseudolabels for downstream training. However, the synthesis
technique is specific to a particular kind of label, such as binary labels or
sequences, and each new label type requires manually designing a new synthesis
algorithm. Instead, we propose a universal technique that enables weak
supervision over any label type while still offering desirable properties,
including practical flexibility, computational efficiency, and theoretical
guarantees. We apply this technique to important problems previously not
tackled by WS frameworks including learning to rank, regression, and learning
in hyperbolic space. Theoretically, our synthesis approach produces a
consistent estimators for learning some challenging but important
generalizations of the exponential family model. Experimentally, we validate
our framework and show improvement over baselines in diverse settings including
real-world learning-to-rank and regression problems along with learning on
hyperbolic manifolds.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Changho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Winfred Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vishwakarma_H/0/1/0/all/0/1&quot;&gt;Harit Vishwakarma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roberts_N/0/1/0/all/0/1&quot;&gt;Nicholas Roberts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2202.03349">
<title>Conditional Gradients for the Approximate Vanishing Ideal. (arXiv:2202.03349v15 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2202.03349</link>
<description rdf:parseType="Literal">&lt;p&gt;The vanishing ideal of a set of points $X\subseteq \mathbb{R}^n$ is the set
of polynomials that evaluate to $0$ over all points $\mathbf{x} \in X$ and
admits an efficient representation by a finite set of polynomials called
generators. To accommodate the noise in the data set, we introduce the pairwise
conditional gradients approximate vanishing ideal algorithm (PCGAVI) that
constructs a set of generators of the approximate vanishing ideal. The
constructed generators capture polynomial structures in data and give rise to a
feature map that can, for example, be used in combination with a linear
classifier for supervised learning. In PCGAVI, we construct the set of
generators by solving constrained convex optimization problems with the
pairwise conditional gradients algorithm. Thus, PCGAVI not only constructs few
but also sparse generators, making the corresponding feature transformation
robust and compact. Furthermore, we derive several learning guarantees for
PCGAVI that make the algorithm theoretically better motivated than related
generator-constructing methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wirth_E/0/1/0/all/0/1&quot;&gt;Elias Wirth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pokutta_S/0/1/0/all/0/1&quot;&gt;Sebastian Pokutta&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.00948">
<title>CD-GAN: a robust fusion-based generative adversarial network for unsupervised remote sensing change detection with heterogeneous sensors. (arXiv:2203.00948v4 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2203.00948</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of Earth observation, change detection boils down to comparing
images acquired at different times by sensors of possibly different spatial
and/or spectral resolutions or different modalities (e.g., optical or radar).
Even when considering only optical images, this task has proven to be
challenging as soon as the sensors differ by their spatial and/or spectral
resolutions. This paper proposes a novel unsupervised change detection method
dedicated to images acquired by such so-called heterogeneous optical sensors.
It capitalizes on recent advances which formulate the change detection task
into a robust fusion framework. Adopting this formulation, the work reported in
this paper shows that any off-the-shelf network trained beforehand to fuse
optical images of different spatial and/or spectral resolutions can be easily
complemented with a network of the same architecture and embedded into an
adversarial framework to perform change detection. A comparison with
state-of-the-art change detection methods demonstrates the versatility and the
effectiveness of the proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jin-Ju Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dobigeon_N/0/1/0/all/0/1&quot;&gt;Nicolas Dobigeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chabert_M/0/1/0/all/0/1&quot;&gt;Marie Chabert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Ding-Cheng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Ting-Zhu Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jie Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11952">
<title>3D helical CT Reconstruction with a Memory Efficient Learned Primal-Dual Architecture. (arXiv:2205.11952v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11952</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning based computed tomography (CT) reconstruction has demonstrated
outstanding performance on simulated 2D low-dose CT data. This applies in
particular to domain adapted neural networks, which incorporate a handcrafted
physics model for CT imaging. Empirical evidence shows that employing such
architectures reduces the demand for training data and improves upon
generalisation. However, their training requires large computational resources
that quickly become prohibitive in 3D helical CT, which is the most common
acquisition geometry used for medical imaging. Furthermore, clinical data also
comes with other challenges not accounted for in simulations, like errors in
flux measurement, resolution mismatch and, most importantly, the absence of the
real ground truth. The necessity to have a computationally feasible training
combined with the need to address these issues has made it difficult to
evaluate deep learning based reconstruction on clinical 3D helical CT. This
paper modifies a domain adapted neural network architecture, the Learned
Primal-Dual (LPD), so that it can be trained and applied to reconstruction in
this setting. We achieve this by splitting the helical trajectory into sections
and applying the unrolled LPD iterations to those sections sequentially. To the
best of our knowledge, this work is the first to apply an unrolled deep
learning architecture for reconstruction on full-sized clinical data, like
those in the Low dose CT image and projection data set (LDCT). Moreover,
training and testing is done on a single GPU card with 24GB of memory.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rudzusika_J/0/1/0/all/0/1&quot;&gt;Jevgenija Rudzusika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bajic_B/0/1/0/all/0/1&quot;&gt;Buda Baji&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Koehler_T/0/1/0/all/0/1&quot;&gt;Thomas Koehler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Oktem_O/0/1/0/all/0/1&quot;&gt;Ozan &amp;#xd6;ktem&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.10586">
<title>D-CIPHER: Discovery of Closed-form Partial Differential Equations. (arXiv:2206.10586v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.10586</link>
<description rdf:parseType="Literal">&lt;p&gt;Closed-form differential equations, including partial differential equations
and higher-order ordinary differential equations, are one of the most important
tools used by scientists to model and better understand natural phenomena.
Discovering these equations directly from data is challenging because it
requires modeling relationships between various derivatives that are not
observed in the data (equation-data mismatch) and it involves searching across
a huge space of possible equations. Current approaches make strong assumptions
about the form of the equation and thus fail to discover many well-known
systems. Moreover, many of them resolve the equation-data mismatch by
estimating the derivatives, which makes them inadequate for noisy and
infrequently sampled systems. To this end, we propose D-CIPHER, which is robust
to measurement artifacts and can uncover a new and very general class of
differential equations. We further design a novel optimization procedure,
CoLLie, to help D-CIPHER search through this class efficiently. Finally, we
demonstrate empirically that it can discover many well-known equations that are
beyond the capabilities of current methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kacprzyk_K/0/1/0/all/0/1&quot;&gt;Krzysztof Kacprzyk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_Z/0/1/0/all/0/1&quot;&gt;Zhaozhi Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaar_M/0/1/0/all/0/1&quot;&gt;Mihaela van der Schaar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.04869">
<title>Graph-based Molecular Representation Learning. (arXiv:2207.04869v3 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2207.04869</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular representation learning (MRL) is a key step to build the connection
between machine learning and chemical science. In particular, it encodes
molecules as numerical vectors preserving the molecular structures and
features, on top of which the downstream tasks (e.g., property prediction) can
be performed. Recently, MRL has achieved considerable progress, especially in
methods based on deep molecular graph learning. In this survey, we
systematically review these graph-based molecular representation techniques,
especially the methods incorporating chemical domain knowledge. Specifically,
we first introduce the features of 2D and 3D molecular graphs. Then we
summarize and categorize MRL methods into three groups based on their input.
Furthermore, we discuss some typical chemical applications supported by MRL. To
facilitate studies in this fast-developing area, we also list the benchmarks
and commonly used datasets in the paper. Finally, we share our thoughts on
future research directions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zhichun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Guo_K/0/1/0/all/0/1&quot;&gt;Kehan Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Nan_B/0/1/0/all/0/1&quot;&gt;Bozhao Nan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yijun Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Iyer_R/0/1/0/all/0/1&quot;&gt;Roshni G. Iyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yihong Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wiest_O/0/1/0/all/0/1&quot;&gt;Olaf Wiest&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiangliang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuxu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chawla_N/0/1/0/all/0/1&quot;&gt;Nitesh V. Chawla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.07626">
<title>Algorithmic Assistance with Recommendation-Dependent Preferences. (arXiv:2208.07626v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.07626</link>
<description rdf:parseType="Literal">&lt;p&gt;When we use algorithms to produce risk assessments, we typically think of
these predictions as providing helpful input to human decisions, such as when
risk scores are presented to judges or doctors. But when a decision-maker
obtains algorithmic assistance, they may not only react to the information. The
decision-maker may view the input of the algorithm as recommending a default
action, making it costly for them to deviate, such as when a judge is reluctant
to overrule a high-risk assessment of a defendant or a doctor fears the
consequences of deviating from recommended procedures. In this article, we
propose a principal-agent model of joint human-machine decision-making. Within
this model, we consider the effect and design of algorithmic recommendations
when they affect choices not just by shifting beliefs, but also by altering
preferences. We motivate this assumption from institutional factors, such as a
desire to avoid audits, as well as from well-established models in behavioral
science that predict loss aversion relative to a reference point, which here is
set by the algorithm. We show that recommendation-dependent preferences create
inefficiencies where the decision-maker is overly responsive to the
recommendation. As a potential remedy, we discuss algorithms that strategically
withhold recommendations, and show how they can improve the quality of final
decisions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McLaughlin_B/0/1/0/all/0/1&quot;&gt;Bryce McLaughlin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Spiess_J/0/1/0/all/0/1&quot;&gt;Jann Spiess&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06591">
<title>Rigorous dynamical mean field theory for stochastic gradient descent methods. (arXiv:2210.06591v3 [math-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06591</link>
<description rdf:parseType="Literal">&lt;p&gt;We prove closed-form equations for the exact high-dimensional asymptotics of
a family of first order gradient-based methods, learning an estimator (e.g.
M-estimator, shallow neural network, ...) from observations on Gaussian data
with empirical risk minimization. This includes widely used algorithms such as
stochastic gradient descent (SGD) or Nesterov acceleration. The obtained
equations match those resulting from the discretization of dynamical mean-field
theory (DMFT) equations from statistical physics when applied to gradient flow.
Our proof method allows us to give an explicit description of how memory
kernels build up in the effective dynamics, and to include non-separable update
functions, allowing datasets with non-identity covariance matrices. Finally, we
provide numerical implementations of the equations for SGD with generic
extensive batch-size and with constant learning rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math-ph/1/au:+Gerbelot_C/0/1/0/all/0/1&quot;&gt;Cedric Gerbelot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math-ph/1/au:+Troiani_E/0/1/0/all/0/1&quot;&gt;Emanuele Troiani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math-ph/1/au:+Mignacco_F/0/1/0/all/0/1&quot;&gt;Francesca Mignacco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math-ph/1/au:+Krzakala_F/0/1/0/all/0/1&quot;&gt;Florent Krzakala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math-ph/1/au:+Zdeborova_L/0/1/0/all/0/1&quot;&gt;Lenka Zdeborova&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.03803">
<title>Quantum-probabilistic Hamiltonian learning for generative modelling &amp; anomaly detection. (arXiv:2211.03803v3 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2211.03803</link>
<description rdf:parseType="Literal">&lt;p&gt;The Hamiltonian of an isolated quantum mechanical system determines its
dynamics and physical behaviour. This study investigates the possibility of
learning and utilising a system&apos;s Hamiltonian and its variational thermal state
estimation for data analysis techniques. For this purpose, we employ the method
of Quantum Hamiltonian-based models for the generative modelling of simulated
Large Hadron Collider data and demonstrate the representability of such data as
a mixed state. In a further step, we use the learned Hamiltonian for anomaly
detection, showing that different sample types can form distinct dynamical
behaviours once treated as a quantum many-body system. We exploit these
characteristics to quantify the difference between sample types. Our findings
show that the methodologies designed for field theory computations can be
utilised in machine learning applications to employ theoretical approaches in
data analysis techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Araz_J/0/1/0/all/0/1&quot;&gt;Jack Y. Araz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Spannowsky_M/0/1/0/all/0/1&quot;&gt;Michael Spannowsky&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.05368">
<title>A Comprehensive Survey on Distributed Training of Graph Neural Networks. (arXiv:2211.05368v3 [cs.DC] UPDATED)</title>
<link>http://arxiv.org/abs/2211.05368</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph neural networks (GNNs) have been demonstrated to be a powerful
algorithmic model in broad application fields for their effectiveness in
learning over graphs. To scale GNN training up for large-scale and ever-growing
graphs, the most promising solution is distributed training which distributes
the workload of training across multiple computing nodes. At present, the
volume of related research on distributed GNN training is exceptionally vast,
accompanied by an extraordinarily rapid pace of publication. Moreover, the
approaches reported in these studies exhibit significant divergence. This
situation poses a considerable challenge for newcomers, hindering their ability
to grasp a comprehensive understanding of the workflows, computational
patterns, communication strategies, and optimization techniques employed in
distributed GNN training. As a result, there is a pressing need for a survey to
provide correct recognition, analysis, and comparisons in this field. In this
paper, we provide a comprehensive survey of distributed GNN training by
investigating various optimization techniques used in distributed GNN training.
First, distributed GNN training is classified into several categories according
to their workflows. In addition, their computational patterns and communication
patterns, as well as the optimization techniques proposed by recent work are
introduced. Second, the software frameworks and hardware platforms of
distributed GNN training are also introduced for a deeper understanding. Third,
distributed GNN training is compared with distributed training of deep neural
networks, emphasizing the uniqueness of distributed GNN training. Finally,
interesting issues and opportunities in this field are discussed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_H/0/1/0/all/0/1&quot;&gt;Haiyang Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Mingyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaochun Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_D/0/1/0/all/0/1&quot;&gt;Dongrui Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_S/0/1/0/all/0/1&quot;&gt;Shirui Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_W/0/1/0/all/0/1&quot;&gt;Wenguang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yuan Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.10649">
<title>LibSignal: An Open Library for Traffic Signal Control. (arXiv:2211.10649v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2211.10649</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces a library for cross-simulator comparison of
reinforcement learning models in traffic signal control tasks. This library is
developed to implement recent state-of-the-art reinforcement learning models
with extensible interfaces and unified cross-simulator evaluation metrics. It
supports commonly-used simulators in traffic signal control tasks, including
Simulation of Urban MObility(SUMO) and CityFlow, and multiple benchmark
datasets for fair comparisons. We conducted experiments to validate our
implementation of the models and to calibrate the simulators so that the
experiments from one simulator could be referential to the other. Based on the
validated models and calibrated environments, this paper compares and reports
the performance of current state-of-the-art RL algorithms across different
datasets and simulators. This is the first time that these methods have been
compared fairly under the same datasets with different simulators.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_X/0/1/0/all/0/1&quot;&gt;Xiaoliang Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Da_L/0/1/0/all/0/1&quot;&gt;Longchao Da&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15646">
<title>Beyond Invariance: Test-Time Label-Shift Adaptation for Distributions with &quot;Spurious&quot; Correlations. (arXiv:2211.15646v4 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15646</link>
<description rdf:parseType="Literal">&lt;p&gt;Changes in the data distribution at test time can have deleterious effects on
the performance of predictive models $p(y|x)$. We consider situations where
there are additional meta-data labels (such as group labels), denoted by $z$,
that can account for such changes in the distribution. In particular, we assume
that the prior distribution $p(y, z)$, which models the dependence between the
class label $y$ and the &quot;nuisance&quot; factors $z$, may change across domains,
either due to a change in the correlation between these terms, or a change in
one of their marginals. However, we assume that the generative model for
features $p(x|y,z)$ is invariant across domains. We note that this corresponds
to an expanded version of the widely used &quot;label shift&quot; assumption, where the
labels now also include the nuisance factors $z$. Based on this observation, we
propose a test-time label shift correction that adapts to changes in the joint
distribution $p(y, z)$ using EM applied to unlabeled samples from the target
domain distribution, $p_t(x)$. Importantly, we are able to avoid fitting a
generative model $p(x|y, z)$, and merely need to reweight the outputs of a
discriminative model $p_s(y, z|x)$ trained on the source distribution. We
evaluate our method, which we call &quot;Test-Time Label-Shift Adaptation&quot; (TTLSA),
on several standard image and text datasets, as well as the CheXpert chest
X-ray dataset, and show that it improves performance over methods that target
invariance to changes in the distribution, as well as baseline empirical risk
minimization methods. Code for reproducing experiments is available at
https://github.com/nalzok/test-time-label-shift .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qingyao Sun&lt;/a&gt; (Cornell University), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Murphy_K/0/1/0/all/0/1&quot;&gt;Kevin Murphy&lt;/a&gt; (Google DeepMind), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ebrahimi_S/0/1/0/all/0/1&quot;&gt;Sayna Ebrahimi&lt;/a&gt; (Google Cloud AI Research), &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+DAmour_A/0/1/0/all/0/1&quot;&gt;Alexander D&amp;#x27;Amour&lt;/a&gt; (Google DeepMind)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00545">
<title>Knockoffs-SPR: Clean Sample Selection in Learning with Noisy Labels. (arXiv:2301.00545v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00545</link>
<description rdf:parseType="Literal">&lt;p&gt;A noisy training set usually leads to the degradation of the generalization
and robustness of neural networks. In this paper, we propose a novel
theoretically guaranteed clean sample selection framework for learning with
noisy labels. Specifically, we first present a Scalable Penalized Regression
(SPR) method, to model the linear relation between network features and one-hot
labels. In SPR, the clean data are identified by the zero mean-shift parameters
solved in the regression model. We theoretically show that SPR can recover
clean data under some conditions. Under general scenarios, the conditions may
be no longer satisfied; and some noisy data are falsely selected as clean data.
To solve this problem, we propose a data-adaptive method for Scalable Penalized
Regression with Knockoff filters (Knockoffs-SPR), which is provable to control
the False-Selection-Rate (FSR) in the selected clean data. To improve the
efficiency, we further present a split algorithm that divides the whole
training set into small pieces that can be solved in parallel to make the
framework scalable to large datasets. While Knockoffs-SPR can be regarded as a
sample selection module for a standard supervised training pipeline, we further
combine it with a semi-supervised algorithm to exploit the support of noisy
data as unlabeled data. Experimental results on several benchmark datasets and
real-world noisy datasets show the effectiveness of our framework and validate
the theoretical results of Knockoffs-SPR. Our code and pre-trained models are
available at https://github.com/Yikai-Wang/Knockoffs-SPR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yikai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Y/0/1/0/all/0/1&quot;&gt;Yanwei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinwei Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.11695">
<title>LegendreTron: Uprising Proper Multiclass Loss Learning. (arXiv:2301.11695v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2301.11695</link>
<description rdf:parseType="Literal">&lt;p&gt;Loss functions serve as the foundation of supervised learning and are often
chosen prior to model development. To avoid potentially ad hoc choices of
losses, statistical decision theory describes a desirable property for losses
known as \emph{properness}, which asserts that Bayes&apos; rule is optimal. Recent
works have sought to \emph{learn losses} and models jointly. Existing methods
do this by fitting an inverse canonical link function which monotonically maps
$\mathbb{R}$ to $[0,1]$ to estimate probabilities for binary problems. In this
paper, we extend monotonicity to maps between $\mathbb{R}^{C-1}$ and the
projected probability simplex $\tilde{\Delta}^{C-1}$ by using monotonicity of
gradients of convex functions. We present {\sc LegendreTron} as a novel and
practical method that jointly learns \emph{proper canonical losses} and
probabilities for multiclass problems. Tested on a benchmark of domains with up
to 1,000 classes, our experimental results show that our method consistently
outperforms the natural multiclass baseline under a $t$-test at 99%
significance on all datasets with greater than 10 classes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lam_K/0/1/0/all/0/1&quot;&gt;Kevin Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Walder_C/0/1/0/all/0/1&quot;&gt;Christian Walder&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Penev_S/0/1/0/all/0/1&quot;&gt;Spiridon Penev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Nock_R/0/1/0/all/0/1&quot;&gt;Richard Nock&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.01714">
<title>Learning End-to-End Channel Coding with Diffusion Models. (arXiv:2302.01714v2 [cs.IT] UPDATED)</title>
<link>http://arxiv.org/abs/2302.01714</link>
<description rdf:parseType="Literal">&lt;p&gt;It is a known problem that deep-learning-based end-to-end (E2E) channel
coding systems depend on a known and differentiable channel model, due to the
learning process and based on the gradient-descent optimization methods. This
places the challenge to approximate or generate the channel or its derivative
from samples generated by pilot signaling in real-world scenarios. Currently,
there are two prevalent methods to solve this problem. One is to generate the
channel via a generative adversarial network (GAN), and the other is to, in
essence, approximate the gradient via reinforcement learning methods. Other
methods include using score-based methods, variational autoencoders, or
mutual-information-based methods. In this paper, we focus on generative models
and, in particular, on a new promising method called diffusion models, which
have shown a higher quality of generation in image-based tasks. We will show
that diffusion models can be used in wireless E2E scenarios and that they work
as good as Wasserstein GANs while having a more stable training procedure and a
better generalization ability in testing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_M/0/1/0/all/0/1&quot;&gt;Muah Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fritschek_R/0/1/0/all/0/1&quot;&gt;Rick Fritschek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schaefer_R/0/1/0/all/0/1&quot;&gt;Rafael F. Schaefer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14545">
<title>Modern Bayesian Experimental Design. (arXiv:2302.14545v2 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14545</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian experimental design (BED) provides a powerful and general framework
for optimizing the design of experiments. However, its deployment often poses
substantial computational challenges that can undermine its practical use. In
this review, we outline how recent advances have transformed our ability to
overcome these challenges and thus utilize BED effectively, before discussing
some key areas for future development in the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Rainforth_T/0/1/0/all/0/1&quot;&gt;Tom Rainforth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Foster_A/0/1/0/all/0/1&quot;&gt;Adam Foster&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Ivanova_D/0/1/0/all/0/1&quot;&gt;Desi R Ivanova&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Smith_F/0/1/0/all/0/1&quot;&gt;Freddie Bickford Smith&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06753">
<title>Modular Quantization-Aware Training: Increasing Accuracy by Decreasing Precision in 6D Object Pose Estimation. (arXiv:2303.06753v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06753</link>
<description rdf:parseType="Literal">&lt;p&gt;Edge applications, such as collaborative robotics and spacecraft rendezvous,
demand efficient 6D object pose estimation on resource-constrained embedded
platforms. Existing 6D pose estimation networks are often too large for such
deployments, necessitating compression while maintaining reliable performance.
To address this challenge, we introduce Modular Quantization-Aware Training
(MQAT), an adaptive and mixed-precision quantization-aware training strategy
that exploits the modular structure of modern 6D pose estimation architectures.
MQAT guides a systematic gradated modular quantization sequence and determines
module-specific bit precisions, leading to quantized models that outperform
those produced by state-of-the-art uniform and mixed-precision quantization
techniques. Our experiments showcase the generality of MQAT across datasets,
architectures, and quantization algorithms. Remarkably, MQAT-trained quantized
models achieve a significant accuracy boost (&amp;gt;7%) over the baseline
full-precision network while reducing model size by a factor of 4x or more.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Javed_S/0/1/0/all/0/1&quot;&gt;Saqib Javed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengkun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Price_A/0/1/0/all/0/1&quot;&gt;Andrew Price&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yinlin Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Salzmann_M/0/1/0/all/0/1&quot;&gt;Mathieu Salzmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07321">
<title>Collision Cross-entropy for Soft Class Labels and Deep Clustering. (arXiv:2303.07321v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07321</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose &quot;collision cross-entropy&quot; as a robust alternative to Shannon&apos;s
cross-entropy (CE) loss when class labels are represented by soft categorical
distributions y. In general, soft labels can naturally represent ambiguous
targets in classification. They are particularly relevant for self-labeled
clustering methods, where latent pseudo-labels are jointly estimated with the
model parameters and uncertainty is prevalent. In case of soft labels,
Shannon&apos;s CE teaches the model predictions to reproduce the uncertainty in each
training example, which inhibits the model&apos;s ability to learn and generalize
from these examples. As an alternative loss, we propose the negative log of
&quot;collision probability&quot; that maximizes the chance of equality between two
random variables, predicted class and unknown true class. We show that it has
the properties of a generalized CE. The proposed collision CE agrees with
Shannon&apos;s CE for one-hot labels, but the training from soft labels differs. For
example, unlike Shannon&apos;s CE, data points where y is a uniform distribution
have zero contribution to the training. Collision CE significantly improves
classification supervised by soft uncertain targets. Unlike Shannon&apos;s,
collision CE is symmetric for y and network predictions, which is particularly
relevant when both distributions are estimated in the context of self-labeled
clustering. Focusing on discriminative deep clustering where self-labeling and
entropy-based losses are dominant, we show that the use of collision CE
improves the state-of-the-art. We also derive an efficient EM algorithm that
significantly speeds up the pseudo-label estimation with collision CE.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhongwen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boykov_Y/0/1/0/all/0/1&quot;&gt;Yuri Boykov&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.10426">
<title>Discovering Predictable Latent Factors for Time Series Forecasting. (arXiv:2303.10426v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.10426</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern time series forecasting methods, such as Transformer and its variants,
have shown strong ability in sequential data modeling. To achieve high
performance, they usually rely on redundant or unexplainable structures to
model complex relations between variables and tune the parameters with
large-scale data. Many real-world data mining tasks, however, lack sufficient
variables for relation reasoning, and therefore these methods may not properly
handle such forecasting problems. With insufficient data, time series appear to
be affected by many exogenous variables, and thus, the modeling becomes
unstable and unpredictable. To tackle this critical issue, in this paper, we
develop a novel algorithmic framework for inferring the intrinsic latent
factors implied by the observable time series. The inferred factors are used to
form multiple independent and predictable signal components that enable not
only sparse relation reasoning for long-term efficiency but also reconstructing
the future temporal data for accurate prediction. To achieve this, we introduce
three characteristics, i.e., predictability, sufficiency, and identifiability,
and model these characteristics via the powerful deep latent dynamics models to
infer the predictable signal components. Empirical results on multiple real
datasets show the efficiency of our method for different kinds of time series
forecasting. The statistical analysis validates the predictability of the
learned latent factors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_J/0/1/0/all/0/1&quot;&gt;Jingyi Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zhen Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiayu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhijie Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15799">
<title>FedAgg: Adaptive Federated Learning with Aggregated Gradients. (arXiv:2303.15799v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15799</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated Learning (FL) has become an emerging norm for distributed model
training, which enables multiple devices cooperatively to train a shared model
utilizing their own datasets scheduled by a central server while keeping
private data localized. However, during the training process, the
non-independent-and-identically-distributed (Non-IID) data generated on
heterogeneous clients and frequent communication across participants may
significantly influence the training performance, slow down the convergent
rate, and increase communication consumption. In this paper, we ameliorate the
standard stochastic gradient descent approach by introducing the aggregated
gradients at each local update epoch and propose an adaptive learning rate
iterative algorithm that further takes the deviation between the local
parameter and global parameter into account. The aforementioned adaptive
learning rate design mechanism requires local information of all clients, which
is challenging as there is no communication during the local update epochs. To
obtain a decentralized adaptive learning rate for each client, we introduce the
mean-field approach by utilizing two mean-field terms to estimate the average
local parameters and gradients respectively without exchanging clients&apos; local
information with each other over time. Through theoretical analysis, we prove
that our method can provide the convergence guarantee for model training and
derive a convergent upper bound for the client drifting term. Extensive
numerical results show that our proposed framework is superior to the
state-of-the-art FL schemes in both model accuracy and convergent rate on
real-world datasets with IID and Non-IID data distribution.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_W/0/1/0/all/0/1&quot;&gt;Wenhao Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuehe Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17713">
<title>Mitigating Source Bias for Fairer Weak Supervision. (arXiv:2303.17713v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17713</link>
<description rdf:parseType="Literal">&lt;p&gt;Weak supervision enables efficient development of training sets by reducing
the need for ground truth labels. However, the techniques that make weak
supervision attractive -- such as integrating any source of signal to estimate
unknown labels -- also entail the danger that the produced pseudolabels are
highly biased. Surprisingly, given everyday use and the potential for increased
bias, weak supervision has not been studied from the point of view of fairness.
We begin such a study, starting with the observation that even when a fair
model can be built from a dataset with access to ground-truth labels, the
corresponding dataset labeled via weak supervision can be arbitrarily unfair.
To address this, we propose and empirically validate a model for source
unfairness in weak supervision, then introduce a simple counterfactual
fairness-based technique that can mitigate these biases. Theoretically, we show
that it is possible for our approach to simultaneously improve both accuracy
and fairness -- in contrast to standard fairness approaches that suffer from
tradeoffs. Empirically, we show that our technique improves accuracy on weak
supervision baselines by as much as 32\% while reducing demographic parity gap
by 82.5\%. A simple extension of our method aimed at maximizing performance
produces state-of-the-art performance in five out of ten datasets in the WRENCH
benchmark.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shin_C/0/1/0/all/0/1&quot;&gt;Changho Shin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cromp_S/0/1/0/all/0/1&quot;&gt;Sonia Cromp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adila_D/0/1/0/all/0/1&quot;&gt;Dyah Adila&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sala_F/0/1/0/all/0/1&quot;&gt;Frederic Sala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01552">
<title>Meta-Learning with a Geometry-Adaptive Preconditioner. (arXiv:2304.01552v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01552</link>
<description rdf:parseType="Literal">&lt;p&gt;Model-agnostic meta-learning (MAML) is one of the most successful
meta-learning algorithms. It has a bi-level optimization structure where the
outer-loop process learns a shared initialization and the inner-loop process
optimizes task-specific weights. Although MAML relies on the standard gradient
descent in the inner-loop, recent studies have shown that controlling the
inner-loop&apos;s gradient descent with a meta-learned preconditioner can be
beneficial. Existing preconditioners, however, cannot simultaneously adapt in a
task-specific and path-dependent way. Additionally, they do not satisfy the
Riemannian metric condition, which can enable the steepest descent learning
with preconditioned gradient. In this study, we propose Geometry-Adaptive
Preconditioned gradient descent (GAP) that can overcome the limitations in
MAML; GAP can efficiently meta-learn a preconditioner that is dependent on
task-specific parameters, and its preconditioner can be shown to be a
Riemannian metric. Thanks to the two properties, the geometry-adaptive
preconditioner is effective for improving the inner-loop optimization.
Experiment results show that GAP outperforms the state-of-the-art MAML family
and preconditioned gradient descent-MAML (PGD-MAML) family in a variety of
few-shot learning tasks. Code is available at:
https://github.com/Suhyun777/CVPR23-GAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_S/0/1/0/all/0/1&quot;&gt;Suhyun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_D/0/1/0/all/0/1&quot;&gt;Duhun Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eo_M/0/1/0/all/0/1&quot;&gt;Moonjung Eo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taesup Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rhee_W/0/1/0/all/0/1&quot;&gt;Wonjong Rhee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01628">
<title>Equivariant Parameter Sharing for Porous Crystalline Materials. (arXiv:2304.01628v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01628</link>
<description rdf:parseType="Literal">&lt;p&gt;Efficiently predicting properties of porous crystalline materials has great
potential to accelerate the high throughput screening process for developing
new materials, as simulations carried out using first principles model are
often computationally expensive. To effectively make use of Deep Learning
methods to model these materials, we need to utilize the symmetries present in
the crystals, which are defined by their space group. Existing methods for
crystal property prediction either have symmetry constraints that are too
restrictive or only incorporate symmetries between unit cells. In addition,
these models do not explicitly model the porous structure of the crystal. In
this paper, we develop a model which incorporates the symmetries of the unit
cell of a crystal in its architecture and explicitly models the porous
structure. We evaluate our model by predicting the heat of adsorption of CO$_2$
for different configurations of the mordenite zeolite. Our results confirm that
our method performs better than existing methods for crystal property
prediction and that the inclusion of pores results in a more efficient model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Petkovic_M/0/1/0/all/0/1&quot;&gt;Marko Petkovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_Marimon_P/0/1/0/all/0/1&quot;&gt;Pablo Romero-Marimon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menkovski_V/0/1/0/all/0/1&quot;&gt;Vlado Menkovski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calero_S/0/1/0/all/0/1&quot;&gt;Sofia Calero&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.07598">
<title>Hausdorff Distance Matching with Adaptive Query Denoising for Rotated Detection Transformer. (arXiv:2305.07598v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.07598</link>
<description rdf:parseType="Literal">&lt;p&gt;The Detection Transformer (DETR) has emerged as a pivotal role in object
detection tasks, setting new performance benchmarks due to its end-to-end
design and scalability. Despite its advancements, the application of DETR in
detecting rotated objects has demonstrated suboptimal performance relative to
established oriented object detectors. Our analysis identifies a key
limitation: the L1 cost used in Hungarian Matching leads to duplicate
predictions due to the square-like problem in oriented object detection,
thereby obstructing the training process of the detector. We introduce a
Hausdorff distance-based cost for Hungarian matching, which more accurately
quantifies the discrepancy between predictions and ground truths. Moreover, we
note that a static denoising approach hampers the training of rotated DETR,
particularly when the detector&apos;s predictions surpass the quality of noised
ground truths. We propose an adaptive query denoising technique, employing
Hungarian matching to selectively filter out superfluous noised queries that no
longer contribute to model improvement. Our proposed modifications to DETR have
resulted in superior performance, surpassing previous rotated DETR models and
other alternatives. This is evidenced by our model&apos;s state-of-the-art
achievements in benchmarks such as DOTA-v1.0/v1.5/v2.0, and DIOR-R.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_H/0/1/0/all/0/1&quot;&gt;Hakjin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Minki Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koo_J/0/1/0/all/0/1&quot;&gt;Jamyoung Koo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Junghoon Seo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10361">
<title>Human Choice Prediction in Language-based Non-Cooperative Games: Simulation-based Off-Policy Evaluation. (arXiv:2305.10361v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10361</link>
<description rdf:parseType="Literal">&lt;p&gt;Persuasion games have been fundamental in economics and AI research, and have
significant practical applications. Recent works in this area have started to
incorporate natural language, moving beyond the traditional stylized message
setting. However, previous research has focused on on-policy prediction, where
the train and test data have the same distribution, which is not representative
of real-life scenarios. In this paper, we tackle the challenging problem of
off-policy evaluation (OPE) in language-based persuasion games. To address the
inherent difficulty of human data collection in this setup, we propose a novel
approach which combines real and simulated human-bot interaction data. Our
simulated data is created by an exogenous model assuming decision makers (DMs)
start with a mixture of random and decision-theoretic based behaviors and
improve over time. We present a deep learning training algorithm that
effectively integrates real interaction and simulated data, substantially
improving over models that train only with interaction data. Our results
demonstrate the potential of real interaction and simulation mixtures as a
cost-effective and scalable solution for OPE in language-based persuasion
games. Our code and the large dataset we collected and generated are submitted
as supplementary material and publicly available in our GitHub repository:
https://github.com/eilamshapira/HumanChoicePrediction
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shapira_E/0/1/0/all/0/1&quot;&gt;Eilam Shapira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Apel_R/0/1/0/all/0/1&quot;&gt;Reut Apel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tennenholtz_M/0/1/0/all/0/1&quot;&gt;Moshe Tennenholtz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1&quot;&gt;Roi Reichart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.12534">
<title>BertRLFuzzer: A BERT and Reinforcement Learning based Fuzzer. (arXiv:2305.12534v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2305.12534</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel tool BertRLFuzzer, a BERT and Reinforcement Learning (RL)
based fuzzer aimed at finding security vulnerabilities for Web applications.
BertRLFuzzer works as follows: given a set of seed inputs, the fuzzer performs
grammar-adhering and attack-provoking mutation operations on them to generate
candidate attack vectors. The key insight of BertRLFuzzer is the use of RL with
a BERT model as an agent to guide the fuzzer to efficiently learn
grammar-adhering and attack-provoking mutation operators. In order to establish
the efficacy of BertRLFuzzer we compare it against a total of 13 black box and
white box fuzzers over a benchmark of 9 victim websites with over 16K LOC. We
observed a significant improvement relative to the nearest competing tool in
terms of time to first attack (54% less), new vulnerabilities found (17 new
vulnerabilities), and attack rate (4.4% more attack vectors generated).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jha_P/0/1/0/all/0/1&quot;&gt;Piyush Jha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scott_J/0/1/0/all/0/1&quot;&gt;Joseph Scott&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganeshna_J/0/1/0/all/0/1&quot;&gt;Jaya Sriram Ganeshna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_M/0/1/0/all/0/1&quot;&gt;Mudit Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganesh_V/0/1/0/all/0/1&quot;&gt;Vijay Ganesh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14912">
<title>SVDinsTN: A Tensor Network Paradigm for Efficient Structure Search from Regularized Modeling Perspective. (arXiv:2305.14912v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14912</link>
<description rdf:parseType="Literal">&lt;p&gt;Tensor network (TN) representation is a powerful technique for computer
vision and machine learning. TN structure search (TN-SS) aims to search for a
customized structure to achieve a compact representation, which is a
challenging NP-hard problem. Recent &quot;sampling-evaluation-based&quot; methods require
sampling an extensive collection of structures and evaluating them one by one,
resulting in prohibitively high computational costs. To address this issue, we
propose a novel TN paradigm, named SVD-inspired TN decomposition (SVDinsTN),
which allows us to efficiently solve the TN-SS problem from a regularized
modeling perspective, eliminating the repeated structure evaluations. To be
specific, by inserting a diagonal factor for each edge of the fully-connected
TN, SVDinsTN allows us to calculate TN cores and diagonal factors
simultaneously, with the factor sparsity revealing a compact TN structure. In
theory, we prove a convergence guarantee for the proposed method. Experimental
results demonstrate that the proposed method achieves approximately 100 to 1000
times acceleration compared to the state-of-the-art TN-SS methods while
maintaining a comparable representation ability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yu-Bang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_X/0/1/0/all/0/1&quot;&gt;Xi-Le Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_J/0/1/0/all/0/1&quot;&gt;Junhua Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qibin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Heng-Chao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_T/0/1/0/all/0/1&quot;&gt;Ting-Zhu Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18381">
<title>Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection. (arXiv:2305.18381v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18381</link>
<description rdf:parseType="Literal">&lt;p&gt;Data-efficient learning has garnered significant attention, especially given
the current trend of large multi-modal models. Recently, dataset distillation
becomes an effective approach for data-efficiency; however, the distillation
process itself can still be inefficient. In this work, we model the dataset
distillation task within the context of information transport. By observing the
substantial data redundancy inherent in the distillation, we argue to put more
emphasis on the samples&apos; utility for the distillation task. We introduce and
validate a family of data utility estimators and optimal data selection methods
to exploit the most valuable samples. This strategy significantly reduces the
training costs and extends various existing distillation algorithms to larger
and more diversified datasets, e.g., in some cases only 0.04% training data is
sufficient for comparable distillation performance. Our method consistently
enhances the distillation algorithms, even on much larger-scale and more
heterogeneous datasets, e.g. ImageNet-1K and Kinetics-400. This paradigm opens
up new avenues in the dynamics of distillation and paves the way for efficient
dataset distillation. Our code is available on
https://github.com/silicx/GoldFromOres .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Y/0/1/0/all/0/1&quot;&gt;Yue Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong-Lu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_K/0/1/0/all/0/1&quot;&gt;Kaitong Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_C/0/1/0/all/0/1&quot;&gt;Cewu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tai_Y/0/1/0/all/0/1&quot;&gt;Yu-Wing Tai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_C/0/1/0/all/0/1&quot;&gt;Chi-Keung Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18409">
<title>Direction-oriented Multi-objective Learning: Simple and Provable Stochastic Algorithms. (arXiv:2305.18409v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18409</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-objective optimization (MOO) has become an influential framework in
many machine learning problems with multiple objectives such as learning with
multiple criteria and multi-task learning (MTL). In this paper, we propose a
new direction-oriented multi-objective problem by regularizing the common
descent direction within a neighborhood of a direction that optimizes a linear
combination of objectives such as the average loss in MTL. This formulation
includes GD and MGDA as special cases, enjoys the direction-oriented benefit as
in CAGrad, and facilitates the design of stochastic algorithms. To solve this
problem, we propose Stochastic Direction-oriented Multi-objective Gradient
descent (SDMGrad) with simple SGD type of updates, and its variant SDMGrad-OS
with an efficient objective sampling in the setting where the number of
objectives is large. For a constant-level regularization parameter $\lambda$,
we show that SDMGrad and SDMGrad-OS provably converge to a Pareto stationary
point with improved complexities and milder assumptions. For an increasing
$\lambda$, this convergent point reduces to a stationary point of the linear
combination of objectives. We demonstrate the superior performance of the
proposed methods in a series of tasks on multi-task supervised learning and
reinforcement learning. Code is provided at
https://github.com/ml-opt-lab/sdmgrad.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1&quot;&gt;Peiyao Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ban_H/0/1/0/all/0/1&quot;&gt;Hao Ban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_K/0/1/0/all/0/1&quot;&gt;Kaiyi Ji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.19891">
<title>Dynamic Neighborhood Construction for Structured Large Discrete Action Spaces. (arXiv:2305.19891v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.19891</link>
<description rdf:parseType="Literal">&lt;p&gt;Large discrete action spaces (LDAS) remain a central challenge in
reinforcement learning. Existing solution approaches can handle unstructured
LDAS with up to a few million actions. However, many real-world applications in
logistics, production, and transportation systems have combinatorial action
spaces, whose size grows well beyond millions of actions, even on small
instances. Fortunately, such action spaces exhibit structure, e.g., equally
spaced discrete resource units. With this work, we focus on handling structured
LDAS (SLDAS) with sizes that cannot be handled by current benchmarks: we
propose Dynamic Neighborhood Construction (DNC), a novel exploitation paradigm
for SLDAS. We present a scalable neighborhood exploration heuristic that
utilizes this paradigm and efficiently explores the discrete neighborhood
around the continuous proxy action in structured action spaces with up to
$10^{73}$ actions. We demonstrate the performance of our method by benchmarking
it against three state-of-the-art approaches designed for large discrete action
spaces across two distinct environments. Our results show that DNC matches or
outperforms state-of-the-art approaches while being computationally more
efficient. Furthermore, our method scales to action spaces that so far remained
computationally intractable for existing methodologies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akkerman_F/0/1/0/all/0/1&quot;&gt;Fabian Akkerman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luy_J/0/1/0/all/0/1&quot;&gt;Julius Luy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heeswijk_W/0/1/0/all/0/1&quot;&gt;Wouter van Heeswijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiffer_M/0/1/0/all/0/1&quot;&gt;Maximilian Schiffer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.03436">
<title>Intellectual Property Protection of Diffusion Models via the Watermark Diffusion Process. (arXiv:2306.03436v2 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2306.03436</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have rapidly become a vital part of deep generative
architectures, given today&apos;s increasing demands. Obtaining large,
high-performance diffusion models demands significant resources, highlighting
their importance as intellectual property worth protecting. However, existing
watermarking techniques for ownership verification are insufficient when
applied to diffusion models. Very recent research in watermarking diffusion
models either exposes watermarks during task generation, which harms the
imperceptibility, or is developed for conditional diffusion models that require
prompts to trigger the watermark. This paper introduces WDM, a novel
watermarking solution for diffusion models without imprinting the watermark
during task generation. It involves training a model to concurrently learn a
Watermark Diffusion Process (WDP) for embedding watermarks alongside the
standard diffusion process for task generation. We provide a detailed
theoretical analysis of WDP training and sampling, relating it to a shifted
Gaussian diffusion process via the same reverse noise. Extensive experiments
are conducted to validate the effectiveness and robustness of our approach in
various trigger and watermark data configurations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_S/0/1/0/all/0/1&quot;&gt;Sen Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yufei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Cong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xiaohua Jia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.15832">
<title>Easing Color Shifts in Score-Based Diffusion Models. (arXiv:2306.15832v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.15832</link>
<description rdf:parseType="Literal">&lt;p&gt;Generated images of score-based models can suffer from errors in their
spatial means, an effect, referred to as a color shift, which grows for larger
images. This paper investigates a previously-introduced approach to mitigate
color shifts in score-based diffusion models. We quantify the performance of a
nonlinear bypass connection in the score network, designed to process the
spatial mean of the input and to predict the mean of the score function. We
show that this network architecture substantially improves the resulting
quality of the generated images, and that this improvement is approximately
independent of the size of the generated images. As a result, this modified
architecture offers a simple solution for the color shift problem across image
sizes. We additionally discuss the origin of color shifts in an idealized
setting in order to motivate the approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deck_K/0/1/0/all/0/1&quot;&gt;Katherine Deck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bischoff_T/0/1/0/all/0/1&quot;&gt;Tobias Bischoff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16772">
<title>Learning from Synthetic Human Group Activities. (arXiv:2306.16772v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16772</link>
<description rdf:parseType="Literal">&lt;p&gt;The study of complex human interactions and group activities has become a
focal point in human-centric computer vision. However, progress in related
tasks is often hindered by the challenges of obtaining large-scale labeled
datasets from real-world scenarios. To address the limitation, we introduce
M3Act, a synthetic data generator for multi-view multi-group multi-person human
atomic actions and group activities. Powered by the Unity engine, M3Act
features multiple semantic groups, highly diverse and photorealistic images,
and a comprehensive set of annotations, which facilitates the learning of
human-centered tasks across single-person, multi-person, and multi-group
conditions. We demonstrate the advantages of M3Act across three core
experiments using various input modalities. First, adding our synthetic data
significantly improves the performance of MOTRv2 on DanceTrack, leading to a
hop on the leaderboard from 10th to 2nd place. With M3Act, we achieve tracking
results on par with MOTRv2*, which is trained with 62.5% more real-world data.
Second, M3Act improves the benchmark performances on CAD2 by 5.59% and 7.43% on
group activity and atomic action accuracy respectively. Moreover, M3Act opens
new research for controllable 3D group activity generation. We define multiple
metrics and propose a competitive baseline for the novel task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_C/0/1/0/all/0/1&quot;&gt;Che-Jui Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Danrui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_D/0/1/0/all/0/1&quot;&gt;Deep Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_P/0/1/0/all/0/1&quot;&gt;Parth Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Honglu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_S/0/1/0/all/0/1&quot;&gt;Seonghyeon Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sohn_S/0/1/0/all/0/1&quot;&gt;Samuel S. Sohn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_S/0/1/0/all/0/1&quot;&gt;Sejong Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pavlovic_V/0/1/0/all/0/1&quot;&gt;Vladimir Pavlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kapadia_M/0/1/0/all/0/1&quot;&gt;Mubbasir Kapadia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13868">
<title>Learning sources of variability from high-dimensional observational studies. (arXiv:2307.13868v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13868</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference studies whether the presence of a variable influences an
observed outcome. As measured by quantities such as the &quot;average treatment
effect,&quot; this paradigm is employed across numerous biological fields, from
vaccine and drug development to policy interventions. Unfortunately, the
majority of these methods are often limited to univariate outcomes. Our work
generalizes causal estimands to outcomes with any number of dimensions or any
measurable space, and formulates traditional causal estimands for nominal
variables as causal discrepancy tests. We propose a simple technique for
adjusting universally consistent conditional independence tests and prove that
these tests are universally consistent causal discrepancy tests. Numerical
experiments illustrate that our method, Causal CDcorr, leads to improvements in
both finite sample validity and power when compared to existing strategies. Our
methods are all open source and available at github.com/ebridge2/cdcorr.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Bridgeford_E/0/1/0/all/0/1&quot;&gt;Eric W. Bridgeford&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chung_J/0/1/0/all/0/1&quot;&gt;Jaewon Chung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gilbert_B/0/1/0/all/0/1&quot;&gt;Brian Gilbert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Panda_S/0/1/0/all/0/1&quot;&gt;Sambit Panda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Adam Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Cencheng Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Badea_A/0/1/0/all/0/1&quot;&gt;Alexandra Badea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Caffo_B/0/1/0/all/0/1&quot;&gt;Brian Caffo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.13903">
<title>Corruption-Robust Lipschitz Contextual Search. (arXiv:2307.13903v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.13903</link>
<description rdf:parseType="Literal">&lt;p&gt;I study the problem of learning a Lipschitz function with corrupted binary
signals. The learner tries to learn a $L$-Lipschitz function $f: [0,1]^d
\rightarrow [0, L]$ that the adversary chooses. There is a total of $T$ rounds.
In each round $t$, the adversary selects a context vector $x_t$ in the input
space, and the learner makes a guess to the true function value $f(x_t)$ and
receives a binary signal indicating whether the guess is high or low. In a
total of $C$ rounds, the signal may be corrupted, though the value of $C$ is
\emph{unknown} to the learner. The learner&apos;s goal is to incur a small
cumulative loss. This work introduces the new algorithmic technique
\emph{agnostic checking} as well as new analysis techniques. I design
algorithms which: for the symmetric loss, the learner achieves regret $L\cdot
O(C\log T)$ with $d = 1$ and $L\cdot O_d(C\log T + T^{(d-1)/d})$ with $d &amp;gt; 1$;
for the pricing loss, the learner achieves regret $L\cdot \widetilde{O}
(T^{d/(d+1)} + C\cdot T^{1/(d+1)})$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_S/0/1/0/all/0/1&quot;&gt;Shiliang Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.01050">
<title>A Counterfactual Safety Margin Perspective on the Scoring of Autonomous Vehicles&apos; Riskiness. (arXiv:2308.01050v4 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2308.01050</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous Vehicles (AVs) promise a range of societal advantages, including
broader access to mobility, reduced road accidents, and enhanced transportation
efficiency. However, evaluating the risks linked to AVs is complex due to
limited historical data and the swift progression of technology. This paper
presents a data-driven framework for assessing the risk of different AVs&apos;
behaviors in various operational design domains (ODDs), based on counterfactual
simulations of &quot;misbehaving&quot; road users. We propose the notion of
counterfactual safety margin, which represents the minimum deviation from
nominal behavior that could cause a collision. This methodology not only
pinpoints the most critical scenarios but also quantifies the (relative) risk&apos;s
frequency and severity concerning AVs. Importantly, we show that our approach
is applicable even when the AV&apos;s behavioral policy remains undisclosed, through
worst- and best-case analyses, benefiting external entities like regulators and
risk evaluators. Our experimental outcomes demonstrate the correlation between
the safety margin, the quality of the driving policy, and the ODD, shedding
light on the relative risks of different AV providers. Overall, this work
contributes to the safety assessment of AVs and addresses legislative and
insurance concerns surrounding this burgeoning technology.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zanardi_A/0/1/0/all/0/1&quot;&gt;Alessandro Zanardi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Censi_A/0/1/0/all/0/1&quot;&gt;Andrea Censi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Atzei_M/0/1/0/all/0/1&quot;&gt;Margherita Atzei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lillo_L/0/1/0/all/0/1&quot;&gt;Luigi Di Lillo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Frazzoli_E/0/1/0/all/0/1&quot;&gt;Emilio Frazzoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.03818">
<title>A sparse coding approach to inverse problems with application to microwave tomography. (arXiv:2308.03818v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2308.03818</link>
<description rdf:parseType="Literal">&lt;p&gt;Inverse imaging problems that are ill-posed can be encountered across
multiple domains of science and technology, ranging from medical diagnosis to
astronomical studies. To reconstruct images from incomplete and distorted data,
it is necessary to create algorithms that can take into account both, the
physical mechanisms responsible for generating these measurements and the
intrinsic characteristics of the images being analyzed. In this work, the
sparse representation of images is reviewed, which is a realistic, compact and
effective generative model for natural images inspired by the visual system of
mammals. It enables us to address ill-posed linear inverse problems by training
the model on a vast collection of images. Moreover, we extend the application
of sparse coding to solve the non-linear and ill-posed problem in microwave
tomography imaging, which could lead to a significant improvement of the
state-of-the-arts algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Caiafa_C/0/1/0/all/0/1&quot;&gt;Cesar F. Caiafa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Irastorza_R/0/1/0/all/0/1&quot;&gt;Ramiro M. Irastorza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.06703">
<title>Understanding the robustness difference between stochastic gradient descent and adaptive gradient methods. (arXiv:2308.06703v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.06703</link>
<description rdf:parseType="Literal">&lt;p&gt;Stochastic gradient descent (SGD) and adaptive gradient methods, such as Adam
and RMSProp, have been widely used in training deep neural networks. We
empirically show that while the difference between the standard generalization
performance of models trained using these methods is small, those trained using
SGD exhibit far greater robustness under input perturbations. Notably, our
investigation demonstrates the presence of irrelevant frequencies in natural
datasets, where alterations do not affect models&apos; generalization performance.
However, models trained with adaptive methods show sensitivity to these
changes, suggesting that their use of irrelevant frequencies can lead to
solutions sensitive to perturbations. To better understand this difference, we
study the learning dynamics of gradient descent (GD) and sign gradient descent
(signGD) on a synthetic dataset that mirrors natural signals. With a
three-dimensional input space, the models optimized with GD and signGD have
standard risks close to zero but vary in their adversarial risks. Our result
shows that linear models&apos; robustness to $\ell_2$-norm bounded changes is
inversely proportional to the model parameters&apos; weight norm: a smaller weight
norm implies better robustness. In the context of deep learning, our
experiments show that SGD-trained neural networks have smaller Lipschitz
constants, explaining the better robustness to input perturbations than those
trained with adaptive gradient methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Avery Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_Y/0/1/0/all/0/1&quot;&gt;Yangchen Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farahmand_A/0/1/0/all/0/1&quot;&gt;Amir-massoud Farahmand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.09791">
<title>An Efficient High-Dimensional Gene Selection Approach based on Binary Horse Herd Optimization Algorithm for Biological Data Classification. (arXiv:2308.09791v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.09791</link>
<description rdf:parseType="Literal">&lt;p&gt;The Horse Herd Optimization Algorithm (HOA) is a new meta-heuristic algorithm
based on the behaviors of horses at different ages. The HOA was introduced
recently to solve complex and high-dimensional problems. This paper proposes a
binary version of the Horse Herd Optimization Algorithm (BHOA) in order to
solve discrete problems and select prominent feature subsets. Moreover, this
study provides a novel hybrid feature selection framework based on the BHOA and
a minimum Redundancy Maximum Relevance (MRMR) filter method. This hybrid
feature selection, which is more computationally efficient, produces a
beneficial subset of relevant and informative features. Since feature selection
is a binary problem, we have applied a new Transfer Function (TF), called
X-shape TF, which transforms continuous problems into binary search spaces.
Furthermore, the Support Vector Machine (SVM) is utilized to examine the
efficiency of the proposed method on ten microarray datasets, namely Lymphoma,
Prostate, Brain-1, DLBCL, SRBCT, Leukemia, Ovarian, Colon, Lung, and MLL. In
comparison to other state-of-the-art, such as the Gray Wolf (GW), Particle
Swarm Optimization (PSO), and Genetic Algorithm (GA), the proposed hybrid
method (MRMR-BHOA) demonstrates superior performance in terms of accuracy and
minimum selected features. Also, experimental results prove that the X-Shaped
BHOA approach outperforms others methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mehrabi_N/0/1/0/all/0/1&quot;&gt;Niloufar Mehrabi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boroujeni_S/0/1/0/all/0/1&quot;&gt;Sayed Pedram Haeri Boroujeni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pashaei_E/0/1/0/all/0/1&quot;&gt;Elnaz Pashaei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.01029">
<title>Explainability for Large Language Models: A Survey. (arXiv:2309.01029v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.01029</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have demonstrated impressive capabilities in
natural language processing. However, their internal mechanisms are still
unclear and this lack of transparency poses unwanted risks for downstream
applications. Therefore, understanding and explaining these models is crucial
for elucidating their behaviors, limitations, and social impacts. In this
paper, we introduce a taxonomy of explainability techniques and provide a
structured overview of methods for explaining Transformer-based language
models. We categorize techniques based on the training paradigms of LLMs:
traditional fine-tuning-based paradigm and prompting-based paradigm. For each
paradigm, we summarize the goals and dominant approaches for generating local
explanations of individual predictions and global explanations of overall model
knowledge. We also discuss metrics for evaluating generated explanations, and
discuss how explanations can be leveraged to debug models and improve
performance. Lastly, we examine key challenges and emerging opportunities for
explanation techniques in the era of LLMs in comparison to conventional machine
learning models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haiyan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hanjie Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_F/0/1/0/all/0/1&quot;&gt;Fan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_N/0/1/0/all/0/1&quot;&gt;Ninghao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_H/0/1/0/all/0/1&quot;&gt;Huiqi Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_H/0/1/0/all/0/1&quot;&gt;Hengyi Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shuaiqiang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Dawei Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_M/0/1/0/all/0/1&quot;&gt;Mengnan Du&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02685">
<title>Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02685</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion generative modeling has become a promising approach for learning
robotic manipulation tasks from stochastic human demonstrations. In this paper,
we present Diffusion-EDFs, a novel SE(3)-equivariant diffusion-based approach
for visual robotic manipulation tasks. We show that our proposed method
achieves remarkable data efficiency, requiring only 5 to 10 human
demonstrations for effective end-to-end training in less than an hour.
Furthermore, our benchmark experiments demonstrate that our approach has
superior generalizability and robustness compared to state-of-the-art methods.
Lastly, we validate our methods with real hardware experiments. Project
Website: https://sites.google.com/view/diffusion-edfs/home
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_H/0/1/0/all/0/1&quot;&gt;Hyunseok An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Junwoo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joohwan Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yubin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_C/0/1/0/all/0/1&quot;&gt;Chaewon Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1&quot;&gt;Roberto Horowitz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03060">
<title>CoLA: Exploiting Compositional Structure for Automatic and Efficient Numerical Linear Algebra. (arXiv:2309.03060v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03060</link>
<description rdf:parseType="Literal">&lt;p&gt;Many areas of machine learning and science involve large linear algebra
problems, such as eigendecompositions, solving linear systems, computing matrix
exponentials, and trace estimation. The matrices involved often have Kronecker,
convolutional, block diagonal, sum, or product structure. In this paper, we
propose a simple but general framework for large-scale linear algebra problems
in machine learning, named CoLA (Compositional Linear Algebra). By combining a
linear operator abstraction with compositional dispatch rules, CoLA
automatically constructs memory and runtime efficient numerical algorithms.
Moreover, CoLA provides memory efficient automatic differentiation, low
precision computation, and GPU acceleration in both JAX and PyTorch, while also
accommodating new objects, operations, and rules in downstream packages via
multiple dispatch. CoLA can accelerate many algebraic operations, while making
it easy to prototype matrix structures and algorithms, providing an appealing
drop-in tool for virtually any computational effort that requires linear
algebra. We showcase its efficacy across a broad range of applications,
including partial differential equations, Gaussian processes, equivariant model
construction, and unsupervised learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potapczynski_A/0/1/0/all/0/1&quot;&gt;Andres Potapczynski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Finzi_M/0/1/0/all/0/1&quot;&gt;Marc Finzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pleiss_G/0/1/0/all/0/1&quot;&gt;Geoff Pleiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wilson_A/0/1/0/all/0/1&quot;&gt;Andrew Gordon Wilson&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03579">
<title>DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.03579</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring distance or similarity between time-series data is a fundamental
aspect of many applications including classification, clustering, and
ensembling/alignment. Existing measures may fail to capture similarities among
local trends (shapes) and may even produce misleading results. Our goal is to
develop a measure that looks for similar trends occurring around similar times
and is easily interpretable for researchers in applied domains. This is
particularly useful for applications where time-series have a sequence of
meaningful local trends that are ordered, such as in epidemics (a surge to an
increase to a peak to a decrease). We propose a novel measure, DTW+S, which
creates an interpretable &quot;closeness-preserving&quot; matrix representation of the
time-series, where each column represents local trends, and then it applies
Dynamic Time Warping to compute distances between these matrices. We present a
theoretical analysis that supports the choice of this representation. We
demonstrate the utility of DTW+S in several tasks. For the clustering of
epidemic curves, we show that DTW+S is the only measure able to produce good
clustering compared to the baselines. For ensemble building, we propose a
combination of DTW+S and barycenter averaging that results in the best
preservation of characteristics of the underlying trajectories. We also
demonstrate that our approach results in better classification compared to
Dynamic Time Warping for a class of datasets, particularly when local trends
rather than scale play a decisive role.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Ajitesh Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06800">
<title>Uncertainty-aware Traffic Prediction under Missing Data. (arXiv:2309.06800v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06800</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction is a crucial topic because of its broad scope of
applications in the transportation domain. Recently, various studies have
achieved promising results. However, most studies assume the prediction
locations have complete or at least partial historical records and cannot be
extended to non-historical recorded locations. In real-life scenarios, the
deployment of sensors could be limited due to budget limitations and
installation availability, which makes most current models not applicable.
Though few pieces of literature tried to impute traffic states at the missing
locations, these methods need the data simultaneously observed at the locations
with sensors, making them not applicable to prediction tasks. Another drawback
is the lack of measurement of uncertainty in prediction, making prior works
unsuitable for risk-sensitive tasks or involving decision-making. To fill the
gap, inspired by the previous inductive graph neural network, this work
proposed an uncertainty-aware framework with the ability to 1) extend
prediction to missing locations with no historical records and significantly
extend spatial coverage of prediction locations while reducing deployment of
sensors and 2) generate probabilistic prediction with uncertainty
quantification to help the management of risk and decision making in the
down-stream tasks. Through extensive experiments on real-life datasets, the
result shows our method achieved promising results on prediction tasks, and the
uncertainty quantification gives consistent results which highly correlated
with the locations with and without historical data. We also show that our
model could help support sensor deployment tasks in the transportation field to
achieve higher accuracy with a limited sensor deployment budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mei_H/0/1/0/all/0/1&quot;&gt;Hao Mei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junxian Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Z/0/1/0/all/0/1&quot;&gt;Zhiming Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_G/0/1/0/all/0/1&quot;&gt;Guanjie Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_B/0/1/0/all/0/1&quot;&gt;Bin Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_H/0/1/0/all/0/1&quot;&gt;Hua Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.12931">
<title>On Separate Normalization in Self-supervised Transformers. (arXiv:2309.12931v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.12931</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised training methods for transformers have demonstrated
remarkable performance across various domains. Previous transformer-based
models, such as masked autoencoders (MAE), typically utilize a single
normalization layer for both the [CLS] symbol and the tokens. We propose in
this paper a simple modification that employs separate normalization layers for
the tokens and the [CLS] symbol to better capture their distinct
characteristics and enhance downstream task performance. Our method aims to
alleviate the potential negative effects of using the same normalization
statistics for both token types, which may not be optimally aligned with their
individual roles. We empirically show that by utilizing a separate
normalization layer, the [CLS] embeddings can better encode the global
contextual information and are distributed more uniformly in its anisotropic
space. When replacing the conventional normalization layer with the two
separate layers, we observe an average 2.7% performance improvement over the
image, natural language, and graph domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yinkai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_Y/0/1/0/all/0/1&quot;&gt;Yuanqi Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassoun_S/0/1/0/all/0/1&quot;&gt;Soha Hassoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Li-Ping Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00489">
<title>Dynamic DAG Discovery for Interpretable Imitation Learning. (arXiv:2310.00489v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00489</link>
<description rdf:parseType="Literal">&lt;p&gt;Imitation learning, which learns agent policy by mimicking expert
demonstration, has shown promising results in many applications such as medical
treatment regimes and self-driving vehicles. However, it remains a difficult
task to interpret control policies learned by the agent. Difficulties mainly
come from two aspects: 1) agents in imitation learning are usually implemented
as deep neural networks, which are black-box models and lack interpretability;
2) the latent causal mechanism behind agents&apos; decisions may vary along the
trajectory, rather than staying static throughout time steps. To increase
transparency and offer better interpretability of the neural agent, we propose
to expose its captured knowledge in the form of a directed acyclic causal
graph, with nodes being action and state variables and edges denoting the
causal relations behind predictions. Furthermore, we design this causal
discovery process to be state-dependent, enabling it to model the dynamics in
latent causal graphs. Concretely, we conduct causal discovery from the
perspective of Granger causality and propose a self-explainable imitation
learning framework, {\method}. The proposed framework is composed of three
parts: a dynamic causal discovery module, a causality encoding module, and a
prediction module, and is trained in an end-to-end manner. After the model is
learned, we can obtain causal relations among states and action variables
behind its decisions, exposing policies learned by it. Experimental results on
both synthetic and real-world datasets demonstrate the effectiveness of the
proposed {\method} in learning the dynamic causal graphs for understanding the
decision-making of imitation learning meanwhile maintaining high prediction
accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_T/0/1/0/all/0/1&quot;&gt;Tianxiang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_W/0/1/0/all/0/1&quot;&gt;Wenchao Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Suhang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuncong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yanchi Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haifeng Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.00965">
<title>Effective Learning with Node Perturbation in Deep Neural Networks. (arXiv:2310.00965v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.00965</link>
<description rdf:parseType="Literal">&lt;p&gt;Backpropagation (BP) is the dominant and most successful method for training
parameters of deep neural network models. However, BP relies on two
computationally distinct phases, does not provide a satisfactory explanation of
biological learning, and can be challenging to apply for training of networks
with discontinuities or noisy node dynamics. By comparison, node perturbation
(NP) proposes learning by the injection of noise into the network activations,
and subsequent measurement of the induced loss change. NP relies on two forward
(inference) passes, does not make use of network derivatives, and has been
proposed as a model for learning in biological systems. However, standard NP is
highly data inefficient and unstable due to its unguided noise-based search
process. In this work, we investigate different formulations of NP and relate
it to the concept of directional derivatives as well as combining it with a
decorrelating mechanism for layer-wise inputs. We find that a closer alignment
with directional derivatives together with input decorrelation at every layer
significantly enhances performance of NP learning, making its performance on
the train set competitive with BP and allowing its application to noisy systems
in which the noise process itself is inaccessible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dalm_S/0/1/0/all/0/1&quot;&gt;Sander Dalm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gerven_M/0/1/0/all/0/1&quot;&gt;Marcel van Gerven&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmad_N/0/1/0/all/0/1&quot;&gt;Nasir Ahmad&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01768">
<title>Backdiff: a diffusion model for generalized transferable protein backmapping. (arXiv:2310.01768v2 [q-bio.QM] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01768</link>
<description rdf:parseType="Literal">&lt;p&gt;Coarse-grained (CG) models play a crucial role in the study of protein
structures, protein thermodynamic properties, and protein conformation
dynamics. Due to the information loss in the coarse-graining process,
backmapping from CG to all-atom configurations is essential in many protein
design and drug discovery applications when detailed atomic representations are
needed for in-depth studies. Despite recent progress in data-driven backmapping
approaches, devising a backmapping method that can be universally applied
across various CG models and proteins remains unresolved. In this work, we
propose BackDiff, a new generative model designed to achieve generalization and
reliability in the protein backmapping problem. BackDiff leverages the
conditional score-based diffusion model with geometric representations. Since
different CG models can contain different coarse-grained sites which include
selected atoms (CG atoms) and simple CG auxiliary functions of atomistic
coordinates (CG auxiliary variables), we design a self-supervised training
framework to adapt to different CG atoms, and constrain the diffusion sampling
paths with arbitrary CG auxiliary variables as conditions. Our method
facilitates end-to-end training and allows efficient sampling across different
proteins and diverse CG models without the need for retraining. Comprehensive
experiments over multiple popular CG models demonstrate BackDiff&apos;s superior
performance to existing state-of-the-art approaches, and generalization and
flexibility that these approaches cannot achieve. A pretrained BackDiff model
can offer a convenient yet reliable plug-and-play solution for protein
researchers, enabling them to investigate further from their own CG models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yikai Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Ming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01870">
<title>DeepDecipher: Accessing and Investigating Neuron Activation in Large Language Models. (arXiv:2310.01870v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01870</link>
<description rdf:parseType="Literal">&lt;p&gt;As large language models (LLMs) become more capable, there is an urgent need
for interpretable and transparent tools. Current methods are difficult to
implement, and accessible tools to analyze model internals are lacking. To
bridge this gap, we present DeepDecipher - an API and interface for probing
neurons in transformer models&apos; MLP layers. DeepDecipher makes the outputs of
advanced interpretability techniques for LLMs readily available. The
easy-to-use interface also makes inspecting these complex models more
intuitive. This paper outlines DeepDecipher&apos;s design and capabilities. We
demonstrate how to analyze neurons, compare models, and gain insights into
model behavior. For example, we contrast DeepDecipher&apos;s functionality with
similar tools like Neuroscope and OpenAI&apos;s Neuron Explainer. DeepDecipher
enables efficient, scalable analysis of LLMs. By granting access to
state-of-the-art interpretability methods, DeepDecipher makes LLMs more
transparent, trustworthy, and safe. Researchers, engineers, and developers can
quickly diagnose issues, audit systems, and advance the field.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garde_A/0/1/0/all/0/1&quot;&gt;Albert Garde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kran_E/0/1/0/all/0/1&quot;&gt;Esben Kran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barez_F/0/1/0/all/0/1&quot;&gt;Fazl Barez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.01929">
<title>Navigating Cultural Chasms: Exploring and Unlocking the Cultural POV of Text-To-Image Models. (arXiv:2310.01929v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.01929</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-To-Image (TTI) models, such as DALL-E and StableDiffusion, have
demonstrated remarkable prompt-based image generation capabilities.
Multilingual encoders may have a substantial impact on the cultural agency of
these models, as language is a conduit of culture. In this study, we explore
the cultural perception embedded in TTI models by characterizing culture across
three hierarchical tiers: cultural dimensions, cultural domains, and cultural
concepts. Based on this ontology, we derive prompt templates to unlock the
cultural knowledge in TTI models, and propose a comprehensive suite of
evaluation techniques, including intrinsic evaluations using the CLIP space,
extrinsic evaluations with a Visual-Question-Answer (VQA) model and human
assessments, to evaluate the cultural content of TTI-generated images. To
bolster our research, we introduce the CulText2I dataset, derived from four
diverse TTI models and spanning ten languages. Our experiments provide insights
regarding Do, What, Which and How research questions about the nature of
cultural encoding in TTI models, paving the way for cross-cultural applications
of these models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ventura_M/0/1/0/all/0/1&quot;&gt;Mor Ventura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_David_E/0/1/0/all/0/1&quot;&gt;Eyal Ben-David&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Korhonen_A/0/1/0/all/0/1&quot;&gt;Anna Korhonen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reichart_R/0/1/0/all/0/1&quot;&gt;Roi Reichart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03605">
<title>FASER: Binary Code Similarity Search through the use of Intermediate Representations. (arXiv:2310.03605v3 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03605</link>
<description rdf:parseType="Literal">&lt;p&gt;Being able to identify functions of interest in cross-architecture software
is useful whether you are analysing for malware, securing the software supply
chain or conducting vulnerability research. Cross-Architecture Binary Code
Similarity Search has been explored in numerous studies and has used a wide
range of different data sources to achieve its goals. The data sources
typically used draw on common structures derived from binaries such as function
control flow graphs or binary level call graphs, the output of the disassembly
process or the outputs of a dynamic analysis approach. One data source which
has received less attention is binary intermediate representations. Binary
Intermediate representations possess two interesting properties: they are cross
architecture by their very nature and encode the semantics of a function
explicitly to support downstream usage. Within this paper we propose Function
as a String Encoded Representation (FASER) which combines long document
transformers with the use of intermediate representations to create a model
capable of cross architecture function search without the need for manual
feature engineering, pre-training or a dynamic analysis step. We compare our
approach against a series of baseline approaches for two tasks; A general
function search task and a targeted vulnerability search task. Our approach
demonstrates strong performance across both tasks, performing better than all
baseline approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Collyer_J/0/1/0/all/0/1&quot;&gt;Josh Collyer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Watson_T/0/1/0/all/0/1&quot;&gt;Tim Watson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_I/0/1/0/all/0/1&quot;&gt;Iain Phillips&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.03684">
<title>SmoothLLM: Defending Large Language Models Against Jailbreaking Attacks. (arXiv:2310.03684v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.03684</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite efforts to align large language models (LLMs) with human values,
widely-used LLMs such as GPT, Llama, Claude, and PaLM are susceptible to
jailbreaking attacks, wherein an adversary fools a targeted LLM into generating
objectionable content. To address this vulnerability, we propose SmoothLLM, the
first algorithm designed to mitigate jailbreaking attacks on LLMs. Based on our
finding that adversarially-generated prompts are brittle to character-level
changes, our defense first randomly perturbs multiple copies of a given input
prompt, and then aggregates the corresponding predictions to detect adversarial
inputs. SmoothLLM reduces the attack success rate on numerous popular LLMs to
below one percentage point, avoids unnecessary conservatism, and admits
provable guarantees on attack mitigation. Moreover, our defense uses
exponentially fewer queries than existing attacks and is compatible with any
LLM. Our code is publicly available at the following link:
https://github.com/arobey1/smooth-llm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Robey_A/0/1/0/all/0/1&quot;&gt;Alexander Robey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_E/0/1/0/all/0/1&quot;&gt;Eric Wong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hassani_H/0/1/0/all/0/1&quot;&gt;Hamed Hassani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pappas_G/0/1/0/all/0/1&quot;&gt;George J. Pappas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05703">
<title>An Attribution Method for Siamese Encoders. (arXiv:2310.05703v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05703</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the success of Siamese encoder models such as sentence transformers
(ST), little is known about the aspects of inputs they pay attention to. A
barrier is that their predictions cannot be attributed to individual features,
as they compare two inputs rather than processing a single one. This paper
derives a local attribution method for Siamese encoders by generalizing the
principle of integrated gradients to models with multiple inputs. The solution
takes the form of feature-pair attributions, and can be reduced to a
token-token matrix for STs. Our method involves the introduction of integrated
Jacobians and inherits the advantageous formal properties of integrated
gradients: it accounts for the model&apos;s full computation graph and is guaranteed
to converge to the actual prediction. A pilot study shows that in an ST few
token-pairs can often explain large fractions of predictions, and it focuses on
nouns and verbs. For accurate predictions, it however needs to attend to the
majority of tokens and parts of speech.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moller_L/0/1/0/all/0/1&quot;&gt;Lucas M&amp;#xf6;ller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikolaev_D/0/1/0/all/0/1&quot;&gt;Dmitry Nikolaev&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pado_S/0/1/0/all/0/1&quot;&gt;Sebastian Pad&amp;#xf3;&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.05866">
<title>Generative quantum machine learning via denoising diffusion probabilistic models. (arXiv:2310.05866v2 [quant-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2310.05866</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models are key-enabling technology to computer vision, text
generation and large language models. Denoising diffusion probabilistic models
(DDPMs) have recently gained much attention due to their ability to generate
diverse and high-quality samples in many computer vision tasks, as well as to
incorporate flexible model architectures and relatively simple training scheme.
Quantum generative models, empowered by entanglement and superposition, have
brought new insight to learning classical and quantum data. Inspired by the
classical counterpart, we propose the quantum denoising diffusion probabilistic
models (QuDDPM) to enable efficiently trainable generative learning of quantum
data. QuDDPM adopts sufficient layers of circuits to guarantee expressivity,
while introduces multiple intermediate training tasks as interpolation between
the target distribution and noise to avoid barren plateau and guarantee
efficient training. We provide bounds on the learning error and demonstrate
QuDDPM&apos;s capability in learning correlated quantum noise model, quantum
many-body phases and topological structure of quantum data. The results provide
a paradigm for versatile and efficient quantum generative learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bingzhi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Xu_P/0/1/0/all/0/1&quot;&gt;Peng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiaohui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Zhuang_Q/0/1/0/all/0/1&quot;&gt;Quntao Zhuang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08235">
<title>GROOT: Learning to Follow Instructions by Watching Gameplay Videos. (arXiv:2310.08235v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08235</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the problem of building a controller that can follow open-ended
instructions in open-world environments. We propose to follow reference videos
as instructions, which offer expressive goal specifications while eliminating
the need for expensive text-gameplay annotations. A new learning framework is
derived to allow learning such instruction-following controllers from gameplay
videos while producing a video instruction encoder that induces a structured
goal space. We implement our agent GROOT in a simple yet effective
encoder-decoder architecture based on causal transformers. We evaluate GROOT
against open-world counterparts and human players on a proposed Minecraft
SkillForge benchmark. The Elo ratings clearly show that GROOT is closing the
human-machine gap as well as exhibiting a 70% winning rate over the best
generalist agent baseline. Qualitative analysis of the induced goal space
further demonstrates some interesting emergent properties, including the goal
composition and complex gameplay behavior synthesis. The project page is
available at https://craftjarvis-groot.github.io.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_S/0/1/0/all/0/1&quot;&gt;Shaofei Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zihao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaojian Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_A/0/1/0/all/0/1&quot;&gt;Anji Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_Y/0/1/0/all/0/1&quot;&gt;Yitao Liang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.08812">
<title>A novel decomposed-ensemble time series forecasting framework: capturing underlying volatility information. (arXiv:2310.08812v4 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2310.08812</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series forecasting represents a significant and challenging task across
various fields. Recently, methods based on mode decomposition have dominated
the forecasting of complex time series because of the advantages of capturing
local characteristics and extracting intrinsic modes from data. Unfortunately,
most models fail to capture the implied volatilities that contain significant
information. To enhance the prediction of contemporary diverse and complex time
series, we propose a novel time series forecasting paradigm that integrates
decomposition with the capability to capture the underlying fluctuation
information of the series. In our methodology, we implement the Variational
Mode Decomposition algorithm to decompose the time series into K distinct
sub-modes. Following this decomposition, we apply the Generalized
Autoregressive Conditional Heteroskedasticity (GARCH) model to extract the
volatility information in these sub-modes. Subsequently, both the numerical
data and the volatility information for each sub-mode are harnessed to train a
neural network. This network is adept at predicting the information of the
sub-modes, and we aggregate the predictions of all sub-modes to generate the
final output. By integrating econometric and artificial intelligence methods,
and taking into account both the numerical and volatility information of the
time series, our proposed framework demonstrates superior performance in time
series forecasting, as evidenced by the significant decrease in MSE, RMSE, and
MAPE in our comparative experimental results.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gui_Z/0/1/0/all/0/1&quot;&gt;Zhengtao Gui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Sijie Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.09949">
<title>Chameleon: a heterogeneous and disaggregated accelerator system for retrieval-augmented language models. (arXiv:2310.09949v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2310.09949</link>
<description rdf:parseType="Literal">&lt;p&gt;A Retrieval-Augmented Language Model (RALM) augments a generative language
model by retrieving context-specific knowledge from an external database. This
strategy facilitates impressive text generation quality even with smaller
models, thus reducing orders of magnitude of computational demands. However,
RALMs introduce unique system design challenges due to (a) the diverse workload
characteristics between LM inference and retrieval and (b) the various system
requirements and bottlenecks for different RALM configurations such as model
sizes, database sizes, and retrieval frequencies. We propose Chameleon, a
heterogeneous accelerator system that integrates both LM and retrieval
accelerators in a disaggregated architecture. The heterogeneity ensures
efficient acceleration of both LM inference and retrieval, while the
accelerator disaggregation enables the system to independently scale both types
of accelerators to fulfill diverse RALM requirements. Our Chameleon prototype
implements retrieval accelerators on FPGAs and assigns LM inference to GPUs,
with a CPU server orchestrating these accelerators over the network. Compared
to CPU-based and CPU-GPU vector search systems, Chameleon achieves up to 23.72x
speedup and 26.2x energy efficiency. Evaluated on various RALMs, Chameleon
exhibits up to 2.16x reduction in latency and 3.18x speedup in throughput
compared to the hybrid CPU-GPU architecture. These promising results pave the
way for bringing accelerator heterogeneity and disaggregation into future RALM
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenqi Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeller_M/0/1/0/all/0/1&quot;&gt;Marco Zeller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waleffe_R/0/1/0/all/0/1&quot;&gt;Roger Waleffe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoefler_T/0/1/0/all/0/1&quot;&gt;Torsten Hoefler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_G/0/1/0/all/0/1&quot;&gt;Gustavo Alonso&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11518">
<title>Guarantees for Self-Play in Multiplayer Games via Polymatrix Decomposability. (arXiv:2310.11518v3 [cs.GT] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11518</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-play is a technique for machine learning in multi-agent systems where a
learning algorithm learns by interacting with copies of itself. Self-play is
useful for generating large quantities of data for learning, but has the
drawback that the agents the learner will face post-training may have
dramatically different behavior than the learner came to expect by interacting
with itself. For the special case of two-player constant-sum games, self-play
that reaches Nash equilibrium is guaranteed to produce strategies that perform
well against any post-training opponent; however, no such guarantee exists for
multiplayer games. We show that in games that approximately decompose into a
set of two-player constant-sum games (called constant-sum polymatrix games)
where global $\epsilon$-Nash equilibria are boundedly far from Nash equilibria
in each subgame (called subgame stability), any no-external-regret algorithm
that learns by self-play will produce a strategy with bounded vulnerability.
For the first time, our results identify a structural property of multiplayer
games that enable performance guarantees for the strategies produced by a broad
class of self-play algorithms. We demonstrate our findings through experiments
on Leduc poker.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+MacQueen_R/0/1/0/all/0/1&quot;&gt;Revan MacQueen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_J/0/1/0/all/0/1&quot;&gt;James R. Wright&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.12819">
<title>Hybrid Search for Efficient Planning with Completeness Guarantees. (arXiv:2310.12819v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2310.12819</link>
<description rdf:parseType="Literal">&lt;p&gt;Solving complex planning problems has been a long-standing challenge in
computer science. Learning-based subgoal search methods have shown promise in
tackling these problems, but they often suffer from a lack of completeness
guarantees, meaning that they may fail to find a solution even if one exists.
In this paper, we propose an efficient approach to augment a subgoal search
method to achieve completeness in discrete action spaces. Specifically, we
augment the high-level search with low-level actions to execute a multi-level
(hybrid) search, which we call complete subgoal search. This solution achieves
the best of both worlds: the practical efficiency of high-level search and the
completeness of low-level search. We apply the proposed search method to a
recently proposed subgoal search algorithm and evaluate the algorithm trained
on offline data on complex planning problems. We demonstrate that our complete
subgoal search not only guarantees completeness but can even improve
performance in terms of search expansions for instances that the high-level
could solve without low-level augmentations. Our approach makes it possible to
apply subgoal-level planning for systems where completeness is a critical
requirement.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kujanpaa_K/0/1/0/all/0/1&quot;&gt;Kalle Kujanp&amp;#xe4;&amp;#xe4;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pajarinen_J/0/1/0/all/0/1&quot;&gt;Joni Pajarinen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ilin_A/0/1/0/all/0/1&quot;&gt;Alexander Ilin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17462">
<title>Towards Learning Monocular 3D Object Localization From 2D Labels using the Physical Laws of Motion. (arXiv:2310.17462v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17462</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel method for precise 3D object localization in single images
from a single calibrated camera using only 2D labels. No expensive 3D labels
are needed. Thus, instead of using 3D labels, our model is trained with
easy-to-annotate 2D labels along with the physical knowledge of the object&apos;s
motion. Given this information, the model can infer the latent third dimension,
even though it has never seen this information during training. Our method is
evaluated on both synthetic and real-world datasets, and we are able to achieve
a mean distance error of just 6 cm in our experiments on real data. The results
indicate the method&apos;s potential as a step towards learning 3D object location
estimation, where collecting 3D data for training is not feasible.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kienzle_D/0/1/0/all/0/1&quot;&gt;Daniel Kienzle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lorenz_J/0/1/0/all/0/1&quot;&gt;Julian Lorenz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ludwig_K/0/1/0/all/0/1&quot;&gt;Katja Ludwig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lienhart_R/0/1/0/all/0/1&quot;&gt;Rainer Lienhart&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.18348">
<title>Meaning Representations from Trajectories in Autoregressive Models. (arXiv:2310.18348v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2310.18348</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose to extract meaning representations from autoregressive language
models by considering the distribution of all possible trajectories extending
an input text. This strategy is prompt-free, does not require fine-tuning, and
is applicable to any pre-trained autoregressive model. Moreover, unlike
vector-based representations, distribution-based representations can also model
asymmetric relations (e.g., direction of logical entailment, hypernym/hyponym
relations) by using algebraic operations between likelihood functions. These
ideas are grounded in distributional perspectives on semantics and are
connected to standard constructions in automata theory, but to our knowledge
they have not been applied to modern language models. We empirically show that
the representations obtained from large models align well with human
annotations, outperform other zero-shot and prompt-free methods on semantic
similarity tasks, and can be used to solve more complex entailment and
containment tasks that standard embeddings cannot handle. Finally, we extend
our method to represent data from different modalities (e.g., image and text)
using multimodal autoregressive models. Our code is available at:
https://github.com/tianyu139/meaning-as-trajectories
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tian Yu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trager_M/0/1/0/all/0/1&quot;&gt;Matthew Trager&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Achille_A/0/1/0/all/0/1&quot;&gt;Alessandro Achille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perera_P/0/1/0/all/0/1&quot;&gt;Pramuditha Perera&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zancato_L/0/1/0/all/0/1&quot;&gt;Luca Zancato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soatto_S/0/1/0/all/0/1&quot;&gt;Stefano Soatto&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.19583">
<title>GC-MVSNet: Multi-View, Multi-Scale, Geometrically-Consistent Multi-View Stereo. (arXiv:2310.19583v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.19583</link>
<description rdf:parseType="Literal">&lt;p&gt;Traditional multi-view stereo (MVS) methods rely heavily on photometric and
geometric consistency constraints, but newer machine learning-based MVS methods
check geometric consistency across multiple source views only as a
post-processing step. In this paper, we present a novel approach that
explicitly encourages geometric consistency of reference view depth maps across
multiple source views at different scales during learning (see Fig. 1). We find
that adding this geometric consistency loss significantly accelerates learning
by explicitly penalizing geometrically inconsistent pixels, reducing the
training iteration requirements to nearly half that of other MVS methods. Our
extensive experiments show that our approach achieves a new state-of-the-art on
the DTU and BlendedMVS datasets, and competitive results on the Tanks and
Temples benchmark. To the best of our knowledge, GC-MVSNet is the first attempt
to enforce multi-view, multi-scale geometric consistency during learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vats_V/0/1/0/all/0/1&quot;&gt;Vibhas K. Vats&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joshi_S/0/1/0/all/0/1&quot;&gt;Sripad Joshi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crandall_D/0/1/0/all/0/1&quot;&gt;David J. Crandall&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reza_M/0/1/0/all/0/1&quot;&gt;Md. Alimoor Reza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_S/0/1/0/all/0/1&quot;&gt;Soon-heung Jung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.00290">
<title>Inference of CO2 flow patterns -- a feasibility study. (arXiv:2311.00290v2 [cs.CE] UPDATED)</title>
<link>http://arxiv.org/abs/2311.00290</link>
<description rdf:parseType="Literal">&lt;p&gt;As the global deployment of carbon capture and sequestration (CCS) technology
intensifies in the fight against climate change, it becomes increasingly
imperative to establish robust monitoring and detection mechanisms for
potential underground CO2 leakage, particularly through pre-existing or induced
faults in the storage reservoir&apos;s seals. While techniques such as history
matching and time-lapse seismic monitoring of CO2 storage have been used
successfully in tracking the evolution of CO2 plumes in the subsurface, these
methods lack principled approaches to characterize uncertainties related to the
CO2 plumes&apos; behavior. Inclusion of systematic assessment of uncertainties is
essential for risk mitigation for the following reasons: (i) CO2 plume-induced
changes are small and seismic data is noisy; (ii) changes between regular and
irregular (e.g., caused by leakage) flow patterns are small; and (iii) the
reservoir properties that control the flow are strongly heterogeneous and
typically only available as distributions. To arrive at a formulation capable
of inferring flow patterns for regular and irregular flow from well and seismic
data, the performance of conditional normalizing flow will be analyzed on a
series of carefully designed numerical experiments. While the inferences
presented are preliminary in the context of an early CO2 leakage detection
system, the results do indicate that inferences with conditional normalizing
flows can produce high-fidelity estimates for CO2 plumes with or without
leakage. We are also confident that the inferred uncertainty is reasonable
because it correlates well with the observed errors. This uncertainty stems
from noise in the seismic data and from the lack of precise knowledge of the
reservoir&apos;s fluid flow properties.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gahlot_A/0/1/0/all/0/1&quot;&gt;Abhinav Prakash Gahlot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdinc_H/0/1/0/all/0/1&quot;&gt;Huseyin Tuna Erdinc&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orozco_R/0/1/0/all/0/1&quot;&gt;Rafael Orozco&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_Z/0/1/0/all/0/1&quot;&gt;Ziyi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Herrmann_F/0/1/0/all/0/1&quot;&gt;Felix J. Herrmann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08569">
<title>Uncertainty Quantification in Neural-Network Based Pain Intensity Estimation. (arXiv:2311.08569v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08569</link>
<description rdf:parseType="Literal">&lt;p&gt;Improper pain management can lead to severe physical or mental consequences,
including suffering, and an increased risk of opioid dependency. Assessing the
presence and severity of pain is imperative to prevent such outcomes and
determine the appropriate intervention. However, the evaluation of pain
intensity is challenging because different individuals experience pain
differently. To overcome this, researchers have employed machine learning
models to evaluate pain intensity objectively. However, these efforts have
primarily focused on point estimation of pain, disregarding the inherent
uncertainty and variability present in the data and model. Consequently, the
point estimates provide only partial information for clinical decision-making.
This study presents a neural network-based method for objective pain interval
estimation, incorporating uncertainty quantification. This work explores three
algorithms: the bootstrap method, lower and upper bound estimation (LossL)
optimized by genetic algorithm, and modified lower and upper bound estimation
(LossS) optimized by gradient descent algorithm. Our empirical results reveal
that LossS outperforms the other two by providing a narrower prediction
interval. As LossS outperforms, we assessed its performance in three different
scenarios for pain assessment: (1) a generalized approach (single model for the
entire population), (2) a personalized approach (separate model for each
individual), and (3) a hybrid approach (separate model for each cluster of
individuals). Our findings demonstrate the hybrid approach&apos;s superior
performance, with notable practicality in clinical contexts. It has the
potential to be a valuable tool for clinicians, enabling objective pain
intensity assessment while taking uncertainty into account. This capability is
crucial in facilitating effective pain management and reducing the risks
associated with improper treatment.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozek_B/0/1/0/all/0/1&quot;&gt;Burcu Ozek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhenyuan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Radhakrishnan_S/0/1/0/all/0/1&quot;&gt;Srinivasan Radhakrishnan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kamarthi_S/0/1/0/all/0/1&quot;&gt;Sagar Kamarthi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08745">
<title>Using Stochastic Gradient Descent to Smooth Nonconvex Functions: Analysis of Implicit Graduated Optimization with Optimal Noise Scheduling. (arXiv:2311.08745v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08745</link>
<description rdf:parseType="Literal">&lt;p&gt;The graduated optimization approach is a heuristic method for finding
globally optimal solutions for nonconvex functions and has been theoretically
analyzed in several studies. This paper defines a new family of nonconvex
functions for graduated optimization, discusses their sufficient conditions,
and provides a convergence analysis of the graduated optimization algorithm for
them. It shows that stochastic gradient descent (SGD) with mini-batch
stochastic gradients has the effect of smoothing the function, the degree of
which is determined by the learning rate and batch size. This finding provides
theoretical insights on why large batch sizes fall into sharp local minima, why
decaying learning rates and increasing batch sizes are superior to fixed
learning rates and batch sizes, and what the optimal learning rate scheduling
is. To the best of our knowledge, this is the first paper to provide a
theoretical explanation for these aspects. Moreover, a new graduated
optimization framework that uses a decaying learning rate and increasing batch
size is analyzed and experimental results of image classification that support
our theoretical findings are reported.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sato_N/0/1/0/all/0/1&quot;&gt;Naoki Sato&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Iiduka_H/0/1/0/all/0/1&quot;&gt;Hideaki Iiduka&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.08972">
<title>Unsupervised approaches based on optimal transport and convex analysis for inverse problems in imaging. (arXiv:2311.08972v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.08972</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised deep learning approaches have recently become one of the crucial
research areas in imaging owing to their ability to learn expressive and
powerful reconstruction operators even when paired high-quality training data
is scarcely available. In this chapter, we review theoretically principled
unsupervised learning schemes for solving imaging inverse problems, with a
particular focus on methods rooted in optimal transport and convex analysis. We
begin by reviewing the optimal transport-based unsupervised approaches such as
the cycle-consistency-based models and learned adversarial regularization
methods, which have clear probabilistic interpretations. Subsequently, we give
an overview of a recent line of works on provably convergent learned
optimization algorithms applied to accelerate the solution of imaging inverse
problems, alongside their dedicated unsupervised training schemes. We also
survey a number of provably convergent plug-and-play algorithms (based on
gradient-step deep denoisers), which are among the most important and widely
applied unsupervised approaches for imaging problems. At the end of this
survey, we provide an overview of a few related unsupervised learning
frameworks that complement our focused schemes. Together with a detailed
survey, we provide an overview of the key mathematical results that underlie
the methods reviewed in the chapter to keep our discussion self-contained.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carioni_M/0/1/0/all/0/1&quot;&gt;Marcello Carioni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mukherjee_S/0/1/0/all/0/1&quot;&gt;Subhadip Mukherjee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_H/0/1/0/all/0/1&quot;&gt;Hong Ye Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Junqi Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.09511">
<title>Identifying Systems with Symmetries using Equivariant Autoregressive Reservoir Computers. (arXiv:2311.09511v2 [eess.SY] UPDATED)</title>
<link>http://arxiv.org/abs/2311.09511</link>
<description rdf:parseType="Literal">&lt;p&gt;The investigation reported in this document focuses on identifying systems
with symmetries using equivariant autoregressive reservoir computers. General
results in structured matrix approximation theory are presented, exploring a
two-fold approach. Firstly, a comprehensive examination of generic
symmetry-preserving nonlinear time delay embedding is conducted. This involves
analyzing time series data sampled from an equivariant system under study.
Secondly, sparse least-squares methods are applied to discern approximate
representations of the output coupling matrices. These matrices play a pivotal
role in determining the nonlinear autoregressive representation of an
equivariant system. The structural characteristics of these matrices are
dictated by the set of symmetries inherent in the system. The document outlines
prototypical algorithms derived from the described techniques, offering insight
into their practical applications. Emphasis is placed on their effectiveness in
the identification and predictive simulation of equivariant nonlinear systems,
regardless of whether such systems exhibit chaotic behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vides_F/0/1/0/all/0/1&quot;&gt;Fredy Vides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Nogueira_I/0/1/0/all/0/1&quot;&gt;Idelfonso B. R. Nogueira&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Banegas_L/0/1/0/all/0/1&quot;&gt;Lendy Banegas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Flores_E/0/1/0/all/0/1&quot;&gt;Evelyn Flores&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.10642">
<title>Rethinking Attention: Exploring Shallow Feed-Forward Neural Networks as an Alternative to Attention Layers in Transformers. (arXiv:2311.10642v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.10642</link>
<description rdf:parseType="Literal">&lt;p&gt;This work presents an analysis of the effectiveness of using standard shallow
feed-forward networks to mimic the behavior of the attention mechanism in the
original Transformer model, a state-of-the-art architecture for
sequence-to-sequence tasks. We substitute key elements of the attention
mechanism in the Transformer with simple feed-forward networks, trained using
the original components via knowledge distillation. Our experiments, conducted
on the IWSLT2017 dataset, reveal the capacity of these &quot;attentionless
Transformers&quot; to rival the performance of the original architecture. Through
rigorous ablation studies, and experimenting with various replacement network
types and sizes, we offer insights that support the viability of our approach.
This not only sheds light on the adaptability of shallow feed-forward networks
in emulating attention mechanisms but also underscores their potential to
streamline complex architectures for sequence-to-sequence tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bozic_V/0/1/0/all/0/1&quot;&gt;Vukasin Bozic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dordevic_D/0/1/0/all/0/1&quot;&gt;Danilo Dordevic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Coppola_D/0/1/0/all/0/1&quot;&gt;Daniele Coppola&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thommes_J/0/1/0/all/0/1&quot;&gt;Joseph Thommes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Sidak Pal Singh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11254">
<title>BOIS: Bayesian Optimization of Interconnected Systems. (arXiv:2311.11254v3 [stat.ML] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11254</link>
<description rdf:parseType="Literal">&lt;p&gt;Bayesian optimization (BO) has proven to be an effective paradigm for the
global optimization of expensive-to-sample systems. One of the main advantages
of BO is its use of Gaussian processes (GPs) to characterize model uncertainty
which can be leveraged to guide the learning and search process. However, BO
typically treats systems as black-boxes and this limits the ability to exploit
structural knowledge (e.g., physics and sparse interconnections). Composite
functions of the form $f(x, y(x))$, wherein GP modeling is shifted from the
performance function $f$ to an intermediate function $y$, offer an avenue for
exploiting structural knowledge. However, the use of composite functions in a
BO framework is complicated by the need to generate a probability density for
$f$ from the Gaussian density of $y$ calculated by the GP (e.g., when $f$ is
nonlinear it is not possible to obtain a closed-form expression). Previous work
has handled this issue using sampling techniques; these are easy to implement
and flexible but are computationally intensive. In this work, we introduce a
new paradigm which allows for the efficient use of composite functions in BO;
this uses adaptive linearizations of $f$ to obtain closed-form expressions for
the statistical moments of the composite function. We show that this simple
approach (which we call BOIS) enables the exploitation of structural knowledge,
such as that arising in interconnected systems as well as systems that embed
multiple GP models and combinations of physics and GP models. Using a chemical
process optimization case study, we benchmark the effectiveness of BOIS against
standard BO and sampling approaches. Our results indicate that BOIS achieves
performance gains and accurately captures the statistics of composite
functions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Gonzalez_L/0/1/0/all/0/1&quot;&gt;Leonardo D. Gonz&amp;#xe1;lez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Zavala_V/0/1/0/all/0/1&quot;&gt;Victor M. Zavala&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.11772">
<title>A Good Feature Extractor Is All You Need for Weakly Supervised Learning in Histopathology. (arXiv:2311.11772v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.11772</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning is revolutionising pathology, offering novel opportunities in
disease prognosis and personalised treatment. Historically, stain normalisation
has been a crucial preprocessing step in computational pathology pipelines, and
persists into the deep learning era. Yet, with the emergence of feature
extractors trained using self-supervised learning (SSL) on diverse pathology
datasets, we call this practice into question. In an empirical evaluation of
publicly available feature extractors, we find that omitting stain
normalisation and image augmentations does not compromise downstream
performance, while incurring substantial savings in memory and compute.
Further, we show that the top-performing feature extractors are remarkably
robust to variations in stain and augmentations like rotation in their latent
space. Contrary to previous patch-level benchmarking studies, our approach
emphasises clinical relevance by focusing on slide-level prediction tasks in a
weakly supervised setting with external validation cohorts. This work
represents the most comprehensive robustness evaluation of public pathology SSL
feature extractors to date, involving more than 6,000 training runs across nine
tasks, five datasets, three downstream architectures, and various preprocessing
setups. Our findings stand to streamline digital pathology workflows by
minimising preprocessing needs and informing the selection of feature
extractors.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolflein_G/0/1/0/all/0/1&quot;&gt;Georg W&amp;#xf6;lflein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferber_D/0/1/0/all/0/1&quot;&gt;Dyke Ferber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meneghetti_A/0/1/0/all/0/1&quot;&gt;Asier Rabasco Meneghetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nahhas_O/0/1/0/all/0/1&quot;&gt;Omar S. M. El Nahhas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truhn_D/0/1/0/all/0/1&quot;&gt;Daniel Truhn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Carrero_Z/0/1/0/all/0/1&quot;&gt;Zunamys I. Carrero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harrison_D/0/1/0/all/0/1&quot;&gt;David J. Harrison&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arandjelovic_O/0/1/0/all/0/1&quot;&gt;Ognjen Arandjelovi&amp;#x107;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kather_J/0/1/0/all/0/1&quot;&gt;Jakob N. Kather&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.12754">
<title>SelfOcc: Self-Supervised Vision-Based 3D Occupancy Prediction. (arXiv:2311.12754v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.12754</link>
<description rdf:parseType="Literal">&lt;p&gt;3D occupancy prediction is an important task for the robustness of
vision-centric autonomous driving, which aims to predict whether each point is
occupied in the surrounding 3D space. Existing methods usually require 3D
occupancy labels to produce meaningful results. However, it is very laborious
to annotate the occupancy status of each voxel. In this paper, we propose
SelfOcc to explore a self-supervised way to learn 3D occupancy using only video
sequences. We first transform the images into the 3D space (e.g., bird&apos;s eye
view) to obtain 3D representation of the scene. We directly impose constraints
on the 3D representations by treating them as signed distance fields. We can
then render 2D images of previous and future frames as self-supervision signals
to learn the 3D representations. We propose an MVS-embedded strategy to
directly optimize the SDF-induced weights with multiple depth proposals. Our
SelfOcc outperforms the previous best method SceneRF by 58.7% using a single
frame as input on SemanticKITTI and is the first self-supervised work that
produces reasonable 3D occupancy for surround cameras on nuScenes. SelfOcc
produces high-quality depth and achieves state-of-the-art results on novel
depth synthesis, monocular depth estimation, and surround-view depth estimation
on the SemanticKITTI, KITTI-2015, and nuScenes, respectively. Code:
https://github.com/huang-yh/SelfOcc.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuanhui Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wenzhao Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Borui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14056">
<title>DPSUR: Accelerating Differentially Private Stochastic Gradient Descent Using Selective Update and Release. (arXiv:2311.14056v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14056</link>
<description rdf:parseType="Literal">&lt;p&gt;Machine learning models are known to memorize private data to reduce their
training loss, which can be inadvertently exploited by privacy attacks such as
model inversion and membership inference. To protect against these attacks,
differential privacy (DP) has become the de facto standard for
privacy-preserving machine learning, particularly those popular training
algorithms using stochastic gradient descent, such as DPSGD. Nonetheless, DPSGD
still suffers from severe utility loss due to its slow convergence. This is
partially caused by the random sampling, which brings bias and variance to the
gradient, and partially by the Gaussian noise, which leads to fluctuation of
gradient updates.
&lt;/p&gt;
&lt;p&gt;Our key idea to address these issues is to apply selective updates to the
model training, while discarding those useless or even harmful updates.
Motivated by this, this paper proposes DPSUR, a Differentially Private training
framework based on Selective Updates and Release, where the gradient from each
iteration is evaluated based on a validation test, and only those updates
leading to convergence are applied to the model. As such, DPSUR ensures the
training in the right direction and thus can achieve faster convergence than
DPSGD. The main challenges lie in two aspects -- privacy concerns arising from
gradient evaluation, and gradient selection strategy for model update. To
address the challenges, DPSUR introduces a clipping strategy for update
randomization and a threshold mechanism for gradient selection. Experiments
conducted on MNIST, FMNIST, CIFAR-10, and IMDB datasets show that DPSUR
significantly outperforms previous works in terms of convergence speed and
model utility.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qingqing Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_H/0/1/0/all/0/1&quot;&gt;Haibo Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhili Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lulu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kuncan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ran_X/0/1/0/all/0/1&quot;&gt;Xun Ran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14675">
<title>Fast and Expressive Gesture Recognition using a Combination-Homomorphic Electromyogram Encoder. (arXiv:2311.14675v2 [cs.HC] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14675</link>
<description rdf:parseType="Literal">&lt;p&gt;We study the task of gesture recognition from electromyography (EMG), with
the goal of enabling expressive human-computer interaction at high accuracy,
while minimizing the time required for new subjects to provide calibration
data. To fulfill these goals, we define combination gestures consisting of a
direction component and a modifier component. New subjects only demonstrate the
single component gestures and we seek to extrapolate from these to all possible
single or combination gestures. We extrapolate to unseen combination gestures
by combining the feature vectors of real single gestures to produce synthetic
training data. This strategy allows us to provide a large and flexible gesture
vocabulary, while not requiring new subjects to demonstrate combinatorially
many example gestures. We pre-train an encoder and a combination operator using
self-supervision, so that we can produce useful synthetic training data for
unseen test subjects. To evaluate the proposed method, we collect a real-world
EMG dataset, and measure the effect of augmented supervision against two
baselines: a partially-supervised model trained with only single gesture data
from the unseen subject, and a fully-supervised model trained with real single
and real combination gesture data from the unseen subject. We find that the
proposed method provides a dramatic improvement over the partially-supervised
model, and achieves a useful classification accuracy that in some cases
approaches the performance of the fully-supervised model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Smedemark_Margulies_N/0/1/0/all/0/1&quot;&gt;Niklas Smedemark-Margulies&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bicer_Y/0/1/0/all/0/1&quot;&gt;Yunus Bicer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sunger_E/0/1/0/all/0/1&quot;&gt;Elifnur Sunger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Imbiriba_T/0/1/0/all/0/1&quot;&gt;Tales Imbiriba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tunik_E/0/1/0/all/0/1&quot;&gt;Eugene Tunik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Erdogmus_D/0/1/0/all/0/1&quot;&gt;Deniz Erdogmus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yarossi_M/0/1/0/all/0/1&quot;&gt;Mathew Yarossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Walters_R/0/1/0/all/0/1&quot;&gt;Robin Walters&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14698">
<title>Business Policy Experiments using Fractional Factorial Designs: Consumer Retention on DoorDash. (arXiv:2311.14698v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14698</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates an approach to both speed up business decision-making
and lower the cost of learning through experimentation by factorizing business
policies and employing fractional factorial experimental designs for their
evaluation. We illustrate how this method integrates with advances in the
estimation of heterogeneous treatment effects, elaborating on its advantages
and foundational assumptions. We empirically demonstrate the implementation and
benefits of our approach and assess its validity in evaluating consumer
promotion policies at DoorDash, which is one of the largest delivery platforms
in the US. Our approach discovers a policy with 5% incremental profit at 67%
lower implementation cost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Tang_Y/0/1/0/all/0/1&quot;&gt;Yixin Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yicong Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Sahni_N/0/1/0/all/0/1&quot;&gt;Navdeep S. Sahni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.14743">
<title>A Baseline Analysis of Reward Models&apos; Ability To Accurately Analyze Foundation Models Under Distribution Shift. (arXiv:2311.14743v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2311.14743</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models, specifically Large Language Models (LLM&apos;s), have lately
gained wide-spread attention and adoption. Reinforcement Learning with Human
Feedback (RLHF) involves training a reward model to capture desired behaviors,
which is then used to align an LLM. These reward models are additionally used
at inference-time to estimate how well LLM responses adhere to those desired
behaviors. However, there is little work measuring how robust these reward
models are to distribution shifts. In this work, we evaluate how reward model
performance - measured via accuracy and calibration (i.e. alignment between
accuracy and confidence) - is affected by distribution shift. We show novel
calibration patterns and accuracy drops due to OOD prompts and responses, and
that the reward model is more sensitive to shifts in responses than prompts.
Additionally, we adapt an OOD detection technique commonly used in
classification to the reward model setting in order to detect these
distribution shifts in prompts and responses.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pikus_B/0/1/0/all/0/1&quot;&gt;Ben Pikus&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+LeVine_W/0/1/0/all/0/1&quot;&gt;Will LeVine&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_T/0/1/0/all/0/1&quot;&gt;Tony Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hendryx_S/0/1/0/all/0/1&quot;&gt;Sean Hendryx&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.15940">
<title>Physics-informed neural networks for transformed geometries and manifolds. (arXiv:2311.15940v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.15940</link>
<description rdf:parseType="Literal">&lt;p&gt;Physics-informed neural networks (PINNs) effectively embed physical
principles into machine learning, but often struggle with complex or
alternating geometries. We propose a novel method for integrating geometric
transformations within PINNs to robustly accommodate geometric variations. Our
method incorporates a diffeomorphism as a mapping of a reference domain and
adapts the derivative computation of the physics-informed loss function. This
generalizes the applicability of PINNs not only to smoothly deformed domains,
but also to lower-dimensional manifolds and allows for direct shape
optimization while training the network. We demonstrate the effectivity of our
approach on several problems: (i) Eikonal equation on Archimedean spiral, (ii)
Poisson problem on surface manifold, (iii) Incompressible Stokes flow in
deformed tube, and (iv) Shape optimization with Laplace operator. Through these
examples, we demonstrate the enhanced flexibility over traditional PINNs,
especially under geometric variations. The proposed framework presents an
outlook for training deep neural operators over parametrized geometries, paving
the way for advanced modeling with PDEs on complex geometries in science and
engineering.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Burbulla_S/0/1/0/all/0/1&quot;&gt;Samuel Burbulla&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16203">
<title>ChatTraffic: Text-to-Traffic Generation via Diffusion Model. (arXiv:2311.16203v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16203</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic prediction is one of the most significant foundations in Intelligent
Transportation Systems (ITS). Traditional traffic prediction methods rely only
on historical traffic data to predict traffic trends and face two main
challenges. 1) insensitivity to unusual events. 2) poor performance in
long-term prediction. In this work, we explore how generative models combined
with text describing the traffic system can be applied for traffic generation
and name the task Text-to-Traffic Generation (TTG). The key challenge of the
TTG task is how to associate text with the spatial structure of the road
network and traffic data for generating traffic situations. To this end, we
propose ChatTraffic, the first diffusion model for text-to-traffic generation.
To guarantee the consistency between synthetic and real data, we augment a
diffusion model with the Graph Convolutional Network (GCN) to extract spatial
correlations of traffic data. In addition, we construct a large dataset
containing text-traffic pairs for the TTG task. We benchmarked our model
qualitatively and quantitatively on the released dataset. The experimental
results indicate that ChatTraffic can generate realistic traffic situations
from the text. Our code and dataset are available at
https://github.com/ChyaZhang/ChatTraffic.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chengyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_Q/0/1/0/all/0/1&quot;&gt;Qitan Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Bo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Y/0/1/0/all/0/1&quot;&gt;Yisheng Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Piao_X/0/1/0/all/0/1&quot;&gt;Xinglin Piao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_B/0/1/0/all/0/1&quot;&gt;Baocai Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16519">
<title>B-LSTM-MIONet: Bayesian LSTM-based Neural Operators for Learning the Response of Complex Dynamical Systems to Length-Variant Multiple Input Functions. (arXiv:2311.16519v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16519</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep Operator Network (DeepONet) is a neural network framework for learning
nonlinear operators such as those from ordinary differential equations (ODEs)
describing complex systems. Multiple-input deep neural operators (MIONet)
extended DeepONet to allow multiple input functions in different Banach spaces.
MIONet offers flexibility in training dataset grid spacing, without constraints
on output location. However, it requires offline inputs and cannot handle
varying sequence lengths in testing datasets, limiting its real-time
application in dynamic complex systems. This work redesigns MIONet, integrating
Long Short Term Memory (LSTM) to learn neural operators from time-dependent
data. This approach overcomes data discretization constraints and harnesses
LSTM&apos;s capability with variable-length, real-time data. Factors affecting
learning performance, like algorithm extrapolation ability are presented. The
framework is enhanced with uncertainty quantification through a novel Bayesian
method, sampling from MIONet parameter distributions. Consequently, we develop
the B-LSTM-MIONet, incorporating LSTM&apos;s temporal strengths with Bayesian
robustness, resulting in a more precise and reliable model for noisy datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_Z/0/1/0/all/0/1&quot;&gt;Zhihao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mollaali_A/0/1/0/all/0/1&quot;&gt;Amirhossein Mollaali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moya_C/0/1/0/all/0/1&quot;&gt;Christian Moya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_N/0/1/0/all/0/1&quot;&gt;Na Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_G/0/1/0/all/0/1&quot;&gt;Guang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16614">
<title>A Multivariate Unimodality Test Harnenssing the Dip Statistic of Mahalanobis Distances Over Random Projections. (arXiv:2311.16614v2 [stat.ME] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16614</link>
<description rdf:parseType="Literal">&lt;p&gt;Unimodality, pivotal in statistical analysis, offers insights into dataset
structures and drives sophisticated analytical procedures. While unimodality&apos;s
confirmation is straightforward for one-dimensional data using methods like
Silverman&apos;s approach and Hartigans&apos; dip statistic, its generalization to higher
dimensions remains challenging. By extrapolating one-dimensional unimodality
principles to multi-dimensional spaces through linear random projections and
leveraging point-to-point distancing, our method, rooted in
$\alpha$-unimodality assumptions, presents a novel multivariate unimodality
test named mud-pod. Both theoretical and empirical studies confirm the efficacy
of our method in unimodality assessment of multidimensional datasets as well as
in estimating the number of clusters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Kolyvakis_P/0/1/0/all/0/1&quot;&gt;Prodromos Kolyvakis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/stat/1/au:+Likas_A/0/1/0/all/0/1&quot;&gt;Aristidis Likas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16834">
<title>Modular Neural Networks for Time Series Forecasting: Interpretability and Feature Selection using Attention. (arXiv:2311.16834v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16834</link>
<description rdf:parseType="Literal">&lt;p&gt;Multivariate time series have many applications, from healthcare and
meteorology to life science. Although deep learning models have shown excellent
predictive performance for time series, they have been criticised for being
&quot;black-boxes&quot; or non-interpretable. This paper proposes a novel modular neural
network model for multivariate time series prediction that is interpretable by
construction. A recurrent neural network learns the temporal dependencies in
the data while an attention-based feature selection component selects the most
relevant features and suppresses redundant features used in the learning of the
temporal dependencies. A modular deep network is trained from the selected
features independently to show the users how features influence outcomes,
making the model interpretable. Experimental results show that this approach
can outperform state-of-the-art interpretable Neural Additive Models (NAM) and
variations thereof in both regression and classification of time series tasks,
achieving a predictive performance that is comparable to the top
non-interpretable methods for time series, LSTM and XGBoost.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qiqi Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kloukinas_C/0/1/0/all/0/1&quot;&gt;Christos Kloukinas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garcez_A/0/1/0/all/0/1&quot;&gt;Artur d&amp;#x27;Avila Garcez&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16883">
<title>Compressing the Backward Pass of Large-Scale Neural Architectures by Structured Activation Pruning. (arXiv:2311.16883v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.16883</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of Deep Neural Networks (DNNs) has led to an increase in model size
and complexity, straining the memory capacity of GPUs. Sparsity in DNNs,
characterized as structural or ephemeral, has gained attention as a solution.
This work focuses on ephemeral sparsity, aiming to reduce memory consumption
during training. It emphasizes the significance of activations, an often
overlooked component, and their role in memory usage. This work employs
structured pruning in Block Sparse Compressed Row (BSR) format in combination
with a magnitude-based criterion to efficiently prune activations. We
furthermore introduce efficient block-sparse operators for GPUs and showcase
their effectiveness, as well as the superior compression offered by block
sparsity. We report the effectiveness of activation pruning by evaluating
training speed, accuracy, and memory usage of large-scale neural architectures
on the example of ResMLP on image classification tasks. As a result, we observe
a memory reduction of up to 32% while maintaining accuracy. Ultimately, our
approach aims to democratize large-scale model training, reduce GPU
requirements, and address ecological concerns.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barley_D/0/1/0/all/0/1&quot;&gt;Daniel Barley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Froning_H/0/1/0/all/0/1&quot;&gt;Holger Fr&amp;#xf6;ning&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16410">
<title>Reduced-order modeling for parameterized PDEs via implicit neural representations. (arXiv:2311.16410v1 [math.NA] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.16410</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a new data-driven reduced-order modeling approach to efficiently
solve parametrized partial differential equations (PDEs) for many-query
problems. This work is inspired by the concept of implicit neural
representation (INR), which models physics signals in a continuous manner and
independent of spatial/temporal discretization. The proposed framework encodes
PDE and utilizes a parametrized neural ODE (PNODE) to learn latent dynamics
characterized by multiple PDE parameters. PNODE can be inferred by a
hypernetwork to reduce the potential difficulties in learning PNODE due to a
complex multilayer perceptron (MLP). The framework uses an INR to decode the
latent dynamics and reconstruct accurate PDE solutions. Further, a
physics-informed loss is also introduced to correct the prediction of unseen
parameter instances. Incorporating the physics-informed loss also enables the
model to be fine-tuned in an unsupervised manner on unseen PDE parameters. A
numerical experiment is performed on a two-dimensional Burgers equation with a
large variation of PDE parameters. We evaluate the proposed method at a large
Reynolds number and obtain up to speedup of O(10^3) and ~1% relative error to
the ground truth values.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Wen_T/0/1/0/all/0/1&quot;&gt;Tianshu Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kookjin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Choi_Y/0/1/0/all/0/1&quot;&gt;Youngsoo Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.16459">
<title>On the Effect of Defections in Federated Learning and How to Prevent Them. (arXiv:2311.16459v1 [cs.LG] CROSS LISTED)</title>
<link>http://arxiv.org/abs/2311.16459</link>
<description rdf:parseType="Literal">&lt;p&gt;Federated learning is a machine learning protocol that enables a large
population of agents to collaborate over multiple rounds to produce a single
consensus model. There are several federated learning applications where agents
may choose to defect permanently$-$essentially withdrawing from the
collaboration$-$if they are content with their instantaneous model in that
round. This work demonstrates the detrimental impact of such defections on the
final model&apos;s robustness and ability to generalize. We also show that current
federated optimization algorithms fail to disincentivize these harmful
defections. We introduce a novel optimization algorithm with theoretical
guarantees to prevent defections while ensuring asymptotic convergence to an
effective solution for all participating agents. We also provide numerical
experiments to corroborate our findings and demonstrate the effectiveness of
our algorithm.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_M/0/1/0/all/0/1&quot;&gt;Minbiao Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_K/0/1/0/all/0/1&quot;&gt;Kumar Kshitij Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_H/0/1/0/all/0/1&quot;&gt;Han Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lingxiao Wang&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>