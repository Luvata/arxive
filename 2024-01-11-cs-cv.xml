<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2024-01-09T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04114" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04150" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04154" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04181" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04210" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04230" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04244" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04247" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04257" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04290" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04317" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04325" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04332" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04339" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04345" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04350" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04354" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04357" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04364" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04377" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04390" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04405" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04425" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04435" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04437" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04441" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04448" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04463" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04464" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04486" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04550" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04560" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04575" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04578" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04585" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04608" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04614" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04647" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04666" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04716" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04718" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04722" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.04730" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2109.05206" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2201.12577" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.10347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2209.08891" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.07675" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.06797" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2211.15374" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.00950" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.13337" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.07545" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.15225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.01899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.12760" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.14811" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.10406" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13035" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.14093" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.16713" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03992" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14277" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.06143" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.07254" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.15216" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11204" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.11650" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2310.17974" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.02960" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2311.13964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.01964" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.04265" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.11535" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15144" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2312.15731" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00155" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.00897" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01286" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.01990" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02032" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.02791" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03836" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2401.03885" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2401.04114">
<title>Timeline-based Process Discovery. (arXiv:2401.04114v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04114</link>
<description rdf:parseType="Literal">&lt;p&gt;A key concern of automatic process discovery is to provide insights into
performance aspects of business processes. Waiting times are of particular
importance in this context. For that reason, it is surprising that current
techniques for automatic process discovery generate directly-follows graphs and
comparable process models, but often miss the opportunity to explicitly
represent the time axis. In this paper, we present an approach for
automatically constructing process models that explicitly align with a time
axis. We exemplify our approach for directly-follows graphs. Our evaluation
using two BPIC datasets and a proprietary dataset highlight the benefits of
this representation in comparison to standard layout techniques.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kaur_H/0/1/0/all/0/1&quot;&gt;Harleen Kaur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mendling_J/0/1/0/all/0/1&quot;&gt;Jan Mendling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rubensson_C/0/1/0/all/0/1&quot;&gt;Christoffer Rubensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kampik_T/0/1/0/all/0/1&quot;&gt;Timotheus Kampik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04116">
<title>Semantic Draw Engineering for Text-to-Image Creation. (arXiv:2401.04116v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2401.04116</link>
<description rdf:parseType="Literal">&lt;p&gt;Text-to-image generation is conducted through Generative Adversarial Networks
(GANs) or transformer models. However, the current challenge lies in accurately
generating images based on textual descriptions, especially in scenarios where
the content and theme of the target image are ambiguous. In this paper, we
propose a method that utilizes artificial intelligence models for thematic
creativity, followed by a classification modeling of the actual painting
process. The method involves converting all visual elements into quantifiable
data structures before creating images. We evaluate the effectiveness of this
approach in terms of semantic accuracy, image reproducibility, and
computational efficiency, in comparison with existing image generation
algorithms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Huaqiang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yangkai Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04143">
<title>RHOBIN Challenge: Reconstruction of Human Object Interaction. (arXiv:2401.04143v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04143</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling the interaction between humans and objects has been an emerging
research direction in recent years. Capturing human-object interaction is
however a very challenging task due to heavy occlusion and complex dynamics,
which requires understanding not only 3D human pose, and object pose but also
the interaction between them. Reconstruction of 3D humans and objects has been
two separate research fields in computer vision for a long time. We hence
proposed the first RHOBIN challenge: reconstruction of human-object
interactions in conjunction with the RHOBIN workshop. It was aimed at bringing
the research communities of human and object reconstruction as well as
interaction modeling together to discuss techniques and exchange ideas. Our
challenge consists of three tracks of 3D reconstruction from monocular RGB
images with a focus on dealing with challenging interaction scenarios. Our
challenge attracted more than 100 participants with more than 300 submissions,
indicating the broad interest in the research communities. This paper describes
the settings of our challenge and discusses the winning methods of each track
in more detail. We observe that the human reconstruction task is becoming
mature even under heavy occlusion settings while object pose estimation and
joint reconstruction remain challenging tasks. With the growing interest in
interaction modeling, we hope this report can provide useful insights and
foster future research in this direction. Our workshop website can be found at
\href{https://rhobin-challenge.github.io/}{https://rhobin-challenge.github.io/}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_X/0/1/0/all/0/1&quot;&gt;Xianghui Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Athanasiou_N/0/1/0/all/0/1&quot;&gt;Nikos Athanasiou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhatnagar_B/0/1/0/all/0/1&quot;&gt;Bharat Lal Bhatnagar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chun-Hao P. Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mo_K/0/1/0/all/0/1&quot;&gt;Kaichun Mo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_X/0/1/0/all/0/1&quot;&gt;Xia Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zerui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_L/0/1/0/all/0/1&quot;&gt;Liangxian Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_X/0/1/0/all/0/1&quot;&gt;Xiao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_B/0/1/0/all/0/1&quot;&gt;Bingqiao Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Jie Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenfei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nam_H/0/1/0/all/0/1&quot;&gt;Hyeongjin Nam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_D/0/1/0/all/0/1&quot;&gt;Daniel Sungho Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kihoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kyoung Mu Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hilliges_O/0/1/0/all/0/1&quot;&gt;Otmar Hilliges&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1&quot;&gt;Gerard Pons-Moll&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04150">
<title>Two-stream joint matching method based on contrastive learning for few-shot action recognition. (arXiv:2401.04150v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04150</link>
<description rdf:parseType="Literal">&lt;p&gt;Although few-shot action recognition based on metric learning paradigm has
achieved significant success, it fails to address the following issues: (1)
inadequate action relation modeling and underutilization of multi-modal
information; (2) challenges in handling video matching problems with different
lengths and speeds, and video matching problems with misalignment of video
sub-actions. To address these issues, we propose a Two-Stream Joint Matching
method based on contrastive learning (TSJM), which consists of two modules:
Multi-modal Contrastive Learning Module (MCL) and Joint Matching Module (JMM).
The objective of the MCL is to extensively investigate the inter-modal mutual
information relationships, thereby thoroughly extracting modal information to
enhance the modeling of action relationships. The JMM aims to simultaneously
address the aforementioned video matching problems. The effectiveness of the
proposed method is evaluated on two widely used few shot action recognition
datasets, namely, SSv2 and Kinetics. Comprehensive ablation experiments are
also conducted to substantiate the efficacy of our proposed approach.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_L/0/1/0/all/0/1&quot;&gt;Long Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Ziqiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_B/0/1/0/all/0/1&quot;&gt;Bingxin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhongming Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Ao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_Y/0/1/0/all/0/1&quot;&gt;Yongxin Ge&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04154">
<title>Efficient Selective Audio Masked Multimodal Bottleneck Transformer for Audio-Video Classification. (arXiv:2401.04154v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04154</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio and video are two most common modalities in the mainstream media
platforms, e.g., YouTube. To learn from multimodal videos effectively, in this
work, we propose a novel audio-video recognition approach termed audio video
Transformer, AVT, leveraging the effective spatio-temporal representation by
the video Transformer to improve action recognition accuracy. For multimodal
fusion, simply concatenating multimodal tokens in a cross-modal Transformer
requires large computational and memory resources, instead we reduce the
cross-modality complexity through an audio-video bottleneck Transformer. To
improve the learning efficiency of multimodal Transformer, we integrate
self-supervised objectives, i.e., audio-video contrastive learning, audio-video
matching, and masked audio and video learning, into AVT training, which maps
diverse audio and video representations into a common multimodal representation
space. We further propose a masked audio segment loss to learn semantic audio
activities in AVT. Extensive experiments and ablation studies on three public
datasets and two in-house datasets consistently demonstrate the effectiveness
of the proposed AVT. Specifically, AVT outperforms its previous
state-of-the-art counterparts on Kinetics-Sounds by 8%. AVT also surpasses one
of the previous state-of-the-art video Transformers [25] by 10% on VGGSound by
leveraging the audio signal. Compared to one of the previous state-of-the-art
multimodal methods, MBT [32], AVT is 1.3% more efficient in terms of FLOPs and
improves the accuracy by 3.8% on Epic-Kitchens-100.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04181">
<title>Language-Conditioned Robotic Manipulation with Fast and Slow Thinking. (arXiv:2401.04181v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.04181</link>
<description rdf:parseType="Literal">&lt;p&gt;The language-conditioned robotic manipulation aims to transfer natural
language instructions into executable actions, from simple pick-and-place to
tasks requiring intent recognition and visual reasoning. Inspired by the dual
process theory in cognitive science, which suggests two parallel systems of
fast and slow thinking in human decision-making, we introduce Robotics with
Fast and Slow Thinking (RFST), a framework that mimics human cognitive
architecture to classify tasks and makes decisions on two systems based on
instruction types. Our RFST consists of two key components: 1) an instruction
discriminator to determine which system should be activated based on the
current user instruction, and 2) a slow-thinking system that is comprised of a
fine-tuned vision language model aligned with the policy networks, which allows
the robot to recognize user intention or perform reasoning tasks. To assess our
methodology, we built a dataset featuring real-world trajectories, capturing
actions ranging from spontaneous impulses to tasks requiring deliberate
contemplation. Our results, both in simulation and real-world scenarios,
confirm that our approach adeptly manages intricate tasks that demand intent
recognition and reasoning. The project is available at
https://jlm-z.github.io/RSFT/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minjie Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Y/0/1/0/all/0/1&quot;&gt;Yichen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Junjie Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Che_Z/0/1/0/all/0/1&quot;&gt;Zhengping Che&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chaomin Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yaxin Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Dong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_F/0/1/0/all/0/1&quot;&gt;Feifei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jian Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04210">
<title>FunnyNet-W: Multimodal Learning of Funny Moments in Videos in the Wild. (arXiv:2401.04210v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04210</link>
<description rdf:parseType="Literal">&lt;p&gt;Automatically understanding funny moments (i.e., the moments that make people
laugh) when watching comedy is challenging, as they relate to various features,
such as body language, dialogues and culture. In this paper, we propose
FunnyNet-W, a model that relies on cross- and self-attention for visual, audio
and text data to predict funny moments in videos. Unlike most methods that rely
on ground truth data in the form of subtitles, in this work we exploit
modalities that come naturally with videos: (a) video frames as they contain
visual information indispensable for scene understanding, (b) audio as it
contains higher-level cues associated with funny moments, such as intonation,
pitch and pauses and (c) text automatically extracted with a speech-to-text
model as it can provide rich information when processed by a Large Language
Model. To acquire labels for training, we propose an unsupervised approach that
spots and labels funny audio moments. We provide experiments on five datasets:
the sitcoms TBBT, MHD, MUStARD, Friends, and the TED talk UR-Funny. Extensive
experiments and analysis show that FunnyNet-W successfully exploits visual,
auditory and textual cues to identify funny moments, while our findings reveal
FunnyNet-W&apos;s ability to predict funny moments in the wild. FunnyNet-W sets the
new state of the art for funny moment detection with multimodal cues on all
datasets with and without using ground truth information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhi-Song Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Courant_R/0/1/0/all/0/1&quot;&gt;Robin Courant&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kalogeiton_V/0/1/0/all/0/1&quot;&gt;Vicky Kalogeiton&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04230">
<title>SOAP: Cross-sensor Domain Adaptation for 3D Object Detection Using Stationary Object Aggregation Pseudo-labelling. (arXiv:2401.04230v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04230</link>
<description rdf:parseType="Literal">&lt;p&gt;We consider the problem of cross-sensor domain adaptation in the context of
LiDAR-based 3D object detection and propose Stationary Object Aggregation
Pseudo-labelling (SOAP) to generate high quality pseudo-labels for stationary
objects. In contrast to the current state-of-the-art in-domain practice of
aggregating just a few input scans, SOAP aggregates entire sequences of point
clouds at the input level to reduce the sensor domain gap. Then, by means of
what we call quasi-stationary training and spatial consistency post-processing,
the SOAP model generates accurate pseudo-labels for stationary objects, closing
a minimum of 30.3% domain gap compared to few-frame detectors. Our results also
show that state-of-the-art domain adaptation approaches can achieve even
greater performance in combination with SOAP, in both the unsupervised and
semi-supervised settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chengjie Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abdelzad_V/0/1/0/all/0/1&quot;&gt;Vahdat Abdelzad&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedwards_S/0/1/0/all/0/1&quot;&gt;Sean Sedwards&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Czarnecki_K/0/1/0/all/0/1&quot;&gt;Krzysztof Czarnecki&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04241">
<title>Data-Agnostic Face Image Synthesis Detection Using Bayesian CNNs. (arXiv:2401.04241v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04241</link>
<description rdf:parseType="Literal">&lt;p&gt;Face image synthesis detection is considerably gaining attention because of
the potential negative impact on society that this type of synthetic data
brings. In this paper, we propose a data-agnostic solution to detect the face
image synthesis process. Specifically, our solution is based on an anomaly
detection framework that requires only real data to learn the inference
process. It is therefore data-agnostic in the sense that it requires no
synthetic face images. The solution uses the posterior probability with respect
to the reference data to determine if new samples are synthetic or not. Our
evaluation results using different synthesizers show that our solution is very
competitive against the state-of-the-art, which requires synthetic data for
training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leyva_R/0/1/0/all/0/1&quot;&gt;Roberto Leyva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1&quot;&gt;Victor Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epiphaniou_G/0/1/0/all/0/1&quot;&gt;Gregory Epiphaniou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maple_C/0/1/0/all/0/1&quot;&gt;Carsten Maple&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04244">
<title>Spatio-Temporal Turbulence Mitigation: A Translational Perspective. (arXiv:2401.04244v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.04244</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering images distorted by atmospheric turbulence is a challenging
inverse problem due to the stochastic nature of turbulence. Although numerous
turbulence mitigation (TM) algorithms have been proposed, their efficiency and
generalization to real-world dynamic scenarios remain severely limited.
Building upon the intuitions of classical TM algorithms, we present the Deep
Atmospheric TUrbulence Mitigation network (DATUM). DATUM aims to overcome major
challenges when transitioning from classical to deep learning approaches. By
carefully integrating the merits of classical multi-frame TM methods into a
deep network structure, we demonstrate that DATUM can efficiently perform
long-range temporal aggregation using a recurrent fashion, while deformable
attention and temporal-channel attention seamlessly facilitate pixel
registration and lucky imaging. With additional supervision, tilt and blur
degradation can be jointly mitigated. These inductive biases empower DATUM to
significantly outperform existing methods while delivering a tenfold increase
in processing speed. A large-scale training dataset, ATSyn, is presented as a
co-invention to enable generalization in real turbulence. Our code and datasets
will be available at
\href{https://xg416.github.io/DATUM}{\textcolor{pink}{https://xg416.github.io/DATUM}}
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xingguang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chimitt_N/0/1/0/all/0/1&quot;&gt;Nicholas Chimitt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chi_Y/0/1/0/all/0/1&quot;&gt;Yiheng Chi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chan_S/0/1/0/all/0/1&quot;&gt;Stanley H. Chan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04247">
<title>Robust Image Watermarking using Stable Diffusion. (arXiv:2401.04247v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04247</link>
<description rdf:parseType="Literal">&lt;p&gt;Watermarking images is critical for tracking image provenance and claiming
ownership. With the advent of generative models, such as stable diffusion, able
to create fake but realistic images, watermarking has become particularly
important, e.g., to make generated images reliably identifiable. Unfortunately,
the very same stable diffusion technology can remove watermarks injected using
existing methods. To address this problem, we present a ZoDiac, which uses a
pre-trained stable diffusion model to inject a watermark into the trainable
latent space, resulting in watermarks that can be reliably detected in the
latent vector, even when attacked. We evaluate ZoDiac on three benchmarks,
MS-COCO, DiffusionDB, and WikiArt, and find that ZoDiac is robust against
state-of-the-art watermark attacks, with a watermark detection rate over 98%
and a false positive rate below 6.4%, outperforming state-of-the-art
watermarking methods. Our research demonstrates that stable diffusion is a
promising approach to robust watermarking, able to withstand even
stable-diffusion-based attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Lijun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_A/0/1/0/all/0/1&quot;&gt;Antoni Viros Martin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bearfield_C/0/1/0/all/0/1&quot;&gt;Cindy Xiong Bearfield&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brun_Y/0/1/0/all/0/1&quot;&gt;Yuriy Brun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_H/0/1/0/all/0/1&quot;&gt;Hui Guan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04257">
<title>Detecting Face Synthesis Using a Concealed Fusion Model. (arXiv:2401.04257v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04257</link>
<description rdf:parseType="Literal">&lt;p&gt;Face image synthesis is gaining more attention in computer security due to
concerns about its potential negative impacts, including those related to fake
biometrics. Hence, building models that can detect the synthesized face images
is an important challenge to tackle. In this paper, we propose a fusion-based
strategy to detect face image synthesis while providing resiliency to several
attacks. The proposed strategy uses a late fusion of the outputs computed by
several undisclosed models by relying on random polynomial coefficients and
exponents to conceal a new feature space. Unlike existing concealing solutions,
our strategy requires no quantization, which helps to preserve the feature
space. Our experiments reveal that our strategy achieves state-of-the-art
performance while providing protection against poisoning, perturbation,
backdoor, and reverse model attacks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leyva_R/0/1/0/all/0/1&quot;&gt;Roberto Leyva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sanchez_V/0/1/0/all/0/1&quot;&gt;Victor Sanchez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Epiphaniou_G/0/1/0/all/0/1&quot;&gt;Gregory Epiphaniou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maple_C/0/1/0/all/0/1&quot;&gt;Carsten Maple&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04290">
<title>StarCraftImage: A Dataset For Prototyping Spatial Reasoning Methods For Multi-Agent Environments. (arXiv:2401.04290v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04290</link>
<description rdf:parseType="Literal">&lt;p&gt;Spatial reasoning tasks in multi-agent environments such as event prediction,
agent type identification, or missing data imputation are important for
multiple applications (e.g., autonomous surveillance over sensor networks and
subtasks for reinforcement learning (RL)). StarCraft II game replays encode
intelligent (and adversarial) multi-agent behavior and could provide a testbed
for these tasks; however, extracting simple and standardized representations
for prototyping these tasks is laborious and hinders reproducibility. In
contrast, MNIST and CIFAR10, despite their extreme simplicity, have enabled
rapid prototyping and reproducibility of ML methods. Following the simplicity
of these datasets, we construct a benchmark spatial reasoning dataset based on
StarCraft II replays that exhibit complex multi-agent behaviors, while still
being as easy to use as MNIST and CIFAR10. Specifically, we carefully summarize
a window of 255 consecutive game states to create 3.6 million summary images
from 60,000 replays, including all relevant metadata such as game outcome and
player races. We develop three formats of decreasing complexity: Hyperspectral
images that include one channel for every unit type (similar to multispectral
geospatial images), RGB images that mimic CIFAR10, and grayscale images that
mimic MNIST. We show how this dataset can be used for prototyping spatial
reasoning methods. All datasets, code for extraction, and code for dataset
loading can be found at https://starcraftdata.davidinouye.com
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kulinski_S/0/1/0/all/0/1&quot;&gt;Sean Kulinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Waytowich_N/0/1/0/all/0/1&quot;&gt;Nicholas R. Waytowich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hare_J/0/1/0/all/0/1&quot;&gt;James Z. Hare&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Inouye_D/0/1/0/all/0/1&quot;&gt;David I. Inouye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04317">
<title>Vision Reimagined: AI-Powered Breakthroughs in WiFi Indoor Imaging. (arXiv:2401.04317v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04317</link>
<description rdf:parseType="Literal">&lt;p&gt;Indoor imaging is a critical task for robotics and internet-of-things. WiFi
as an omnipresent signal is a promising candidate for carrying out passive
imaging and synchronizing the up-to-date information to all connected devices.
This is the first research work to consider WiFi indoor imaging as a
multi-modal image generation task that converts the measured WiFi power into a
high-resolution indoor image. Our proposed WiFi-GEN network achieves a shape
reconstruction accuracy that is 275% of that achieved by physical model-based
inversion methods. Additionally, the Frechet Inception Distance score has been
significantly reduced by 82%. To examine the effectiveness of models for this
task, the first large-scale dataset is released containing 80,000 pairs of WiFi
signal and imaging target. Our model absorbs challenges for the model-based
methods including the non-linearity, ill-posedness and non-certainty into
massive parameters of our generative AI network. The network is also designed
to best fit measured WiFi signals and the desired imaging output. For
reproducibility, we will release the data and code upon acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Jianyang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bowen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dubey_A/0/1/0/all/0/1&quot;&gt;Amartansh Dubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murch_R/0/1/0/all/0/1&quot;&gt;Ross Murch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Liwen Jing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04325">
<title>RadarCam-Depth: Radar-Camera Fusion for Depth Estimation with Learned Metric Scale. (arXiv:2401.04325v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04325</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach for metric dense depth estimation based on the
fusion of a single-view image and a sparse, noisy Radar point cloud. The direct
fusion of heterogeneous Radar and image data, or their encodings, tends to
yield dense depth maps with significant artifacts, blurred boundaries, and
suboptimal accuracy. To circumvent this issue, we learn to augment versatile
and robust monocular depth prediction with the dense metric scale induced from
sparse and noisy Radar data. We propose a Radar-Camera framework for highly
accurate and fine-detailed dense depth estimation with four stages, including
monocular depth prediction, global scale alignment of monocular depth with
sparse Radar points, quasi-dense scale estimation through learning the
association between Radar points and image patches, and local scale refinement
of dense depth using a scale map learner. Our proposed method significantly
outperforms the state-of-the-art Radar-Camera depth estimation methods by
reducing the mean absolute error (MAE) of depth estimation by 25.6% and 40.2%
on the challenging nuScenes dataset and our self-collected ZJU-4DRadarCam
dataset, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Han Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Y/0/1/0/all/0/1&quot;&gt;Yukai Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Y/0/1/0/all/0/1&quot;&gt;Yaqing Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kewei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_X/0/1/0/all/0/1&quot;&gt;Xingxing Zuo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04330">
<title>BD-MSA: Body decouple VHR Remote Sensing Image Change Detection method guided by multi-scale feature information aggregation. (arXiv:2401.04330v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04330</link>
<description rdf:parseType="Literal">&lt;p&gt;The purpose of remote sensing image change detection (RSCD) is to detect
differences between bi-temporal images taken at the same place. Deep learning
has been extensively used to RSCD tasks, yielding significant results in terms
of result recognition. However, due to the shooting angle of the satellite, the
impacts of thin clouds, and certain lighting conditions, the problem of fuzzy
edges in the change region in some remote sensing photographs cannot be
properly handled using current RSCD algorithms. To solve this issue, we
proposed a Body Decouple Multi-Scale by fearure Aggregation change detection
(BD-MSA), a novel model that collects both global and local feature map
information in the channel and space dimensions of the feature map during the
training and prediction phases. This approach allows us to successfully extract
the change region&apos;s boundary information while also divorcing the change
region&apos;s main body from its boundary. Numerous studies have shown that the
assessment metrics and evaluation effects of the model described in this paper
on the publicly available datasets DSIFN-CD and S2Looking are the best when
compared to other models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yonghui Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaolong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yishu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ai_J/0/1/0/all/0/1&quot;&gt;Jinquan Ai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04332">
<title>Mix-GENEO: A flexible filtration for multiparameter persistent homology detects digital images. (arXiv:2401.04332v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04332</link>
<description rdf:parseType="Literal">&lt;p&gt;Two important problems in the field of Topological Data Analysis are defining
practical multifiltrations on objects and showing ability of TDA to detect the
geometry. Motivated by the problems, we constuct three multifiltrations named
multi-GENEO, multi-DGENEO and mix-GENEO, and prove the stability of both the
interleaving distance and multiparameter persistence landscape of multi-GENEO
with respect to the pseudometric of the subspace of bounded functions. We also
give the estimations of upper bound for multi-DGENEO and mix-GENEO. Finally, we
provide experiment results on MNIST dataset to demonstrate our bifiltrations
have ability to detect geometric and topological differences of digital images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jiaxing He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hou_B/0/1/0/all/0/1&quot;&gt;Bingzhe Hou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_T/0/1/0/all/0/1&quot;&gt;Tieru Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_Y/0/1/0/all/0/1&quot;&gt;Yue Xin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04339">
<title>Memory-Efficient Personalization using Quantized Diffusion Model. (arXiv:2401.04339v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04339</link>
<description rdf:parseType="Literal">&lt;p&gt;The rise of billion-parameter diffusion models like Stable Diffusion XL,
Imagen, and Dall-E3 markedly advances the field of generative AI. However,
their large-scale nature poses challenges in fine-tuning and deployment due to
high resource demands and slow inference speed. This paper ventures into the
relatively unexplored yet promising realm of fine-tuning quantized diffusion
models. We establish a strong baseline by customizing three models: PEQA for
fine-tuning quantization parameters, Q-Diffusion for post-training
quantization, and DreamBooth for personalization. Our analysis reveals a
notable trade-off between subject and prompt fidelity within the baseline
model. To address these issues, we introduce two strategies, inspired by the
distinct roles of different timesteps in diffusion models: S1 optimizing a
single set of fine-tuning parameters exclusively at selected intervals, and S2
creating multiple fine-tuning parameter sets, each specialized for different
timestep intervals. Our approach not only enhances personalization but also
upholds prompt fidelity and image quality, significantly outperforming the
baseline qualitatively and quantitatively. The code will be made publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyogon Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_S/0/1/0/all/0/1&quot;&gt;Seohyun Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shim_H/0/1/0/all/0/1&quot;&gt;Hyunjung Shim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04345">
<title>RomniStereo: Recurrent Omnidirectional Stereo Matching. (arXiv:2401.04345v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04345</link>
<description rdf:parseType="Literal">&lt;p&gt;Omnidirectional stereo matching (OSM) is an essential and reliable means for
$360^{\circ}$ depth sensing. However, following earlier works on conventional
stereo matching, prior state-of-the-art (SOTA) methods rely on a 3D
encoder-decoder block to regularize the cost volume, causing the whole system
complicated and sub-optimal results. Recently, the Recurrent All-pairs Field
Transforms (RAFT) based approach employs the recurrent update in 2D and has
efficiently improved image-matching tasks, \ie, optical flow, and stereo
matching. To bridge the gap between OSM and RAFT, we mainly propose an opposite
adaptive weighting scheme to seamlessly transform the outputs of spherical
sweeping of OSM into the required inputs for the recurrent update, thus
creating a recurrent omnidirectional stereo matching (RomniStereo) algorithm.
Furthermore, we introduce two techniques, \ie, grid embedding and adaptive
context feature generation, which also contribute to RomniStereo&apos;s performance.
Our best model improves the average MAE metric by 40.7\% over the previous SOTA
baseline across five datasets. When visualizing the results, our models
demonstrate clear advantages on both synthetic and realistic examples. The code
is available at \url{https://github.com/HalleyJiang/RomniStereo}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_H/0/1/0/all/0/1&quot;&gt;Hualie Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Rui Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Minglang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wenjie Jiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04350">
<title>Pre-trained Model Guided Fine-Tuning for Zero-Shot Adversarial Robustness. (arXiv:2401.04350v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04350</link>
<description rdf:parseType="Literal">&lt;p&gt;Large-scale pre-trained vision-language models like CLIP have demonstrated
impressive performance across various tasks, and exhibit remarkable zero-shot
generalization capability, while they are also vulnerable to imperceptible
adversarial examples. Existing works typically employ adversarial training
(fine-tuning) as a defense method against adversarial examples. However, direct
application to the CLIP model may result in overfitting, compromising the
model&apos;s capacity for generalization. In this paper, we propose Pre-trained
Model Guided Adversarial Fine-Tuning (PMG-AFT) method, which leverages
supervision from the original pre-trained model by carefully designing an
auxiliary branch, to enhance the model&apos;s zero-shot adversarial robustness.
Specifically, PMG-AFT minimizes the distance between the features of
adversarial examples in the target model and those in the pre-trained model,
aiming to preserve the generalization features already captured by the
pre-trained model. Extensive Experiments on 15 zero-shot datasets demonstrate
that PMG-AFT significantly outperforms the state-of-the-art method, improving
the top-1 robust accuracy by an average of 4.99%. Furthermore, our approach
consistently improves clean accuracy by an average of 8.72%.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sibo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Z/0/1/0/all/0/1&quot;&gt;Zheng Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shan_S/0/1/0/all/0/1&quot;&gt;Shiguang Shan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04354">
<title>Knowledge-enhanced Multi-perspective Video Representation Learning for Scene Recognition. (arXiv:2401.04354v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04354</link>
<description rdf:parseType="Literal">&lt;p&gt;With the explosive growth of video data in real-world applications, a
comprehensive representation of videos becomes increasingly important. In this
paper, we address the problem of video scene recognition, whose goal is to
learn a high-level video representation to classify scenes in videos. Due to
the diversity and complexity of video contents in realistic scenarios, this
task remains a challenge. Most existing works identify scenes for videos only
from visual or textual information in a temporal perspective, ignoring the
valuable information hidden in single frames, while several earlier studies
only recognize scenes for separate images in a non-temporal perspective. We
argue that these two perspectives are both meaningful for this task and
complementary to each other, meanwhile, externally introduced knowledge can
also promote the comprehension of videos. We propose a novel two-stream
framework to model video representations from multiple perspectives, i.e.
temporal and non-temporal perspectives, and integrate the two perspectives in
an end-to-end manner by self-distillation. Besides, we design a
knowledge-enhanced feature fusion and label prediction method that contributes
to naturally introducing knowledge into the task of video scene recognition.
Experiments conducted on a real-world dataset demonstrate the effectiveness of
our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xuzheng Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_C/0/1/0/all/0/1&quot;&gt;Chen Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gan_T/0/1/0/all/0/1&quot;&gt;Tian Gan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chao_L/0/1/0/all/0/1&quot;&gt;Linlin Chao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jianan Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_Y/0/1/0/all/0/1&quot;&gt;Yuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Q/0/1/0/all/0/1&quot;&gt;Qingpei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_W/0/1/0/all/0/1&quot;&gt;Wei Chu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04357">
<title>Iterative Feedback Network for Unsupervised Point Cloud Registration. (arXiv:2401.04357v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04357</link>
<description rdf:parseType="Literal">&lt;p&gt;As a fundamental problem in computer vision, point cloud registration aims to
seek the optimal transformation for aligning a pair of point clouds. In most
existing methods, the information flows are usually forward transferring, thus
lacking the guidance from high-level information to low-level information.
Besides, excessive high-level information may be overly redundant, and directly
using it may conflict with the original low-level information. In this paper,
we propose a novel Iterative Feedback Network (IFNet) for unsupervised point
cloud registration, in which the representation of low-level features is
efficiently enriched by rerouting subsequent high-level features. Specifically,
our IFNet is built upon a series of Feedback Registration Block (FRB) modules,
with each module responsible for generating the feedforward rigid
transformation and feedback high-level features. These FRB modules are cascaded
and recurrently unfolded over time. Further, the Feedback Transformer is
designed to efficiently select relevant information from feedback high-level
features, which is utilized to refine the low-level features. What&apos;s more, we
incorporate a geometry-awareness descriptor to empower the network for making
full use of most geometric information, which leads to more precise
registration results. Extensive experiments on various benchmark datasets
demonstrate the superior registration performance of our IFNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yifan Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Boyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shiqi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_J/0/1/0/all/0/1&quot;&gt;Jihua Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04362">
<title>Representative Feature Extraction During Diffusion Process for Sketch Extraction with One Example. (arXiv:2401.04362v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04362</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce DiffSketch, a method for generating a variety of stylized
sketches from images. Our approach focuses on selecting representative features
from the rich semantics of deep features within a pretrained diffusion model.
This novel sketch generation method can be trained with one manual drawing.
Furthermore, efficient sketch extraction is ensured by distilling a trained
generator into a streamlined extractor. We select denoising diffusion features
through analysis and integrate these selected features with VAE features to
produce sketches. Additionally, we propose a sampling scheme for training
models using a conditional generative approach. Through a series of
comparisons, we verify that distilled DiffSketch not only outperforms existing
state-of-the-art sketch extraction methods but also surpasses diffusion-based
stylization methods in the task of extracting sketches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_K/0/1/0/all/0/1&quot;&gt;Kwan Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Youngseo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_K/0/1/0/all/0/1&quot;&gt;Kwanggyoon Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_C/0/1/0/all/0/1&quot;&gt;Chang Wook Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noh_J/0/1/0/all/0/1&quot;&gt;Junyong Noh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04364">
<title>SoK: Facial Deepfake Detectors. (arXiv:2401.04364v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04364</link>
<description rdf:parseType="Literal">&lt;p&gt;Deepfakes have rapidly emerged as a profound and serious threat to society,
primarily due to their ease of creation and dissemination. This situation has
triggered an accelerated development of deepfake detection technologies.
However, many existing detectors rely heavily on lab-generated datasets for
validation, which may not effectively prepare them for novel, emerging, and
real-world deepfake techniques. In this paper, we conduct an extensive and
comprehensive review and analysis of the latest state-of-the-art deepfake
detectors, evaluating them against several critical criteria. These criteria
facilitate the categorization of these detectors into 4 high-level groups and
13 fine-grained sub-groups, all aligned with a unified standard conceptual
framework. This classification and framework offer deep and practical insights
into the factors that affect detector efficacy. We assess the generalizability
of 16 leading detectors across various standard attack scenarios, including
black-box, white-box, and gray-box settings. Our systematized analysis and
experimentation lay the groundwork for a deeper understanding of deepfake
detectors and their generalizability, paving the way for future research
focused on creating detectors adept at countering various attack scenarios.
Additionally, this work offers insights for developing more proactive defenses
against deepfakes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_B/0/1/0/all/0/1&quot;&gt;Binh M. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tariq_S/0/1/0/all/0/1&quot;&gt;Shahroz Tariq&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moore_K/0/1/0/all/0/1&quot;&gt;Kristen Moore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abuadbba_A/0/1/0/all/0/1&quot;&gt;Alsharif Abuadbba&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Simon S. Woo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04377">
<title>Towards Real-World Aerial Vision Guidance with Categorical 6D Pose Tracker. (arXiv:2401.04377v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2401.04377</link>
<description rdf:parseType="Literal">&lt;p&gt;Tracking the object 6-DoF pose is crucial for various downstream robot tasks
and real-world applications. In this paper, we investigate the real-world robot
task of aerial vision guidance for aerial robotics manipulation, utilizing
category-level 6-DoF pose tracking. Aerial conditions inevitably introduce
special challenges, such as rapid viewpoint changes in pitch and roll. To
support this task and challenge, we firstly introduce a robust category-level
6-DoF pose tracker (Robust6DoF). This tracker leverages shape and temporal
prior knowledge to explore optimal inter-frame keypoint pairs, generated under
a priori structural adaptive supervision in a coarse-to-fine manner. Notably,
our Robust6DoF employs a Spatial-Temporal Augmentation module to deal with the
problems of the inter-frame differences and intra-class shape variations
through both temporal dynamic filtering and shape-similarity filtering. We
further present a Pose-Aware Discrete Servo strategy (PAD-Servo), serving as a
decoupling approach to implement the final aerial vision guidance task. It
contains two servo action policies to better accommodate the structural
properties of aerial robotics manipulation. Exhaustive experiments on four
well-known public benchmarks demonstrate the superiority of our Robust6DoF.
Real-world tests directly verify that our Robust6DoF along with PAD-Servo can
be readily used in real-world aerial robotic applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jingtao Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yaonan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Danwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04390">
<title>Learning with Noisy Labels: Interconnection of Two Expectation-Maximizations. (arXiv:2401.04390v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04390</link>
<description rdf:parseType="Literal">&lt;p&gt;Labor-intensive labeling becomes a bottleneck in developing computer vision
algorithms based on deep learning. For this reason, dealing with imperfect
labels has increasingly gained attention and has become an active field of
study. We address learning with noisy labels (LNL) problem, which is formalized
as a task of finding a structured manifold in the midst of noisy data. In this
framework, we provide a proper objective function and an optimization algorithm
based on two expectation-maximization (EM) cycles. The separate networks
associated with the two EM cycles collaborate to optimize the objective
function, where one model is for distinguishing clean labels from corrupted
ones while the other is for refurbishing the corrupted labels. This approach
results in a non-collapsing LNL-flywheel model in the end. Experiments show
that our algorithm achieves state-of-the-art performance in multiple standard
benchmarks with substantial margins under various types of label noise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_H/0/1/0/all/0/1&quot;&gt;Heewon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_H/0/1/0/all/0/1&quot;&gt;Hyun Sung Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kiho Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jaeyun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bohyung Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04403">
<title>MST: Adaptive Multi-Scale Tokens Guided Interactive Segmentation. (arXiv:2401.04403v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04403</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of Industrial Informatics, interactive segmentation has gained
significant attention for its application in human-computer interaction and
data annotation. Existing algorithms, however, face challenges in balancing the
segmentation accuracy between large and small targets, often leading to an
increased number of user interactions. To tackle this, a novel multi-scale
token adaptation algorithm, leveraging token similarity, has been devised to
enhance segmentation across varying target sizes. This algorithm utilizes a
differentiable top-k tokens selection mechanism, allowing for fewer tokens to
be used while maintaining efficient multi-scale token interaction. Furthermore,
a contrastive loss is introduced to better discriminate between target and
background tokens, improving the correctness and robustness of the tokens
similar to the target. Extensive benchmarking shows that the algorithm achieves
state-of-the-art (SOTA) performance compared to current methods. An interactive
demo and all reproducible codes will be released at
https://github.com/hahamyt/mst.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Long Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shanghong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yongquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jun Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04405">
<title>Optimal Transcoding Resolution Prediction for Efficient Per-Title Bitrate Ladder Estimation. (arXiv:2401.04405v1 [cs.MM])</title>
<link>http://arxiv.org/abs/2401.04405</link>
<description rdf:parseType="Literal">&lt;p&gt;Adaptive video streaming requires efficient bitrate ladder construction to
meet heterogeneous network conditions and end-user demands. Per-title optimized
encoding typically traverses numerous encoding parameters to search the
Pareto-optimal operating points for each video. Recently, researchers have
attempted to predict the content-optimized bitrate ladder for pre-encoding
overhead reduction. However, existing methods commonly estimate the encoding
parameters on the Pareto front and still require subsequent pre-encodings. In
this paper, we propose to directly predict the optimal transcoding resolution
at each preset bitrate for efficient bitrate ladder construction. We adopt a
Temporal Attentive Gated Recurrent Network to capture spatial-temporal features
and predict transcoding resolutions as a multi-task classification problem. We
demonstrate that content-optimized bitrate ladders can thus be efficiently
determined without any pre-encoding. Our method well approximates the
ground-truth bitrate-resolution pairs with a slight Bj{\o}ntegaard Delta rate
loss of 1.21% and significantly outperforms the state-of-the-art fixed ladder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jinhai Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Mengxi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shijie Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04406">
<title>MapAI: Precision in Building Segmentation. (arXiv:2401.04406v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04406</link>
<description rdf:parseType="Literal">&lt;p&gt;MapAI: Precision in Building Segmentation is a competition arranged with the
Norwegian Artificial Intelligence Research Consortium (NORA) in collaboration
with Centre for Artificial Intelligence Research at the University of Agder
(CAIR), the Norwegian Mapping Authority, AI:Hub, Norkart, and the Danish Agency
for Data Supply and Infrastructure. The competition will be held in the fall of
2022. It will be concluded at the Northern Lights Deep Learning conference
focusing on the segmentation of buildings using aerial images and laser data.
We propose two different tasks to segment buildings, where the first task can
only utilize aerial images, while the second must use laser data (LiDAR) with
or without aerial images. Furthermore, we use IoU and Boundary IoU to properly
evaluate the precision of the models, with the latter being an IoU measure that
evaluates the results&apos; boundaries. We provide the participants with a training
dataset and keep a test dataset for evaluation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jyhne_S/0/1/0/all/0/1&quot;&gt;Sander Riis&amp;#xf8;en Jyhne&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goodwin_M/0/1/0/all/0/1&quot;&gt;Morten Goodwin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andersen_P/0/1/0/all/0/1&quot;&gt;Per Arne Andersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oveland_I/0/1/0/all/0/1&quot;&gt;Ivar Oveland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nossum_A/0/1/0/all/0/1&quot;&gt;Alexander Salveson Nossum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ormseth_K/0/1/0/all/0/1&quot;&gt;Karianne Ormseth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Orstavik_M/0/1/0/all/0/1&quot;&gt;Mathilde &amp;#xd8;rstavik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Flatman_A/0/1/0/all/0/1&quot;&gt;Andrew C. Flatman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04425">
<title>Meta-forests: Domain generalization on random forests with meta-learning. (arXiv:2401.04425v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04425</link>
<description rdf:parseType="Literal">&lt;p&gt;Domain generalization is a popular machine learning technique that enables
models to perform well on the unseen target domain, by learning from multiple
source domains. Domain generalization is useful in cases where data is limited,
difficult, or expensive to collect, such as in object recognition and
biomedicine. In this paper, we propose a novel domain generalization algorithm
called &quot;meta-forests&quot;, which builds upon the basic random forests model by
incorporating the meta-learning strategy and maximum mean discrepancy measure.
The aim of meta-forests is to enhance the generalization ability of classifiers
by reducing the correlation among trees and increasing their strength. More
specifically, meta-forests conducts meta-learning optimization during each
meta-task, while also utilizing the maximum mean discrepancy as a
regularization term to penalize poor generalization performance in the
meta-test process. To evaluate the effectiveness of our algorithm, we test it
on two publicly object recognition datasets and a glucose monitoring dataset
that we have used in a previous study. Our results show that meta-forests
outperforms state-of-the-art approaches in terms of generalization performance
on both object recognition and glucose monitoring datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yuyang Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kosmas_P/0/1/0/all/0/1&quot;&gt;Panagiotis Kosmas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04435">
<title>Uncertainty-aware Sampling for Long-tailed Semi-supervised Learning. (arXiv:2401.04435v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04435</link>
<description rdf:parseType="Literal">&lt;p&gt;For semi-supervised learning with imbalance classes, the long-tailed
distribution of data will increase the model prediction bias toward dominant
classes, undermining performance on less frequent classes. Existing methods
also face challenges in ensuring the selection of sufficiently reliable
pseudo-labels for model training and there is a lack of mechanisms to adjust
the selection of more reliable pseudo-labels based on different training
stages. To mitigate this issue, we introduce uncertainty into the modeling
process for pseudo-label sampling, taking into account that the model
performance on the tailed classes varies over different training stages. For
example, at the early stage of model training, the limited predictive accuracy
of model results in a higher rate of uncertain pseudo-labels. To counter this,
we propose an Uncertainty-Aware Dynamic Threshold Selection (UDTS) approach.
This approach allows the model to perceive the uncertainty of pseudo-labels at
different training stages, thereby adaptively adjusting the selection
thresholds for different classes. Compared to other methods such as the
baseline method FixMatch, UDTS achieves an increase in accuracy of at least
approximately 5.26%, 1.75%, 9.96%, and 1.28% on the natural scene image
datasets CIFAR10-LT, CIFAR100-LT, STL-10-LT, and the medical image dataset
TissueMNIST, respectively. The source code of UDTS is publicly available at:
https://github.com/yangk/UDTS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_K/0/1/0/all/0/1&quot;&gt;Kuo Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_D/0/1/0/all/0/1&quot;&gt;Duo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_M/0/1/0/all/0/1&quot;&gt;Menghan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xiaokang Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiao-Ping Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04437">
<title>Empirical Analysis of Anomaly Detection on Hyperspectral Imaging Using Dimension Reduction Methods. (arXiv:2401.04437v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04437</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies try to use hyperspectral imaging (HSI) to detect foreign
matters in products because it enables to visualize the invisible wavelengths
including ultraviolet and infrared. Considering the enormous image channels of
the HSI, several dimension reduction methods-e.g., PCA or UMAP-can be
considered to reduce but those cannot ease the fundamental limitations, as
follows: (1) latency of HSI capturing. (2) less explanation ability of the
important channels. In this paper, to circumvent the aforementioned methods,
one of the ways to channel reduction, on anomaly detection proposed HSI.
Different from feature extraction methods (i.e., PCA or UMAP), feature
selection can sort the feature by impact and show better explainability so we
might redesign the task-optimized and cost-effective spectroscopic camera. Via
the extensive experiment results with synthesized MVTec AD dataset, we confirm
that the feature selection method shows 6.90x faster at the inference phase
compared with feature extraction-based approaches while preserving anomaly
detection performance. Ultimately, we conclude the advantage of feature
selection which is effective yet fast.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Dongeon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_Y/0/1/0/all/0/1&quot;&gt;YeongHyeon Park&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04441">
<title>Image classification network enhancement methods based on knowledge injection. (arXiv:2401.04441v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04441</link>
<description rdf:parseType="Literal">&lt;p&gt;The current deep neural network algorithm still stays in the end-to-end
training supervision method like Image-Label pairs, which makes traditional
algorithm is difficult to explain the reason for the results, and the
prediction logic is difficult to understand and analyze. The current algorithm
does not use the existing human knowledge information, which makes the model
not in line with the human cognition model and makes the model not suitable for
human use. In order to solve the above problems, the present invention provides
a deep neural network training method based on the human knowledge, which uses
the human cognition model to construct the deep neural network training model,
and uses the existing human knowledge information to construct the deep neural
network training model. This paper proposes a multi-level hierarchical deep
learning algorithm, which is composed of multi-level hierarchical deep neural
network architecture and multi-level hierarchical deep learning framework. The
experimental results show that the proposed algorithm can effectively explain
the hidden information of the neural network. The goal of our study is to
improve the interpretability of deep neural networks (DNNs) by providing an
analysis of the impact of knowledge injection on the classification task. We
constructed a knowledge injection dataset with matching knowledge data and
image classification data. The knowledge injection dataset is the benchmark
dataset for the experiments in the paper. Our model expresses the improvement
in interpretability and classification task performance of hidden layers at
different scales.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yishuang Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_N/0/1/0/all/0/1&quot;&gt;Ning Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Liang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04448">
<title>A Novel Dataset for Non-Destructive Inspection of Handwritten Documents. (arXiv:2401.04448v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04448</link>
<description rdf:parseType="Literal">&lt;p&gt;Forensic handwriting examination is a branch of Forensic Science that aims to
examine handwritten documents in order to properly define or hypothesize the
manuscript&apos;s author. These analysis involves comparing two or more (digitized)
documents through a comprehensive comparison of intrinsic local and global
features. If a correlation exists and specific best practices are satisfied,
then it will be possible to affirm that the documents under analysis were
written by the same individual. The need to create sophisticated tools capable
of extracting and comparing significant features has led to the development of
cutting-edge software with almost entirely automated processes, improving the
forensic examination of handwriting and achieving increasingly objective
evaluations. This is made possible by algorithmic solutions based on purely
mathematical concepts. Machine Learning and Deep Learning models trained with
specific datasets could turn out to be the key elements to best solve the task
at hand. In this paper, we proposed a new and challenging dataset consisting of
two subsets: the first consists of 21 documents written either by the classic
``pen and paper&quot; approach (and later digitized) and directly acquired on common
devices such as tablets; the second consists of 362 handwritten manuscripts by
124 different people, acquired following a specific pipeline. Our study
pioneered a comparison between traditionally handwritten documents and those
produced with digital tools (e.g., tablets). Preliminary results on the
proposed datasets show that 90% classification accuracy can be achieved on the
first subset (documents written on both paper and pen and later digitized and
on tablets) and 96% on the second portion of the data. The datasets are
available at
https://iplab.dmi.unict.it/mfs/forensic-handwriting-analysis/novel-dataset-2023/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Breci_E/0/1/0/all/0/1&quot;&gt;Eleonora Breci&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guarnera_L/0/1/0/all/0/1&quot;&gt;Luca Guarnera&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1&quot;&gt;Sebastiano Battiato&lt;/a&gt; (1) ((1) University of Catania)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04463">
<title>D3AD: Dynamic Denoising Diffusion Probabilistic Model for Anomaly Detection. (arXiv:2401.04463v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04463</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have found valuable applications in anomaly detection by
capturing the nominal data distribution and identifying anomalies via
reconstruction. Despite their merits, they struggle to localize anomalies of
varying scales, especially larger anomalies like entire missing components.
Addressing this, we present a novel framework that enhances the capability of
diffusion models, by extending the previous introduced implicit conditioning
approach Meng et al. (2022) in three significant ways. First, we incorporate a
dynamic step size computation that allows for variable noising steps in the
forward process guided by an initial anomaly prediction. Second, we demonstrate
that denoising an only scaled input, without any added noise, outperforms
conventional denoising process. Third, we project images in a latent space to
abstract away from fine details that interfere with reconstruction of large
missing components. Additionally, we propose a fine-tuning mechanism that
facilitates the model to effectively grasp the nuances of the target domain.
Our method undergoes rigorous evaluation on two prominent anomaly detection
datasets VISA and BTAD, yielding state-of-the-art performance. Importantly, our
framework effectively localizes anomalies regardless of their scale, marking a
pivotal advancement in diffusion-based anomaly detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tebbe_J/0/1/0/all/0/1&quot;&gt;Justin Tebbe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tayyub_J/0/1/0/all/0/1&quot;&gt;Jawad Tayyub&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04464">
<title>PhilEO Bench: Evaluating Geo-Spatial Foundation Models. (arXiv:2401.04464v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04464</link>
<description rdf:parseType="Literal">&lt;p&gt;Massive amounts of unlabelled data are captured by Earth Observation (EO)
satellites, with the Sentinel-2 constellation generating 1.6 TB of data daily.
This makes Remote Sensing a data-rich domain well suited to Machine Learning
(ML) solutions. However, a bottleneck in applying ML models to EO is the lack
of annotated data as annotation is a labour-intensive and costly process. As a
result, research in this domain has focused on Self-Supervised Learning and
Foundation Model approaches. This paper addresses the need to evaluate
different Foundation Models on a fair and uniform benchmark by introducing the
PhilEO Bench, a novel evaluation framework for EO Foundation Models. The
framework comprises of a testbed and a novel 400 GB Sentinel-2 dataset
containing labels for three downstream tasks, building density estimation, road
segmentation, and land cover classification. We present experiments using our
framework evaluating different Foundation Models, including Prithvi and SatMAE,
at multiple n-shots and convergence rates.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fibaek_C/0/1/0/all/0/1&quot;&gt;Casper Fibaek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilleri_L/0/1/0/all/0/1&quot;&gt;Luke Camilleri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luyts_A/0/1/0/all/0/1&quot;&gt;Andreas Luyts&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dionelis_N/0/1/0/all/0/1&quot;&gt;Nikolaos Dionelis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saux_B/0/1/0/all/0/1&quot;&gt;Bertrand Le Saux&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04468">
<title>MagicVideo-V2: Multi-Stage High-Aesthetic Video Generation. (arXiv:2401.04468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04468</link>
<description rdf:parseType="Literal">&lt;p&gt;The growing demand for high-fidelity video generation from textual
descriptions has catalyzed significant research in this field. In this work, we
introduce MagicVideo-V2 that integrates the text-to-image model, video motion
generator, reference image embedding module and frame interpolation module into
an end-to-end video generation pipeline. Benefiting from these architecture
designs, MagicVideo-V2 can generate an aesthetically pleasing, high-resolution
video with remarkable fidelity and smoothness. It demonstrates superior
performance over leading Text-to-Video systems such as Runway, Pika 1.0, Morph,
Moon Valley and Stable Video Diffusion model via user evaluation at large
scale.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Weimin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiawei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhijie Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_J/0/1/0/all/0/1&quot;&gt;Jiangqiao Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shuo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Low_C/0/1/0/all/0/1&quot;&gt;Chetwin Low&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_T/0/1/0/all/0/1&quot;&gt;Tuyen Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liew_J/0/1/0/all/0/1&quot;&gt;Jun Hao Liew&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_H/0/1/0/all/0/1&quot;&gt;Hanshu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Daquan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiashi Feng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04486">
<title>Take A Shortcut Back: Mitigating the Gradient Vanishing for Training Spiking Neural Networks. (arXiv:2401.04486v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04486</link>
<description rdf:parseType="Literal">&lt;p&gt;The Spiking Neural Network (SNN) is a biologically inspired neural network
infrastructure that has recently garnered significant attention. It utilizes
binary spike activations to transmit information, thereby replacing
multiplications with additions and resulting in high energy efficiency.
However, training an SNN directly poses a challenge due to the undefined
gradient of the firing spike process. Although prior works have employed
various surrogate gradient training methods that use an alternative function to
replace the firing process during back-propagation, these approaches ignore an
intrinsic problem: gradient vanishing. To address this issue, we propose a
shortcut back-propagation method in our paper, which advocates for transmitting
the gradient directly from the loss to the shallow layers. This enables us to
present the gradient to the shallow layers directly, thereby significantly
mitigating the gradient vanishing problem. Additionally, this method does not
introduce any burden during the inference phase. To strike a balance between
final accuracy and ease of training, we also propose an evolutionary training
framework and implement it by inducing a balance coefficient that dynamically
changes with the training epoch, which further improves the network&apos;s
performance. Extensive experiments conducted over static and dynamic datasets
using several popular network structures reveal that our method consistently
outperforms state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yufei Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yuanpei Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04550">
<title>WaveletFormerNet: A Transformer-based Wavelet Network for Real-world Non-homogeneous and Dense Fog Removal. (arXiv:2401.04550v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04550</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep convolutional neural networks have achieved remarkable success
in removing synthetic fog, it is essential to be able to process images taken
in complex foggy conditions, such as dense or non-homogeneous fog, in the real
world. However, the haze distribution in the real world is complex, and
downsampling can lead to color distortion or loss of detail in the output
results as the resolution of a feature map or image resolution decreases. In
addition to the challenges of obtaining sufficient training data, overfitting
can also arise in deep learning techniques for foggy image processing, which
can limit the generalization abilities of the model, posing challenges for its
practical applications in real-world scenarios. Considering these issues, this
paper proposes a Transformer-based wavelet network (WaveletFormerNet) for
real-world foggy image recovery. We embed the discrete wavelet transform into
the Vision Transformer by proposing the WaveletFormer and IWaveletFormer
blocks, aiming to alleviate texture detail loss and color distortion in the
image due to downsampling. We introduce parallel convolution in the Transformer
block, which allows for the capture of multi-frequency information in a
lightweight mechanism. Additionally, we have implemented a feature aggregation
module (FAM) to maintain image resolution and enhance the feature extraction
capacity of our model, further contributing to its impressive performance in
real-world foggy image recovery tasks. Extensive experiments demonstrate that
our WaveletFormerNet performs better than state-of-the-art methods, as shown
through quantitative and qualitative evaluations of minor model complexity.
Additionally, our satisfactory results on real-world dust removal and
application tests showcase the superior generalization ability and improved
performance of WaveletFormerNet in computer vision-related applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shengli Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_S/0/1/0/all/0/1&quot;&gt;Sen Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04560">
<title>Phase-shifted remote photoplethysmography for estimating heart rate and blood pressure from facial video. (arXiv:2401.04560v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04560</link>
<description rdf:parseType="Literal">&lt;p&gt;Human health can be critically affected by cardiovascular diseases, such as
hypertension, arrhythmias, and stroke. Heart rate and blood pressure are
important biometric information for the monitoring of cardiovascular system and
early diagnosis of cardiovascular diseases. Existing methods for estimating the
heart rate are based on electrocardiography and photoplethyomography, which
require contacting the sensor to the skin surface. Moreover, catheter and
cuff-based methods for measuring blood pressure cause inconvenience and have
limited applicability. Therefore, in this thesis, we propose a vision-based
method for estimating the heart rate and blood pressure. This thesis proposes a
2-stage deep learning framework consisting of a dual remote
photoplethysmography network (DRP-Net) and bounded blood pressure network
(BBP-Net). In the first stage, DRP-Net infers remote photoplethysmography
(rPPG) signals for the acral and facial regions, and these phase-shifted rPPG
signals are utilized to estimate the heart rate. In the second stage, BBP-Net
integrates temporal features and analyzes phase discrepancy between the acral
and facial rPPG signals to estimate SBP and DBP values. To improve the accuracy
of estimating the heart rate, we employed a data augmentation method based on a
frame interpolation model. Moreover, we designed BBP-Net to infer blood
pressure within a predefined range by incorporating a scaled sigmoid function.
Our method resulted in estimating the heart rate with the mean absolute error
(MAE) of 1.78 BPM, reducing the MAE by 34.31 % compared to the recent method,
on the MMSE-HR dataset. The MAE for estimating the systolic blood pressure
(SBP) and diastolic blood pressure (DBP) were 10.19 mmHg and 7.09 mmHg. On the
V4V dataset, the MAE for the heart rate, SBP, and DBP were 3.83 BPM, 13.64
mmHg, and 9.4 mmHg, respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_G/0/1/0/all/0/1&quot;&gt;Gyutae Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sang Jun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04570">
<title>An Automatic Cascaded Model for Hemorrhagic Stroke Segmentation and Hemorrhagic Volume Estimation. (arXiv:2401.04570v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.04570</link>
<description rdf:parseType="Literal">&lt;p&gt;Hemorrhagic Stroke (HS) has a rapid onset and is a serious condition that
poses a great health threat. Promptly and accurately delineating the bleeding
region and estimating the volume of bleeding in Computer Tomography (CT) images
can assist clinicians in treatment planning, leading to improved treatment
outcomes for patients. In this paper, a cascaded 3D model is constructed based
on UNet to perform a two-stage segmentation of the hemorrhage area in CT images
from rough to fine, and the hemorrhage volume is automatically calculated from
the segmented area. On a dataset with 341 cases of hemorrhagic stroke CT scans,
the proposed model provides high-quality segmentation outcome with higher
accuracy (DSC 85.66%) and better computation efficiency (6.2 second per sample)
when compared to the traditional Tada formula with respect to hemorrhage volume
estimation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weijin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sha_Z/0/1/0/all/0/1&quot;&gt;Zhuang Sha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Huihua Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jiang_R/0/1/0/all/0/1&quot;&gt;Rongcai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhanying Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wentao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Su_R/0/1/0/all/0/1&quot;&gt;Ruisheng Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04575">
<title>Let&apos;s Go Shopping (LGS) -- Web-Scale Image-Text Dataset for Visual Concept Understanding. (arXiv:2401.04575v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04575</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision and vision-language applications of neural networks, such as image
classification and captioning, rely on large-scale annotated datasets that
require non-trivial data-collecting processes. This time-consuming endeavor
hinders the emergence of large-scale datasets, limiting researchers and
practitioners to a small number of choices. Therefore, we seek more efficient
ways to collect and annotate images. Previous initiatives have gathered
captions from HTML alt-texts and crawled social media postings, but these data
sources suffer from noise, sparsity, or subjectivity. For this reason, we turn
to commercial shopping websites whose data meet three criteria: cleanliness,
informativeness, and fluency. We introduce the Let&apos;s Go Shopping (LGS) dataset,
a large-scale public dataset with 15 million image-caption pairs from publicly
available e-commerce websites. When compared with existing general-domain
datasets, the LGS images focus on the foreground object and have less complex
backgrounds. Our experiments on LGS show that the classifiers trained on
existing benchmark datasets do not readily generalize to e-commerce data, while
specific self-supervised visual feature extractors can better generalize.
Furthermore, LGS&apos;s high-quality e-commerce-focused images and bimodal nature
make it advantageous for vision-language bi-modal tasks: LGS enables
image-captioning models to generate richer captions and helps text-to-image
generation models achieve e-commerce style transfer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yatong Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_U/0/1/0/all/0/1&quot;&gt;Utsav Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shanker_A/0/1/0/all/0/1&quot;&gt;Apaar Shanker&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haoming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Parajuli_S/0/1/0/all/0/1&quot;&gt;Samyak Parajuli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bas_E/0/1/0/all/0/1&quot;&gt;Erhan Bas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Filipovic_I/0/1/0/all/0/1&quot;&gt;Isidora Filipovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_A/0/1/0/all/0/1&quot;&gt;Amelia N. Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fomitcheva_E/0/1/0/all/0/1&quot;&gt;Eugenia D Fomitcheva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Branson_E/0/1/0/all/0/1&quot;&gt;Elliot Branson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_A/0/1/0/all/0/1&quot;&gt;Aerin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sojoudi_S/0/1/0/all/0/1&quot;&gt;Somayeh Sojoudi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_K/0/1/0/all/0/1&quot;&gt;Kyunghyun Cho&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04578">
<title>Effective pruning of web-scale datasets based on complexity of concept clusters. (arXiv:2401.04578v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04578</link>
<description rdf:parseType="Literal">&lt;p&gt;Utilizing massive web-scale datasets has led to unprecedented performance
gains in machine learning models, but also imposes outlandish compute
requirements for their training. In order to improve training and data
efficiency, we here push the limits of pruning large-scale multimodal datasets
for training CLIP-style models. Today&apos;s most effective pruning method on
ImageNet clusters data samples into separate concepts according to their
embedding and prunes away the most prototypical samples. We scale this approach
to LAION and improve it by noting that the pruning rate should be
concept-specific and adapted to the complexity of the concept. Using a simple
and intuitive complexity measure, we are able to reduce the training cost to a
quarter of regular training. By filtering from the LAION dataset, we find that
training on a smaller set of high-quality data can lead to higher performance
with significantly lower training costs. More specifically, we are able to
outperform the LAION-trained OpenCLIP-ViT-B32 model on ImageNet zero-shot
accuracy by 1.1p.p. while only using 27.7% of the data and training compute.
Despite a strong reduction in training cost, we also see improvements on
ImageNet dist. shifts, retrieval tasks and VTAB. On the DataComp Medium
benchmark, we achieve a new state-of-the-art ImageNet zero-shot accuracy and a
competitive average zero-shot accuracy on 38 evaluation tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abbas_A/0/1/0/all/0/1&quot;&gt;Amro Abbas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rusak_E/0/1/0/all/0/1&quot;&gt;Evgenia Rusak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tirumala_K/0/1/0/all/0/1&quot;&gt;Kushal Tirumala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brendel_W/0/1/0/all/0/1&quot;&gt;Wieland Brendel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhuri_K/0/1/0/all/0/1&quot;&gt;Kamalika Chaudhuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morcos_A/0/1/0/all/0/1&quot;&gt;Ari S. Morcos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04585">
<title>Enhanced Distribution Alignment for Post-Training Quantization of Diffusion Models. (arXiv:2401.04585v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04585</link>
<description rdf:parseType="Literal">&lt;p&gt;Diffusion models have achieved great success in image generation tasks
through iterative noise estimation. However, the heavy denoising process and
complex neural networks hinder their low-latency applications in real-world
scenarios. Quantization can effectively reduce model complexity, and
post-training quantization (PTQ), which does not require fine-tuning, is highly
promising in accelerating the denoising process. Unfortunately, we find that
due to the highly dynamic distribution of activations in different denoising
steps, existing PTQ methods for diffusion models suffer from distribution
mismatch issues at both calibration sample level and reconstruction output
level, which makes the performance far from satisfactory, especially in low-bit
cases. In this paper, we propose Enhanced Distribution Alignment for
Post-Training Quantization of Diffusion Models (EDA-DM) to address the above
issues. Specifically, at the calibration sample level, we select calibration
samples based on the density and diversity in the latent space, thus
facilitating the alignment of their distribution with the overall samples; and
at the reconstruction output level, we propose Fine-grained Block
Reconstruction, which can align the outputs of the quantized model and the
full-precision model at different network granularity. Extensive experiments
demonstrate that EDA-DM outperforms the existing post-training quantization
frameworks in both unconditional and conditional generation scenarios. At
low-bit precision, the quantized models with our method even outperform the
full-precision models on most datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuewen Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhikai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_J/0/1/0/all/0/1&quot;&gt;Junrui Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_Q/0/1/0/all/0/1&quot;&gt;Qingyi Gu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04608">
<title>EmoGen: Emotional Image Content Generation with Text-to-Image Diffusion Models. (arXiv:2401.04608v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04608</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed remarkable progress in image generation task,
where users can create visually astonishing images with high-quality. However,
existing text-to-image diffusion models are proficient in generating concrete
concepts (dogs) but encounter challenges with more abstract ones (emotions).
Several efforts have been made to modify image emotions with color and style
adjustments, facing limitations in effectively conveying emotions with fixed
image contents. In this work, we introduce Emotional Image Content Generation
(EICG), a new task to generate semantic-clear and emotion-faithful images given
emotion categories. Specifically, we propose an emotion space and construct a
mapping network to align it with the powerful Contrastive Language-Image
Pre-training (CLIP) space, providing a concrete interpretation of abstract
emotions. Attribute loss and emotion confidence are further proposed to ensure
the semantic diversity and emotion fidelity of the generated images. Our method
outperforms the state-of-the-art text-to-image approaches both quantitatively
and qualitatively, where we derive three custom metrics, i.e., emotion
accuracy, semantic clarity and semantic diversity. In addition to generation,
our method can help emotion understanding and inspire emotional art design.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jingyuan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_J/0/1/0/all/0/1&quot;&gt;Jiawei Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hui Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04614">
<title>Generic Knowledge Boosted Pre-training For Remote Sensing Images. (arXiv:2401.04614v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04614</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning models are essential for scene classification, change
detection, land cover segmentation, and other remote sensing image
understanding tasks. Most backbones of existing remote sensing deep learning
models are typically initialized by pre-trained weights obtained from ImageNet
pre-training (IMP). However, domain gaps exist between remote sensing images
and natural images (e.g., ImageNet), making deep learning models initialized by
pre-trained weights of IMP perform poorly for remote sensing image
understanding. Although some pre-training methods are studied in the remote
sensing community, current remote sensing pre-training methods face the problem
of vague generalization by only using remote sensing images. In this paper, we
propose a novel remote sensing pre-training framework, Generic Knowledge
Boosted Remote Sensing Pre-training (GeRSP), to learn robust representations
from remote sensing and natural images for remote sensing understanding tasks.
GeRSP contains two pre-training branches: (1) A self-supervised pre-training
branch is adopted to learn domain-related representations from unlabeled remote
sensing images. (2) A supervised pre-training branch is integrated into GeRSP
for general knowledge learning from labeled natural images. Moreover, GeRSP
combines two pre-training branches using a teacher-student architecture to
simultaneously learn representations with general and special knowledge, which
generates a powerful pre-trained model for deep learning model initialization.
Finally, we evaluate GeRSP and other remote sensing pre-training methods on
three downstream tasks, i.e., object detection, semantic segmentation, and
scene classification. The extensive experimental results consistently
demonstrate that GeRSP can effectively learn robust representations in a
unified manner, improving the performance of remote sensing downstream tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Ziyue Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mingming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gong_Y/0/1/0/all/0/1&quot;&gt;Yuan Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Qingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yunhong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04647">
<title>Advancing Ante-Hoc Explainable Models through Generative Adversarial Networks. (arXiv:2401.04647v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04647</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a novel concept learning framework for enhancing model
interpretability and performance in visual classification tasks. Our approach
appends an unsupervised explanation generator to the primary classifier network
and makes use of adversarial training. During training, the explanation module
is optimized to extract visual concepts from the classifier&apos;s latent
representations, while the GAN-based module aims to discriminate images
generated from concepts, from true images. This joint training scheme enables
the model to implicitly align its internally learned concepts with
human-interpretable visual properties. Comprehensive experiments demonstrate
the robustness of our approach, while producing coherent concept activations.
We analyse the learned concepts, showing their semantic concordance with object
parts and visual attributes. We also study how perturbations in the adversarial
training protocol impact both classification and concept acquisition. In
summary, this work presents a significant step towards building inherently
interpretable deep vision models with task-aligned concept representations - a
key enabler for developing trustworthy AI for real-world perception tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Garg_T/0/1/0/all/0/1&quot;&gt;Tanmay Garg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vemuri_D/0/1/0/all/0/1&quot;&gt;Deepika Vemuri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balasubramanian_V/0/1/0/all/0/1&quot;&gt;Vineeth N Balasubramanian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04651">
<title>Learning to Prompt Segment Anything Models. (arXiv:2401.04651v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04651</link>
<description rdf:parseType="Literal">&lt;p&gt;Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great
potential in learning to segment anything. The core design of SAMs lies with
Promptable Segmentation, which takes a handcrafted prompt as input and returns
the expected segmentation mask. SAMs work with two types of prompts including
spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work
together to prompt SAMs to segment anything on downstream datasets. Despite the
important role of prompts, how to acquire suitable prompts for SAMs is largely
under-explored. In this work, we examine the architecture of SAMs and identify
two challenges for learning effective prompts for SAMs. To this end, we propose
spatial-semantic prompt learning (SSPrompt) that learns effective semantic and
spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial
prompt learning and semantic prompt learning, which optimize spatial prompts
and semantic prompts directly over the embedding space and selectively leverage
the knowledge encoded in pre-trained prompt encoders. Extensive experiments
show that SSPrompt achieves superior image segmentation performance
consistently across multiple widely adopted datasets.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_J/0/1/0/all/0/1&quot;&gt;Jiaxing Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_K/0/1/0/all/0/1&quot;&gt;Kai Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingyi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiu_H/0/1/0/all/0/1&quot;&gt;Han Qiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Lewei Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04666">
<title>Benchmark Analysis of Various Pre-trained Deep Learning Models on ASSIRA Cats and Dogs Dataset. (arXiv:2401.04666v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04666</link>
<description rdf:parseType="Literal">&lt;p&gt;As the most basic application and implementation of deep learning, image
classification has grown in popularity. Various datasets are provided by
renowned data science communities for benchmarking machine learning algorithms
and pre-trained models. The ASSIRA Cats &amp;amp; Dogs dataset is one of them and is
being used in this research for its overall acceptance and benchmark standards.
A comparison of various pre-trained models is demonstrated by using different
types of optimizers and loss functions. Hyper-parameters are changed to gain
the best result from a model. By applying this approach, we have got higher
accuracy without major changes in the training model. To run the experiment, we
used three different computer architectures: a laptop equipped with NVIDIA
GeForce GTX 1070, a laptop equipped with NVIDIA GeForce RTX 3080Ti, and a
desktop equipped with NVIDIA GeForce RTX 3090. The acquired results demonstrate
supremacy in terms of accuracy over the previously done experiments on this
dataset. From this experiment, the highest accuracy which is 99.65% is gained
using the NASNet Large.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Himel_G/0/1/0/all/0/1&quot;&gt;Galib Muhammad Shahriar Himel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Islam_M/0/1/0/all/0/1&quot;&gt;Md. Masudul Islam&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04680">
<title>CoordGate: Efficiently Computing Spatially-Varying Convolutions in Convolutional Neural Networks. (arXiv:2401.04680v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04680</link>
<description rdf:parseType="Literal">&lt;p&gt;Optical imaging systems are inherently limited in their resolution due to the
point spread function (PSF), which applies a static, yet spatially-varying,
convolution to the image. This degradation can be addressed via Convolutional
Neural Networks (CNNs), particularly through deblurring techniques. However,
current solutions face certain limitations in efficiently computing
spatially-varying convolutions. In this paper we propose CoordGate, a novel
lightweight module that uses a multiplicative gate and a coordinate encoding
network to enable efficient computation of spatially-varying convolutions in
CNNs. CoordGate allows for selective amplification or attenuation of filters
based on their spatial position, effectively acting like a locally connected
neural network. The effectiveness of the CoordGate solution is demonstrated
within the context of U-Nets and applied to the challenging problem of image
deblurring. The experimental results show that CoordGate outperforms
conventional approaches, offering a more robust and spatially aware solution
for CNNs in various computer vision applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Howard_S/0/1/0/all/0/1&quot;&gt;Sunny Howard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norreys_P/0/1/0/all/0/1&quot;&gt;Peter Norreys&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dopp_A/0/1/0/all/0/1&quot;&gt;Andreas D&amp;#xf6;pp&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04716">
<title>Low-Resource Vision Challenges for Foundation Models. (arXiv:2401.04716v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04716</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-resource settings are well-established in natural language processing,
where many languages lack sufficient data for machine learning at scale.
However, low-resource problems are under-explored in computer vision. In this
paper, we strive to address this gap and explore the challenges of low-resource
image tasks with vision foundation models. Thus, we first collect a benchmark
of genuinely low-resource image data, covering historic maps, circuit diagrams,
and mechanical drawings. These low-resource settings all share the three
challenges of data scarcity, fine-grained differences, and the distribution
shift from natural images to the specialized domain of interest. While existing
foundation models have shown impressive generalizability, we find they cannot
transfer well to our low-resource tasks. To begin to tackle the challenges of
low-resource vision, we introduce one simple baseline per challenge.
Specifically, we propose to i) enlarge the data space by generative models, ii)
adopt the best sub-kernels to encode local regions for fine-grained difference
discovery and iii) learn attention for specialized domains. Experiments on the
three low-resource data sources in our benchmark demonstrate our proposals
already provide a better baseline than common transfer learning, data
augmentation, and fine-grained methods. This highlights the unique
characteristics and challenges of low-resource vision for foundation models
that warrant further investigation. Project website:
https://xiaobai1217.github.io/Low-Resource-Vision/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yunhua Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Doughty_H/0/1/0/all/0/1&quot;&gt;Hazel Doughty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Snoek_C/0/1/0/all/0/1&quot;&gt;Cees G.M. Snoek&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04718">
<title>Jump Cut Smoothing for Talking Heads. (arXiv:2401.04718v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04718</link>
<description rdf:parseType="Literal">&lt;p&gt;A jump cut offers an abrupt, sometimes unwanted change in the viewing
experience. We present a novel framework for smoothing these jump cuts, in the
context of talking head videos. We leverage the appearance of the subject from
the other source frames in the video, fusing it with a mid-level representation
driven by DensePose keypoints and face landmarks. To achieve motion, we
interpolate the keypoints and landmarks between the end frames around the cut.
We then use an image translation network from the keypoints and source frames,
to synthesize pixels. Because keypoints can contain errors, we propose a
cross-modal attention scheme to select and pick the most appropriate source
amongst multiple options for each key point. By leveraging this mid-level
representation, our method can achieve stronger results than a strong video
interpolation baseline. We demonstrate our method on various jump cuts in the
talking head videos, such as cutting filler words, pauses, and even random
cuts. Our experiments show that we can achieve seamless transitions, even in
the challenging cases where the talking head rotates or moves drastically in
the jump cut.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_T/0/1/0/all/0/1&quot;&gt;Taesung Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yang Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shechtman_E/0/1/0/all/0/1&quot;&gt;Eli Shechtman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Richard Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04720">
<title>Low-resource finetuning of foundation models beats state-of-the-art in histopathology. (arXiv:2401.04720v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04720</link>
<description rdf:parseType="Literal">&lt;p&gt;To handle the large scale of whole slide images in computational pathology,
most approaches first tessellate the images into smaller patches, extract
features from these patches, and finally aggregate the feature vectors with
weakly-supervised learning. The performance of this workflow strongly depends
on the quality of the extracted features. Recently, foundation models in
computer vision showed that leveraging huge amounts of data through supervised
or self-supervised learning improves feature quality and generalizability for a
variety of tasks. In this study, we benchmark the most popular vision
foundation models as feature extractors for histopathology data. We evaluate
the models in two settings: slide-level classification and patch-level
classification. We show that foundation models are a strong baseline. Our
experiments demonstrate that by finetuning a foundation model on a single GPU
for only two hours or three days depending on the dataset, we can match or
outperform state-of-the-art feature extractors for computational pathology.
These findings imply that even with little resources one can finetune a feature
extractor tailored towards a specific downstream task and dataset. This is a
considerable shift from the current state, where only few institutions with
large amounts of resources and datasets are able to train a feature extractor.
We publish all code used for training and evaluation as well as the finetuned
models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Roth_B/0/1/0/all/0/1&quot;&gt;Benedikt Roth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koch_V/0/1/0/all/0/1&quot;&gt;Valentin Koch&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wagner_S/0/1/0/all/0/1&quot;&gt;Sophia J. Wagner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schnabel_J/0/1/0/all/0/1&quot;&gt;Julia A. Schnabel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marr_C/0/1/0/all/0/1&quot;&gt;Carsten Marr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_T/0/1/0/all/0/1&quot;&gt;Tingying Peng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04722">
<title>U-Mamba: Enhancing Long-range Dependency for Biomedical Image Segmentation. (arXiv:2401.04722v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2401.04722</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional Neural Networks (CNNs) and Transformers have been the most
popular architectures for biomedical image segmentation, but both of them have
limited ability to handle long-range dependencies because of inherent locality
or computational complexity. To address this challenge, we introduce U-Mamba, a
general-purpose network for biomedical image segmentation. Inspired by the
State Space Sequence Models (SSMs), a new family of deep sequence models known
for their strong capability in handling long sequences, we design a hybrid
CNN-SSM block that integrates the local feature extraction power of
convolutional layers with the abilities of SSMs for capturing the long-range
dependency. Moreover, U-Mamba enjoys a self-configuring mechanism, allowing it
to automatically adapt to various datasets without manual intervention. We
conduct extensive experiments on four diverse tasks, including the 3D abdominal
organ segmentation in CT and MR images, instrument segmentation in endoscopy
images, and cell segmentation in microscopy images. The results reveal that
U-Mamba outperforms state-of-the-art CNN-based and Transformer-based
segmentation networks across all tasks. This opens new avenues for efficient
long-range dependency modeling in biomedical image analysis. The code, models,
and data are publicly available at https://wanglab.ai/u-mamba.html.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jun Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_F/0/1/0/all/0/1&quot;&gt;Feifei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bo Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04727">
<title>Revisiting Adversarial Training at Scale. (arXiv:2401.04727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04727</link>
<description rdf:parseType="Literal">&lt;p&gt;The machine learning community has witnessed a drastic change in the training
pipeline, pivoted by those &apos;&apos;foundation models&apos;&apos; with unprecedented scales.
However, the field of adversarial training is lagging behind, predominantly
centered around small model sizes like ResNet-50, and tiny and low-resolution
datasets like CIFAR-10. To bridge this transformation gap, this paper provides
a modern re-examination with adversarial training, investigating its potential
benefits when applied at scale. Additionally, we introduce an efficient and
effective training strategy to enable adversarial training with giant models
and web-scale data at an affordable computing cost. We denote this newly
introduced framework as AdvXL.
&lt;/p&gt;
&lt;p&gt;Empirical results demonstrate that AdvXL establishes new state-of-the-art
robust accuracy records under AutoAttack on ImageNet-1K. For example, by
training on DataComp-1B dataset, our AdvXL empowers a vanilla ViT-g model to
substantially surpass the previous records of $l_{\infty}$-, $l_{2}$-, and
$l_{1}$-robust accuracy by margins of 11.4%, 14.2% and 12.9%, respectively.
This achievement posits AdvXL as a pioneering approach, charting a new
trajectory for the efficient training of robust visual representations at
significantly larger scales. Our code is available at
https://github.com/UCSC-VLAA/AdvXL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xianhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongru Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_C/0/1/0/all/0/1&quot;&gt;Cihang Xie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04728">
<title>Morphable Diffusion: 3D-Consistent Diffusion for Single-image Avatar Creation. (arXiv:2401.04728v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04728</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in generative diffusion models have enabled the previously
unfeasible capability of generating 3D assets from a single input image or a
text prompt. In this work, we aim to enhance the quality and functionality of
these models for the task of creating controllable, photorealistic human
avatars. We achieve this by integrating a 3D morphable model into the
state-of-the-art multiview-consistent diffusion approach. We demonstrate that
accurate conditioning of a generative pipeline on the articulated 3D model
enhances the baseline model performance on the task of novel view synthesis
from a single image. More importantly, this integration facilitates a seamless
and accurate incorporation of facial expression and body pose control into the
generation process. To the best of our knowledge, our proposed framework is the
first diffusion model to enable the creation of fully 3D-consistent,
animatable, and photorealistic human avatars from a single image of an unseen
subject; extensive quantitative and qualitative evaluations demonstrate the
advantages of our approach over existing state-of-the-art avatar creation
models on both novel view and novel expression synthesis tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiyi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mihajlovic_M/0/1/0/all/0/1&quot;&gt;Marko Mihajlovic&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shaofei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prokudin_S/0/1/0/all/0/1&quot;&gt;Sergey Prokudin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_S/0/1/0/all/0/1&quot;&gt;Siyu Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.04730">
<title>A Simple Baseline for Spoken Language to Sign Language Translation with 3D Avatars. (arXiv:2401.04730v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2401.04730</link>
<description rdf:parseType="Literal">&lt;p&gt;The objective of this paper is to develop a functional system for translating
spoken languages into sign languages, referred to as Spoken2Sign translation.
The Spoken2Sign task is orthogonal and complementary to traditional sign
language to spoken language (Sign2Spoken) translation. To enable Spoken2Sign
translation, we present a simple baseline consisting of three steps: 1)
creating a gloss-video dictionary using existing Sign2Spoken benchmarks; 2)
estimating a 3D sign for each sign video in the dictionary; 3) training a
Spoken2Sign model, which is composed of a Text2Gloss translator, a sign
connector, and a rendering module, with the aid of the yielded gloss-3D sign
dictionary. The translation results are then displayed through a sign avatar.
As far as we know, we are the first to present the Spoken2Sign task in an
output format of 3D signs. In addition to its capability of Spoken2Sign
translation, we also demonstrate that two by-products of our approach-3D
keypoint augmentation and multi-view understanding-can assist in keypoint-based
sign language understanding. Code and models will be available at
https://github.com/FangyunWei/SLRT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_R/0/1/0/all/0/1&quot;&gt;Ronglai Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_F/0/1/0/all/0/1&quot;&gt;Fangyun Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zenggui Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mak_B/0/1/0/all/0/1&quot;&gt;Brian Mak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_J/0/1/0/all/0/1&quot;&gt;Jiaolong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_X/0/1/0/all/0/1&quot;&gt;Xin Tong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2109.05206">
<title>PHPQ: Pyramid Hybrid Pooling Quantization for Efficient Fine-Grained Image Retrieval. (arXiv:2109.05206v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2109.05206</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep hashing approaches, including deep quantization and deep binary hashing,
have become a common solution to large-scale image retrieval due to their high
computation and storage efficiency. Most existing hashing methods cannot
produce satisfactory results for fine-grained retrieval, because they usually
adopt the outputs of the last CNN layer to generate binary codes. Since deeper
layers tend to summarize visual clues, e.g., texture, into abstract semantics,
e.g., dogs and cats, the feature produced by the last CNN layer is less
effective in capturing subtle but discriminative visual details that mostly
exist in shallow layers. To improve fine-grained image hashing, we propose
Pyramid Hybrid Pooling Quantization (PHPQ). Specifically, we propose a Pyramid
Hybrid Pooling (PHP) module to capture and preserve fine-grained semantic
information from multi-level features, which emphasizes the subtle
discrimination of different sub-categories. Besides, we propose a learnable
quantization module with a partial codebook attention mechanism, which helps to
optimize the most relevant codewords and improves the quantization.
Comprehensive experiments on two widely-used public benchmarks, i.e.,
CUB-200-2011 and Stanford Dogs, demonstrate that PHPQ outperforms
state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Ziyun Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jinpeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Bin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhi Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2201.12577">
<title>Volley Revolver: A Novel Matrix-Encoding Method for Privacy-Preserving Neural Networks (Inference). (arXiv:2201.12577v4 [cs.CR] UPDATED)</title>
<link>http://arxiv.org/abs/2201.12577</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present a novel matrix-encoding method that is particularly
convenient for neural networks to make predictions in a privacy-preserving
manner using homomorphic encryption. Based on this encoding method, we
implement a convolutional neural network for handwritten image classification
over encryption. For two matrices $A$ and $B$ to perform homomorphic
multiplication, the main idea behind it, in a simple version, is to encrypt
matrix $A$ and the transpose of matrix $B$ into two ciphertexts respectively.
With additional operations, the homomorphic matrix multiplication can be
calculated over encrypted matrices efficiently. For the convolution operation,
we in advance span each convolution kernel to a matrix space of the same size
as the input image so as to generate several ciphertexts, each of which is
later used together with the ciphertext encrypting input images for calculating
some of the final convolution results. We accumulate all these intermediate
results and thus complete the convolution operation.
&lt;/p&gt;
&lt;p&gt;In a public cloud with 40 vCPUs, our convolutional neural network
implementation on the MNIST testing dataset takes $\sim$ 287 seconds to compute
ten likelihoods of 32 encrypted images of size $28 \times 28$ simultaneously.
The data owner only needs to upload one ciphertext ($\sim 19.8$ MB) encrypting
these 32 images to the public cloud.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chiang_J/0/1/0/all/0/1&quot;&gt;John Chiang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.10347">
<title>Diverse super-resolution with pretrained deep hiererarchical VAEs. (arXiv:2205.10347v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.10347</link>
<description rdf:parseType="Literal">&lt;p&gt;We investigate the problem of producing diverse solutions to an image
super-resolution problem. From a probabilistic perspective, this can be done by
sampling from the posterior distribution of an inverse problem, which requires
the definition of a prior distribution on the high-resolution images. In this
work, we propose to use a pretrained hierarchical variational autoencoder
(HVAE) as a prior. We train a lightweight stochastic encoder to encode
low-resolution images in the latent space of a pretrained HVAE. At inference,
we combine the low-resolution encoder and the pretrained generative model to
super-resolve an image. We demonstrate on the task of face super-resolution
that our method provides an advantageous trade-off between the computational
efficiency of conditional normalizing flows techniques and the sample quality
of diffusion based methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prost_J/0/1/0/all/0/1&quot;&gt;Jean Prost&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Houdard_A/0/1/0/all/0/1&quot;&gt;Antoine Houdard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Almansa_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9;s Almansa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadakis_N/0/1/0/all/0/1&quot;&gt;Nicolas Papadakis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2209.08891">
<title>Exploiting Cultural Biases via Homoglyphs in Text-to-Image Synthesis. (arXiv:2209.08891v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2209.08891</link>
<description rdf:parseType="Literal">&lt;p&gt;Models for text-to-image synthesis, such as DALL-E~2 and Stable Diffusion,
have recently drawn a lot of interest from academia and the general public.
These models are capable of producing high-quality images that depict a variety
of concepts and styles when conditioned on textual descriptions. However, these
models adopt cultural characteristics associated with specific Unicode scripts
from their vast amount of training data, which may not be immediately apparent.
We show that by simply inserting single non-Latin characters in a textual
description, common models reflect cultural stereotypes and biases in their
generated images. We analyze this behavior both qualitatively and
quantitatively, and identify a model&apos;s text encoder as the root cause of the
phenomenon. Additionally, malicious users or service providers may try to
intentionally bias the image generation to create racist stereotypes by
replacing Latin characters with similarly-looking characters from non-Latin
scripts, so-called homoglyphs. To mitigate such unnoticed script attacks, we
propose a novel homoglyph unlearning method to fine-tune a text encoder, making
it robust against homoglyph manipulations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Struppek_L/0/1/0/all/0/1&quot;&gt;Lukas Struppek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hintersdorf_D/0/1/0/all/0/1&quot;&gt;Dominik Hintersdorf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Friedrich_F/0/1/0/all/0/1&quot;&gt;Felix Friedrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brack_M/0/1/0/all/0/1&quot;&gt;Manuel Brack&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schramowski_P/0/1/0/all/0/1&quot;&gt;Patrick Schramowski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kersting_K/0/1/0/all/0/1&quot;&gt;Kristian Kersting&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.07675">
<title>Learning image representations for anomaly detection: application to discovery of histological alterations in drug development. (arXiv:2210.07675v7 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.07675</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a system for anomaly detection in histopathological images. In
histology, normal samples are usually abundant, whereas anomalous
(pathological) cases are scarce or not available. Under such settings,
one-class classifiers trained on healthy data can detect out-of-distribution
anomalous samples. Such approaches combined with pre-trained Convolutional
Neural Network (CNN) representations of images were previously employed for
anomaly detection (AD). However, pre-trained off-the-shelf CNN representations
may not be sensitive to abnormal conditions in tissues, while natural
variations of healthy tissue may result in distant representations. To adapt
representations to relevant details in healthy tissue we propose training a CNN
on an auxiliary task that discriminates healthy tissue of different species,
organs, and staining reagents. Almost no additional labeling workload is
required, since healthy samples come automatically with aforementioned labels.
During training we enforce compact image representations with a center-loss
term, which further improves representations for AD. The proposed system
outperforms established AD methods on a published dataset of liver anomalies.
Moreover, it provided comparable results to conventional methods specifically
tailored for quantification of liver anomalies. We show that our approach can
be used for toxicity assessment of candidate drugs at early development stages
and thereby may reduce expensive late-stage drug attrition.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zingman_I/0/1/0/all/0/1&quot;&gt;Igor Zingman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stierstorfer_B/0/1/0/all/0/1&quot;&gt;Birgit Stierstorfer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lempp_C/0/1/0/all/0/1&quot;&gt;Charlotte Lempp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heinemann_F/0/1/0/all/0/1&quot;&gt;Fabian Heinemann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.06797">
<title>Perceptual Video Coding for Machines via Satisfied Machine Ratio Modeling. (arXiv:2211.06797v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.06797</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Coding for Machines (VCM) aims to compress visual signals for machine
analysis. However, existing methods only consider a few machines, neglecting
the majority. Moreover, the machine&apos;s perceptual characteristics are not
leveraged effectively, resulting in suboptimal compression efficiency. To
overcome these limitations, this paper introduces Satisfied Machine Ratio
(SMR), a metric that statistically evaluates the perceptual quality of
compressed images and videos for machines by aggregating satisfaction scores
from them. Each score is derived from machine perceptual differences between
original and compressed images. Targeting image classification and object
detection tasks, we build two representative machine libraries for SMR
annotation and create a large-scale SMR dataset to facilitate SMR studies. We
then propose an SMR prediction model based on the correlation between deep
feature differences and SMR. Furthermore, we introduce an auxiliary task to
increase the prediction accuracy by predicting the SMR difference between two
images in different quality. Extensive experiments demonstrate that SMR models
significantly improve compression performance for machines and exhibit robust
generalizability on unseen machines, codecs, datasets, and frame types. SMR
enables perceptual coding for machines and propels VCM from specificity to
generality. Code is available at https://github.com/ywwynm/SMR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Q/0/1/0/all/0/1&quot;&gt;Qi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shanshe Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinfeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_C/0/1/0/all/0/1&quot;&gt;Chuanmin Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_S/0/1/0/all/0/1&quot;&gt;Siwei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_W/0/1/0/all/0/1&quot;&gt;Wen Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2211.15374">
<title>Identification of Surface Defects on Solar PV Panels and Wind Turbine Blades using Attention based Deep Learning Model. (arXiv:2211.15374v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2211.15374</link>
<description rdf:parseType="Literal">&lt;p&gt;The global generation of renewable energy has rapidly increased, primarily
due to the installation of large-scale renewable energy power plants. However,
monitoring renewable energy assets in these large plants remains challenging
due to environmental factors that could result in reduced power generation,
malfunctioning, and degradation of asset life. Therefore, the detection of
surface defects on renewable energy assets is crucial for maintaining the
performance and efficiency of these plants. This paper proposes an innovative
detection framework to achieve an economical surface monitoring system for
renewable energy assets. High-resolution images of the assets are captured
regularly and inspected to identify surface or structural damages on solar
panels and wind turbine blades. {Vision transformer (ViT), one of the latest
attention-based deep learning (DL) models in computer vision, is proposed in
this work to classify surface defects.} The ViT model outperforms other DL
models, including MobileNet, VGG16, Xception, EfficientNetB7, and ResNet50,
achieving high accuracy scores above 97\% for both wind and solar plant assets.
From the results, our proposed model demonstrates its potential for monitoring
and detecting damages in renewable energy assets for efficient and reliable
operation of renewable power plants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dwivedi_D/0/1/0/all/0/1&quot;&gt;Divyanshi Dwivedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Babu_K/0/1/0/all/0/1&quot;&gt;K. Victor Sam Moses Babu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yemula_P/0/1/0/all/0/1&quot;&gt;Pradeep Kumar Yemula&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chakraborty_P/0/1/0/all/0/1&quot;&gt;Pratyush Chakraborty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pal_M/0/1/0/all/0/1&quot;&gt;Mayukha Pal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.00950">
<title>Class-Continuous Conditional Generative Neural Radiance Field. (arXiv:2301.00950v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.00950</link>
<description rdf:parseType="Literal">&lt;p&gt;The 3D-aware image synthesis focuses on conserving spatial consistency
besides generating high-resolution images with fine details. Recently, Neural
Radiance Field (NeRF) has been introduced for synthesizing novel views with low
computational cost and superior performance. While several works investigate a
generative NeRF and show remarkable achievement, they cannot handle conditional
and continuous feature manipulation in the generation procedure. In this work,
we introduce a novel model, called Class-Continuous Conditional Generative NeRF
($\text{C}^{3}$G-NeRF), which can synthesize conditionally manipulated
photorealistic 3D-consistent images by projecting conditional features to the
generator and the discriminator. The proposed $\text{C}^{3}$G-NeRF is evaluated
with three image datasets, AFHQ, CelebA, and Cars. As a result, our model shows
strong 3D-consistency with fine details and smooth interpolation in conditional
feature manipulation. For instance, $\text{C}^{3}$G-NeRF exhibits a Fr\&apos;echet
Inception Distance (FID) of 7.64 in 3D-aware face image synthesis with a
$\text{128}^{2}$ resolution. Additionally, we provide FIDs of generated
3D-aware images of each class of the datasets as it is possible to synthesize
class-conditional images with $\text{C}^{3}$G-NeRF.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwook Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyeok Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.13337">
<title>DAFD: Domain Adaptation via Feature Disentanglement for Image Classification. (arXiv:2301.13337v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.13337</link>
<description rdf:parseType="Literal">&lt;p&gt;A good feature representation is the key to image classification. In
practice, image classifiers may be applied in scenarios different from what
they have been trained on. This so-called domain shift leads to a significant
performance drop in image classification. Unsupervised domain adaptation (UDA)
reduces the domain shift by transferring the knowledge learned from a labeled
source domain to an unlabeled target domain. We perform feature disentanglement
for UDA by distilling category-relevant features and excluding
category-irrelevant features from the global feature maps. This disentanglement
prevents the network from overfitting to category-irrelevant information and
makes it focus on information useful for classification. This reduces the
difficulty of domain alignment and improves the classification accuracy on the
target domain. We propose a coarse-to-fine domain adaptation method called
Domain Adaptation via Feature Disentanglement~(DAFD), which has two components:
(1)the Category-Relevant Feature Selection (CRFS) module, which disentangles
the category-relevant features from the category-irrelevant features, and
(2)the Dynamic Local Maximum Mean Discrepancy (DLMMD) module, which achieves
fine-grained alignment by reducing the discrepancy within the category-relevant
features from different domains. Combined with the CRFS, the DLMMD module can
align the category-relevant features properly. We conduct comprehensive
experiment on four standard datasets. Our results clearly demonstrate the
robustness and effectiveness of our approach in domain adaptive image
classification tasks and its competitiveness to the state of the art.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhize Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_C/0/1/0/all/0/1&quot;&gt;Changjiang Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_L/0/1/0/all/0/1&quot;&gt;Le Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_M/0/1/0/all/0/1&quot;&gt;Ming Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_F/0/1/0/all/0/1&quot;&gt;Fan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nian_F/0/1/0/all/0/1&quot;&gt;Fudong Nian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weise_T/0/1/0/all/0/1&quot;&gt;Thomas Weise&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.07545">
<title>Implicit and Explicit Commonsense for Multi-sentence Video Captioning. (arXiv:2303.07545v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.07545</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing dense or paragraph video captioning approaches rely on holistic
representations of videos, possibly coupled with learned object/action
representations, to condition hierarchical language decoders. However, they
fundamentally lack the commonsense knowledge of the world required to reason
about progression of events, causality, and even the function of certain
objects within a scene. To address this limitation we propose a novel video
captioning Transformer-based model, that takes into account both implicit
(visuo-lingual and purely linguistic) and explicit (knowledge-base) commonsense
knowledge. We show that these forms of knowledge, in isolation and in
combination, enhance the quality of produced captions. Further, inspired by
imitation learning, we propose a new task of instruction generation, where the
goal is to produce a set of linguistic instructions from a video demonstration
of its performance. We formalize the task using the ALFRED dataset [54]
generated using an AI2-THOR environment. While instruction generation is
conceptually similar to paragraph captioning, it differs in the fact that it
exhibits stronger object persistence, as well as spatially-aware and causal
sentence structure. We show that our commonsense knowledge enhanced approach
produces significant improvements on this task (up to 57% in METEOR and 8.5% in
CIDEr), as well as the state-of-the-art result on more traditional video
captioning in the ActivityNet Captions dataset [29].
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chou_S/0/1/0/all/0/1&quot;&gt;Shih-Han Chou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Little_J/0/1/0/all/0/1&quot;&gt;James J. Little&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sigal_L/0/1/0/all/0/1&quot;&gt;Leonid Sigal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.15225">
<title>GP-PCS: One-shot Feature-Preserving Point Cloud Simplification with Gaussian Processes on Riemannian Manifolds. (arXiv:2303.15225v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.15225</link>
<description rdf:parseType="Literal">&lt;p&gt;The processing, storage and transmission of large-scale point clouds is an
ongoing challenge in the computer vision community which hinders progress in
the application of 3D models to real-world settings, such as autonomous
driving, virtual reality and remote sensing. We propose a novel, one-shot point
cloud simplification method which preserves both the salient structural
features and the overall shape of a point cloud without any prior surface
reconstruction step. Our method employs Gaussian processes suitable for
functions defined on Riemannian manifolds, allowing us to model the surface
variation function across any given point cloud. A simplified version of the
original cloud is obtained by sequentially selecting points using a greedy
sparsification scheme. The selection criterion used for this scheme ensures
that the simplified cloud best represents the surface variation of the original
point cloud. We evaluate our method on several benchmark and self-acquired
point clouds, compare it to a range of existing methods, demonstrate its
application in downstream tasks of registration and surface reconstruction, and
show that our method is competitive both in terms of empirical performance and
computational efficiency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_S/0/1/0/all/0/1&quot;&gt;Stuti Pathak&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McDonald_T/0/1/0/all/0/1&quot;&gt;Thomas M. McDonald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sels_S/0/1/0/all/0/1&quot;&gt;Seppe Sels&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Penne_R/0/1/0/all/0/1&quot;&gt;Rudi Penne&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.01899">
<title>Cross-Class Feature Augmentation for Class Incremental Learning. (arXiv:2304.01899v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.01899</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a novel class incremental learning approach by incorporating a
feature augmentation technique motivated by adversarial attacks. We employ a
classifier learned in the past to complement training examples rather than
simply play a role as a teacher for knowledge distillation towards subsequent
models. The proposed approach has a unique perspective to utilize the previous
knowledge in class incremental learning since it augments features of arbitrary
target classes using examples in other classes via adversarial attacks on a
previously learned classifier. By allowing the cross-class feature
augmentations, each class in the old tasks conveniently populates samples in
the feature space, which alleviates the collapse of the decision boundaries
caused by sample deficiency for the previous tasks, especially when the number
of stored exemplars is small. This idea can be easily incorporated into
existing class incremental learning algorithms without any architecture
modification. Extensive experiments on the standard benchmarks show that our
method consistently outperforms existing class incremental learning methods by
significant margins in various scenarios, especially under an environment with
an extremely limited memory budget.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jaeyoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_B/0/1/0/all/0/1&quot;&gt;Bohyung Han&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.12760">
<title>Parallel Spiking Neurons with High Efficiency and Ability to Learn Long-term Dependencies. (arXiv:2304.12760v4 [cs.NE] UPDATED)</title>
<link>http://arxiv.org/abs/2304.12760</link>
<description rdf:parseType="Literal">&lt;p&gt;Vanilla spiking neurons in Spiking Neural Networks (SNNs) use
charge-fire-reset neuronal dynamics, which can only be simulated serially and
can hardly learn long-time dependencies. We find that when removing reset, the
neuronal dynamics can be reformulated in a non-iterative form and parallelized.
By rewriting neuronal dynamics without reset to a general formulation, we
propose the Parallel Spiking Neuron (PSN), which generates hidden states that
are independent of their predecessors, resulting in parallelizable neuronal
dynamics and extremely high simulation speed. The weights of inputs in the PSN
are fully connected, which maximizes the utilization of temporal information.
To avoid the use of future inputs for step-by-step inference, the weights of
the PSN can be masked, resulting in the masked PSN. By sharing weights across
time-steps based on the masked PSN, the sliding PSN is proposed to handle
sequences of varying lengths. We evaluate the PSN family on simulation speed
and temporal/static data classification, and the results show the overwhelming
advantage of the PSN family in efficiency and accuracy. To the best of our
knowledge, this is the first study about parallelizing spiking neurons and can
be a cornerstone for the spiking deep learning research. Our codes are
available at \url{https://github.com/fangwei123456/Parallel-Spiking-Neuron}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_W/0/1/0/all/0/1&quot;&gt;Wei Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhaofei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Zhaokun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Ding Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yanqi Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Masquelier_T/0/1/0/all/0/1&quot;&gt;Timoth&amp;#xe9;e Masquelier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yonghong Tian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.14811">
<title>NeRF-LiDAR: Generating Realistic LiDAR Point Clouds with Neural Radiance Fields. (arXiv:2304.14811v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.14811</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeling LiDAR point clouds for training autonomous driving is extremely
expensive and difficult. LiDAR simulation aims at generating realistic LiDAR
data with labels for training and verifying self-driving algorithms more
efficiently. Recently, Neural Radiance Fields (NeRF) have been proposed for
novel view synthesis using implicit reconstruction of 3D scenes. Inspired by
this, we present NeRF-LIDAR, a novel LiDAR simulation method that leverages
real-world information to generate realistic LIDAR point clouds. Different from
existing LiDAR simulators, we use real images and point cloud data collected by
self-driving cars to learn the 3D scene representation, point cloud generation
and label rendering. We verify the effectiveness of our NeRF-LiDAR by training
different 3D segmentation models on the generated LiDAR point clouds. It
reveals that the trained models are able to achieve similar accuracy when
compared with the same model trained on the real LiDAR data. Besides, the
generated data is capable of boosting the accuracy through pre-training which
helps reduce the requirements of the real labeled data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junge Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Feihu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuang_S/0/1/0/all/0/1&quot;&gt;Shaochen Kuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.10406">
<title>Variational Classification. (arXiv:2305.10406v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2305.10406</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a latent variable model for classification that provides a novel
probabilistic interpretation of neural network softmax classifiers. We derive a
variational objective to train the model, analogous to the evidence lower bound
(ELBO) used to train variational auto-encoders, that generalises the softmax
cross-entropy loss. Treating inputs to the softmax layer as samples of a latent
variable, our abstracted perspective reveals a potential inconsistency between
their anticipated distribution, required for accurate label predictions, and
their empirical distribution found in practice. We augment the variational
objective to mitigate such inconsistency and induce a chosen latent
distribution, instead of the implicit assumption found in a standard softmax
layer. Overall, we provide new theoretical insight into the inner workings of
widely-used softmax classifiers. Empirical evaluation on image and text
classification datasets demonstrates that our proposed approach, variational
classification, maintains classification accuracy while the reshaped latent
space improves other desirable properties of a classifier, such as calibration,
adversarial robustness, robustness to distribution shift and sample efficiency
useful in low data settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dhuliawala_S/0/1/0/all/0/1&quot;&gt;Shehzaad Dhuliawala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sachan_M/0/1/0/all/0/1&quot;&gt;Mrinmaya Sachan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allen_C/0/1/0/all/0/1&quot;&gt;Carl Allen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13035">
<title>Getting ViT in Shape: Scaling Laws for Compute-Optimal Model Design. (arXiv:2305.13035v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13035</link>
<description rdf:parseType="Literal">&lt;p&gt;Scaling laws have been recently employed to derive compute-optimal model size
(number of parameters) for a given compute duration. We advance and refine such
methods to infer compute-optimal model shapes, such as width and depth, and
successfully implement this in vision transformers. Our shape-optimized vision
transformer, SoViT, achieves results competitive with models that exceed twice
its size, despite being pre-trained with an equivalent amount of compute. For
example, SoViT-400m/14 achieves 90.3% fine-tuning accuracy on ILSRCV2012,
surpassing the much larger ViT-g/14 and approaching ViT-G/14 under identical
settings, with also less than half the inference cost. We conduct a thorough
evaluation across multiple tasks, such as image classification, captioning, VQA
and zero-shot transfer, demonstrating the effectiveness of our model across a
broad range of domains and identifying limitations. Overall, our findings
challenge the prevailing approach of blindly scaling up vision models and pave
a path for a more informed scaling.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alabdulmohsin_I/0/1/0/all/0/1&quot;&gt;Ibrahim Alabdulmohsin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_X/0/1/0/all/0/1&quot;&gt;Xiaohua Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolesnikov_A/0/1/0/all/0/1&quot;&gt;Alexander Kolesnikov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyer_L/0/1/0/all/0/1&quot;&gt;Lucas Beyer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.14093">
<title>Weakly Supervised 3D Open-vocabulary Segmentation. (arXiv:2305.14093v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.14093</link>
<description rdf:parseType="Literal">&lt;p&gt;Open-vocabulary segmentation of 3D scenes is a fundamental function of human
perception and thus a crucial objective in computer vision research. However,
this task is heavily impeded by the lack of large-scale and diverse 3D
open-vocabulary segmentation datasets for training robust and generalizable
models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation
models helps but it compromises the open-vocabulary feature as the 2D models
are mostly finetuned with close-vocabulary datasets. We tackle the challenges
in 3D open-vocabulary segmentation by exploiting pre-trained foundation models
CLIP and DINO in a weakly supervised manner. Specifically, given only the
open-vocabulary text descriptions of the objects in a scene, we distill the
open-vocabulary multimodal knowledge and object reasoning capability of CLIP
and DINO into a neural radiance field (NeRF), which effectively lifts 2D
features into view-consistent 3D segmentation. A notable aspect of our approach
is that it does not require any manual segmentation annotations for either the
foundation models or the distillation process. Extensive experiments show that
our method even outperforms fully supervised models trained with segmentation
annotations in certain scenes, suggesting that 3D open-vocabulary segmentation
can be effectively learned from 2D images and text-image pairs. Code is
available at \url{https://github.com/Kunhao-Liu/3D-OVS}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_K/0/1/0/all/0/1&quot;&gt;Kunhao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhan_F/0/1/0/all/0/1&quot;&gt;Fangneng Zhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jiahui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Muyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yingchen Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saddik_A/0/1/0/all/0/1&quot;&gt;Abdulmotaleb El Saddik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_E/0/1/0/all/0/1&quot;&gt;Eric Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shijian Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16645">
<title>Summarizing Stream Data for Memory-Constrained Online Continual Learning. (arXiv:2305.16645v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16645</link>
<description rdf:parseType="Literal">&lt;p&gt;Replay-based methods have proved their effectiveness on online continual
learning by rehearsing past samples from an auxiliary memory. With many efforts
made on improving training schemes based on the memory, however, the
information carried by each sample in the memory remains under-investigated.
Under circumstances with restricted storage space, the informativeness of the
memory becomes critical for effective replay. Although some works design
specific strategies to select representative samples, by only employing a small
number of original images, the storage space is still not well utilized. To
this end, we propose to Summarize the knowledge from the Stream Data (SSD) into
more informative samples by distilling the training characteristics of real
images. Through maintaining the consistency of training gradients and
relationship to the past tasks, the summarized samples are more representative
for the stream data compared to the original images. Extensive experiments are
conducted on multiple online continual learning benchmarks to support that the
proposed SSD method significantly enhances the replay effects. We demonstrate
that with limited extra computational overhead, SSD provides more than 3%
accuracy boost for sequential CIFAR-100 under extremely restricted memory
buffer. Code in https://github.com/vimar-gu/SSD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jianyang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_W/0/1/0/all/0/1&quot;&gt;Wei Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_Y/0/1/0/all/0/1&quot;&gt;Yang You&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.16713">
<title>ReConPatch : Contrastive Patch Representation Learning for Industrial Anomaly Detection. (arXiv:2305.16713v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.16713</link>
<description rdf:parseType="Literal">&lt;p&gt;Anomaly detection is crucial to the advanced identification of product
defects such as incorrect parts, misaligned components, and damages in
industrial manufacturing. Due to the rare observations and unknown types of
defects, anomaly detection is considered to be challenging in machine learning.
To overcome this difficulty, recent approaches utilize the common visual
representations pre-trained from natural image datasets and distill the
relevant features. However, existing approaches still have the discrepancy
between the pre-trained feature and the target data, or require the input
augmentation which should be carefully designed, particularly for the
industrial dataset. In this paper, we introduce ReConPatch, which constructs
discriminative features for anomaly detection by training a linear modulation
of patch features extracted from the pre-trained model. ReConPatch employs
contrastive representation learning to collect and distribute features in a way
that produces a target-oriented and easily separable representation. To address
the absence of labeled pairs for the contrastive learning, we utilize two
similarity measures between data representations, pairwise and contextual
similarities, as pseudo-labels. Our method achieves the state-of-the-art
anomaly detection performance (99.72%) for the widely used and challenging
MVTec AD dataset. Additionally, we achieved a state-of-the-art anomaly
detection performance (95.8%) for the BTAD dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hyun_J/0/1/0/all/0/1&quot;&gt;Jeeho Hyun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sangyun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_G/0/1/0/all/0/1&quot;&gt;Giyoung Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Seung Hwan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_K/0/1/0/all/0/1&quot;&gt;Kyunghoon Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_B/0/1/0/all/0/1&quot;&gt;Byung Jun Kang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16741">
<title>Foundation Model for Endoscopy Video Analysis via Large-scale Self-supervised Pre-train. (arXiv:2306.16741v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16741</link>
<description rdf:parseType="Literal">&lt;p&gt;Foundation models have exhibited remarkable success in various applications,
such as disease diagnosis and text report generation. To date, a foundation
model for endoscopic video analysis is still lacking. In this paper, we propose
Endo-FM, a foundation model specifically developed using massive endoscopic
video data. First, we build a video transformer, which captures both local and
global long-range dependencies across spatial and temporal dimensions. Second,
we pre-train our transformer model using global and local views via a
self-supervised manner, aiming to make it robust to spatial-temporal variations
and discriminative across different scenes. To develop the foundation model, we
construct a large-scale endoscopy video dataset by combining 9 publicly
available datasets and a privately collected dataset from Baoshan Branch of
Renji Hospital in Shanghai, China. Our dataset overall consists of over 33K
video clips with up to 5 million frames, encompassing various protocols, target
organs, and disease types. Our pre-trained Endo-FM can be easily adopted for a
given downstream task via fine-tuning by serving as the backbone. With
experiments on 3 different types of downstream tasks, including classification,
segmentation, and detection, our Endo-FM surpasses the current state-of-the-art
(SOTA) self-supervised pre-training and adapter-based transfer learning methods
by a significant margin, such as VCL (3.1% F1, 4.8% Dice, and 5.5% F1 for
classification, segmentation, and detection) and ST-Adapter (5.9% F1, 9.6%
Dice, and 9.9% F1 for classification, segmentation, and detection). Code,
datasets, and models are released at https://github.com/med-air/Endo-FM.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaoting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dou_Q/0/1/0/all/0/1&quot;&gt;Qi Dou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03992">
<title>Stimulating the Diffusion Model for Image Denoising via Adaptive Embedding and Ensembling. (arXiv:2307.03992v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03992</link>
<description rdf:parseType="Literal">&lt;p&gt;Image denoising is a fundamental problem in computational photography, where
achieving high-quality perceptual performance with low distortion is highly
demanding. Current methods either struggle with perceptual performance or
suffer from significant distortion. Recently, the emerging diffusion model
achieves state-of-the-art performance in various tasks, and its denoising
mechanism demonstrates great potential for image denoising. However,
stimulating diffusion models for image denoising is not straightforward and
requires solving several critical problems. On the one hand, the input
inconsistency hinders the connection of diffusion models and image denoising.
On the other hand, the content inconsistency between the generated image and
the desired denoised image introduces additional distortion. To tackle these
problems, we present a novel strategy called Diffusion Model for Image
Denoising (DMID) by understanding and rethinking the diffusion model from a
denoising perspective. Our DMID strategy includes an adaptive embedding method
that embeds the noisy image into a pre-trained diffusion model, and an adaptive
ensembling method that reduces distortion in the denoised image. Our DMID
strategy achieves state-of-the-art performance on all distortion-based and
perceptual metrics, for both Gaussian and real-world image denoising.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_H/0/1/0/all/0/1&quot;&gt;Hansen Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lizhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hua Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14277">
<title>G2L: Semantically Aligned and Uniform Video Grounding via Geodesic and Game Theory. (arXiv:2307.14277v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14277</link>
<description rdf:parseType="Literal">&lt;p&gt;The recent video grounding works attempt to introduce vanilla contrastive
learning into video grounding. However, we claim that this naive solution is
suboptimal. Contrastive learning requires two key properties: (1)
\emph{alignment} of features of similar samples, and (2) \emph{uniformity} of
the induced distribution of the normalized features on the hypersphere. Due to
two annoying issues in video grounding: (1) the co-existence of some visual
entities in both ground truth and other moments, \ie semantic overlapping; (2)
only a few moments in the video are annotated, \ie sparse annotation dilemma,
vanilla contrastive learning is unable to model the correlations between
temporally distant moments and learned inconsistent video representations. Both
characteristics lead to vanilla contrastive learning being unsuitable for video
grounding. In this paper, we introduce Geodesic and Game Localization (G2L), a
semantically aligned and uniform video grounding framework via geodesic and
game theory. We quantify the correlations among moments leveraging the geodesic
distance that guides the model to learn the correct cross-modal
representations. Furthermore, from the novel perspective of game theory, we
propose semantic Shapley interaction based on geodesic distance sampling to
learn fine-grained semantic alignment in similar moments. Experiments on three
benchmarks demonstrate the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongxiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_M/0/1/0/all/0/1&quot;&gt;Meng Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_X/0/1/0/all/0/1&quot;&gt;Xuxin Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yaowei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihong Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Y/0/1/0/all/0/1&quot;&gt;Yuexian Zou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.06143">
<title>Improving Generalization Capability of Deep Learning-Based Nuclei Instance Segmentation by Non-deterministic Train Time and Deterministic Test Time Stain Normalization. (arXiv:2309.06143v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.06143</link>
<description rdf:parseType="Literal">&lt;p&gt;With the advent of digital pathology and microscopic systems that can scan
and save whole slide histological images automatically, there is a growing
trend to use computerized methods to analyze acquired images. Among different
histopathological image analysis tasks, nuclei instance segmentation plays a
fundamental role in a wide range of clinical and research applications. While
many semi- and fully-automatic computerized methods have been proposed for
nuclei instance segmentation, deep learning (DL)-based approaches have been
shown to deliver the best performances. However, the performance of such
approaches usually degrades when tested on unseen datasets.
&lt;/p&gt;
&lt;p&gt;In this work, we propose a novel method to improve the generalization
capability of a DL-based automatic segmentation approach. Besides utilizing one
of the state-of-the-art DL-based models as a baseline, our method incorporates
non-deterministic train time and deterministic test time stain normalization,
and ensembling to boost the segmentation performance. We trained the model with
one single training set and evaluated its segmentation performance on seven
test datasets. Our results show that the proposed method provides up to 4.9%,
5.4%, and 5.9% better average performance in segmenting nuclei based on Dice
score, aggregated Jaccard index, and panoptic quality score, respectively,
compared to the baseline segmentation model.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mahbod_A/0/1/0/all/0/1&quot;&gt;Amirreza Mahbod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dorffner_G/0/1/0/all/0/1&quot;&gt;Georg Dorffner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ellinger_I/0/1/0/all/0/1&quot;&gt;Isabella Ellinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Woitek_R/0/1/0/all/0/1&quot;&gt;Ramona Woitek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hatamikia_S/0/1/0/all/0/1&quot;&gt;Sepideh Hatamikia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.07254">
<title>Mitigate Replication and Copying in Diffusion Models with Generalized Caption and Dual Fusion Enhancement. (arXiv:2309.07254v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2309.07254</link>
<description rdf:parseType="Literal">&lt;p&gt;While diffusion models demonstrate a remarkable capability for generating
high-quality images, their tendency to `replicate&apos; training data raises privacy
concerns. Although recent research suggests that this replication may stem from
the insufficient generalization of training data captions and duplication of
training images, effective mitigation strategies remain elusive. To address
this gap, our paper first introduces a generality score that measures the
caption generality and employ large language model (LLM) to generalize training
captions. Subsequently, we leverage generalized captions and propose a novel
dual fusion enhancement approach to mitigate the replication of diffusion
models. Our empirical results demonstrate that our proposed methods can
significantly reduce replication by 43.5% compared to the original diffusion
model while maintaining the diversity and quality of generations. Code is
available at https://github.com/HowardLi0816/dual-fusion-diffusion.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenghao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dake Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuke Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beerel_P/0/1/0/all/0/1&quot;&gt;Peter A. Beerel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.15216">
<title>A Comparative Study of Filters and Deep Learning Models to predict Diabetic Retinopathy. (arXiv:2309.15216v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2309.15216</link>
<description rdf:parseType="Literal">&lt;p&gt;The retina is an essential component of the visual system, and maintaining
eyesight depends on the timely and accurate detection of disorders. The
early-stage detection and severity classification of Diabetic Retinopathy (DR),
a significant risk to the public&apos;s health is the primary goal of this work.
This study compares the outcomes of various deep learning models, including
InceptionNetV3, DenseNet121, and other CNN-based models, utilizing a variety of
image filters, including Gaussian, grayscale, and Gabor. These models could
detect subtle pathological alterations and use that information to estimate the
risk of retinal illnesses. The objective is to improve the diagnostic processes
for DR, the primary cause of diabetes-related blindness, by utilizing deep
learning models. A comparative analysis between Greyscale, Gaussian and Gabor
filters has been provided after applying these filters on the retinal images.
The Gaussian filter has been identified as the most promising filter by
resulting in 96% accuracy using InceptionNetV3.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muddaluru_R/0/1/0/all/0/1&quot;&gt;Roshan Vasu Muddaluru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoguluva_S/0/1/0/all/0/1&quot;&gt;Sharvaani Ravikumar Thoguluva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prabha_S/0/1/0/all/0/1&quot;&gt;Shruti Prabha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Reddy_T/0/1/0/all/0/1&quot;&gt;Tanuja Konda Reddy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palaniswamy_D/0/1/0/all/0/1&quot;&gt;Dr. Suja Palaniswamy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11204">
<title>Improving Video Deepfake Detection: A DCT-Based Approach with Patch-Level Analysis. (arXiv:2310.11204v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11204</link>
<description rdf:parseType="Literal">&lt;p&gt;A new algorithm for the detection of deepfakes in digital videos is
presented. The I-frames were extracted in order to provide faster computation
and analysis than approaches described in the literature. To identify the
discriminating regions within individual video frames, the entire frame,
background, face, eyes, nose, mouth, and face frame were analyzed separately.
From the Discrete Cosine Transform (DCT), the Beta components were extracted
from the AC coefficients and used as input to standard classifiers.
Experimental results show that the eye and mouth regions are those most
discriminative and able to determine the nature of the video under analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guarnera_L/0/1/0/all/0/1&quot;&gt;Luca Guarnera&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manganello_S/0/1/0/all/0/1&quot;&gt;Salvatore Manganello&lt;/a&gt; (1), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Battiato_S/0/1/0/all/0/1&quot;&gt;Sebastiano Battiato&lt;/a&gt; (1) ((1) University of Catania)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.11650">
<title>VKIE: The Application of Key Information Extraction on Video Text. (arXiv:2310.11650v2 [cs.IR] UPDATED)</title>
<link>http://arxiv.org/abs/2310.11650</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting structured information from videos is critical for numerous
downstream applications in the industry. In this paper, we define a significant
task of extracting hierarchical key information from visual texts on videos. To
fulfill this task, we decouple it into four subtasks and introduce two
implementation solutions called PipVKIE and UniVKIE. PipVKIE sequentially
completes the four subtasks in continuous stages, while UniVKIE is improved by
unifying all the subtasks into one backbone. Both PipVKIE and UniVKIE leverage
multimodal information from vision, text, and coordinates for feature
representation. Extensive experiments on one well-defined dataset demonstrate
that our solutions can achieve remarkable performance and efficient inference
speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+An_S/0/1/0/all/0/1&quot;&gt;Siyu An&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_H/0/1/0/all/0/1&quot;&gt;Haoyuan Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_D/0/1/0/all/0/1&quot;&gt;Di Yin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2310.17974">
<title>FaultSeg Swin-UNETR: Transformer-Based Self-Supervised Pretraining Model for Fault Recognition. (arXiv:2310.17974v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2310.17974</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper introduces an approach to enhance seismic fault recognition
through self-supervised pretraining. Seismic fault interpretation holds great
significance in the fields of geophysics and geology. However, conventional
methods for seismic fault recognition encounter various issues, including
dependence on data quality and quantity, as well as susceptibility to
interpreter subjectivity. Currently, automated fault recognition methods
proposed based on small synthetic datasets experience performance degradation
when applied to actual seismic data. To address these challenges, we have
introduced the concept of self-supervised learning, utilizing a substantial
amount of relatively easily obtainable unlabeled seismic data for pretraining.
Specifically, we have employed the Swin Transformer model as the core network
and employed the SimMIM pretraining task to capture unique features related to
discontinuities in seismic data. During the fine-tuning phase, inspired by edge
detection techniques, we have also refined the structure of the Swin-UNETR
model, enabling multiscale decoding and fusion for more effective fault
detection. Experimental results demonstrate that our proposed method attains
state-of-the-art performance on the Thebe dataset, as measured by the OIS and
ODS metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeren Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jinwen Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.02960">
<title>Understanding Deep Representation Learning via Layerwise Feature Compression and Discrimination. (arXiv:2311.02960v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2311.02960</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decade, deep learning has proven to be a highly effective tool
for learning meaningful features from raw data. However, it remains an open
question how deep networks perform hierarchical feature learning across layers.
In this work, we attempt to unveil this mystery by investigating the structures
of intermediate features. Motivated by our empirical findings that linear
layers mimic the roles of deep layers in nonlinear networks for feature
learning, we explore how deep linear networks transform input data into output
by investigating the output (i.e., features) of each layer after training in
the context of multi-class classification problems. Toward this goal, we first
define metrics to measure within-class compression and between-class
discrimination of intermediate features, respectively. Through theoretical
analysis of these two metrics, we show that the evolution of features follows a
simple and quantitative pattern from shallow to deep layers when the input data
is nearly orthogonal and the network weights are minimum-norm, balanced, and
approximate low-rank: Each layer of the linear network progressively compresses
within-class features at a geometric rate and discriminates between-class
features at a linear rate with respect to the number of layers that data have
passed through. To the best of our knowledge, this is the first quantitative
characterization of feature evolution in hierarchical representations of deep
linear networks. Empirically, our extensive experiments not only validate our
theoretical results numerically but also reveal a similar pattern in deep
nonlinear networks which aligns well with recent empirical studies. Moreover,
we demonstrate the practical implications of our results in transfer learning.
Our code is available at \url{https://github.com/Heimine/PNC_DLN}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yaras_C/0/1/0/all/0/1&quot;&gt;Can Yaras&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zhihui Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balzano_L/0/1/0/all/0/1&quot;&gt;Laura Balzano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Wei Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Q/0/1/0/all/0/1&quot;&gt;Qing Qu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2311.13964">
<title>Deep Interactive Segmentation of Medical Images: A Systematic Review and Taxonomy. (arXiv:2311.13964v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2311.13964</link>
<description rdf:parseType="Literal">&lt;p&gt;Interactive segmentation is a crucial research area in medical image analysis
aiming to boost the efficiency of costly annotations by incorporating human
feedback. This feedback takes the form of clicks, scribbles, or masks and
allows for iterative refinement of the model output so as to efficiently guide
the system towards the desired behavior. In recent years, deep learning-based
approaches have propelled results to a new level causing a rapid growth in the
field with 121 methods proposed in the medical imaging domain alone. In this
review, we provide a structured overview of this emerging field featuring a
comprehensive taxonomy, a systematic review of existing methods, and an
in-depth analysis of current practices. Based on these contributions, we
discuss the challenges and opportunities in the field. For instance, we find
that there is a severe lack of comparison across methods which needs to be
tackled by standardized baselines and benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Marinov_Z/0/1/0/all/0/1&quot;&gt;Zdravko Marinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jager_P/0/1/0/all/0/1&quot;&gt;Paul F. J&amp;#xe4;ger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Egger_J/0/1/0/all/0/1&quot;&gt;Jan Egger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kleesiek_J/0/1/0/all/0/1&quot;&gt;Jens Kleesiek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stiefelhagen_R/0/1/0/all/0/1&quot;&gt;Rainer Stiefelhagen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.01964">
<title>Semantics-aware Motion Retargeting with Vision-Language Models. (arXiv:2312.01964v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.01964</link>
<description rdf:parseType="Literal">&lt;p&gt;Capturing and preserving motion semantics is essential to motion retargeting
between animation characters. However, most of the previous works neglect the
semantic information or rely on human-designed joint-level representations.
Here, we present a novel Semantics-aware Motion reTargeting (SMT) method with
the advantage of vision-language models to extract and maintain meaningful
motion semantics. We utilize a differentiable module to render 3D motions. Then
the high-level motion semantics are incorporated into the motion retargeting
process by feeding the vision-language model with the rendered images and
aligning the extracted semantic embeddings. To ensure the preservation of
fine-grained motion details and high-level semantics, we adopt a two-stage
pipeline consisting of skeleton-aware pre-training and fine-tuning with
semantics and geometry constraints. Experimental results show the effectiveness
of the proposed method in producing high-quality motion retargeting results
while accurately preserving motion semantics. Project page can be found at
https://sites.google.com/view/smtnet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haodong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;ZhiKe Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haocheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_L/0/1/0/all/0/1&quot;&gt;Lei Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xiaofei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhensong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_R/0/1/0/all/0/1&quot;&gt;Rong Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.04265">
<title>Stronger, Fewer, &amp; Superior: Harnessing Vision Foundation Models for Domain Generalized Semantic Segmentation. (arXiv:2312.04265v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.04265</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we first assess and harness various Vision Foundation Models
(VFMs) in the context of Domain Generalized Semantic Segmentation (DGSS).
Driven by the motivation that Leveraging Stronger pre-trained models and Fewer
trainable parameters for Superior generalizability, we introduce a robust
fine-tuning approach, namely Rein, to parameter-efficiently harness VFMs for
DGSS. Built upon a set of trainable tokens, each linked to distinct instances,
Rein precisely refines and forwards the feature maps from each layer to the
next layer within the backbone. This process produces diverse refinements for
different categories within a single image. With fewer trainable parameters,
Rein efficiently fine-tunes VFMs for DGSS tasks, surprisingly surpassing full
parameter fine-tuning. Extensive experiments across various settings
demonstrate that Rein significantly outperforms state-of-the-art methods.
Remarkably, with just an extra 1% of trainable parameters within the frozen
backbone, Rein achieves a mIoU of 68.1% on the Cityscapes, without accessing
any real urban-scene datasets.Code is available at
https://github.com/w1oves/Rein.git.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_T/0/1/0/all/0/1&quot;&gt;Tianle Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_P/0/1/0/all/0/1&quot;&gt;Pengyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Ben Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huaian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_J/0/1/0/all/0/1&quot;&gt;Jinjin Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.11535">
<title>Customize-It-3D: High-Quality 3D Creation from A Single Image Using Subject-Specific Knowledge Prior. (arXiv:2312.11535v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.11535</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel two-stage approach that fully utilizes the
information provided by the reference image to establish a customized knowledge
prior for image-to-3D generation. While previous approaches primarily rely on a
general diffusion prior, which struggles to yield consistent results with the
reference image, we propose a subject-specific and multi-modal diffusion model.
This model not only aids NeRF optimization by considering the shading mode for
improved geometry but also enhances texture from the coarse results to achieve
superior refinement. Both aspects contribute to faithfully aligning the 3D
content with the subject. Extensive experiments showcase the superiority of our
method, Customize-It-3D, outperforming previous works by a substantial margin.
It produces faithful 360-degree reconstructions with impressive visual quality,
making it well-suited for various applications, including text-to-3D creation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_N/0/1/0/all/0/1&quot;&gt;Nan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Ting Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shanghang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15144">
<title>Spatial-Temporal Decoupling Contrastive Learning for Skeleton-based Human Action Recognition. (arXiv:2312.15144v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15144</link>
<description rdf:parseType="Literal">&lt;p&gt;Skeleton-based action recognition is a central task of human-computer
interaction. However, most of the previous methods suffer from two issues: (i)
semantic ambiguity arising from spatiotemporal information mixture; and (ii)
overlooking the explicit exploitation of the latent data distributions (i.e.,
the intra-class variations and inter-class relations), thereby leading to local
optimum solutions of the skeleton encoders. To mitigate this, we propose a
spatial-temporal decoupling contrastive learning (STD-CL) framework to obtain
discriminative and semantically distinct representations from the sequences,
which can be incorporated into almost all previous skeleton encoders and have
no impact on the skeleton encoders when testing. Specifically, we decouple the
global features into spatial-specific and temporal-specific features to reduce
the spatiotemporal coupling of features. Furthermore, to explicitly exploit the
latent data distributions, we employ the attentive features to contrastive
learning, which models the cross-sequence semantic relations by pulling
together the features from the positive pairs and pushing away the negative
pairs. Extensive experiments show that STD-CL with four various skeleton
encoders (HCN, 2S-AGCN, CTR-GCN, and Hyperformer) achieves solid improvement on
NTU60, NTU120, and NW-UCLA benchmarks. The code will be released.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_S/0/1/0/all/0/1&quot;&gt;Shaojie Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_J/0/1/0/all/0/1&quot;&gt;Jianqin Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_Y/0/1/0/all/0/1&quot;&gt;Yonghao Dang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2312.15731">
<title>Adaptive FSS: A Novel Few-Shot Segmentation Framework via Prototype Enhancement. (arXiv:2312.15731v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2312.15731</link>
<description rdf:parseType="Literal">&lt;p&gt;The Few-Shot Segmentation (FSS) aims to accomplish the novel class
segmentation task with a few annotated images. Current FSS research based on
meta-learning focus on designing a complex interaction mechanism between the
query and support feature. However, unlike humans who can rapidly learn new
things from limited samples, the existing approach relies solely on fixed
feature matching to tackle new tasks, lacking adaptability. In this paper, we
propose a novel framework based on the adapter mechanism, namely Adaptive FSS,
which can efficiently adapt the existing FSS model to the novel classes. In
detail, we design the Prototype Adaptive Module (PAM), which utilizes accurate
category information provided by the support set to derive class prototypes,
enhancing class-specific information in the multi-stage representation. In
addition, our approach is compatible with diverse FSS methods with different
backbones by simply inserting PAM between the layers of the encoder.
Experiments demonstrate that our method effectively improves the performance of
the FSS models (e.g., MSANet, HDMNet, FPTrans, and DCAMA) and achieve new
state-of-the-art (SOTA) results (i.e., 72.4\% and 79.1\% mIoU on PASCAL-5$^i$
1-shot and 5-shot settings, 52.7\% and 60.0\% mIoU on COCO-20$^i$ 1-shot and
5-shot settings). Our code can be available at
https://github.com/jingw193/AdaptiveFSS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinagyun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yisi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_H/0/1/0/all/0/1&quot;&gt;Haoran Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_T/0/1/0/all/0/1&quot;&gt;Tianxiang Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00155">
<title>A comprehensive framework for occluded human pose estimation. (arXiv:2401.00155v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00155</link>
<description rdf:parseType="Literal">&lt;p&gt;Occlusion presents a significant challenge in human pose estimation. The
challenges posed by occlusion can be attributed to the following factors: 1)
Data: The collection and annotation of occluded human pose samples are
relatively challenging. 2) Feature: Occlusion can cause feature confusion due
to the high similarity between the target person and interfering individuals.
3) Inference: Robust inference becomes challenging due to the loss of complete
body structural information. The existing methods designed for occluded human
pose estimation usually focus on addressing only one of these factors. In this
paper, we propose a comprehensive framework DAG (Data, Attention, Graph) to
address the performance degradation caused by occlusion. Specifically, we
introduce the mask joints with instance paste data augmentation technique to
simulate occlusion scenarios. Additionally, an Adaptive Discriminative
Attention Module (ADAM) is proposed to effectively enhance the features of
target individuals. Furthermore, we present the Feature-Guided Multi-Hop GCN
(FGMP-GCN) to fully explore the prior knowledge of body structure and improve
pose estimation results. Through extensive experiments conducted on three
benchmark datasets for occluded human pose estimation, we demonstrate that the
proposed method outperforms existing methods. Code and data will be publicly
available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linhao Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_L/0/1/0/all/0/1&quot;&gt;Lin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_X/0/1/0/all/0/1&quot;&gt;Xinxin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Di Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guangyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Kedong Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.00897">
<title>Masked Modeling for Self-supervised Representation Learning on Vision and Beyond. (arXiv:2401.00897v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.00897</link>
<description rdf:parseType="Literal">&lt;p&gt;As the deep learning revolution marches on, self-supervised learning has
garnered increasing attention in recent years thanks to its remarkable
representation learning ability and the low dependence on labeled data. Among
these varied self-supervised techniques, masked modeling has emerged as a
distinctive approach that involves predicting parts of the original data that
are proportionally masked during training. This paradigm enables deep models to
learn robust representations and has demonstrated exceptional performance in
the context of computer vision, natural language processing, and other
modalities. In this survey, we present a comprehensive review of the masked
modeling framework and its methodology. We elaborate on the details of
techniques within masked modeling, including diverse masking strategies,
recovering targets, network architectures, and more. Then, we systematically
investigate its wide-ranging applications across domains. Furthermore, we also
explore the commonalities and differences between masked modeling methods in
different fields. Toward the end of this paper, we conclude by discussing the
limitations of current techniques and point out several potential avenues for
advancing masked modeling research. A paper list project with this survey is
available at \url{https://github.com/Lupin1998/Awesome-MIM}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Siyuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Luyuan Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zedong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_D/0/1/0/all/0/1&quot;&gt;Di Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_L/0/1/0/all/0/1&quot;&gt;Lirong Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zicheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_J/0/1/0/all/0/1&quot;&gt;Jun Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_C/0/1/0/all/0/1&quot;&gt;Cheng Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_B/0/1/0/all/0/1&quot;&gt;Baigui Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Stan Z. Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01286">
<title>A Comprehensive Study of Knowledge Editing for Large Language Models. (arXiv:2401.01286v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01286</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown extraordinary capabilities in
understanding and generating text that closely mirrors human communication.
However, a primary limitation lies in the significant computational demands
during training, arising from their extensive parameterization. This challenge
is further intensified by the dynamic nature of the world, necessitating
frequent updates to LLMs to correct outdated information or integrate new
knowledge, thereby ensuring their continued relevance. Note that many
applications demand continual model adjustments post-training to address
deficiencies or undesirable behaviors. There is an increasing interest in
efficient, lightweight methods for on-the-fly model modifications. To this end,
recent years have seen a burgeoning in the techniques of knowledge editing for
LLMs, which aim to efficiently modify LLMs&apos; behaviors within specific domains
while preserving overall performance across various inputs. In this paper, we
first define the knowledge editing problem and then provide a comprehensive
review of cutting-edge approaches. Drawing inspiration from educational and
cognitive research theories, we propose a unified categorization criterion that
classifies knowledge editing methods into three groups: resorting to external
knowledge, merging knowledge into the model, and editing intrinsic knowledge.
Furthermore, we introduce a new benchmark, KnowEdit, for a comprehensive
empirical evaluation of representative knowledge editing approaches.
Additionally, we provide an in-depth analysis of knowledge location, which can
give a deeper understanding of the knowledge structures inherent within LLMs.
Finally, we discuss several potential applications of knowledge editing,
outlining its broad and impactful implications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_N/0/1/0/all/0/1&quot;&gt;Ningyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yunzhi Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_B/0/1/0/all/0/1&quot;&gt;Bozhong Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Shumin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_M/0/1/0/all/0/1&quot;&gt;Mengru Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_Z/0/1/0/all/0/1&quot;&gt;Zekun Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_S/0/1/0/all/0/1&quot;&gt;Shengyu Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jintian Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_Y/0/1/0/all/0/1&quot;&gt;Yuansheng Ni&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Siyuan Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Z/0/1/0/all/0/1&quot;&gt;Ziwen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xin Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jia-Chen Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yong Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_P/0/1/0/all/0/1&quot;&gt;Pengjun Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_F/0/1/0/all/0/1&quot;&gt;Fei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_L/0/1/0/all/0/1&quot;&gt;Lei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_X/0/1/0/all/0/1&quot;&gt;Xiaowei Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huajun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.01990">
<title>GPS-SSL: Guided Positive Sampling to Inject Prior Into Self-Supervised Learning. (arXiv:2401.01990v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.01990</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose Guided Positive Sampling Self-Supervised Learning (GPS-SSL), a
general method to inject a priori knowledge into Self-Supervised Learning (SSL)
positive samples selection. Current SSL methods leverage Data-Augmentations
(DA) for generating positive samples and incorporate prior knowledge - an
incorrect, or too weak DA will drastically reduce the quality of the learned
representation. GPS-SSL proposes instead to design a metric space where
Euclidean distances become a meaningful proxy for semantic relationship. In
that space, it is now possible to generate positive samples from nearest
neighbor sampling. Any prior knowledge can now be embedded into that metric
space independently from the employed DA. From its simplicity, GPS-SSL is
applicable to any SSL method, e.g. SimCLR or BYOL. A key benefit of GPS-SSL is
in reducing the pressure in tailoring strong DAs. For example GPS-SSL reaches
85.58% on Cifar10 with weak DA while the baseline only reaches 37.51%. We
therefore move a step forward towards the goal of making SSL less reliant on
DA. We also show that even when using strong DAs, GPS-SSL outperforms the
baselines on under-studied domains. We evaluate GPS-SSL along with multiple
baseline SSL methods on numerous downstream datasets from different domains
when the models use strong or minimal data augmentations. We hope that GPS-SSL
will open new avenues in studying how to inject a priori knowledge into SSL in
a principled manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feizi_A/0/1/0/all/0/1&quot;&gt;Aarash Feizi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Balestriero_R/0/1/0/all/0/1&quot;&gt;Randall Balestriero&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Romero_Soriano_A/0/1/0/all/0/1&quot;&gt;Adriana Romero-Soriano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rabbany_R/0/1/0/all/0/1&quot;&gt;Reihaneh Rabbany&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02032">
<title>DiffusionEdge: Diffusion Probabilistic Model for Crisp Edge Detection. (arXiv:2401.02032v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02032</link>
<description rdf:parseType="Literal">&lt;p&gt;Limited by the encoder-decoder architecture, learning-based edge detectors
usually have difficulty predicting edge maps that satisfy both correctness and
crispness. With the recent success of the diffusion probabilistic model (DPM),
we found it is especially suitable for accurate and crisp edge detection since
the denoising process is directly applied to the original image size.
Therefore, we propose the first diffusion model for the task of general edge
detection, which we call DiffusionEdge. To avoid expensive computational
resources while retaining the final performance, we apply DPM in the latent
space and enable the classic cross-entropy loss which is uncertainty-aware in
pixel level to directly optimize the parameters in latent space in a
distillation manner. We also adopt a decoupled architecture to speed up the
denoising process and propose a corresponding adaptive Fourier filter to adjust
the latent features of specific frequencies. With all the technical designs,
DiffusionEdge can be stably trained with limited resources, predicting crisp
and accurate edge maps with much fewer augmentation strategies. Extensive
experiments on four edge detection benchmarks demonstrate the superiority of
DiffusionEdge both in correctness and crispness. On the NYUDv2 dataset,
compared to the second best, we increase the ODS, OIS (without post-processing)
and AC by 30.2%, 28.1% and 65.1%, respectively. Code:
https://github.com/GuHuangAI/DiffusionEdge.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Y/0/1/0/all/0/1&quot;&gt;Yunfan Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_K/0/1/0/all/0/1&quot;&gt;Kai Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yuhang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yi_R/0/1/0/all/0/1&quot;&gt;Renjiao Yi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhiping Cai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.02791">
<title>Weakly Semi-supervised Tool Detection in Minimally Invasive Surgery Videos. (arXiv:2401.02791v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.02791</link>
<description rdf:parseType="Literal">&lt;p&gt;Surgical tool detection is essential for analyzing and evaluating minimally
invasive surgery videos. Current approaches are mostly based on supervised
methods that require large, fully instance-level labels (i.e., bounding boxes).
However, large image datasets with instance-level labels are often limited
because of the burden of annotation. Thus, surgical tool detection is important
when providing image-level labels instead of instance-level labels since
image-level annotations are considerably more time-efficient than
instance-level annotations. In this work, we propose to strike a balance
between the extremely costly annotation burden and detection performance. We
further propose a co-occurrence loss, which considers a characteristic that
some tool pairs often co-occur together in an image to leverage image-level
labels. Encapsulating the knowledge of co-occurrence using the co-occurrence
loss helps to overcome the difficulty in classification that originates from
the fact that some tools have similar shapes and textures. Extensive
experiments conducted on the Endovis2018 dataset in various data settings show
the effectiveness of our method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujii_R/0/1/0/all/0/1&quot;&gt;Ryo Fujii&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hachiuma_R/0/1/0/all/0/1&quot;&gt;Ryo Hachiuma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saito_H/0/1/0/all/0/1&quot;&gt;Hideo Saito&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03800">
<title>MvKSR: Multi-view Knowledge-guided Scene Recovery for Hazy and Rainy Degradation. (arXiv:2401.03800v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03800</link>
<description rdf:parseType="Literal">&lt;p&gt;High-quality imaging is crucial for ensuring safety supervision and
intelligent deployment in fields like transportation and industry. It enables
precise and detailed monitoring of operations, facilitating timely detection of
potential hazards and efficient management. However, adverse weather
conditions, such as atmospheric haziness and precipitation, can have a
significant impact on image quality. When the atmosphere contains dense haze or
water droplets, the incident light scatters, leading to degraded captured
images. This degradation is evident in the form of image blur and reduced
contrast, increasing the likelihood of incorrect assessments and
interpretations by intelligent imaging systems (IIS). To address the challenge
of restoring degraded images in hazy and rainy conditions, this paper proposes
a novel multi-view knowledge-guided scene recovery network (termed MvKSR).
Specifically, guided filtering is performed on the degraded image to separate
high/low-frequency components. Subsequently, an en-decoder-based multi-view
feature coarse extraction module (MCE) is used to coarsely extract features
from different views of the degraded image. The multi-view feature fine fusion
module (MFF) will learn and infer the restoration of degraded images through
mixed supervision under different views. Additionally, we suggest an atrous
residual block to handle global restoration and local repair in
hazy/rainy/mixed scenes. Extensive experimental results demonstrate that MvKSR
outperforms other state-of-the-art methods in terms of efficiency and stability
for restoring degraded scenarios in IIS.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_D/0/1/0/all/0/1&quot;&gt;Dong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Wenyu Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yuxu Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yu Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03836">
<title>WidthFormer: Toward Efficient Transformer-based BEV View Transformation. (arXiv:2401.03836v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03836</link>
<description rdf:parseType="Literal">&lt;p&gt;In this work, we present WidthFormer, a novel transformer-based
Bird&apos;s-Eye-View (BEV) 3D detection method tailored for real-time
autonomous-driving applications. WidthFormer is computationally efficient,
robust and does not require any special engineering effort to deploy. In this
work, we propose a novel 3D positional encoding mechanism capable of accurately
encapsulating 3D geometric information, which enables our model to generate
high-quality BEV representations with only a single transformer decoder layer.
This mechanism is also beneficial for existing sparse 3D object detectors.
Inspired by the recently-proposed works, we further improve our model&apos;s
efficiency by vertically compressing the image features when serving as
attention keys and values. We also introduce two modules to compensate for
potential information loss due to feature compression. Experimental evaluation
on the widely-used nuScenes 3D object detection benchmark demonstrates that our
method outperforms previous approaches across different 3D detection
architectures. More importantly, our model is highly efficient. For example,
when using $256\times 704$ input images, it achieves 1.5 ms and 2.8 ms latency
on NVIDIA 3090 GPU and Horizon Journey-5 edge computing chips, respectively.
Furthermore, WidthFormer also exhibits strong robustness to different degrees
of camera perturbations. Our study offers valuable insights into the deployment
of BEV transformation methods in real-world, complex road environments. Code is
available at https://github.com/ChenhongyiYang/WidthFormer .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chenhongyi Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_T/0/1/0/all/0/1&quot;&gt;Tianwei Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lichao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crowley_E/0/1/0/all/0/1&quot;&gt;Elliot J. Crowley&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2401.03885">
<title>Hyperspectral Image Denoising via Spatial-Spectral Recurrent Transformer. (arXiv:2401.03885v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2401.03885</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral images (HSIs) often suffer from noise arising from both
intra-imaging mechanisms and environmental factors. Leveraging domain knowledge
specific to HSIs, such as global spectral correlation (GSC) and non-local
spatial self-similarity (NSS), is crucial for effective denoising. Existing
methods tend to independently utilize each of these knowledge components with
multiple blocks, overlooking the inherent 3D nature of HSIs where domain
knowledge is strongly interlinked, resulting in suboptimal performance. To
address this challenge, this paper introduces a spatial-spectral recurrent
transformer U-Net (SSRT-UNet) for HSI denoising. The proposed SSRT-UNet
integrates NSS and GSC properties within a single SSRT block. This block
consists of a spatial branch and a spectral branch. The spectral branch employs
a combination of transformer and recurrent neural network to perform recurrent
computations across bands, allowing for GSC exploitation beyond a fixed number
of bands. Concurrently, the spatial branch encodes NSS for each band by sharing
keys and values with the spectral branch under the guidance of GSC. This
interaction between the two branches enables the joint utilization of NSS and
GSC, avoiding their independent treatment. Experimental results demonstrate
that our method outperforms several alternative approaches. The source code
will be available at https://github.com/lronkitty/SSRT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fu_G/0/1/0/all/0/1&quot;&gt;Guanyiman Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xiong_F/0/1/0/all/0/1&quot;&gt;Fengchao Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jianfeng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jun Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiantao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qian_Y/0/1/0/all/0/1&quot;&gt;Yuntao Qian&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>