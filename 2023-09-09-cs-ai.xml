<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.AI updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Artificial Intelligence (cs.AI) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-09-07T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Artificial Intelligence</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03208" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03209" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03212" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03213" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03215" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03217" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03219" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03222" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03224" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03227" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03229" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03234" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03239" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03241" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03242" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03251" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03295" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03318" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03322" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03367" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03386" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03387" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03404" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03409" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03450" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03453" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03466" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03467" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03468" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03469" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03506" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03517" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03523" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03549" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03579" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03581" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03590" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03613" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03617" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03638" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03645" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03648" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03651" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03652" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03665" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03671" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03685" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03720" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03773" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03799" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03800" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03812" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03824" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03831" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03852" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03876" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03883" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.03893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2010.10274" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2206.02231" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.01708" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.03680" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16193" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.17040" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.05798" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.04838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.14051" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.11899" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13592" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.17780" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.00526" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13596" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05766" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08461" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09882" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.14971" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00868" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.00904" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.02335" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11432" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.11764" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.13280" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15116" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.15225" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16137" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2308.16898" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00237" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.00646" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02553" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02589" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2309.02685" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2309.03208">
<title>A Circuit Domain Generalization Framework for Efficient Logic Synthesis in Chip Design. (arXiv:2309.03208v1 [cs.AR])</title>
<link>http://arxiv.org/abs/2309.03208</link>
<description rdf:parseType="Literal">&lt;p&gt;Logic Synthesis (LS) plays a vital role in chip design -- a cornerstone of
the semiconductor industry. A key task in LS is to transform circuits --
modeled by directed acyclic graphs (DAGs) -- into simplified circuits with
equivalent functionalities. To tackle this task, many LS operators apply
transformations to subgraphs -- rooted at each node on an input DAG --
sequentially. However, we found that a large number of transformations are
ineffective, which makes applying these operators highly time-consuming. In
particular, we notice that the runtime of the Resub and Mfs2 operators often
dominates the overall runtime of LS optimization processes. To address this
challenge, we propose a novel data-driven LS operator paradigm, namely PruneX,
to reduce ineffective transformations. The major challenge of developing PruneX
is to learn models that well generalize to unseen circuits, i.e., the
out-of-distribution (OOD) generalization problem. Thus, the major technical
contribution of PruneX is the novel circuit domain generalization framework,
which learns domain-invariant representations based on the
transformation-invariant domain-knowledge. To the best of our knowledge, PruneX
is the first approach to tackle the OOD problem in LS operators. We integrate
PruneX with the aforementioned Resub and Mfs2 operators. Experiments
demonstrate that PruneX significantly improves their efficiency while keeping
comparable optimization performance on industrial and very large-scale
circuits, achieving up to $3.1\times$ faster runtime.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhihai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jie Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_Y/0/1/0/all/0/1&quot;&gt;Yinqi Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xijun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_M/0/1/0/all/0/1&quot;&gt;Mingxuan Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_J/0/1/0/all/0/1&quot;&gt;Jianye Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yongdong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Feng Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03209">
<title>A Human-Machine Joint Learning Framework to Boost Endogenous BCI Training. (arXiv:2309.03209v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.03209</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain-computer interfaces (BCIs) provide a direct pathway from the brain to
external devices and have demonstrated great potential for assistive and
rehabilitation technologies. Endogenous BCIs based on electroencephalogram
(EEG) signals, such as motor imagery (MI) BCIs, can provide some level of
control. However, mastering spontaneous BCI control requires the users to
generate discriminative and stable brain signal patterns by imagery, which is
challenging and is usually achieved over a very long training time
(weeks/months). Here, we propose a human-machine joint learning framework to
boost the learning process in endogenous BCIs, by guiding the user to generate
brain signals towards an optimal distribution estimated by the decoder, given
the historical brain signals of the user. To this end, we firstly model the
human-machine joint learning process in a uniform formulation. Then a
human-machine joint learning framework is proposed: 1) for the human side, we
model the learning process in a sequential trial-and-error scenario and propose
a novel ``copy/new&apos;&apos; feedback paradigm to help shape the signal generation of
the subject toward the optimal distribution; 2) for the machine side, we
propose a novel adaptive learning algorithm to learn an optimal signal
distribution along with the subject&apos;s learning process. Specifically, the
decoder reweighs the brain signals generated by the subject to focus more on
``good&apos;&apos; samples to cope with the learning process of the subject. Online and
psuedo-online BCI experiments with 18 healthy subjects demonstrated the
advantages of the proposed joint learning process over co-adaptive approaches
in both learning efficiency and effectiveness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hanwen Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_Y/0/1/0/all/0/1&quot;&gt;Yu Qi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_L/0/1/0/all/0/1&quot;&gt;Lin Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yueming Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farina_D/0/1/0/all/0/1&quot;&gt;Dario Farina&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_G/0/1/0/all/0/1&quot;&gt;Gang Pan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03211">
<title>Improving the State of the Art for Training Human-AI Teams: Technical Report #1 -- Results of Subject-Matter Expert Knowledge Elicitation Survey. (arXiv:2309.03211v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.03211</link>
<description rdf:parseType="Literal">&lt;p&gt;A consensus report produced for the Air Force Research Laboratory by the
National Academies of Sciences, Engineering, and Mathematics documented a
prevalent and increasing desire to support human-Artificial Intelligence (AI)
teaming across military service branches. Sonalysts has begun an internal
initiative to explore the training of human-AI teams. The first step in this
effort is to develop a Synthetic Task Environment (STE) that is capable of
facilitating research on human-AI teams. We decided to use Joint All-Domain
Command and Control (JADC2) as a focus point for developing the STE because the
volume of sensor inputs and decision options within the JADC2 concept likely
requires the use of AI systems to enable timely decisions. Given this focus, we
engaged a number of Subject-Matter Experts (SMEs) with Command and Control
experience to gain insight into developing a STE that embodied the teaming
challenges associated with JADC2. This report documents our initial engagement
with those stakeholders. The research team identified thirteen Sonalysts
employees with military backgrounds and Command and Control experience, and
invited them to participate. Twelve respondents completed the survey. The team
then analyzed the responses to identify themes that emerged and topics that
would benefit from further analysis. The results indicated that our SMEs were
amenable to research using tasks that were analogous to those encountered in
military environments, as long as they required teams to process a great deal
of incoming data to arrive at complex decisions. The SMEs felt that the testbed
should support &apos;teams of teams&quot; that represented a matrixed organization, and
that it should support a robust array to spoken, text-based, and face-to-face
communications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarthy_J/0/1/0/all/0/1&quot;&gt;James E. McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asiala_L/0/1/0/all/0/1&quot;&gt;Lillian Asiala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maryeski_L/0/1/0/all/0/1&quot;&gt;LeeAnn Maryeski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warren_N/0/1/0/all/0/1&quot;&gt;Nyla Warren&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03212">
<title>Improving the State of the Art for Training Human-AI Teams: Technical Report #2 -- Results of Researcher Knowledge Elicitation Survey. (arXiv:2309.03212v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.03212</link>
<description rdf:parseType="Literal">&lt;p&gt;A consensus report produced for the Air Force Research Laboratory (AFRL) by
the National Academies of Sciences, Engineering, and Mathematics documented a
prevalent and increasing desire to support human-Artificial Intelligence (AI)
teaming across military service branches. Sonalysts has begun an internal
initiative to explore the training of Human-AI teams. The first step in this
effort is to develop a Synthetic Task Environment (STE) that is capable of
facilitating research on Human-AI teams. Our goal is to create a STE that
offers a task environment that could support the breadth of research that
stakeholders plan to perform within this domain. As a result, we wanted to
sample the priorities of the relevant research community broadly, and the
effort documented in this report is our initial attempt to do so. We created a
survey that featured two types of questions. The first asked respondents to
report their agreement with STE features that we anticipated might be
important. The second represented open-ended questions that asked respondents
to specify their priorities within several dimensions of the anticipated STE.
The research team invited nineteen researchers from academic and Government
labs to participate, and 11 were able to complete the survey. The team analyzed
their responses to identify themes that emerged and topics that would benefit
from further analysis. The most significant finding of the survey was that a
number of researchers felt that various open-source STEs that would meet our
needs already exist. Researchers also emphasized the need for automated
transcription and coding tools to ease the burden of assessing inter-team
communications; the importance of robust data capture and export capabilities;
and the desirability of extensive flexibility across many aspects of the tool.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarthy_J/0/1/0/all/0/1&quot;&gt;James E. McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asiala_L/0/1/0/all/0/1&quot;&gt;Lillian Asiala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maryeski_L/0/1/0/all/0/1&quot;&gt;LeeAnn Maryeski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sillars_D/0/1/0/all/0/1&quot;&gt;Dawn Sillars&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03213">
<title>Improving the State of the Art for Training Human-AI Teams: Technical Report #3 -- Analysis of Testbed Alternatives. (arXiv:2309.03213v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.03213</link>
<description rdf:parseType="Literal">&lt;p&gt;Sonalysts is working on an initiative to expand our current expertise in
teaming to Human-Artificial Intelligence (AI) teams by developing original
research in this area. To provide a foundation for that research, Sonalysts is
investigating the development of a Synthetic Task Environment (STE). In a
previous report, we documented the findings of a recent outreach effort in
which we asked military Subject Matter Experts (SMEs) and other researchers in
the Human-AI teaming domain to identify the qualities that they most valued in
a testbed. A surprising finding from that outreach was that several respondents
recommended that our team look into existing human-AI teaming testbeds, rather
than creating something new. Based on that recommendation, we conducted a
systematic investigation of the associated landscape. In this report, we
describe the results of that investigation. Building on the survey results, we
developed testbed evaluation criteria, identified potential testbeds, and
conducted qualitative and quantitative evaluations of candidate testbeds. The
evaluation process led to five candidate testbeds for the research team to
consider. In the coming months, we will assess the viability of the various
alternatives and begin to execute our program of research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Asiala_L/0/1/0/all/0/1&quot;&gt;Lillian Asiala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McCarthy_J/0/1/0/all/0/1&quot;&gt;James E. McCarthy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_L/0/1/0/all/0/1&quot;&gt;Lixiao Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03215">
<title>Explainable and Trustworthy Traffic Sign Detection for Safe Autonomous Driving: An Inductive Logic Programming Approach. (arXiv:2309.03215v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03215</link>
<description rdf:parseType="Literal">&lt;p&gt;Traffic sign detection is a critical task in the operation of Autonomous
Vehicles (AV), as it ensures the safety of all road users. Current DNN-based
sign classification systems rely on pixel-level features to detect traffic
signs and can be susceptible to adversarial attacks. These attacks involve
small, imperceptible changes to a sign that can cause traditional classifiers
to misidentify the sign. We propose an Inductive Logic Programming (ILP) based
approach for stop sign detection in AVs to address this issue. This method
utilises high-level features of a sign, such as its shape, colour, and text, to
detect categories of traffic signs. This approach is more robust against
adversarial attacks, as it mimics human-like perception and is less susceptible
to the limitations of current DNN classifiers. We consider two adversarial
attacking methods to evaluate our approach: Robust Physical Perturbation (PR2)
and Adversarial Camouflage (AdvCam). These attacks are able to deceive DNN
classifiers, causing them to misidentify stop signs as other signs with high
confidence. The results show that the proposed ILP-based technique is able to
correctly identify all targeted stop signs, even in the presence of PR2 and
ADvCam attacks. The proposed learning method is also efficient as it requires
minimal training data. Moreover, it is fully explainable, making it possible to
debug AVs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaghazardi_Z/0/1/0/all/0/1&quot;&gt;Zahra Chaghazardi&lt;/a&gt; (University of Surrey), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fallah_S/0/1/0/all/0/1&quot;&gt;Saber Fallah&lt;/a&gt; (University of Surrey), &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tamaddoni_Nezhad_A/0/1/0/all/0/1&quot;&gt;Alireza Tamaddoni-Nezhad&lt;/a&gt; (University of Surrey)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03217">
<title>Algebraic Models for Qualified Aggregation in General Rough Sets, and Reasoning Bias Discovery. (arXiv:2309.03217v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03217</link>
<description rdf:parseType="Literal">&lt;p&gt;In the context of general rough sets, the act of combining two things to form
another is not straightforward. The situation is similar for other theories
that concern uncertainty and vagueness. Such acts can be endowed with
additional meaning that go beyond structural conjunction and disjunction as in
the theory of $*$-norms and associated implications over $L$-fuzzy sets. In the
present research, algebraic models of acts of combining things in generalized
rough sets over lattices with approximation operators (called rough convenience
lattices) is invented. The investigation is strongly motivated by the desire to
model skeptical or pessimistic, and optimistic or possibilistic aggregation in
human reasoning, and the choice of operations is constrained by the
perspective. Fundamental results on the weak negations and implications
afforded by the minimal models are proved. In addition, the model is suitable
for the study of discriminatory/toxic behavior in human reasoning, and of ML
algorithms learning such behavior.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+A_M/0/1/0/all/0/1&quot;&gt;Mani A&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03219">
<title>Companion Animal Disease Diagnostics based on Literal-aware Medical Knowledge Graph Representation Learning. (arXiv:2309.03219v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03219</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graph (KG) embedding has been used to benefit the diagnosis of
animal diseases by analyzing electronic medical records (EMRs), such as notes
and veterinary records. However, learning representations to capture entities
and relations with literal information in KGs is challenging as the KGs show
heterogeneous properties and various types of literal information. Meanwhile,
the existing methods mostly aim to preserve graph structures surrounding target
nodes without considering different types of literals, which could also carry
significant information. In this paper, we propose a knowledge graph embedding
model for the efficient diagnosis of animal diseases, which could learn various
types of literal information and graph structure and fuse them into unified
representations, namely LiteralKG. Specifically, we construct a knowledge graph
that is built from EMRs along with literal information collected from various
animal hospitals. We then fuse different types of entities and node feature
information into unified vector representations through gate networks. Finally,
we propose a self-supervised learning task to learn graph structure in pretext
tasks and then towards various downstream tasks. Experimental results on link
prediction tasks demonstrate that our model outperforms the baselines that
consist of state-of-the-art models. The source code is available at
https://github.com/NSLab-CUK/LiteralKG.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoang_V/0/1/0/all/0/1&quot;&gt;Van Thuy Hoang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_S/0/1/0/all/0/1&quot;&gt;Sang Thanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangmyeong Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jooho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_L/0/1/0/all/0/1&quot;&gt;Luong Vuong Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_O/0/1/0/all/0/1&quot;&gt;O-Joun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03222">
<title>Sherlock Holmes Doesn&apos;t Play Dice: The significance of Evidence Theory for the Social and Life Sciences. (arXiv:2309.03222v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03222</link>
<description rdf:parseType="Literal">&lt;p&gt;While Evidence Theory (Demster-Shafer Theory, Belief Functions Theory) is
being increasingly used in data fusion, its potentialities in the Social and
Life Sciences are often obscured by lack of awareness of its distinctive
features. With this paper we stress that Evidence Theory can express the
uncertainty deriving from the fear that events may materialize, that one has
not been able to figure out. By contrast, Probability Theory must limit itself
to the possibilities that a decision-maker is currently envisaging.
&lt;/p&gt;
&lt;p&gt;Subsequently, we illustrate how Dempster-Shafer&apos;s combination rule relates to
Bayes&apos; Theorem for various versions of Probability Theory and discuss which
applications of Information Theory can be enhanced by Evidence Theory. Finally,
we illustrate our claims with an example where Evidence Theory is used to make
sense of the partially overlapping, partially contradictory solutions that
appear in an auditing exercise.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chinthalapati_V/0/1/0/all/0/1&quot;&gt;V. L. Raju Chinthalapati&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fioretti_G/0/1/0/all/0/1&quot;&gt;Guido Fioretti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03224">
<title>No Train Still Gain. Unleash Mathematical Reasoning of Large Language Models with Monte Carlo Tree Search Guided by Energy Function. (arXiv:2309.03224v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03224</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) exhibit impressive language understanding and
in-context learning abilities including natural language processing (NLP) tasks
and challenging mathematical reasoning. However, due to the lack of
process-supervision, applying PLMs to mathematical reasoning tasks often fail
to generate correct reasoning steps and final answer even though solutions have
high probabilities. To unleash the mathematical reasoning of finetuned-LLMs
without any further fineutuning steps, we propose a method to endow LLMs with
immediate reaction and delicate reasoning system via Monte Carlo Tree
Search(MCTS) and a light energy function to rank the decision steps. In
particular, We first re-formalize the finetuned-LLMs to a Residual-based Energy
Model~(Residual-EBM) and apply noise contrastive estimation to estimate the
parameters of energy function . Then we use MCTS with energy function as path
verifier to search the output space and evaluating the reasoning path. Through
extensive experiments on two mathematical reasoning benchmarks, namely GSM8k
and MATH, we reveal the extraordinary capabilities of our method that improve
the pass@1 of the finetuned-model without further finetuning or RLHF alignment
by a substantial margin.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Haotian Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03225">
<title>Amortizing Pragmatic Program Synthesis with Rankings. (arXiv:2309.03225v1 [cs.PL])</title>
<link>http://arxiv.org/abs/2309.03225</link>
<description rdf:parseType="Literal">&lt;p&gt;In program synthesis, an intelligent system takes in a set of user-generated
examples and returns a program that is logically consistent with these
examples. The usage of Rational Speech Acts (RSA) framework has been successful
in building \emph{pragmatic} program synthesizers that return programs which --
in addition to being logically consistent -- account for the fact that a user
chooses their examples informatively. However, the computational burden of
running the RSA algorithm has restricted the application of pragmatic program
synthesis to domains with a small number of possible programs. This work
presents a novel method of amortizing the RSA algorithm by leveraging a
\emph{global pragmatic ranking} -- a single, total ordering of all the
hypotheses. We prove that for a pragmatic synthesizer that uses a single
demonstration, our global ranking method exactly replicates RSA&apos;s ranked
responses. We further empirically show that global rankings effectively
approximate the full pragmatic synthesizer in an online, multi-demonstration
setting. Experiments on two program synthesis domains using our pragmatic
ranking method resulted in orders of magnitudes of speed ups compared to the
RSA synthesizer, while outperforming the standard, non-pragmatic synthesizer.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pu_Y/0/1/0/all/0/1&quot;&gt;Yewen Pu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaduguru_S/0/1/0/all/0/1&quot;&gt;Saujas Vaduguru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vaithilingam_P/0/1/0/all/0/1&quot;&gt;Priyan Vaithilingam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glassman_E/0/1/0/all/0/1&quot;&gt;Elena Glassman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fried_D/0/1/0/all/0/1&quot;&gt;Daniel Fried&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03227">
<title>Learning a Patent-Informed Biomedical Knowledge Graph Reveals Technological Potential of Drug Repositioning Candidates. (arXiv:2309.03227v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03227</link>
<description rdf:parseType="Literal">&lt;p&gt;Drug repositioning-a promising strategy for discovering new therapeutic uses
for existing drugs-has been increasingly explored in the computational science
literature using biomedical databases. However, the technological potential of
drug repositioning candidates has often been overlooked. This study presents a
novel protocol to comprehensively analyse various sources such as
pharmaceutical patents and biomedical databases, and identify drug
repositioning candidates with both technological potential and scientific
evidence. To this end, first, we constructed a scientific biomedical knowledge
graph (s-BKG) comprising relationships between drugs, diseases, and genes
derived from biomedical databases. Our protocol involves identifying drugs that
exhibit limited association with the target disease but are closely located in
the s-BKG, as potential drug candidates. We constructed a patent-informed
biomedical knowledge graph (p-BKG) by adding pharmaceutical patent information.
Finally, we developed a graph embedding protocol to ascertain the structure of
the p-BKG, thereby calculating the relevance scores of those candidates with
target disease-related patents to evaluate their technological potential. Our
case study on Alzheimer&apos;s disease demonstrates its efficacy and feasibility,
while the quantitative outcomes and systematic methods are expected to bridge
the gap between computational discoveries and successful market applications in
drug repositioning research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jegal_Y/0/1/0/all/0/1&quot;&gt;Yongseung Jegal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jaewoong Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jiho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_K/0/1/0/all/0/1&quot;&gt;Ki-Su Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seyoung Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Janghyeok Yoon&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03229">
<title>Which algorithm to select in sports timetabling?. (arXiv:2309.03229v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03229</link>
<description rdf:parseType="Literal">&lt;p&gt;Any sports competition needs a timetable, specifying when and where teams
meet each other. The recent International Timetabling Competition (ITC2021) on
sports timetabling showed that, although it is possible to develop general
algorithms, the performance of each algorithm varies considerably over the
problem instances. This paper provides an instance space analysis for sports
timetabling, resulting in powerful insights into the strengths and weaknesses
of eight state-of-the-art algorithms. Based on machine learning techniques, we
propose an algorithm selection system that predicts which algorithm is likely
to perform best when given the characteristics of a sports timetabling problem
instance. Furthermore, we identify which characteristics are important in
making that prediction, providing insights in the performance of the
algorithms, and suggestions to further improve them. Finally, we assess the
empirical hardness of the instances. Our results are based on large
computational experiments involving about 50 years of CPU time on more than 500
newly generated problem instances.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bulck_D/0/1/0/all/0/1&quot;&gt;David Van Bulck&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goossens_D/0/1/0/all/0/1&quot;&gt;Dries Goossens&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Clarner_J/0/1/0/all/0/1&quot;&gt;Jan-Patrick Clarner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimitsas_A/0/1/0/all/0/1&quot;&gt;Angelos Dimitsas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fonseca_G/0/1/0/all/0/1&quot;&gt;George H. G. Fonseca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lamas_Fernandez_C/0/1/0/all/0/1&quot;&gt;Carlos Lamas-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lester_M/0/1/0/all/0/1&quot;&gt;Martin Mariusz Lester&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pedersen_J/0/1/0/all/0/1&quot;&gt;Jaap Pedersen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phillips_A/0/1/0/all/0/1&quot;&gt;Antony E. Phillips&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rosati_R/0/1/0/all/0/1&quot;&gt;Roberto Maria Rosati&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03231">
<title>Quantum-AI empowered Intelligent Surveillance: Advancing Public Safety Through Innovative Contraband Detection. (arXiv:2309.03231v1 [quant-ph])</title>
<link>http://arxiv.org/abs/2309.03231</link>
<description rdf:parseType="Literal">&lt;p&gt;Surveillance systems have emerged as crucial elements in upholding peace and
security in the modern world. Their ubiquity aids in monitoring suspicious
activities effectively. However, in densely populated environments, continuous
active monitoring becomes impractical, necessitating the development of
intelligent surveillance systems. AI integration in the surveillance domain was
a big revolution, however, speed issues have prevented its widespread
implementation in the field. It has been observed that quantum artificial
intelligence has led to a great breakthrough. Quantum artificial
intelligence-based surveillance systems have shown to be more accurate as well
as capable of performing well in real-time scenarios, which had never been seen
before. In this research, a RentinaNet model is integrated with Quantum CNN and
termed as Quantum-RetinaNet. By harnessing the Quantum capabilities of QCNN,
Quantum-RetinaNet strikes a balance between accuracy and speed. This innovative
integration positions it as a game-changer, addressing the challenges of active
monitoring in densely populated scenarios. As demand for efficient surveillance
solutions continues to grow, Quantum-RetinaNet offers a compelling alternative
to existing CNN models, upholding accuracy standards without sacrificing
real-time performance. The unique attributes of Quantum-RetinaNet have
far-reaching implications for the future of intelligent surveillance. With its
enhanced processing speed, it is poised to revolutionize the field, catering to
the pressing need for rapid yet precise monitoring. As Quantum-RetinaNet
becomes the new standard, it ensures public safety and security while pushing
the boundaries of AI in surveillance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Shah_S/0/1/0/all/0/1&quot;&gt;Syed Atif Ali Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Algeelani_N/0/1/0/all/0/1&quot;&gt;Nasir Algeelani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/quant-ph/1/au:+Al_Sammarraie_N/0/1/0/all/0/1&quot;&gt;Najeeb Al-Sammarraie&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03234">
<title>Natural Example-Based Explainability: a Survey. (arXiv:2309.03234v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03234</link>
<description rdf:parseType="Literal">&lt;p&gt;Explainable Artificial Intelligence (XAI) has become increasingly significant
for improving the interpretability and trustworthiness of machine learning
models. While saliency maps have stolen the show for the last few years in the
XAI field, their ability to reflect models&apos; internal processes has been
questioned. Although less in the spotlight, example-based XAI methods have
continued to improve. It encompasses methods that use examples as explanations
for a machine learning model&apos;s predictions. This aligns with the psychological
mechanisms of human reasoning and makes example-based explanations natural and
intuitive for users to understand. Indeed, humans learn and reason by forming
mental representations of concepts based on examples.
&lt;/p&gt;
&lt;p&gt;This paper provides an overview of the state-of-the-art in natural
example-based XAI, describing the pros and cons of each approach. A &quot;natural&quot;
example simply means that it is directly drawn from the training data without
involving any generative process. The exclusion of methods that require
generating examples is justified by the need for plausibility which is in some
regards required to gain a user&apos;s trust. Consequently, this paper will explore
the following family of methods: similar examples, counterfactual and
semi-factual, influential instances, prototypes, and concepts. In particular,
it will compare their semantic definition, their cognitive impact, and added
values. We hope it will encourage and facilitate future work on natural
example-based XAI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poche_A/0/1/0/all/0/1&quot;&gt;Antonin Poch&amp;#xe9;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hervier_L/0/1/0/all/0/1&quot;&gt;Lucas Hervier&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bakkay_M/0/1/0/all/0/1&quot;&gt;Mohamed-Chafik Bakkay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03239">
<title>Spatio-Temporal Contrastive Self-Supervised Learning for POI-level Crowd Flow Inference. (arXiv:2309.03239v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03239</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate acquisition of crowd flow at Points of Interest (POIs) is pivotal
for effective traffic management, public service, and urban planning. Despite
this importance, due to the limitations of urban sensing techniques, the data
quality from most sources is inadequate for monitoring crowd flow at each POI.
This renders the inference of accurate crowd flow from low-quality data a
critical and challenging task. The complexity is heightened by three key
factors: 1) \emph{The scarcity and rarity of labeled data}, 2) \emph{The
intricate spatio-temporal dependencies among POIs}, and 3) \emph{The myriad
correlations between precise crowd flow and GPS reports}.
&lt;/p&gt;
&lt;p&gt;To address these challenges, we recast the crowd flow inference problem as a
self-supervised attributed graph representation learning task and introduce a
novel \underline{C}ontrastive \underline{S}elf-learning framework for
\underline{S}patio-\underline{T}emporal data (\model). Our approach initiates
with the construction of a spatial adjacency graph founded on the POIs and
their respective distances. We then employ a contrastive learning technique to
exploit large volumes of unlabeled spatio-temporal data. We adopt a swapped
prediction approach to anticipate the representation of the target subgraph
from similar instances. Following the pre-training phase, the model is
fine-tuned with accurate crowd flow data. Our experiments, conducted on two
real-world datasets, demonstrate that the \model pre-trained on extensive noisy
data consistently outperforms models trained from scratch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ke_S/0/1/0/all/0/1&quot;&gt;Songyu Ke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Ting Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_L/0/1/0/all/0/1&quot;&gt;Li Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yanping Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Q/0/1/0/all/0/1&quot;&gt;Qintian Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Junbo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_Y/0/1/0/all/0/1&quot;&gt;Yu Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03241">
<title>GPT Can Solve Mathematical Problems Without a Calculator. (arXiv:2309.03241v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03241</link>
<description rdf:parseType="Literal">&lt;p&gt;Previous studies have typically assumed that large language models are unable
to accurately perform arithmetic operations, particularly multiplication of &amp;gt;8
digits, and operations involving decimals and fractions, without the use of
calculator tools. This paper aims to challenge this misconception. With
sufficient training data, a 2 billion-parameter language model can accurately
perform multi-digit arithmetic operations with almost 100% accuracy without
data leakage, significantly surpassing GPT-4 (whose multi-digit multiplication
accuracy is only 4.3%). We also demonstrate that our MathGLM, fine-tuned from
GLM-10B on a dataset with additional multi-step arithmetic operations and math
problems described in text, achieves similar performance to GPT-4 on a
5,000-samples Chinese math problem test set.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Z/0/1/0/all/0/1&quot;&gt;Zhen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_M/0/1/0/all/0/1&quot;&gt;Ming Ding&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lv_Q/0/1/0/all/0/1&quot;&gt;Qingsong Lv&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Z/0/1/0/all/0/1&quot;&gt;Zhihuan Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_Z/0/1/0/all/0/1&quot;&gt;Zehai He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yuyi Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_J/0/1/0/all/0/1&quot;&gt;Jinfeng Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jie Tang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03242">
<title>Automated Bioinformatics Analysis via AutoBA. (arXiv:2309.03242v1 [q-bio.GN])</title>
<link>http://arxiv.org/abs/2309.03242</link>
<description rdf:parseType="Literal">&lt;p&gt;With the fast-growing and evolving omics data, the demand for streamlined and
adaptable tools to handle the analysis continues to grow. In response to this
need, we introduce Auto Bioinformatics Analysis (AutoBA), an autonomous AI
agent based on a large language model designed explicitly for conventional
omics data analysis. AutoBA simplifies the analytical process by requiring
minimal user input while delivering detailed step-by-step plans for various
bioinformatics tasks. Through rigorous validation by expert bioinformaticians,
AutoBA&apos;s robustness and adaptability are affirmed across a diverse range of
omics analysis cases, including whole genome sequencing (WGS), RNA sequencing
(RNA-seq), single-cell RNA-seq, ChIP-seq, and spatial transcriptomics. AutoBA&apos;s
unique capacity to self-design analysis processes based on input data
variations further underscores its versatility. Compared with online
bioinformatic services, AutoBA deploys the analysis locally, preserving data
privacy. Moreover, different from the predefined pipeline, AutoBA has
adaptability in sync with emerging bioinformatics tools. Overall, AutoBA
represents a convenient tool, offering robustness and adaptability for complex
omics data analysis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Juexiao Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xiuying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xiaopeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Siyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gao_X/0/1/0/all/0/1&quot;&gt;Xin Gao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03251">
<title>Temporal Inductive Path Neural Network for Temporal Knowledge Graph Reasoning. (arXiv:2309.03251v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03251</link>
<description rdf:parseType="Literal">&lt;p&gt;Temporal Knowledge Graph (TKG) is an extension of traditional Knowledge Graph
(KG) that incorporates the dimension of time. Reasoning on TKGs is a crucial
task that aims to predict future facts based on historical occurrences. The key
challenge lies in uncovering structural dependencies within historical
subgraphs and temporal patterns. Most existing approaches model TKGs relying on
entity modeling, as nodes in the graph play a crucial role in knowledge
representation. However, the real-world scenario often involves an extensive
number of entities, with new entities emerging over time. This makes it
challenging for entity-dependent methods to cope with extensive volumes of
entities, and effectively handling newly emerging entities also becomes a
significant challenge. Therefore, we propose Temporal Inductive Path Neural
Network (TiPNN), which models historical information in an entity-independent
perspective. Specifically, TiPNN adopts a unified graph, namely history
temporal graph, to comprehensively capture and encapsulate information from
history. Subsequently, we utilize the defined query-aware temporal paths to
model historical path information related to queries on history temporal graph
for the reasoning. Extensive experiments illustrate that the proposed model not
only attains significant performance enhancements but also handles inductive
settings, while additionally facilitating the provision of reasoning evidence
through history temporal graphs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_M/0/1/0/all/0/1&quot;&gt;Meng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ning_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Ning&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Pengfei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yuanchun Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03295">
<title>Comparative Analysis of Deep-Fake Algorithms. (arXiv:2309.03295v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03295</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the widespread use of smartphones with high-quality digital cameras
and easy access to a wide range of software apps for recording, editing, and
sharing videos and images, as well as the deep learning AI platforms, a new
phenomenon of &apos;faking&apos; videos has emerged. Deepfake algorithms can create fake
images and videos that are virtually indistinguishable from authentic ones.
Therefore, technologies that can detect and assess the integrity of digital
visual media are crucial. Deepfakes, also known as deep learning-based fake
videos, have become a major concern in recent years due to their ability to
manipulate and alter images and videos in a way that is virtually
indistinguishable from the original. These deepfake videos can be used for
malicious purposes such as spreading misinformation, impersonating individuals,
and creating fake news. Deepfake detection technologies use various approaches
such as facial recognition, motion analysis, and audio-visual synchronization
to identify and flag fake videos. However, the rapid advancement of deepfake
technologies has made it increasingly difficult to detect these videos with
high accuracy. In this paper, we aim to provide a comprehensive review of the
current state of deepfake creation and detection technologies. We examine the
various deep learning-based approaches used for creating deepfakes, as well as
the techniques used for detecting them. Additionally, we analyze the
limitations and challenges of current deepfake detection methods and discuss
future research directions in this field. Overall, the paper highlights the
importance of continued research and development in deepfake detection
technologies in order to combat the negative impact of deepfakes on society and
ensure the integrity of digital visual media.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sontakke_N/0/1/0/all/0/1&quot;&gt;Nikhil Sontakke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Utekar_S/0/1/0/all/0/1&quot;&gt;Sejal Utekar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rastogi_S/0/1/0/all/0/1&quot;&gt;Shivansh Rastogi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sonawane_S/0/1/0/all/0/1&quot;&gt;Shriraj Sonawane&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03318">
<title>Fitness Approximation through Machine Learning. (arXiv:2309.03318v1 [cs.NE])</title>
<link>http://arxiv.org/abs/2309.03318</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to performing fitness approximation in genetic
algorithms (GAs) using machine-learning (ML) models, focusing on evolutionary
agents in Gymnasium (game) simulators -- where fitness computation is costly.
Maintaining a dataset of sampled individuals along with their actual fitness
scores, we continually update throughout an evolutionary run a
fitness-approximation ML model. We compare different methods for: 1) switching
between actual and approximate fitness, 2) sampling the population, and 3)
weighting the samples. Experimental findings demonstrate significant
improvement in evolutionary runtimes, with fitness scores that are either
identical or slightly lower than that of the fully run GA -- depending on the
ratio of approximate-to-actual-fitness computation. Our approach is generic and
can be easily applied to many different domains.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tzruia_I/0/1/0/all/0/1&quot;&gt;Itai Tzruia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Halperin_T/0/1/0/all/0/1&quot;&gt;Tomer Halperin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1&quot;&gt;Moshe Sipper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elyasaf_A/0/1/0/all/0/1&quot;&gt;Achiya Elyasaf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03322">
<title>REBOOT: Reuse Data for Bootstrapping Efficient Real-World Dexterous Manipulation. (arXiv:2309.03322v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03322</link>
<description rdf:parseType="Literal">&lt;p&gt;Dexterous manipulation tasks involving contact-rich interactions pose a
significant challenge for both model-based control systems and imitation
learning algorithms. The complexity arises from the need for multi-fingered
robotic hands to dynamically establish and break contacts, balance
non-prehensile forces, and control large degrees of freedom. Reinforcement
learning (RL) offers a promising approach due to its general applicability and
capacity to autonomously acquire optimal manipulation strategies. However, its
real-world application is often hindered by the necessity to generate a large
number of samples, reset the environment, and obtain reward signals. In this
work, we introduce an efficient system for learning dexterous manipulation
skills with RL to alleviate these challenges. The main idea of our approach is
the integration of recent advances in sample-efficient RL and replay buffer
bootstrapping. This combination allows us to utilize data from different tasks
or objects as a starting point for training new tasks, significantly improving
learning efficiency. Additionally, our system completes the real-world training
cycle by incorporating learned resets via an imitation-based pickup policy as
well as learned reward functions, eliminating the need for manual resets and
reward engineering. We demonstrate the benefits of reusing past data as replay
buffer initialization for new tasks, for instance, the fast acquisition of
intricate manipulation skills in the real world on a four-fingered robotic
hand. (Videos: https://sites.google.com/view/reboot-dexterous)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zheyuan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rovinsky_A/0/1/0/all/0/1&quot;&gt;Aaron Rovinsky&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jianlan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_V/0/1/0/all/0/1&quot;&gt;Vikash Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_A/0/1/0/all/0/1&quot;&gt;Abhishek Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levine_S/0/1/0/all/0/1&quot;&gt;Sergey Levine&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03367">
<title>Self-Supervised Masked Digital Elevation Models Encoding for Low-Resource Downstream Tasks. (arXiv:2309.03367v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03367</link>
<description rdf:parseType="Literal">&lt;p&gt;The lack of quality labeled data is one of the main bottlenecks for training
Deep Learning models. As the task increases in complexity, there is a higher
penalty for overfitting and unstable learning. The typical paradigm employed
today is Self-Supervised learning, where the model attempts to learn from a
large corpus of unstructured and unlabeled data and then transfer that
knowledge to the required task. Some notable examples of self-supervision in
other modalities are BERT for Large Language Models, Wav2Vec for Speech
Recognition, and the Masked AutoEncoder for Vision, which all utilize
Transformers to solve a masked prediction task. GeoAI is uniquely poised to
take advantage of the self-supervised methodology due to the decades of data
collected, little of which is precisely and dependably annotated. Our goal is
to extract building and road segmentations from Digital Elevation Models (DEM)
that provide a detailed topography of the earths surface. The proposed
architecture is the Masked Autoencoder pre-trained on ImageNet (with the
limitation that there is a large domain discrepancy between ImageNet and DEM)
with an UperNet Head for decoding segmentations. We tested this model with 450
and 50 training images only, utilizing roughly 5% and 0.5% of the original data
respectively. On the building segmentation task, this model obtains an 82.1%
Intersection over Union (IoU) with 450 Images and 69.1% IoU with only 50
images. On the more challenging road detection task the model obtains an 82.7%
IoU with 450 images and 73.2% IoU with only 50 images. Any hand-labeled dataset
made today about the earths surface will be immediately obsolete due to the
constantly changing nature of the landscape. This motivates the clear necessity
for data-efficient learners that can be used for a wide variety of downstream
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mazumdar_P/0/1/0/all/0/1&quot;&gt;Priyam Mazumdar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Soliman_A/0/1/0/all/0/1&quot;&gt;Aiman Soliman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kindratenko_V/0/1/0/all/0/1&quot;&gt;Volodymyr Kindratenko&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Marini_L/0/1/0/all/0/1&quot;&gt;Luigi Marini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+McHenry_K/0/1/0/all/0/1&quot;&gt;Kenton McHenry&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03386">
<title>Community-Based Hierarchical Positive-Unlabeled (PU) Model Fusion for Chronic Disease Prediction. (arXiv:2309.03386v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03386</link>
<description rdf:parseType="Literal">&lt;p&gt;Positive-Unlabeled (PU) Learning is a challenge presented by binary
classification problems where there is an abundance of unlabeled data along
with a small number of positive data instances, which can be used to address
chronic disease screening problem. State-of-the-art PU learning methods have
resulted in the development of various risk estimators, yet they neglect the
differences among distinct populations. To address this issue, we present a
novel Positive-Unlabeled Learning Tree (PUtree) algorithm. PUtree is designed
to take into account communities such as different age or income brackets, in
tasks of chronic disease prediction. We propose a novel approach for binary
decision-making, which hierarchically builds community-based PU models and then
aggregates their deliverables. Our method can explicate each PU model on the
tree for the optimized non-leaf PU node splitting. Furthermore, a mask-recovery
data augmentation strategy enables sufficient training of the model in
individual communities. Additionally, the proposed approach includes an
adversarial PU risk estimator to capture hierarchical PU-relationships, and a
model fusion network that integrates data from each tree path, resulting in
robust binary classification results. We demonstrate the superior performance
of PUtree as well as its variants on two benchmarks and a new
diabetes-prediction dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Y/0/1/0/all/0/1&quot;&gt;Yang Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xurui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuhong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_Y/0/1/0/all/0/1&quot;&gt;Yangyang Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Changlong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaozhong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03387">
<title>Efficient Baselines for Motion Prediction in Autonomous Driving. (arXiv:2309.03387v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.03387</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion Prediction (MP) of multiple surroundings agents is a crucial task in
arbitrarily complex environments, from simple robots to Autonomous Driving
Stacks (ADS). Current techniques tackle this problem using end-to-end
pipelines, where the input data is usually a rendered top-view of the physical
information and the past trajectories of the most relevant agents; leveraging
this information is a must to obtain optimal performance. In that sense, a
reliable ADS must produce reasonable predictions on time. However, despite many
approaches use simple ConvNets and LSTMs to obtain the social latent features,
State-Of-The-Art (SOTA) models might be too complex for real-time applications
when using both sources of information (map and past trajectories) as well as
little interpretable, specially considering the physical information. Moreover,
the performance of such models highly depends on the number of available inputs
for each particular traffic scenario, which are expensive to obtain,
particularly, annotated High-Definition (HD) maps.
&lt;/p&gt;
&lt;p&gt;In this work, we propose several efficient baselines for the well-known
Argoverse 1 Motion Forecasting Benchmark. We aim to develop compact models
using SOTA techniques for MP, including attention mechanisms and GNNs. Our
lightweight models use standard social information and interpretable map
information such as points from the driveable area and plausible centerlines by
means of a novel preprocessing step based on kinematic constraints, in
opposition to black-box CNN-based or too-complex graphs methods for map
encoding, to generate plausible multimodal trajectories achieving up-to-pair
accuracy with less operations and parameters than other SOTA methods. Our code
is publicly available at https://github.com/Cram3r95/mapfe4mp .
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomez_Huelamo_C/0/1/0/all/0/1&quot;&gt;Carlos G&amp;#xf3;mez-Hu&amp;#xe9;lamo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Conde_M/0/1/0/all/0/1&quot;&gt;Marcos V. Conde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barea_R/0/1/0/all/0/1&quot;&gt;Rafael Barea&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ocana_M/0/1/0/all/0/1&quot;&gt;Manuel Oca&amp;#xf1;a&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bergasa_L/0/1/0/all/0/1&quot;&gt;Luis M. Bergasa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03404">
<title>The Role of Communication and Reference Songs in the Mixing Process: Insights from Professional Mix Engineers. (arXiv:2309.03404v1 [cs.HC])</title>
<link>http://arxiv.org/abs/2309.03404</link>
<description rdf:parseType="Literal">&lt;p&gt;Effective music mixing requires technical and creative finesse, but clear
communication with the client is crucial. The mixing engineer must grasp the
client&apos;s expectations, and preferences, and collaborate to achieve the desired
sound. The tacit agreement for the desired sound of the mix is often
established using guides like reference songs and demo mixes exchanged between
the artist and the engineer and sometimes verbalised using semantic terms. This
paper presents the findings of a two-phased exploratory study aimed at
understanding how professional mixing engineers interact with clients and use
their feedback to guide the mixing process. For phase one, semi-structured
interviews were conducted with five mixing engineers with the aim of gathering
insights about their communication strategies, creative processes, and
decision-making criteria. Based on the inferences from these interviews, an
online questionnaire was designed and administered to a larger group of 22
mixing engineers during the second phase. The results of this study shed light
on the importance of collaboration, empathy, and intention in the mixing
process, and can inform the development of smart multi-track mixing systems
that better support these practices. By highlighting the significance of these
findings, this paper contributes to the growing body of research on the
collaborative nature of music production and provides actionable
recommendations for the design and implementation of innovative mixing tools.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vanka_S/0/1/0/all/0/1&quot;&gt;Soumya Sai Vanka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Safi_M/0/1/0/all/0/1&quot;&gt;MAryam Safi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rolland_J/0/1/0/all/0/1&quot;&gt;Jean-Baptiste Rolland&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fazekas_G/0/1/0/all/0/1&quot;&gt;Gy&amp;#xf6;rgy Fazekas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03409">
<title>Large Language Models as Optimizers. (arXiv:2309.03409v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03409</link>
<description rdf:parseType="Literal">&lt;p&gt;Optimization is ubiquitous. While derivative-based algorithms have been
powerful tools for various problems, the absence of gradient imposes challenges
on many real-world applications. In this work, we propose Optimization by
PROmpting (OPRO), a simple and effective approach to leverage large language
models (LLMs) as optimizers, where the optimization task is described in
natural language. In each optimization step, the LLM generates new solutions
from the prompt that contains previously generated solutions with their values,
then the new solutions are evaluated and added to the prompt for the next
optimization step. We first showcase OPRO on linear regression and traveling
salesman problems, then move on to prompt optimization where the goal is to
find instructions that maximize the task accuracy. With a variety of LLMs, we
demonstrate that the best prompts optimized by OPRO outperform human-designed
prompts by up to 8% on GSM8K, and by up to 50% on Big-Bench Hard tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Chengrun Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xuezhi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yifeng Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hanxiao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_Q/0/1/0/all/0/1&quot;&gt;Quoc V. Le&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_D/0/1/0/all/0/1&quot;&gt;Denny Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xinyun Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03450">
<title>XGen-7B Technical Report. (arXiv:2309.03450v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.03450</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have become ubiquitous across various domains,
transforming the way we interact with information and conduct research.
However, most high-performing LLMs remain confined behind proprietary walls,
hindering scientific progress. Most open-source LLMs, on the other hand, are
limited in their ability to support longer sequence lengths, which is a key
requirement for many tasks that require inference over an input context. To
address this, we have trained XGen, a series of 7B parameter models on up to 8K
sequence length for up to 1.5T tokens. We have also finetuned the XGen models
on public-domain instructional data, creating their instruction-tuned
counterparts (XGen-Inst). We open-source our models for both research
advancements and commercial applications. Our evaluation on standard benchmarks
shows that XGen models achieve comparable or better results when compared with
state-of-the-art open-source LLMs. Our targeted evaluation on long sequence
modeling tasks shows the benefits of our 8K-sequence models over 2K-sequence
open-source LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nijkamp_E/0/1/0/all/0/1&quot;&gt;Erik Nijkamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_T/0/1/0/all/0/1&quot;&gt;Tian Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayashi_H/0/1/0/all/0/1&quot;&gt;Hiroaki Hayashi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_B/0/1/0/all/0/1&quot;&gt;Bo Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_C/0/1/0/all/0/1&quot;&gt;Congying Xia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xing_C/0/1/0/all/0/1&quot;&gt;Chen Xing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vig_J/0/1/0/all/0/1&quot;&gt;Jesse Vig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yavuz_S/0/1/0/all/0/1&quot;&gt;Semih Yavuz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laban_P/0/1/0/all/0/1&quot;&gt;Philippe Laban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krause_B/0/1/0/all/0/1&quot;&gt;Ben Krause&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purushwalkam_S/0/1/0/all/0/1&quot;&gt;Senthil Purushwalkam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_T/0/1/0/all/0/1&quot;&gt;Tong Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kryscinski_W/0/1/0/all/0/1&quot;&gt;Wojciech Kry&amp;#x15b;ci&amp;#x144;ski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Murakhovska_L/0/1/0/all/0/1&quot;&gt;Lidiya Murakhovs&amp;#x27;ka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choubey_P/0/1/0/all/0/1&quot;&gt;Prafulla Kumar Choubey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fabbri_A/0/1/0/all/0/1&quot;&gt;Alex Fabbri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Ye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_R/0/1/0/all/0/1&quot;&gt;Rui Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_L/0/1/0/all/0/1&quot;&gt;Lifu Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bhat_M/0/1/0/all/0/1&quot;&gt;Meghana Bhat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Chien-Sheng Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Savarese_S/0/1/0/all/0/1&quot;&gt;Silvio Savarese&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yingbo Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Joty_S/0/1/0/all/0/1&quot;&gt;Shafiq Joty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_C/0/1/0/all/0/1&quot;&gt;Caiming Xiong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03453">
<title>SyncDreamer: Generating Multiview-consistent Images from a Single-view Image. (arXiv:2309.03453v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03453</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we present a novel diffusion model called that generates
multiview-consistent images from a single-view image. Using pretrained
large-scale 2D diffusion models, recent work Zero123 demonstrates the ability
to generate plausible novel views from a single-view image of an object.
However, maintaining consistency in geometry and colors for the generated
images remains a challenge. To address this issue, we propose a synchronized
multiview diffusion model that models the joint probability distribution of
multiview images, enabling the generation of multiview-consistent images in a
single reverse process. SyncDreamer synchronizes the intermediate states of all
the generated images at every step of the reverse process through a 3D-aware
feature attention mechanism that correlates the corresponding features across
different views. Experiments show that SyncDreamer generates images with high
consistency across different views, thus making it well-suited for various 3D
generation tasks such as novel-view-synthesis, text-to-3D, and image-to-3D.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yuan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_C/0/1/0/all/0/1&quot;&gt;Cheng Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_Z/0/1/0/all/0/1&quot;&gt;Zijiao Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Long_X/0/1/0/all/0/1&quot;&gt;Xiaoxiao Long&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Komura_T/0/1/0/all/0/1&quot;&gt;Taku Komura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenping Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03466">
<title>MIRA: Cracking Black-box Watermarking on Deep Neural Networks via Model Inversion-based Removal Attacks. (arXiv:2309.03466v1 [cs.CR])</title>
<link>http://arxiv.org/abs/2309.03466</link>
<description rdf:parseType="Literal">&lt;p&gt;To protect the intellectual property of well-trained deep neural networks
(DNNs), black-box DNN watermarks, which are embedded into the prediction
behavior of DNN models on a set of specially-crafted samples, have gained
increasing popularity in both academy and industry. Watermark robustness is
usually implemented against attackers who steal the protected model and
obfuscate its parameters for watermark removal. Recent studies empirically
prove the robustness of most black-box watermarking schemes against known
removal attempts.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a novel Model Inversion-based Removal Attack
(\textsc{Mira}), which is watermark-agnostic and effective against most of
mainstream black-box DNN watermarking schemes. In general, our attack pipeline
exploits the internals of the protected model to recover and unlearn the
watermark message. We further design target class detection and recovered
sample splitting algorithms to reduce the utility loss caused by \textsc{Mira}
and achieve data-free watermark removal on half of the watermarking schemes. We
conduct comprehensive evaluation of \textsc{Mira} against ten mainstream
black-box watermarks on three benchmark datasets and DNN architectures.
Compared with six baseline removal attacks, \textsc{Mira} achieves strong
watermark removal effects on the covered watermarks, preserving at least $90\%$
of the stolen model utility, under more relaxed or even no assumptions on the
dataset availability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yifan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Wenxuan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Mi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_X/0/1/0/all/0/1&quot;&gt;Xudong Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_M/0/1/0/all/0/1&quot;&gt;Min Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03467">
<title>Autoregressive Omni-Aware Outpainting for Open-Vocabulary 360-Degree Image Generation. (arXiv:2309.03467v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03467</link>
<description rdf:parseType="Literal">&lt;p&gt;A 360-degree (omni-directional) image provides an all-encompassing spherical
view of a scene. Recently, there has been an increasing interest in
synthesising 360-degree images from conventional narrow field of view (NFoV)
images captured by digital cameras and smartphones, for providing immersive
experiences in various scenarios such as virtual reality. Yet, existing methods
typically fall short in synthesizing intricate visual details or ensure the
generated images align consistently with user-provided prompts. In this study,
autoregressive omni-aware generative network (AOG-Net) is proposed for
360-degree image generation by out-painting an incomplete 360-degree image
progressively with NFoV and text guidances joinly or individually. This
autoregressive scheme not only allows for deriving finer-grained and
text-consistent patterns by dynamically generating and adjusting the process
but also offers users greater flexibility to edit their conditions throughout
the generation process. A global-local conditioning mechanism is devised to
comprehensively formulate the outpainting guidance in each autoregressive step.
Text guidances, omni-visual cues, NFoV inputs and omni-geometry are encoded and
further formulated with cross-attention based transformers into a global stream
and a local stream into a conditioned generative backbone model. As AOG-Net is
compatible to leverage large-scale models for the conditional encoder and the
generative prior, it enables the generation to use extensive open-vocabulary
text guidances. Comprehensive experiments on two commonly used 360-degree image
datasets for both indoor and outdoor settings demonstrate the state-of-the-art
performance of our proposed method. Our code will be made publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Z/0/1/0/all/0/1&quot;&gt;Zhuqiang Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kun Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_L/0/1/0/all/0/1&quot;&gt;Lei Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03468">
<title>Cross-Image Context Matters for Bongard Problems. (arXiv:2309.03468v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03468</link>
<description rdf:parseType="Literal">&lt;p&gt;Current machine learning methods struggle to solve Bongard problems, which
are a type of IQ test that requires deriving an abstract &quot;concept&quot; from a set
of positive and negative &quot;support&quot; images, and then classifying whether or not
a new query image depicts the key concept. On Bongard-HOI, a benchmark for
natural-image Bongard problems, existing methods have only reached 66% accuracy
(where chance is 50%). Low accuracy is often attributed to neural nets&apos; lack of
ability to find human-like symbolic rules. In this work, we point out that many
existing methods are forfeiting accuracy due to a much simpler problem: they do
not incorporate information contained in the support set as a whole, and rely
instead on information extracted from individual supports. This is a critical
issue, because unlike in few-shot learning tasks concerning object
classification, the &quot;key concept&quot; in a typical Bongard problem can only be
distinguished using multiple positives and multiple negatives. We explore a
variety of simple methods to take this cross-image context into account, and
demonstrate substantial gains over prior methods, leading to new
state-of-the-art performance on Bongard-LOGO (75.3%) and Bongard-HOI (72.45%)
and strong performance on the original Bongard problem set (60.84%).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Raghuraman_N/0/1/0/all/0/1&quot;&gt;Nikhil Raghuraman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Harley_A/0/1/0/all/0/1&quot;&gt;Adam W. Harley&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guibas_L/0/1/0/all/0/1&quot;&gt;Leonidas Guibas&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03469">
<title>Fast FixMatch: Faster Semi-Supervised Learning with Curriculum Batch Size. (arXiv:2309.03469v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03469</link>
<description rdf:parseType="Literal">&lt;p&gt;Advances in Semi-Supervised Learning (SSL) have almost entirely closed the
gap between SSL and Supervised Learning at a fraction of the number of labels.
However, recent performance improvements have often come \textit{at the cost of
significantly increased training computation}. To address this, we propose
Curriculum Batch Size (CBS), \textit{an unlabeled batch size curriculum which
exploits the natural training dynamics of deep neural networks.} A small
unlabeled batch size is used in the beginning of training and is gradually
increased to the end of training. A fixed curriculum is used regardless of
dataset, model or number of epochs, and reduced training computations is
demonstrated on all settings. We apply CBS, strong labeled augmentation,
Curriculum Pseudo Labeling (CPL) \citep{FlexMatch} to FixMatch \citep{FixMatch}
and term the new SSL algorithm Fast FixMatch. We perform an ablation study to
show that strong labeled augmentation and/or CPL do not significantly reduce
training computations, but, in synergy with CBS, they achieve optimal
performance. Fast FixMatch also achieves substantially higher data utilization
compared to previous state-of-the-art. Fast FixMatch achieves between
$2.1\times$ - $3.4\times$ reduced training computations on CIFAR-10 with all
but 40, 250 and 4000 labels removed, compared to vanilla FixMatch, while
attaining the same cited state-of-the-art error rate \citep{FixMatch}. Similar
results are achieved for CIFAR-100, SVHN and STL-10. Finally, Fast MixMatch
achieves between $2.6\times$ - $3.3\times$ reduced training computations in
federated SSL tasks and online/streaming learning SSL tasks, which further
demonstrate the generializbility of Fast MixMatch to different scenarios and
tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;John Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dun_C/0/1/0/all/0/1&quot;&gt;Chen Dun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kyrillidis_A/0/1/0/all/0/1&quot;&gt;Anastasios Kyrillidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03475">
<title>InteractionNet: Joint Planning and Prediction for Autonomous Driving with Transformers. (arXiv:2309.03475v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.03475</link>
<description rdf:parseType="Literal">&lt;p&gt;Planning and prediction are two important modules of autonomous driving and
have experienced tremendous advancement recently. Nevertheless, most existing
methods regard planning and prediction as independent and ignore the
correlation between them, leading to the lack of consideration for interaction
and dynamic changes of traffic scenarios. To address this challenge, we propose
InteractionNet, which leverages transformer to share global contextual
reasoning among all traffic participants to capture interaction and
interconnect planning and prediction to achieve joint. Besides, InteractionNet
deploys another transformer to help the model pay extra attention to the
perceived region containing critical or unseen vehicles. InteractionNet
outperforms other baselines in several benchmarks, especially in terms of
safety, which benefits from the joint consideration of planning and
forecasting. The code will be available at
https://github.com/fujiawei0724/InteractionNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jiawei Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_Y/0/1/0/all/0/1&quot;&gt;Yanqing Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jian_Z/0/1/0/all/0/1&quot;&gt;Zhiqiang Jian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shitao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xin_J/0/1/0/all/0/1&quot;&gt;Jingmin Xin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_N/0/1/0/all/0/1&quot;&gt;Nanning Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03506">
<title>Towards Robust Natural-Looking Mammography Lesion Synthesis on Ipsilateral Dual-Views Breast Cancer Analysis. (arXiv:2309.03506v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03506</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, many mammographic image analysis methods have been
introduced for improving cancer classification tasks. Two major issues of
mammogram classification tasks are leveraging multi-view mammographic
information and class-imbalance handling. In the first problem, many multi-view
methods have been released for concatenating features of two or more views for
the training and inference stage. Having said that, most multi-view existing
methods are not explainable in the meaning of feature fusion, and treat many
views equally for diagnosing. Our work aims to propose a simple but novel
method for enhancing examined view (main view) by leveraging low-level feature
information from the auxiliary view (ipsilateral view) before learning the
high-level feature that contains the cancerous features. For the second issue,
we also propose a simple but novel malignant mammogram synthesis framework for
upsampling minor class samples. Our easy-to-implement and no-training framework
has eliminated the current limitation of the CutMix algorithm which is
unreliable synthesized images with random pasted patches, hard-contour
problems, and domain shift problems. Our results on VinDr-Mammo and CMMD
datasets show the effectiveness of our two new frameworks for both multi-view
training and synthesizing mammographic images, outperforming the previous
conventional methods in our experimental settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_T/0/1/0/all/0/1&quot;&gt;Thanh-Huy Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kha_Q/0/1/0/all/0/1&quot;&gt;Quang Hien Kha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Truong_T/0/1/0/all/0/1&quot;&gt;Thai Ngoc Toan Truong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lam_B/0/1/0/all/0/1&quot;&gt;Ba Thinh Lam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ngo_B/0/1/0/all/0/1&quot;&gt;Ba Hung Ngo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dinh_Q/0/1/0/all/0/1&quot;&gt;Quang Vinh Dinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Le_N/0/1/0/all/0/1&quot;&gt;Nguyen Quoc Khanh Le&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03517">
<title>Parameterized Aspects of Distinct Kemeny Rank Aggregation. (arXiv:2309.03517v1 [cs.DS])</title>
<link>http://arxiv.org/abs/2309.03517</link>
<description rdf:parseType="Literal">&lt;p&gt;The Kemeny method is one of the popular tools for rank aggregation. However,
computing an optimal Kemeny ranking is NP-hard. Consequently, the computational
task of finding a Kemeny ranking has been studied under the lens of
parameterized complexity with respect to many parameters. We first present a
comprehensive relationship, both theoretical and empirical, among these
parameters. Further, we study the problem of computing all distinct Kemeny
rankings under the lens of parameterized complexity. We consider the target
Kemeny score, number of candidates, average distance of input rankings, maximum
range of any candidate, and unanimity width as our parameters. For all these
parameters, we already have FPT algorithms. We find that any desirable number
of Kemeny rankings can also be found without substantial increase in running
time. We also present FPT approximation algorithms for Kemeny rank aggregation
with respect to these parameters.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+De_K/0/1/0/all/0/1&quot;&gt;Koustav De&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mittal_H/0/1/0/all/0/1&quot;&gt;Harshil Mittal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dey_P/0/1/0/all/0/1&quot;&gt;Palash Dey&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Misra_N/0/1/0/all/0/1&quot;&gt;Neeldhara Misra&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03523">
<title>DGC: Training Dynamic Graphs with Spatio-Temporal Non-Uniformity using Graph Partitioning by Chunks. (arXiv:2309.03523v1 [cs.DC])</title>
<link>http://arxiv.org/abs/2309.03523</link>
<description rdf:parseType="Literal">&lt;p&gt;Dynamic Graph Neural Network (DGNN) has shown a strong capability of learning
dynamic graphs by exploiting both spatial and temporal features. Although DGNN
has recently received considerable attention by AI community and various DGNN
models have been proposed, building a distributed system for efficient DGNN
training is still challenging. It has been well recognized that how to
partition the dynamic graph and assign workloads to multiple GPUs plays a
critical role in training acceleration. Existing works partition a dynamic
graph into snapshots or temporal sequences, which only work well when the graph
has uniform spatio-temporal structures. However, dynamic graphs in practice are
not uniformly structured, with some snapshots being very dense while others are
sparse. To address this issue, we propose DGC, a distributed DGNN training
system that achieves a 1.25x - 7.52x speedup over the state-of-the-art in our
testbed. DGC&apos;s success stems from a new graph partitioning method that
partitions dynamic graphs into chunks, which are essentially subgraphs with
modest training workloads and few inter connections. This partitioning
algorithm is based on graph coarsening, which can run very fast on large
graphs. In addition, DGC has a highly efficient run-time, powered by the
proposed chunk fusion and adaptive stale aggregation techniques. Extensive
experimental results on 3 typical DGNN models and 4 popular dynamic graph
datasets are presented to show the effectiveness of DGC.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_F/0/1/0/all/0/1&quot;&gt;Fahao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Peng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_C/0/1/0/all/0/1&quot;&gt;Celimuge Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03549">
<title>Reuse and Diffuse: Iterative Denoising for Text-to-Video Generation. (arXiv:2309.03549v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03549</link>
<description rdf:parseType="Literal">&lt;p&gt;Inspired by the remarkable success of Latent Diffusion Models (LDMs) for
image synthesis, we study LDM for text-to-video generation, which is a
formidable challenge due to the computational and memory constraints during
both model training and inference. A single LDM is usually only capable of
generating a very limited number of video frames. Some existing works focus on
separate prediction models for generating more video frames, which suffer from
additional training cost and frame-level jittering, however. In this paper, we
propose a framework called &quot;Reuse and Diffuse&quot; dubbed $\textit{VidRD}$ to
produce more frames following the frames already generated by an LDM.
Conditioned on an initial video clip with a small number of frames, additional
frames are iteratively generated by reusing the original latent features and
following the previous diffusion process. Besides, for the autoencoder used for
translation between pixel space and latent space, we inject temporal layers
into its decoder and fine-tune these layers for higher temporal consistency. We
also propose a set of strategies for composing video-text data that involve
diverse content from multiple existing datasets including video datasets for
action recognition and image-text datasets. Extensive experiments show that our
method achieves good results in both quantitative and qualitative evaluations.
Our project page is available
$\href{https://anonymous0x233.github.io/ReuseAndDiffuse/}{here}$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_J/0/1/0/all/0/1&quot;&gt;Jiaxi Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shicong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haoyu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_T/0/1/0/all/0/1&quot;&gt;Tianyi Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zuxuan Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_S/0/1/0/all/0/1&quot;&gt;Songcen Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Wei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Y/0/1/0/all/0/1&quot;&gt;Yu-Gang Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_H/0/1/0/all/0/1&quot;&gt;Hang Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03579">
<title>DTW+S: Shape-based Comparison of Time-series with Ordered Local Trend. (arXiv:2309.03579v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03579</link>
<description rdf:parseType="Literal">&lt;p&gt;Measuring distance or similarity between time-series data is a fundamental
aspect of many applications including classification and clustering. Existing
measures may fail to capture similarities due to local trends (shapes) and may
even produce misleading results. Our goal is to develop a measure that looks
for similar trends occurring around similar times and is easily interpretable
for researchers in applied domains. This is particularly useful for
applications where time-series have a sequence of meaningful local trends that
are ordered, such as in epidemics (a surge to an increase to a peak to a
decrease). We propose a novel measure, DTW+S, which creates an interpretable
&quot;closeness-preserving&quot; matrix representation of the time-series, where each
column represents local trends, and then it applies Dynamic Time Warping to
compute distances between these matrices. We present a theoretical analysis
that supports the choice of this representation. We demonstrate the utility of
DTW+S in ensemble building and clustering of epidemic curves. We also
demonstrate that our approach results in better classification compared to
Dynamic Time Warping for a class of datasets, particularly when local trends
rather than scale play a decisive role.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Srivastava_A/0/1/0/all/0/1&quot;&gt;Ajitesh Srivastava&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03581">
<title>Interactive Hyperparameter Optimization in Multi-Objective Problems via Preference Learning. (arXiv:2309.03581v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03581</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperparameter optimization (HPO) is important to leverage the full potential
of machine learning (ML). In practice, users are often interested in
multi-objective (MO) problems, i.e., optimizing potentially conflicting
objectives, like accuracy and energy consumption. To tackle this, the vast
majority of MO-ML algorithms return a Pareto front of non-dominated machine
learning models to the user. Optimizing the hyperparameters of such algorithms
is non-trivial as evaluating a hyperparameter configuration entails evaluating
the quality of the resulting Pareto front. In literature, there are known
indicators that assess the quality of a Pareto front (e.g., hypervolume, R2) by
quantifying different properties (e.g., volume, proximity to a reference
point). However, choosing the indicator that leads to the desired Pareto front
might be a hard task for a user. In this paper, we propose a human-centered
interactive HPO approach tailored towards multi-objective ML leveraging
preference learning to extract desiderata from users that guide the
optimization. Instead of relying on the user guessing the most suitable
indicator for their needs, our approach automatically learns an appropriate
indicator. Concretely, we leverage pairwise comparisons of distinct Pareto
fronts to learn such an appropriate quality indicator. Then, we optimize the
hyperparameters of the underlying MO-ML algorithm towards this learned
indicator using a state-of-the-art HPO approach. In an experimental study
targeting the environmental impact of ML, we demonstrate that our approach
leads to substantially better Pareto fronts compared to optimizing based on a
wrong indicator pre-selected by the user, and performs comparable in the case
of an advanced user knowing which indicator to pick.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Giovanelli_J/0/1/0/all/0/1&quot;&gt;Joseph Giovanelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tornede_A/0/1/0/all/0/1&quot;&gt;Alexander Tornede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tornede_T/0/1/0/all/0/1&quot;&gt;Tanja Tornede&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindauer_M/0/1/0/all/0/1&quot;&gt;Marius Lindauer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03590">
<title>Spatial encoding of BOLD fMRI time series for categorizing static images across visual datasets: A pilot study on human vision. (arXiv:2309.03590v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03590</link>
<description rdf:parseType="Literal">&lt;p&gt;Functional MRI (fMRI) is widely used to examine brain functionality by
detecting alteration in oxygenated blood flow that arises with brain activity.
In this study, complexity specific image categorization across different visual
datasets is performed using fMRI time series (TS) to understand differences in
neuronal activities related to vision. Publicly available BOLD5000 dataset is
used for this purpose, containing fMRI scans while viewing 5254 images of
diverse categories, drawn from three standard computer vision datasets: COCO,
ImageNet and SUN. To understand vision, it is important to study how brain
functions while looking at different images. To achieve this, spatial encoding
of fMRI BOLD TS has been performed that uses classical Gramian Angular Field
(GAF) and Markov Transition Field (MTF) to obtain 2D BOLD TS, representing
images of COCO, Imagenet and SUN. For classification, individual GAF and MTF
features are fed into regular CNN. Subsequently, parallel CNN model is employed
that uses combined 2D features for classifying images across COCO, Imagenet and
SUN. The result of 2D CNN models is also compared with 1D LSTM and Bi-LSTM that
utilizes raw fMRI BOLD signal for classification. It is seen that parallel CNN
model outperforms other network models with an improvement of 7% for
multi-class classification. Clinical relevance- The obtained result of this
analysis establishes a baseline in studying how differently human brain
functions while looking at images of diverse complexities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kancharala_V/0/1/0/all/0/1&quot;&gt;Vamshi K. Kancharala&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bhattacharya_D/0/1/0/all/0/1&quot;&gt;Debanjali Bhattacharya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sinha_N/0/1/0/all/0/1&quot;&gt;Neelam Sinha&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03613">
<title>Evaluating ChatGPT as a Recommender System: A Rigorous Approach. (arXiv:2309.03613v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2309.03613</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent popularity surrounds large AI language models due to their impressive
natural language capabilities. They contribute significantly to
language-related tasks, including prompt-based learning, making them valuable
for various specific tasks. This approach unlocks their full potential,
enhancing precision and generalization. Research communities are actively
exploring their applications, with ChatGPT receiving recognition. Despite
extensive research on large language models, their potential in recommendation
scenarios still needs to be explored. This study aims to fill this gap by
investigating ChatGPT&apos;s capabilities as a zero-shot recommender system. Our
goals include evaluating its ability to use user preferences for
recommendations, reordering existing recommendation lists, leveraging
information from similar users, and handling cold-start situations. We assess
ChatGPT&apos;s performance through comprehensive experiments using three datasets
(MovieLens Small, Last.FM, and Facebook Book). We compare ChatGPT&apos;s performance
against standard recommendation algorithms and other large language models,
such as GPT-3.5 and PaLM-2. To measure recommendation effectiveness, we employ
widely-used evaluation metrics like Mean Average Precision (MAP), Recall,
Precision, F1, normalized Discounted Cumulative Gain (nDCG), Item Coverage,
Expected Popularity Complement (EPC), Average Coverage of Long Tail (ACLT),
Average Recommendation Popularity (ARP), and Popularity-based Ranking-based
Equal Opportunity (PopREO). Through thoroughly exploring ChatGPT&apos;s abilities in
recommender systems, our study aims to contribute to the growing body of
research on the versatility and potential applications of large language
models. Our experiment code is available on the GitHub repository:
https://github.com/sisinflab/Recommender-ChatGPT
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Palma_D/0/1/0/all/0/1&quot;&gt;Dario Di Palma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biancofiore_G/0/1/0/all/0/1&quot;&gt;Giovanni Maria Biancofiore&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anelli_V/0/1/0/all/0/1&quot;&gt;Vito Walter Anelli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Narducci_F/0/1/0/all/0/1&quot;&gt;Fedelucio Narducci&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Noia_T/0/1/0/all/0/1&quot;&gt;Tommaso Di Noia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sciascio_E/0/1/0/all/0/1&quot;&gt;Eugenio Di Sciascio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03617">
<title>NeuroCodeBench: a plain C neural network benchmark for software verification. (arXiv:2309.03617v1 [cs.SE])</title>
<link>http://arxiv.org/abs/2309.03617</link>
<description rdf:parseType="Literal">&lt;p&gt;Safety-critical systems with neural network components require strong
guarantees. While existing neural network verification techniques have shown
great progress towards this goal, they cannot prove the absence of software
faults in the network implementation. This paper presents NeuroCodeBench - a
verification benchmark for neural network code written in plain C. It contains
32 neural networks with 607 safety properties divided into 6 categories: maths
library, activation functions, error-correcting networks, transfer function
approximation, probability density estimation and reinforcement learning. Our
preliminary evaluation shows that state-of-the-art software verifiers struggle
to provide correct verdicts, due to their incomplete support of the standard C
mathematical library and the complexity of larger neural networks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manino_E/0/1/0/all/0/1&quot;&gt;Edoardo Manino&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Menezes_R/0/1/0/all/0/1&quot;&gt;Rafael S&amp;#xe1; Menezes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shmarov_F/0/1/0/all/0/1&quot;&gt;Fedor Shmarov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cordeiro_L/0/1/0/all/0/1&quot;&gt;Lucas C. Cordeiro&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03638">
<title>Beyond XAI:Obstacles Towards Responsible AI. (arXiv:2309.03638v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03638</link>
<description rdf:parseType="Literal">&lt;p&gt;The rapidly advancing domain of Explainable Artificial Intelligence (XAI) has
sparked significant interests in developing techniques to make AI systems more
transparent and understandable. Nevertheless, in real-world contexts, the
methods of explainability and their evaluation strategies present numerous
limitations.Moreover, the scope of responsible AI extends beyond just
explainability. In this paper, we explore these limitations and discuss their
implications in a boarder context of responsible AI when considering other
important aspects, including privacy, fairness and contestability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pi_Y/0/1/0/all/0/1&quot;&gt;Yulu Pi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03645">
<title>VideolandGPT: A User Study on a Conversational Recommender System. (arXiv:2309.03645v1 [cs.IR])</title>
<link>http://arxiv.org/abs/2309.03645</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper investigates how large language models (LLMs) can enhance
recommender systems, with a specific focus on Conversational Recommender
Systems that leverage user preferences and personalised candidate selections
from existing ranking models. We introduce VideolandGPT, a recommender system
for a Video-on-Demand (VOD) platform, Videoland, which uses ChatGPT to select
from a predetermined set of contents, considering the additional context
indicated by users&apos; interactions with a chat interface. We evaluate ranking
metrics, user experience, and fairness of recommendations, comparing a
personalised and a non-personalised version of the system, in a between-subject
user study. Our results indicate that the personalised version outperforms the
non-personalised in terms of accuracy and general user satisfaction, while both
versions increase the visibility of items which are not in the top of the
recommendation lists. However, both versions present inconsistent behavior in
terms of fairness, as the system may generate recommendations which are not
available on Videoland.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Granada_M/0/1/0/all/0/1&quot;&gt;Mateo Gutierrez Granada&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zilbershtein_D/0/1/0/all/0/1&quot;&gt;Dina Zilbershtein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Odijk_D/0/1/0/all/0/1&quot;&gt;Daan Odijk&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barile_F/0/1/0/all/0/1&quot;&gt;Francesco Barile&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03648">
<title>Characterizing Lipschitz Stability of GNN for Fairness. (arXiv:2309.03648v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03648</link>
<description rdf:parseType="Literal">&lt;p&gt;The Lipschitz bound, a technique from robust statistics, can limit the
maximum changes in the output concerning the input, taking into account
associated irrelevant biased factors. It is an efficient and provable method
for examining the output stability of machine learning models without incurring
additional computation costs. Recently, Graph Neural Networks (GNNs), which
operate on non-Euclidean data, have gained significant attention. However, no
previous research has investigated the GNN Lipschitz bounds to shed light on
stabilizing model outputs, especially when working on non-Euclidean data with
inherent biases. Given the inherent biases in common graph data used for GNN
training, it poses a serious challenge to constraining the GNN output
perturbations induced by input biases, thereby safeguarding fairness during
training. Recently, despite the Lipschitz constant&apos;s use in controlling the
stability of Euclideanneural networks, the calculation of the precise Lipschitz
constant remains elusive for non-Euclidean neural networks like GNNs,
especially within fairness contexts. To narrow this gap, we begin with the
general GNNs operating on an attributed graph, and formulate a Lipschitz bound
to limit the changes in the output regarding biases associated with the input.
Additionally, we theoretically analyze how the Lipschitz constant of a GNN
model could constrain the output perturbations induced by biases learned from
data for fairness training. We experimentally validate the Lipschitz bound&apos;s
effectiveness in limiting biases of the model output. Finally, from a training
dynamics perspective, we demonstrate why the theoretical Lipschitz bound can
effectively guide the GNN training to better trade-off between accuracy and
fairness.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Y/0/1/0/all/0/1&quot;&gt;Yaning Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chunhui Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jundong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chuxu Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03651">
<title>Learning of Generalizable and Interpretable Knowledge in Grid-Based Reinforcement Learning Environments. (arXiv:2309.03651v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03651</link>
<description rdf:parseType="Literal">&lt;p&gt;Understanding the interactions of agents trained with deep reinforcement
learning is crucial for deploying agents in games or the real world. In the
former, unreasonable actions confuse players. In the latter, that effect is
even more significant, as unexpected behavior cause accidents with potentially
grave and long-lasting consequences for the involved individuals. In this work,
we propose using program synthesis to imitate reinforcement learning policies
after seeing a trajectory of the action sequence. Programs have the advantage
that they are inherently interpretable and verifiable for correctness. We adapt
the state-of-the-art program synthesis system DreamCoder for learning concepts
in grid-based environments, specifically, a navigation task and two miniature
versions of Atari games, Space Invaders and Asterix. By inspecting the
generated libraries, we can make inferences about the concepts the black-box
agent has learned and better understand the agent&apos;s behavior. We achieve the
same by visualizing the agent&apos;s decision-making process for the imitated
sequences. We evaluate our approach with different types of program
synthesizers based on a search-only method, a neural-guided search, and a
language model fine-tuned on code.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eberhardinger_M/0/1/0/all/0/1&quot;&gt;Manuel Eberhardinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maucher_J/0/1/0/all/0/1&quot;&gt;Johannes Maucher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maghsudi_S/0/1/0/all/0/1&quot;&gt;Setareh Maghsudi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03652">
<title>Anatomy-informed Data Augmentation for Enhanced Prostate Cancer Detection. (arXiv:2309.03652v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2309.03652</link>
<description rdf:parseType="Literal">&lt;p&gt;Data augmentation (DA) is a key factor in medical image analysis, such as in
prostate cancer (PCa) detection on magnetic resonance images. State-of-the-art
computer-aided diagnosis systems still rely on simplistic spatial
transformations to preserve the pathological label post transformation.
However, such augmentations do not substantially increase the organ as well as
tumor shape variability in the training set, limiting the model&apos;s ability to
generalize to unseen cases with more diverse localized soft-tissue
deformations. We propose a new anatomy-informed transformation that leverages
information from adjacent organs to simulate typical physiological deformations
of the prostate and generates unique lesion shapes without altering their
label. Due to its lightweight computational requirements, it can be easily
integrated into common DA frameworks. We demonstrate the effectiveness of our
augmentation on a dataset of 774 biopsy-confirmed examinations, by evaluating a
state-of-the-art method for PCa detection with different augmentation settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kovacs_B/0/1/0/all/0/1&quot;&gt;Balint Kovacs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Netzer_N/0/1/0/all/0/1&quot;&gt;Nils Netzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Baumgartner_M/0/1/0/all/0/1&quot;&gt;Michael Baumgartner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Eith_C/0/1/0/all/0/1&quot;&gt;Carolin Eith&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bounias_D/0/1/0/all/0/1&quot;&gt;Dimitrios Bounias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meinzer_C/0/1/0/all/0/1&quot;&gt;Clara Meinzer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jaeger_P/0/1/0/all/0/1&quot;&gt;Paul F. Jaeger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kevin S. Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Floca_R/0/1/0/all/0/1&quot;&gt;Ralf Floca&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schrader_A/0/1/0/all/0/1&quot;&gt;Adrian Schrader&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Isensee_F/0/1/0/all/0/1&quot;&gt;Fabian Isensee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gnirs_R/0/1/0/all/0/1&quot;&gt;Regula Gnirs&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Goertz_M/0/1/0/all/0/1&quot;&gt;Magdalena Goertz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schuetz_V/0/1/0/all/0/1&quot;&gt;Viktoria Schuetz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Stenzinger_A/0/1/0/all/0/1&quot;&gt;Albrecht Stenzinger&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hohenfellner_M/0/1/0/all/0/1&quot;&gt;Markus Hohenfellner&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schlemmer_H/0/1/0/all/0/1&quot;&gt;Heinz-Peter Schlemmer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wolf_I/0/1/0/all/0/1&quot;&gt;Ivo Wolf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bonekamp_D/0/1/0/all/0/1&quot;&gt;David Bonekamp&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Maier_Hein_K/0/1/0/all/0/1&quot;&gt;Klaus H. Maier-Hein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03659">
<title>Towards Comparable Knowledge Distillation in Semantic Image Segmentation. (arXiv:2309.03659v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03659</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge Distillation (KD) is one proposed solution to large model sizes and
slow inference speed in semantic segmentation. In our research we identify 25
proposed distillation loss terms from 14 publications in the last 4 years.
Unfortunately, a comparison of terms based on published results is often
impossible, because of differences in training configurations. A good
illustration of this problem is the comparison of two publications from 2022.
Using the same models and dataset, Structural and Statistical Texture
Distillation (SSTKD) reports an increase of student mIoU of 4.54 and a final
performance of 29.19, while Adaptive Perspective Distillation (APD) only
improves student performance by 2.06 percentage points, but achieves a final
performance of 39.25. The reason for such extreme differences is often a
suboptimal choice of hyperparameters and a resulting underperformance of the
student model used as reference point. In our work, we reveal problems of
insufficient hyperparameter tuning by showing that distillation improvements of
two widely accepted frameworks, SKD and IFVD, vanish when hyperparameters are
optimized sufficiently. To improve comparability of future research in the
field, we establish a solid baseline for three datasets and two student models
and provide extensive information on hyperparameter tuning. We find that only
two out of eight techniques can compete with our simple baseline on the ADE20K
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niemann_O/0/1/0/all/0/1&quot;&gt;Onno Niemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vox_C/0/1/0/all/0/1&quot;&gt;Christopher Vox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Werner_T/0/1/0/all/0/1&quot;&gt;Thorben Werner&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03665">
<title>How adversarial attacks can disrupt seemingly stable accurate classifiers. (arXiv:2309.03665v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03665</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial attacks dramatically change the output of an otherwise accurate
learning system using a seemingly inconsequential modification to a piece of
input data. Paradoxically, empirical evidence indicates that even systems which
are robust to large random perturbations of the input data remain susceptible
to small, easily constructed, adversarial perturbations of their inputs. Here,
we show that this may be seen as a fundamental feature of classifiers working
with high dimensional input data. We introduce a simple generic and
generalisable framework for which key behaviours observed in practical systems
arise with high probability -- notably the simultaneous susceptibility of the
(otherwise accurate) model to easily constructed adversarial attacks, and
robustness to random perturbations of the input data. We confirm that the same
phenomena are directly observed in practical neural networks trained on
standard image classification problems, where even large additive random noise
fails to trigger the adversarial instability of the network. A surprising
takeaway is that even small margins separating a classifier&apos;s decision surface
from training and testing data can hide adversarial susceptibility from being
detected using randomly sampled perturbations. Counterintuitively, using
additive noise during training or testing is therefore inefficient for
eradicating or detecting adversarial examples, and more demanding adversarial
training is required.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutton_O/0/1/0/all/0/1&quot;&gt;Oliver J. Sutton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qinghua Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tyukin_I/0/1/0/all/0/1&quot;&gt;Ivan Y. Tyukin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gorban_A/0/1/0/all/0/1&quot;&gt;Alexander N. Gorban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bastounis_A/0/1/0/all/0/1&quot;&gt;Alexander Bastounis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Higham_D/0/1/0/all/0/1&quot;&gt;Desmond J. Higham&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03671">
<title>Dataset Generation and Bonobo Classification from Weakly Labelled Videos. (arXiv:2309.03671v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03671</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents a bonobo detection and classification pipeline built from
the commonly used machine learning methods. Such application is motivated by
the need to test bonobos in their enclosure using touch screen devices without
human assistance. This work introduces a newly acquired dataset based on bonobo
recordings generated semi-automatically. The recordings are weakly labelled and
fed to a macaque detector in order to spatially detect the individual present
in the video. Handcrafted features coupled with different classification
algorithms and deep-learning methods using a ResNet architecture are
investigated for bonobo identification. Performance is compared in terms of
classification accuracy on the splits of the database using different data
separation methods. We demonstrate the importance of data preparation and how a
wrong data separation can lead to false good results. Finally, after a
meaningful separation of the data, the best classification performance is
obtained using a fine-tuned ResNet model and reaches 75% of accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Martin_P/0/1/0/all/0/1&quot;&gt;Pierre-Etienne Martin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03685">
<title>PyGraft: Configurable Generation of Schemas and Knowledge Graphs at Your Fingertips. (arXiv:2309.03685v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03685</link>
<description rdf:parseType="Literal">&lt;p&gt;Knowledge graphs (KGs) have emerged as a prominent data representation and
management paradigm. Being usually underpinned by a schema (e.g. an ontology),
KGs capture not only factual information but also contextual knowledge. In some
tasks, a few KGs established themselves as standard benchmarks. However, recent
works outline that relying on a limited collection of datasets is not
sufficient to assess the generalization capability of an approach. In some
data-sensitive fields such as education or medicine, access to public datasets
is even more limited. To remedy the aforementioned issues, we release PyGraft,
a Python-based tool that generates highly customized, domain-agnostic schemas
and knowledge graphs. The synthesized schemas encompass various RDFS and OWL
constructs, while the synthesized KGs emulate the characteristics and scale of
real-world KGs. Logical consistency of the generated resources is ultimately
ensured by running a description logic (DL) reasoner. By providing a way of
generating both a schema and KG in a single pipeline, PyGraft&apos;s aim is to
empower the generation of a more diverse array of KGs for benchmarking novel
approaches in areas such as graph-based machine learning (ML), or more
generally KG processing. In graph-based ML in particular, this should foster a
more holistic evaluation of model performance and generalization capability,
thereby going beyond the limited collection of available benchmarks. PyGraft is
available at: https://github.com/nicolas-hbt/pygraft.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hubert_N/0/1/0/all/0/1&quot;&gt;Nicolas Hubert&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monnin_P/0/1/0/all/0/1&quot;&gt;Pierre Monnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+dAquin_M/0/1/0/all/0/1&quot;&gt;Mathieu d&amp;#x27;Aquin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brun_A/0/1/0/all/0/1&quot;&gt;Armelle Brun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Monticolo_D/0/1/0/all/0/1&quot;&gt;Davy Monticolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03720">
<title>A Natural Gas Consumption Forecasting System for Continual Learning Scenarios based on Hoeffding Trees with Change Point Detection Mechanism. (arXiv:2309.03720v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03720</link>
<description rdf:parseType="Literal">&lt;p&gt;Forecasting natural gas consumption, considering seasonality and trends, is
crucial in planning its supply and consumption and optimizing the cost of
obtaining it, mainly by industrial entities. However, in times of threats to
its supply, it is also a critical element that guarantees the supply of this
raw material to meet individual consumers&apos; needs, ensuring society&apos;s energy
security. This article introduces a novel multistep ahead forecasting of
natural gas consumption with change point detection integration for model
collection selection with continual learning capabilities using data stream
processing. The performance of the forecasting models based on the proposed
approach is evaluated in a complex real-world use case of natural gas
consumption forecasting. We employed Hoeffding tree predictors as forecasting
models and the Pruned Exact Linear Time (PELT) algorithm for the change point
detection procedure. The change point detection integration enables selecting a
different model collection for successive time frames. Thus, three model
collection selection procedures (with and without an error feedback loop) are
defined and evaluated for forecasting scenarios with various densities of
detected change points. These models were compared with change point agnostic
baseline approaches. Our experiments show that fewer change points result in a
lower forecasting error regardless of the model collection selection procedure
employed. Also, simpler model collection selection procedures omitting
forecasting error feedback leads to more robust forecasting models suitable for
continual learning tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Svoboda_R/0/1/0/all/0/1&quot;&gt;Radek Svoboda&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Basterrech_S/0/1/0/all/0/1&quot;&gt;Sebastian Basterrech&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kozal_J/0/1/0/all/0/1&quot;&gt;J&amp;#x119;drzej Kozal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Platos_J/0/1/0/all/0/1&quot;&gt;Jan Plato&amp;#x161;&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wozniak_M/0/1/0/all/0/1&quot;&gt;Micha&amp;#x142; Wo&amp;#x17a;niak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03748">
<title>Enhancing Pipeline-Based Conversational Agents with Large Language Models. (arXiv:2309.03748v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.03748</link>
<description rdf:parseType="Literal">&lt;p&gt;The latest advancements in AI and deep learning have led to a breakthrough in
large language model (LLM)-based agents such as GPT-4. However, many commercial
conversational agent development tools are pipeline-based and have limitations
in holding a human-like conversation. This paper investigates the capabilities
of LLMs to enhance pipeline-based conversational agents during two phases: 1)
in the design and development phase and 2) during operations. In 1) LLMs can
aid in generating training data, extracting entities and synonyms,
localization, and persona design. In 2) LLMs can assist in contextualization,
intent classification to prevent conversational breakdown and handle
out-of-scope questions, auto-correcting utterances, rephrasing responses,
formulating disambiguation questions, summarization, and enabling closed
question-answering capabilities. We conducted informal experiments with GPT-4
in the private banking domain to demonstrate the scenarios above with a
practical example. Companies may be hesitant to replace their pipeline-based
agents with LLMs entirely due to privacy concerns and the need for deep
integration within their existing ecosystems. A hybrid approach in which LLMs&apos;
are integrated into the pipeline-based agents allows them to save time and
costs of building and running agents by capitalizing on the capabilities of
LLMs while retaining the integration and privacy safeguards of their existing
systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Foosherian_M/0/1/0/all/0/1&quot;&gt;Mina Foosherian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Purwins_H/0/1/0/all/0/1&quot;&gt;Hendrik Purwins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rathnayake_P/0/1/0/all/0/1&quot;&gt;Purna Rathnayake&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alam_T/0/1/0/all/0/1&quot;&gt;Touhidul Alam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Teimao_R/0/1/0/all/0/1&quot;&gt;Rui Teimao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thoben_K/0/1/0/all/0/1&quot;&gt;Klaus-Dieter Thoben&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03755">
<title>TSGBench: Time Series Generation Benchmark. (arXiv:2309.03755v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03755</link>
<description rdf:parseType="Literal">&lt;p&gt;Synthetic Time Series Generation (TSG) is crucial in a range of applications,
including data augmentation, anomaly detection, and privacy preservation.
Although significant strides have been made in this field, existing methods
exhibit three key limitations: (1) They often benchmark against similar model
types, constraining a holistic view of performance capabilities. (2) The use of
specialized synthetic and private datasets introduces biases and hampers
generalizability. (3) Ambiguous evaluation measures, often tied to custom
networks or downstream tasks, hinder consistent and fair comparison.
&lt;/p&gt;
&lt;p&gt;To overcome these limitations, we introduce \textsf{TSGBench}, the inaugural
TSG Benchmark, designed for a unified and comprehensive assessment of TSG
methods. It comprises three modules: (1) a curated collection of publicly
available, real-world datasets tailored for TSG, together with a standardized
preprocessing pipeline; (2) a comprehensive evaluation measures suite including
vanilla measures, new distance-based assessments, and visualization tools; (3)
a pioneering generalization test rooted in Domain Adaptation (DA), compatible
with all methods. We have conducted extensive experiments across ten real-world
datasets from diverse domains, utilizing ten advanced TSG methods and twelve
evaluation measures, all gauged through \textsf{TSGBench}. The results
highlight its remarkable efficacy and consistency. More importantly,
\textsf{TSGBench} delivers a statistical breakdown of method rankings,
illuminating performance variations across different datasets and measures, and
offering nuanced insights into the effectiveness of each method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ang_Y/0/1/0/all/0/1&quot;&gt;Yihao Ang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Y/0/1/0/all/0/1&quot;&gt;Yifan Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tung_A/0/1/0/all/0/1&quot;&gt;Anthony K. H. Tung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhiyong Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03758">
<title>Hybrid of representation learning and reinforcement learning for dynamic and complex robotic motion planning. (arXiv:2309.03758v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2309.03758</link>
<description rdf:parseType="Literal">&lt;p&gt;Motion planning is the soul of robot decision making. Classical planning
algorithms like graph search and reaction-based algorithms face challenges in
cases of dense and dynamic obstacles. Deep learning algorithms generate
suboptimal one-step predictions that cause many collisions. Reinforcement
learning algorithms generate optimal or near-optimal time-sequential
predictions. However, they suffer from slow convergence, suboptimal converged
results, and overfittings. This paper introduces a hybrid algorithm for robotic
motion planning: long short-term memory (LSTM) pooling and skip connection for
attention-based discrete soft actor critic (LSA-DSAC). First, graph network
(relational graph) and attention network (attention weight) interpret the
environmental state for the learning of the discrete soft actor critic
algorithm. The expressive power of attention network outperforms that of graph
in our task by difference analysis of these two representation methods.
However, attention based DSAC faces the overfitting problem in training.
Second, the skip connection method is integrated to attention based DSAC to
mitigate overfitting and improve convergence speed. Third, LSTM pooling is
taken to replace the sum operator of attention weigh and eliminate overfitting
by slightly sacrificing convergence speed at early-stage training. Experiments
show that LSA-DSAC outperforms the state-of-the-art in training and most
evaluations. The physical robot is also implemented and tested in the real
world.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Chengmin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_J/0/1/0/all/0/1&quot;&gt;Jiapeng Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_B/0/1/0/all/0/1&quot;&gt;Bingding Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaoxu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Franti_P/0/1/0/all/0/1&quot;&gt;Pasi Fr&amp;#xe4;nti&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03773">
<title>Extending Transductive Knowledge Graph Embedding Models for Inductive Logical Relational Inference. (arXiv:2309.03773v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2309.03773</link>
<description rdf:parseType="Literal">&lt;p&gt;Many downstream inference tasks for knowledge graphs, such as relation
prediction, have been handled successfully by knowledge graph embedding
techniques in the transductive setting. To address the inductive setting
wherein new entities are introduced into the knowledge graph at inference time,
more recent work opts for models which learn implicit representations of the
knowledge graph through a complex function of a network&apos;s subgraph structure,
often parametrized by graph neural network architectures. These come at the
cost of increased parametrization, reduced interpretability and limited
generalization to other downstream inference tasks. In this work, we bridge the
gap between traditional transductive knowledge graph embedding approaches and
more recent inductive relation prediction models by introducing a generalized
form of harmonic extension which leverages representations learned through
transductive embedding methods to infer representations of new entities
introduced at inference time as in the inductive setting. This harmonic
extension technique provides the best such approximation, can be implemented
via an efficient iterative scheme, and can be employed to answer a family of
conjunctive logical queries over the knowledge graph, further expanding the
capabilities of transductive embedding methods. In experiments on a number of
large-scale knowledge graph embedding benchmarks, we find that this approach
for extending the functionality of transductive knowledge graph embedding
models to perform knowledge graph completion and answer logical queries in the
inductive setting is competitive with--and in some scenarios
outperforms--several state-of-the-art models derived explicitly for such
inductive tasks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gebhart_T/0/1/0/all/0/1&quot;&gt;Thomas Gebhart&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cobb_J/0/1/0/all/0/1&quot;&gt;John Cobb&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03779">
<title>CPU frequency scheduling of real-time applications on embedded devices with temporal encoding-based deep reinforcement learning. (arXiv:2309.03779v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03779</link>
<description rdf:parseType="Literal">&lt;p&gt;Small devices are frequently used in IoT and smart-city applications to
perform periodic dedicated tasks with soft deadlines. This work focuses on
developing methods to derive efficient power-management methods for periodic
tasks on small devices. We first study the limitations of the existing Linux
built-in methods used in small devices. We illustrate three typical
workload/system patterns that are challenging to manage with Linux&apos;s built-in
solutions. We develop a reinforcement-learning-based technique with temporal
encoding to derive an effective DVFS governor even with the presence of the
three system patterns. The derived governor uses only one performance counter,
the same as the built-in Linux mechanism, and does not require an explicit task
model for the workload. We implemented a prototype system on the Nvidia Jetson
Nano Board and experimented with it with six applications, including two
self-designed and four benchmark applications. Under different deadline
constraints, our approach can quickly derive a DVFS governor that can adapt to
performance requirements and outperform the built-in Linux approach in energy
saving. On Mibench workloads, with performance slack ranging from 0.04 s to 0.4
s, the proposed method can save 3% - 11% more energy compared to Ondemand.
AudioReg and FaceReg applications tested have 5%- 14% energy-saving
improvement. We have open-sourced the implementation of our in-kernel quantized
neural network engine. The codebase can be found at:
https://github.com/coladog/tinyagent.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_T/0/1/0/all/0/1&quot;&gt;Ti Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Man Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03799">
<title>FisheyePP4AV: A privacy-preserving method for autonomous vehicles on fisheye camera images. (arXiv:2309.03799v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03799</link>
<description rdf:parseType="Literal">&lt;p&gt;In many parts of the world, the use of vast amounts of data collected on
public roadways for autonomous driving has increased. In order to detect and
anonymize pedestrian faces and nearby car license plates in actual road-driving
scenarios, there is an urgent need for effective solutions. As more data is
collected, privacy concerns regarding it increase, including but not limited to
pedestrian faces and surrounding vehicle license plates. Normal and fisheye
cameras are the two common camera types that are typically mounted on
collection vehicles. With complex camera distortion models, fisheye camera
images were deformed in contrast to regular images. It causes computer vision
tasks to perform poorly when using numerous deep learning models. In this work,
we pay particular attention to protecting privacy while yet adhering to several
laws for fisheye camera photos taken by driverless vehicles. First, we suggest
a framework for extracting face and plate identification knowledge from several
teacher models. Our second suggestion is to transform both the image and the
label from a regular image to fisheye-like data using a varied and realistic
fisheye transformation. Finally, we run a test using the open-source PP4AV
dataset. The experimental findings demonstrated that our model outperformed
baseline methods when trained on data from autonomous vehicles, even when the
data were softly labeled. The implementation code is available at our github:
https://github.com/khaclinh/FisheyePP4AV.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Trinh_L/0/1/0/all/0/1&quot;&gt;Linh Trinh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ha_B/0/1/0/all/0/1&quot;&gt;Bach Ha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tran_T/0/1/0/all/0/1&quot;&gt;Tu Tran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03800">
<title>Pareto Frontiers in Neural Feature Learning: Data, Compute, Width, and Luck. (arXiv:2309.03800v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03800</link>
<description rdf:parseType="Literal">&lt;p&gt;This work investigates the nuanced algorithm design choices for deep learning
in the presence of computational-statistical gaps. We begin by considering
offline sparse parity learning, a supervised classification problem which
admits a statistical query lower bound for gradient-based training of a
multilayer perceptron. This lower bound can be interpreted as a multi-resource
tradeoff frontier: successful learning can only occur if one is sufficiently
rich (large model), knowledgeable (large dataset), patient (many training
iterations), or lucky (many random guesses). We show, theoretically and
experimentally, that sparse initialization and increasing network width yield
significant improvements in sample efficiency in this setting. Here, width
plays the role of parallel search: it amplifies the probability of finding
&quot;lottery ticket&quot; neurons, which learn sparse features more sample-efficiently.
Finally, we show that the synthetic sparse parity task can be useful as a proxy
for real problems requiring axis-aligned feature learning. We demonstrate
improved sample efficiency on tabular classification benchmarks by using wide,
sparsely-initialized MLP models; these networks sometimes outperform tuned
random forests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Edelman_B/0/1/0/all/0/1&quot;&gt;Benjamin L. Edelman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Goel_S/0/1/0/all/0/1&quot;&gt;Surbhi Goel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kakade_S/0/1/0/all/0/1&quot;&gt;Sham Kakade&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malach_E/0/1/0/all/0/1&quot;&gt;Eran Malach&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Cyril Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03812">
<title>AnthroNet: Conditional Generation of Humans via Anthropometrics. (arXiv:2309.03812v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03812</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel human body model formulated by an extensive set of
anthropocentric measurements, which is capable of generating a wide range of
human body shapes and poses. The proposed model enables direct modeling of
specific human identities through a deep generative architecture, which can
produce humans in any arbitrary pose. It is the first of its kind to have been
trained end-to-end using only synthetically generated data, which not only
provides highly accurate human mesh representations but also allows for precise
anthropometry of the body. Moreover, using a highly diverse animation library,
we articulated our synthetic humans&apos; body and hands to maximize the diversity
of the learnable priors for model training. Our model was trained on a dataset
of $100k$ procedurally-generated posed human meshes and their corresponding
anthropometric measurements. Our synthetic data generator can be used to
generate millions of unique human identities and poses for non-commercial
academic research purposes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Picetti_F/0/1/0/all/0/1&quot;&gt;Francesco Picetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshpande_S/0/1/0/all/0/1&quot;&gt;Shrinath Deshpande&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leban_J/0/1/0/all/0/1&quot;&gt;Jonathan Leban&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shahtalebi_S/0/1/0/all/0/1&quot;&gt;Soroosh Shahtalebi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Patel_J/0/1/0/all/0/1&quot;&gt;Jay Patel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_P/0/1/0/all/0/1&quot;&gt;Peifeng Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chunpu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Metze_C/0/1/0/all/0/1&quot;&gt;Charles Metze III&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_C/0/1/0/all/0/1&quot;&gt;Cameron Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Laidlaw_C/0/1/0/all/0/1&quot;&gt;Cera Laidlaw&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Warren_J/0/1/0/all/0/1&quot;&gt;James Warren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huynh_K/0/1/0/all/0/1&quot;&gt;Kathy Huynh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Page_R/0/1/0/all/0/1&quot;&gt;River Page&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hogins_J/0/1/0/all/0/1&quot;&gt;Jonathan Hogins&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Crespi_A/0/1/0/all/0/1&quot;&gt;Adam Crespi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ganguly_S/0/1/0/all/0/1&quot;&gt;Sujoy Ganguly&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ebadi_S/0/1/0/all/0/1&quot;&gt;Salehe Erfanian Ebadi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03824">
<title>Training Acceleration of Low-Rank Decomposed Networks using Sequential Freezing and Rank Quantization. (arXiv:2309.03824v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2309.03824</link>
<description rdf:parseType="Literal">&lt;p&gt;Low Rank Decomposition (LRD) is a model compression technique applied to the
weight tensors of deep learning models in order to reduce the number of
trainable parameters and computational complexity. However, due to high number
of new layers added to the architecture after applying LRD, it may not lead to
a high training/inference acceleration if the decomposition ranks are not small
enough. The issue is that using small ranks increases the risk of significant
accuracy drop after decomposition. In this paper, we propose two techniques for
accelerating low rank decomposed models without requiring to use small ranks
for decomposition. These methods include rank optimization and sequential
freezing of decomposed layers. We perform experiments on both convolutional and
transformer-based models. Experiments show that these techniques can improve
the model throughput up to 60% during training and 37% during inference when
combined together while preserving the accuracy close to that of the original
models
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hajimolahoseini_H/0/1/0/all/0/1&quot;&gt;Habib Hajimolahoseini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_W/0/1/0/all/0/1&quot;&gt;Walid Ahmed&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yang Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03831">
<title>Uncovering Drift in Textual Data: An Unsupervised Method for Detecting and Mitigating Drift in Machine Learning Models. (arXiv:2309.03831v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.03831</link>
<description rdf:parseType="Literal">&lt;p&gt;Drift in machine learning refers to the phenomenon where the statistical
properties of data or context, in which the model operates, change over time
leading to a decrease in its performance. Therefore, maintaining a constant
monitoring process for machine learning model performance is crucial in order
to proactively prevent any potential performance regression. However,
supervised drift detection methods require human annotation and consequently
lead to a longer time to detect and mitigate the drift. In our proposed
unsupervised drift detection method, we follow a two step process. Our first
step involves encoding a sample of production data as the target distribution,
and the model training data as the reference distribution. In the second step,
we employ a kernel-based statistical test that utilizes the maximum mean
discrepancy (MMD) distance metric to compare the reference and target
distributions and estimate any potential drift. Our method also identifies the
subset of production data that is the root cause of the drift. The models
retrained using these identified high drift samples show improved performance
on online customer experience quality metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khaki_S/0/1/0/all/0/1&quot;&gt;Saeed Khaki&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aditya_A/0/1/0/all/0/1&quot;&gt;Akhouri Abhinav Aditya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Karnin_Z/0/1/0/all/0/1&quot;&gt;Zohar Karnin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Lan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_O/0/1/0/all/0/1&quot;&gt;Olivia Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chandrashekar_S/0/1/0/all/0/1&quot;&gt;Samarth Marudheri Chandrashekar&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03852">
<title>FLM-101B: An Open LLM and How to Train It with $100K Budget. (arXiv:2309.03852v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.03852</link>
<description rdf:parseType="Literal">&lt;p&gt;Large language models (LLMs) have achieved remarkable success in NLP and
multimodal tasks. Despite these successes, their development faces two main
challenges: (i) high computational cost; and (ii) difficulty in conducting fair
and objective evaluations. LLMs are prohibitively expensive, making it feasible
for only a few major players to undertake their training, thereby constraining
both research and application opportunities. This underscores the importance of
cost-effective LLM training. In this paper, we utilize a growth strategy to
significantly reduce LLM training cost. We demonstrate that an LLM with 101B
parameters and 0.31TB tokens can be trained on a $100K budget. We also adopt a
systematic evaluation paradigm for the IQ evaluation of LLMs, in complement to
existing evaluations that focus more on knowledge-oriented abilities. We
introduce our benchmark including evaluations on important aspects of
intelligence including symbolic mapping, itrule understanding, pattern mining,
and anti-interference. Such evaluations minimize the potential impact of
memorization. Experimental results show that our model FLM-101B, trained with a
budget of $100K, achieves comparable performance to powerful and well-known
models, eg GPT-3 and GLM-130B, especially in the IQ benchmark evaluations with
contexts unseen in training data. The checkpoint of FLM-101B will be
open-sourced at https://huggingface.co/CofeAI/FLM-101B.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_Y/0/1/0/all/0/1&quot;&gt;Yiqun Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_X/0/1/0/all/0/1&quot;&gt;Xin Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_X/0/1/0/all/0/1&quot;&gt;Xuezhi Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_X/0/1/0/all/0/1&quot;&gt;Xuying Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Siqi Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_P/0/1/0/all/0/1&quot;&gt;Peng Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jing Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Du_L/0/1/0/all/0/1&quot;&gt;Li Du&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_B/0/1/0/all/0/1&quot;&gt;Bowen Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_A/0/1/0/all/0/1&quot;&gt;Aixin Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yequan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03876">
<title>OpinionGPT: Modelling Explicit Biases in Instruction-Tuned LLMs. (arXiv:2309.03876v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.03876</link>
<description rdf:parseType="Literal">&lt;p&gt;Instruction-tuned Large Language Models (LLMs) have recently showcased
remarkable ability to generate fitting responses to natural language
instructions. However, an open research question concerns the inherent biases
of trained models and their responses. For instance, if the data used to tune
an LLM is dominantly written by persons with a specific political bias, we
might expect generated answers to share this bias. Current research work seeks
to de-bias such models, or suppress potentially biased answers. With this
demonstration, we take a different view on biases in instruction-tuning: Rather
than aiming to suppress them, we aim to make them explicit and transparent. To
this end, we present OpinionGPT, a web demo in which users can ask questions
and select all biases they wish to investigate. The demo will answer this
question using a model fine-tuned on text representing each of the selected
biases, allowing side-by-side comparison. To train the underlying model, we
identified 11 different biases (political, geographic, gender, age) and derived
an instruction-tuning corpus in which each answer was written by members of one
of these demographics. This paper presents OpinionGPT, illustrates how we
trained the bias-aware model and showcases the web application (available at
https://opiniongpt.informatik.hu-berlin.de).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Haller_P/0/1/0/all/0/1&quot;&gt;Patrick Haller&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aynetdinov_A/0/1/0/all/0/1&quot;&gt;Ansar Aynetdinov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Akbik_A/0/1/0/all/0/1&quot;&gt;Alan Akbik&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03883">
<title>DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models. (arXiv:2309.03883v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.03883</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite their impressive capabilities, large language models (LLMs) are prone
to hallucinations, i.e., generating content that deviates from facts seen
during pretraining. We propose a simple decoding strategy for reducing
hallucinations with pretrained LLMs that does not require conditioning on
retrieved external knowledge nor additional fine-tuning. Our approach obtains
the next-token distribution by contrasting the differences in logits obtained
from projecting the later layers versus earlier layers to the vocabulary space,
exploiting the fact that factual knowledge in an LLMs has generally been shown
to be localized to particular transformer layers. We find that this Decoding by
Contrasting Layers (DoLa) approach is able to better surface factual knowledge
and reduce the generation of incorrect facts. DoLa consistently improves the
truthfulness across multiple choices tasks and open-ended generation tasks, for
example improving the performance of LLaMA family models on TruthfulQA by
12-17% absolute points, demonstrating its potential in making LLMs reliably
generate truthful facts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_Y/0/1/0/all/0/1&quot;&gt;Yung-Sung Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_Y/0/1/0/all/0/1&quot;&gt;Yujia Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Hongyin Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yoon Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Glass_J/0/1/0/all/0/1&quot;&gt;James Glass&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_P/0/1/0/all/0/1&quot;&gt;Pengcheng He&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03886">
<title>A Function Interpretation Benchmark for Evaluating Interpretability Methods. (arXiv:2309.03886v1 [cs.CL])</title>
<link>http://arxiv.org/abs/2309.03886</link>
<description rdf:parseType="Literal">&lt;p&gt;Labeling neural network submodules with human-legible descriptions is useful
for many downstream tasks: such descriptions can surface failures, guide
interventions, and perhaps even explain important model behaviors. To date,
most mechanistic descriptions of trained networks have involved small models,
narrowly delimited phenomena, and large amounts of human labor. Labeling all
human-interpretable sub-computations in models of increasing size and
complexity will almost certainly require tools that can generate and validate
descriptions automatically. Recently, techniques that use learned models
in-the-loop for labeling have begun to gain traction, but methods for
evaluating their efficacy are limited and ad-hoc. How should we validate and
compare open-ended labeling tools? This paper introduces FIND (Function
INterpretation and Description), a benchmark suite for evaluating the building
blocks of automated interpretability methods. FIND contains functions that
resemble components of trained neural networks, and accompanying descriptions
of the kind we seek to generate. The functions are procedurally constructed
across textual and numeric domains, and involve a range of real-world
complexities, including noise, composition, approximation, and bias. We
evaluate new and existing methods that use language models (LMs) to produce
code-based and language descriptions of function behavior. We find that an
off-the-shelf LM augmented with only black-box access to functions can
sometimes infer their structure, acting as a scientist by forming hypotheses,
proposing experiments, and updating descriptions in light of new data. However,
LM-based descriptions tend to capture global function behavior and miss local
corruptions. These results show that FIND will be useful for characterizing the
performance of more sophisticated interpretability methods before they are
applied to real-world models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schwettmann_S/0/1/0/all/0/1&quot;&gt;Sarah Schwettmann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaham_T/0/1/0/all/0/1&quot;&gt;Tamar Rott Shaham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Materzynska_J/0/1/0/all/0/1&quot;&gt;Joanna Materzynska&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chowdhury_N/0/1/0/all/0/1&quot;&gt;Neil Chowdhury&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shuang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andreas_J/0/1/0/all/0/1&quot;&gt;Jacob Andreas&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bau_D/0/1/0/all/0/1&quot;&gt;David Bau&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torralba_A/0/1/0/all/0/1&quot;&gt;Antonio Torralba&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.03893">
<title>DiffusionEngine: Diffusion Model is Scalable Data Engine for Object Detection. (arXiv:2309.03893v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2309.03893</link>
<description rdf:parseType="Literal">&lt;p&gt;Data is the cornerstone of deep learning. This paper reveals that the
recently developed Diffusion Model is a scalable data engine for object
detection. Existing methods for scaling up detection-oriented data often
require manual collection or generative models to obtain target images,
followed by data augmentation and labeling to produce training pairs, which are
costly, complex, or lacking diversity. To address these issues, we
presentDiffusionEngine (DE), a data scaling-up engine that provides
high-quality detection-oriented training pairs in a single stage. DE consists
of a pre-trained diffusion model and an effective Detection-Adapter,
contributing to generating scalable, diverse and generalizable detection data
in a plug-and-play manner. Detection-Adapter is learned to align the implicit
semantic and location knowledge in off-the-shelf diffusion models with
detection-aware signals to make better bounding-box predictions. Additionally,
we contribute two datasets, i.e., COCO-DE and VOC-DE, to scale up existing
detection benchmarks for facilitating follow-up research. Extensive experiments
demonstrate that data scaling-up via DE can achieve significant improvements in
diverse scenarios, such as various detection algorithms, self-supervised
pre-training, data-sparse, label-scarce, cross-domain, and semi-supervised
learning. For example, when using DE with a DINO-based adapter to scale up
data, mAP is improved by 3.1% on COCO, 7.6% on VOC, and 11.5% on Clipart.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Manlin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_J/0/1/0/all/0/1&quot;&gt;Jie Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_Y/0/1/0/all/0/1&quot;&gt;Yuxi Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Ming Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_J/0/1/0/all/0/1&quot;&gt;Jie Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_X/0/1/0/all/0/1&quot;&gt;Xuefeng Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_W/0/1/0/all/0/1&quot;&gt;Wei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Rui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_M/0/1/0/all/0/1&quot;&gt;Min Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_A/0/1/0/all/0/1&quot;&gt;Andy J. Ma&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2010.10274">
<title>Graph Fairing Convolutional Networks for Anomaly Detection. (arXiv:2010.10274v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2010.10274</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolution is a fundamental building block for many deep neural
networks on graph-structured data. In this paper, we introduce a simple, yet
very effective graph convolutional network with skip connections for
semi-supervised anomaly detection. The proposed layerwise propagation rule of
our model is theoretically motivated by the concept of implicit fairing in
geometry processing, and comprises a graph convolution module for aggregating
information from immediate node neighbors and a skip connection module for
combining layer-wise neighborhood representations. This propagation rule is
derived from the iterative solution of the implicit fairing equation via the
Jacobi method. In addition to capturing information from distant graph nodes
through skip connections between the network&apos;s layers, our approach exploits
both the graph structure and node features for learning discriminative node
representations. These skip connections are integrated by design in our
proposed network architecture. The effectiveness of our model is demonstrated
through extensive experiments on five benchmark datasets, achieving better or
comparable anomaly detection results against strong baseline methods. We also
demonstrate through an ablation study that skip connection helps improve the
model performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mesgaran_M/0/1/0/all/0/1&quot;&gt;Mahsa Mesgaran&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hamza_A/0/1/0/all/0/1&quot;&gt;A. Ben Hamza&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2206.02231">
<title>Models of human preference for learning reward functions. (arXiv:2206.02231v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2206.02231</link>
<description rdf:parseType="Literal">&lt;p&gt;The utility of reinforcement learning is limited by the alignment of reward
functions with the interests of human stakeholders. One promising method for
alignment is to learn the reward function from human-generated preferences
between pairs of trajectory segments, a type of reinforcement learning from
human feedback (RLHF). These human preferences are typically assumed to be
informed solely by partial return, the sum of rewards along each segment. We
find this assumption to be flawed and propose modeling human preferences
instead as informed by each segment&apos;s regret, a measure of a segment&apos;s
deviation from optimal decision-making. Given infinitely many preferences
generated according to regret, we prove that we can identify a reward function
equivalent to the reward function that generated those preferences, and we
prove that the previous partial return model lacks this identifiability
property in multiple contexts. We empirically show that our proposed regret
preference model outperforms the partial return preference model with finite
training data in otherwise the same setting. Additionally, we find that our
proposed regret preference model better predicts real human preferences and
also learns reward functions from these preferences that lead to policies that
are better human-aligned. Overall, this work establishes that the choice of
preference model is impactful, and our proposed regret preference model
provides an improvement upon a core assumption of recent research. We have open
sourced our experimental code, the human preferences dataset we gathered, and
our training and preference elicitation interfaces for gathering a such a
dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Knox_W/0/1/0/all/0/1&quot;&gt;W. Bradley Knox&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hatgis_Kessell_S/0/1/0/all/0/1&quot;&gt;Stephane Hatgis-Kessell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Booth_S/0/1/0/all/0/1&quot;&gt;Serena Booth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niekum_S/0/1/0/all/0/1&quot;&gt;Scott Niekum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stone_P/0/1/0/all/0/1&quot;&gt;Peter Stone&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Allievi_A/0/1/0/all/0/1&quot;&gt;Alessandro Allievi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.01708">
<title>Autonomous Agriculture Robot for Smart Farming. (arXiv:2208.01708v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2208.01708</link>
<description rdf:parseType="Literal">&lt;p&gt;This project aims to develop and demonstrate a ground robot with intelligence
capable of conducting semi-autonomous farm operations for different low-heights
vegetable crops referred as Agriculture Application Robot(AAR). AAR is a
lightweight, solar-electric powered robot that uses intelligent perception for
conducting detection and classification of plants and their characteristics.
The system also has a robotic arm for the autonomous weed cutting process. The
robot can deliver fertilizer spraying, insecticide, herbicide, and other fluids
to the targets such as crops, weeds, and other pests. Besides, it provides
information for future research into higher-level tasks such as yield
estimation, crop, and soil health monitoring. We present the design of robot
and the associated experiments which show the promising results in real world
environments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ummadi_V/0/1/0/all/0/1&quot;&gt;Vinay Ummadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gundlapalle_A/0/1/0/all/0/1&quot;&gt;Aravind Gundlapalle&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaik_A/0/1/0/all/0/1&quot;&gt;Althaf Shaik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+B_S/0/1/0/all/0/1&quot;&gt;Shaik Mohammad Rafi B&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.03680">
<title>Accelerating Numerical Solvers for Large-Scale Simulation of Dynamical System via NeurVec. (arXiv:2208.03680v2 [cs.CE] UPDATED)</title>
<link>http://arxiv.org/abs/2208.03680</link>
<description rdf:parseType="Literal">&lt;p&gt;The large-scale simulation of dynamical systems is critical in numerous
scientific and engineering disciplines. However, traditional numerical solvers
are limited by the choice of step sizes when estimating integration, resulting
in a trade-off between accuracy and computational efficiency. To address this
challenge, we introduce a deep learning-based corrector called Neural Vector
(NeurVec), which can compensate for integration errors and enable larger time
step sizes in simulations. Our extensive experiments on a variety of complex
dynamical system benchmarks demonstrate that NeurVec exhibits remarkable
generalization capability on a continuous phase space, even when trained using
limited and discrete data. NeurVec significantly accelerates traditional
solvers, achieving speeds tens to hundreds of times faster while maintaining
high levels of accuracy and stability. Moreover, NeurVec&apos;s simple-yet-effective
design, combined with its ease of implementation, has the potential to
establish a new paradigm for fast-solving differential equations based on deep
learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhongzhan Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_S/0/1/0/all/0/1&quot;&gt;Senwei Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Hong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Haizhao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Liang Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16193">
<title>M3FGM:a node masking and multi-granularity message passing-based federated graph model for spatial-temporal data prediction. (arXiv:2210.16193v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16193</link>
<description rdf:parseType="Literal">&lt;p&gt;Researchers are solving the challenges of spatial-temporal prediction by
combining Federated Learning (FL) and graph models with respect to the
constrain of privacy and security. In order to make better use of the power of
graph model, some researchs also combine split learning(SL). However, there are
still several issues left unattended: 1) Clients might not be able to access
the server during inference phase; 2) The graph of clients designed manually in
the server model may not reveal the proper relationship between clients. This
paper proposes a new GNN-oriented split federated learning method, named node
{\bfseries M}asking and {\bfseries M}ulti-granularity {\bfseries M}essage
passing-based Federated Graph Model (M$^3$FGM) for the above issues. For the
first issue, the server model of M$^3$FGM employs a MaskNode layer to simulate
the case of clients being offline. We also redesign the decoder of the client
model using a dual-sub-decoders structure so that each client model can use its
local data to predict independently when offline. As for the second issue, a
new GNN layer named Multi-Granularity Message Passing (MGMP) layer enables each
client node to perceive global and local information. We conducted extensive
experiments in two different scenarios on two real traffic datasets. Results
show that M$^3$FGM outperforms the baselines and variant models, achieves the
best results in both datasets and scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_Y/0/1/0/all/0/1&quot;&gt;Yuxing Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zheng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_Y/0/1/0/all/0/1&quot;&gt;Yanwen Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Song Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_J/0/1/0/all/0/1&quot;&gt;Jiachi Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.17040">
<title>CodeEditor: Learning to Edit Source Code with Pre-trained Models. (arXiv:2210.17040v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2210.17040</link>
<description rdf:parseType="Literal">&lt;p&gt;Developers often perform repetitive code editing activities for various
reasons (e.g., code refactoring) during software development. Pre-trained code
editing models have achieved the state-of-the-art (SOTA) results. Pre-trained
models are first pre-trained with pre-training tasks and fine-tuned with the
code editing task. Existing pre-training tasks mainly are code infilling tasks
(e.g., masked language modeling), which are derived from the natural language
processing field and are not designed for automatic code editing.
&lt;/p&gt;
&lt;p&gt;This paper proposes a novel pre-training task specialized in code editing and
presents an effective pre-trained code editing model named CodeEditor. Our
pre-training task further improves the performance and generalization ability
of code editing models. Specifically, we collect lots of real-world code
snippets as the ground truth and use a powerful generator to rewrite them into
mutated versions. Then, we pre-train our CodeEditor to edit mutated versions
into the corresponding ground truth, to learn edit patterns. We conduct
experiments on four code editing datasets and evaluate the pre-trained
CodeEditor in three settings. (1) In the fine-tuning setting, we train the
pre-trained CodeEditor with four datasets and evaluate it on the test data.
CodeEditor outperforms the SOTA baselines by 15%, 25.5%, and 9.4% and 26.6% on
four datasets. (2) In the few-shot setting, we train the pre-trained CodeEditor
with limited data and evaluate it on the test data. CodeEditor substantially
performs better than all baselines. (3) In the zero-shot setting, CodeEditor
correctly edits 1,113 programs while the SOTA baselines can not work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zhuo Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xing Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kechi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_Z/0/1/0/all/0/1&quot;&gt;Zhiyi Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.05798">
<title>BigText-QA: Question Answering over a Large-Scale Hybrid Knowledge Graph. (arXiv:2212.05798v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2212.05798</link>
<description rdf:parseType="Literal">&lt;p&gt;Answering complex questions over textual resources remains a challenge,
particularly when dealing with nuanced relationships between multiple entities
expressed within natural-language sentences. To this end, curated knowledge
bases (KBs) like YAGO, DBpedia, Freebase, and Wikidata have been widely used
and gained great acceptance for question-answering (QA) applications in the
past decade. While these KBs offer a structured knowledge representation, they
lack the contextual diversity found in natural-language sources. To address
this limitation, BigText-QA introduces an integrated QA approach, which is able
to answer questions based on a more redundant form of a knowledge graph (KG)
that organizes both structured and unstructured (i.e., &quot;hybrid&quot;) knowledge in a
unified graphical representation. Thereby, BigText-QA is able to combine the
best of both worlds$\unicode{x2013}$a canonical set of named entities, mapped
to a structured background KB (such as YAGO or Wikidata), as well as an open
set of textual clauses providing highly diversified relational paraphrases with
rich context information. Our experimental results demonstrate that BigText-QA
outperforms DrQA, a neural-network-based QA system, and achieves competitive
results to QUEST, a graph-based unsupervised QA system.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_J/0/1/0/all/0/1&quot;&gt;Jingjing Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Biryukov_M/0/1/0/all/0/1&quot;&gt;Maria Biryukov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobald_M/0/1/0/all/0/1&quot;&gt;Martin Theobald&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venugopal_V/0/1/0/all/0/1&quot;&gt;Vinu Ellampallil Venugopal&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.04838">
<title>LB-SimTSC: An Efficient Similarity-Aware Graph Neural Network for Semi-Supervised Time Series Classification. (arXiv:2301.04838v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2301.04838</link>
<description rdf:parseType="Literal">&lt;p&gt;Time series classification is an important data mining task that has received
a lot of interest in the past two decades. Due to the label scarcity in
practice, semi-supervised time series classification with only a few labeled
samples has become popular. Recently, Similarity-aware Time Series
Classification (SimTSC) is proposed to address this problem by using a graph
neural network classification model on the graph generated from pairwise
Dynamic Time Warping (DTW) distance of batch data. It shows excellent accuracy
and outperforms state-of-the-art deep learning models in several few-label
settings. However, since SimTSC relies on pairwise DTW distances, the quadratic
complexity of DTW limits its usability to only reasonably sized datasets. To
address this challenge, we propose a new efficient semi-supervised time series
classification technique, LB-SimTSC, with a new graph construction module.
Instead of using DTW, we propose to utilize a lower bound of DTW, LB_Keogh, to
approximate the dissimilarity between instances in linear time, while retaining
the relative proximity relationships one would have obtained via computing DTW.
We construct the pairwise distance matrix using LB_Keogh and build a graph for
the graph neural network. We apply this approach to the ten largest datasets
from the well-known UCR time series classification archive. The results
demonstrate that this approach can be up to 104x faster than SimTSC when
constructing the graph on large datasets without significantly decreasing
classification accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_W/0/1/0/all/0/1&quot;&gt;Wenjie Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jain_A/0/1/0/all/0/1&quot;&gt;Arnav Jain&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_J/0/1/0/all/0/1&quot;&gt;Jessica Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.14051">
<title>Internet Explorer: Targeted Representation Learning on the Open Web. (arXiv:2302.14051v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.14051</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern vision models typically rely on fine-tuning general-purpose models
pre-trained on large, static datasets. These general-purpose models only
capture the knowledge within their pre-training datasets, which are tiny,
out-of-date snapshots of the Internet -- where billions of images are uploaded
each day. We suggest an alternate approach: rather than hoping our static
datasets transfer to our desired tasks after large-scale pre-training, we
propose dynamically utilizing the Internet to quickly train a small-scale model
that does extremely well on the task at hand. Our approach, called Internet
Explorer, explores the web in a self-supervised manner to progressively find
relevant examples that improve performance on a desired target dataset. It
cycles between searching for images on the Internet with text queries,
self-supervised training on downloaded images, determining which images were
useful, and prioritizing what to search for next. We evaluate Internet Explorer
across several datasets and show that it outperforms or matches CLIP oracle
performance by using just a single GPU desktop to actively query the Internet
for 30--40 hours. Results, visualizations, and videos at
https://internet-explorer-ssl.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_A/0/1/0/all/0/1&quot;&gt;Alexander C. Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brown_E/0/1/0/all/0/1&quot;&gt;Ellis Brown&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Efros_A/0/1/0/all/0/1&quot;&gt;Alexei A. Efros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pathak_D/0/1/0/all/0/1&quot;&gt;Deepak Pathak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.11899">
<title>Large-Scale Traffic Signal Control Using Constrained Network Partition and Adaptive Deep Reinforcement Learning. (arXiv:2303.11899v5 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2303.11899</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-agent Deep Reinforcement Learning (MADRL) based traffic signal control
becomes a popular research topic in recent years. To alleviate the scalability
issue of completely centralized RL techniques and the non-stationarity issue of
completely decentralized RL techniques on large-scale traffic networks, some
literature utilizes a regional control approach where the whole network is
firstly partitioned into multiple disjoint regions, followed by applying the
centralized RL approach to each region. However, the existing partitioning
rules either have no constraints on the topology of regions or require the same
topology for all regions. Meanwhile, no existing regional control approach
explores the performance of optimal joint action in an exponentially growing
regional action space when intersections are controlled by 4-phase traffic
signals (EW, EWL, NS, NSL). In this paper, we propose a novel RL training
framework named RegionLight to tackle the above limitations. Specifically, the
topology of regions is firstly constrained to a star network which comprises
one center and an arbitrary number of leaves. Next, the network partitioning
problem is modeled as an optimization problem to minimize the number of
regions. Then, an Adaptive Branching Dueling Q-Network (ABDQ) model is proposed
to decompose the regional control task into several joint signal control
sub-tasks corresponding to particular intersections. Subsequently, these
sub-tasks maximize the regional benefits cooperatively. Finally, the global
control strategy for the whole network is obtained by concatenating the optimal
joint actions of all regions. Experimental results demonstrate the superiority
of our proposed framework over all baselines under both real and synthetic
datasets in all evaluation metrics.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gu_H/0/1/0/all/0/1&quot;&gt;Hankang Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shangbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoguang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_D/0/1/0/all/0/1&quot;&gt;Dongyao Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_G/0/1/0/all/0/1&quot;&gt;Guoqiang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_E/0/1/0/all/0/1&quot;&gt;Eng Gee Lim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wong_C/0/1/0/all/0/1&quot;&gt;Cheuk Pong Ryan Wong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13592">
<title>Prompting Multilingual Large Language Models to Generate Code-Mixed Texts: The Case of South East Asian Languages. (arXiv:2303.13592v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13592</link>
<description rdf:parseType="Literal">&lt;p&gt;While code-mixing is a common linguistic practice in many parts of the world,
collecting high-quality and low-cost code-mixed data remains a challenge for
natural language processing (NLP) research. The recent proliferation of Large
Language Models (LLMs) compels one to ask: how capable are these systems in
generating code-mixed data? In this paper, we explore prompting multilingual
LLMs in a zero-shot manner to generate code-mixed data for seven languages in
South East Asia (SEA), namely Indonesian, Malay, Chinese, Tagalog, Vietnamese,
Tamil, and Singlish. We find that publicly available multilingual
instruction-tuned models such as BLOOMZ and Flan-T5-XXL are incapable of
producing texts with phrases or clauses from different languages. ChatGPT
exhibits inconsistent capabilities in generating code-mixed texts, wherein its
performance varies depending on the prompt template and language pairing. For
instance, ChatGPT generates fluent and natural Singlish texts (an English-based
creole spoken in Singapore), but for English-Tamil language pair, the system
mostly produces grammatically incorrect or semantically meaningless utterances.
Furthermore, it may erroneously introduce languages not specified in the
prompt. Based on our investigation, existing multilingual LLMs exhibit a wide
range of proficiency in code-mixed data generation for SEA languages. As such,
we advise against using LLMs in this context without extensive human checks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yong_Z/0/1/0/all/0/1&quot;&gt;Zheng-Xin Yong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Ruochen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Forde_J/0/1/0/all/0/1&quot;&gt;Jessica Zosa Forde&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Skyler Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cahyawijaya_S/0/1/0/all/0/1&quot;&gt;Samuel Cahyawijaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lovenia_H/0/1/0/all/0/1&quot;&gt;Holy Lovenia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Winata_G/0/1/0/all/0/1&quot;&gt;Genta Indra Winata&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sutawika_L/0/1/0/all/0/1&quot;&gt;Lintang Sutawika&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cruz_J/0/1/0/all/0/1&quot;&gt;Jan Christian Blaise Cruz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Phan_L/0/1/0/all/0/1&quot;&gt;Long Phan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Y/0/1/0/all/0/1&quot;&gt;Yin Lin Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Solorio_T/0/1/0/all/0/1&quot;&gt;Thamar Solorio&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aji_A/0/1/0/all/0/1&quot;&gt;Alham Fikri Aji&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.17780">
<title>AceCoder: Utilizing Existing Code to Enhance Code Generation. (arXiv:2303.17780v3 [cs.SE] UPDATED)</title>
<link>http://arxiv.org/abs/2303.17780</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have shown great success in code generation.
LLMs take as the input a prompt and output the code. A key question is how to
make prompts (i.e., Prompting Techniques). Existing prompting techniques are
designed for natural language generation and have low accuracy in code
generation.
&lt;/p&gt;
&lt;p&gt;In this paper, we propose a new prompting technique named AceCoder. Our
motivation is that code generation meets two unique challenges (i.e.,
requirement understanding and code implementation). AceCoder contains two novel
mechanisms (i.e., guided code generation and example retrieval) to solve these
challenges. (1) Guided code generation asks LLMs first to analyze requirements
and output an intermediate preliminary (e.g., test cases). The preliminary is
used to clarify requirements and tell LLMs &quot;what to write&quot;. (2) Example
retrieval selects similar programs as examples in prompts, which provide lots
of relevant content (e.g., algorithms, APIs) and teach LLMs &quot;how to write&quot;. We
apply AceCoder to three LLMs (e.g., Codex) and evaluate it on three public
benchmarks using the Pass@k. Results show that AceCoder can significantly
improve the performance of LLMs on code generation. (1) In terms of Pass@1,
AceCoder outperforms the state-of-the-art baseline by up to 56.4% in MBPP,
70.7% in MBJP, and 88.4% in MBJSP. (2) AceCoder is effective in LLMs with
different sizes (i.e., 6B to 13B) and different languages (i.e., Python, Java,
and JavaScript). (3) Human evaluation shows human developers prefer programs
from AceCoder.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jia Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yunfei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yongmin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Ge Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.00526">
<title>Layout and Task Aware Instruction Prompt for Zero-shot Document Image Question Answering. (arXiv:2306.00526v4 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2306.00526</link>
<description rdf:parseType="Literal">&lt;p&gt;Layout-aware pre-trained models has achieved significant progress on document
image question answering. They introduce extra learnable modules into existing
language models to capture layout information within document images from text
bounding box coordinates obtained by OCR tools. However, extra modules
necessitate pre-training on extensive document images. This prevents these
methods from directly utilizing off-the-shelf instruction-tuning language
foundation models, which have recently shown promising potential in zero-shot
learning. Instead, in this paper, we find that instruction-tuning language
models like Claude and ChatGPT can understand layout by spaces and line breaks.
Based on this observation, we propose the LAyout and Task aware Instruction
Prompt (LATIN-Prompt), which consists of layout-aware document content and
task-aware instruction. Specifically, the former uses appropriate spaces and
line breaks to recover the layout information among text segments obtained by
OCR tools, and the latter ensures that generated answers adhere to formatting
requirements. Moreover, we propose the LAyout and Task aware Instruction Tuning
(LATIN-Tuning) to improve the performance of small instruction-tuning models
like Alpaca. Experimental results show that LATIN-Prompt enables zero-shot
performance of Claude and ChatGPT to be comparable to the fine-tuning
performance of SOTAs on document image question answering, and LATIN-Tuning
enhances the zero-shot performance of Alpaca significantly. For example,
LATIN-Prompt improves the performance of Claude and ChatGPT on DocVQA by 263%
and 20% respectively. LATIN-Tuning improves the performance of Alpaca on DocVQA
by 87.7%. Quantitative and qualitative analyses demonstrate the effectiveness
of LATIN-Prompt and LATIN-Tuning. We provide the code in supplementary and will
release it to facilitate future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunhao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ou_Y/0/1/0/all/0/1&quot;&gt;Yixin Ou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yin Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13596">
<title>Max-Margin Token Selection in Attention Mechanism. (arXiv:2306.13596v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13596</link>
<description rdf:parseType="Literal">&lt;p&gt;Attention mechanism is a central component of the transformer architecture
which led to the phenomenal success of large language models. However, the
theoretical principles underlying the attention mechanism are poorly
understood, especially its nonconvex optimization dynamics. In this work, we
explore the seminal softmax-attention model $f(\boldsymbol{X})=\langle
\boldsymbol{Xv}, \texttt{softmax}(\boldsymbol{XWp})\rangle$, where
$\boldsymbol{X}$ is the token sequence and
$(\boldsymbol{v},\boldsymbol{W},\boldsymbol{p})$ are trainable parameters. We
prove that running gradient descent on $\boldsymbol{p}$, or equivalently
$\boldsymbol{W}$, converges in direction to a max-margin solution that
separates $\textit{locally-optimal}$ tokens from non-optimal ones. This clearly
formalizes attention as an optimal token selection mechanism. Remarkably, our
results are applicable to general data and precisely characterize
$\textit{optimality}$ of tokens in terms of the value embeddings
$\boldsymbol{Xv}$ and problem geometry. We also provide a broader
regularization path analysis that establishes the margin maximizing nature of
attention even for nonlinear prediction heads. When optimizing $\boldsymbol{v}$
and $\boldsymbol{p}$ simultaneously with logistic loss, we identify conditions
under which the regularization paths directionally converge to their respective
hard-margin SVM solutions where $\boldsymbol{v}$ separates the input features
based on their labels. Interestingly, the SVM formulation of $\boldsymbol{p}$
is influenced by the support vector geometry of $\boldsymbol{v}$. Finally, we
verify our theoretical findings via numerical experiments and provide insights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1&quot;&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingcong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xuechen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05766">
<title>Rad-ReStruct: A Novel VQA Benchmark and Method for Structured Radiology Reporting. (arXiv:2307.05766v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05766</link>
<description rdf:parseType="Literal">&lt;p&gt;Radiology reporting is a crucial part of the communication between
radiologists and other medical professionals, but it can be time-consuming and
error-prone. One approach to alleviate this is structured reporting, which
saves time and enables a more accurate evaluation than free-text reports.
However, there is limited research on automating structured reporting, and no
public benchmark is available for evaluating and comparing different methods.
To close this gap, we introduce Rad-ReStruct, a new benchmark dataset that
provides fine-grained, hierarchically ordered annotations in the form of
structured reports for X-Ray images. We model the structured reporting task as
hierarchical visual question answering (VQA) and propose hi-VQA, a novel method
that considers prior context in the form of previously asked questions and
answers for populating a structured radiology report. Our experiments show that
hi-VQA achieves competitive performance to the state-of-the-art on the medical
VQA benchmark VQARad while performing best among methods without
domain-specific vision-language pretraining and provides a strong baseline on
Rad-ReStruct. Our work represents a significant step towards the automated
population of structured radiology reports and provides a valuable first
benchmark for future research in this area. Our dataset and code is available
at https://github.com/ChantalMP/Rad-ReStruct.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pellegrini_C/0/1/0/all/0/1&quot;&gt;Chantal Pellegrini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keicher_M/0/1/0/all/0/1&quot;&gt;Matthias Keicher&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozsoy_E/0/1/0/all/0/1&quot;&gt;Ege &amp;#xd6;zsoy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08461">
<title>Towards eXplainable AI for Mobility Data Science. (arXiv:2307.08461v3 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08461</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper presents our ongoing work towards XAI for Mobility Data Science
applications, focusing on explainable models that can learn from dense
trajectory data, such as GPS tracks of vehicles and vessels using temporal
graph neural networks (GNNs) and counterfactuals. We review the existing GeoXAI
studies, argue the need for comprehensible explanations with human-centered
approaches, and outline a research path toward XAI for Mobility Data Science.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jalali_A/0/1/0/all/0/1&quot;&gt;Anahid Jalali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Graser_A/0/1/0/all/0/1&quot;&gt;Anita Graser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heistracher_C/0/1/0/all/0/1&quot;&gt;Clemens Heistracher&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09882">
<title>Adversarial Likelihood Estimation With One-Way Flows. (arXiv:2307.09882v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09882</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Adversarial Networks (GANs) can produce high-quality samples, but
do not provide an estimate of the probability density around the samples.
However, it has been noted that maximizing the log-likelihood within an
energy-based setting can lead to an adversarial framework where the
discriminator provides unnormalized density (often called energy). We further
develop this perspective, incorporate importance sampling, and show that 1)
Wasserstein GAN performs a biased estimate of the partition function, and we
propose instead to use an unbiased estimator; and 2) when optimizing for
likelihood, one must maximize generator entropy. This is hypothesized to
provide a better mode coverage. Different from previous works, we explicitly
compute the density of the generated samples. This is the key enabler to
designing an unbiased estimator of the partition function and computation of
the generator entropy term. The generator density is obtained via a new type of
flow network, called one-way flow network, that is less constrained in terms of
architecture, as it does not require a tractable inverse function. Our
experimental results show that our method converges faster, produces comparable
sample quality to GANs with similar architecture, successfully avoids
over-fitting to commonly used datasets and produces smooth low-dimensional
latent representations of the training data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ben_Dov_O/0/1/0/all/0/1&quot;&gt;Omri Ben-Dov&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gupta_P/0/1/0/all/0/1&quot;&gt;Pravir Singh Gupta&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abrevaya_V/0/1/0/all/0/1&quot;&gt;Victoria Abrevaya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Black_M/0/1/0/all/0/1&quot;&gt;Michael J. Black&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ghosh_P/0/1/0/all/0/1&quot;&gt;Partha Ghosh&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.14971">
<title>Take-A-Photo: 3D-to-2D Generative Pre-training of Point Cloud Models. (arXiv:2307.14971v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.14971</link>
<description rdf:parseType="Literal">&lt;p&gt;With the overwhelming trend of mask image modeling led by MAE, generative
pre-training has shown a remarkable potential to boost the performance of
fundamental models in 2D vision. However, in 3D vision, the over-reliance on
Transformer-based backbones and the unordered nature of point clouds have
restricted the further development of generative pre-training. In this paper,
we propose a novel 3D-to-2D generative pre-training method that is adaptable to
any point cloud model. We propose to generate view images from different
instructed poses via the cross-attention mechanism as the pre-training scheme.
Generating view images has more precise supervision than its point cloud
counterpart, thus assisting 3D backbones to have a finer comprehension of the
geometrical structure and stereoscopic relations of the point cloud.
Experimental results have proved the superiority of our proposed 3D-to-2D
generative pre-training over previous pre-training methods. Our method is also
effective in boosting the performance of architecture-oriented approaches,
achieving state-of-the-art performance when fine-tuning on ScanObjectNN
classification and ShapeNetPart segmentation tasks. Code is available at
https://github.com/wangzy22/TAP.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Ziyi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_X/0/1/0/all/0/1&quot;&gt;Xumin Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rao_Y/0/1/0/all/0/1&quot;&gt;Yongming Rao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jie Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_J/0/1/0/all/0/1&quot;&gt;Jiwen Lu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00868">
<title>Beneficent Intelligence: A Capability Approach to Modeling Benefit, Assistance, and Associated Moral Failures through AI Systems. (arXiv:2308.00868v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00868</link>
<description rdf:parseType="Literal">&lt;p&gt;The prevailing discourse around AI ethics lacks the language and formalism
necessary to capture the diverse ethical concerns that emerge when AI systems
interact with individuals. Drawing on Sen and Nussbaum&apos;s capability approach,
we present a framework formalizing a network of ethical concepts and
entitlements necessary for AI systems to confer meaningful benefit or
assistance to stakeholders. Such systems enhance stakeholders&apos; ability to
advance their life plans and well-being while upholding their fundamental
rights. We characterize two necessary conditions for morally permissible
interactions between AI systems and those impacted by their functioning, and
two sufficient conditions for realizing the ideal of meaningful benefit. We
then contrast this ideal with several salient failure modes, namely, forms of
social interactions that constitute unjustified paternalism, coercion,
deception, exploitation and domination. The proliferation of incidents
involving AI in high-stakes domains underscores the gravity of these issues and
the imperative to take an ethics-led approach to AI systems from their
inception.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+London_A/0/1/0/all/0/1&quot;&gt;Alex John London&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Heidari_H/0/1/0/all/0/1&quot;&gt;Hoda Heidari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.00904">
<title>VLUCI: Variational Learning of Unobserved Confounders for Counterfactual Inference. (arXiv:2308.00904v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.00904</link>
<description rdf:parseType="Literal">&lt;p&gt;Causal inference plays a vital role in diverse domains like epidemiology,
healthcare, and economics. De-confounding and counterfactual prediction in
observational data has emerged as a prominent concern in causal inference
research. While existing models tackle observed confounders, the presence of
unobserved confounders remains a significant challenge, distorting causal
inference and impacting counterfactual outcome accuracy. To address this, we
propose a novel variational learning model of unobserved confounders for
counterfactual inference (VLUCI), which generates the posterior distribution of
unobserved confounders. VLUCI relaxes the unconfoundedness assumption often
overlooked by most causal inference methods. By disentangling observed and
unobserved confounders, VLUCI constructs a doubly variational inference model
to approximate the distribution of unobserved confounders, which are used for
inferring more accurate counterfactual outcomes. Extensive experiments on
synthetic and semi-synthetic datasets demonstrate VLUCI&apos;s superior performance
in inferring unobserved confounders. It is compatible with state-of-the-art
counterfactual inference models, significantly improving inference accuracy at
both group and individual levels. Additionally, VLUCI provides confidence
intervals for counterfactual outcomes, aiding decision-making in risk-sensitive
domains. We further clarify the considerations when applying VLUCI to cases
where unobserved confounders don&apos;t strictly conform to our model assumptions
using the public IHDP dataset as an example, highlighting the practical
advantages of VLUCI.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yonghe Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Q/0/1/0/all/0/1&quot;&gt;Qiang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Siwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_Y/0/1/0/all/0/1&quot;&gt;Yun Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_H/0/1/0/all/0/1&quot;&gt;Huiyan Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.02335">
<title>RAHNet: Retrieval Augmented Hybrid Network for Long-tailed Graph Classification. (arXiv:2308.02335v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.02335</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph classification is a crucial task in many real-world multimedia
applications, where graphs can represent various multimedia data types such as
images, videos, and social networks. Previous efforts have applied graph neural
networks (GNNs) in balanced situations where the class distribution is
balanced. However, real-world data typically exhibit long-tailed class
distributions, resulting in a bias towards the head classes when using GNNs and
limited generalization ability over the tail classes. Recent approaches mainly
focus on re-balancing different classes during model training, which fails to
explicitly introduce new knowledge and sacrifices the performance of the head
classes. To address these drawbacks, we propose a novel framework called
Retrieval Augmented Hybrid Network (RAHNet) to jointly learn a robust feature
extractor and an unbiased classifier in a decoupled manner. In the feature
extractor training stage, we develop a graph retrieval module to search for
relevant graphs that directly enrich the intra-class diversity for the tail
classes. Moreover, we innovatively optimize a category-centered supervised
contrastive loss to obtain discriminative representations, which is more
suitable for long-tailed scenarios. In the classifier fine-tuning stage, we
balance the classifier weights with two weight regularization techniques, i.e.,
Max-norm and weight decay. Experiments on various popular benchmarks verify the
superiority of the proposed method against state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mao_Z/0/1/0/all/0/1&quot;&gt;Zhengyang Mao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ju_W/0/1/0/all/0/1&quot;&gt;Wei Ju&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yifang Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_X/0/1/0/all/0/1&quot;&gt;Xiao Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_M/0/1/0/all/0/1&quot;&gt;Ming Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11432">
<title>A Survey on Large Language Model based Autonomous Agents. (arXiv:2308.11432v2 [cs.AI] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11432</link>
<description rdf:parseType="Literal">&lt;p&gt;Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_C/0/1/0/all/0/1&quot;&gt;Chen Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_X/0/1/0/all/0/1&quot;&gt;Xueyang Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingsen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Z/0/1/0/all/0/1&quot;&gt;Zhiyuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_J/0/1/0/all/0/1&quot;&gt;Jiakai Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Y/0/1/0/all/0/1&quot;&gt;Yankai Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wayne Xin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhewei Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wen_J/0/1/0/all/0/1&quot;&gt;Ji-Rong Wen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.11764">
<title>Halo: Estimation and Reduction of Hallucinations in Open-Source Weak Large Language Models. (arXiv:2308.11764v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.11764</link>
<description rdf:parseType="Literal">&lt;p&gt;Large Language Models (LLMs) have revolutionized Natural Language Processing
(NLP). Although convenient for research and practical applications, open-source
LLMs with fewer parameters often suffer from severe hallucinations compared to
their larger counterparts. This paper focuses on measuring and reducing
hallucinations in BLOOM 7B, a representative of such weaker open-source LLMs
that are publicly available for research and commercial applications. We
introduce HaloCheck, a lightweight BlackBox knowledge-free framework designed
to quantify the severity of hallucinations in LLMs. Additionally, we explore
techniques like knowledge injection and teacher-student approaches to alleviate
hallucinations in low-parameter LLMs. Our experiments effectively demonstrate
the reduction of hallucinations in challenging domains for these LLMs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Elaraby_M/0/1/0/all/0/1&quot;&gt;Mohamed Elaraby&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_M/0/1/0/all/0/1&quot;&gt;Mengyin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dunn_J/0/1/0/all/0/1&quot;&gt;Jacob Dunn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xueying Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yu Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shizhu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_P/0/1/0/all/0/1&quot;&gt;Pingchuan Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuping Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.13280">
<title>AtmoRep: A stochastic model of atmosphere dynamics using large scale representation learning. (arXiv:2308.13280v2 [physics.ao-ph] UPDATED)</title>
<link>http://arxiv.org/abs/2308.13280</link>
<description rdf:parseType="Literal">&lt;p&gt;The atmosphere affects humans in a multitude of ways, from loss of life due
to adverse weather effects to long-term social and economic impacts on
societies. Computer simulations of atmospheric dynamics are, therefore, of
great importance for the well-being of our and future generations. Here, we
propose AtmoRep, a novel, task-independent stochastic computer model of
atmospheric dynamics that can provide skillful results for a wide range of
applications. AtmoRep uses large-scale representation learning from artificial
intelligence to determine a general description of the highly complex,
stochastic dynamics of the atmosphere from the best available estimate of the
system&apos;s historical trajectory as constrained by observations. This is enabled
by a novel self-supervised learning objective and a unique ensemble that
samples from the stochastic model with a variability informed by the one in the
historical record. The task-independent nature of AtmoRep enables skillful
results for a diverse set of applications without specifically training for
them and we demonstrate this for nowcasting, temporal interpolation, model
correction, and counterfactuals. We also show that AtmoRep can be improved with
additional data, for example radar observations, and that it can be extended to
tasks such as downscaling. Our work establishes that large-scale neural
networks can provide skillful, task-independent models of atmospheric dynamics.
With this, they provide a novel means to make the large record of atmospheric
observations accessible for applications and for scientific inquiry,
complementing existing simulations based on first principles.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Lessig_C/0/1/0/all/0/1&quot;&gt;Christian Lessig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Luise_I/0/1/0/all/0/1&quot;&gt;Ilaria Luise&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Gong_B/0/1/0/all/0/1&quot;&gt;Bing Gong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Langguth_M/0/1/0/all/0/1&quot;&gt;Michael Langguth&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Stadler_S/0/1/0/all/0/1&quot;&gt;Scarlet Stadler&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/physics/1/au:+Schultz_M/0/1/0/all/0/1&quot;&gt;Martin Schultz&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15116">
<title>Mixup-Augmented Meta-Learning for Sample-Efficient Fine-Tuning of Protein Simulators. (arXiv:2308.15116v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15116</link>
<description rdf:parseType="Literal">&lt;p&gt;Molecular dynamics simulations have emerged as a fundamental instrument for
studying biomolecules. At the same time, it is desirable to perform simulations
of a collection of particles under various conditions in which the molecules
can fluctuate. In this paper, we explore and adapt the soft prompt-based
learning method to molecular dynamics tasks. Our model can remarkably
generalize to unseen and out-of-distribution scenarios with limited training
data. While our work focuses on temperature as a test case, the versatility of
our approach allows for efficient simulation through any continuous dynamic
conditions, such as pressure and volumes. Our framework has two stages: 1)
Pre-trains with data mixing technique, augments molecular structure data and
temperature prompts, then applies a curriculum learning method by increasing
the ratio of them smoothly. 2) Meta-learning-based fine-tuning framework
improves sample-efficiency of fine-tuning process and gives the soft
prompt-tuning better initialization points. Comprehensive experiments reveal
that our framework excels in accuracy for in-domain data and demonstrates
strong generalization capabilities for unseen and out-of-distribution samples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jingbang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yian Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qu_X/0/1/0/all/0/1&quot;&gt;Xingwei Qu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shuangjia Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yaodong Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_H/0/1/0/all/0/1&quot;&gt;Hao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_J/0/1/0/all/0/1&quot;&gt;Jie Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.15225">
<title>From DDMs to DNNs: Using process data and models of decision-making to improve human-AI interactions. (arXiv:2308.15225v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2308.15225</link>
<description rdf:parseType="Literal">&lt;p&gt;Over the past decades, cognitive neuroscientists and behavioral economists
have recognized the value of describing the process of decision making in
detail and modeling the emergence of decisions over time. For example, the time
it takes to decide can reveal more about an agent&apos;s true hidden preferences
than only the decision itself. Similarly, data that track the ongoing decision
process such as eye movements or neural recordings contain critical information
that can be exploited, even if no decision is made. Here, we argue that
artificial intelligence (AI) research would benefit from a stronger focus on
insights about how decisions emerge over time and incorporate related process
data to improve AI predictions in general and human-AI interactions in
particular. First, we introduce a highly established computational framework
that assumes decisions to emerge from the noisy accumulation of evidence, and
we present related empirical work in psychology, neuroscience, and economics.
Next, we discuss to what extent current approaches in multi-agent AI do or do
not incorporate process data and models of decision making. Finally, we outline
how a more principled inclusion of the evidence-accumulation framework into the
training and use of AI can help to improve human-AI interactions in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gopnarayan_M/0/1/0/all/0/1&quot;&gt;Mrugsen Nagsen Gopnarayan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Aru_J/0/1/0/all/0/1&quot;&gt;Jaan Aru&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Gluth_S/0/1/0/all/0/1&quot;&gt;Sebastian Gluth&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16137">
<title>LM-Infinite: Simple On-the-Fly Length Generalization for Large Language Models. (arXiv:2308.16137v3 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16137</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, there have been remarkable advancements in the performance
of Transformer-based Large Language Models (LLMs) across various domains. As
these LLMs are deployed for increasingly complex tasks, they often face the
need to conduct longer reasoning processes or understand larger contexts. In
these situations, the length generalization failure of LLMs on long sequences
becomes more prominent. Most pre-training schemes truncate training sequences
to a fixed length. LLMs often struggle to generate fluent and coherent texts,
let alone carry out downstream tasks, after longer contexts, even with relative
positional encoding designed to cope with this problem. Common solutions such
as finetuning on longer corpora often involve daunting hardware and time costs
and require careful training process design. To more efficiently leverage the
generation capacity of existing LLMs, we theoretically and empirically
investigate the main out-of-distribution (OOD) factors contributing to this
problem. Inspired by this diagnosis, we propose a simple yet effective solution
for on-the-fly length generalization, LM-Infinite. It involves only a
$\Lambda$-shaped attention mask (to avoid excessive attended tokens) and a
distance limit (to avoid unseen distances) while requiring no parameter updates
or learning. We find it applicable to a variety of LLMs using relative-position
encoding methods. LM-Infinite is computationally efficient with $O(n)$ time and
space, and demonstrates consistent text generation fluency and quality to as
long as 32k tokens on ArXiv and OpenWebText2 datasets, with 2.72x decoding
speedup. On downstream tasks such as passkey retrieval, it continues to work on
inputs much longer than training lengths where vanilla models fail immediately.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chi Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Q/0/1/0/all/0/1&quot;&gt;Qifan Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiong_W/0/1/0/all/0/1&quot;&gt;Wenhan Xiong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_H/0/1/0/all/0/1&quot;&gt;Heng Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Sinong Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16763">
<title>Ladder-of-Thought: Using Knowledge as Steps to Elevate Stance Detection. (arXiv:2308.16763v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16763</link>
<description rdf:parseType="Literal">&lt;p&gt;Stance detection aims to identify the attitude expressed in a document
towards a given target. Techniques such as Chain-of-Thought (CoT) prompting
have advanced this task, enhancing a model&apos;s reasoning capabilities through the
derivation of intermediate rationales. However, CoT relies primarily on a
model&apos;s pre-trained internal knowledge during reasoning, thereby neglecting the
valuable external information that is previously unknown to the model. This
omission, especially within the unsupervised reasoning process, can affect the
model&apos;s overall performance. Moreover, while CoT enhances Large Language Models
(LLMs), smaller LMs, though efficient operationally, face challenges in
delivering nuanced reasoning. In response to these identified gaps, we
introduce the Ladder-of-Thought (LoT) for the stance detection task.
Constructed through a dual-phase Progressive Optimization Framework, LoT
directs the small LMs to assimilate high-quality external knowledge, refining
the intermediate rationales produced. These bolstered rationales subsequently
serve as the foundation for more precise predictions - akin to how a ladder
facilitates reaching elevated goals. LoT achieves a balance between efficiency
and performance. Our empirical evaluations underscore LoT&apos;s efficacy, marking a
16% improvement over GPT-3.5 and a 10% enhancement compared to GPT-3.5 with CoT
on stance detection task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_K/0/1/0/all/0/1&quot;&gt;Kairui Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_M/0/1/0/all/0/1&quot;&gt;Ming Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Joey Tianyi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tsang_I/0/1/0/all/0/1&quot;&gt;Ivor W. Tsang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_W/0/1/0/all/0/1&quot;&gt;Wen Haw Chong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yap_Y/0/1/0/all/0/1&quot;&gt;Yong Keong Yap&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2308.16898">
<title>Transformers as Support Vector Machines. (arXiv:2308.16898v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2308.16898</link>
<description rdf:parseType="Literal">&lt;p&gt;Since its inception in &quot;Attention Is All You Need&quot;, transformer architecture
has led to revolutionary advancements in NLP. The attention layer within the
transformer admits a sequence of input tokens $X$ and makes them interact
through pairwise similarities computed as softmax$(XQK^\top X^\top)$, where
$(K,Q)$ are the trainable key-query parameters. In this work, we establish a
formal equivalence between the optimization geometry of self-attention and a
hard-margin SVM problem that separates optimal input tokens from non-optimal
tokens using linear constraints on the outer-products of token pairs. This
formalism allows us to characterize the implicit bias of 1-layer transformers
optimized with gradient descent: (1) Optimizing the attention layer with
vanishing regularization, parameterized by $(K,Q)$, converges in direction to
an SVM solution minimizing the nuclear norm of the combined parameter
$W=KQ^\top$. Instead, directly parameterizing by $W$ minimizes a Frobenius norm
objective. We characterize this convergence, highlighting that it can occur
toward locally-optimal directions rather than global ones. (2) Complementing
this, we prove the local/global directional convergence of gradient descent
under suitable geometric conditions. Importantly, we show that
over-parameterization catalyzes global convergence by ensuring the feasibility
of the SVM problem and by guaranteeing a benign optimization landscape devoid
of stationary points. (3) While our theory applies primarily to linear
prediction heads, we propose a more general SVM equivalence that predicts the
implicit bias with nonlinear heads. Our findings are applicable to arbitrary
datasets and their validity is verified via experiments. We also introduce
several open problems and research directions. We believe these findings
inspire the interpretation of transformers as a hierarchy of SVMs that
separates and selects optimal tokens.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tarzanagh_D/0/1/0/all/0/1&quot;&gt;Davoud Ataee Tarzanagh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yingcong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thrampoulidis_C/0/1/0/all/0/1&quot;&gt;Christos Thrampoulidis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oymak_S/0/1/0/all/0/1&quot;&gt;Samet Oymak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00237">
<title>Publicly Shareable Clinical Large Language Model Built on Synthetic Clinical Notes. (arXiv:2309.00237v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00237</link>
<description rdf:parseType="Literal">&lt;p&gt;The development of large language models tailored for handling patients&apos;
clinical notes is often hindered by the limited accessibility and usability of
these notes due to strict privacy regulations. To address these challenges, we
first create synthetic large-scale clinical notes using publicly available case
reports extracted from biomedical literature. We then use these synthetic notes
to train our specialized clinical large language model, Asclepius. While
Asclepius is trained on synthetic data, we assess its potential performance in
real-world applications by evaluating it using real clinical notes. We
benchmark Asclepius against several other large language models, including
GPT-3.5-turbo and other open-source alternatives. To further validate our
approach using synthetic notes, we also compare Asclepius with its variants
trained on real clinical notes. Our findings convincingly demonstrate that
synthetic clinical notes can serve as viable substitutes for real ones when
constructing high-performing clinical language models. This conclusion is
supported by detailed evaluations conducted by both GPT-4 and medical
professionals. All resources including weights, codes, and data used in the
development of Asclepius are made publicly accessible for future research.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kweon_S/0/1/0/all/0/1&quot;&gt;Sunjun Kweon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Junu Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiyoun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Im_S/0/1/0/all/0/1&quot;&gt;Sujeong Im&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_E/0/1/0/all/0/1&quot;&gt;Eunbyeol Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bae_S/0/1/0/all/0/1&quot;&gt;Seongsu Bae&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oh_J/0/1/0/all/0/1&quot;&gt;Jungwoo Oh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_G/0/1/0/all/0/1&quot;&gt;Gyubok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_J/0/1/0/all/0/1&quot;&gt;Jong Hak Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+You_S/0/1/0/all/0/1&quot;&gt;Seng Chan You&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baek_S/0/1/0/all/0/1&quot;&gt;Seungjin Baek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Han_C/0/1/0/all/0/1&quot;&gt;Chang Hoon Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jung_Y/0/1/0/all/0/1&quot;&gt;Yoon Bin Jung&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jo_Y/0/1/0/all/0/1&quot;&gt;Yohan Jo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_E/0/1/0/all/0/1&quot;&gt;Edward Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.00646">
<title>Intelligence as a Measure of Consciousness. (arXiv:2309.00646v2 [q-bio.NC] UPDATED)</title>
<link>http://arxiv.org/abs/2309.00646</link>
<description rdf:parseType="Literal">&lt;p&gt;Evaluating artificial systems for signs of consciousness is increasingly
becoming a pressing concern, and a rigorous psychometric measurement framework
may be of crucial importance in evaluating large language models in this
regard. Most prominent theories of consciousness, both scientific and
metaphysical, argue for different kinds of information coupling as a necessary
component of human-like consciousness. By comparing information coupling in
human and animal brains, human cognitive development, emergent abilities, and
mental representation development to analogous phenomena in large language
models, I argue that psychometric measures of intelligence, such as the
g-factor or IQ, indirectly approximate the extent of conscious experience.
Based on a broader source of both scientific and metaphysical theories of
consciousness, I argue that all systems possess a degree of consciousness
ascertainable psychometrically and that psychometric measures of intelligence
may be used to gauge relative similarities of conscious experiences across
disparate systems, be they artificial or human.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/q-bio/1/au:+Sevo_I/0/1/0/all/0/1&quot;&gt;Igor &amp;#x160;evo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02553">
<title>Automating Behavioral Testing in Machine Translation. (arXiv:2309.02553v2 [cs.CL] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02553</link>
<description rdf:parseType="Literal">&lt;p&gt;Behavioral testing in NLP allows fine-grained evaluation of systems by
examining their linguistic capabilities through the analysis of input-output
behavior. Unfortunately, existing work on behavioral testing in Machine
Translation (MT) is currently restricted to largely handcrafted tests covering
a limited range of capabilities and languages. To address this limitation, we
propose to use Large Language Models (LLMs) to generate a diverse set of source
sentences tailored to test the behavior of MT models in a range of situations.
We can then verify whether the MT model exhibits the expected behavior through
matching candidate sets that are also generated using LLMs. Our approach aims
to make behavioral testing of MT systems practical while requiring only minimal
human effort. In our experiments, we apply our proposed evaluation framework to
assess multiple available MT systems, revealing that while in general
pass-rates follow the trends observable from traditional accuracy-based
metrics, our method was able to uncover several important differences and
potential bugs that go unnoticed when relying only on accuracy.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ferrando_J/0/1/0/all/0/1&quot;&gt;Javier Ferrando&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sperber_M/0/1/0/all/0/1&quot;&gt;Matthias Sperber&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Setiawan_H/0/1/0/all/0/1&quot;&gt;Hendra Setiawan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Telaar_D/0/1/0/all/0/1&quot;&gt;Dominic Telaar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hasan_S/0/1/0/all/0/1&quot;&gt;Sa&amp;#x161;a Hasan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02589">
<title>Approximating High-Dimensional Minimal Surfaces with Physics-Informed Neural Networks. (arXiv:2309.02589v2 [math.AP] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02589</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we compute numerical approximations of the minimal surfaces,
an essential type of Partial Differential Equation (PDE), in higher dimensions.
Classical methods cannot handle it in this case because of the Curse of
Dimensionality, where the computational cost of these methods increases
exponentially fast in response to higher problem dimensions, far beyond the
computing capacity of any modern supercomputers. Only in the past few years
have machine learning researchers been able to mitigate this problem. The
solution method chosen here is a model known as a Physics-Informed Neural
Network (PINN) which trains a deep neural network (DNN) to solve the minimal
surface PDE. It can be scaled up into higher dimensions and trained relatively
quickly even on a laptop with no GPU. Due to the inability to view the
high-dimension output, our data is presented as snippets of a higher-dimension
shape with enough fixed axes so that it is viewable with 3-D graphs. Not only
will the functionality of this method be tested, but we will also explore
potential limitations in the method&apos;s performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Steven Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/math/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaojing Ye&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2309.02685">
<title>Diffusion-EDFs: Bi-equivariant Denoising Generative Modeling on SE(3) for Visual Robotic Manipulation. (arXiv:2309.02685v2 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2309.02685</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent studies have verified that equivariant methods can significantly
improve the data efficiency, generalizability, and robustness in robot
learning. Meanwhile, denoising diffusion-based generative modeling has recently
gained significant attention as a promising approach for robotic manipulation
learning from demonstrations with stochastic behaviors. In this paper, we
present Diffusion-EDFs, a novel approach that incorporates spatial
roto-translation equivariance, i.e., SE(3)-equivariance to diffusion generative
modeling. By integrating SE(3)-equivariance into our model architectures, we
demonstrate that our proposed method exhibits remarkable data efficiency,
requiring only 5 to 10 task demonstrations for effective end-to-end training.
Furthermore, our approach showcases superior generalizability compared to
previous diffusion-based manipulation methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ryu_H/0/1/0/all/0/1&quot;&gt;Hyunwoo Ryu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jiwoo Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_J/0/1/0/all/0/1&quot;&gt;Junwoo Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_H/0/1/0/all/0/1&quot;&gt;Hyun Seok Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Seo_J/0/1/0/all/0/1&quot;&gt;Joohwan Seo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_T/0/1/0/all/0/1&quot;&gt;Taehan Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_Y/0/1/0/all/0/1&quot;&gt;Yubin Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jongeun Choi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Horowitz_R/0/1/0/all/0/1&quot;&gt;Roberto Horowitz&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>