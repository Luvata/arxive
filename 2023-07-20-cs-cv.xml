<?xml version="1.0" encoding="UTF-8"?>

<rdf:RDF
 xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
 xmlns="http://purl.org/rss/1.0/"
 xmlns:content="http://purl.org/rss/1.0/modules/content/"
 xmlns:taxo="http://purl.org/rss/1.0/modules/taxonomy/"
 xmlns:dc="http://purl.org/dc/elements/1.1/"
 xmlns:syn="http://purl.org/rss/1.0/modules/syndication/"
 xmlns:admin="http://webns.net/mvcb/"
>

<channel rdf:about="http://arxiv.org/">
<title>cs.CV updates on arXiv.org</title>
<link>http://arxiv.org/</link>
<description rdf:parseType="Literal">Computer Science -- Computer Vision and Pattern Recognition (cs.CV) updates on the arXiv.org e-print archive</description>
<dc:language>en-us</dc:language>
<dc:date>2023-07-19T20:30:00-05:00</dc:date>
<dc:publisher>help@arxiv.org</dc:publisher>
<dc:subject>Computer Science -- Computer Vision and Pattern Recognition</dc:subject>
<syn:updateBase>1901-01-01T00:00+00:00</syn:updateBase>
<syn:updateFrequency>1</syn:updateFrequency>
<syn:updatePeriod>daily</syn:updatePeriod>
<items>
 <rdf:Seq>
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09520" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09542" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09548" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09555" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09562" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09570" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09582" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09588" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09604" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09615" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09621" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09636" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09642" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09662" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09676" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09693" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09696" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09715" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09721" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09724" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09728" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09729" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09732" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09742" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09748" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09749" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09755" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09756" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09758" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09763" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09769" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09781" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09787" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09788" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09794" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09795" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09804" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09810" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09815" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09818" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09821" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09822" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09823" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09827" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09829" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09841" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09847" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09856" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09857" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09861" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09880" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09886" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09892" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09893" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09906" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09915" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09929" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09931" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09933" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09936" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09944" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09947" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09972" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09973" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09981" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09988" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09994" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09997" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10001" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10008" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10011" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10036" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10039" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10046" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10062" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10094" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10097" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10123" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10129" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10157" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10159" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10160" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10165" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10166" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.10173" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2001.05887" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2009.06205" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2103.03328" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.03544" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2110.04830" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2111.01396" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2112.06809" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2203.16475" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2205.11397" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2207.00419" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10741" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2208.10967" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.03189" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.06551" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.12003" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2210.16117" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2212.04761" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2301.02307" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.02330" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.05086" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2302.08890" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.06635" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.08096" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.09340" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13126" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2303.13479" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.05417" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.06403" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2304.10727" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.03238" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09211" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.09946" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.13659" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2305.18060" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07591" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.07894" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.09618" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.13074" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2306.16197" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.00574" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.01533" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.02203" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.03135" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04639" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.04838" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.05151" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06385" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.06689" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07813" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07859" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07873" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07928" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.07935" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08015" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08347" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08723" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08779" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.08913" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09023" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09153" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09362" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09416" />
  <rdf:li rdf:resource="http://arxiv.org/abs/2307.09456" />
 </rdf:Seq>
</items>
<image rdf:resource="http://arxiv.org/icons/sfx.gif" />
</channel>
<image rdf:about="http://arxiv.org/icons/sfx.gif">
<title>arXiv.org</title>
<url>http://arxiv.org/icons/sfx.gif</url>
<link>http://arxiv.org/</link>
</image>
<item rdf:about="http://arxiv.org/abs/2307.09520">
<title>Adversarial Bayesian Augmentation for Single-Source Domain Generalization. (arXiv:2307.09520v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09520</link>
<description rdf:parseType="Literal">&lt;p&gt;Generalizing to unseen image domains is a challenging problem primarily due
to the lack of diverse training data, inaccessible target data, and the large
domain shift that may exist in many real-world settings. As such data
augmentation is a critical component of domain generalization methods that seek
to address this problem. We present Adversarial Bayesian Augmentation (ABA), a
novel algorithm that learns to generate image augmentations in the challenging
single-source domain generalization setting. ABA draws on the strengths of
adversarial learning and Bayesian neural networks to guide the generation of
diverse data augmentations -- these synthesized image domains aid the
classifier in generalizing to unseen domains. We demonstrate the strength of
ABA on several types of domain shift including style shift, subpopulation
shift, and shift in the medical imaging setting. ABA outperforms all previous
state-of-the-art methods, including pre-specified augmentations, pixel-based
and convolutional-based augmentations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sheng Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gokhale_T/0/1/0/all/0/1&quot;&gt;Tejas Gokhale&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yezhou Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09542">
<title>Can Neural Network Memorization Be Localized?. (arXiv:2307.09542v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09542</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent efforts at explaining the interplay of memorization and generalization
in deep overparametrized networks have posited that neural networks
$\textit{memorize}$ &quot;hard&quot; examples in the final few layers of the model.
Memorization refers to the ability to correctly predict on $\textit{atypical}$
examples of the training set. In this work, we show that rather than being
confined to individual layers, memorization is a phenomenon confined to a small
set of neurons in various layers of the model. First, via three experimental
sources of converging evidence, we find that most layers are redundant for the
memorization of examples and the layers that contribute to example memorization
are, in general, not the final layers. The three sources are $\textit{gradient
accounting}$ (measuring the contribution to the gradient norms from memorized
and clean examples), $\textit{layer rewinding}$ (replacing specific model
weights of a converged model with previous training checkpoints), and
$\textit{retraining}$ (training rewound layers only on clean examples). Second,
we ask a more generic question: can memorization be localized
$\textit{anywhere}$ in a model? We discover that memorization is often confined
to a small number of neurons or channels (around 5) of the model. Based on
these insights we propose a new form of dropout -- $\textit{example-tied
dropout}$ that enables us to direct the memorization of examples to an apriori
determined set of neurons. By dropping out these neurons, we are able to reduce
the accuracy on memorized examples from $100\%\to3\%$, while also reducing the
generalization gap.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Maini_P/0/1/0/all/0/1&quot;&gt;Pratyush Maini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozer_M/0/1/0/all/0/1&quot;&gt;Michael C. Mozer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sedghi_H/0/1/0/all/0/1&quot;&gt;Hanie Sedghi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lipton_Z/0/1/0/all/0/1&quot;&gt;Zachary C. Lipton&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kolter_J/0/1/0/all/0/1&quot;&gt;J. Zico Kolter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_C/0/1/0/all/0/1&quot;&gt;Chiyuan Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09548">
<title>Surgical Action Triplet Detection by Mixed Supervised Learning of Instrument-Tissue Interactions. (arXiv:2307.09548v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09548</link>
<description rdf:parseType="Literal">&lt;p&gt;Surgical action triplets describe instrument-tissue interactions as
(instrument, verb, target) combinations, thereby supporting a detailed analysis
of surgical scene activities and workflow. This work focuses on surgical action
triplet detection, which is challenging but more precise than the traditional
triplet recognition task as it consists of joint (1) localization of surgical
instruments and (2) recognition of the surgical action triplet associated with
every localized instrument. Triplet detection is highly complex due to the lack
of spatial triplet annotation. We analyze how the amount of instrument spatial
annotations affects triplet detection and observe that accurate instrument
localization does not guarantee better triplet detection due to the risk of
erroneous associations with the verbs and targets. To solve the two tasks, we
propose MCIT-IG, a two-stage network, that stands for Multi-Class
Instrument-aware Transformer-Interaction Graph. The MCIT stage of our network
models per class embedding of the targets as additional features to reduce the
risk of misassociating triplets. Furthermore, the IG stage constructs a
bipartite dynamic graph to model the interaction between the instruments and
targets, cast as the verbs. We utilize a mixed-supervised learning strategy
that combines weak target presence labels for MCIT and pseudo triplet labels
for IG to train our network. We observed that complementing minimal instrument
spatial annotations with target embeddings results in better triplet detection.
We evaluate our model on the CholecT50 dataset and show improved performance on
both instrument localization and triplet detection, topping the leaderboard of
the CholecTriplet challenge in MICCAI 2022.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_S/0/1/0/all/0/1&quot;&gt;Saurav Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nwoye_C/0/1/0/all/0/1&quot;&gt;Chinedu Innocent Nwoye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mutter_D/0/1/0/all/0/1&quot;&gt;Didier Mutter&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Padoy_N/0/1/0/all/0/1&quot;&gt;Nicolas Padoy&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09555">
<title>Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction. (arXiv:2307.09555v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09555</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling
scene appearance and geometry from multiview imagery. Recent work has also
begun to explore how to use additional supervision from lidar or depth sensor
measurements in the NeRF framework. However, previous lidar-supervised NeRFs
focus on rendering conventional camera imagery and use lidar-derived point
cloud data as auxiliary supervision; thus, they fail to incorporate the
underlying image formation model of the lidar. Here, we propose a novel method
for rendering transient NeRFs that take as input the raw, time-resolved photon
count histograms measured by a single-photon lidar system, and we seek to
render such histograms from novel views. Different from conventional NeRFs, the
approach relies on a time-resolved version of the volume rendering equation to
render the lidar measurements and capture transient light transport phenomena
at picosecond timescales. We evaluate our method on a first-of-its-kind dataset
of simulated and captured transient multiview scans from a prototype
single-photon lidar. Overall, our work brings NeRFs to a new dimension of
imaging at transient timescales, newly enabling rendering of transient imagery
from novel views. Additionally, we show that our approach recovers improved
geometry and conventional appearance compared to point cloud-based supervision
when training on few input viewpoints. Transient NeRFs may be especially useful
for applications which seek to simulate raw lidar measurements for downstream
tasks in autonomous driving, robotics, and remote sensing.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Malik_A/0/1/0/all/0/1&quot;&gt;Anagh Malik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mirdehghan_P/0/1/0/all/0/1&quot;&gt;Parsa Mirdehghan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nousias_S/0/1/0/all/0/1&quot;&gt;Sotiris Nousias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kutulakos_K/0/1/0/all/0/1&quot;&gt;Kiriakos N. Kutulakos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lindell_D/0/1/0/all/0/1&quot;&gt;David B. Lindell&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09562">
<title>Rethinking Intersection Over Union for Small Object Detection in Few-Shot Regime. (arXiv:2307.09562v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09562</link>
<description rdf:parseType="Literal">&lt;p&gt;In Few-Shot Object Detection (FSOD), detecting small objects is extremely
difficult. The limited supervision cripples the localization capabilities of
the models and a few pixels shift can dramatically reduce the Intersection over
Union (IoU) between the ground truth and predicted boxes for small objects. To
this end, we propose Scale-adaptive Intersection over Union (SIoU), a novel box
similarity measure. SIoU changes with the objects&apos; size, it is more lenient
with small object shifts. We conducted a user study and SIoU better aligns than
IoU with human judgment. Employing SIoU as an evaluation criterion helps to
build more user-oriented models. SIoU can also be used as a loss function to
prioritize small objects during training, outperforming existing loss
functions. SIoU improves small object detection in the non-few-shot regime, but
this setting is unrealistic in the industry as annotated detection datasets are
often too expensive to acquire. Hence, our experiments mainly focus on the
few-shot regime to demonstrate the superiority and versatility of SIoU loss.
SIoU improves significantly FSOD performance on small objects in both natural
(Pascal VOC and COCO datasets) and aerial images (DOTA and DIOR). In aerial
imagery, small objects are critical and SIoU loss achieves new state-of-the-art
FSOD on DOTA and DIOR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeune_P/0/1/0/all/0/1&quot;&gt;Pierre Le Jeune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mokraoui_A/0/1/0/all/0/1&quot;&gt;Anissa Mokraoui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09570">
<title>SAM-Path: A Segment Anything Model for Semantic Segmentation in Digital Pathology. (arXiv:2307.09570v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09570</link>
<description rdf:parseType="Literal">&lt;p&gt;Semantic segmentations of pathological entities have crucial clinical value
in computational pathology workflows. Foundation models, such as the Segment
Anything Model (SAM), have been recently proposed for universal use in
segmentation tasks. SAM shows remarkable promise in instance segmentation on
natural images. However, the applicability of SAM to computational pathology
tasks is limited due to the following factors: (1) lack of comprehensive
pathology datasets used in SAM training and (2) the design of SAM is not
inherently optimized for semantic segmentation tasks. In this work, we adapt
SAM for semantic segmentation by introducing trainable class prompts, followed
by further enhancements through the incorporation of a pathology encoder,
specifically a pathology foundation model. Our framework, SAM-Path enhances
SAM&apos;s ability to conduct semantic segmentation in digital pathology without
human input prompts. Through experiments on two public pathology datasets, the
BCSS and the CRAG datasets, we demonstrate that the fine-tuning with trainable
class prompts outperforms vanilla SAM with manual prompts and post-processing
by 27.52% in Dice score and 71.63% in IOU. On these two datasets, the proposed
additional pathology foundation model further achieves a relative improvement
of 5.07% to 5.12% in Dice score and 4.50% to 8.48% in IOU.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jingwei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ma_K/0/1/0/all/0/1&quot;&gt;Ke Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kapse_S/0/1/0/all/0/1&quot;&gt;Saarthak Kapse&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Saltz_J/0/1/0/all/0/1&quot;&gt;Joel Saltz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Vakalopoulou_M/0/1/0/all/0/1&quot;&gt;Maria Vakalopoulou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Prasanna_P/0/1/0/all/0/1&quot;&gt;Prateek Prasanna&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Samaras_D/0/1/0/all/0/1&quot;&gt;Dimitris Samaras&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09582">
<title>Guided Linear Upsampling. (arXiv:2307.09582v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09582</link>
<description rdf:parseType="Literal">&lt;p&gt;Guided upsampling is an effective approach for accelerating high-resolution
image processing. In this paper, we propose a simple yet effective guided
upsampling method. Each pixel in the high-resolution image is represented as a
linear interpolation of two low-resolution pixels, whose indices and weights
are optimized to minimize the upsampling error. The downsampling can be jointly
optimized in order to prevent missing small isolated regions. Our method can be
derived from the color line model and local color transformations. Compared to
previous methods, our method can better preserve detail effects while
suppressing artifacts such as bleeding and blurring. It is efficient, easy to
implement, and free of sensitive parameters. We evaluate the proposed method
with a wide range of image operators, and show its advantages through
quantitative and qualitative analysis. We demonstrate the advantages of our
method for both interactive image editing and real-time high-resolution video
processing. In particular, for interactive editing, the joint optimization can
be precomputed, thus allowing for instant feedback without hardware
acceleration.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shuangbing Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_F/0/1/0/all/0/1&quot;&gt;Fan Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianju Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_X/0/1/0/all/0/1&quot;&gt;Xueying Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_C/0/1/0/all/0/1&quot;&gt;Changhe Tu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09588">
<title>Automating Wood Species Detection and Classification in Microscopic Images of Fibrous Materials with Deep Learning. (arXiv:2307.09588v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09588</link>
<description rdf:parseType="Literal">&lt;p&gt;We have developed a methodology for the systematic generation of a large
image dataset of macerated wood references, which we used to generate image
data for nine hardwood genera. This is the basis for a substantial approach to
automate, for the first time, the identification of hardwood species in
microscopic images of fibrous materials by deep learning. Our methodology
includes a flexible pipeline for easy annotation of vessel elements. We compare
the performance of different neural network architectures and hyperparameters.
Our proposed method performs similarly well to human experts. In the future,
this will improve controls on global wood fiber product flows to protect
forests.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nieradzik_L/0/1/0/all/0/1&quot;&gt;Lars Nieradzik&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sieburg_Rockel_J/0/1/0/all/0/1&quot;&gt;J&amp;#xf6;rdis Sieburg-Rockel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Helmling_S/0/1/0/all/0/1&quot;&gt;Stephanie Helmling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Weibel_T/0/1/0/all/0/1&quot;&gt;Thomas Weibel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Olbrich_A/0/1/0/all/0/1&quot;&gt;Andrea Olbrich&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Stephani_H/0/1/0/all/0/1&quot;&gt;Henrike Stephani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09591">
<title>Gradient strikes back: How filtering out high frequencies improves explanations. (arXiv:2307.09591v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.09591</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent years have witnessed an explosion in the development of novel
prediction-based attribution methods, which have slowly been supplanting older
gradient-based methods to explain the decisions of deep neural networks.
However, it is still not clear why prediction-based methods outperform
gradient-based ones. Here, we start with an empirical observation: these two
approaches yield attribution maps with very different power spectra, with
gradient-based methods revealing more high-frequency content than
prediction-based methods. This observation raises multiple questions: What is
the source of this high-frequency information, and does it truly reflect
decisions made by the system? Lastly, why would the absence of high-frequency
information in prediction-based methods yield better explainability scores
along multiple metrics? We analyze the gradient of three representative visual
classification models and observe that it contains noisy information emanating
from high-frequencies. Furthermore, our analysis reveals that the operations
used in Convolutional Neural Networks (CNNs) for downsampling appear to be a
significant source of this high-frequency content -- suggesting aliasing as a
possible underlying basis. We then apply an optimal low-pass filter for
attribution maps and demonstrate that it improves gradient-based attribution
methods. We show that (i) removing high-frequency noise yields significant
improvements in the explainability scores obtained with gradient-based methods
across multiple models -- leading to (ii) a novel ranking of state-of-the-art
methods with gradient-based methods at the top. We believe that our results
will spur renewed interest in simpler and computationally more efficient
gradient-based methods for explainability.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Muzellec_S/0/1/0/all/0/1&quot;&gt;Sabine Muzellec&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Andeol_L/0/1/0/all/0/1&quot;&gt;Leo Andeol&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fel_T/0/1/0/all/0/1&quot;&gt;Thomas Fel&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+VanRullen_R/0/1/0/all/0/1&quot;&gt;Rufin VanRullen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Serre_T/0/1/0/all/0/1&quot;&gt;Thomas Serre&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09604">
<title>DenseMP: Unsupervised Dense Pre-training for Few-shot Medical Image Segmentation. (arXiv:2307.09604v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09604</link>
<description rdf:parseType="Literal">&lt;p&gt;Few-shot medical image semantic segmentation is of paramount importance in
the domain of medical image analysis. However, existing methodologies grapple
with the challenge of data scarcity during the training phase, leading to
over-fitting. To mitigate this issue, we introduce a novel Unsupervised Dense
Few-shot Medical Image Segmentation Model Training Pipeline (DenseMP) that
capitalizes on unsupervised dense pre-training. DenseMP is composed of two
distinct stages: (1) segmentation-aware dense contrastive pre-training, and (2)
few-shot-aware superpixel guided dense pre-training. These stages
collaboratively yield a pre-trained initial model specifically designed for
few-shot medical image segmentation, which can subsequently be fine-tuned on
the target dataset. Our proposed pipeline significantly enhances the
performance of the widely recognized few-shot segmentation model, PA-Net,
achieving state-of-the-art results on the Abd-CT and Abd-MRI datasets. Code
will be released after acceptance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_Z/0/1/0/all/0/1&quot;&gt;Zhaoxin Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_P/0/1/0/all/0/1&quot;&gt;Puquan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zeren Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Ce Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_T/0/1/0/all/0/1&quot;&gt;Tianyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Siyang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_M/0/1/0/all/0/1&quot;&gt;Min Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09615">
<title>Looking deeper into interpretable deep learning in neuroimaging: a comprehensive survey. (arXiv:2307.09615v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09615</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning (DL) models have been popular due to their ability to learn
directly from the raw data in an end-to-end paradigm, alleviating the concern
of a separate error-prone feature extraction phase. Recent DL-based
neuroimaging studies have also witnessed a noticeable performance advancement
over traditional machine learning algorithms. But the challenges of deep
learning models still exist because of the lack of transparency in these models
for their successful deployment in real-world applications. In recent years,
Explainable AI (XAI) has undergone a surge of developments mainly to get
intuitions of how the models reached the decisions, which is essential for
safety-critical domains such as healthcare, finance, and law enforcement
agencies. While the interpretability domain is advancing noticeably,
researchers are still unclear about what aspect of model learning a post hoc
method reveals and how to validate its reliability. This paper comprehensively
reviews interpretable deep learning models in the neuroimaging domain. Firstly,
we summarize the current status of interpretability resources in general,
focusing on the progression of methods, associated challenges, and opinions.
Secondly, we discuss how multiple recent neuroimaging studies leveraged model
interpretability to capture anatomical and functional brain alterations most
relevant to model predictions. Finally, we discuss the limitations of the
current practices and offer some valuable insights and guidance on how we can
steer our future research directions to make deep learning models substantially
interpretable and thus advance scientific understanding of brain disorders.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rahman_M/0/1/0/all/0/1&quot;&gt;Md. Mahfuzur Rahman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Calhoun_V/0/1/0/all/0/1&quot;&gt;Vince D. Calhoun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Plis_S/0/1/0/all/0/1&quot;&gt;Sergey M. Plis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09621">
<title>Conditional 360-degree Image Synthesis for Immersive Indoor Scene Decoration. (arXiv:2307.09621v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09621</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we address the problem of conditional scene decoration for
360-degree images. Our method takes a 360-degree background photograph of an
indoor scene and generates decorated images of the same scene in the panorama
view. To do this, we develop a 360-aware object layout generator that learns
latent object vectors in the 360-degree view to enable a variety of furniture
arrangements for an input 360-degree background image. We use this object
layout to condition a generative adversarial network to synthesize images of an
input scene. To further reinforce the generation capability of our model, we
develop a simple yet effective scene emptier that removes the generated
furniture and produces an emptied scene for our model to learn a cyclic
constraint. We train the model on the Structure3D dataset and show that our
model can generate diverse decorations with controllable object layout. Our
method achieves state-of-the-art performance on the Structure3D dataset and
generalizes well to the Zillow indoor scene dataset. Our user study confirms
the immersive experiences provided by the realistic image quality and furniture
layout in our generation results. Our implementation will be made available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_K/0/1/0/all/0/1&quot;&gt;Ka Chun Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pang_H/0/1/0/all/0/1&quot;&gt;Hong-Wing Pang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_B/0/1/0/all/0/1&quot;&gt;Binh-Son Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nguyen_D/0/1/0/all/0/1&quot;&gt;Duc Thanh Nguyen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yeung_S/0/1/0/all/0/1&quot;&gt;Sai-Kit Yeung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09636">
<title>Traffic-Domain Video Question Answering with Automatic Captioning. (arXiv:2307.09636v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09636</link>
<description rdf:parseType="Literal">&lt;p&gt;Video Question Answering (VidQA) exhibits remarkable potential in
facilitating advanced machine reasoning capabilities within the domains of
Intelligent Traffic Monitoring and Intelligent Transportation Systems.
Nevertheless, the integration of urban traffic scene knowledge into VidQA
systems has received limited attention in previous research endeavors. In this
work, we present a novel approach termed Traffic-domain Video Question
Answering with Automatic Captioning (TRIVIA), which serves as a
weak-supervision technique for infusing traffic-domain knowledge into large
video-language models. Empirical findings obtained from the SUTD-TrafficQA task
highlight the substantial enhancements achieved by TRIVIA, elevating the
accuracy of representative video-language models by a remarkable 6.5 points
(19.88%) compared to baseline settings. This pioneering methodology holds great
promise for driving advancements in the field, inspiring researchers and
practitioners alike to unlock the full potential of emerging video-language
models in traffic-related applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qasemi_E/0/1/0/all/0/1&quot;&gt;Ehsan Qasemi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Francis_J/0/1/0/all/0/1&quot;&gt;Jonathan M. Francis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Oltramari_A/0/1/0/all/0/1&quot;&gt;Alessandro Oltramari&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09642">
<title>Skin Lesion Correspondence Localization in Total Body Photography. (arXiv:2307.09642v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09642</link>
<description rdf:parseType="Literal">&lt;p&gt;Longitudinal tracking of skin lesions - finding correspondence, changes in
morphology, and texture - is beneficial to the early detection of melanoma.
However, it has not been well investigated in the context of full-body imaging.
We propose a novel framework combining geometric and texture information to
localize skin lesion correspondence from a source scan to a target scan in
total body photography (TBP). Body landmarks or sparse correspondence are first
created on the source and target 3D textured meshes. Every vertex on each of
the meshes is then mapped to a feature vector characterizing the geodesic
distances to the landmarks on that mesh. Then, for each lesion of interest
(LOI) on the source, its corresponding location on the target is first coarsely
estimated using the geometric information encoded in the feature vectors and
then refined using the texture information. We evaluated the framework
quantitatively on both a public and a private dataset, for which our success
rates (at 10 mm criterion) are comparable to the only reported longitudinal
study. As full-body 3D capture becomes more prevalent and has higher quality,
we expect the proposed method to constitute a valuable step in the longitudinal
tracking of skin lesions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Wei-Lun Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tashayyod_D/0/1/0/all/0/1&quot;&gt;Davood Tashayyod&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kang_J/0/1/0/all/0/1&quot;&gt;Jun Kang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gandjbakhche_A/0/1/0/all/0/1&quot;&gt;Amir Gandjbakhche&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazhdan_M/0/1/0/all/0/1&quot;&gt;Michael Kazhdan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Armand_M/0/1/0/all/0/1&quot;&gt;Mehran Armand&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09662">
<title>Object-aware Gaze Target Detection. (arXiv:2307.09662v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09662</link>
<description rdf:parseType="Literal">&lt;p&gt;Gaze target detection aims to predict the image location where the person is
looking and the probability that a gaze is out of the scene. Several works have
tackled this task by regressing a gaze heatmap centered on the gaze location,
however, they overlooked decoding the relationship between the people and the
gazed objects. This paper proposes a Transformer-based architecture that
automatically detects objects (including heads) in the scene to build
associations between every head and the gazed-head/object, resulting in a
comprehensive, explainable gaze analysis composed of: gaze target area, gaze
pixel point, the class and the image location of the gazed-object. Upon
evaluation of the in-the-wild benchmarks, our method achieves state-of-the-art
results on all metrics (up to 2.91% gain in AUC, 50% reduction in gaze
distance, and 9% gain in out-of-frame average precision) for gaze target
detection and 11-13% improvement in average precision for the classification
and the localization of the gazed-objects. The code of the proposed method is
available https://github.com/francescotonini/object-aware-gaze-target-detection
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tonini_F/0/1/0/all/0/1&quot;&gt;Francesco Tonini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DallAsen_N/0/1/0/all/0/1&quot;&gt;Nicola Dall&amp;#x27;Asen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1&quot;&gt;Cigdem Beyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09676">
<title>Domain Adaptation for Enhanced Object Detection in Foggy and Rainy Weather for Autonomous Driving. (arXiv:2307.09676v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09676</link>
<description rdf:parseType="Literal">&lt;p&gt;Most object detection models for autonomous driving may experience a
significant drop in performance when deployed in real-world applications, due
to the well-known domain shift issue. Supervised object detection methods for
autonomous driving usually assume a consistent feature distribution between
training and testing data, however, such assumptions may not always be the case
when weather conditions differ significantly. For example, an object detection
model trained under clear weather may not perform well in foggy or rainy
weather, due to the domain gap. Overcoming detection bottlenecks in foggy or
rainy weather scenarios is a significant challenge for autonomous vehicles
deployed in the wild. To address the domain gap in different weather
conditions, This paper proposes a novel domain adaptive object detection
framework for autonomous driving in foggy and rainy weather. Our method
leverages both image-level and object-level adaptation to reduce the domain
discrepancy in image style and object appearance. Additionally, to enhance the
model&apos;s performance under challenging samples, we introduce a new adversarial
gradient reversal layer that performs adversarial mining on hard examples
alongside domain adaptation. Moreover, we propose to generate an auxiliary
domain by data augmentation to enforce a new domain-level metric
regularization. Experimental results on public benchmarks demonstrate that
object detection performance is significantly improved when using our proposed
method in domain shift scenarios for autonomous driving applications.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jin Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1&quot;&gt;Qin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongkai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09693">
<title>GlobalMapper: Arbitrary-Shaped Urban Layout Generation. (arXiv:2307.09693v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09693</link>
<description rdf:parseType="Literal">&lt;p&gt;Modeling and designing urban building layouts is of significant interest in
computer vision, computer graphics, and urban applications. A building layout
consists of a set of buildings in city blocks defined by a network of roads. We
observe that building layouts are discrete structures, consisting of multiple
rows of buildings of various shapes, and are amenable to skeletonization for
mapping arbitrary city block shapes to a canonical form. Hence, we propose a
fully automatic approach to building layout generation using graph attention
networks. Our method generates realistic urban layouts given arbitrary road
networks, and enables conditional generation based on learned priors. Our
results, including user study, demonstrate superior performance as compared to
prior layout generation networks, support arbitrary city block and varying
building shapes as demonstrated by generating layouts for 28 large cities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_L/0/1/0/all/0/1&quot;&gt;Liu He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aliaga_D/0/1/0/all/0/1&quot;&gt;Daniel Aliaga&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09696">
<title>Towards Saner Deep Image Registration. (arXiv:2307.09696v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09696</link>
<description rdf:parseType="Literal">&lt;p&gt;With recent advances in computing hardware and surges of deep-learning
architectures, learning-based deep image registration methods have surpassed
their traditional counterparts, in terms of metric performance and inference
time. However, these methods focus on improving performance measurements such
as Dice, resulting in less attention given to model behaviors that are equally
desirable for registrations, especially for medical imaging. This paper
investigates these behaviors for popular learning-based deep registrations
under a sanity-checking microscope. We find that most existing registrations
suffer from low inverse consistency and nondiscrimination of identical pairs
due to overly optimized image similarities. To rectify these behaviors, we
propose a novel regularization-based sanity-enforcer method that imposes two
sanity checks on the deep model to reduce its inverse consistency errors and
increase its discriminative power simultaneously. Moreover, we derive a set of
theoretical guarantees for our sanity-checked image registration method, with
experimental results supporting our theoretical findings and their
effectiveness in increasing the sanity of models without sacrificing any
performance. Our code and models are available at
\url{https://github.com/tuffr5/Saner-deep-registration}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_B/0/1/0/all/0/1&quot;&gt;Bin Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1&quot;&gt;Ming Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Y/0/1/0/all/0/1&quot;&gt;Yan Yan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09715">
<title>Semantic-Aware Dual Contrastive Learning for Multi-label Image Classification. (arXiv:2307.09715v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09715</link>
<description rdf:parseType="Literal">&lt;p&gt;Extracting image semantics effectively and assigning corresponding labels to
multiple objects or attributes for natural images is challenging due to the
complex scene contents and confusing label dependencies. Recent works have
focused on modeling label relationships with graph and understanding object
regions using class activation maps (CAM). However, these methods ignore the
complex intra- and inter-category relationships among specific semantic
features, and CAM is prone to generate noisy information. To this end, we
propose a novel semantic-aware dual contrastive learning framework that
incorporates sample-to-sample contrastive learning (SSCL) as well as
prototype-to-sample contrastive learning (PSCL). Specifically, we leverage
semantic-aware representation learning to extract category-related local
discriminative features and construct category prototypes. Then based on SSCL,
label-level visual representations of the same category are aggregated
together, and features belonging to distinct categories are separated.
Meanwhile, we construct a novel PSCL module to narrow the distance between
positive samples and category prototypes and push negative samples away from
the corresponding category prototypes. Finally, the discriminative label-level
features related to the image content are accurately captured by the joint
training of the above three parts. Experiments on five challenging large-scale
public datasets demonstrate that our proposed method is effective and
outperforms the state-of-the-art methods. Code and supplementary materials are
released on https://github.com/yu-gi-oh-leilei/SADCL.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_L/0/1/0/all/0/1&quot;&gt;Leilei Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_D/0/1/0/all/0/1&quot;&gt;Dengdi Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_H/0/1/0/all/0/1&quot;&gt;Haifang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_B/0/1/0/all/0/1&quot;&gt;Bin Luo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09721">
<title>Multi-Grained Multimodal Interaction Network for Entity Linking. (arXiv:2307.09721v1 [cs.AI])</title>
<link>http://arxiv.org/abs/2307.09721</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal entity linking (MEL) task, which aims at resolving ambiguous
mentions to a multimodal knowledge graph, has attracted wide attention in
recent years. Though large efforts have been made to explore the complementary
effect among multiple modalities, however, they may fail to fully absorb the
comprehensive expression of abbreviated textual context and implicit visual
indication. Even worse, the inevitable noisy data may cause inconsistency of
different modalities during the learning process, which severely degenerates
the performance. To address the above issues, in this paper, we propose a novel
Multi-GraIned Multimodal InteraCtion Network $\textbf{(MIMIC)}$ framework for
solving the MEL task. Specifically, the unified inputs of mentions and entities
are first encoded by textual/visual encoders separately, to extract global
descriptive features and local detailed features. Then, to derive the
similarity matching score for each mention-entity pair, we device three
interaction units to comprehensively explore the intra-modal interaction and
inter-modal fusion among features of entities and mentions. In particular,
three modules, namely the Text-based Global-Local interaction Unit (TGLU),
Vision-based DuaL interaction Unit (VDLU) and Cross-Modal Fusion-based
interaction Unit (CMFU) are designed to capture and integrate the fine-grained
representation lying in abbreviated text and implicit visual cues. Afterwards,
we introduce a unit-consistency objective function via contrastive learning to
avoid inconsistency and model degradation. Experimental results on three public
benchmark datasets demonstrate that our solution outperforms various
state-of-the-art baselines, and ablation studies verify the effectiveness of
designed modules.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_P/0/1/0/all/0/1&quot;&gt;Pengfei Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_T/0/1/0/all/0/1&quot;&gt;Tong Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_S/0/1/0/all/0/1&quot;&gt;Shiwei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chen Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_L/0/1/0/all/0/1&quot;&gt;Linli Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_E/0/1/0/all/0/1&quot;&gt;Enhong Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09724">
<title>AesPA-Net: Aesthetic Pattern-Aware Style Transfer Networks. (arXiv:2307.09724v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09724</link>
<description rdf:parseType="Literal">&lt;p&gt;To deliver the artistic expression of the target style, recent studies
exploit the attention mechanism owing to its ability to map the local patches
of the style image to the corresponding patches of the content image. However,
because of the low semantic correspondence between arbitrary content and
artworks, the attention module repeatedly abuses specific local patches from
the style image, resulting in disharmonious and evident repetitive artifacts.
To overcome this limitation and accomplish impeccable artistic style transfer,
we focus on enhancing the attention mechanism and capturing the rhythm of
patterns that organize the style. In this paper, we introduce a novel metric,
namely pattern repeatability, that quantifies the repetition of patterns in the
style image. Based on the pattern repeatability, we propose Aesthetic
Pattern-Aware style transfer Networks (AesPA-Net) that discover the sweet spot
of local and global style expressions. In addition, we propose a novel
self-supervisory task to encourage the attention mechanism to learn precise and
meaningful semantic correspondence. Lastly, we introduce the patch-wise style
loss to transfer the elaborate rhythm of local patterns. Through qualitative
and quantitative evaluations, we verify the reliability of the proposed pattern
repeatability that aligns with human perception, and demonstrate the
superiority of the proposed framework.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_K/0/1/0/all/0/1&quot;&gt;Kibeom Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jeon_S/0/1/0/all/0/1&quot;&gt;Seogkyu Jeon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Junsoo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahn_N/0/1/0/all/0/1&quot;&gt;Namhyuk Ahn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_K/0/1/0/all/0/1&quot;&gt;Kunhee Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_P/0/1/0/all/0/1&quot;&gt;Pilhyeon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_D/0/1/0/all/0/1&quot;&gt;Daesik Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Uh_Y/0/1/0/all/0/1&quot;&gt;Youngjung Uh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Byun_H/0/1/0/all/0/1&quot;&gt;Hyeran Byun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09727">
<title>SAMConvex: Fast Discrete Optimization for CT Registration using Self-supervised Anatomical Embedding and Correlation Pyramid. (arXiv:2307.09727v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09727</link>
<description rdf:parseType="Literal">&lt;p&gt;Estimating displacement vector field via a cost volume computed in the
feature space has shown great success in image registration, but it suffers
excessive computation burdens. Moreover, existing feature descriptors only
extract local features incapable of representing the global semantic
information, which is especially important for solving large transformations.
To address the discussed issues, we propose SAMConvex, a fast coarse-to-fine
discrete optimization method for CT registration that includes a decoupled
convex optimization procedure to obtain deformation fields based on a
self-supervised anatomical embedding (SAM) feature extractor that captures both
local and global information. To be specific, SAMConvex extracts per-voxel
features and builds 6D correlation volumes based on SAM features, and
iteratively updates a flow field by performing lookups on the correlation
volumes with a coarse-to-fine scheme. SAMConvex outperforms the
state-of-the-art learning-based methods and optimization-based methods over two
inter-patient registration datasets (Abdomen CT and HeadNeck CT) and one
intra-patient registration dataset (Lung CT). Moreover, as an
optimization-based method, SAMConvex only takes $\sim2$s ($\sim5s$ with
instance optimization) for one paired images.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tian_L/0/1/0/all/0/1&quot;&gt;Lin Tian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mok_T/0/1/0/all/0/1&quot;&gt;Tony C. W. Mok&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_X/0/1/0/all/0/1&quot;&gt;Xiaoyu Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Puyang Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ge_J/0/1/0/all/0/1&quot;&gt;Jia Ge&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jingren Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_L/0/1/0/all/0/1&quot;&gt;Le Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xianghua Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_K/0/1/0/all/0/1&quot;&gt;Ke Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_D/0/1/0/all/0/1&quot;&gt;Dakai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09728">
<title>Uncertainty-Driven Multi-Scale Feature Fusion Network for Real-time Image Deraining. (arXiv:2307.09728v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09728</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual-based measurement systems are frequently affected by rainy weather due
to the degradation caused by rain streaks in captured images, and existing
imaging devices struggle to address this issue in real-time. While most efforts
leverage deep networks for image deraining and have made progress, their large
parameter sizes hinder deployment on resource-constrained devices.
Additionally, these data-driven models often produce deterministic results,
without considering their inherent epistemic uncertainty, which can lead to
undesired reconstruction errors. Well-calibrated uncertainty can help alleviate
prediction errors and assist measurement devices in mitigating risks and
improving usability. Therefore, we propose an Uncertainty-Driven Multi-Scale
Feature Fusion Network (UMFFNet) that learns the probability mapping
distribution between paired images to estimate uncertainty. Specifically, we
introduce an uncertainty feature fusion block (UFFB) that utilizes uncertainty
information to dynamically enhance acquired features and focus on blurry
regions obscured by rain streaks, reducing prediction errors. In addition, to
further boost the performance of UMFFNet, we fused feature information from
multiple scales to guide the network for efficient collaborative rain removal.
Extensive experiments demonstrate that UMFFNet achieves significant performance
improvements with few parameters, surpassing state-of-the-art image deraining
methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tong_M/0/1/0/all/0/1&quot;&gt;Ming Tong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_X/0/1/0/all/0/1&quot;&gt;Xuefeng Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yongzhen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09729">
<title>NTIRE 2023 Quality Assessment of Video Enhancement Challenge. (arXiv:2307.09729v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09729</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper reports on the NTIRE 2023 Quality Assessment of Video Enhancement
Challenge, which will be held in conjunction with the New Trends in Image
Restoration and Enhancement Workshop (NTIRE) at CVPR 2023. This challenge is to
address a major challenge in the field of video processing, namely, video
quality assessment (VQA) for enhanced videos. The challenge uses the VQA
Dataset for Perceptual Video Enhancement (VDPVE), which has a total of 1211
enhanced videos, including 600 videos with color, brightness, and contrast
enhancements, 310 videos with deblurring, and 301 deshaked videos. The
challenge has a total of 167 registered participants. 61 participating teams
submitted their prediction results during the development phase, with a total
of 3168 submissions. A total of 176 submissions were submitted by 37
participating teams during the final testing phase. Finally, 19 participating
teams submitted their models and fact sheets, and detailed the methods they
used. Some methods have achieved better results than baseline methods, and the
winning methods have demonstrated superior prediction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaohong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Min_X/0/1/0/all/0/1&quot;&gt;Xiongkuo Min&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_W/0/1/0/all/0/1&quot;&gt;Wei Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_K/0/1/0/all/0/1&quot;&gt;Kai Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Timofte_R/0/1/0/all/0/1&quot;&gt;Radu Timofte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhai_G/0/1/0/all/0/1&quot;&gt;Guangtao Zhai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yixuan Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_Y/0/1/0/all/0/1&quot;&gt;Yuqin Cao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kou_T/0/1/0/all/0/1&quot;&gt;Tengchuan Kou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Y/0/1/0/all/0/1&quot;&gt;Yunlong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jia_Z/0/1/0/all/0/1&quot;&gt;Ziheng Jia&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yilin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shuming Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deng_S/0/1/0/all/0/1&quot;&gt;Sibin Deng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_P/0/1/0/all/0/1&quot;&gt;Pengxiang Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Ying Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_K/0/1/0/all/0/1&quot;&gt;Kai Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_K/0/1/0/all/0/1&quot;&gt;Kun Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_M/0/1/0/all/0/1&quot;&gt;Ming Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cong_H/0/1/0/all/0/1&quot;&gt;Heng Cong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_L/0/1/0/all/0/1&quot;&gt;Lingzhi Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yusheng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_R/0/1/0/all/0/1&quot;&gt;Rongyu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_H/0/1/0/all/0/1&quot;&gt;Hang Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_Q/0/1/0/all/0/1&quot;&gt;Qihang Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiao_L/0/1/0/all/0/1&quot;&gt;Longan Xiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhiliang Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Agarla_M/0/1/0/all/0/1&quot;&gt;Mirko Agarla&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Celona_L/0/1/0/all/0/1&quot;&gt;Luigi Celona&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rota_C/0/1/0/all/0/1&quot;&gt;Claudio Rota&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schettini_R/0/1/0/all/0/1&quot;&gt;Raimondo Schettini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Z/0/1/0/all/0/1&quot;&gt;Zhiwei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yanan Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xiaotao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_L/0/1/0/all/0/1&quot;&gt;Lei Lei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hongye Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_W/0/1/0/all/0/1&quot;&gt;Wei Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chuang_I/0/1/0/all/0/1&quot;&gt;Ironhead Chuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_A/0/1/0/all/0/1&quot;&gt;Allen Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guan_D/0/1/0/all/0/1&quot;&gt;Drake Guan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_I/0/1/0/all/0/1&quot;&gt;Iris Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lou_K/0/1/0/all/0/1&quot;&gt;Kae Lou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_W/0/1/0/all/0/1&quot;&gt;Willy Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tasi_Y/0/1/0/all/0/1&quot;&gt;Yachun Tasi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kao_Y/0/1/0/all/0/1&quot;&gt;Yvonne Kao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Haotian Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_F/0/1/0/all/0/1&quot;&gt;Fangyuan Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_S/0/1/0/all/0/1&quot;&gt;Shiqi Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_H/0/1/0/all/0/1&quot;&gt;Hao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lai_Y/0/1/0/all/0/1&quot;&gt;Yu Lai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_S/0/1/0/all/0/1&quot;&gt;Shanshan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_H/0/1/0/all/0/1&quot;&gt;Haoning Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chaofeng Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_C/0/1/0/all/0/1&quot;&gt;Chunzheng Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Z/0/1/0/all/0/1&quot;&gt;Zekun Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shiling Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_H/0/1/0/all/0/1&quot;&gt;Haibing Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hongkui Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meftah_H/0/1/0/all/0/1&quot;&gt;Hanene Brachemi Meftah&lt;/a&gt;, et al. (8 additional authors not shown)</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09732">
<title>ClickSeg: 3D Instance Segmentation with Click-Level Weak Annotations. (arXiv:2307.09732v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09732</link>
<description rdf:parseType="Literal">&lt;p&gt;3D instance segmentation methods often require fully-annotated dense labels
for training, which are costly to obtain. In this paper, we present ClickSeg, a
novel click-level weakly supervised 3D instance segmentation method that
requires one point per instance annotation merely. Such a problem is very
challenging due to the extremely limited labels, which has rarely been solved
before. We first develop a baseline weakly-supervised training method, which
generates pseudo labels for unlabeled data by the model itself. To utilize the
property of click-level annotation setting, we further propose a new training
framework. Instead of directly using the model inference way, i.e., mean-shift
clustering, to generate the pseudo labels, we propose to use k-means with fixed
initial seeds: the annotated points. New similarity metrics are further
designed for clustering. Experiments on ScanNetV2 and S3DIS datasets show that
the proposed ClickSeg surpasses the previous best weakly supervised instance
segmentation result by a large margin (e.g., +9.4% mAP on ScanNetV2). Using
0.02% supervision signals merely, ClickSeg achieves $\sim$90% of the accuracy
of the fully-supervised counterpart. Meanwhile, it also achieves
state-of-the-art semantic segmentation results among weakly supervised methods
that use the same annotation settings.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Leyao Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kong_T/0/1/0/all/0/1&quot;&gt;Tao Kong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_M/0/1/0/all/0/1&quot;&gt;Minzhao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jiashuo Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Lu Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09742">
<title>Improved Distribution Matching for Dataset Condensation. (arXiv:2307.09742v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09742</link>
<description rdf:parseType="Literal">&lt;p&gt;Dataset Condensation aims to condense a large dataset into a smaller one
while maintaining its ability to train a well-performing model, thus reducing
the storage cost and training effort in deep learning applications. However,
conventional dataset condensation methods are optimization-oriented and
condense the dataset by performing gradient or parameter matching during model
optimization, which is computationally intensive even on small datasets and
models. In this paper, we propose a novel dataset condensation method based on
distribution matching, which is more efficient and promising. Specifically, we
identify two important shortcomings of naive distribution matching (i.e.,
imbalanced feature numbers and unvalidated embeddings for distance computation)
and address them with three novel techniques (i.e., partitioning and expansion
augmentation, efficient and enriched model sampling, and class-aware
distribution regularization). Our simple yet effective method outperforms most
previous optimization-oriented methods with much fewer computational resources,
thereby scaling data condensation to larger datasets and models. Extensive
experiments demonstrate the effectiveness of our method. Codes are available at
https://github.com/uitrbn/IDM
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_G/0/1/0/all/0/1&quot;&gt;Ganlong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_G/0/1/0/all/0/1&quot;&gt;Guanbin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qin_Y/0/1/0/all/0/1&quot;&gt;Yipeng Qin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Y/0/1/0/all/0/1&quot;&gt;Yizhou Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09748">
<title>Watch out Venomous Snake Species: A Solution to SnakeCLEF2023. (arXiv:2307.09748v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09748</link>
<description rdf:parseType="Literal">&lt;p&gt;The SnakeCLEF2023 competition aims to the development of advanced algorithms
for snake species identification through the analysis of images and
accompanying metadata. This paper presents a method leveraging utilization of
both images and metadata. Modern CNN models and strong data augmentation are
utilized to learn better representation of images. To relieve the challenge of
long-tailed distribution, seesaw loss is utilized in our method. We also design
a light model to calculate prior probabilities using metadata features
extracted from CLIP in post processing stage. Besides, we attach more
importance to venomous species by assigning venomous species labels to some
examples that model is uncertain about. Our method achieves 91.31% score of the
final metric combined of F1 and other metrics on private leaderboard, which is
the 1st place among the participators. The code is available at
https://github.com/xiaoxsparraw/CLEF2023.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_F/0/1/0/all/0/1&quot;&gt;Feiran Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yangyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duan_C/0/1/0/all/0/1&quot;&gt;Chenlong Duan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_Z/0/1/0/all/0/1&quot;&gt;Zijian Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_F/0/1/0/all/0/1&quot;&gt;Fei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_F/0/1/0/all/0/1&quot;&gt;Faen Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xiu-Shen Wei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09749">
<title>Towards Robust Scene Text Image Super-resolution via Explicit Location Enhancement. (arXiv:2307.09749v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09749</link>
<description rdf:parseType="Literal">&lt;p&gt;Scene text image super-resolution (STISR), aiming to improve image quality
while boosting downstream scene text recognition accuracy, has recently
achieved great success. However, most existing methods treat the foreground
(character regions) and background (non-character regions) equally in the
forward process, and neglect the disturbance from the complex background, thus
limiting the performance. To address these issues, in this paper, we propose a
novel method LEMMA that explicitly models character regions to produce
high-level text-specific guidance for super-resolution. To model the location
of characters effectively, we propose the location enhancement module to
extract character region features based on the attention map sequence. Besides,
we propose the multi-modal alignment module to perform bidirectional
visual-semantic alignment to generate high-quality prior guidance, which is
then incorporated into the super-resolution branch in an adaptive manner using
the proposed adaptive fusion module. Experiments on TextZoom and four scene
text recognition benchmarks demonstrate the superiority of our method over
other state-of-the-art methods. Code is available at
https://github.com/csguoh/LEMMA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hang Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_T/0/1/0/all/0/1&quot;&gt;Tao Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_G/0/1/0/all/0/1&quot;&gt;Guanghao Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xia_S/0/1/0/all/0/1&quot;&gt;Shu-Tao Xia&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09755">
<title>Space Engage: Collaborative Space Supervision for Contrastive-based Semi-Supervised Semantic Segmentation. (arXiv:2307.09755v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09755</link>
<description rdf:parseType="Literal">&lt;p&gt;Semi-Supervised Semantic Segmentation (S4) aims to train a segmentation model
with limited labeled images and a substantial volume of unlabeled images. To
improve the robustness of representations, powerful methods introduce a
pixel-wise contrastive learning approach in latent space (i.e., representation
space) that aggregates the representations to their prototypes in a fully
supervised manner. However, previous contrastive-based S4 methods merely rely
on the supervision from the model&apos;s output (logits) in logit space during
unlabeled training. In contrast, we utilize the outputs in both logit space and
representation space to obtain supervision in a collaborative way. The
supervision from two spaces plays two roles: 1) reduces the risk of
over-fitting to incorrect semantic information in logits with the help of
representations; 2) enhances the knowledge exchange between the two spaces.
Furthermore, unlike previous approaches, we use the similarity between
representations and prototypes as a new indicator to tilt training those
under-performing representations and achieve a more efficient contrastive
learning process. Results on two public benchmarks demonstrate the competitive
performance of our method compared with state-of-the-art methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Changqi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_H/0/1/0/all/0/1&quot;&gt;Haoyu Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_Y/0/1/0/all/0/1&quot;&gt;Yuhui Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_C/0/1/0/all/0/1&quot;&gt;Chong Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yue_X/0/1/0/all/0/1&quot;&gt;Xiangyu Yue&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09756">
<title>Generative Prompt Model for Weakly Supervised Object Localization. (arXiv:2307.09756v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09756</link>
<description rdf:parseType="Literal">&lt;p&gt;Weakly supervised object localization (WSOL) remains challenging when
learning object localization models from image category labels. Conventional
methods that discriminatively train activation models ignore representative yet
less discriminative object parts. In this study, we propose a generative prompt
model (GenPromp), defining the first generative pipeline to localize less
discriminative object parts by formulating WSOL as a conditional image
denoising procedure. During training, GenPromp converts image category labels
to learnable prompt embeddings which are fed to a generative model to
conditionally recover the input image with noise and learn representative
embeddings. During inference, enPromp combines the representative embeddings
with discriminative embeddings (queried from an off-the-shelf vision-language
model) for both representative and discriminative capacity. The combined
embeddings are finally used to generate multi-scale high-quality attention
maps, which facilitate localizing full object extent. Experiments on
CUB-200-2011 and ILSVRC show that GenPromp respectively outperforms the best
discriminative models by 5.2% and 5.6% (Top-1 Loc), setting a solid baseline
for WSOL with the generative model. Code is available at
https://github.com/callsys/GenPromp.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Y/0/1/0/all/0/1&quot;&gt;Yuzhong Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_Q/0/1/0/all/0/1&quot;&gt;Qixiang Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Weijia Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_F/0/1/0/all/0/1&quot;&gt;Fang Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09758">
<title>Longitudinal Data and a Semantic Similarity Reward for Chest X-Ray Report Generation. (arXiv:2307.09758v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09758</link>
<description rdf:parseType="Literal">&lt;p&gt;Chest X-Ray (CXR) report generation is a promising approach to improving the
efficiency of CXR interpretation. However, a significant increase in diagnostic
accuracy is required before that can be realised. Motivated by this, we propose
a framework that is more inline with a radiologist&apos;s workflow by considering
longitudinal data. Here, the decoder is additionally conditioned on the report
from the subject&apos;s previous imaging study via a prompt. We also propose a new
reward for reinforcement learning based on CXR-BERT, which computes the
similarity between reports. We conduct experiments on the MIMIC-CXR dataset.
The results indicate that longitudinal data improves CXR report generation.
CXR-BERT is also shown to be a promising alternative to the current
state-of-the-art reward based on RadGraph. This investigation indicates that
longitudinal CXR report generation can offer a substantial increase in
diagnostic accuracy. Our Hugging Face model is available at:
https://huggingface.co/aehrc/cxrmate and code is available at:
https://github.com/aehrc/cxrmate.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolson_A/0/1/0/all/0/1&quot;&gt;Aaron Nicolson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dowling_J/0/1/0/all/0/1&quot;&gt;Jason Dowling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koopman_B/0/1/0/all/0/1&quot;&gt;Bevan Koopman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09763">
<title>Towards Building More Robust Models with Frequency Bias. (arXiv:2307.09763v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09763</link>
<description rdf:parseType="Literal">&lt;p&gt;The vulnerability of deep neural networks to adversarial samples has been a
major impediment to their broad applications, despite their success in various
fields. Recently, some works suggested that adversarially-trained models
emphasize the importance of low-frequency information to achieve higher
robustness. While several attempts have been made to leverage this frequency
characteristic, they have all faced the issue that applying low-pass filters
directly to input images leads to irreversible loss of discriminative
information and poor generalizability to datasets with distinct frequency
features. This paper presents a plug-and-play module called the Frequency
Preference Control Module that adaptively reconfigures the low- and
high-frequency components of intermediate feature representations, providing
better utilization of frequency in robust learning. Empirical studies show that
our proposed module can be easily incorporated into any adversarial training
framework, further improving model robustness across different architectures
and datasets. Additionally, experiments were conducted to examine how the
frequency bias of robust models impacts the adversarial training process and
its final robustness, revealing interesting insights.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bu_Q/0/1/0/all/0/1&quot;&gt;Qingwen Bu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_D/0/1/0/all/0/1&quot;&gt;Dong Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_H/0/1/0/all/0/1&quot;&gt;Heming Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09769">
<title>Source-Free Domain Adaptation for Medical Image Segmentation via Prototype-Anchored Feature Alignment and Contrastive Learning. (arXiv:2307.09769v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09769</link>
<description rdf:parseType="Literal">&lt;p&gt;Unsupervised domain adaptation (UDA) has increasingly gained interests for
its capacity to transfer the knowledge learned from a labeled source domain to
an unlabeled target domain. However, typical UDA methods require concurrent
access to both the source and target domain data, which largely limits its
application in medical scenarios where source data is often unavailable due to
privacy concern. To tackle the source data-absent problem, we present a novel
two-stage source-free domain adaptation (SFDA) framework for medical image
segmentation, where only a well-trained source segmentation model and unlabeled
target data are available during domain adaptation. Specifically, in the
prototype-anchored feature alignment stage, we first utilize the weights of the
pre-trained pixel-wise classifier as source prototypes, which preserve the
information of source features. Then, we introduce the bi-directional transport
to align the target features with class prototypes by minimizing its expected
cost. On top of that, a contrastive learning stage is further devised to
utilize those pixels with unreliable predictions for a more compact target
feature distribution. Extensive experiments on a cross-modality medical
segmentation task demonstrate the superiority of our method in large domain
discrepancy settings compared with the state-of-the-art SFDA approaches and
even some UDA methods. Code is available at
https://github.com/CSCYQJ/MICCAI23-ProtoContra-SFDA.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Q/0/1/0/all/0/1&quot;&gt;Qinji Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xi_N/0/1/0/all/0/1&quot;&gt;Nan Xi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yuan_J/0/1/0/all/0/1&quot;&gt;Junsong Yuan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Z/0/1/0/all/0/1&quot;&gt;Ziyu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dang_K/0/1/0/all/0/1&quot;&gt;Kang Dang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ding_X/0/1/0/all/0/1&quot;&gt;Xiaowei Ding&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09781">
<title>Text2Layer: Layered Image Generation using Latent Diffusion Model. (arXiv:2307.09781v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09781</link>
<description rdf:parseType="Literal">&lt;p&gt;Layer compositing is one of the most popular image editing workflows among
both amateurs and professionals. Motivated by the success of diffusion models,
we explore layer compositing from a layered image generation perspective.
Instead of generating an image, we propose to generate background, foreground,
layer mask, and the composed image simultaneously. To achieve layered image
generation, we train an autoencoder that is able to reconstruct layered images
and train diffusion models on the latent representation. One benefit of the
proposed problem is to enable better compositing workflows in addition to the
high-quality image output. Another benefit is producing higher-quality layer
masks compared to masks produced by a separate step of image segmentation.
Experimental results show that the proposed method is able to generate
high-quality layered images and initiates a benchmark for future work.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xinyang Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_W/0/1/0/all/0/1&quot;&gt;Wentian Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_X/0/1/0/all/0/1&quot;&gt;Xin Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chien_J/0/1/0/all/0/1&quot;&gt;Jeff Chien&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09787">
<title>DVPT: Dynamic Visual Prompt Tuning of Large Pre-trained Models for Medical Image Analysis. (arXiv:2307.09787v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09787</link>
<description rdf:parseType="Literal">&lt;p&gt;Limited labeled data makes it hard to train models from scratch in medical
domain, and an important paradigm is pre-training and then fine-tuning. Large
pre-trained models contain rich representations, which can be adapted to
downstream medical tasks. However, existing methods either tune all the
parameters or the task-specific layers of the pre-trained models, ignoring the
input variations of medical images, and thus they are not efficient or
effective. In this work, we aim to study parameter-efficient fine-tuning (PEFT)
for medical image analysis, and propose a dynamic visual prompt tuning method,
named DVPT. It can extract knowledge beneficial to downstream tasks from large
models with a few trainable parameters. Firstly, the frozen features are
transformed by an lightweight bottleneck layer to learn the domain-specific
distribution of downstream medical tasks, and then a few learnable visual
prompts are used as dynamic queries and then conduct cross-attention with the
transformed features, attempting to acquire sample-specific knowledge that are
suitable for each sample. Finally, the features are projected to original
feature dimension and aggregated with the frozen features. This DVPT module can
be shared between different Transformer layers, further reducing the trainable
parameters. To validate DVPT, we conduct extensive experiments with different
pre-trained models on medical classification and segmentation tasks. We find
such PEFT method can not only efficiently adapt the pre-trained models to the
medical domain, but also brings data efficiency with partial labeled data. For
example, with 0.5\% extra trainable parameters, our method not only outperforms
state-of-the-art PEFT methods, even surpasses the full fine-tuning by more than
2.20\% Kappa score on medical classification task. It can saves up to 60\%
labeled data and 99\% storage cost of ViT-B/16.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_A/0/1/0/all/0/1&quot;&gt;Along He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_K/0/1/0/all/0/1&quot;&gt;Kai Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zhihong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_T/0/1/0/all/0/1&quot;&gt;Tao Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_H/0/1/0/all/0/1&quot;&gt;Huazhu Fu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09788">
<title>Density-invariant Features for Distant Point Cloud Registration. (arXiv:2307.09788v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09788</link>
<description rdf:parseType="Literal">&lt;p&gt;Registration of distant outdoor LiDAR point clouds is crucial to extending
the 3D vision of collaborative autonomous vehicles, and yet is challenging due
to small overlapping area and a huge disparity between observed point
densities. In this paper, we propose Group-wise Contrastive Learning (GCL)
scheme to extract density-invariant geometric features to register distant
outdoor LiDAR point clouds. We mark through theoretical analysis and
experiments that, contrastive positives should be independent and identically
distributed (i.i.d.), in order to train densityinvariant feature extractors. We
propose upon the conclusion a simple yet effective training scheme to force the
feature of multiple point clouds in the same spatial location (referred to as
positive groups) to be similar, which naturally avoids the sampling bias
introduced by a pair of point clouds to conform with the i.i.d. principle. The
resulting fully-convolutional feature extractor is more powerful and
density-invariant than state-of-the-art methods, improving the registration
recall of distant scenarios on KITTI and nuScenes benchmarks by 40.9% and
26.9%, respectively. The code will be open-sourced.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Q/0/1/0/all/0/1&quot;&gt;Quan Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_H/0/1/0/all/0/1&quot;&gt;Hongzi Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Y/0/1/0/all/0/1&quot;&gt;Yunsong Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongyang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_S/0/1/0/all/0/1&quot;&gt;Shan Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Minyi Guo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09794">
<title>DiffDP: Radiotherapy Dose Prediction via a Diffusion Model. (arXiv:2307.09794v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09794</link>
<description rdf:parseType="Literal">&lt;p&gt;Currently, deep learning (DL) has achieved the automatic prediction of dose
distribution in radiotherapy planning, enhancing its efficiency and quality.
However, existing methods suffer from the over-smoothing problem for their
commonly used L_1 or L_2 loss with posterior average calculations. To alleviate
this limitation, we innovatively introduce a diffusion-based dose prediction
(DiffDP) model for predicting the radiotherapy dose distribution of cancer
patients. Specifically, the DiffDP model contains a forward process and a
reverse process. In the forward process, DiffDP gradually transforms dose
distribution maps into Gaussian noise by adding small noise and trains a noise
predictor to predict the noise added in each timestep. In the reverse process,
it removes the noise from the original Gaussian noise in multiple steps with
the well-trained noise predictor and finally outputs the predicted dose
distribution map. To ensure the accuracy of the prediction, we further design a
structure encoder to extract anatomical information from patient anatomy images
and enable the noise predictor to be aware of the dose constraints within
several essential organs, i.e., the planning target volume and organs at risk.
Extensive experiments on an in-house dataset with 130 rectum cancer patients
demonstrate the s
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_Z/0/1/0/all/0/1&quot;&gt;Zhenghao Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wen_L/0/1/0/all/0/1&quot;&gt;Lu Wen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_P/0/1/0/all/0/1&quot;&gt;Peng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yan_B/0/1/0/all/0/1&quot;&gt;Binyu Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xi Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhou_J/0/1/0/all/0/1&quot;&gt;Jiliu Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09795">
<title>From West to East: Who can understand the music of the others better?. (arXiv:2307.09795v1 [cs.SD])</title>
<link>http://arxiv.org/abs/2307.09795</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent developments in MIR have led to several benchmark deep learning models
whose embeddings can be used for a variety of downstream tasks. At the same
time, the vast majority of these models have been trained on Western pop/rock
music and related styles. This leads to research questions on whether these
models can be used to learn representations for different music cultures and
styles, or whether we can build similar music audio embedding models trained on
data from different cultures or styles. To that end, we leverage transfer
learning methods to derive insights about the similarities between the
different music cultures to which the data belongs to. We use two Western music
datasets, two traditional/folk datasets coming from eastern Mediterranean
cultures, and two datasets belonging to Indian art music. Three deep audio
embedding models are trained and transferred across domains, including two
CNN-based and a Transformer-based architecture, to perform auto-tagging for
each target domain dataset. Experimental results show that competitive
performance is achieved in all domains via transfer learning, while the best
source dataset varies for each music culture. The implementation and the
trained models are both provided in a public repository.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papaioannou_C/0/1/0/all/0/1&quot;&gt;Charilaos Papaioannou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benetos_E/0/1/0/all/0/1&quot;&gt;Emmanouil Benetos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potamianos_A/0/1/0/all/0/1&quot;&gt;Alexandros Potamianos&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09804">
<title>Fix your downsampling ASAP! Be natively more robust via Aliasing and Spectral Artifact free Pooling. (arXiv:2307.09804v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09804</link>
<description rdf:parseType="Literal">&lt;p&gt;Convolutional neural networks encode images through a sequence of
convolutions, normalizations and non-linearities as well as downsampling
operations into potentially strong semantic embeddings. Yet, previous work
showed that even slight mistakes during sampling, leading to aliasing, can be
directly attributed to the networks&apos; lack in robustness. To address such issues
and facilitate simpler and faster adversarial training, [12] recently proposed
FLC pooling, a method for provably alias-free downsampling - in theory. In this
work, we conduct a further analysis through the lens of signal processing and
find that such current pooling methods, which address aliasing in the frequency
domain, are still prone to spectral leakage artifacts. Hence, we propose
aliasing and spectral artifact-free pooling, short ASAP. While only introducing
a few modifications to FLC pooling, networks using ASAP as downsampling method
exhibit higher native robustness against common corruptions, a property that
FLC pooling was missing. ASAP also increases native robustness against
adversarial attacks on high and low resolution data while maintaining similar
clean accuracy or even outperforming the baseline.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabinski_J/0/1/0/all/0/1&quot;&gt;Julia Grabinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09810">
<title>GenKL: An Iterative Framework for Resolving Label Ambiguity and Label Non-conformity in Web Images Via a New Generalized KL Divergence. (arXiv:2307.09810v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09810</link>
<description rdf:parseType="Literal">&lt;p&gt;Web image datasets curated online inherently contain ambiguous
in-distribution (ID) instances and out-of-distribution (OOD) instances, which
we collectively call non-conforming (NC) instances. In many recent approaches
for mitigating the negative effects of NC instances, the core implicit
assumption is that the NC instances can be found via entropy maximization. For
&quot;entropy&quot; to be well-defined, we are interpreting the output prediction vector
of an instance as the parameter vector of a multinomial random variable, with
respect to some trained model with a softmax output layer. Hence, entropy
maximization is based on the idealized assumption that NC instances have
predictions that are &quot;almost&quot; uniformly distributed. However, in real-world web
image datasets, there are numerous NC instances whose predictions are far from
being uniformly distributed. To tackle the limitation of entropy maximization,
we propose $(\alpha, \beta)$-generalized KL divergence,
$\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$, which can be used to identify
significantly more NC instances. Theoretical properties of
$\mathcal{D}_{\text{KL}}^{\alpha, \beta}(p\|q)$ are proven, and we also show
empirically that a simple use of $\mathcal{D}_{\text{KL}}^{\alpha,
\beta}(p\|q)$ outperforms all baselines on the NC instance identification task.
Building upon $(\alpha,\beta)$-generalized KL divergence, we also introduce a
new iterative training framework, GenKL, that identifies and relabels NC
instances. When evaluated on three web image datasets, Clothing1M,
Food101/Food101N, and mini WebVision 1.0, we achieved new state-of-the-art
classification accuracies: $81.34\%$, $85.73\%$ and $78.99\%$/$92.54\%$
(top-1/top-5), respectively.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_X/0/1/0/all/0/1&quot;&gt;Xia Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chong_K/0/1/0/all/0/1&quot;&gt;Kai Fong Ernest Chong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09815">
<title>LDP: Language-driven Dual-Pixel Image Defocus Deblurring Network. (arXiv:2307.09815v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09815</link>
<description rdf:parseType="Literal">&lt;p&gt;Recovering sharp images from dual-pixel (DP) pairs with disparity-dependent
blur is a challenging task.~Existing blur map-based deblurring methods have
demonstrated promising results. In this paper, we propose, to the best of our
knowledge, the first framework to introduce the contrastive language-image
pre-training framework (CLIP) to achieve accurate blur map estimation from DP
pairs unsupervisedly. To this end, we first carefully design text prompts to
enable CLIP to understand blur-related geometric prior knowledge from the DP
pair. Then, we propose a format to input stereo DP pair to the CLIP without any
fine-tuning, where the CLIP is pre-trained on monocular images. Given the
estimated blur map, we introduce a blur-prior attention block, a blur-weighting
loss and a blur-aware loss to recover the all-in-focus image. Our method
achieves state-of-the-art performance in extensive experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_H/0/1/0/all/0/1&quot;&gt;Hao Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_L/0/1/0/all/0/1&quot;&gt;Liyuan Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Y/0/1/0/all/0/1&quot;&gt;Yan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Miaomiao Liu&lt;/a&gt;, </dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09818">
<title>Deep unrolling Shrinkage Network for Dynamic MR imaging. (arXiv:2307.09818v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09818</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep unrolling networks that utilize sparsity priors have achieved great
success in dynamic magnetic resonance (MR) imaging. The convolutional neural
network (CNN) is usually utilized to extract the transformed domain, and then
the soft thresholding (ST) operator is applied to the CNN-transformed data to
enforce the sparsity priors. However, the ST operator is usually constrained to
be the same across all channels of the CNN-transformed data. In this paper, we
propose a novel operator, called soft thresholding with channel attention
(AST), that learns the threshold for each channel. In particular, we put
forward a novel deep unrolling shrinkage network (DUS-Net) by unrolling the
alternating direction method of multipliers (ADMM) for optimizing the
transformed $l_1$ norm dynamic MR reconstruction model. Experimental results on
an open-access dynamic cine MR dataset demonstrate that the proposed DUS-Net
outperforms the state-of-the-art methods. The source code is available at
\url{https://github.com/yhao-z/DUS-Net}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yinghao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiaodi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_W/0/1/0/all/0/1&quot;&gt;Weihang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Hu_Y/0/1/0/all/0/1&quot;&gt;Yue Hu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09821">
<title>Hierarchical Semantic Perceptual Listener Head Video Generation: A High-performance Pipeline. (arXiv:2307.09821v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09821</link>
<description rdf:parseType="Literal">&lt;p&gt;In dyadic speaker-listener interactions, the listener&apos;s head reactions along
with the speaker&apos;s head movements, constitute an important non-verbal semantic
expression together. The listener Head generation task aims to synthesize
responsive listener&apos;s head videos based on audios of the speaker and reference
images of the listener. Compared to the Talking-head generation, it is more
challenging to capture the correlation clues from the speaker&apos;s audio and
visual information. Following the ViCo baseline scheme, we propose a
high-performance solution by enhancing the hierarchical semantic extraction
capability of the audio encoder module and improving the decoder part, renderer
and post-processing modules. Our solution gets the first place on the official
leaderboard for the track of listening head generation. This paper is a
technical report of ViCo@2023 Conversational Head Generation Challenge in ACM
Multimedia 2023 conference.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chang_Z/0/1/0/all/0/1&quot;&gt;Zhigang Chang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_W/0/1/0/all/0/1&quot;&gt;Weitai Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_Q/0/1/0/all/0/1&quot;&gt;Qing Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_S/0/1/0/all/0/1&quot;&gt;Shibao Zheng&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09822">
<title>A Siamese-based Verification System for Open-set Architecture Attribution of Synthetic Images. (arXiv:2307.09822v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09822</link>
<description rdf:parseType="Literal">&lt;p&gt;Despite the wide variety of methods developed for synthetic image
attribution, most of them can only attribute images generated by models or
architectures included in the training set and do not work with unknown
architectures, hindering their applicability in real-world scenarios. In this
paper, we propose a verification framework that relies on a Siamese Network to
address the problem of open-set attribution of synthetic images to the
architecture that generated them. We consider two different settings. In the
first setting, the system determines whether two images have been produced by
the same generative architecture or not. In the second setting, the system
verifies a claim about the architecture used to generate a synthetic image,
utilizing one or multiple reference images generated by the claimed
architecture. The main strength of the proposed system is its ability to
operate in both closed and open-set scenarios so that the input images, either
the query and reference images, can belong to the architectures considered
during training or not. Experimental evaluations encompassing various
generative architectures such as GANs, diffusion models, and transformers,
focusing on synthetic face image generation, confirm the excellent performance
of our method in both closed and open-set settings, as well as its strong
generalization capabilities.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abady_L/0/1/0/all/0/1&quot;&gt;Lydia Abady&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jun Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tondi_B/0/1/0/all/0/1&quot;&gt;Benedetta Tondi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barni_M/0/1/0/all/0/1&quot;&gt;Mauro Barni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09823">
<title>Multi-modal Learning based Prediction for Disease. (arXiv:2307.09823v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09823</link>
<description rdf:parseType="Literal">&lt;p&gt;Non alcoholic fatty liver disease (NAFLD) is the most common cause of chronic
liver disease, which can be predicted accurately to prevent advanced fibrosis
and cirrhosis. While, a liver biopsy, the gold standard for NAFLD diagnosis, is
invasive, expensive, and prone to sampling errors. Therefore, non-invasive
studies are extremely promising, yet they are still in their infancy due to the
lack of comprehensive research data and intelligent methods for multi-modal
data. This paper proposes a NAFLD diagnosis system (DeepFLDDiag) combining a
comprehensive clinical dataset (FLDData) and a multi-modal learning based NAFLD
prediction method (DeepFLD). The dataset includes over 6000 participants
physical examinations, laboratory and imaging studies, extensive
questionnaires, and facial images of partial participants, which is
comprehensive and valuable for clinical studies. From the dataset, we
quantitatively analyze and select clinical metadata that most contribute to
NAFLD prediction. Furthermore, the proposed DeepFLD, a deep neural network
model designed to predict NAFLD using multi-modal input, including metadata and
facial images, outperforms the approach that only uses metadata. Satisfactory
performance is also verified on other unseen datasets. Inspiringly, DeepFLD can
achieve competitive results using only facial images as input rather than
metadata, paving the way for a more robust and simpler non-invasive NAFLD
diagnosis.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yaran Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xueyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Han_Y/0/1/0/all/0/1&quot;&gt;Yu Han&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Haoran Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhao_D/0/1/0/all/0/1&quot;&gt;Dongbin Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jingzhong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Wang_X/0/1/0/all/0/1&quot;&gt;Xu Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09827">
<title>Online Continual Learning for Robust Indoor Object Recognition. (arXiv:2307.09827v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.09827</link>
<description rdf:parseType="Literal">&lt;p&gt;Vision systems mounted on home robots need to interact with unseen classes in
changing environments. Robots have limited computational resources, labelled
data and storage capability. These requirements pose some unique challenges:
models should adapt without forgetting past knowledge in a data- and
parameter-efficient way. We characterize the problem as few-shot (FS) online
continual learning (OCL), where robotic agents learn from a non-repeated stream
of few-shot data updating only a few model parameters. Additionally, such
models experience variable conditions at test time, where objects may appear in
different poses (e.g., horizontal or vertical) and environments (e.g., day or
night). To improve robustness of CL agents, we propose RobOCLe, which; 1)
constructs an enriched feature space computing high order statistical moments
from the embedded features of samples; and 2) computes similarity between high
order statistics of the samples on the enriched feature space, and predicts
their class labels. We evaluate robustness of CL models to train/test
augmentations in various cases. We show that different moments allow RobOCLe to
capture different properties of deformations, providing higher robustness with
no decrease of inference speed.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Michieli_U/0/1/0/all/0/1&quot;&gt;Umberto Michieli&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ozay_M/0/1/0/all/0/1&quot;&gt;Mete Ozay&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09829">
<title>What do neural networks learn in image classification? A frequency shortcut perspective. (arXiv:2307.09829v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09829</link>
<description rdf:parseType="Literal">&lt;p&gt;Frequency analysis is useful for understanding the mechanisms of
representation learning in neural networks (NNs). Most research in this area
focuses on the learning dynamics of NNs for regression tasks, while little for
classification. This study empirically investigates the latter and expands the
understanding of frequency shortcuts. First, we perform experiments on
synthetic datasets, designed to have a bias in different frequency bands. Our
results demonstrate that NNs tend to find simple solutions for classification,
and what they learn first during training depends on the most distinctive
frequency characteristics, which can be either low- or high-frequencies.
Second, we confirm this phenomenon on natural images. We propose a metric to
measure class-wise frequency characteristics and a method to identify frequency
shortcuts. The results show that frequency shortcuts can be texture-based or
shape-based, depending on what best simplifies the objective. Third, we
validate the transferability of frequency shortcuts on out-of-distribution
(OOD) test sets. Our results suggest that frequency shortcuts can be
transferred across datasets and cannot be fully avoided by larger model
capacity and data augmentation. We recommend that future research should focus
on effective training schemes mitigating frequency shortcut learning.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_S/0/1/0/all/0/1&quot;&gt;Shunxin Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Veldhuis_R/0/1/0/all/0/1&quot;&gt;Raymond Veldhuis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Brune_C/0/1/0/all/0/1&quot;&gt;Christoph Brune&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Strisciuglio_N/0/1/0/all/0/1&quot;&gt;Nicola Strisciuglio&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09841">
<title>Compressive Image Scanning Microscope. (arXiv:2307.09841v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09841</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a novel approach to implement compressive sensing in laser
scanning microscopes (LSM), specifically in image scanning microscopy (ISM),
using a single-photon avalanche diode (SPAD) array detector. Our method
addresses two significant limitations in applying compressive sensing to LSM:
the time to compute the sampling matrix and the quality of reconstructed
images. We employ a fixed sampling strategy, skipping alternate rows and
columns during data acquisition, which reduces the number of points scanned by
a factor of four and eliminates the need to compute different sampling
matrices. By exploiting the parallel images generated by the SPAD array, we
improve the quality of the reconstructed compressive-ISM images compared to
standard compressive confocal LSM images. Our results demonstrate the
effectiveness of our approach in producing higher-quality images with reduced
data acquisition time and potential benefits in reducing photobleaching.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gunalan_A/0/1/0/all/0/1&quot;&gt;Ajay Gunalan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Castello_M/0/1/0/all/0/1&quot;&gt;Marco Castello&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Piazza_S/0/1/0/all/0/1&quot;&gt;Simonluca Piazza&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shunlei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Diaspro_A/0/1/0/all/0/1&quot;&gt;Alberto Diaspro&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Mattos_L/0/1/0/all/0/1&quot;&gt;Leonardo S. Mattos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bianchini_P/0/1/0/all/0/1&quot;&gt;Paolo Bianchini&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09847">
<title>Cryo-forum: A framework for orientation recovery with uncertainty measure with the application in cryo-EM image analysis. (arXiv:2307.09847v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.09847</link>
<description rdf:parseType="Literal">&lt;p&gt;In single-particle cryo-electron microscopy (cryo-EM), the efficient
determination of orientation parameters for 2D projection images poses a
significant challenge yet is crucial for reconstructing 3D structures. This
task is complicated by the high noise levels present in the cryo-EM datasets,
which often include outliers, necessitating several time-consuming 2D clean-up
processes. Recently, solutions based on deep learning have emerged, offering a
more streamlined approach to the traditionally laborious task of orientation
estimation. These solutions often employ amortized inference, eliminating the
need to estimate parameters individually for each image. However, these methods
frequently overlook the presence of outliers and may not adequately concentrate
on the components used within the network. This paper introduces a novel
approach that uses a 10-dimensional feature vector to represent the orientation
and applies a Quadratically-Constrained Quadratic Program to derive the
predicted orientation as a unit quaternion, supplemented by an uncertainty
metric. Furthermore, we propose a unique loss function that considers the
pairwise distances between orientations, thereby enhancing the accuracy of our
method. Finally, we also comprehensively evaluate the design choices involved
in constructing the encoder network, a topic that has not received sufficient
attention in the literature. Our numerical analysis demonstrates that our
methodology effectively recovers orientations from 2D cryo-EM images in an
end-to-end manner. Importantly, the inclusion of uncertainty quantification
allows for direct clean-up of the dataset at the 3D level. Lastly, we package
our proposed methods into a user-friendly software suite named cryo-forum,
designed for easy accessibility by the developers.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chung_S/0/1/0/all/0/1&quot;&gt;Szu-Chi Chung&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09856">
<title>Hierarchical Spatio-Temporal Representation Learning for Gait Recognition. (arXiv:2307.09856v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09856</link>
<description rdf:parseType="Literal">&lt;p&gt;Gait recognition is a biometric technique that identifies individuals by
their unique walking styles, which is suitable for unconstrained environments
and has a wide range of applications. While current methods focus on exploiting
body part-based representations, they often neglect the hierarchical
dependencies between local motion patterns. In this paper, we propose a
hierarchical spatio-temporal representation learning (HSTL) framework for
extracting gait features from coarse to fine. Our framework starts with a
hierarchical clustering analysis to recover multi-level body structures from
the whole body to local details. Next, an adaptive region-based motion
extractor (ARME) is designed to learn region-independent motion features. The
proposed HSTL then stacks multiple ARMEs in a top-down manner, with each ARME
corresponding to a specific partition level of the hierarchy. An adaptive
spatio-temporal pooling (ASTP) module is used to capture gait features at
different levels of detail to perform hierarchical feature mapping. Finally, a
frame-level temporal aggregation (FTA) module is employed to reduce redundant
information in gait sequences through multi-scale temporal downsampling.
Extensive experiments on CASIA-B, OUMVLP, GREW, and Gait3D datasets demonstrate
that our method outperforms the state-of-the-art while maintaining a reasonable
balance between model accuracy and complexity.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lei Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_B/0/1/0/all/0/1&quot;&gt;Bo Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liang_F/0/1/0/all/0/1&quot;&gt;Fangfang Liang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Bincheng Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09857">
<title>Blind Image Quality Assessment Using Multi-Stream Architecture with Spatial and Channel Attention. (arXiv:2307.09857v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09857</link>
<description rdf:parseType="Literal">&lt;p&gt;BIQA (Blind Image Quality Assessment) is an important field of study that
evaluates images automatically. Although significant progress has been made,
blind image quality assessment remains a difficult task since images vary in
content and distortions. Most algorithms generate quality without emphasizing
the important region of interest. In order to solve this, a multi-stream
spatial and channel attention-based algorithm is being proposed. This algorithm
generates more accurate predictions with a high correlation to human perceptual
assessment by combining hybrid features from two different backbones, followed
by spatial and channel attention to provide high weights to the region of
interest. Four legacy image quality assessment datasets are used to validate
the effectiveness of our proposed approach. Authentic and synthetic distortion
image databases are used to demonstrate the effectiveness of the proposed
method, and we show that it has excellent generalization properties with a
particular focus on the perceptual foreground information.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khalid_H/0/1/0/all/0/1&quot;&gt;Hassan Khalid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ahmed_N/0/1/0/all/0/1&quot;&gt;Nisar Ahmed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09861">
<title>BSDM: Background Suppression Diffusion Model for Hyperspectral Anomaly Detection. (arXiv:2307.09861v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09861</link>
<description rdf:parseType="Literal">&lt;p&gt;Hyperspectral anomaly detection (HAD) is widely used in Earth observation and
deep space exploration. A major challenge for HAD is the complex background of
the input hyperspectral images (HSIs), resulting in anomalies confused in the
background. On the other hand, the lack of labeled samples for HSIs leads to
poor generalization of existing HAD methods. This paper starts the first
attempt to study a new and generalizable background learning problem without
labeled samples. We present a novel solution BSDM (background suppression
diffusion model) for HAD, which can simultaneously learn latent background
distributions and generalize to different datasets for suppressing complex
background. It is featured in three aspects: (1) For the complex background of
HSIs, we design pseudo background noise and learn the potential background
distribution in it with a diffusion model (DM). (2) For the generalizability
problem, we apply a statistical offset module so that the BSDM adapts to
datasets of different domains without labeling samples. (3) For achieving
background suppression, we innovatively improve the inference process of DM by
feeding the original HSIs into the denoising network, which removes the
background as noise. Our work paves a new background suppression way for HAD
that can improve HAD performance without the prerequisite of manually labeled
data. Assessments and generalization experiments of four HAD methods on several
real HSI datasets demonstrate the above three unique properties of the proposed
method. The code is available at https://github.com/majitao-xd/BSDM-HAD.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jitao Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xie_W/0/1/0/all/0/1&quot;&gt;Weiying Xie&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yunsong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_L/0/1/0/all/0/1&quot;&gt;Leyuan Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09880">
<title>A3D: Adaptive, Accurate, and Autonomous Navigation for Edge-Assisted Drones. (arXiv:2307.09880v1 [cs.NI])</title>
<link>http://arxiv.org/abs/2307.09880</link>
<description rdf:parseType="Literal">&lt;p&gt;Accurate navigation is of paramount importance to ensure flight safety and
efficiency for autonomous drones. Recent research starts to use Deep Neural
Networks to enhance drone navigation given their remarkable predictive
capability for visual perception. However, existing solutions either run DNN
inference tasks on drones in situ, impeded by the limited onboard resource, or
offload the computation to external servers which may incur large network
latency. Few works consider jointly optimizing the offloading decisions along
with image transmission configurations and adapting them on the fly. In this
paper, we propose A3D, an edge server assisted drone navigation framework that
can dynamically adjust task execution location, input resolution, and image
compression ratio in order to achieve low inference latency, high prediction
accuracy, and long flight distances. Specifically, we first augment
state-of-the-art convolutional neural networks for drone navigation and define
a novel metric called Quality of Navigation as our optimization objective which
can effectively capture the above goals. We then design a deep reinforcement
learning based neural scheduler at the drone side for which an information
encoder is devised to reshape the state features and thus improve its learning
ability. To further support simultaneous multi-drone serving, we extend the
edge server design by developing a network-aware resource allocation algorithm,
which allows provisioning containerized resources aligned with drones&apos; demand.
We finally implement a proof-of-concept prototype with realistic devices and
validate its performance in a real-world campus scene, as well as a simulation
environment for thorough evaluation upon AirSim. Extensive experimental results
show that A3D can reduce end-to-end latency by 28.06% and extend the flight
distance by up to 27.28% compared with non-adaptive solutions.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_L/0/1/0/all/0/1&quot;&gt;Liekang Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Haowei Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Daipeng Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_X/0/1/0/all/0/1&quot;&gt;Xiaoxi Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xu Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09886">
<title>A reinforcement learning approach for VQA validation: an application to diabetic macular edema grading. (arXiv:2307.09886v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09886</link>
<description rdf:parseType="Literal">&lt;p&gt;Recent advances in machine learning models have greatly increased the
performance of automated methods in medical image analysis. However, the
internal functioning of such models is largely hidden, which hinders their
integration in clinical practice. Explainability and trust are viewed as
important aspects of modern methods, for the latter&apos;s widespread use in
clinical communities. As such, validation of machine learning models represents
an important aspect and yet, most methods are only validated in a limited way.
In this work, we focus on providing a richer and more appropriate validation
approach for highly powerful Visual Question Answering (VQA) algorithms. To
better understand the performance of these methods, which answer arbitrary
questions related to images, this work focuses on an automatic visual Turing
test (VTT). That is, we propose an automatic adaptive questioning method, that
aims to expose the reasoning behavior of a VQA algorithm. Specifically, we
introduce a reinforcement learning (RL) agent that observes the history of
previously asked questions, and uses it to select the next question to pose. We
demonstrate our approach in the context of evaluating algorithms that
automatically answer questions related to diabetic macular edema (DME) grading.
The experiments show that such an agent has similar behavior to a clinician,
whereby asking questions that are relevant to key clinical concepts.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fountoukidou_T/0/1/0/all/0/1&quot;&gt;Tatiana Fountoukidou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sznitman_R/0/1/0/all/0/1&quot;&gt;Raphael Sznitman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09892">
<title>3Deformer: A Common Framework for Image-Guided Mesh Deformation. (arXiv:2307.09892v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09892</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose 3Deformer, a general-purpose framework for interactive 3D shape
editing. Given a source 3D mesh with semantic materials, and a user-specified
semantic image, 3Deformer can accurately edit the source mesh following the
shape guidance of the semantic image, while preserving the source topology as
rigid as possible. Recent studies of 3D shape editing mostly focus on learning
neural networks to predict 3D shapes, which requires high-cost 3D training
datasets and is limited to handling objects involved in the datasets. Unlike
these studies, our 3Deformer is a non-training and common framework, which only
requires supervision of readily-available semantic images, and is compatible
with editing various objects unlimited by datasets. In 3Deformer, the source
mesh is deformed utilizing the differentiable renderer technique, according to
the correspondences between semantic images and mesh materials. However,
guiding complex 3D shapes with a simple 2D image incurs extra challenges, that
is, the deform accuracy, surface smoothness, geometric rigidity, and global
synchronization of the edited mesh should be guaranteed. To address these
challenges, we propose a hierarchical optimization architecture to balance the
global and local shape features, and propose further various strategies and
losses to improve properties of accuracy, smoothness, rigidity, and so on.
Extensive experiments show that our 3Deformer is able to produce impressive
results and reaches the state-of-the-art level.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuefeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1&quot;&gt;Jianwei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Ji Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_X/0/1/0/all/0/1&quot;&gt;Xinghao Wu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09893">
<title>Learning from Abstract Images: on the Importance of Occlusion in a Minimalist Encoding of Human Poses. (arXiv:2307.09893v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09893</link>
<description rdf:parseType="Literal">&lt;p&gt;Existing 2D-to-3D pose lifting networks suffer from poor performance in
cross-dataset benchmarks. Although the use of 2D keypoints joined by
&quot;stick-figure&quot; limbs has shown promise as an intermediate step, stick-figures
do not account for occlusion information that is often inherent in an image. In
this paper, we propose a novel representation using opaque 3D limbs that
preserves occlusion information while implicitly encoding joint locations.
Crucially, when training on data with accurate three-dimensional keypoints and
without part-maps, this representation allows training on abstract synthetic
images, with occlusion, from as many synthetic viewpoints as desired. The
result is a pose defined by limb angles rather than joint positions
$\unicode{x2013}$ because poses are, in the real world, independent of cameras
$\unicode{x2013}$ allowing us to predict poses that are completely independent
of camera viewpoint. The result provides not only an improvement in
same-dataset benchmarks, but a &quot;quantum leap&quot; in cross-dataset benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Manzur_S/0/1/0/all/0/1&quot;&gt;Saad Manzur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hayes_W/0/1/0/all/0/1&quot;&gt;Wayne Hayes&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09906">
<title>Implicit Identity Representation Conditioned Memory Compensation Network for Talking Head video Generation. (arXiv:2307.09906v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09906</link>
<description rdf:parseType="Literal">&lt;p&gt;Talking head video generation aims to animate a human face in a still image
with dynamic poses and expressions using motion information derived from a
target-driving video, while maintaining the person&apos;s identity in the source
image. However, dramatic and complex motions in the driving video cause
ambiguous generation, because the still source image cannot provide sufficient
appearance information for occluded regions or delicate expression variations,
which produces severe artifacts and significantly degrades the generation
quality. To tackle this problem, we propose to learn a global facial
representation space, and design a novel implicit identity representation
conditioned memory compensation network, coined as MCNet, for high-fidelity
talking head generation.~Specifically, we devise a network module to learn a
unified spatial facial meta-memory bank from all training samples, which can
provide rich facial structure and appearance priors to compensate warped source
facial features for the generation. Furthermore, we propose an effective query
mechanism based on implicit identity representations learned from the discrete
keypoints of the source image. It can greatly facilitate the retrieval of more
correlated information from the memory bank for the compensation. Extensive
experiments demonstrate that MCNet can learn representative and complementary
facial memory, and can clearly outperform previous state-of-the-art talking
head generation methods on VoxCeleb1 and CelebV datasets. Please check our
\href{https://github.com/harlanhong/ICCV2023-MCNET}{Project}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_F/0/1/0/all/0/1&quot;&gt;Fa-Ting Hong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_D/0/1/0/all/0/1&quot;&gt;Dan Xu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09915">
<title>Embedded Heterogeneous Attention Transformer for Cross-lingual Image Captioning. (arXiv:2307.09915v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09915</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-lingual image captioning is confronted with both cross-lingual and
cross-modal challenges for multimedia analysis. The crucial issue in this task
is to model the global and local matching between the image and different
languages. Existing cross-modal embedding methods based on Transformer
architecture oversight the local matching between the image region and
monolingual words, not to mention in the face of a variety of differentiated
languages. Due to the heterogeneous property of the cross-modal and
cross-lingual task, we utilize the heterogeneous network to establish
cross-domain relationships and the local correspondences between the image and
different languages. In this paper, we propose an Embedded Heterogeneous
Attention Transformer (EHAT) to build reasoning paths bridging cross-domain for
cross-lingual image captioning and integrate into transformer. The proposed
EHAT consists of a Masked Heterogeneous Cross-attention (MHCA), Heterogeneous
Attention Reasoning Network (HARN) and Heterogeneous Co-attention (HCA). HARN
as the core network, models and infers cross-domain relationship anchored by
vision bounding box representation features to connect two languages word
features and learn the heterogeneous maps. MHCA and HCA implement cross-domain
integration in the encoder through the special heterogeneous attention and
enable single model to generate two language captioning. We test on MSCOCO
dataset to generate English and Chinese, which are most widely used and have
obvious difference between their language families. Our experiments show that
our method even achieve better than advanced monolingual methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zijie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_Z/0/1/0/all/0/1&quot;&gt;Zhenzhen Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hong_R/0/1/0/all/0/1&quot;&gt;Richang Hong&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09929">
<title>Measuring and Modeling Uncertainty Degree for Monocular Depth Estimation. (arXiv:2307.09929v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09929</link>
<description rdf:parseType="Literal">&lt;p&gt;Effectively measuring and modeling the reliability of a trained model is
essential to the real-world deployment of monocular depth estimation (MDE)
models. However, the intrinsic ill-posedness and ordinal-sensitive nature of
MDE pose major challenges to the estimation of uncertainty degree of the
trained models. On the one hand, utilizing current uncertainty modeling methods
may increase memory consumption and are usually time-consuming. On the other
hand, measuring the uncertainty based on model accuracy can also be
problematic, where uncertainty reliability and prediction accuracy are not well
decoupled. In this paper, we propose to model the uncertainty of MDE models
from the perspective of the inherent probability distributions originating from
the depth probability volume and its extensions, and to assess it more fairly
with more comprehensive metrics. By simply introducing additional training
regularization terms, our model, with surprisingly simple formations and
without requiring extra modules or multiple inferences, can provide uncertainty
estimations with state-of-the-art reliability, and can be further improved when
combined with ensemble or sampling methods. A series of experiments demonstrate
the effectiveness of our methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xiang_M/0/1/0/all/0/1&quot;&gt;Mochu Xiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jing Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Barnes_N/0/1/0/all/0/1&quot;&gt;Nick Barnes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Y/0/1/0/all/0/1&quot;&gt;Yuchao Dai&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09931">
<title>DISA: DIfferentiable Similarity Approximation for Universal Multimodal Registration. (arXiv:2307.09931v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09931</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal image registration is a challenging but essential step for
numerous image-guided procedures. Most registration algorithms rely on the
computation of complex, frequently non-differentiable similarity metrics to
deal with the appearance discrepancy of anatomical structures between imaging
modalities. Recent Machine Learning based approaches are limited to specific
anatomy-modality combinations and do not generalize to new settings. We propose
a generic framework for creating expressive cross-modal descriptors that enable
fast deformable global registration. We achieve this by approximating existing
metrics with a dot-product in the feature space of a small convolutional neural
network (CNN) which is inherently differentiable can be trained without
registered data. Our method is several orders of magnitude faster than local
patch-based metrics and can be directly applied in clinical settings by
replacing the similarity measure with the proposed one. Experiments on three
different datasets demonstrate that our approach generalizes well beyond the
training data, yielding a broad capture range even on unseen anatomies and
modality pairs, without the need for specialized retraining. We make our
training code and data publicly available.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ronchetti_M/0/1/0/all/0/1&quot;&gt;Matteo Ronchetti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wein_W/0/1/0/all/0/1&quot;&gt;Wolfgang Wein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Navab_N/0/1/0/all/0/1&quot;&gt;Nassir Navab&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zettinig_O/0/1/0/all/0/1&quot;&gt;Oliver Zettinig&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Prevost_R/0/1/0/all/0/1&quot;&gt;Raphael Prevost&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09933">
<title>Spuriosity Didn&apos;t Kill the Classifier: Using Invariant Predictions to Harness Spurious Features. (arXiv:2307.09933v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09933</link>
<description rdf:parseType="Literal">&lt;p&gt;To avoid failures on out-of-distribution data, recent works have sought to
extract features that have a stable or invariant relationship with the label
across domains, discarding the &quot;spurious&quot; or unstable features whose
relationship with the label changes across domains. However, unstable features
often carry complementary information about the label that could boost
performance if used correctly in the test domain. Our main contribution is to
show that it is possible to learn how to use these unstable features in the
test domain without labels. In particular, we prove that pseudo-labels based on
stable features provide sufficient guidance for doing so, provided that stable
and unstable features are conditionally independent given the label. Based on
this theoretical insight, we propose Stable Feature Boosting (SFB), an
algorithm for: (i) learning a predictor that separates stable and
conditionally-independent unstable features; and (ii) using the stable-feature
predictions to adapt the unstable-feature predictions in the test domain.
Theoretically, we prove that SFB can learn an asymptotically-optimal predictor
without test-domain labels. Empirically, we demonstrate the effectiveness of
SFB on real and synthetic data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Eastwood_C/0/1/0/all/0/1&quot;&gt;Cian Eastwood&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Singh_S/0/1/0/all/0/1&quot;&gt;Shashank Singh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nicolicioiu_A/0/1/0/all/0/1&quot;&gt;Andrei Liviu Nicolicioiu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vlastelica_M/0/1/0/all/0/1&quot;&gt;Marin Vlastelica&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kugelgen_J/0/1/0/all/0/1&quot;&gt;Julius von K&amp;#xfc;gelgen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Scholkopf_B/0/1/0/all/0/1&quot;&gt;Bernhard Sch&amp;#xf6;lkopf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09936">
<title>AGAR: Attention Graph-RNN for Adaptative Motion Prediction of Point Clouds of Deformable Objects. (arXiv:2307.09936v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09936</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper focuses on motion prediction for point cloud sequences in the
challenging case of deformable 3D objects, such as human body motion. First, we
investigate the challenges caused by deformable shapes and complex motions
present in this type of representation, with the ultimate goal of understanding
the technical limitations of state-of-the-art models. From this understanding,
we propose an improved architecture for point cloud prediction of deformable 3D
objects. Specifically, to handle deformable shapes, we propose a graph-based
approach that learns and exploits the spatial structure of point clouds to
extract more representative features. Then we propose a module able to combine
the learned features in an adaptative manner according to the point cloud
movements. The proposed adaptative module controls the composition of local and
global motions for each point, enabling the network to model complex motions in
deformable 3D objects more effectively. We tested the proposed method on the
following datasets: MNIST moving digits, the Mixamo human bodies motions, JPEG
and CWIPC-SXR real-world dynamic bodies. Simulation results demonstrate that
our method outperforms the current baseline methods given its improved ability
to model complex movements as well as preserve point cloud shape. Furthermore,
we demonstrate the generalizability of the proposed framework for dynamic
feature learning, by testing the framework for action recognition on the
MSRAction3D dataset and achieving results on-par with state-of-the-art methods
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gomes_P/0/1/0/all/0/1&quot;&gt;Pedro Gomes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rossi_S/0/1/0/all/0/1&quot;&gt;Silvia Rossi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Toni_L/0/1/0/all/0/1&quot;&gt;Laura Toni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09944">
<title>ProtoCaps: A Fast and Non-Iterative Capsule Network Routing Method. (arXiv:2307.09944v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09944</link>
<description rdf:parseType="Literal">&lt;p&gt;Capsule Networks have emerged as a powerful class of deep learning
architectures, known for robust performance with relatively few parameters
compared to Convolutional Neural Networks (CNNs). However, their inherent
efficiency is often overshadowed by their slow, iterative routing mechanisms
which establish connections between Capsule layers, posing computational
challenges resulting in an inability to scale. In this paper, we introduce a
novel, non-iterative routing mechanism, inspired by trainable prototype
clustering. This innovative approach aims to mitigate computational complexity,
while retaining, if not enhancing, performance efficacy. Furthermore, we
harness a shared Capsule subspace, negating the need to project each
lower-level Capsule to each higher-level Capsule, thereby significantly
reducing memory requisites during training. Our approach demonstrates superior
results compared to the current best non-iterative Capsule Network and tests on
the Imagewoof dataset, which is too computationally demanding to handle
efficiently by iterative approaches. Our findings underscore the potential of
our proposed methodology in enhancing the operational efficiency and
performance of Capsule Networks, paving the way for their application in
increasingly complex computational scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Everett_M/0/1/0/all/0/1&quot;&gt;Miles Everett&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhong_M/0/1/0/all/0/1&quot;&gt;Mingjun Zhong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leontidis_G/0/1/0/all/0/1&quot;&gt;Georgios Leontidis&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09947">
<title>U-CE: Uncertainty-aware Cross-Entropy for Semantic Segmentation. (arXiv:2307.09947v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09947</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep neural networks have shown exceptional performance in various tasks, but
their lack of robustness, reliability, and tendency to be overconfident pose
challenges for their deployment in safety-critical applications like autonomous
driving. In this regard, quantifying the uncertainty inherent to a model&apos;s
prediction is a promising endeavour to address these shortcomings. In this
work, we present a novel Uncertainty-aware Cross-Entropy loss (U-CE) that
incorporates dynamic predictive uncertainties into the training process by
pixel-wise weighting of the well-known cross-entropy loss (CE). Through
extensive experimentation, we demonstrate the superiority of U-CE over regular
CE training on two benchmark datasets, Cityscapes and ACDC, using two common
backbone architectures, ResNet-18 and ResNet-101. With U-CE, we manage to train
models that not only improve their segmentation performance but also provide
meaningful uncertainties after training. Consequently, we contribute to the
development of more robust and reliable segmentation models, ultimately
advancing the state-of-the-art in safety-critical applications and beyond.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Landgraf_S/0/1/0/all/0/1&quot;&gt;Steven Landgraf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hillemann_M/0/1/0/all/0/1&quot;&gt;Markus Hillemann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wursthorn_K/0/1/0/all/0/1&quot;&gt;Kira Wursthorn&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ulrich_M/0/1/0/all/0/1&quot;&gt;Markus Ulrich&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09972">
<title>Fine-grained Text-Video Retrieval with Frozen Image Encoders. (arXiv:2307.09972v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09972</link>
<description rdf:parseType="Literal">&lt;p&gt;State-of-the-art text-video retrieval (TVR) methods typically utilize CLIP
and cosine similarity for efficient retrieval. Meanwhile, cross attention
methods, which employ a transformer decoder to compute attention between each
text query and all frames in a video, offer a more comprehensive interaction
between text and videos. However, these methods lack important fine-grained
spatial information as they directly compute attention between text and
video-level tokens. To address this issue, we propose CrossTVR, a two-stage
text-video retrieval architecture. In the first stage, we leverage existing TVR
methods with cosine similarity network for efficient text/video candidate
selection. In the second stage, we propose a novel decoupled video text cross
attention module to capture fine-grained multimodal information in spatial and
temporal dimensions. Additionally, we employ the frozen CLIP model strategy in
fine-grained retrieval, enabling scalability to larger pre-trained vision
models like ViT-G, resulting in improved retrieval performance. Experiments on
text video retrieval datasets demonstrate the effectiveness and scalability of
our proposed CrossTVR compared to state-of-the-art approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_Z/0/1/0/all/0/1&quot;&gt;Zuozhuo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shao_F/0/1/0/all/0/1&quot;&gt;Fangtao Shao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_Q/0/1/0/all/0/1&quot;&gt;Qingkun Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_Z/0/1/0/all/0/1&quot;&gt;Zilong Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_S/0/1/0/all/0/1&quot;&gt;Siyu Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09973">
<title>Source-Free Domain Adaptive Fundus Image Segmentation with Class-Balanced Mean Teacher. (arXiv:2307.09973v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09973</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper studies source-free domain adaptive fundus image segmentation
which aims to adapt a pretrained fundus segmentation model to a target domain
using unlabeled images. This is a challenging task because it is highly risky
to adapt a model only using unlabeled data. Most existing methods tackle this
task mainly by designing techniques to carefully generate pseudo labels from
the model&apos;s predictions and use the pseudo labels to train the model. While
often obtaining positive adaption effects, these methods suffer from two major
issues. First, they tend to be fairly unstable - incorrect pseudo labels
abruptly emerged may cause a catastrophic impact on the model. Second, they
fail to consider the severe class imbalance of fundus images where the
foreground (e.g., cup) region is usually very small. This paper aims to address
these two issues by proposing the Class-Balanced Mean Teacher (CBMT) model.
CBMT addresses the unstable issue by proposing a weak-strong augmented mean
teacher learning scheme where only the teacher model generates pseudo labels
from weakly augmented images to train a student model that takes strongly
augmented images as input. The teacher is updated as the moving average of the
instantly trained student, which could be noisy. This prevents the teacher
model from being abruptly impacted by incorrect pseudo-labels. For the class
imbalance issue, CBMT proposes a novel loss calibration approach to highlight
foreground classes according to global statistics. Experiments show that CBMT
well addresses these two issues and outperforms existing methods on multiple
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tang_L/0/1/0/all/0/1&quot;&gt;Longxiang Tang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_K/0/1/0/all/0/1&quot;&gt;Kai Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_C/0/1/0/all/0/1&quot;&gt;Chunming He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yulun Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xiu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09981">
<title>Lazy Visual Localization via Motion Averaging. (arXiv:2307.09981v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09981</link>
<description rdf:parseType="Literal">&lt;p&gt;Visual (re)localization is critical for various applications in computer
vision and robotics. Its goal is to estimate the 6 degrees of freedom (DoF)
camera pose for each query image, based on a set of posed database images.
Currently, all leading solutions are structure-based that either explicitly
construct 3D metric maps from the database with structure-from-motion, or
implicitly encode the 3D information with scene coordinate regression models.
On the contrary, visual localization without reconstructing the scene in 3D
offers clear benefits. It makes deployment more convenient by reducing database
pre-processing time, releasing storage requirements, and remaining unaffected
by imperfect reconstruction, etc. In this technical report, we demonstrate that
it is possible to achieve high localization accuracy without reconstructing the
scene from the database. The key to achieving this owes to a tailored motion
averaging over database-query pairs. Experiments show that our visual
localization proposal, LazyLoc, achieves comparable performance against
state-of-the-art structure-based methods. Furthermore, we showcase the
versatility of LazyLoc, which can be easily extended to handle complex
configurations such as multi-query co-localization and camera rigs.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_S/0/1/0/all/0/1&quot;&gt;Siyan Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_S/0/1/0/all/0/1&quot;&gt;Shaohui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_H/0/1/0/all/0/1&quot;&gt;Hengkai Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_B/0/1/0/all/0/1&quot;&gt;Baoquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pollefeys_M/0/1/0/all/0/1&quot;&gt;Marc Pollefeys&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09988">
<title>TinyTrain: Deep Neural Network Training at the Extreme Edge. (arXiv:2307.09988v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09988</link>
<description rdf:parseType="Literal">&lt;p&gt;On-device training is essential for user personalisation and privacy. With
the pervasiveness of IoT devices and microcontroller units (MCU), this task
becomes more challenging due to the constrained memory and compute resources,
and the limited availability of labelled user data. Nonetheless, prior works
neglect the data scarcity issue, require excessively long training time (e.g. a
few hours), or induce substantial accuracy loss ($\geq$10\%). We propose
TinyTrain, an on-device training approach that drastically reduces training
time by selectively updating parts of the model and explicitly coping with data
scarcity. TinyTrain introduces a task-adaptive sparse-update method that
dynamically selects the layer/channel based on a multi-objective criterion that
jointly captures user data, the memory, and the compute capabilities of the
target device, leading to high accuracy on unseen tasks with reduced
computation and memory footprint. TinyTrain outperforms vanilla fine-tuning of
the entire network by 3.6-5.0\% in accuracy, while reducing the backward-pass
memory and computation cost by up to 2,286$\times$ and 7.68$\times$,
respectively. Targeting broadly used real-world edge devices, TinyTrain
achieves 9.5$\times$ faster and 3.5$\times$ more energy-efficient training over
status-quo approaches, and 2.8$\times$ smaller memory footprint than SOTA
approaches, while remaining within the 1 MB memory envelope of MCU-grade
platforms.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kwon_Y/0/1/0/all/0/1&quot;&gt;Young D. Kwon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_R/0/1/0/all/0/1&quot;&gt;Rui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Venieris_S/0/1/0/all/0/1&quot;&gt;Stylianos I. Venieris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chauhan_J/0/1/0/all/0/1&quot;&gt;Jagmohan Chauhan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lane_N/0/1/0/all/0/1&quot;&gt;Nicholas D. Lane&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mascolo_C/0/1/0/all/0/1&quot;&gt;Cecilia Mascolo&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09994">
<title>Impact of Disentanglement on Pruning Neural Networks. (arXiv:2307.09994v1 [cs.LG])</title>
<link>http://arxiv.org/abs/2307.09994</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying deep learning neural networks on edge devices, to accomplish task
specific objectives in the real-world, requires a reduction in their memory
footprint, power consumption, and latency. This can be realized via efficient
model compression. Disentangled latent representations produced by variational
autoencoder (VAE) networks are a promising approach for achieving model
compression because they mainly retain task-specific information, discarding
useless information for the task at hand. We make use of the Beta-VAE framework
combined with a standard criterion for pruning to investigate the impact of
forcing the network to learn disentangled representations on the pruning
process for the task of classification. In particular, we perform experiments
on MNIST and CIFAR10 datasets, examine disentanglement challenges, and propose
a path forward for future works.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shneider_C/0/1/0/all/0/1&quot;&gt;Carl Shneider&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rostami_P/0/1/0/all/0/1&quot;&gt;Peyman Rostami&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kacem_A/0/1/0/all/0/1&quot;&gt;Anis Kacem&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sinha_N/0/1/0/all/0/1&quot;&gt;Nilotpal Sinha&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shabayek_A/0/1/0/all/0/1&quot;&gt;Abd El Rahman Shabayek&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aouada_D/0/1/0/all/0/1&quot;&gt;Djamila Aouada&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09997">
<title>TUNeS: A Temporal U-Net with Self-Attention for Video-based Surgical Phase Recognition. (arXiv:2307.09997v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.09997</link>
<description rdf:parseType="Literal">&lt;p&gt;To enable context-aware computer assistance in the operating room of the
future, cognitive systems need to understand automatically which surgical phase
is being performed by the medical team. The primary source of information for
surgical phase recognition is typically video, which presents two challenges:
extracting meaningful features from the video stream and effectively modeling
temporal information in the sequence of visual features. For temporal modeling,
attention mechanisms have gained popularity due to their ability to capture
long-range dependencies. In this paper, we explore design choices for attention
in existing temporal models for surgical phase recognition and propose a novel
approach that does not resort to local attention or regularization of attention
weights: TUNeS is an efficient and simple temporal model that incorporates
self-attention at the coarsest stage of a U-Net-like structure. In addition, we
propose to train the feature extractor, a standard CNN, together with an LSTM
on preferably long video segments, i.e., with long temporal context. In our
experiments, all temporal models performed better on top of feature extractors
that were trained with longer temporal context. On top of these contextualized
features, TUNeS achieves state-of-the-art results on Cholec80.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Funke_I/0/1/0/all/0/1&quot;&gt;Isabel Funke&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rivoir_D/0/1/0/all/0/1&quot;&gt;Dominik Rivoir&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Krell_S/0/1/0/all/0/1&quot;&gt;Stefanie Krell&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Speidel_S/0/1/0/all/0/1&quot;&gt;Stefanie Speidel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10001">
<title>As large as it gets: Learning infinitely large Filters via Neural Implicit Functions in the Fourier Domain. (arXiv:2307.10001v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10001</link>
<description rdf:parseType="Literal">&lt;p&gt;Motivated by the recent trend towards the usage of larger receptive fields
for more context-aware neural networks in vision applications, we aim to
investigate how large these receptive fields really need to be. To facilitate
such study, several challenges need to be addressed, most importantly: (i) We
need to provide an effective way for models to learn large filters (potentially
as large as the input data) without increasing their memory consumption during
training or inference, (ii) the study of filter sizes has to be decoupled from
other effects such as the network width or number of learnable parameters, and
(iii) the employed convolution operation should be a plug-and-play module that
can replace any conventional convolution in a Convolutional Neural Network
(CNN) and allow for an efficient implementation in current frameworks. To
facilitate such models, we propose to learn not spatial but frequency
representations of filter weights as neural implicit functions, such that even
infinitely large filters can be parameterized by only a few learnable weights.
The resulting neural implicit frequency CNNs are the first models to achieve
results on par with the state-of-the-art on large image classification
benchmarks while executing convolutions solely in the frequency domain and can
be employed within any CNN architecture. They allow us to provide an extensive
analysis of the learned receptive fields. Interestingly, our analysis shows
that, although the proposed networks could learn very large convolution
kernels, the learned filters practically translate into well-localized and
relatively small convolution kernels in the spatial domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grabinski_J/0/1/0/all/0/1&quot;&gt;Julia Grabinski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_J/0/1/0/all/0/1&quot;&gt;Janis Keuper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keuper_M/0/1/0/all/0/1&quot;&gt;Margret Keuper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10003">
<title>TbExplain: A Text-based Explanation Method for Scene Classification Models with the Statistical Prediction Correction. (arXiv:2307.10003v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10003</link>
<description rdf:parseType="Literal">&lt;p&gt;The field of Explainable Artificial Intelligence (XAI) aims to improve the
interpretability of black-box machine learning models. Building a heatmap based
on the importance value of input features is a popular method for explaining
the underlying functions of such models in producing their predictions.
Heatmaps are almost understandable to humans, yet they are not without flaws.
Non-expert users, for example, may not fully understand the logic of heatmaps
(the logic in which relevant pixels to the model&apos;s prediction are highlighted
with different intensities or colors). Additionally, objects and regions of the
input image that are relevant to the model prediction are frequently not
entirely differentiated by heatmaps. In this paper, we propose a framework
called TbExplain that employs XAI techniques and a pre-trained object detector
to present text-based explanations of scene classification models. Moreover,
TbExplain incorporates a novel method to correct predictions and textually
explain them based on the statistics of objects in the input image when the
initial prediction is unreliable. To assess the trustworthiness and validity of
the text-based explanations, we conducted a qualitative experiment, and the
findings indicated that these explanations are sufficiently reliable.
Furthermore, our quantitative and qualitative experiments on TbExplain with
scene classification datasets reveal an improvement in classification accuracy
over ResNet variants.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Aminimehr_A/0/1/0/all/0/1&quot;&gt;Amirhossein Aminimehr&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khani_P/0/1/0/all/0/1&quot;&gt;Pouya Khani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Molaei_A/0/1/0/all/0/1&quot;&gt;Amirali Molaei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kazemeini_A/0/1/0/all/0/1&quot;&gt;Amirmohammad Kazemeini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cambria_E/0/1/0/all/0/1&quot;&gt;Erik Cambria&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10008">
<title>MODA: Mapping-Once Audio-driven Portrait Animation with Dual Attentions. (arXiv:2307.10008v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10008</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-driven portrait animation aims to synthesize portrait videos that are
conditioned by given audio. Animating high-fidelity and multimodal video
portraits has a variety of applications. Previous methods have attempted to
capture different motion modes and generate high-fidelity portrait videos by
training different models or sampling signals from given videos. However,
lacking correlation learning between lip-sync and other movements (e.g., head
pose/eye blinking) usually leads to unnatural results. In this paper, we
propose a unified system for multi-person, diverse, and high-fidelity talking
portrait generation. Our method contains three stages, i.e., 1) Mapping-Once
network with Dual Attentions (MODA) generates talking representation from given
audio. In MODA, we design a dual-attention module to encode accurate mouth
movements and diverse modalities. 2) Facial composer network generates dense
and detailed face landmarks, and 3) temporal-guided renderer syntheses stable
videos. Extensive evaluations demonstrate that the proposed system produces
more natural and realistic video portraits compared to previous methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yunfei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_L/0/1/0/all/0/1&quot;&gt;Lijian Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_F/0/1/0/all/0/1&quot;&gt;Fei Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_C/0/1/0/all/0/1&quot;&gt;Changyin Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10011">
<title>Towards Fair Face Verification: An In-depth Analysis of Demographic Biases. (arXiv:2307.10011v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10011</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep learning-based person identification and verification systems have
remarkably improved in terms of accuracy in recent years; however, such
systems, including widely popular cloud-based solutions, have been found to
exhibit significant biases related to race, age, and gender, a problem that
requires in-depth exploration and solutions. This paper presents an in-depth
analysis, with a particular emphasis on the intersectionality of these
demographic factors. Intersectional bias refers to the performance
discrepancies w.r.t. the different combinations of race, age, and gender
groups, an area relatively unexplored in current literature. Furthermore, the
reliance of most state-of-the-art approaches on accuracy as the principal
evaluation metric often masks significant demographic disparities in
performance. To counter this crucial limitation, we incorporate five additional
metrics in our quantitative analysis, including disparate impact and
mistreatment metrics, which are typically ignored by the relevant
fairness-aware approaches. Results on the Racial Faces in-the-Wild (RFW)
benchmark indicate pervasive biases in face recognition systems, extending
beyond race, with different demographic factors yielding significantly
disparate outcomes. In particular, Africans demonstrate an 11.25% lower True
Positive Rate (TPR) compared to Caucasians, while only a 3.51% accuracy drop is
observed. Even more concerning, the intersections of multiple protected groups,
such as African females over 60 years old, demonstrate a +39.89% disparate
mistreatment rate compared to the highest Caucasians rate. By shedding light on
these biases and their implications, this paper aims to stimulate further
research towards developing fairer, more equitable face recognition and
verification systems.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sarridis_I/0/1/0/all/0/1&quot;&gt;Ioannis Sarridis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Koutlis_C/0/1/0/all/0/1&quot;&gt;Christos Koutlis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Papadopoulos_S/0/1/0/all/0/1&quot;&gt;Symeon Papadopoulos&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Diou_C/0/1/0/all/0/1&quot;&gt;Christos Diou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10036">
<title>Class Attention to Regions of Lesion for Imbalanced Medical Image Recognition. (arXiv:2307.10036v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10036</link>
<description rdf:parseType="Literal">&lt;p&gt;Automated medical image classification is the key component in intelligent
diagnosis systems. However, most medical image datasets contain plenty of
samples of common diseases and just a handful of rare ones, leading to major
class imbalances. Currently, it is an open problem in intelligent diagnosis to
effectively learn from imbalanced training data. In this paper, we propose a
simple yet effective framework, named \textbf{C}lass \textbf{A}ttention to
\textbf{RE}gions of the lesion (CARE), to handle data imbalance issues by
embedding attention into the training process of \textbf{C}onvolutional
\textbf{N}eural \textbf{N}etworks (CNNs). The proposed attention module helps
CNNs attend to lesion regions of rare diseases, therefore helping CNNs to learn
their characteristics more effectively. In addition, this attention module
works only during the training phase and does not change the architecture of
the original network, so it can be directly combined with any existing CNN
architecture. The CARE framework needs bounding boxes to represent the lesion
regions of rare diseases. To alleviate the need for manual annotation, we
further developed variants of CARE by leveraging the traditional saliency
methods or a pretrained segmentation model for bounding box generation. Results
show that the CARE variants with automated bounding box generation are
comparable to the original CARE framework with \textit{manual} bounding box
annotations. A series of experiments on an imbalanced skin image dataset and a
pneumonia dataset indicates that our method can effectively help the network
focus on the lesion regions of rare diseases and remarkably improves the
classification performance of rare diseases.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhuang_J/0/1/0/all/0/1&quot;&gt;Jia-Xin Zhuang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_J/0/1/0/all/0/1&quot;&gt;Jiabin Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_J/0/1/0/all/0/1&quot;&gt;Jianguo Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_W/0/1/0/all/0/1&quot;&gt;Wei-shi Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_R/0/1/0/all/0/1&quot;&gt;Ruixuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10039">
<title>Deteksi Sampah di Permukaan dan Dalam Perairan pada Objek Video dengan Metode Robust and Efficient Post-Processing dan Tubelet-Level Bounding Box Linking. (arXiv:2307.10039v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10039</link>
<description rdf:parseType="Literal">&lt;p&gt;Indonesia, as a maritime country, has a significant portion of its territory
covered by water. Ineffective waste management has resulted in a considerable
amount of trash in Indonesian waters, leading to various issues. The
development of an automated trash-collecting robot can be a solution to address
this problem. The robot requires a system capable of detecting objects in
motion, such as in videos. However, using naive object detection methods in
videos has limitations, particularly when image focus is reduced and the target
object is obstructed by other objects. This paper&apos;s contribution provides an
explanation of the methods that can be applied to perform video object
detection in an automated trash-collecting robot. The study utilizes the YOLOv5
model and the Robust &amp;amp; Efficient Post Processing (REPP) method, along with
tubelet-level bounding box linking on the FloW and Roboflow datasets. The
combination of these methods enhances the performance of naive object detection
from YOLOv5 by considering the detection results in adjacent frames. The
results show that the post-processing stage and tubelet-level bounding box
linking can improve the quality of detection, achieving approximately 3% better
performance compared to YOLOv5 alone. The use of these methods has the
potential to detect surface and underwater trash and can be applied to a
real-time image-based trash-collecting robot. Implementing this system is
expected to mitigate the damage caused by trash in the past and improve
Indonesia&apos;s waste management system in the future.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tjandra_B/0/1/0/all/0/1&quot;&gt;Bryan Tjandra&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Negara_M/0/1/0/all/0/1&quot;&gt;Made S. N. Negara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Handoko_N/0/1/0/all/0/1&quot;&gt;Nyoo S. C. Handoko&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10046">
<title>Divert More Attention to Vision-Language Object Tracking. (arXiv:2307.10046v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10046</link>
<description rdf:parseType="Literal">&lt;p&gt;Multimodal vision-language (VL) learning has noticeably pushed the tendency
toward generic intelligence owing to emerging large foundation models. However,
tracking, as a fundamental vision problem, surprisingly enjoys less bonus from
recent flourishing VL learning. We argue that the reasons are two-fold: the
lack of large-scale vision-language annotated videos and ineffective
vision-language interaction learning of current works. These nuisances motivate
us to design more effective vision-language representation for tracking,
meanwhile constructing a large database with language annotation for model
learning. Particularly, in this paper, we first propose a general attribute
annotation strategy to decorate videos in six popular tracking benchmarks,
which contributes a large-scale vision-language tracking database with more
than 23,000 videos. We then introduce a novel framework to improve tracking by
learning a unified-adaptive VL representation, where the cores are the proposed
asymmetric architecture search and modality mixer (ModaMixer). To further
improve VL representation, we introduce a contrastive loss to align different
modalities. To thoroughly evidence the effectiveness of our method, we
integrate the proposed framework on three tracking methods with different
designs, i.e., the CNN-based SiamCAR, the Transformer-based OSTrack, and the
hybrid structure TransT. The experiments demonstrate that our framework can
significantly improve all baselines on six benchmarks. Besides empirical
results, we theoretically analyze our approach to show its rationality. By
revealing the potential of VL representation, we expect the community to divert
more attention to VL tracking and hope to open more possibilities for future
tracking with diversified multimodal messages.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_M/0/1/0/all/0/1&quot;&gt;Mingzhe Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Z/0/1/0/all/0/1&quot;&gt;Zhipeng Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jing_L/0/1/0/all/0/1&quot;&gt;Liping Jing&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Haibin Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_H/0/1/0/all/0/1&quot;&gt;Heng Fan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10062">
<title>Unsupervised Accuracy Estimation of Deep Visual Models using Domain-Adaptive Adversarial Perturbation without Source Samples. (arXiv:2307.10062v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10062</link>
<description rdf:parseType="Literal">&lt;p&gt;Deploying deep visual models can lead to performance drops due to the
discrepancies between source and target distributions. Several approaches
leverage labeled source data to estimate target domain accuracy, but accessing
labeled source data is often prohibitively difficult due to data
confidentiality or resource limitations on serving devices. Our work proposes a
new framework to estimate model accuracy on unlabeled target data without
access to source data. We investigate the feasibility of using pseudo-labels
for accuracy estimation and evolve this idea into adopting recent advances in
source-free domain adaptation algorithms. Our approach measures the
disagreement rate between the source hypothesis and the target pseudo-labeling
function, adapted from the source hypothesis. We mitigate the impact of
erroneous pseudo-labels that may arise due to a high ideal joint hypothesis
risk by employing adaptive adversarial perturbation on the input of the target
model. Our proposed source-free framework effectively addresses the challenging
distribution shift scenarios and outperforms existing methods requiring source
data and labels for training.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;JoonHo Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_J/0/1/0/all/0/1&quot;&gt;Jae Oh Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moon_H/0/1/0/all/0/1&quot;&gt;Hankyu Moon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kwonho Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10094">
<title>Make-A-Volume: Leveraging Latent Diffusion Models for Cross-Modality 3D Brain MRI Synthesis. (arXiv:2307.10094v1 [eess.IV])</title>
<link>http://arxiv.org/abs/2307.10094</link>
<description rdf:parseType="Literal">&lt;p&gt;Cross-modality medical image synthesis is a critical topic and has the
potential to facilitate numerous applications in the medical imaging field.
Despite recent successes in deep-learning-based generative models, most current
medical image synthesis methods rely on generative adversarial networks and
suffer from notorious mode collapse and unstable training. Moreover, the 2D
backbone-driven approaches would easily result in volumetric inconsistency,
while 3D backbones are challenging and impractical due to the tremendous memory
cost and training difficulty. In this paper, we introduce a new paradigm for
volumetric medical data synthesis by leveraging 2D backbones and present a
diffusion-based framework, Make-A-Volume, for cross-modality 3D medical image
synthesis. To learn the cross-modality slice-wise mapping, we employ a latent
diffusion model and learn a low-dimensional latent space, resulting in high
computational efficiency. To enable the 3D image synthesis and mitigate
volumetric inconsistency, we further insert a series of volumetric layers in
the 2D slice-mapping model and fine-tune them with paired 3D data. This
paradigm extends the 2D image diffusion model to a volumetric version with a
slightly increasing number of parameters and computation, offering a principled
solution for generic cross-modality 3D medical image synthesis. We showcase the
effectiveness of our Make-A-Volume framework on an in-house SWI-MRA brain MRI
dataset and a public T1-T2 brain MRI dataset. Experimental results demonstrate
that our framework achieves superior synthesis results with volumetric
consistency.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_L/0/1/0/all/0/1&quot;&gt;Lingting Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Xue_Z/0/1/0/all/0/1&quot;&gt;Zeyue Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Jin_Z/0/1/0/all/0/1&quot;&gt;Zhenchao Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xian Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+He_J/0/1/0/all/0/1&quot;&gt;Jingzhen He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Yu_L/0/1/0/all/0/1&quot;&gt;Lequan Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10097">
<title>Boundary-Refined Prototype Generation: A General End-to-End Paradigm for Semi-Supervised Semantic Segmentation. (arXiv:2307.10097v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10097</link>
<description rdf:parseType="Literal">&lt;p&gt;Prototype-based classification is a classical method in machine learning, and
recently it has achieved remarkable success in semi-supervised semantic
segmentation. However, the current approach isolates the prototype
initialization process from the main training framework, which appears to be
unnecessary. Furthermore, while the direct use of K-Means algorithm for
prototype generation has considered rich intra-class variance, it may not be
the optimal solution for the classification task. To tackle these problems, we
propose a novel boundary-refined prototype generation (BRPG) method, which is
incorporated into the whole training framework. Specifically, our approach
samples and clusters high- and low-confidence features separately based on a
confidence threshold, aiming to generate prototypes closer to the class
boundaries. Moreover, an adaptive prototype optimization strategy is introduced
to make prototype augmentation for categories with scattered feature
distributions. Extensive experiments on the PASCAL VOC 2012 and Cityscapes
datasets demonstrate the superiority and scalability of the proposed method,
outperforming the current state-of-the-art approaches. The code is available at
xxxxxxxxxxxxxx.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dong_J/0/1/0/all/0/1&quot;&gt;Junhao Dong&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Meng_Z/0/1/0/all/0/1&quot;&gt;Zhu Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_D/0/1/0/all/0/1&quot;&gt;Delong Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Zhicheng Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_F/0/1/0/all/0/1&quot;&gt;Fei Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10123">
<title>Two Approaches to Supervised Image Segmentation. (arXiv:2307.10123v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10123</link>
<description rdf:parseType="Literal">&lt;p&gt;Though performed almost effortlessly by humans, segmenting 2D gray-scale or
color images in terms of their constituent regions of interest
(e.g.~background, objects or portions of objects) constitutes one of the
greatest challenges in science and technology as a consequence of the involved
dimensionality reduction(3D to 2D), noise, reflections, shades, and occlusions,
among many other possible effects. While a large number of interesting
approaches have been respectively suggested along the last decades, it was
mainly with the more recent development of deep learning that more effective
and general solutions have been obtained, currently constituting the basic
comparison reference for this type of operation. Also developed recently, a
multiset-based methodology has been described that is capable of encouraging
performance that combines spatial accuracy, stability, and robustness while
requiring minimal computational resources (hardware and/or training and
recognition time). The interesting features of the latter methodology mostly
follow from the enhanced selectivity and sensitivity, as well as good
robustness to data perturbations and outliers, allowed by the coincidence
similarity index on which the multiset approach to supervised image
segmentation is based. After describing the deep learning and multiset
approaches, the present work develops two comparison experiments between them
which are primarily aimed at illustrating their respective main interesting
features when applied to the adopted specific type of data and parameter
configurations. While the deep learning approach confirmed its potential for
performing image segmentation, the alternative multiset methodology allowed for
encouraging accuracy while requiring little computational resources.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Benatti_A/0/1/0/all/0/1&quot;&gt;Alexandre Benatti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Costa_L/0/1/0/all/0/1&quot;&gt;Luciano da F. Costa&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10129">
<title>General vs. Long-Tailed Age Estimation: An Approach to Kill Two Birds with One Stone. (arXiv:2307.10129v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10129</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial age estimation has received a lot of attention for its diverse
application scenarios. Most existing studies treat each sample equally and aim
to reduce the average estimation error for the entire dataset, which can be
summarized as General Age Estimation. However, due to the long-tailed
distribution prevalent in the dataset, treating all samples equally will
inevitably bias the model toward the head classes (usually the adult with a
majority of samples). Driven by this, some works suggest that each class should
be treated equally to improve performance in tail classes (with a minority of
samples), which can be summarized as Long-tailed Age Estimation. However,
Long-tailed Age Estimation usually faces a performance trade-off, i.e.,
achieving improvement in tail classes by sacrificing the head classes. In this
paper, our goal is to design a unified framework to perform well on both tasks,
killing two birds with one stone. To this end, we propose a simple, effective,
and flexible training paradigm named GLAE, which is two-fold. Our GLAE provides
a surprising improvement on Morph II, reaching the lowest MAE and CMAE of 1.14
and 1.27 years, respectively. Compared to the previous best method, MAE dropped
by up to 34%, which is an unprecedented improvement, and for the first time,
MAE is close to 1 year old. Extensive experiments on other age benchmark
datasets, including CACD, MIVIA, and Chalearn LAP 2015, also indicate that GLAE
outperforms the state-of-the-art approaches significantly.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bao_Z/0/1/0/all/0/1&quot;&gt;Zenghao Bao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tan_Z/0/1/0/all/0/1&quot;&gt;Zichang Tan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jun Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Jun Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xibo Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lei_Z/0/1/0/all/0/1&quot;&gt;Zhen Lei&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10135">
<title>An Improved NeuMIP with Better Accuracy. (arXiv:2307.10135v1 [cs.GR])</title>
<link>http://arxiv.org/abs/2307.10135</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural reflectance models are capable of accurately reproducing the
spatially-varying appearance of many real-world materials at different scales.
However, existing methods have difficulties handling highly glossy materials.
To address this problem, we introduce a new neural reflectance model which,
compared with existing methods, better preserves not only specular highlights
but also fine-grained details. To this end, we enhance the neural network
performance by encoding input data to frequency space, inspired by NeRF, to
better preserve the details. Furthermore, we introduce a gradient-based loss
and employ it in multiple stages, adaptive to the progress of the learning
phase. Lastly, we utilize an optional extension to the decoder network using
the Inception module for more accurate yet costly performance. We demonstrate
the effectiveness of our method using a variety of synthetic and real examples.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_B/0/1/0/all/0/1&quot;&gt;Bowen Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_S/0/1/0/all/0/1&quot;&gt;Shuang Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jensen_H/0/1/0/all/0/1&quot;&gt;Henrik Wann Jensen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Montazeri_Z/0/1/0/all/0/1&quot;&gt;Zahra Montazeri&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10157">
<title>Leveraging Visemes for Better Visual Speech Representation and Lip Reading. (arXiv:2307.10157v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10157</link>
<description rdf:parseType="Literal">&lt;p&gt;Lip reading is a challenging task that has many potential applications in
speech recognition, human-computer interaction, and security systems. However,
existing lip reading systems often suffer from low accuracy due to the
limitations of video features. In this paper, we propose a novel approach that
leverages visemes, which are groups of phonetically similar lip shapes, to
extract more discriminative and robust video features for lip reading. We
evaluate our approach on various tasks, including word-level and sentence-level
lip reading, and audiovisual speech recognition using the Arman-AV dataset, a
largescale Persian corpus. Our experimental results show that our viseme based
approach consistently outperforms the state-of-theart methods in all these
tasks. The proposed method reduces the lip-reading word error rate (WER) by
9.1% relative to the best previous method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peymanfard_J/0/1/0/all/0/1&quot;&gt;Javad Peymanfard&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Saeedi_V/0/1/0/all/0/1&quot;&gt;Vahid Saeedi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mohammadi_M/0/1/0/all/0/1&quot;&gt;Mohammad Reza Mohammadi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeinali_H/0/1/0/all/0/1&quot;&gt;Hossein Zeinali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mozayani_N/0/1/0/all/0/1&quot;&gt;Nasser Mozayani&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10159">
<title>FABRIC: Personalizing Diffusion Models with Iterative Feedback. (arXiv:2307.10159v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10159</link>
<description rdf:parseType="Literal">&lt;p&gt;In an era where visual content generation is increasingly driven by machine
learning, the integration of human feedback into generative models presents
significant opportunities for enhancing user experience and output quality.
This study explores strategies for incorporating iterative human feedback into
the generative process of diffusion-based text-to-image models. We propose
FABRIC, a training-free approach applicable to a wide range of popular
diffusion models, which exploits the self-attention layer present in the most
widely used architectures to condition the diffusion process on a set of
feedback images. To ensure a rigorous assessment of our approach, we introduce
a comprehensive evaluation methodology, offering a robust mechanism to quantify
the performance of generative visual models that integrate human feedback. We
show that generation results improve over multiple rounds of iterative feedback
through exhaustive analysis, implicitly optimizing arbitrary user preferences.
The potential applications of these findings extend to fields such as
personalized content creation and customization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rutte_D/0/1/0/all/0/1&quot;&gt;Dimitri von R&amp;#xfc;tte&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fedele_E/0/1/0/all/0/1&quot;&gt;Elisabetta Fedele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thomm_J/0/1/0/all/0/1&quot;&gt;Jonathan Thomm&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wolf_L/0/1/0/all/0/1&quot;&gt;Lukas Wolf&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10160">
<title>Robust Driving Policy Learning with Guided Meta Reinforcement Learning. (arXiv:2307.10160v1 [cs.RO])</title>
<link>http://arxiv.org/abs/2307.10160</link>
<description rdf:parseType="Literal">&lt;p&gt;Although deep reinforcement learning (DRL) has shown promising results for
autonomous navigation in interactive traffic scenarios, existing work typically
adopts a fixed behavior policy to control social vehicles in the training
environment. This may cause the learned driving policy to overfit the
environment, making it difficult to interact well with vehicles with different,
unseen behaviors. In this work, we introduce an efficient method to train
diverse driving policies for social vehicles as a single meta-policy. By
randomizing the interaction-based reward functions of social vehicles, we can
generate diverse objectives and efficiently train the meta-policy through
guiding policies that achieve specific objectives. We further propose a
training strategy to enhance the robustness of the ego vehicle&apos;s driving policy
using the environment where social vehicles are controlled by the learned
meta-policy. Our method successfully learns an ego driving policy that
generalizes well to unseen situations with out-of-distribution (OOD) social
agents&apos; behaviors in a challenging uncontrolled T-intersection scenario.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_K/0/1/0/all/0/1&quot;&gt;Kanghoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jiachen Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Isele_D/0/1/0/all/0/1&quot;&gt;David Isele&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_J/0/1/0/all/0/1&quot;&gt;Jinkyoo Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fujimura_K/0/1/0/all/0/1&quot;&gt;Kikuo Fujimura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kochenderfer_M/0/1/0/all/0/1&quot;&gt;Mykel J. Kochenderfer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10165">
<title>Drone navigation and license place detection for vehicle location in indoor spaces. (arXiv:2307.10165v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10165</link>
<description rdf:parseType="Literal">&lt;p&gt;Millions of vehicles are transported every year, tightly parked in vessels or
boats. To reduce the risks of associated safety issues like fires, knowing the
location of vehicles is essential, since different vehicles may need different
mitigation measures, e.g. electric cars. This work is aimed at creating a
solution based on a nano-drone that navigates across rows of parked vehicles
and detects their license plates. We do so via a wall-following algorithm, and
a CNN trained to detect license plates. All computations are done in real-time
on the drone, which just sends position and detected images that allow the
creation of a 2D map with the position of the plates. Our solution is capable
of reading all plates across eight test cases (with several rows of plates,
different drone speeds, or low light) by aggregation of measurements across
several drone journeys.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arvidsson_M/0/1/0/all/0/1&quot;&gt;Moa Arvidsson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sawirot_S/0/1/0/all/0/1&quot;&gt;Sithichot Sawirot&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Englund_C/0/1/0/all/0/1&quot;&gt;Cristofer Englund&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Alonso_Fernandez_F/0/1/0/all/0/1&quot;&gt;Fernando Alonso-Fernandez&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torstensson_M/0/1/0/all/0/1&quot;&gt;Martin Torstensson&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duran_B/0/1/0/all/0/1&quot;&gt;Boris Duran&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10166">
<title>Adversarial Latent Autoencoder with Self-Attention for Structural Image Synthesis. (arXiv:2307.10166v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10166</link>
<description rdf:parseType="Literal">&lt;p&gt;Generative Engineering Design approaches driven by Deep Generative Models
(DGM) have been proposed to facilitate industrial engineering processes. In
such processes, designs often come in the form of images, such as blueprints,
engineering drawings, and CAD models depending on the level of detail. DGMs
have been successfully employed for synthesis of natural images, e.g.,
displaying animals, human faces and landscapes. However, industrial design
images are fundamentally different from natural scenes in that they contain
rich structural patterns and long-range dependencies, which are challenging for
convolution-based DGMs to generate. Moreover, DGM-driven generation process is
typically triggered based on random noisy inputs, which outputs unpredictable
samples and thus cannot perform an efficient industrial design exploration. We
tackle these challenges by proposing a novel model Self-Attention Adversarial
Latent Autoencoder (SA-ALAE), which allows generating feasible design images of
complex engineering parts. With SA-ALAE, users can not only explore novel
variants of an existing design, but also control the generation process by
operating in latent space. The potential of SA-ALAE is shown by generating
engineering blueprints in a real automotive design task.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_J/0/1/0/all/0/1&quot;&gt;Jiajie Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vuaille_L/0/1/0/all/0/1&quot;&gt;Laure Vuaille&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_H/0/1/0/all/0/1&quot;&gt;Hao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Back_T/0/1/0/all/0/1&quot;&gt;Thomas B&amp;#xe4;ck&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.10173">
<title>DNA-Rendering: A Diverse Neural Actor Repository for High-Fidelity Human-centric Rendering. (arXiv:2307.10173v1 [cs.CV])</title>
<link>http://arxiv.org/abs/2307.10173</link>
<description rdf:parseType="Literal">&lt;p&gt;Realistic human-centric rendering plays a key role in both computer vision
and computer graphics. Rapid progress has been made in the algorithm aspect
over the years, yet existing human-centric rendering datasets and benchmarks
are rather impoverished in terms of diversity, which are crucial for rendering
effect. Researchers are usually constrained to explore and evaluate a small set
of rendering problems on current datasets, while real-world applications
require methods to be robust across different scenarios. In this work, we
present DNA-Rendering, a large-scale, high-fidelity repository of human
performance data for neural actor rendering. DNA-Rendering presents several
alluring attributes. First, our dataset contains over 1500 human subjects, 5000
motion sequences, and 67.5M frames&apos; data volume. Second, we provide rich assets
for each subject -- 2D/3D human body keypoints, foreground masks, SMPLX models,
cloth/accessory materials, multi-view images, and videos. These assets boost
the current method&apos;s accuracy on downstream rendering tasks. Third, we
construct a professional multi-view system to capture data, which contains 60
synchronous cameras with max 4096 x 3000 resolution, 15 fps speed, and stern
camera calibration steps, ensuring high-quality resources for task training and
evaluation. Along with the dataset, we provide a large-scale and quantitative
benchmark in full-scale, with multiple tasks to evaluate the existing progress
of novel view synthesis, novel pose animation synthesis, and novel identity
rendering methods. In this manuscript, we describe our DNA-Rendering effort as
a revealing of new observations, challenges, and future directions to
human-centric rendering. The dataset, code, and benchmarks will be publicly
available at https://dna-rendering.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_W/0/1/0/all/0/1&quot;&gt;Wei Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_R/0/1/0/all/0/1&quot;&gt;Ruixiang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yin_W/0/1/0/all/0/1&quot;&gt;Wanqi Yin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fan_S/0/1/0/all/0/1&quot;&gt;Siming Fan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Keyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+He_H/0/1/0/all/0/1&quot;&gt;Honglin He&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_H/0/1/0/all/0/1&quot;&gt;Huiwen Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cai_Z/0/1/0/all/0/1&quot;&gt;Zhongang Cai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingbo Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Gao_Y/0/1/0/all/0/1&quot;&gt;Yang Gao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_Z/0/1/0/all/0/1&quot;&gt;Zhengming Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_Z/0/1/0/all/0/1&quot;&gt;Zhengyu Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_D/0/1/0/all/0/1&quot;&gt;Daxuan Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_L/0/1/0/all/0/1&quot;&gt;Lei Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Ziwei Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Loy_C/0/1/0/all/0/1&quot;&gt;Chen Change Loy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_C/0/1/0/all/0/1&quot;&gt;Chen Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wayne Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_D/0/1/0/all/0/1&quot;&gt;Dahua Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dai_B/0/1/0/all/0/1&quot;&gt;Bo Dai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_K/0/1/0/all/0/1&quot;&gt;Kwan-Yee Lin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2001.05887">
<title>MixPath: A Unified Approach for One-shot Neural Architecture Search. (arXiv:2001.05887v4 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2001.05887</link>
<description rdf:parseType="Literal">&lt;p&gt;Blending multiple convolutional kernels is proved advantageous in neural
architecture design. However, current two-stage neural architecture search
methods are mainly limited to single-path search spaces. How to efficiently
search models of multi-path structures remains a difficult problem. In this
paper, we are motivated to train a one-shot multi-path supernet to accurately
evaluate the candidate architectures. Specifically, we discover that in the
studied search spaces, feature vectors summed from multiple paths are nearly
multiples of those from a single path. Such disparity perturbs the supernet
training and its ranking ability. Therefore, we propose a novel mechanism
called Shadow Batch Normalization (SBN) to regularize the disparate feature
statistics. Extensive experiments prove that SBNs are capable of stabilizing
the optimization and improving ranking performance. We call our unified
multi-path one-shot approach as MixPath, which generates a series of models
that achieve state-of-the-art results on ImageNet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chu_X/0/1/0/all/0/1&quot;&gt;Xiangxiang Chu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_S/0/1/0/all/0/1&quot;&gt;Shun Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xudong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_B/0/1/0/all/0/1&quot;&gt;Bo Zhang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2009.06205">
<title>Joint Demosaicking and Denoising Benefits from a Two-stage Training Strategy. (arXiv:2009.06205v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2009.06205</link>
<description rdf:parseType="Literal">&lt;p&gt;Image demosaicking and denoising are the first two key steps of the color
image production pipeline. The classical processing sequence has for a long
time consisted of applying denoising first, and then demosaicking. Applying the
operations in this order leads to oversmoothing and checkerboard effects. Yet,
it was difficult to change this order, because once the image is demosaicked,
the statistical properties of the noise are dramatically changed and hard to
handle by traditional denoising models. In this paper, we address this problem
by a hybrid machine learning method. We invert the traditional color filter
array (CFA) processing pipeline by first demosaicking and then denoising. Our
demosaicking algorithm, trained on noiseless images, combines a traditional
method and a residual convolutional neural network (CNN). This first stage
retains all known information, which is the key point to obtain faithful final
results. The noisy demosaicked image is then passed through a second CNN
restoring a noiseless full-color image. This pipeline order completely avoids
checkerboard effects and restores fine image detail. Although CNNs can be
trained to solve jointly demosaicking-denoising end-to-end, we find that this
two-stage training performs better and is less prone to failure. It is shown
experimentally to improve on the state of the art, both quantitatively and in
terms of visual quality.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yu Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Q/0/1/0/all/0/1&quot;&gt;Qiyu Jin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Facciolo_G/0/1/0/all/0/1&quot;&gt;Gabriele Facciolo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zeng_T/0/1/0/all/0/1&quot;&gt;Tieyong Zeng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Morel_J/0/1/0/all/0/1&quot;&gt;Jean-Michel Morel&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2103.03328">
<title>Evaluation of Complexity Measures for Deep Learning Generalization in Medical Image Analysis. (arXiv:2103.03328v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2103.03328</link>
<description rdf:parseType="Literal">&lt;p&gt;The generalization performance of deep learning models for medical image
analysis often decreases on images collected with different devices for data
acquisition, device settings, or patient population. A better understanding of
the generalization capacity on new images is crucial for clinicians&apos;
trustworthiness in deep learning. Although significant research efforts have
been recently directed toward establishing generalization bounds and complexity
measures, still, there is often a significant discrepancy between the predicted
and actual generalization performance. As well, related large empirical studies
have been primarily based on validation with general-purpose image datasets.
This paper presents an empirical study that investigates the correlation
between 25 complexity measures and the generalization abilities of supervised
deep learning classifiers for breast ultrasound images. The results indicate
that PAC-Bayes flatness-based and path norm-based measures produce the most
consistent explanation for the combination of models and data. We also
investigate the use of multi-task classification and segmentation approach for
breast images, and report that such learning approach acts as an implicit
regularizer and is conducive toward improved generalization.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vakanski_A/0/1/0/all/0/1&quot;&gt;Aleksandar Vakanski&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xian_M/0/1/0/all/0/1&quot;&gt;Min Xian&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.03544">
<title>RAR: Region-Aware Point Cloud Registration. (arXiv:2110.03544v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.03544</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper concerns the research problem of point cloud registration to find
the rigid transformation to optimally align the source point set with the
target one. Learning robust point cloud registration models with deep neural
networks has emerged as a powerful paradigm, offering promising performance in
predicting the global geometric transformation for a pair of point sets.
Existing methods firstly leverage an encoder to regress a latent shape
embedding, which is then decoded into a shape-conditioned transformation via
concatenation-based conditioning. However, different regions of a 3D shape vary
in their geometric structures which makes it more sense that we have a
region-conditioned transformation instead of the shape-conditioned one. In this
paper we present a \underline{R}egion-\underline{A}ware point cloud
\underline{R}egistration, denoted as RAR, to predict transformation for
pairwise point sets in the self-supervised learning fashion. More specifically,
we develop a novel region-aware decoder (RAD) module that is formed with an
implicit neural region representation parameterized by neural networks. The
implicit neural region representation is learned with a self-supervised 3D
shape reconstruction loss without the need for region labels. Consequently, the
region-aware decoder (RAD) module guides the training of the region-aware
transformation (RAT) module and region-aware weight (RAW) module, which predict
the transforms and weights for different regions respectively. The global
geometric transformation from source point set to target one is then formed by
the weighted fusion of region-aware transforms. Compared to the
state-of-the-art approaches, our experiments show that our RAR achieves
superior registration performance over various benchmark datasets (e.g.
ModelNet40).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hao_Y/0/1/0/all/0/1&quot;&gt;Yu Hao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yi Fang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2110.04830">
<title>MARVEL: Raster Manga Vectorization via Primitive-wise Deep Reinforcement Learning. (arXiv:2110.04830v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2110.04830</link>
<description rdf:parseType="Literal">&lt;p&gt;Manga is a fashionable Japanese-style comic form that is composed of
black-and-white strokes and is generally displayed as raster images on digital
devices. Typical mangas have simple textures, wide lines, and few color
gradients, which are vectorizable natures to enjoy the merits of vector
graphics, e.g., adaptive resolutions and small file sizes. In this paper, we
propose MARVEL (MAnga&apos;s Raster to VEctor Learning), a primitive-wise approach
for vectorizing raster mangas by Deep Reinforcement Learning (DRL). Unlike
previous learning-based methods which predict vector parameters for an entire
image, MARVEL introduces a new perspective that regards an entire manga as a
collection of basic primitives\textemdash stroke lines, and designs a DRL model
to decompose the target image into a primitive sequence for achieving accurate
vectorization. To improve vectorization accuracies and decrease file sizes, we
further propose a stroke accuracy reward to predict accurate stroke lines, and
a pruning mechanism to avoid generating erroneous and repeated strokes.
Extensive subjective and objective experiments show that our MARVEL can
generate impressive results and reaches the state-of-the-art level. Our code is
open-source at: https://github.com/SwordHolderSH/Mang2Vec.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Niu_J/0/1/0/all/0/1&quot;&gt;Jianwei Niu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xuefeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jiahe Cui&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_J/0/1/0/all/0/1&quot;&gt;Ji Wan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2111.01396">
<title>Boundary Distribution Estimation for Precise Object Detection. (arXiv:2111.01396v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2111.01396</link>
<description rdf:parseType="Literal">&lt;p&gt;In the field of state-of-the-art object detection, the task of object
localization is typically accomplished through a dedicated subnet that
emphasizes bounding box regression. This subnet traditionally predicts the
object&apos;s position by regressing the box&apos;s center position and scaling factors.
Despite the widespread adoption of this approach, we have observed that the
localization results often suffer from defects, leading to unsatisfactory
detector performance. In this paper, we address the shortcomings of previous
methods through theoretical analysis and experimental verification and present
an innovative solution for precise object detection. Instead of solely focusing
on the object&apos;s center and size, our approach enhances the accuracy of bounding
box localization by refining the box edges based on the estimated distribution
at the object&apos;s boundary. Experimental results demonstrate the potential and
generalizability of our proposed method.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhi_P/0/1/0/all/0/1&quot;&gt;Peng Zhi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_H/0/1/0/all/0/1&quot;&gt;Haoran Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hang Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_R/0/1/0/all/0/1&quot;&gt;Rui Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_R/0/1/0/all/0/1&quot;&gt;Rui Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_Q/0/1/0/all/0/1&quot;&gt;Qingguo Zhou&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2112.06809">
<title>Persistent Animal Identification Leveraging Non-Visual Markers. (arXiv:2112.06809v8 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2112.06809</link>
<description rdf:parseType="Literal">&lt;p&gt;Our objective is to locate and provide a unique identifier for each mouse in
a cluttered home-cage environment through time, as a precursor to automated
behaviour recognition for biological research. This is a very challenging
problem due to (i) the lack of distinguishing visual features for each mouse,
and (ii) the close confines of the scene with constant occlusion, making
standard visual tracking approaches unusable. However, a coarse estimate of
each mouse&apos;s location is available from a unique RFID implant, so there is the
potential to optimally combine information from (weak) tracking with coarse
information on identity. To achieve our objective, we make the following key
contributions: (a) the formulation of the object identification problem as an
assignment problem (solved using Integer Linear Programming), and (b) a novel
probabilistic model of the affinity between tracklets and RFID data. The latter
is a crucial part of the model, as it provides a principled probabilistic
treatment of object detections given coarse localisation. Our approach achieves
77% accuracy on this animal identification problem, and is able to reject
spurious detections when the animals are hidden.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Camilleri_M/0/1/0/all/0/1&quot;&gt;Michael P. J. Camilleri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Li Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bains_R/0/1/0/all/0/1&quot;&gt;Rasneer S. Bains&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zisserman_A/0/1/0/all/0/1&quot;&gt;Andrew Zisserman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Williams_C/0/1/0/all/0/1&quot;&gt;Christopher K. I. Williams&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2203.16475">
<title>ConceptEvo: Interpreting Concept Evolution in Deep Learning Training. (arXiv:2203.16475v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2203.16475</link>
<description rdf:parseType="Literal">&lt;p&gt;We present ConceptEvo, a unified interpretation framework for deep neural
networks (DNNs) that reveals the inception and evolution of learned concepts
during training. Our work fills a critical gap in DNN interpretation research,
as existing methods focus on post-hoc interpretation after training. ConceptEvo
presents two novel technical contributions: (1) an algorithm that generates a
unified semantic space that enables side-by-side comparison of different models
during training; and (2) an algorithm that discovers and quantifies important
concept evolutions for class predictions. Through a large-scale human
evaluation with 260 participants and quantitative experiments, we show that
ConceptEvo discovers evolutions across different models that are meaningful to
humans and important for predictions. ConceptEvo works for both modern
(ConvNeXt) and classic DNNs (e.g., VGGs, InceptionV3).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_H/0/1/0/all/0/1&quot;&gt;Haekyu Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Seongmin Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoover_B/0/1/0/all/0/1&quot;&gt;Benjamin Hoover&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wright_A/0/1/0/all/0/1&quot;&gt;Austin Wright&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shaikh_O/0/1/0/all/0/1&quot;&gt;Omar Shaikh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Duggal_R/0/1/0/all/0/1&quot;&gt;Rahul Duggal&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Das_N/0/1/0/all/0/1&quot;&gt;Nilaksh Das&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hoffman_J/0/1/0/all/0/1&quot;&gt;Judy Hoffman&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chau_D/0/1/0/all/0/1&quot;&gt;Duen Horng Chau&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2205.11397">
<title>Super Vision Transformer. (arXiv:2205.11397v5 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2205.11397</link>
<description rdf:parseType="Literal">&lt;p&gt;We attempt to reduce the computational costs in vision transformers (ViTs),
which increase quadratically in the token number. We present a novel training
paradigm that trains only one ViT model at a time, but is capable of providing
improved image recognition performance with various computational costs. Here,
the trained ViT model, termed super vision transformer (SuperViT), is empowered
with the versatile ability to solve incoming patches of multiple sizes as well
as preserve informative tokens with multiple keeping rates (the ratio of
keeping tokens) to achieve good hardware efficiency for inference, given that
the available hardware resources often change from time to time. Experimental
results on ImageNet demonstrate that our SuperViT can considerably reduce the
computational costs of ViT models with even performance increase. For example,
we reduce 2x FLOPs of DeiT-S while increasing the Top-1 accuracy by 0.2% and
0.7% for 1.5x reduction. Also, our SuperViT significantly outperforms existing
studies on efficient vision transformers. For example, when consuming the same
amount of FLOPs, our SuperViT surpasses the recent state-of-the-art (SOTA) EViT
by 1.1% when using DeiT-S as their backbones. The project of this work is made
publicly available at https://github.com/lmbxmu/SuperViT.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lin_M/0/1/0/all/0/1&quot;&gt;Mingbao Lin&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_M/0/1/0/all/0/1&quot;&gt;Mengzhao Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuxin Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shen_C/0/1/0/all/0/1&quot;&gt;Chunhua Shen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ji_R/0/1/0/all/0/1&quot;&gt;Rongrong Ji&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cao_L/0/1/0/all/0/1&quot;&gt;Liujuan Cao&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2207.00419">
<title>Self-Supervised Learning for Videos: A Survey. (arXiv:2207.00419v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2207.00419</link>
<description rdf:parseType="Literal">&lt;p&gt;The remarkable success of deep learning in various domains relies on the
availability of large-scale annotated datasets. However, obtaining annotations
is expensive and requires great effort, which is especially challenging for
videos. Moreover, the use of human-generated annotations leads to models with
biased learning and poor domain generalization and robustness. As an
alternative, self-supervised learning provides a way for representation
learning which does not require annotations and has shown promise in both image
and video domains. Different from the image domain, learning video
representations are more challenging due to the temporal dimension, bringing in
motion and other environmental dynamics. This also provides opportunities for
video-exclusive ideas that advance self-supervised learning in the video and
multimodal domain. In this survey, we provide a review of existing approaches
on self-supervised learning focusing on the video domain. We summarize these
methods into four different categories based on their learning objectives: 1)
pretext tasks, 2) generative learning, 3) contrastive learning, and 4)
cross-modal agreement. We further introduce the commonly used datasets,
downstream evaluation tasks, insights into the limitations of existing works,
and the potential future directions in this area.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Schiappa_M/0/1/0/all/0/1&quot;&gt;Madeline C. Schiappa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rawat_Y/0/1/0/all/0/1&quot;&gt;Yogesh S. Rawat&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_M/0/1/0/all/0/1&quot;&gt;Mubarak Shah&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10741">
<title>Hierarchically Decomposed Graph Convolutional Networks for Skeleton-Based Action Recognition. (arXiv:2208.10741v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10741</link>
<description rdf:parseType="Literal">&lt;p&gt;Graph convolutional networks (GCNs) are the most commonly used methods for
skeleton-based action recognition and have achieved remarkable performance.
Generating adjacency matrices with semantically meaningful edges is
particularly important for this task, but extracting such edges is challenging
problem. To solve this, we propose a hierarchically decomposed graph
convolutional network (HD-GCN) architecture with a novel hierarchically
decomposed graph (HD-Graph). The proposed HD-GCN effectively decomposes every
joint node into several sets to extract major structurally adjacent and distant
edges, and uses them to construct an HD-Graph containing those edges in the
same semantic spaces of a human skeleton. In addition, we introduce an
attention-guided hierarchy aggregation (A-HA) module to highlight the dominant
hierarchical edge sets of the HD-Graph. Furthermore, we apply a new six-way
ensemble method, which uses only joint and bone stream without any motion
stream. The proposed model is evaluated and achieves state-of-the-art
performance on four large, popular datasets. Finally, we demonstrate the
effectiveness of our model with various comparative experiments.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_D/0/1/0/all/0/1&quot;&gt;Dogyoon Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangyoun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2208.10967">
<title>The Value of Out-of-Distribution Data. (arXiv:2208.10967v5 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2208.10967</link>
<description rdf:parseType="Literal">&lt;p&gt;We expect the generalization error to improve with more samples from a
similar task, and to deteriorate with more samples from an out-of-distribution
(OOD) task. In this work, we show a counter-intuitive phenomenon: the
generalization error of a task can be a non-monotonic function of the number of
OOD samples. As the number of OOD samples increases, the generalization error
on the target task improves before deteriorating beyond a threshold. In other
words, there is value in training on small amounts of OOD data. We use Fisher&apos;s
Linear Discriminant on synthetic datasets and deep networks on computer vision
benchmarks such as MNIST, CIFAR-10, CINIC-10, PACS and DomainNet to demonstrate
and analyze this phenomenon. In the idealistic setting where we know which
samples are OOD, we show that these non-monotonic trends can be exploited using
an appropriately weighted objective of the target and OOD empirical risk. While
its practical utility is limited, this does suggest that if we can detect OOD
samples, then there may be ways to benefit from them. When we do not know which
samples are OOD, we show how a number of go-to strategies such as
data-augmentation, hyper-parameter optimization, and pre-training are not
enough to ensure that the target generalization error does not deteriorate with
the number of OOD samples in the dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_A/0/1/0/all/0/1&quot;&gt;Ashwin De Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramesh_R/0/1/0/all/0/1&quot;&gt;Rahul Ramesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Priebe_C/0/1/0/all/0/1&quot;&gt;Carey E. Priebe&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chaudhari_P/0/1/0/all/0/1&quot;&gt;Pratik Chaudhari&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vogelstein_J/0/1/0/all/0/1&quot;&gt;Joshua T. Vogelstein&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.03189">
<title>FocalUNETR: A Focal Transformer for Boundary-aware Segmentation of CT Images. (arXiv:2210.03189v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.03189</link>
<description rdf:parseType="Literal">&lt;p&gt;Computed Tomography (CT) based precise prostate segmentation for treatment
planning is challenging due to (1) the unclear boundary of the prostate derived
from CT&apos;s poor soft tissue contrast and (2) the limitation of convolutional
neural network-based models in capturing long-range global context. Here we
propose a novel focal transformer-based image segmentation architecture to
effectively and efficiently extract local visual features and global context
from CT images. Additionally, we design an auxiliary boundary-induced label
regression task coupled with the main prostate segmentation task to address the
unclear boundary issue in CT images. We demonstrate that this design
significantly improves the quality of the CT-based prostate segmentation task
over other competing methods, resulting in substantially improved performance,
i.e., higher Dice Similarity Coefficient, lower Hausdorff Distance, and Average
Symmetric Surface Distance, on both private and public CT image datasets. Our
code is available at this
\href{https://github.com/ChengyinLee/FocalUNETR.git}{link}.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chengyin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Qiang_Y/0/1/0/all/0/1&quot;&gt;Yao Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sultan_R/0/1/0/all/0/1&quot;&gt;Rafi Ibn Sultan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bagher_Ebadian_H/0/1/0/all/0/1&quot;&gt;Hassan Bagher-Ebadian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khanduri_P/0/1/0/all/0/1&quot;&gt;Prashant Khanduri&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Chetty_I/0/1/0/all/0/1&quot;&gt;Indrin J. Chetty&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Zhu_D/0/1/0/all/0/1&quot;&gt;Dongxiao Zhu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.06551">
<title>MotionBERT: A Unified Perspective on Learning Human Motion Representations. (arXiv:2210.06551v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.06551</link>
<description rdf:parseType="Literal">&lt;p&gt;We present a unified perspective on tackling various human-centric video
tasks by learning human motion representations from large-scale and
heterogeneous data resources. Specifically, we propose a pretraining stage in
which a motion encoder is trained to recover the underlying 3D motion from
noisy partial 2D observations. The motion representations acquired in this way
incorporate geometric, kinematic, and physical knowledge about human motion,
which can be easily transferred to multiple downstream tasks. We implement the
motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer)
neural network. It could capture long-range spatio-temporal relationships among
the skeletal joints comprehensively and adaptively, exemplified by the lowest
3D pose estimation error so far when trained from scratch. Furthermore, our
proposed framework achieves state-of-the-art performance on all three
downstream tasks by simply finetuning the pretrained motion encoder with a
simple regression head (1-2 layers), which demonstrates the versatility of the
learned motion representations. Code and models are available at
https://motionbert.github.io/
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhu_W/0/1/0/all/0/1&quot;&gt;Wentao Zhu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_X/0/1/0/all/0/1&quot;&gt;Xiaoxuan Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Z/0/1/0/all/0/1&quot;&gt;Zhaoyang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Libin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_W/0/1/0/all/0/1&quot;&gt;Wayne Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Y/0/1/0/all/0/1&quot;&gt;Yizhou Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.12003">
<title>HDHumans: A Hybrid Approach for High-fidelity Digital Humans. (arXiv:2210.12003v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.12003</link>
<description rdf:parseType="Literal">&lt;p&gt;Photo-real digital human avatars are of enormous importance in graphics, as
they enable immersive communication over the globe, improve gaming and
entertainment experiences, and can be particularly beneficial for AR and VR
settings. However, current avatar generation approaches either fall short in
high-fidelity novel view synthesis, generalization to novel motions,
reproduction of loose clothing, or they cannot render characters at the high
resolution offered by modern displays. To this end, we propose HDHumans, which
is the first method for HD human character synthesis that jointly produces an
accurate and temporally coherent 3D deforming surface and highly
photo-realistic images of arbitrary novel views and of motions not seen at
training time. At the technical core, our method tightly integrates a classical
deforming character template with neural radiance fields (NeRF). Our method is
carefully designed to achieve a synergy between classical surface deformation
and NeRF. First, the template guides the NeRF, which allows synthesizing novel
views of a highly dynamic and articulated character and even enables the
synthesis of novel motions. Second, we also leverage the dense pointclouds
resulting from NeRF to further improve the deforming surface via 3D-to-3D
supervision. We outperform the state of the art quantitatively and
qualitatively in terms of synthesis quality and resolution, as well as the
quality of 3D surface reconstruction.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Habermann_M/0/1/0/all/0/1&quot;&gt;Marc Habermann&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_L/0/1/0/all/0/1&quot;&gt;Lingjie Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_W/0/1/0/all/0/1&quot;&gt;Weipeng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pons_Moll_G/0/1/0/all/0/1&quot;&gt;Gerard Pons-Moll&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zollhoefer_M/0/1/0/all/0/1&quot;&gt;Michael Zollhoefer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Theobalt_C/0/1/0/all/0/1&quot;&gt;Christian Theobalt&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2210.16117">
<title>Improving the Transferability of Adversarial Attacks on Face Recognition with Beneficial Perturbation Feature Augmentation. (arXiv:2210.16117v4 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2210.16117</link>
<description rdf:parseType="Literal">&lt;p&gt;Face recognition (FR) models can be easily fooled by adversarial examples,
which are crafted by adding imperceptible perturbations on benign face images.
The existence of adversarial face examples poses a great threat to the security
of society. In order to build a more sustainable digital nation, in this paper,
we improve the transferability of adversarial face examples to expose more
blind spots of existing FR models. Though generating hard samples has shown its
effectiveness in improving the generalization of models in training tasks, the
effectiveness of utilizing this idea to improve the transferability of
adversarial face examples remains unexplored. To this end, based on the
property of hard samples and the symmetry between training tasks and
adversarial attack tasks, we propose the concept of hard models, which have
similar effects as hard samples for adversarial attack tasks. Utilizing the
concept of hard models, we propose a novel attack method called Beneficial
Perturbation Feature Augmentation Attack (BPFA), which reduces the overfitting
of adversarial examples to surrogate FR models by constantly generating new
hard models to craft the adversarial examples. Specifically, in the
backpropagation, BPFA records the gradients on pre-selected feature maps and
uses the gradient on the input image to craft the adversarial example. In the
next forward propagation, BPFA leverages the recorded gradients to add
beneficial perturbations on their corresponding feature maps to increase the
loss. Extensive experiments demonstrate that BPFA can significantly boost the
transferability of adversarial attacks on FR.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhou_F/0/1/0/all/0/1&quot;&gt;Fengfan Zhou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_H/0/1/0/all/0/1&quot;&gt;Hefei Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yuxuan Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiazhong Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Z/0/1/0/all/0/1&quot;&gt;Zongyi Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_P/0/1/0/all/0/1&quot;&gt;Ping Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2212.04761">
<title>Leveraging Spatio-Temporal Dependency for Skeleton-Based Action Recognition. (arXiv:2212.04761v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2212.04761</link>
<description rdf:parseType="Literal">&lt;p&gt;Skeleton-based action recognition has attracted considerable attention due to
its compact representation of the human body&apos;s skeletal sructure. Many recent
methods have achieved remarkable performance using graph convolutional networks
(GCNs) and convolutional neural networks (CNNs), which extract spatial and
temporal features, respectively. Although spatial and temporal dependencies in
the human skeleton have been explored separately, spatio-temporal dependency is
rarely considered. In this paper, we propose the Spatio-Temporal Curve Network
(STC-Net) to effectively leverage the spatio-temporal dependency of the human
skeleton. Our proposed network consists of two novel elements: 1) The
Spatio-Temporal Curve (STC) module; and 2) Dilated Kernels for Graph
Convolution (DK-GC). The STC module dynamically adjusts the receptive field by
identifying meaningful node connections between every adjacent frame and
generating spatio-temporal curves based on the identified node connections,
providing an adaptive spatio-temporal coverage. In addition, we propose DK-GC
to consider long-range dependencies, which results in a large receptive field
without any additional parameters by applying an extended kernel to the given
adjacency matrices of the graph. Our STC-Net combines these two modules and
achieves state-of-the-art performance on four skeleton-based action recognition
benchmarks.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jungho Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_M/0/1/0/all/0/1&quot;&gt;Minhyeok Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cho_S/0/1/0/all/0/1&quot;&gt;Suhwan Cho&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Woo_S/0/1/0/all/0/1&quot;&gt;Sungmin Woo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jang_S/0/1/0/all/0/1&quot;&gt;Sungjun Jang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_S/0/1/0/all/0/1&quot;&gt;Sangyoun Lee&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2301.02307">
<title>What You Say Is What You Show: Visual Narration Detection in Instructional Videos. (arXiv:2301.02307v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2301.02307</link>
<description rdf:parseType="Literal">&lt;p&gt;Narrated &apos;&apos;how-to&apos;&apos; videos have emerged as a promising data source for a wide
range of learning problems, from learning visual representations to training
robot policies. However, this data is extremely noisy, as the narrations do not
always describe the actions demonstrated in the video. To address this problem
we introduce the novel task of visual narration detection, which entails
determining whether a narration is visually depicted by the actions in the
video. We propose What You Say is What You Show (WYS^2), a method that
leverages multi-modal cues and pseudo-labeling to learn to detect visual
narrations with only weakly labeled data. Our model successfully detects visual
narrations in in-the-wild videos, outperforming strong baselines, and we
demonstrate its impact for state-of-the-art summarization and temporal
alignment of instructional videos.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ashutosh_K/0/1/0/all/0/1&quot;&gt;Kumar Ashutosh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Girdhar_R/0/1/0/all/0/1&quot;&gt;Rohit Girdhar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Torresani_L/0/1/0/all/0/1&quot;&gt;Lorenzo Torresani&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Grauman_K/0/1/0/all/0/1&quot;&gt;Kristen Grauman&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.02330">
<title>CIPER: Combining Invariant and Equivariant Representations Using Contrastive and Predictive Learning. (arXiv:2302.02330v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.02330</link>
<description rdf:parseType="Literal">&lt;p&gt;Self-supervised representation learning (SSRL) methods have shown great
success in computer vision. In recent studies, augmentation-based contrastive
learning methods have been proposed for learning representations that are
invariant or equivariant to pre-defined data augmentation operations. However,
invariant or equivariant features favor only specific downstream tasks
depending on the augmentations chosen. They may result in poor performance when
the learned representation does not match task requirements. Here, we consider
an active observer that can manipulate views of an object and has knowledge of
the action(s) that generated each view. We introduce Contrastive Invariant and
Predictive Equivariant Representation learning (CIPER). CIPER comprises both
invariant and equivariant learning objectives using one shared encoder and two
different output heads on top of the encoder. One output head is a projection
head with a state-of-the-art contrastive objective to encourage invariance to
augmentations. The other is a prediction head estimating the augmentation
parameters, capturing equivariant features. Both heads are discarded after
training and only the encoder is used for downstream tasks. We evaluate our
method on static image tasks and time-augmented image datasets. Our results
show that CIPER outperforms a baseline contrastive method on various tasks.
Interestingly, CIPER encourages the formation of hierarchically structured
representations where different views of an object become systematically
organized in the latent representation space.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_X/0/1/0/all/0/1&quot;&gt;Xia Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Triesch_J/0/1/0/all/0/1&quot;&gt;Jochen Triesch&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.05086">
<title>Making Substitute Models More Bayesian Can Enhance Transferability of Adversarial Examples. (arXiv:2302.05086v3 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2302.05086</link>
<description rdf:parseType="Literal">&lt;p&gt;The transferability of adversarial examples across deep neural networks
(DNNs) is the crux of many black-box attacks. Many prior efforts have been
devoted to improving the transferability via increasing the diversity in inputs
of some substitute models. In this paper, by contrast, we opt for the diversity
in substitute models and advocate to attack a Bayesian model for achieving
desirable transferability. Deriving from the Bayesian formulation, we develop a
principled strategy for possible finetuning, which can be combined with many
off-the-shelf Gaussian posterior approximations over DNN parameters. Extensive
experiments have been conducted to verify the effectiveness of our method, on
common benchmark datasets, and the results demonstrate that our method
outperforms recent state-of-the-arts by large margins (roughly 19% absolute
increase in average attack success rate on ImageNet), and, by combining with
these recent methods, further performance gain can be obtained. Our code:
https://github.com/qizhangli/MoreBayesian-attack.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Q/0/1/0/all/0/1&quot;&gt;Qizhang Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Guo_Y/0/1/0/all/0/1&quot;&gt;Yiwen Guo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zuo_W/0/1/0/all/0/1&quot;&gt;Wangmeng Zuo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Hao Chen&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2302.08890">
<title>Deep Learning for Event-based Vision: A Comprehensive Survey and Benchmarks. (arXiv:2302.08890v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2302.08890</link>
<description rdf:parseType="Literal">&lt;p&gt;Event cameras are bio-inspired sensors that capture the per-pixel intensity
changes asynchronously and produce event streams encoding the time, pixel
position, and polarity (sign) of the intensity changes. Event cameras possess a
myriad of advantages over canonical frame-based cameras, such as high temporal
resolution, high dynamic range, low latency, etc. Being capable of capturing
information in challenging visual conditions, event cameras have the potential
to overcome the limitations of frame-based cameras in the computer vision and
robotics community. In very recent years, deep learning (DL) has been brought
to this emerging field and inspired active research endeavors in mining its
potential. However, there is still a lack of taxonomies in DL techniques for
event-based vision. We first scrutinize the typical event representations with
quality enhancement methods as they play a pivotal role as inputs to the DL
models. We then provide a comprehensive taxonomy for existing DL-based methods
by structurally grouping them into two major categories: 1) image
reconstruction and restoration; 2) event-based scene understanding and 3D
vision. Importantly, we conduct benchmark experiments for the existing methods
in some representative research directions (eg, object recognition) to identify
some critical insights and problems. Finally, we make important discussions
regarding the challenges and provide new perspectives for inspiring more
research studies.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_X/0/1/0/all/0/1&quot;&gt;Xu Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yexin Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lu_Y/0/1/0/all/0/1&quot;&gt;Yunfan Lu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hua_T/0/1/0/all/0/1&quot;&gt;Tongyan Hua&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Pan_T/0/1/0/all/0/1&quot;&gt;Tianbo Pan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weiming Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tao_D/0/1/0/all/0/1&quot;&gt;Dacheng Tao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Lin Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.06635">
<title>Schema Inference for Interpretable Image Classification. (arXiv:2303.06635v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.06635</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we study a novel inference paradigm, termed as schema
inference, that learns to deductively infer the explainable predictions by
rebuilding the prior deep neural network (DNN) forwarding scheme, guided by the
prevalent philosophical cognitive concept of schema. We strive to reformulate
the conventional model inference pipeline into a graph matching policy that
associates the extracted visual concepts of an image with the pre-computed
scene impression, by analogy with human reasoning mechanism via impression
matching. To this end, we devise an elaborated architecture, termed as
SchemaNet, as a dedicated instantiation of the proposed schema inference
concept, that models both the visual semantics of input instances and the
learned abstract imaginations of target categories as topological relational
graphs. Meanwhile, to capture and leverage the compositional contributions of
visual semantics in a global view, we also introduce a universal Feat2Graph
scheme in SchemaNet to establish the relational graphs that contain abundant
interaction information. Both the theoretical analysis and the experimental
results on several benchmarks demonstrate that the proposed schema inference
achieves encouraging performance and meanwhile yields a clear picture of the
deductive process leading to the predictions. Our code is available at
https://github.com/zhfeing/SchemaNet-PyTorch.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_H/0/1/0/all/0/1&quot;&gt;Haofei Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xue_M/0/1/0/all/0/1&quot;&gt;Mengqi Xue&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaokang Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_K/0/1/0/all/0/1&quot;&gt;Kaixuan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_J/0/1/0/all/0/1&quot;&gt;Jie Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_M/0/1/0/all/0/1&quot;&gt;Mingli Song&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.08096">
<title>MELON: NeRF with Unposed Images in SO(3). (arXiv:2303.08096v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.08096</link>
<description rdf:parseType="Literal">&lt;p&gt;Neural radiance fields enable novel-view synthesis and scene reconstruction
with photorealistic quality from a few images, but require known and accurate
camera poses. Conventional pose estimation algorithms fail on smooth or
self-similar scenes, while methods performing inverse rendering from unposed
views require a rough initialization of the camera orientations. The main
difficulty of pose estimation lies in real-life objects being almost invariant
under certain transformations, making the photometric distance between rendered
views non-convex with respect to the camera parameters. Using an equivalence
relation that matches the distribution of local minima in camera space, we
reduce this space to its quotient set, in which pose estimation becomes a more
convex problem. Using a neural-network to regularize pose estimation, we
demonstrate that our method - MELON - can reconstruct a neural radiance field
from unposed images with state-of-the-art accuracy while requiring ten times
fewer views than adversarial approaches.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Levy_A/0/1/0/all/0/1&quot;&gt;Axel Levy&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Matthews_M/0/1/0/all/0/1&quot;&gt;Mark Matthews&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sela_M/0/1/0/all/0/1&quot;&gt;Matan Sela&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wetzstein_G/0/1/0/all/0/1&quot;&gt;Gordon Wetzstein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lagun_D/0/1/0/all/0/1&quot;&gt;Dmitry Lagun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.09340">
<title>Improving Automated Hemorrhage Detection in Sparse-view Computed Tomography via Deep Convolutional Neural Network based Artifact Reduction. (arXiv:2303.09340v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.09340</link>
<description rdf:parseType="Literal">&lt;p&gt;Purpose: Sparse-view computed tomography (CT) is an effective way to reduce
dose by lowering the total number of views acquired, albeit at the expense of
image quality, which, in turn, can impact the ability to detect diseases. We
explore deep learning-based artifact reduction in sparse-view cranial CT scans
and its impact on automated hemorrhage detection. Methods: We trained a U-Net
for artefact reduction on simulated sparse-view cranial CT scans from 3000
patients obtained from a public dataset and reconstructed with varying levels
of sub-sampling. Additionally, we trained a convolutional neural network on
fully sampled CT data from 17,545 patients for automated hemorrhage detection.
We evaluated the classification performance using the area under the receiver
operator characteristic curves (AUC-ROCs) with corresponding 95% confidence
intervals (CIs) and the DeLong test, along with confusion matrices. The
performance of the U-Net was compared to an analytical approach based on total
variation (TV). Results: The U-Net performed superior compared to unprocessed
and TV-processed images with respect to image quality and automated hemorrhage
diagnosis. With U-Net post-processing, the number of views can be reduced from
4096 (AUC-ROC: 0.974; 95% CI: 0.972-0.976) views to 512 views (0.973;
0.971-0.975) with minimal decrease in hemorrhage detection (P&amp;lt;.001) and to 256
views (0.967; 0.964-0.969) with a slight performance decrease (P&amp;lt;.001).
Conclusion: The results suggest that U-Net based artifact reduction
substantially enhances automated hemorrhage detection in sparse-view cranial
CTs. Our findings highlight that appropriate post-processing is crucial for
optimal image quality and diagnostic accuracy while minimizing radiation dose.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Thalhammer_J/0/1/0/all/0/1&quot;&gt;Johannes Thalhammer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schultheiss_M/0/1/0/all/0/1&quot;&gt;Manuel Schultheiss&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Dorosti_T/0/1/0/all/0/1&quot;&gt;Tina Dorosti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Lasser_T/0/1/0/all/0/1&quot;&gt;Tobias Lasser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfeiffer_F/0/1/0/all/0/1&quot;&gt;Franz Pfeiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Pfeiffer_D/0/1/0/all/0/1&quot;&gt;Daniela Pfeiffer&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Schaff_F/0/1/0/all/0/1&quot;&gt;Florian Schaff&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13126">
<title>MagicFusion: Boosting Text-to-Image Generation Performance by Fusing Diffusion Models. (arXiv:2303.13126v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13126</link>
<description rdf:parseType="Literal">&lt;p&gt;The advent of open-source AI communities has produced a cornucopia of
powerful text-guided diffusion models that are trained on various datasets.
While few explorations have been conducted on ensembling such models to combine
their strengths. In this work, we propose a simple yet effective method called
Saliency-aware Noise Blending (SNB) that can empower the fused text-guided
diffusion models to achieve more controllable generation. Specifically, we
experimentally find that the responses of classifier-free guidance are highly
related to the saliency of generated images. Thus we propose to trust different
models in their areas of expertise by blending the predicted noises of two
diffusion models in a saliency-aware manner. SNB is training-free and can be
completed within a DDIM sampling process. Additionally, it can automatically
align the semantics of two noise spaces without requiring additional
annotations such as masks. Extensive experiments show the impressive
effectiveness of SNB in various applications. Project page is available at
https://magicfusion.github.io/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_J/0/1/0/all/0/1&quot;&gt;Jing Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_H/0/1/0/all/0/1&quot;&gt;Heliang Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chaoyue Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lan_L/0/1/0/all/0/1&quot;&gt;Long Lan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenjing Yang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2303.13479">
<title>IST-Net: Prior-free Category-level Pose Estimation with Implicit Space Transformation. (arXiv:2303.13479v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2303.13479</link>
<description rdf:parseType="Literal">&lt;p&gt;Category-level 6D pose estimation aims to predict the poses and sizes of
unseen objects from a specific category. Thanks to prior deformation, which
explicitly adapts a category-specific 3D prior (i.e., a 3D template) to a given
object instance, prior-based methods attained great success and have become a
major research stream. However, obtaining category-specific priors requires
collecting a large amount of 3D models, which is labor-consuming and often not
accessible in practice. This motivates us to investigate whether priors are
necessary to make prior-based methods effective. Our empirical study shows that
the 3D prior itself is not the credit to the high performance. The keypoint
actually is the explicit deformation process, which aligns camera and world
coordinates supervised by world-space 3D models (also called canonical space).
Inspired by these observations, we introduce a simple prior-free implicit space
transformation network, namely IST-Net, to transform camera-space features to
world-space counterparts and build correspondence between them in an implicit
manner without relying on 3D priors. Besides, we design camera- and world-space
enhancers to enrich the features with pose-sensitive information and
geometrical constraints, respectively. Albeit simple, IST-Net achieves
state-of-the-art performance based-on prior-free design, with top inference
speed on the REAL275 benchmark. Our code and models are available at
https://github.com/CVMI-Lab/IST-Net.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jianhui Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_Y/0/1/0/all/0/1&quot;&gt;Yukang Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ye_X/0/1/0/all/0/1&quot;&gt;Xiaoqing Ye&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qi_X/0/1/0/all/0/1&quot;&gt;Xiaojuan Qi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.05417">
<title>The MONET dataset: Multimodal drone thermal dataset recorded in rural scenarios. (arXiv:2304.05417v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.05417</link>
<description rdf:parseType="Literal">&lt;p&gt;We present MONET, a new multimodal dataset captured using a thermal camera
mounted on a drone that flew over rural areas, and recorded human and vehicle
activities. We captured MONET to study the problem of object localisation and
behaviour understanding of targets undergoing large-scale variations and being
recorded from different and moving viewpoints. Target activities occur in two
different land sites, each with unique scene structures and cluttered
backgrounds. MONET consists of approximately 53K images featuring 162K manually
annotated bounding boxes. Each image is timestamp-aligned with drone metadata
that includes information about attitudes, speed, altitude, and GPS
coordinates. MONET is different from previous thermal drone datasets because it
features multimodal data, including rural scenes captured with thermal cameras
containing both person and vehicle targets, along with trajectory information
and metadata. We assessed the difficulty of the dataset in terms of transfer
learning between the two sites and evaluated nine object detection algorithms
to identify the open challenges associated with this type of data. Project
page: https://github.com/fabiopoiesi/monet_dataset.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Riz_L/0/1/0/all/0/1&quot;&gt;Luigi Riz&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Caraffa_A/0/1/0/all/0/1&quot;&gt;Andrea Caraffa&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bortolon_M/0/1/0/all/0/1&quot;&gt;Matteo Bortolon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Mekhalfi_M/0/1/0/all/0/1&quot;&gt;Mohamed Lamine Mekhalfi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boscaini_D/0/1/0/all/0/1&quot;&gt;Davide Boscaini&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Moura_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Moura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Antunes_J/0/1/0/all/0/1&quot;&gt;Jos&amp;#xe9; Antunes&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dias_A/0/1/0/all/0/1&quot;&gt;Andr&amp;#xe9; Dias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Silva_H/0/1/0/all/0/1&quot;&gt;Hugo Silva&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Leonidou_A/0/1/0/all/0/1&quot;&gt;Andreas Leonidou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Constantinides_C/0/1/0/all/0/1&quot;&gt;Christos Constantinides&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Keleshis_C/0/1/0/all/0/1&quot;&gt;Christos Keleshis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Abate_D/0/1/0/all/0/1&quot;&gt;Dante Abate&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Poiesi_F/0/1/0/all/0/1&quot;&gt;Fabio Poiesi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.06403">
<title>Leveraging triplet loss for unsupervised action segmentation. (arXiv:2304.06403v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.06403</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a novel fully unsupervised framework that learns
action representations suitable for the action segmentation task from the
single input video itself, without requiring any training data. Our method is a
deep metric learning approach rooted in a shallow network with a triplet loss
operating on similarity distributions and a novel triplet selection strategy
that effectively models temporal and semantic priors to discover actions in the
new representational space. Under these circumstances, we successfully recover
temporal boundaries in the learned action representations with higher quality
compared with existing unsupervised approaches. The proposed method is
evaluated on two widely used benchmark datasets for the action segmentation
task and it achieves competitive performance by applying a generic clustering
algorithm on the learned representations.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bueno_Benito_E/0/1/0/all/0/1&quot;&gt;E. Bueno-Benito&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tura_B/0/1/0/all/0/1&quot;&gt;B. Tura&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dimiccoli_M/0/1/0/all/0/1&quot;&gt;M. Dimiccoli&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2304.10727">
<title>RoCOCO: Robustness Benchmark of MS-COCO to Stress-test Image-Text Matching Models. (arXiv:2304.10727v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2304.10727</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we propose a robustness benchmark for image-text matching
models to assess their vulnerabilities. To this end, we insert adversarial
texts and images into the search pool (i.e., gallery set) and evaluate models
with the adversarial data. Specifically, we replace a word in the text to
change the meaning of the text and mix images with different images to create
perceptible changes in pixels. We assume that such explicit alterations would
not deceive a robust model, as they should understand the holistic meaning of
texts and images simultaneously. However, in our evaluations on the proposed
benchmark, many state-of-the-art models show significant performance
degradation, e.g., Recall@1: 81.9% $\rightarrow$ 64.5% in BLIP, 66.1%
$\rightarrow$ 37.5% in VSE$\infty$, where the models favor adversarial
texts/images over the original ones. This reveals the current vision-language
models may not account for subtle changes or understand the overall context of
texts and images. Our findings can provide insights for improving the
robustness of the vision-language models and devising more diverse stress-test
methods in cross-modal retrieval task. Source code and dataset will be
available at https://github.com/pseulki/rococo.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Park_S/0/1/0/all/0/1&quot;&gt;Seulki Park&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Um_D/0/1/0/all/0/1&quot;&gt;Daeho Um&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_H/0/1/0/all/0/1&quot;&gt;Hajung Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chun_S/0/1/0/all/0/1&quot;&gt;Sanghyuk Chun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yun_S/0/1/0/all/0/1&quot;&gt;Sangdoo Yun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Choi_J/0/1/0/all/0/1&quot;&gt;Jin Young Choi&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.03238">
<title>Reduction of Class Activation Uncertainty with Background Information. (arXiv:2305.03238v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.03238</link>
<description rdf:parseType="Literal">&lt;p&gt;Multitask learning is a popular approach to training high-performing neural
networks with improved generalization. In this paper, we propose a background
class to achieve improved generalization at a lower computation compared to
multitask learning to help researchers and organizations with limited
computation power. We also present a methodology for selecting background
images and discuss potential future improvements. We apply our approach to
several datasets and achieved improved generalization with much lower
computation. We also investigate class activation mappings (CAMs) of the
trained model and observed the tendency towards looking at a bigger picture in
a few class classification problems with the proposed model training
methodology. Applying transformer with the proposed background class, we
receive state-of-the-art (SOTA) performance on STL-10, Caltech-101, and
CINIC-10 datasets. Example scripts are available in the `CAM&apos; folder of the
following GitHub Repository: github.com/dipuk0506/UQ
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kabir_H/0/1/0/all/0/1&quot;&gt;H M Dipu Kabir&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09211">
<title>CB-HVTNet: A channel-boosted hybrid vision transformer network for lymphocyte assessment in histopathological images. (arXiv:2305.09211v3 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09211</link>
<description rdf:parseType="Literal">&lt;p&gt;Transformers, due to their ability to learn long range dependencies, have
overcome the shortcomings of convolutional neural networks (CNNs) for global
perspective learning. Therefore, they have gained the focus of researchers for
several vision related tasks including medical diagnosis. However, their
multi-head attention module only captures global level feature representations,
which is insufficient for medical images. To address this issue, we propose a
Channel Boosted Hybrid Vision Transformer (CB HVT) that uses transfer learning
to generate boosted channels and employs both transformers and CNNs to analyse
lymphocytes in histopathological images. The proposed CB HVT comprises five
modules, including a channel generation module, channel exploitation module,
channel merging module, region-aware module, and a detection and segmentation
head, which work together to effectively identify lymphocytes. The channel
generation module uses the idea of channel boosting through transfer learning
to extract diverse channels from different auxiliary learners. In the CB HVT,
these boosted channels are first concatenated and ranked using an attention
mechanism in the channel exploitation module. A fusion block is then utilized
in the channel merging module for a gradual and systematic merging of the
diverse boosted channels to improve the network&apos;s learning representations. The
CB HVT also employs a proposal network in its region aware module and a head to
effectively identify objects, even in overlapping regions and with artifacts.
We evaluated the proposed CB HVT on two publicly available datasets for
lymphocyte assessment in histopathological images. The results show that CB HVT
outperformed other state of the art detection models, and has good
generalization ability, demonstrating its value as a tool for pathologists.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ali_M/0/1/0/all/0/1&quot;&gt;Momina Liaqat Ali&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Rauf_Z/0/1/0/all/0/1&quot;&gt;Zunaira Rauf&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Khan_A/0/1/0/all/0/1&quot;&gt;Asifullah Khan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Sohail_A/0/1/0/all/0/1&quot;&gt;Anabia Sohail&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Ullah_R/0/1/0/all/0/1&quot;&gt;Rafi Ullah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gwak_J/0/1/0/all/0/1&quot;&gt;Jeonghwan Gwak&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.09946">
<title>AdaMSS: Adaptive Multi-Modality Segmentation-to-Survival Learning for Survival Outcome Prediction from PET/CT Images. (arXiv:2305.09946v2 [eess.IV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.09946</link>
<description rdf:parseType="Literal">&lt;p&gt;Survival prediction is a major concern for cancer management. Deep survival
models based on deep learning have been widely adopted to perform end-to-end
survival prediction from medical images. Recent deep survival models achieved
promising performance by jointly performing tumor segmentation with survival
prediction, where the models were guided to extract tumor-related information
through Multi-Task Learning (MTL). However, these deep survival models have
difficulties in exploring out-of-tumor prognostic information. In addition,
existing deep survival models are unable to effectively leverage multi-modality
images. Empirically-designed fusion strategies were commonly adopted to fuse
multi-modality information via task-specific manually-designed networks, thus
limiting the adaptability to different scenarios. In this study, we propose an
Adaptive Multi-modality Segmentation-to-Survival model (AdaMSS) for survival
prediction from PET/CT images. Instead of adopting MTL, we propose a novel
Segmentation-to-Survival Learning (SSL) strategy, where our AdaMSS is trained
for tumor segmentation and survival prediction sequentially in two stages. This
strategy enables the AdaMSS to focus on tumor regions in the first stage and
gradually expand its focus to include other prognosis-related regions in the
second stage. We also propose a data-driven strategy to fuse multi-modality
information, which realizes adaptive optimization of fusion strategies based on
training data during training. With the SSL and data-driven fusion strategies,
our AdaMSS is designed as an adaptive model that can self-adapt its focus
regions and fusion strategy for different training stages. Extensive
experiments with two large clinical datasets show that our AdaMSS outperforms
state-of-the-art survival prediction methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Meng_M/0/1/0/all/0/1&quot;&gt;Mingyuan Meng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Gu_B/0/1/0/all/0/1&quot;&gt;Bingxin Gu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Fulham_M/0/1/0/all/0/1&quot;&gt;Michael Fulham&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Song_S/0/1/0/all/0/1&quot;&gt;Shaoli Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Feng_D/0/1/0/all/0/1&quot;&gt;Dagan Feng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Bi_L/0/1/0/all/0/1&quot;&gt;Lei Bi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/eess/1/au:+Kim_J/0/1/0/all/0/1&quot;&gt;Jinman Kim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.13659">
<title>Flare-Aware Cross-modal Enhancement Network for Multi-spectral Vehicle Re-identification. (arXiv:2305.13659v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.13659</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-spectral vehicle re-identification aims to address the challenge of
identifying vehicles in complex lighting conditions by incorporating
complementary visible and infrared information. However, in harsh environments,
the discriminative cues in RGB and NIR modalities are often lost due to strong
flares from vehicle lamps or sunlight, and existing multi-modal fusion methods
are limited in their ability to recover these important cues. To address this
problem, we propose a Flare-Aware Cross-modal Enhancement Network that
adaptively restores flare-corrupted RGB and NIR features with guidance from the
flare-immunized thermal infrared spectrum. First, to reduce the influence of
locally degraded appearance due to intense flare, we propose a Mutual Flare
Mask Prediction module to jointly obtain flare-corrupted masks in RGB and NIR
modalities in a self-supervised manner. Second, to use the flare-immunized TI
information to enhance the masked RGB and NIR, we propose a Flare-Aware
Cross-modal Enhancement module that adaptively guides feature extraction of
masked RGB and NIR spectra with prior flare-immunized knowledge from the TI
spectrum. Third, to extract common informative semantic information from RGB
and NIR, we propose an Inter-modality Consistency loss that enforces semantic
consistency between the two modalities. Finally, to evaluate the proposed
FACENet in handling intense flare, we introduce a new multi-spectral vehicle
re-ID dataset, called WMVEID863, with additional challenges such as motion
blur, significant background changes, and particularly intense flare
degradation. Comprehensive experiments on both the newly collected dataset and
public benchmark multi-spectral vehicle re-ID datasets demonstrate the superior
performance of the proposed FACENet compared to state-of-the-art methods,
especially in handling strong flares. The code and dataset will be released at
this link.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_A/0/1/0/all/0/1&quot;&gt;Aihua Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_Z/0/1/0/all/0/1&quot;&gt;Zhiqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_Z/0/1/0/all/0/1&quot;&gt;Zi Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_C/0/1/0/all/0/1&quot;&gt;Chenglong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2305.18060">
<title>Mining Negative Temporal Contexts For False Positive Suppression In Real-Time Ultrasound Lesion Detection. (arXiv:2305.18060v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2305.18060</link>
<description rdf:parseType="Literal">&lt;p&gt;During ultrasonic scanning processes, real-time lesion detection can assist
radiologists in accurate cancer diagnosis. However, this essential task remains
challenging and underexplored. General-purpose real-time object detection
models can mistakenly report obvious false positives (FPs) when applied to
ultrasound videos, potentially misleading junior radiologists. One key issue is
their failure to utilize negative symptoms in previous frames, denoted as
negative temporal contexts (NTC). To address this issue, we propose to extract
contexts from previous frames, including NTC, with the guidance of inverse
optical flow. By aggregating extracted contexts, we endow the model with the
ability to suppress FPs by leveraging NTC. We call the resulting model
UltraDet. The proposed UltraDet demonstrates significant improvement over
previous state-of-the-arts and achieves real-time inference speed. We release
the code, checkpoints, and high-quality labels of the CVA-BUS dataset in
https://github.com/HaojunYu1998/UltraDet.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Haojun Yu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Youcheng Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Q/0/1/0/all/0/1&quot;&gt;QuanLin Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Z/0/1/0/all/0/1&quot;&gt;Ziwei Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_D/0/1/0/all/0/1&quot;&gt;Dengbo Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_D/0/1/0/all/0/1&quot;&gt;Dong Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_L/0/1/0/all/0/1&quot;&gt;Liwei Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07591">
<title>I See Dead People: Gray-Box Adversarial Attack on Image-To-Text Models. (arXiv:2306.07591v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07591</link>
<description rdf:parseType="Literal">&lt;p&gt;Modern image-to-text systems typically adopt the encoder-decoder framework,
which comprises two main components: an image encoder, responsible for
extracting image features, and a transformer-based decoder, used for generating
captions. Taking inspiration from the analysis of neural networks&apos; robustness
against adversarial perturbations, we propose a novel gray-box algorithm for
creating adversarial examples in image-to-text models. Unlike image
classification tasks that have a finite set of class labels, finding visually
similar adversarial examples in an image-to-text task poses greater challenges
because the captioning system allows for a virtually infinite space of possible
captions. In this paper, we present a gray-box adversarial attack on
image-to-text, both untargeted and targeted. We formulate the process of
discovering adversarial perturbations as an optimization problem that uses only
the image-encoder component, meaning the proposed attack is language-model
agnostic. Through experiments conducted on the ViT-GPT2 model, which is the
most-used image-to-text model in Hugging Face, and the Flickr30k dataset, we
demonstrate that our proposed attack successfully generates visually similar
adversarial examples, both with untargeted and targeted captions. Notably, our
attack operates in a gray-box manner, requiring no knowledge about the decoder
module. We also show that our attacks fool the popular open-source platform
Hugging Face.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lapid_R/0/1/0/all/0/1&quot;&gt;Raz Lapid&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sipper_M/0/1/0/all/0/1&quot;&gt;Moshe Sipper&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.07894">
<title>iSLAM: Imperative SLAM. (arXiv:2306.07894v3 [cs.RO] UPDATED)</title>
<link>http://arxiv.org/abs/2306.07894</link>
<description rdf:parseType="Literal">&lt;p&gt;Simultaneous localization and mapping (SLAM) stands as one of the critical
challenges in robot navigation. Recent advancements suggest that methods based
on supervised learning deliver impressive performance in front-end odometry,
while traditional optimization-based methods still play a vital role in the
back-end for minimizing estimation drift. In this paper, we found that such
decoupled paradigm can lead to only sub-optimal performance, consequently
curtailing system capabilities and generalization potential. To solve this
problem, we proposed a novel self-supervised learning framework, imperative
SLAM (iSLAM), which fosters reciprocal correction between the front-end and
back-end, thus enhancing performance without necessitating any external
supervision. Specifically, we formulate a SLAM system as a bi-level
optimization problem so that the two components are bidirectionally connected.
As a result, the front-end model is able to learn global geometric knowledge
obtained through pose graph optimization by back-propagating the residuals from
the back-end. This significantly improves the generalization ability of the
entire system and thus achieves the accuracy improvement up to 45%. To the best
of our knowledge, iSLAM is the first SLAM system showing that the front-end and
back-end can learn jointly and mutually contribute to each other in a
self-supervised manner.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fu_T/0/1/0/all/0/1&quot;&gt;Taimeng Fu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_S/0/1/0/all/0/1&quot;&gt;Shaoshu Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_C/0/1/0/all/0/1&quot;&gt;Chen Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.09618">
<title>Emergent Asymmetry of Precision and Recall for Measuring Fidelity and Diversity of Generative Models in High Dimensions. (arXiv:2306.09618v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2306.09618</link>
<description rdf:parseType="Literal">&lt;p&gt;Precision and Recall are two prominent metrics of generative performance,
which were proposed to separately measure the fidelity and diversity of
generative models. Given their central role in comparing and improving
generative models, understanding their limitations are crucially important. To
that end, in this work, we identify a critical flaw in the common approximation
of these metrics using k-nearest-neighbors, namely, that the very
interpretations of fidelity and diversity that are assigned to Precision and
Recall can fail in high dimensions, resulting in very misleading conclusions.
Specifically, we empirically and theoretically show that as the number of
dimensions grows, two model distributions with supports at equal point-wise
distance from the support of the real distribution, can have vastly different
Precision and Recall regardless of their respective distributions, hence an
emergent asymmetry in high dimensions. Based on our theoretical insights, we
then provide simple yet effective modifications to these metrics to construct
symmetric metrics regardless of the number of dimensions. Finally, we provide
experiments on real-world datasets to illustrate that the identified flaw is
not merely a pathological case, and that our proposed metrics are effective in
alleviating its impact.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Khayatkhoei_M/0/1/0/all/0/1&quot;&gt;Mahyar Khayatkhoei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+AbdAlmageed_W/0/1/0/all/0/1&quot;&gt;Wael AbdAlmageed&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.13074">
<title>Iterative Scale-Up ExpansionIoU and Deep Features Association for Multi-Object Tracking in Sports. (arXiv:2306.13074v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.13074</link>
<description rdf:parseType="Literal">&lt;p&gt;Multi-object tracking algorithms have made significant advancements due to
the recent developments in object detection. However, most existing methods
primarily focus on tracking pedestrians or vehicles, which exhibit relatively
simple and regular motion patterns. Consequently, there is a scarcity of
algorithms that address the tracking of targets with irregular or non-linear
motion, such as multi-athlete tracking. Furthermore, popular tracking
algorithms often rely on the Kalman filter for object motion modeling, which
fails to track objects when their motion contradicts the linear motion
assumption of the Kalman filter. Due to this reason, we proposed a novel online
and robust multi-object tracking approach, named Iterative Scale-Up
ExpansionIoU and Deep Features for multi-object tracking. Unlike conventional
methods, we abandon the use of the Kalman filter and propose utilizing the
iterative scale-up expansion IoU. This approach achieves superior tracking
performance without requiring additional training data or adopting a more
robust detector, all while maintaining a lower computational cost compared to
other appearance-based methods. Our proposed method demonstrates remarkable
effectiveness in tracking irregular motion objects, achieving a score of 76.9%
in HOTA. It outperforms all state-of-the-art tracking algorithms on the
SportsMOT dataset, covering various kinds of sport scenarios.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_H/0/1/0/all/0/1&quot;&gt;Hsiang-Wei Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_C/0/1/0/all/0/1&quot;&gt;Cheng-Yen Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_J/0/1/0/all/0/1&quot;&gt;Jiacheng Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hwang_J/0/1/0/all/0/1&quot;&gt;Jenq-Neng Hwang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_C/0/1/0/all/0/1&quot;&gt;Chung-I Huang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2306.16197">
<title>Multi-IMU with Online Self-Consistency for Freehand 3D Ultrasound Reconstruction. (arXiv:2306.16197v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2306.16197</link>
<description rdf:parseType="Literal">&lt;p&gt;Ultrasound (US) imaging is a popular tool in clinical diagnosis, offering
safety, repeatability, and real-time capabilities. Freehand 3D US is a
technique that provides a deeper understanding of scanned regions without
increasing complexity. However, estimating elevation displacement and
accumulation error remains challenging, making it difficult to infer the
relative position using images alone. The addition of external lightweight
sensors has been proposed to enhance reconstruction performance without adding
complexity, which has been shown to be beneficial. We propose a novel online
self-consistency network (OSCNet) using multiple inertial measurement units
(IMUs) to improve reconstruction performance. OSCNet utilizes a modal-level
self-supervised strategy to fuse multiple IMU information and reduce
differences between reconstruction results obtained from each IMU data.
Additionally, a sequence-level self-consistency strategy is proposed to improve
the hierarchical consistency of prediction results among the scanning sequence
and its sub-sequences. Experiments on large-scale arm and carotid datasets with
multiple scanning tactics demonstrate that our OSCNet outperforms previous
methods, achieving state-of-the-art reconstruction performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_M/0/1/0/all/0/1&quot;&gt;Mingyuan Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_X/0/1/0/all/0/1&quot;&gt;Xin Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yan_Z/0/1/0/all/0/1&quot;&gt;Zhongnuo Yan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Junyu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yuanji Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_J/0/1/0/all/0/1&quot;&gt;Jiongquan Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_X/0/1/0/all/0/1&quot;&gt;Xindi Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qian_J/0/1/0/all/0/1&quot;&gt;Jikuan Qian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_J/0/1/0/all/0/1&quot;&gt;Jun Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ni_D/0/1/0/all/0/1&quot;&gt;Dong Ni&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.00574">
<title>Bidirectional Temporal Diffusion Model for Temporally Consistent Human Animation. (arXiv:2307.00574v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.00574</link>
<description rdf:parseType="Literal">&lt;p&gt;We introduce a method to generate temporally coherent human animation from a
single image, a video, or a random noise. This problem has been formulated as
modeling of an auto-regressive generation, i.e., to regress past frames to
decode future frames. However, such unidirectional generation is highly prone
to motion drifting over time, generating unrealistic human animation with
significant artifacts such as appearance distortion. We claim that
bidirectional temporal modeling enforces temporal coherence on a generative
network by largely suppressing the motion ambiguity of human appearance. To
prove our claim, we design a novel human animation framework using a denoising
diffusion model: a neural network learns to generate the image of a person by
denoising temporal Gaussian noises whose intermediate results are
cross-conditioned bidirectionally between consecutive frames. In the
experiments, our method demonstrates strong performance compared to existing
unidirectional approaches with realistic temporal coherence
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Adiya_T/0/1/0/all/0/1&quot;&gt;Tserendorj Adiya&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kim_S/0/1/0/all/0/1&quot;&gt;Sanghun Kim&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lee_J/0/1/0/all/0/1&quot;&gt;Jung Eun Lee&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yoon_J/0/1/0/all/0/1&quot;&gt;Jae Shin Yoon&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Lim_H/0/1/0/all/0/1&quot;&gt;Hwasup Lim&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.01533">
<title>Unsupervised Video Anomaly Detection with Diffusion Models Conditioned on Compact Motion Representations. (arXiv:2307.01533v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.01533</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to address the unsupervised video anomaly detection (VAD)
problem, which involves classifying each frame in a video as normal or
abnormal, without any access to labels. To accomplish this, the proposed method
employs conditional diffusion models, where the input data is the
spatiotemporal features extracted from a pre-trained network, and the condition
is the features extracted from compact motion representations that summarize a
given video segment in terms of its motion and appearance. Our method utilizes
a data-driven threshold and considers a high reconstruction error as an
indicator of anomalous events. This study is the first to utilize compact
motion representations for VAD and the experiments conducted on two large-scale
VAD benchmarks demonstrate that they supply relevant information to the
diffusion model, and consequently improve VAD performances w.r.t the prior art.
Importantly, our method exhibits better generalization performance across
different datasets, notably outperforming both the state-of-the-art and
baseline methods. The code of our method is available at
https://github.com/AnilOsmanTur/conditioned_video_anomaly_diffusion
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tur_A/0/1/0/all/0/1&quot;&gt;Anil Osman Tur&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+DallAsen_N/0/1/0/all/0/1&quot;&gt;Nicola Dall&amp;#x27;Asen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Beyan_C/0/1/0/all/0/1&quot;&gt;Cigdem Beyan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ricci_E/0/1/0/all/0/1&quot;&gt;Elisa Ricci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.02203">
<title>Neural Fields for Interactive Visualization of Statistical Dependencies in 3D Simulation Ensembles. (arXiv:2307.02203v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.02203</link>
<description rdf:parseType="Literal">&lt;p&gt;We present the first neural network that has learned to compactly represent
and can efficiently reconstruct the statistical dependencies between the values
of physical variables at different spatial locations in large 3D simulation
ensembles. Going beyond linear dependencies, we consider mutual information as
a measure of non-linear dependence. We demonstrate learning and reconstruction
with a large weather forecast ensemble comprising 1000 members, each storing
multiple physical variables at a 250 x 352 x 20 simulation grid. By
circumventing compute-intensive statistical estimators at runtime, we
demonstrate significantly reduced memory and computation requirements for
reconstructing the major dependence structures. This enables embedding the
estimator into a GPU-accelerated direct volume renderer and interactively
visualizing all mutual dependencies for a selected domain point.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Farokhmanesh_F/0/1/0/all/0/1&quot;&gt;Fatemeh Farokhmanesh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hohlein_K/0/1/0/all/0/1&quot;&gt;Kevin H&amp;#xf6;hlein&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Neuhauser_C/0/1/0/all/0/1&quot;&gt;Christoph Neuhauser&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Westermann_R/0/1/0/all/0/1&quot;&gt;R&amp;#xfc;diger Westermann&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.03135">
<title>Distilling Large Vision-Language Model with Out-of-Distribution Generalizability. (arXiv:2307.03135v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.03135</link>
<description rdf:parseType="Literal">&lt;p&gt;Large vision-language models have achieved outstanding performance, but their
size and computational requirements make their deployment on
resource-constrained devices and time-sensitive tasks impractical. Model
distillation, the process of creating smaller, faster models that maintain the
performance of larger models, is a promising direction towards the solution.
This paper investigates the distillation of visual representations in large
teacher vision-language models into lightweight student models using a small-
or mid-scale dataset. Notably, this study focuses on open-vocabulary
out-of-distribution (OOD) generalization, a challenging problem that has been
overlooked in previous model distillation literature. We propose two principles
from vision and language modality perspectives to enhance student&apos;s OOD
generalization: (1) by better imitating teacher&apos;s visual representation space,
and carefully promoting better coherence in vision-language alignment with the
teacher; (2) by enriching the teacher&apos;s language representations with
informative and finegrained semantic attributes to effectively distinguish
between different labels. We propose several metrics and conduct extensive
experiments to investigate their techniques. The results demonstrate
significant improvements in zero-shot and few-shot student performance on
open-vocabulary out-of-distribution classification, highlighting the
effectiveness of our proposed approaches. Code released at
https://github.com/xuanlinli17/large_vlm_distillation_ood
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_X/0/1/0/all/0/1&quot;&gt;Xuanlin Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_Y/0/1/0/all/0/1&quot;&gt;Yunhao Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_M/0/1/0/all/0/1&quot;&gt;Minghua Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_Z/0/1/0/all/0/1&quot;&gt;Zhan Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_Z/0/1/0/all/0/1&quot;&gt;Zhuowen Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_H/0/1/0/all/0/1&quot;&gt;Hao Su&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04639">
<title>Multimodal brain age estimation using interpretable adaptive population-graph learning. (arXiv:2307.04639v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04639</link>
<description rdf:parseType="Literal">&lt;p&gt;Brain age estimation is clinically important as it can provide valuable
information in the context of neurodegenerative diseases such as Alzheimer&apos;s.
Population graphs, which include multimodal imaging information of the subjects
along with the relationships among the population, have been used in literature
along with Graph Convolutional Networks (GCNs) and have proved beneficial for a
variety of medical imaging tasks. A population graph is usually static and
constructed manually using non-imaging information. However, graph construction
is not a trivial task and might significantly affect the performance of the
GCN, which is inherently very sensitive to the graph structure. In this work,
we propose a framework that learns a population graph structure optimized for
the downstream task. An attention mechanism assigns weights to a set of imaging
and non-imaging features (phenotypes), which are then used for edge extraction.
The resulting graph is used to train the GCN. The entire pipeline can be
trained end-to-end. Additionally, by visualizing the attention weights that
were the most important for the graph construction, we increase the
interpretability of the graph. We use the UK Biobank, which provides a large
variety of neuroimaging and non-imaging phenotypes, to evaluate our method on
brain age regression and classification. The proposed method outperforms
competing static graph approaches and other state-of-the-art adaptive methods.
We further show that the assigned attention scores indicate that there are both
imaging and non-imaging phenotypes that are informative for brain age
estimation and are in agreement with the relevant literature.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bintsi_K/0/1/0/all/0/1&quot;&gt;Kyriaki-Margarita Bintsi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baltatzis_V/0/1/0/all/0/1&quot;&gt;Vasileios Baltatzis&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Potamias_R/0/1/0/all/0/1&quot;&gt;Rolandos Alexandros Potamias&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hammers_A/0/1/0/all/0/1&quot;&gt;Alexander Hammers&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Rueckert_D/0/1/0/all/0/1&quot;&gt;Daniel Rueckert&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.04838">
<title>CREPE: Learnable Prompting With CLIP Improves Visual Relationship Prediction. (arXiv:2307.04838v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.04838</link>
<description rdf:parseType="Literal">&lt;p&gt;In this paper, we explore the potential of Vision-Language Models (VLMs),
specifically CLIP, in predicting visual object relationships, which involves
interpreting visual features from images into language-based relations. Current
state-of-the-art methods use complex graphical models that utilize language
cues and visual features to address this challenge. We hypothesize that the
strong language priors in CLIP embeddings can simplify these graphical models
paving for a simpler approach. We adopt the UVTransE relation prediction
framework, which learns the relation as a translational embedding with subject,
object, and union box embeddings from a scene. We systematically explore the
design of CLIP-based subject, object, and union-box representations within the
UVTransE framework and propose CREPE (CLIP Representation Enhanced Predicate
Estimation). CREPE utilizes text-based representations for all three bounding
boxes and introduces a novel contrastive training strategy to automatically
infer the text prompt for union-box. Our approach achieves state-of-the-art
performance in predicate estimation, mR@5 27.79, and mR@20 31.95 on the Visual
Genome benchmark, achieving a 15.3\% gain in performance over recent
state-of-the-art at mR@20. This work demonstrates CLIP&apos;s effectiveness in
object relation prediction and encourages further research on VLMs in this
challenging domain.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Subramanyam_R/0/1/0/all/0/1&quot;&gt;Rakshith Subramanyam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jayram_T/0/1/0/all/0/1&quot;&gt;T. S. Jayram&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Anirudh_R/0/1/0/all/0/1&quot;&gt;Rushil Anirudh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Thiagarajan_J/0/1/0/all/0/1&quot;&gt;Jayaraman J. Thiagarajan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.05151">
<title>ExFaceGAN: Exploring Identity Directions in GAN&apos;s Learned Latent Space for Synthetic Identity Generation. (arXiv:2307.05151v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.05151</link>
<description rdf:parseType="Literal">&lt;p&gt;Deep generative models have recently presented impressive results in
generating realistic face images of random synthetic identities.
&lt;/p&gt;
&lt;p&gt;To generate multiple samples of a certain synthetic identity, previous works
proposed to disentangle the latent space of GANs by incorporating additional
supervision or regularization, enabling the manipulation of certain attributes.
Others proposed to disentangle specific factors in unconditional pretrained
GANs latent spaces to control their output, which also requires supervision by
attribute classifiers. Moreover, these attributes are entangled in GAN&apos;s latent
space, making it difficult to manipulate them without affecting the identity
information. We propose in this work a framework, ExFaceGAN, to disentangle
identity information in pretrained GANs latent spaces, enabling the generation
of multiple samples of any synthetic identity. Given a reference latent code of
any synthetic image and latent space of pretrained GAN, our ExFaceGAN learns an
identity directional boundary that disentangles the latent space into two
sub-spaces, with latent codes of samples that are either identity similar or
dissimilar to a reference image. By sampling from each side of the boundary,
our ExFaceGAN can generate multiple samples of synthetic identity without the
need for designing a dedicated architecture or supervision from attribute
classifiers. We demonstrate the generalizability and effectiveness of ExFaceGAN
by integrating it into learned latent spaces of three SOTA GAN approaches. As
an example of the practical benefit of our ExFaceGAN, we empirically prove that
data generated by ExFaceGAN can be successfully used to train face recognition
models (\url{https://github.com/fdbtrs/ExFaceGAN}).
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Boutros_F/0/1/0/all/0/1&quot;&gt;Fadi Boutros&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Klemt_M/0/1/0/all/0/1&quot;&gt;Marcel Klemt&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Fang_M/0/1/0/all/0/1&quot;&gt;Meiling Fang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kuijper_A/0/1/0/all/0/1&quot;&gt;Arjan Kuijper&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Damer_N/0/1/0/all/0/1&quot;&gt;Naser Damer&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06385">
<title>Temporal Label-Refinement for Weakly-Supervised Audio-Visual Event Localization. (arXiv:2307.06385v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06385</link>
<description rdf:parseType="Literal">&lt;p&gt;Audio-Visual Event Localization (AVEL) is the task of temporally localizing
and classifying \emph{audio-visual events}, i.e., events simultaneously visible
and audible in a video. In this paper, we solve AVEL in a weakly-supervised
setting, where only video-level event labels (their presence/absence, but not
their locations in time) are available as supervision for training. Our idea is
to use a base model to estimate labels on the training data at a finer temporal
resolution than at the video level and re-train the model with these labels.
I.e., we determine the subset of labels for each \emph{slice} of frames in a
training video by (i) replacing the frames outside the slice with those from a
second video having no overlap in video-level labels, and (ii) feeding this
synthetic video into the base model to extract labels for just the slice in
question. To handle the out-of-distribution nature of our synthetic videos, we
propose an auxiliary objective for the base model that induces more reliable
predictions of the localized event labels as desired. Our three-stage pipeline
outperforms several existing AVEL methods with no architectural changes and
improves performance on a related weakly-supervised task as well.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ramakrishnan_K/0/1/0/all/0/1&quot;&gt;Kalyan Ramakrishnan&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.06689">
<title>YOLIC: An Efficient Method for Object Localization and Classification on Edge Devices. (arXiv:2307.06689v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.06689</link>
<description rdf:parseType="Literal">&lt;p&gt;In the realm of Tiny AI, we introduce &quot;You Only Look at Interested Cells&quot;
(YOLIC), an efficient method for object localization and classification on edge
devices. Seamlessly blending the strengths of semantic segmentation and object
detection, YOLIC offers superior computational efficiency and precision. By
adopting Cells of Interest for classification instead of individual pixels,
YOLIC encapsulates relevant information, reduces computational load, and
enables rough object shape inference. Importantly, the need for bounding box
regression is obviated, as YOLIC capitalizes on the predetermined cell
configuration that provides information about potential object location, size,
and shape. To tackle the issue of single-label classification limitations, a
multi-label classification approach is applied to each cell, effectively
recognizing overlapping or closely situated objects. This paper presents
extensive experiments on multiple datasets, demonstrating that YOLIC achieves
detection performance comparable to the state-of-the-art YOLO algorithms while
surpassing in speed, exceeding 30fps on a Raspberry Pi 4B CPU. All resources
related to this study, including datasets, cell designer, image annotation
tool, and source code, have been made publicly available on our project website
at https://kai3316.github.io/yolic.github.io
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_K/0/1/0/all/0/1&quot;&gt;Kai Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhao_Q/0/1/0/all/0/1&quot;&gt;Qiangfu Zhao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tomioka_Y/0/1/0/all/0/1&quot;&gt;Yoichi Tomioka&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_Y/0/1/0/all/0/1&quot;&gt;Yong Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07813">
<title>TinyTracker: Ultra-Fast and Ultra-Low-Power Edge Vision In-Sensor for Gaze Estimation. (arXiv:2307.07813v3 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07813</link>
<description rdf:parseType="Literal">&lt;p&gt;Intelligent edge vision tasks encounter the critical challenge of ensuring
power and latency efficiency due to the typically heavy computational load they
impose on edge platforms.This work leverages one of the first &quot;AI in sensor&quot;
vision platforms, IMX500 by Sony, to achieve ultra-fast and ultra-low-power
end-to-end edge vision applications. We evaluate the IMX500 and compare it to
other edge platforms, such as the Google Coral Dev Micro and Sony Spresense, by
exploring gaze estimation as a case study. We propose TinyTracker, a highly
efficient, fully quantized model for 2D gaze estimation designed to maximize
the performance of the edge vision systems considered in this study.
TinyTracker achieves a 41x size reduction (600Kb) compared to iTracker [1]
without significant loss in gaze estimation accuracy (maximum of 0.16 cm when
fully quantized). TinyTracker&apos;s deployment on the Sony IMX500 vision sensor
results in end-to-end latency of around 19ms. The camera takes around 17.9ms to
read, process and transmit the pixels to the accelerator. The inference time of
the network is 0.86ms with an additional 0.24 ms for retrieving the results
from the sensor. The overall energy consumption of the end-to-end system is 4.9
mJ, including 0.06 mJ for inference. The end-to-end study shows that IMX500 is
1.7x faster than CoralMicro (19ms vs 34.4ms) and 7x more power efficient (4.9mJ
VS 34.2mJ)
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bonazzi_P/0/1/0/all/0/1&quot;&gt;Pietro Bonazzi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ruegg_T/0/1/0/all/0/1&quot;&gt;Thomas Ruegg&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bian_S/0/1/0/all/0/1&quot;&gt;Sizhen Bian&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_Y/0/1/0/all/0/1&quot;&gt;Yawei Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Magno_M/0/1/0/all/0/1&quot;&gt;Michele Magno&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07859">
<title>Unified Adversarial Patch for Cross-modal Attacks in the Physical World. (arXiv:2307.07859v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07859</link>
<description rdf:parseType="Literal">&lt;p&gt;Recently, physical adversarial attacks have been presented to evade
DNNs-based object detectors. To ensure the security, many scenarios are
simultaneously deployed with visible sensors and infrared sensors, leading to
the failures of these single-modal physical attacks. To show the potential
risks under such scenes, we propose a unified adversarial patch to perform
cross-modal physical attacks, i.e., fooling visible and infrared object
detectors at the same time via a single patch. Considering different imaging
mechanisms of visible and infrared sensors, our work focuses on modeling the
shapes of adversarial patches, which can be captured in different modalities
when they change. To this end, we design a novel boundary-limited shape
optimization to achieve the compact and smooth shapes, and thus they can be
easily implemented in the physical world. In addition, to balance the fooling
degree between visible detector and infrared detector during the optimization
process, we propose a score-aware iterative evaluation, which can guide the
adversarial patch to iteratively reduce the predicted scores of the multi-modal
sensors. We finally test our method against the one-stage detector: YOLOv3 and
the two-stage detector: Faster RCNN. Results show that our unified patch
achieves an Attack Success Rate (ASR) of 73.33% and 69.17%, respectively. More
importantly, we verify the effective attacks in the physical world when visible
and infrared sensors shoot the objects under various settings like different
angles, distances, postures, and scenes.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_X/0/1/0/all/0/1&quot;&gt;Xingxing Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Huang_Y/0/1/0/all/0/1&quot;&gt;Yao Huang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_Y/0/1/0/all/0/1&quot;&gt;Yitong Sun&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_J/0/1/0/all/0/1&quot;&gt;Jie Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07873">
<title>Why Does Little Robustness Help? Understanding Adversarial Transferability From Surrogate Training. (arXiv:2307.07873v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07873</link>
<description rdf:parseType="Literal">&lt;p&gt;Adversarial examples (AEs) for DNNs have been shown to be transferable: AEs
that successfully fool white-box surrogate models can also deceive other
black-box models with different architectures. Although a bunch of empirical
studies have provided guidance on generating highly transferable AEs, many of
these findings lack explanations and even lead to inconsistent advice. In this
paper, we take a further step towards understanding adversarial
transferability, with a particular focus on surrogate aspects. Starting from
the intriguing little robustness phenomenon, where models adversarially trained
with mildly perturbed adversarial samples can serve as better surrogates, we
attribute it to a trade-off between two predominant factors: model smoothness
and gradient similarity. Our investigations focus on their joint effects,
rather than their separate correlations with transferability. Through a series
of theoretical and empirical analyses, we conjecture that the data distribution
shift in adversarial training explains the degradation of gradient similarity.
Building on these insights, we explore the impacts of data augmentation and
gradient regularization on transferability and identify that the trade-off
generally exists in the various training mechanisms, thus building a
comprehensive blueprint for the regulation mechanism behind transferability.
Finally, we provide a general route for constructing better surrogates to boost
transferability which optimizes both model smoothness and gradient similarity
simultaneously, e.g., the combination of input gradient regularization and
sharpness-aware minimization (SAM), validated by extensive experiments. In
summary, we call for attention to the united impacts of these two factors for
launching effective transfer attacks, rather than optimizing one while ignoring
the other, and emphasize the crucial role of manipulating surrogate models.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_Y/0/1/0/all/0/1&quot;&gt;Yechao Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Hu_S/0/1/0/all/0/1&quot;&gt;Shengshan Hu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_L/0/1/0/all/0/1&quot;&gt;Leo Yu Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_J/0/1/0/all/0/1&quot;&gt;Junyu Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_M/0/1/0/all/0/1&quot;&gt;Minghui Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xiaogeng Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wan_W/0/1/0/all/0/1&quot;&gt;Wei Wan&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_H/0/1/0/all/0/1&quot;&gt;Hai Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07928">
<title>Reinforced Disentanglement for Face Swapping without Skip Connection. (arXiv:2307.07928v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07928</link>
<description rdf:parseType="Literal">&lt;p&gt;The SOTA face swap models still suffer the problem of either target identity
(i.e., shape) being leaked or the target non-identity attributes (i.e.,
background, hair) failing to be fully preserved in the final results. We show
that this insufficient disentanglement is caused by two flawed designs that
were commonly adopted in prior models: (1) counting on only one compressed
encoder to represent both the semantic-level non-identity facial
attributes(i.e., pose) and the pixel-level non-facial region details, which is
contradictory to satisfy at the same time; (2) highly relying on long
skip-connections between the encoder and the final generator, leaking a certain
amount of target face identity into the result. To fix them, we introduce a new
face swap framework called &apos;WSC-swap&apos; that gets rid of skip connections and
uses two target encoders to respectively capture the pixel-level non-facial
region attributes and the semantic non-identity attributes in the face region.
To further reinforce the disentanglement learning for the target encoder, we
employ both identity removal loss via adversarial training (i.e., GAN) and the
non-identity preservation loss via prior 3DMM models like [11]. Extensive
experiments on both FaceForensics++ and CelebA-HQ show that our results
significantly outperform previous works on a rich set of metrics, including one
novel metric for measuring identity consistency that was completely neglected
before.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ren_X/0/1/0/all/0/1&quot;&gt;Xiaohang Ren&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_X/0/1/0/all/0/1&quot;&gt;Xingyu Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yao_P/0/1/0/all/0/1&quot;&gt;Pengfei Yao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shum_H/0/1/0/all/0/1&quot;&gt;Heung-Yeung Shum&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_B/0/1/0/all/0/1&quot;&gt;Baoyuan Wang&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.07935">
<title>S2R-ViT for Multi-Agent Cooperative Perception: Bridging the Gap from Simulation to Reality. (arXiv:2307.07935v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.07935</link>
<description rdf:parseType="Literal">&lt;p&gt;Due to the lack of real multi-agent data and time-consuming of labeling,
existing multi-agent cooperative perception algorithms usually select the
simulated sensor data for training and validating. However, the perception
performance is degraded when these simulation-trained models are deployed to
the real world, due to the significant domain gap between the simulated and
real data. In this paper, we propose the first Simulation-to-Reality transfer
learning framework for multi-agent cooperative perception using a novel Vision
Transformer, named as S2R-ViT, which considers both the Implementation Gap and
Feature Gap between simulated and real data. We investigate the effects of
these two types of domain gaps and propose a novel uncertainty-aware vision
transformer to effectively relief the Implementation Gap and an agent-based
feature adaptation module with inter-agent and ego-agent discriminators to
reduce the Feature Gap. Our intensive experiments on the public multi-agent
cooperative perception datasets OPV2V and V2V4Real demonstrate that the
proposed S2R-ViT can effectively bridge the gap from simulation to reality and
outperform other methods significantly for point cloud-based 3D object
detection.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_J/0/1/0/all/0/1&quot;&gt;Jinlong Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Xu_R/0/1/0/all/0/1&quot;&gt;Runsheng Xu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_X/0/1/0/all/0/1&quot;&gt;Xinyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_B/0/1/0/all/0/1&quot;&gt;Baolu Li&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zou_Q/0/1/0/all/0/1&quot;&gt;Qin Zou&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ma_J/0/1/0/all/0/1&quot;&gt;Jiaqi Ma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yu_H/0/1/0/all/0/1&quot;&gt;Hongkai Yu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08015">
<title>Boosting 3-DoF Ground-to-Satellite Camera Localization Accuracy via Geometry-Guided Cross-View Transformer. (arXiv:2307.08015v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08015</link>
<description rdf:parseType="Literal">&lt;p&gt;Image retrieval-based cross-view localization methods often lead to very
coarse camera pose estimation, due to the limited sampling density of the
database satellite images. In this paper, we propose a method to increase the
accuracy of a ground camera&apos;s location and orientation by estimating the
relative rotation and translation between the ground-level image and its
matched/retrieved satellite image. Our approach designs a geometry-guided
cross-view transformer that combines the benefits of conventional geometry and
learnable cross-view transformers to map the ground-view observations to an
overhead view. Given the synthesized overhead view and observed satellite
feature maps, we construct a neural pose optimizer with strong global
information embedding ability to estimate the relative rotation between them.
After aligning their rotations, we develop an uncertainty-guided spatial
correlation to generate a probability map of the vehicle locations, from which
the relative translation can be determined. Experimental results demonstrate
that our method significantly outperforms the state-of-the-art. Notably, the
likelihood of restricting the vehicle lateral pose to be within 1m of its
Ground Truth (GT) value on the cross-view KITTI dataset has been improved from
$35.54\%$ to $76.44\%$, and the likelihood of restricting the vehicle
orientation to be within $1^{\circ}$ of its GT value has been improved from
$19.64\%$ to $99.10\%$.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shi_Y/0/1/0/all/0/1&quot;&gt;Yujiao Shi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_F/0/1/0/all/0/1&quot;&gt;Fei Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Vora_A/0/1/0/all/0/1&quot;&gt;Ankit Vora&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Perincherry_A/0/1/0/all/0/1&quot;&gt;Akhil Perincherry&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_H/0/1/0/all/0/1&quot;&gt;Hongdong Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08347">
<title>M-FLAG: Medical Vision-Language Pre-training with Frozen Language Models and Latent Space Geometry Optimization. (arXiv:2307.08347v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08347</link>
<description rdf:parseType="Literal">&lt;p&gt;Medical vision-language models enable co-learning and integrating features
from medical imaging and clinical text. However, these models are not easy to
train and the latent representation space can be complex. Here we propose a
novel way for pre-training and regularising medical vision-language models. The
proposed method, named Medical vision-language pre-training with Frozen
language models and Latent spAce Geometry optimization (M-FLAG), leverages a
frozen language model for training stability and efficiency and introduces a
novel orthogonality loss to harmonize the latent space geometry. We demonstrate
the potential of the pre-trained model on three downstream tasks: medical image
classification, segmentation, and object detection. Extensive experiments
across five public datasets demonstrate that M-FLAG significantly outperforms
existing medical vision-language pre-training approaches and reduces the number
of parameters by 78\%. Notably, M-FLAG achieves outstanding performance on the
segmentation task while using only 1\% of the RSNA dataset, even outperforming
ImageNet pre-trained models that have been fine-tuned using 100\% of the data.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Che Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cheng_S/0/1/0/all/0/1&quot;&gt;Sibo Cheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_C/0/1/0/all/0/1&quot;&gt;Chen Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiao_M/0/1/0/all/0/1&quot;&gt;Mengyun Qiao&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zhang_W/0/1/0/all/0/1&quot;&gt;Weitong Zhang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Shah_A/0/1/0/all/0/1&quot;&gt;Anand Shah&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Bai_W/0/1/0/all/0/1&quot;&gt;Wenjia Bai&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Arcucci_R/0/1/0/all/0/1&quot;&gt;Rossella Arcucci&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08723">
<title>Revisiting Scene Text Recognition: A Data Perspective. (arXiv:2307.08723v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08723</link>
<description rdf:parseType="Literal">&lt;p&gt;This paper aims to re-assess scene text recognition (STR) from a
data-oriented perspective. We begin by revisiting the six commonly used
benchmarks in STR and observe a trend of performance saturation, whereby only
2.91% of the benchmark images cannot be accurately recognized by an ensemble of
13 representative models. While these results are impressive and suggest that
STR could be considered solved, however, we argue that this is primarily due to
the less challenging nature of the common benchmarks, thus concealing the
underlying issues that STR faces. To this end, we consolidate a large-scale
real STR dataset, namely Union14M, which comprises 4 million labeled images and
10 million unlabeled images, to assess the performance of STR models in more
complex real-world scenarios. Our experiments demonstrate that the 13 models
can only achieve an average accuracy of 66.53% on the 4 million labeled images,
indicating that STR still faces numerous challenges in the real world. By
analyzing the error patterns of the 13 models, we identify seven open
challenges in STR and develop a challenge-driven benchmark consisting of eight
distinct subsets to facilitate further progress in the field. Our exploration
demonstrates that STR is far from being solved and leveraging data may be a
promising solution. In this regard, we find that utilizing the 10 million
unlabeled images through self-supervised pre-training can significantly improve
the robustness of STR model in real-world scenarios and leads to
state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jiang_Q/0/1/0/all/0/1&quot;&gt;Qing Jiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jiapeng Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Peng_D/0/1/0/all/0/1&quot;&gt;Dezhi Peng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_C/0/1/0/all/0/1&quot;&gt;Chongyu Liu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_L/0/1/0/all/0/1&quot;&gt;Lianwen Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08779">
<title>Similarity Min-Max: Zero-Shot Day-Night Domain Adaptation. (arXiv:2307.08779v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08779</link>
<description rdf:parseType="Literal">&lt;p&gt;Low-light conditions not only hamper human visual experience but also degrade
the model&apos;s performance on downstream vision tasks. While existing works make
remarkable progress on day-night domain adaptation, they rely heavily on domain
knowledge derived from the task-specific nighttime dataset. This paper
challenges a more complicated scenario with border applicability, i.e.,
zero-shot day-night domain adaptation, which eliminates reliance on any
nighttime data. Unlike prior zero-shot adaptation approaches emphasizing either
image-level translation or model-level adaptation, we propose a similarity
min-max paradigm that considers them under a unified framework. On the image
level, we darken images towards minimum feature similarity to enlarge the
domain gap. Then on the model level, we maximize the feature similarity between
the darkened images and their normal-light counterparts for better model
adaptation. To the best of our knowledge, this work represents the pioneering
effort in jointly optimizing both aspects, resulting in a significant
improvement of model generalizability. Extensive experiments demonstrate our
method&apos;s effectiveness and broad applicability on various nighttime vision
tasks, including classification, semantic segmentation, visual place
recognition, and video action recognition. Code and pre-trained models are
available at https://red-fairy.github.io/ZeroShotDayNightDA-Webpage/.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Luo_R/0/1/0/all/0/1&quot;&gt;Rundong Luo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_W/0/1/0/all/0/1&quot;&gt;Wenjing Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Yang_W/0/1/0/all/0/1&quot;&gt;Wenhan Yang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Liu_J/0/1/0/all/0/1&quot;&gt;Jiaying Liu&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.08913">
<title>Towards the Sparseness of Projection Head in Self-Supervised Learning. (arXiv:2307.08913v2 [cs.LG] UPDATED)</title>
<link>http://arxiv.org/abs/2307.08913</link>
<description rdf:parseType="Literal">&lt;p&gt;In recent years, self-supervised learning (SSL) has emerged as a promising
approach for extracting valuable representations from unlabeled data. One
successful SSL method is contrastive learning, which aims to bring positive
examples closer while pushing negative examples apart. Many current contrastive
learning approaches utilize a parameterized projection head. Through a
combination of empirical analysis and theoretical investigation, we provide
insights into the internal mechanisms of the projection head and its
relationship with the phenomenon of dimensional collapse. Our findings
demonstrate that the projection head enhances the quality of representations by
performing contrastive loss in a projected subspace. Therefore, we propose an
assumption that only a subset of features is necessary when minimizing the
contrastive loss of a mini-batch of data. Theoretical analysis further suggests
that a sparse projection head can enhance generalization, leading us to
introduce SparseHead - a regularization term that effectively constrains the
sparsity of the projection head, and can be seamlessly integrated with any
self-supervised learning (SSL) approaches. Our experimental results validate
the effectiveness of SparseHead, demonstrating its ability to improve the
performance of existing contrastive methods.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Song_Z/0/1/0/all/0/1&quot;&gt;Zeen Song&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Su_X/0/1/0/all/0/1&quot;&gt;Xingzhe Su&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wang_J/0/1/0/all/0/1&quot;&gt;Jingyao Wang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Qiang_W/0/1/0/all/0/1&quot;&gt;Wenwen Qiang&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Zheng_C/0/1/0/all/0/1&quot;&gt;Changwen Zheng&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sun_F/0/1/0/all/0/1&quot;&gt;Fuchun Sun&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09023">
<title>LA-Net: Landmark-Aware Learning for Reliable Facial Expression Recognition under Label Noise. (arXiv:2307.09023v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09023</link>
<description rdf:parseType="Literal">&lt;p&gt;Facial expression recognition (FER) remains a challenging task due to the
ambiguity of expressions. The derived noisy labels significantly harm the
performance in real-world scenarios. To address this issue, we present a new
FER model named Landmark-Aware Net~(LA-Net), which leverages facial landmarks
to mitigate the impact of label noise from two perspectives. Firstly, LA-Net
uses landmark information to suppress the uncertainty in expression space and
constructs the label distribution of each sample by neighborhood aggregation,
which in turn improves the quality of training supervision. Secondly, the model
incorporates landmark information into expression representations using the
devised expression-landmark contrastive loss. The enhanced expression feature
extractor can be less susceptible to label noise. Our method can be integrated
with any deep neural network for better training supervision without
introducing extra inference costs. We conduct extensive experiments on both
in-the-wild datasets and synthetic noisy datasets and demonstrate that LA-Net
achieves state-of-the-art performance.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wu_Z/0/1/0/all/0/1&quot;&gt;Zhiyu Wu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cui_J/0/1/0/all/0/1&quot;&gt;Jinshi Cui&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09153">
<title>OPHAvatars: One-shot Photo-realistic Head Avatars. (arXiv:2307.09153v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09153</link>
<description rdf:parseType="Literal">&lt;p&gt;We propose a method for synthesizing photo-realistic digital avatars from
only one portrait as the reference. Given a portrait, our method synthesizes a
coarse talking head video using driving keypoints features. And with the coarse
video, our method synthesizes a coarse talking head avatar with a deforming
neural radiance field. With rendered images of the coarse avatar, our method
updates the low-quality images with a blind face restoration model. With
updated images, we retrain the avatar for higher quality. After several
iterations, our method can synthesize a photo-realistic animatable 3D neural
head avatar. The motivation of our method is deformable neural radiance field
can eliminate the unnatural distortion caused by the image2video method. Our
method outperforms state-of-the-art methods in quantitative and qualitative
studies on various subjects.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Li_S/0/1/0/all/0/1&quot;&gt;Shaoxu Li&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09362">
<title>Disentangle then Parse:Night-time Semantic Segmentation with Illumination Disentanglement. (arXiv:2307.09362v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09362</link>
<description rdf:parseType="Literal">&lt;p&gt;Most prior semantic segmentation methods have been developed for day-time
scenes, while typically underperforming in night-time scenes due to
insufficient and complicated lighting conditions. In this work, we tackle this
challenge by proposing a novel night-time semantic segmentation paradigm, i.e.,
disentangle then parse (DTP). DTP explicitly disentangles night-time images
into light-invariant reflectance and light-specific illumination components and
then recognizes semantics based on their adaptive fusion. Concretely, the
proposed DTP comprises two key components: 1) Instead of processing
lighting-entangled features as in prior works, our Semantic-Oriented
Disentanglement (SOD) framework enables the extraction of reflectance component
without being impeded by lighting, allowing the network to consistently
recognize the semantics under cover of varying and complicated lighting
conditions. 2) Based on the observation that the illumination component can
serve as a cue for some semantically confused regions, we further introduce an
Illumination-Aware Parser (IAParser) to explicitly learn the correlation
between semantics and lighting, and aggregate the illumination features to
yield more precise predictions. Extensive experiments on the night-time
segmentation task with various settings demonstrate that DTP significantly
outperforms state-of-the-art methods. Furthermore, with negligible additional
parameters, DTP can be directly used to benefit existing day-time methods for
night-time segmentation.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Wei_Z/0/1/0/all/0/1&quot;&gt;Zhixiang Wei&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_L/0/1/0/all/0/1&quot;&gt;Lin Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tu_T/0/1/0/all/0/1&quot;&gt;Tao Tu&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Chen_H/0/1/0/all/0/1&quot;&gt;Huaian Chen&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Ling_P/0/1/0/all/0/1&quot;&gt;Pengyang Ling&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Jin_Y/0/1/0/all/0/1&quot;&gt;Yi Jin&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09416">
<title>Let&apos;s ViCE! Mimicking Human Cognitive Behavior in Image Generation Evaluation. (arXiv:2307.09416v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09416</link>
<description rdf:parseType="Literal">&lt;p&gt;Research in Image Generation has recently made significant progress,
particularly boosted by the introduction of Vision-Language models which are
able to produce high-quality visual content based on textual inputs. Despite
ongoing advancements in terms of generation quality and realism, no methodical
frameworks have been defined yet to quantitatively measure the quality of the
generated content and the adherence with the prompted requests: so far, only
human-based evaluations have been adopted for quality satisfaction and for
comparing different generative methods. We introduce a novel automated method
for Visual Concept Evaluation (ViCE), i.e. to assess consistency between a
generated/edited image and the corresponding prompt/instructions, with a
process inspired by the human cognitive behaviour. ViCE combines the strengths
of Large Language Models (LLMs) and Visual Question Answering (VQA) into a
unified pipeline, aiming to replicate the human cognitive process in quality
assessment. This method outlines visual concepts, formulates image-specific
verification questions, utilizes the Q&amp;amp;A system to investigate the image, and
scores the combined outcome. Although this brave new hypothesis of mimicking
humans in the image evaluation process is in its preliminary assessment stage,
results are promising and open the door to a new form of automatic evaluation
which could have significant impact as the image generation or the image target
editing tasks become more and more sophisticated.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Betti_F/0/1/0/all/0/1&quot;&gt;Federico Betti&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Staiano_J/0/1/0/all/0/1&quot;&gt;Jacopo Staiano&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Baraldi_L/0/1/0/all/0/1&quot;&gt;Lorenzo Baraldi&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Cucchiara_R/0/1/0/all/0/1&quot;&gt;Rita Cucchiara&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sebe_N/0/1/0/all/0/1&quot;&gt;Nicu Sebe&lt;/a&gt;</dc:creator>
</item>
<item rdf:about="http://arxiv.org/abs/2307.09456">
<title>A comparative analysis of SRGAN models. (arXiv:2307.09456v2 [cs.CV] UPDATED)</title>
<link>http://arxiv.org/abs/2307.09456</link>
<description rdf:parseType="Literal">&lt;p&gt;In this study, we evaluate the performance of multiple state-of-the-art SRGAN
(Super Resolution Generative Adversarial Network) models, ESRGAN, Real-ESRGAN
and EDSR, on a benchmark dataset of real-world images which undergo degradation
using a pipeline. Our results show that some models seem to significantly
increase the resolution of the input images while preserving their visual
quality, this is assessed using Tesseract OCR engine. We observe that EDSR-BASE
model from huggingface outperforms the remaining candidate models in terms of
both quantitative metrics and subjective visual quality assessments with least
compute overhead. Specifically, EDSR generates images with higher peak
signal-to-noise ratio (PSNR) and structural similarity index (SSIM) values and
are seen to return high quality OCR results with Tesseract OCR engine. These
findings suggest that EDSR is a robust and effective approach for single-image
super-resolution and may be particularly well-suited for applications where
high-quality visual fidelity is critical and optimized compute.
&lt;/p&gt;
</description>
<dc:creator> &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Nikroo_F/0/1/0/all/0/1&quot;&gt;Fatemeh Rezapoor Nikroo&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Deshmukh_A/0/1/0/all/0/1&quot;&gt;Ajinkya Deshmukh&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Sharma_A/0/1/0/all/0/1&quot;&gt;Anantha Sharma&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Tam_A/0/1/0/all/0/1&quot;&gt;Adrian Tam&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Kumar_K/0/1/0/all/0/1&quot;&gt;Kaarthik Kumar&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Norris_C/0/1/0/all/0/1&quot;&gt;Cleo Norris&lt;/a&gt;, &lt;a href=&quot;http://arxiv.org/find/cs/1/au:+Dangi_A/0/1/0/all/0/1&quot;&gt;Aditya Dangi&lt;/a&gt;</dc:creator>
</item>
</rdf:RDF>